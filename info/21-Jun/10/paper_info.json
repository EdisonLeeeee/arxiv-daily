[
  {
    "id": "arXiv:2106.04575",
    "title": "DNS attack mitigation Using OpenStack Isolation",
    "abstract": "The Domain Name System (DNS) is essential for the Internet, giving a\nmechanism to resolve hostnames into Internet Protocol (IP) addresses. DNS is\nknown as the world's largest distributed database that manages hostnames and\nInternet Protocol. By having the DNS, only simple names that can be easily\nmemorized will be used and then the domain name system will map it into the\nnumeric Internet Protocol addresses that are used by computers to communicate.\nThis research aims to propose a model for the development of a private cloud\ninfrastructure to host DNS. The cloud infrastructure will be created using the\nOpenStack software platform where each server will be hosted separately in a\ndifferent virtual machine. Virtual network architecture will be created using\nthe Software Defined Networking (SDN) approach and it will be secured using\nFirewall as a Service (FWaaS). By hosting DNS in private cloud infrastructure,\nthe DNS servers will be out of reach by attackers which will prevent DNS\nattacks. Besides, available research had proven that the cloud is the best\nchoice for DNS. A prototype had been implemented and evaluated for its\nefficiencies. The findings from the evaluation carried out shown a positive\nresult.",
    "descriptor": "\nComments: 6 pages, 3 figures, and 2 tables\n",
    "authors": [
      "Hassnain ul hassan",
      "Rizal Mohd Nor",
      "Md Amiruzzaman",
      "Sharyar Wani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04575"
  },
  {
    "id": "arXiv:2106.04590",
    "title": "PEARL: Data Synthesis via Private Embeddings and Adversarial  Reconstruction Learning",
    "abstract": "We propose a new framework of synthesizing data using deep generative models\nin a differentially private manner. Within our framework, sensitive data are\nsanitized with rigorous privacy guarantees in a one-shot fashion, such that\ntraining deep generative models is possible without re-using the original data.\nHence, no extra privacy costs or model constraints are incurred, in contrast to\npopular approaches such as Differentially Private Stochastic Gradient Descent\n(DP-SGD), which, among other issues, causes degradation in privacy guarantees\nas the training iteration increases. We demonstrate a realization of our\nframework by making use of the characteristic function and an adversarial\nre-weighting objective, which are of independent interest as well. Our proposal\nhas theoretical guarantees of performance, and empirical evaluations on\nmultiple datasets show that our approach outperforms other methods at\nreasonable levels of privacy.",
    "descriptor": "\nComments: 23 pages, 8 figures, 3 tables\n",
    "authors": [
      "Seng Pei Liew",
      "Tsubasa Takahashi",
      "Michihiko Ueno"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04590"
  },
  {
    "id": "arXiv:2106.04605",
    "title": "Check It Again: Progressive Visual Question Answering via Visual  Entailment",
    "abstract": "While sophisticated Visual Question Answering models have achieved remarkable\nsuccess, they tend to answer questions only according to superficial\ncorrelations between question and answer. Several recent approaches have been\ndeveloped to address this language priors problem. However, most of them\npredict the correct answer according to one best output without checking the\nauthenticity of answers. Besides, they only explore the interaction between\nimage and question, ignoring the semantics of candidate answers. In this paper,\nwe propose a select-and-rerank (SAR) progressive framework based on Visual\nEntailment. Specifically, we first select the candidate answers relevant to the\nquestion or the image, then we rerank the candidate answers by a visual\nentailment task, which verifies whether the image semantically entails the\nsynthetic statement of the question and each candidate answer. Experimental\nresults show the effectiveness of our proposed framework, which establishes a\nnew state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.",
    "descriptor": "\nComments: ACL-2021\n",
    "authors": [
      "Qingyi Si",
      "Zheng Lin",
      "Mingyu Zheng",
      "Peng Fu",
      "Weiping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04605"
  },
  {
    "id": "arXiv:2106.04612",
    "title": "Neural Extractive Search",
    "abstract": "Domain experts often need to extract structured information from large\ncorpora. We advocate for a search paradigm called ``extractive search'', in\nwhich a search query is enriched with capture-slots, to allow for such rapid\nextraction. Such an extractive search system can be built around syntactic\nstructures, resulting in high-precision, low-recall results. We show how the\nrecall can be improved using neural retrieval and alignment. The goals of this\npaper are to concisely introduce the extractive-search paradigm; and to\ndemonstrate a prototype neural retrieval system for extractive search and its\nbenefits and potential. Our prototype is available at\n\\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is\navailable at \\url{https://vimeo.com/559586687}.",
    "descriptor": "\nComments: Accepted as a demo paper in ACL2021\n",
    "authors": [
      "Shauli Ravfogel",
      "Hillel Taub-Tabib",
      "Yoav Goldberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04612"
  },
  {
    "id": "arXiv:2106.04615",
    "title": "Vector Quantized Models for Planning",
    "abstract": "Recent developments in the field of model-based RL have proven successful in\na range of environments, especially ones where planning is essential. However,\nsuch successes have been limited to deterministic fully-observed environments.\nWe present a new approach that handles stochastic and partially-observable\nenvironments. Our key insight is to use discrete autoencoders to capture the\nmultiple possible effects of an action in a stochastic environment. We use a\nstochastic variant of \\emph{Monte Carlo tree search} to plan over both the\nagent's actions and the discrete latent variables representing the\nenvironment's response. Our approach significantly outperforms an offline\nversion of MuZero on a stochastic interpretation of chess where the opponent is\nconsidered part of the environment. We also show that our approach scales to\n\\emph{DeepMind Lab}, a first-person 3D environment with large visual\nobservations and partial observability.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Sherjil Ozair",
      "Yazhe Li",
      "Ali Razavi",
      "Ioannis Antonoglou",
      "A\u00e4ron van den Oord",
      "Oriol Vinyals"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04615"
  },
  {
    "id": "arXiv:2106.04618",
    "title": "EXPObench: Benchmarking Surrogate-based Optimisation Algorithms on  Expensive Black-box Functions",
    "abstract": "Surrogate algorithms such as Bayesian optimisation are especially designed\nfor black-box optimisation problems with expensive objectives, such as\nhyperparameter tuning or simulation-based optimisation. In the literature,\nthese algorithms are usually evaluated with synthetic benchmarks which are well\nestablished but have no expensive objective, and only on one or two real-life\napplications which vary wildly between papers. There is a clear lack of\nstandardisation when it comes to benchmarking surrogate algorithms on\nreal-life, expensive, black-box objective functions. This makes it very\ndifficult to draw conclusions on the effect of algorithmic contributions. A new\nbenchmark library, EXPObench, provides first steps towards such a\nstandardisation. The library is used to provide an extensive comparison of six\ndifferent surrogate algorithms on four expensive optimisation problems from\ndifferent real-life applications. This has led to new insights regarding the\nrelative importance of exploration, the evaluation time of the objective, and\nthe used model. A further contribution is that we make the algorithms and\nbenchmark problem instances publicly available, contributing to more uniform\nanalysis of surrogate algorithms. Most importantly, we include the performance\nof the six algorithms on all evaluated problem instances. This results in a\nunique new dataset that lowers the bar for researching new methods as the\nnumber of expensive evaluations required for comparison is significantly\nreduced.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Laurens Bliek",
      "Arthur Guijt",
      "Rickard Karlsson",
      "Sicco Verwer",
      "Mathijs de Weerdt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04618"
  },
  {
    "id": "arXiv:2106.04625",
    "title": "Don't Get Yourself into Trouble! Risk-aware Decision-Making for  Autonomous Vehicles",
    "abstract": "Risk is traditionally described as the expected likelihood of an undesirable\noutcome, such as collisions for autonomous vehicles. Accurately predicting risk\nor potentially risky situations is critical for the safe operation of\nautonomous vehicles. In our previous work, we showed that risk could be\ncharacterized by two components: 1) the probability of an undesirable outcome\nand 2) an estimate of how undesirable the outcome is (loss). This paper is an\nextension to our previous work. In this paper, using our trained deep\nreinforcement learning model for navigating around crowds, we developed a\nrisk-based decision-making framework for the autonomous vehicle that integrates\nthe high-level risk-based path planning with the reinforcement learning-based\nlow-level control. We evaluated our method in a high-fidelity simulation such\nas CARLA. This work can improve safety by allowing an autonomous vehicle to one\nday avoid and react to risky situations.",
    "descriptor": "\nComments: 8 pages, 4 Figures, 2 Tables\n",
    "authors": [
      "Kasra Mokhtari",
      "Alan R. Wagner"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04625"
  },
  {
    "id": "arXiv:2106.04627",
    "title": "Densely connected normalizing flows",
    "abstract": "Normalizing flows are bijective mappings between inputs and latent\nrepresentations with a fully factorized distribution. They are very attractive\ndue to exact likelihood evaluation and efficient sampling. However, their\neffective capacity is often insufficient since the bijectivity constraint\nlimits the model width. We address this issue by incrementally padding\nintermediate representations with noise. We precondition the noise in\naccordance with previous invertible units, which we describe as cross-unit\ncoupling. Our invertible glow-like modules express intra-unit affine coupling\nas a fusion of a densely connected block and Nystr\\\"om self-attention. We refer\nto our architecture as DenseFlow since both cross-unit and intra-unit couplings\nrely on dense connectivity. Experiments show significant improvements due to\nthe proposed contributions, and reveal state-of-the-art density estimation\namong all generative models under moderate computing budgets.",
    "descriptor": "",
    "authors": [
      "Matej Grci\u0107",
      "Ivan Grubi\u0161i\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04627"
  },
  {
    "id": "arXiv:2106.04629",
    "title": "New Competitive Semi-online Scheduling Algorithms for Small Number of  Identical Machines",
    "abstract": "Design and analysis of constant competitive deterministic semi-online\nalgorithms for the multi-processor scheduling problem with small number of\nidentical machines have gained significant research interest in the last two\ndecades. In the semi-online scheduling problem for makespan minimization, we\nare given a sequence of independent jobs one by one in order and upon arrival,\neach job must be allocated to a machine with prior knowledge of some Extra\nPiece of Information (EPI) about the future jobs. Researchers have designed\nmultiple variants of semi-online scheduling algorithms with constant\ncompetitive ratios by considering one or more EPI. In this paper, we propose\nfour new variants of competitive deterministic semi-online algorithms for\nsmaller number of identical machines by considering two EPI such as Decr and\nSum. We obtain improved upper bound and lower bound results on the competitive\nratio for our proposed algorithms, which are comparable to the best known\nresults in the literature. In two identical machines setting with known Sum, we\nshow a tight bound of 1.33 on the competitive ratio by considering a sequence\nof equal size jobs. In the same setting we achieve a lower bound of 1.04 and an\nupper bound of 1.16 by considering Sum and a sequence of jobs arriving in order\nof decreasing sizes. For three identical machines setting with known Decr and\nSum, we show a lower bound of 1.11 on the competitive ratio. In this setting,\nwe obtain an upper bound of 1.5 for scheduling a sequence of equal size jobs\nand achieves an upper bound of 1.2 by considering a sequence of decreasing size\njobs. Further we develop an improved competitive algorithm with an upper bound\nof 1.11 on the competitive ratio.",
    "descriptor": "\nComments: 24 Pages, 4 Tables\n",
    "authors": [
      "Debasis Dwibedy",
      "Rakesh Mohanty"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04629"
  },
  {
    "id": "arXiv:2106.04630",
    "title": "PAM: Understanding Product Images in Cross Product Category Attribute  Extraction",
    "abstract": "Understanding product attributes plays an important role in improving online\nshopping experience for customers and serves as an integral part for\nconstructing a product knowledge graph. Most existing methods focus on\nattribute extraction from text description or utilize visual information from\nproduct images such as shape and color. Compared to the inputs considered in\nprior works, a product image in fact contains more information, represented by\na rich mixture of words and visual clues with a layout carefully designed to\nimpress customers. This work proposes a more inclusive framework that fully\nutilizes these different modalities for attribute extraction. Inspired by\nrecent works in visual question answering, we use a transformer based sequence\nto sequence model to fuse representations of product text, Optical Character\nRecognition (OCR) tokens and visual objects detected in the product image. The\nframework is further extended with the capability to extract attribute value\nacross multiple product categories with a single model, by training the decoder\nto predict both product category and attribute value and conditioning its\noutput on product category. The model provides a unified attribute extraction\nsolution desirable at an e-commerce platform that offers numerous product\ncategories with a diverse body of product attributes. We evaluated the model on\ntwo product attributes, one with many possible values and one with a small set\nof possible values, over 14 product categories and found the model could\nachieve 15% gain on the Recall and 10% gain on the F1 score compared to\nexisting methods using text-only features.",
    "descriptor": "\nComments: KDD 2021\n",
    "authors": [
      "Rongmei Lin",
      "Xiang He",
      "Jie Feng",
      "Nasser Zalmout",
      "Yan Liang",
      "Li Xiong",
      "Xin Luna Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04630"
  },
  {
    "id": "arXiv:2106.04631",
    "title": "On the Lack of Robust Interpretability of Neural Text Classifiers",
    "abstract": "With the ever-increasing complexity of neural language models, practitioners\nhave turned to methods for understanding the predictions of these models. One\nof the most well-adopted approaches for model interpretability is feature-based\ninterpretability, i.e., ranking the features in terms of their impact on model\npredictions. Several prior studies have focused on assessing the fidelity of\nfeature-based interpretability methods, i.e., measuring the impact of dropping\nthe top-ranked features on the model output. However, relatively little work\nhas been conducted on quantifying the robustness of interpretations. In this\nwork, we assess the robustness of interpretations of neural text classifiers,\nspecifically, those based on pretrained Transformer encoders, using two\nrandomization tests. The first compares the interpretations of two models that\nare identical except for their initializations. The second measures whether the\ninterpretations differ between a model with trained parameters and a model with\nrandom parameters. Both tests show surprising deviations from expected\nbehavior, raising questions about the extent of insights that practitioners may\ndraw from interpretations.",
    "descriptor": "\nComments: Appearing at ACL Findings 2021\n",
    "authors": [
      "Muhammad Bilal Zafar",
      "Michele Donini",
      "Dylan Slack",
      "C\u00e9dric Archambeau",
      "Sanjiv Das",
      "Krishnaram Kenthapadi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04631"
  },
  {
    "id": "arXiv:2106.04632",
    "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding  Evaluation",
    "abstract": "Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-leaderboard.github.io/.",
    "descriptor": "\nComments: VALUE is available at this https URL\n",
    "authors": [
      "Linjie Li",
      "Jie Lei",
      "Zhe Gan",
      "Licheng Yu",
      "Yen-Chun Chen",
      "Rohit Pillai",
      "Yu Cheng",
      "Luowei Zhou",
      "Xin Eric Wang",
      "William Yang Wang",
      "Tamara Lee Berg",
      "Mohit Bansal",
      "Jingjing Liu",
      "Lijuan Wang",
      "Zicheng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04632"
  },
  {
    "id": "arXiv:2106.04639",
    "title": "Optimising Hearing Aid Fittings for Speech in Noise with a  Differentiable Hearing Loss Model",
    "abstract": "Current hearing aids normally provide amplification based on a general\nprescriptive fitting, and the benefits provided by the hearing aids vary among\ndifferent listening environments despite the inclusion of noise suppression\nfeature. Motivated by this fact, this paper proposes a data-driven machine\nlearning technique to develop hearing aid fittings that are customised to\nspeech in different noisy environments. A differentiable hearing loss model is\nproposed and used to optimise fittings with back-propagation. The customisation\nis reflected on the data of speech in different noise with also the\nconsideration of noise suppression. The objective evaluation shows the\nadvantages of optimised custom fittings over general prescriptive fittings.",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Zehai Tu",
      "Ning Ma",
      "Jon Barker"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04639"
  },
  {
    "id": "arXiv:2106.04641",
    "title": "Predicting the Success of Domain Adaptation in Text Similarity",
    "abstract": "Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.",
    "descriptor": "",
    "authors": [
      "Nicolai Pogrebnyakov",
      "Shohreh Shaghaghian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04641"
  },
  {
    "id": "arXiv:2106.04647",
    "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
    "abstract": "Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers.\nSpecifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared ``slow'' weights and ``fast'' rank-one\nmatrices defined per Compacter layer. By only training 0.047% of a pretrained\nmodel's parameters, Compacter performs on par with standard fine-tuning on GLUE\nand outperforms fine-tuning in low-resource settings. Our code is publicly\navailable in https://github.com/rabeehk/compacter/",
    "descriptor": "",
    "authors": [
      "Rabeeh Karimi Mahabadi",
      "James Henderson",
      "Sebastian Ruder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04647"
  },
  {
    "id": "arXiv:2106.04653",
    "title": "Comprehension Based Question Answering using Bloom's Taxonomy",
    "abstract": "Current pre-trained language models have lots of knowledge, but a more\nlimited ability to use that knowledge. Bloom's Taxonomy helps educators teach\nchildren how to use knowledge by categorizing comprehension skills, so we use\nit to analyze and improve the comprehension skills of large pre-trained\nlanguage models. Our experiments focus on zero-shot question answering, using\nthe taxonomy to provide proximal context that helps the model answer questions\nby being relevant to those questions. We show targeting context in this manner\nimproves performance across 4 popular common sense question answer datasets.",
    "descriptor": "",
    "authors": [
      "Pritish Sahu",
      "Michael Cogswell",
      "Sara Rutherford-Quach",
      "Ajay Divakaran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04653"
  },
  {
    "id": "arXiv:2106.04655",
    "title": "Dynamic Software Updates for Unmodified Browsers through Multi-Version  Execution",
    "abstract": "In this paper, we present the design, implementation, and evaluation of\nSINATRA, which supports instantaneous browser updates that do not result in any\ndata loss through a novel Multi-Version eXecution (MVX) approach for JavaScript\nprograms. SINATRA works in pure JavaScript, does not require any browser\nsupport, thus works on closed-source browsers, and requires trivial changes to\neach target page, that can be automated. First, SINATRA captures all the\nnon-determinism available to a JavaScript program (e.g., event handlers\nexecuted, expired timers, invocations of Math.random). Our evaluation shows\nthat SINATRA requires 5MB to store such events, and the memory grows at a\nmodest rate of 23.1KB/s as the user keeps interacting with each page. When an\nupdate becomes available, SINATRA transfer the state by re-executing the same\nset of non-deterministic events on the new browser. During this time, which can\nbe as long as 13 seconds, SINATRA uses MVX to allow the user to keep\ninteracting with the old browser. Finally, SINATRA changes the roles in 353ms,\nand the user starts interacting with the new browser, effectively performing a\nbrowser update with zero downtime and no loss of state.",
    "descriptor": "\nComments: 23 pages, 5 tables, 8 figures\n",
    "authors": [
      "Siddhanth Venkateshwaran",
      "Ellen Kidane",
      "Lu\u00eds Pina"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.04655"
  },
  {
    "id": "arXiv:2106.04656",
    "title": "Probabilistic Neural Network to Quantify Uncertainty of Wind Power  Estimation",
    "abstract": "Each year a growing number of wind farms are being added to power grids to\ngenerate electricity. The power curve of a wind turbine, which exhibits the\nrelationship between generated power and wind speed, plays a major role in\nassessing the performance of a wind farm. Neural networks have been used for\npower curve estimation. However, they do not produce a confidence measure for\ntheir output, unless computationally prohibitive Bayesian methods are used. In\nthis paper, a probabilistic neural network with Monte Carlo dropout is\nconsidered to quantify the model (epistemic) uncertainty of the power curve\nestimation. This approach offers a minimal increase in computational complexity\nover deterministic approaches. Furthermore, by incorporating a probabilistic\nloss function, the noise or aleatoric uncertainty in the data is estimated. The\ndeveloped network captures both model and noise uncertainty which is found to\nbe useful tools in assessing performance. Also, the developed network is\ncompared with existing ones across a public domain dataset showing superior\nperformance in terms of prediction accuracy.",
    "descriptor": "",
    "authors": [
      "Farzad Karami",
      "Nasser Kehtarnavaz",
      "Mario Rotea"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04656"
  },
  {
    "id": "arXiv:2106.04660",
    "title": "Sequential End-to-End Intent and Slot Label Classification and  Localization",
    "abstract": "Human-computer interaction (HCI) is significantly impacted by delayed\nresponses from a spoken dialogue system. Hence, end-to-end (e2e) spoken\nlanguage understanding (SLU) solutions have recently been proposed to decrease\nlatency. Such approaches allow for the extraction of semantic information\ndirectly from the speech signal, thus bypassing the need for a transcript from\nan automatic speech recognition (ASR) system. In this paper, we propose a\ncompact e2e SLU architecture for streaming scenarios, where chunks of the\nspeech signal are processed continuously to predict intent and slot values. Our\nmodel is based on a 3D convolutional neural network (3D-CNN) and a\nunidirectional long short-term memory (LSTM). We compare the performance of two\nalignment-free losses: the connectionist temporal classification (CTC) method\nand its adapted version, namely connectionist temporal localization (CTL). The\nlatter performs not only the classification but also localization of sequential\naudio events. The proposed solution is evaluated on the Fluent Speech Command\ndataset and results show our model ability to process incoming speech signal,\nreaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on\nsingle-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL\non two-label prediction.",
    "descriptor": "\nComments: Accepted at Interspeech 2021\n",
    "authors": [
      "Yiran Cao",
      "Nihal Potdar",
      "Anderson R. Avila"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04660"
  },
  {
    "id": "arXiv:2106.04661",
    "title": "Towards a Survey of Visualization Methods for Power Grids",
    "abstract": "While many visualization techniques have been developed in recent years, few\nhave been applied to power grid visualization. With the ongoing emergence of\nsmart and distributed grids, it becomes increasingly important to improve and\nmodernize legacy infrastructure. Visualization and simulation of power grids\nunder real-life scenarios can help with and sometimes even discover new\napproaches and techniques that significantly improve and simplify network\nanalysis, maintenance, and planning. This enables operators to spot key issues\nwhich are hard to detect otherwise. Creating visualizations for these problems\nis challenging as no obvious or trivial solutions exist, and many of them are\nassociated with multi-dimensional data. This paper aims to provide a\ncomprehensive overview of the methods developed for the visualization of power\ngrids while evaluating the advantages and disadvantages of the single\napproaches. An outlook discusses open research questions and possible further\nimprovements to the field.",
    "descriptor": "\nComments: 11 pages, 6 figures, 1 table, 2018\n",
    "authors": [
      "Maximilian T. Fischer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04661"
  },
  {
    "id": "arXiv:2106.04662",
    "title": "On the Explanation of Similarity for Developing and Deploying CBR  Systems",
    "abstract": "During the early stages of developing Case-Based Reasoning (CBR) systems the\ndefinition of similarity measures is challenging since this task requires\ntransferring implicit knowledge of domain experts into knowledge\nrepresentations. While an entire CBR system is very explanatory, the similarity\nmeasure determines the ranking but do not necessarily show which features\ncontribute to high (or low) rankings. In this paper we present our work on\nopening the knowledge engineering process for similarity modelling. This work\npresent is a result of an interdisciplinary research collaboration between AI\nand public health researchers developing e-Health applications. During this\nwork explainability and transparency of the development process is crucial to\nallow in-depth quality assurance of the by the domain experts.",
    "descriptor": "\nComments: 5 pages, 5 figures, Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference\n",
    "authors": [
      "Kerstin Bach",
      "Paul Jarle Mork"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04662"
  },
  {
    "id": "arXiv:2106.04663",
    "title": "Solving Structured Hierarchical Games Using Differential Backward  Induction",
    "abstract": "Many real-world systems possess a hierarchical structure where a strategic\nplan is forwarded and implemented in a top-down manner. Examples include\nbusiness activities in large companies or policy making for reducing the spread\nduring pandemics. We introduce a novel class of games that we call structured\nhierarchical games (SHGs) to capture these strategic interactions. In an SHG,\neach player is represented as a vertex in a multi-layer decision tree and\ncontrols a real-valued action vector reacting to orders from its predecessors\nand influencing its descendants' behaviors strategically based on its own\nsubjective utility. SHGs generalize extensive form games as well as Stackelberg\ngames. For general SHGs with (possibly) nonconvex payoffs and high-dimensional\naction spaces, we propose a new solution concept which we call local subgame\nperfect equilibrium. By exploiting the hierarchical structure and strategic\ndependencies in payoffs, we derive a back propagation-style gradient-based\nalgorithm which we call Differential Backward Induction to compute an\nequilibrium. We theoretically characterize the convergence properties of DBI\nand empirically demonstrate a large overlap between the stable points reached\nby DBI and equilibrium solutions. Finally, we demonstrate the effectiveness of\nour algorithm in finding \\emph{globally} stable solutions and its scalability\nfor a recently introduced class of SHGs for pandemic policy making.",
    "descriptor": "",
    "authors": [
      "Zun Li",
      "Feiran Jia",
      "Aditya Mate",
      "Shahin Jabbari",
      "Mithun Chakraborty",
      "Milind Tambe",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.04663"
  },
  {
    "id": "arXiv:2106.04664",
    "title": "zbMATH Open: API Solutions and Research Challenges",
    "abstract": "We present zbMATH Open, the most comprehensive collection of reviews and\nbibliographic metadata of scholarly literature in mathematics. Besides our\nwebsite https://zbMATH.org which is openly accessible since the beginning of\nthis year, we provide API endpoints to offer our data. The API improves\ninteroperability with others, i.e., digital libraries, and allows using our\ndata for research purposes. In this article, we\n(1) illustrate the current and future overview of the services offered by\nzbMATH;\n(2) present the initial version of the zbMATH links API;\n(3) analyze potentials and limitations of the links API based on the example\nof the NIST Digital Library of Mathematical Functions;\n(4) and finally, present the zbMATH Open dataset as a research resource and\ndiscuss connected open research problems.",
    "descriptor": "",
    "authors": [
      "Matteo Petrera",
      "Dennis Trautwein andIsabel Beckenbach",
      "Dariush Ehsani",
      "FabianMueller",
      "Olaf Teschke",
      "Bela Gipp",
      "Moritz Schubotz"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.04664"
  },
  {
    "id": "arXiv:2106.04668",
    "title": "Dynamic Instance-Wise Classification in Correlated Feature Spaces",
    "abstract": "In a typical supervised machine learning setting, the predictions on all test\ninstances are based on a common subset of features discovered during model\ntraining. However, using a different subset of features that is most\ninformative for each test instance individually may not only improve prediction\naccuracy, but also the overall interpretability of the model. At the same time,\nfeature selection methods for classification have been known to be the most\neffective when many features are irrelevant and/or uncorrelated. In fact,\nfeature selection ignoring correlations between features can lead to poor\nclassification performance. In this work, a Bayesian network is utilized to\nmodel feature dependencies. Using the dependency network, a new method is\nproposed that sequentially selects the best feature to evaluate for each test\ninstance individually, and stops the selection process to make a prediction\nonce it determines that no further improvement can be achieved with respect to\nclassification accuracy. The optimum number of features to acquire and the\noptimum classification strategy are derived for each test instance. The\ntheoretical properties of the optimum solution are analyzed, and a new\nalgorithm is proposed that takes advantage of these properties to implement a\nrobust and scalable solution for high dimensional settings. The effectiveness,\ngeneralizability, and scalability of the proposed method is illustrated on a\nvariety of real-world datasets from diverse application domains.",
    "descriptor": "",
    "authors": [
      "Yasitha Warahena Liyanage",
      "Daphney-Stavroula Zois",
      "Charalampos Chelmis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04668"
  },
  {
    "id": "arXiv:2106.04675",
    "title": "Streetonomics: Quantifying Culture Using Street Names",
    "abstract": "Quantifying a society's value system is important because it suggests what\npeople deeply care about -- it reflects who they actually are and, more\nimportantly, who they will like to be. This cultural quantification has been\ntypically done by studying literary production. However, a society's value\nsystem might well be implicitly quantified based on the decisions that people\ntook in the past and that were mediated by what they care about. It turns out\nthat one class of these decisions is visible in ordinary settings: it is\nvisible in street names. We studied the names of 4,932 honorific streets in the\ncities of Paris, Vienna, London and New York. We chose these four cities\nbecause they were important centers of cultural influence for the Western world\nin the 20th century. We found that street names greatly reflect the extent to\nwhich a society is gender biased, which professions are considered elite ones,\nand the extent to which a city is influenced by the rest of the world. This way\nof quantifying a society's value system promises to inform new methodologies in\nDigital Humanities; makes it possible for municipalities to reflect on their\npast to inform their future; and informs the design of everyday's educational\ntools that promote historical awareness in a playful way.",
    "descriptor": "\nComments: 17 pages, 6 figures, 2 tables\n",
    "authors": [
      "Melanie Bancilhon",
      "Marios Constantinides",
      "Edyta Paulina Bogucka",
      "Luca Maria Aiello",
      "Daniele Quercia"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.04675"
  },
  {
    "id": "arXiv:2106.04677",
    "title": "Entropy of the Conditional Expectation under Gaussian Noise",
    "abstract": "This paper considers an additive Gaussian noise channel with arbitrarily\ndistributed finite variance input signals. It studies the differential entropy\nof the minimum mean-square error (MMSE) estimator and provides a new lower\nbound which connects the entropy of the input, output, and conditional mean.\nThat is, the sum of entropies of the conditional mean and output is always\ngreater than or equal to twice the input entropy. Various other properties such\nas upper bounds, asymptotics, Taylor series expansion, and connection to Fisher\nInformation are obtained. An application of the lower bound in the\nremote-source coding problem is discussed, and extensions of the lower and\nupper bounds to the vector Gaussian channel are given.",
    "descriptor": "",
    "authors": [
      "Arda Atalik",
      "Alper K\u00f6se",
      "Michael Gastpar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04677"
  },
  {
    "id": "arXiv:2106.04678",
    "title": "Incentivizing Efficient Equilibria in Traffic Networks with Mixed  Autonomy",
    "abstract": "Traffic congestion has large economic and social costs. The introduction of\nautonomous vehicles can potentially reduce this congestion by increasing road\ncapacity via vehicle platooning and by creating an avenue for influencing\npeople's choice of routes. We consider a network of parallel roads with two\nmodes of transportation: (i) human drivers, who will choose the quickest route\navailable to them, and (ii) a ride hailing service, which provides an array of\nautonomous vehicle route options, each with different prices, to users. We\nformalize a model of vehicle flow in mixed autonomy and a model of how\nautonomous service users make choices between routes with different prices and\nlatencies. Developing an algorithm to learn the preferences of the users, we\nformulate a planning optimization that chooses prices to maximize a social\nobjective. We demonstrate the benefit of the proposed scheme by comparing the\nresults to theoretical benchmarks which we show can be efficiently calculated.",
    "descriptor": "\nComments: 12 pages, 7 figures, 2 tables. To appear at IEEE Transactions on Control of Network Systems (TCNS). arXiv admin note: substantial text overlap with arXiv:1904.02209\n",
    "authors": [
      "Erdem B\u0131y\u0131k",
      "Daniel A. Lazar",
      "Ramtin Pedarsani",
      "Dorsa Sadigh"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04678"
  },
  {
    "id": "arXiv:2106.04679",
    "title": "Self-Adaptive Swarm System (SASS)",
    "abstract": "Distributed artificial intelligence (DAI) studies artificial intelligence\nentities working together to reason, plan, solve problems, organize behaviors\nand strategies, make collective decisions and learn. This Ph.D. research\nproposes a principled Multi-Agent Systems (MAS) cooperation framework,\nSelf-Adaptive Swarm System (SASS), to bridge the fourth level automation gap\nbetween perception, communication, planning, execution, decision-making, and\nlearning.",
    "descriptor": "\nComments: The preprint for IJCAI 2021 Doctoral Consortium (already has been accepted)\n",
    "authors": [
      "Qin Yang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04679"
  },
  {
    "id": "arXiv:2106.04681",
    "title": "Data Expansion using Back Translation and Paraphrasing for Hate Speech  Detection",
    "abstract": "With proliferation of user generated contents in social media platforms,\nestablishing mechanisms to automatically identify toxic and abusive content\nbecomes a prime concern for regulators, researchers, and society. Keeping the\nbalance between freedom of speech and respecting each other dignity is a major\nconcern of social media platform regulators. Although, automatic detection of\noffensive content using deep learning approaches seems to provide encouraging\nresults, training deep learning-based models requires large amounts of\nhigh-quality labeled data, which is often missing. In this regard, we present\nin this paper a new deep learning-based method that fuses a Back Translation\nmethod, and a Paraphrasing technique for data augmentation. Our pipeline\ninvestigates different word-embedding-based architectures for classification of\nhate speech. The back translation technique relies on an encoder-decoder\narchitecture pre-trained on a large corpus and mostly used for machine\ntranslation. In addition, paraphrasing exploits the transformer model and the\nmixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are\ncompared to seek enhanced classification results. We evaluate our proposal on\nfive publicly available datasets; namely, AskFm corpus, Formspring dataset,\nWarner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The\nperformance of the proposal together with comparison to some related\nstate-of-art results demonstrate the effectiveness and soundness of our\nproposal.",
    "descriptor": "",
    "authors": [
      "Djamila Romaissa Beddiar",
      "Md Saroar Jahan",
      "Mourad Oussalah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04681"
  },
  {
    "id": "arXiv:2106.04682",
    "title": "Bayesian Optimization over Hybrid Spaces",
    "abstract": "We consider the problem of optimizing hybrid structures (mixture of discrete\nand continuous input variables) via expensive black-box function evaluations.\nThis problem arises in many real-world applications. For example, in materials\ndesign optimization via lab experiments, discrete and continuous variables\ncorrespond to the presence/absence of primitive elements and their relative\nconcentrations respectively. The key challenge is to accurately model the\ncomplex interactions between discrete and continuous variables. In this paper,\nwe propose a novel approach referred as Hybrid Bayesian Optimization (HyBO) by\nutilizing diffusion kernels, which are naturally defined over continuous and\ndiscrete variables. We develop a principled approach for constructing diffusion\nkernels over hybrid spaces by utilizing the additive kernel formulation, which\nallows additive interactions of all orders in a tractable manner. We\ntheoretically analyze the modeling strength of additive hybrid kernels and\nprove that it has the universal approximation property. Our experiments on\nsynthetic and six diverse real-world benchmarks show that HyBO significantly\noutperforms the state-of-the-art methods.",
    "descriptor": "\nComments: 14 pages, 18 figures\n",
    "authors": [
      "Aryan Deshwal",
      "Syrine Belakaria",
      "Janardhan Rao Doppa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04682"
  },
  {
    "id": "arXiv:2106.04683",
    "title": "General Rough Modeling of Cluster Analysis",
    "abstract": "In this research, a general theoretical framework for clustering is proposed\nover specific partial algebraic systems by the present author. Her theory helps\nin isolating minimal assumptions necessary for different concepts of clustering\ninformation in any form to be realized in a situation (and therefore in a\nsemantics). \\emph{It is well-known that of the limited number of proofs in the\ntheory of hard and soft clustering that are known to exist, most involve\nstatistical assumptions}. Many methods seem to work because they seem to work\nin specific empirical practice. A new general rough method of analyzing\nclusterings is invented, and this opens the subject to clearer conceptions and\ncontamination-free theoretical proofs. Numeric ideas of validation are also\nproposed to be replaced by those based on general rough approximation. The\nessence of the approach is explained in brief and supported by an example.",
    "descriptor": "\nComments: Preprint of paper In IFSA-EUSFLAT 2021 Proceedings\n",
    "authors": [
      "A. Mani"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.04683"
  },
  {
    "id": "arXiv:2106.04684",
    "title": "Explainable AI for medical imaging: Explaining pneumothorax diagnoses  with Bayesian Teaching",
    "abstract": "Limited expert time is a key bottleneck in medical imaging. Due to advances\nin image classification, AI can now serve as decision-support for medical\nexperts, with the potential for great gains in radiologist productivity and, by\nextension, public health. However, these gains are contingent on building and\nmaintaining experts' trust in the AI agents. Explainable AI may build such\ntrust by helping medical experts to understand the AI decision processes behind\ndiagnostic judgements. Here we introduce and evaluate explanations based on\nBayesian Teaching, a formal account of explanation rooted in the cognitive\nscience of human learning. We find that medical experts exposed to explanations\ngenerated by Bayesian Teaching successfully predict the AI's diagnostic\ndecisions and are more likely to certify the AI for cases when the AI is\ncorrect than when it is wrong, indicating appropriate trust. These results show\nthat Explainable AI can be used to support human-AI collaboration in medical\nimaging.",
    "descriptor": "",
    "authors": [
      "Tomas Folke",
      "Scott Cheng-Hsin Yang",
      "Sean Anderson",
      "Patrick Shafto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04684"
  },
  {
    "id": "arXiv:2106.04687",
    "title": "Does class size matter? An in-depth assessment of the effect of class  size in software defect prediction",
    "abstract": "In the past 20 years, defect prediction studies have generally acknowledged\nthe effect of class size on software prediction performance. To quantify the\nrelationship between object-oriented (OO) metrics and defects, modelling has to\ntake into account the direct, and potentially indirect, effects of class size\non defects. However, some studies have shown that size cannot be simply\ncontrolled or ignored, when building prediction models. As such, there remains\na question whether, and when, to control for class size. This study provides a\nnew in-depth examination of the impact of class size on the relationship\nbetween OO metrics and software defects or defect-proneness. We assess the\nimpact of class size on the number of defects and defect-proneness in software\nsystems by employing a regression-based mediation (with bootstrapping) and\nmoderation analysis to investigate the direct and indirect effect of class size\nin count and binary defect prediction. Our results show that the size effect is\nnot always significant for all metrics. Of the seven OO metrics we\ninvestigated, size consistently has significant mediation impact only on the\nrelationship between Coupling Between Objects (CBO) and\ndefects/defect-proneness, and a potential moderation impact on the relationship\nbetween Fan-out and defects/defect-proneness. Based on our results we make\nthree recommendations. One, we encourage researchers and practitioners to\nexamine the impact of class size for the specific data they have in hand and\nthrough the use of the proposed statistical mediation/moderation procedures.\nTwo, we encourage empirical studies to investigate the indirect effect of\npossible additional variables in their models when relevant. Three, the\nstatistical procedures adopted in this study could be used in other empirical\nsoftware engineering research to investigate the influence of potential\nmediators/moderators.",
    "descriptor": "\nComments: Accepted to Empirical Software Engineering (to appear). arXiv admin note: text overlap with arXiv:2104.12349\n",
    "authors": [
      "Amjed Tahir",
      "Kwabena E. Bennin",
      "Xun Xiao",
      "Stephen G. MacDonell"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.04687"
  },
  {
    "id": "arXiv:2106.04688",
    "title": "Cartographic Design of Cultural Maps",
    "abstract": "Throughout history, maps have been used as a tool to explore cities. They\nvisualize a city's urban fabric through its streets, buildings, and points of\ninterest. Besides purely navigation purposes, street names also reflect a\ncity's culture through its commemorative practices. Therefore, cultural maps\nthat unveil socio-cultural characteristics encoded in street names could\npotentially raise citizens' historical awareness. But designing effective\ncultural maps is challenging, not only due to data scarcity but also due to the\nlack of effective approaches to engage citizens with data exploration. To\naddress these challenges, we collected a dataset of 5,000 streets across the\ncities of Paris, Vienna, London, and New York, and built their cultural maps\ngrounded on cartographic storytelling techniques. Through data exploration\nscenarios, we demonstrated how cultural maps engage users and allow them to\ndiscover distinct patterns in the ways these cities are gender-biased,\ncelebrate various professions, and embrace foreign cultures.",
    "descriptor": "\nComments: 9 pages, 4 figures, 1 table\n",
    "authors": [
      "Edyta Paulina Bogucka",
      "Marios Constantinides",
      "Luca Maria Aiello",
      "Daniele Quercia",
      "Wonyoung So",
      "Melanie Bancilhon"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.04688"
  },
  {
    "id": "arXiv:2106.04689",
    "title": "Learning to Price Against a Moving Target",
    "abstract": "In the Learning to Price setting, a seller posts prices over time with the\ngoal of maximizing revenue while learning the buyer's valuation. This problem\nis very well understood when values are stationary (fixed or iid). Here we\nstudy the problem where the buyer's value is a moving target, i.e., they change\nover time either by a stochastic process or adversarially with bounded\nvariation. In either case, we provide matching upper and lower bounds on the\noptimal revenue loss. Since the target is moving, any information learned soon\nbecomes out-dated, which forces the algorithms to keep switching between\nexploring and exploiting phases.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Renato Paes Leme",
      "Balasubramanian Sivan",
      "Yifeng Teng",
      "Pratik Worah"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04689"
  },
  {
    "id": "arXiv:2106.04690",
    "title": "Handcrafted Backdoors in Deep Neural Networks",
    "abstract": "Deep neural networks (DNNs), while accurate, are expensive to train. Many\npractitioners, therefore, outsource the training process to third parties or\nuse pre-trained DNNs. This practice makes DNNs vulnerable to $backdoor$\n$attacks$: the third party who trains the model may act maliciously to inject\nhidden behaviors into the otherwise accurate model. Until now, the mechanism to\ninject backdoors has been limited to $poisoning$.\nWe argue that such a supply-chain attacker has more attack techniques\navailable. To study this hypothesis, we introduce a handcrafted attack that\ndirectly manipulates the parameters of a pre-trained model to inject backdoors.\nOur handcrafted attacker has more degrees of freedom in manipulating model\nparameters than poisoning. This makes it difficult for a defender to identify\nor remove the manipulations with straightforward methods, such as statistical\nanalysis, adding random noises to model parameters, or clipping their values\nwithin a certain range. Further, our attacker can combine the handcrafting\nprocess with additional techniques, $e.g.$, jointly optimizing a trigger\npattern, to inject backdoors into complex networks effectively$-$the\nmeet-in-the-middle attack.\nIn evaluations, our handcrafted backdoors remain effective across four\ndatasets and four network architectures with a success rate above 96%. Our\nbackdoored models are resilient to both parameter-level backdoor removal\ntechniques and can evade existing defenses by slightly changing the backdoor\nattack configurations. Moreover, we demonstrate the feasibility of suppressing\nunwanted behaviors otherwise caused by poisoning. Our results suggest that\nfurther research is needed for understanding the complete space of supply-chain\nbackdoor attacks.",
    "descriptor": "\nComments: 16 pages, 13 figures, 11 tables\n",
    "authors": [
      "Sanghyun Hong",
      "Nicholas Carlini",
      "Alexey Kurakin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04690"
  },
  {
    "id": "arXiv:2106.04692",
    "title": "Provably Faster Algorithms for Bilevel Optimization",
    "abstract": "Bilevel optimization has been widely applied in many important machine\nlearning applications such as hyperparameter optimization and meta-learning.\nRecently, several momentum-based algorithms have been proposed to solve bilevel\noptimization problems faster. However, those momentum-based algorithms do not\nachieve provably better computational complexity than\n$\\mathcal{O}(\\epsilon^{-2})$ of the SGD-based algorithm. In this paper, we\npropose two new algorithms for bilevel optimization, where the first algorithm\nadopts momentum-based recursive iterations, and the second algorithm adopts\nrecursive gradient estimations in nested loops to decrease the variance. We\nshow that both algorithms achieve the complexity of\n$\\mathcal{O}(\\epsilon^{-1.5})$, which outperforms all existing algorithms by\nthe order of magnitude. Our experiments validate our theoretical results and\ndemonstrate the superior empirical performance of our algorithms in\nhyperparameter applications. Our codes for MRBO, VRBO and other benchmarks are\navailable $\\text{online}^1$.",
    "descriptor": "\nComments: This paper was submitted in May 2021 for publication\n",
    "authors": [
      "Junjie Yang",
      "Kaiyi Ji",
      "Yingbin Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04692"
  },
  {
    "id": "arXiv:2106.04693",
    "title": "On the Evolution of Neuron Communities in a Deep Learning Architecture",
    "abstract": "Deep learning techniques are increasingly being adopted for classification\ntasks over the past decade, yet explaining how deep learning architectures can\nachieve state-of-the-art performance is still an elusive goal. While all the\ntraining information is embedded deeply in a trained model, we still do not\nunderstand much about its performance by only analyzing the model. This paper\nexamines the neuron activation patterns of deep learning-based classification\nmodels and explores whether the models' performances can be explained through\nneurons' activation behavior. We propose two approaches: one that models\nneurons' activation behavior as a graph and examines whether the neurons form\nmeaningful communities, and the other examines the predictability of neurons'\nbehavior using entropy. Our comprehensive experimental study reveals that both\nthe community quality (modularity) and entropy are closely related to the deep\nlearning models' performances, thus paves a novel way of explaining deep\nlearning models directly from the neurons' activation pattern.",
    "descriptor": "",
    "authors": [
      "Sakib Mostafa",
      "Debajyoti Mondal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04693"
  },
  {
    "id": "arXiv:2106.04695",
    "title": "A tipping point for open citation data",
    "abstract": "Open citation data can improve the transparency and robustness of scientific\nportfolio analysis, improve science policy decision-making, stimulate\ndownstream commercial activity, and increase the discoverability of scientific\narticles. Once sparsely populated, public-domain citation databases crossed a\nthreshold of one billion citations in February 2021, during the COVID-19\npandemic. Shortly thereafter, the threshold of one billion public-domain\ncitations from the Crossref database alone. Since the relative advantage of\nwithholding data in closed databases has diminished with the flood of\npublic-domain data, this likely constitutes an irreversible change in the\ncitation data ecosystem. The successes of this movement can guide future open\ndata efforts.",
    "descriptor": "",
    "authors": [
      "B. Ian Hutchins"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.04695"
  },
  {
    "id": "arXiv:2106.04696",
    "title": "Curriculum Design for Teaching via Demonstrations: Theory and  Applications",
    "abstract": "We consider the problem of teaching via demonstrations in sequential\ndecision-making settings. In particular, we study how to design a personalized\ncurriculum over demonstrations to speed up the learner's convergence. We\nprovide a unified curriculum strategy for two popular learner models: Maximum\nCausal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy\nBehavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over\ndemonstrations based on a notion of difficulty scores computed w.r.t. the\nteacher's optimal policy and the learner's current policy. Compared to the\nstate of the art, our strategy doesn't require access to the learner's internal\ndynamics and still enjoys similar convergence guarantees under mild technical\nconditions. Furthermore, we adapt our curriculum strategy to teach a learner\nusing domain knowledge in the form of task-specific difficulty scores when the\nteacher's optimal policy is unknown. Experiments on a car driving simulator\nenvironment and shortest path problems in a grid-world environment demonstrate\nthe effectiveness of our proposed curriculum strategy.",
    "descriptor": "",
    "authors": [
      "Gaurav Yengera",
      "Rati Devidze",
      "Parameswaran Kamalaruban",
      "Adish Singla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04696"
  },
  {
    "id": "arXiv:2106.04699",
    "title": "Network Topologies for Composable Data Centres",
    "abstract": "Suitable composable data center networks (DCNs) are essential to support the\ndisaggregation of compute components in highly efficient next generation data\ncenters (DCs). However, designing such composable DCNs can be challenging. A\ncomposable DCN that adopts a full mesh backplane between disaggregated compute\ncomponents within a rack and employs dedicated interfaces on each\npoint-to-point link is wasteful and expensive. In this paper, we propose and\ndescribe two (i.e., electrical, and electrical-optical) variants of a network\nfor composable DC (NetCoD). NetCoD adopts a targeted design to reduce the\nnumber of transceivers required when a mesh physical backplane is deployed\nbetween disaggregated compute components in the same rack. The targeted design\nleverages optical communication techniques and components to achieve this with\nminimal or no network performance degradation. We formulate a MILP model to\nevaluate the performance of both variants of NetCoD in rack-scale composable\nDCs that implement different forms of disaggregation. The electrical-optical\nvariant of NetCoD achieves similar performance as a reference network while\nutilizing fewer transceivers per compute node. The targeted adoption of optical\ntechnologies by both variants of NetCoD achieves greater (4 - 5 times greater)\nutilization of available network throughput than the reference network which\nimplements a generic design. Under the various forms of disaggregation\nconsidered, both variant of NetCoD achieve near-optimal compute energy\nefficiency in the composable DC while satisfying both compute and network\nconstraints. This is because marginal concession of optimal compute energy\nefficiency is often required to achieve overall optimal energy efficiency in\ncomposable DCs.",
    "descriptor": "",
    "authors": [
      "Opeyemi O. Ajibola",
      "Taisir E. H. El-Gorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04699"
  },
  {
    "id": "arXiv:2106.04700",
    "title": "Scale Free Adversarial Multi Armed Bandits",
    "abstract": "We consider the Scale-Free Adversarial Multi Armed Bandit(MAB) problem, where\nthe player only knows the number of arms $n$ and not the scale or magnitude of\nthe losses. It sees bandit feedback about the loss vectors $l_1,\\dots, l_T \\in\n\\mathbb{R}^n$. The goal is to bound its regret as a function of $n$ and\n$l_1,\\dots, l_T$. We design a Follow The Regularized Leader(FTRL) algorithm,\nwhich comes with the first scale-free regret guarantee for MAB. It uses the log\nbarrier regularizer, the importance weighted estimator, an adaptive learning\nrate, and an adaptive exploration parameter. In the analysis, we introduce a\nsimple, unifying technique for obtaining regret inequalities for FTRL and\nOnline Mirror Descent(OMD) on the probability simplex using Potential Functions\nand Mixed Bregmans. We also develop a new technique for obtaining local-norm\nlower bounds for Bregman Divergences, which are crucial in bandit regret\nbounds. These tools could be of independent interest.",
    "descriptor": "",
    "authors": [
      "Sudeep Raja Putta",
      "Shipra Agrawal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04700"
  },
  {
    "id": "arXiv:2106.04704",
    "title": "Pricing Ordered Items",
    "abstract": "We study the revenue guarantees and approximability of item pricing. Recent\nwork shows that with $n$ heterogeneous items, item-pricing guarantees an\n$O(\\log n)$ approximation to the optimal revenue achievable by any (buy-many)\nmechanism, even when buyers have arbitrarily combinatorial valuations. However,\nfinding good item prices is challenging -- it is known that even under\nunit-demand valuations, it is NP-hard to find item prices that approximate the\nrevenue of the optimal item pricing better than $O(\\sqrt{n})$.\nOur work provides a more fine-grained analysis of the revenue guarantees and\ncomputational complexity in terms of the number of item ``categories'' which\nmay be significantly fewer than $n$. We assume the items are partitioned in $k$\ncategories so that items within a category are totally-ordered and a buyer's\nvalue for a bundle depends only on the best item contained from every category.\nWe show that item-pricing guarantees an $O(\\log k)$ approximation to the\noptimal (buy-many) revenue and provide a PTAS for computing the optimal\nitem-pricing when $k$ is constant. We also provide a matching lower bound\nshowing that the problem is (strongly) NP-hard even when $k=1$. Our results\nnaturally extend to the case where items are only partially ordered, in which\ncase the revenue guarantees and computational complexity depend on the width of\nthe partial ordering, i.e. the largest set for which no two items are\ncomparable.",
    "descriptor": "",
    "authors": [
      "Shuchi Chawla",
      "Rojin Rezvan",
      "Yifeng Teng",
      "Christos Tzamos"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04704"
  },
  {
    "id": "arXiv:2106.04707",
    "title": "Job Dispatching Policies for Queueing Systems with Unknown Service Rates",
    "abstract": "In multi-server queueing systems where there is no central queue holding all\nincoming jobs, job dispatching policies are used to assign incoming jobs to the\nqueue at one of the servers. Classic job dispatching policies such as\njoin-the-shortest-queue and shortest expected delay assume that the service\nrates and queue lengths of the servers are known to the dispatcher. In this\nwork, we tackle the problem of job dispatching without the knowledge of service\nrates and queue lengths, where the dispatcher can only obtain noisy estimates\nof the service rates by observing job departures. This problem presents a novel\nexploration-exploitation trade-off between sending jobs to all the servers to\nestimate their service rates, and exploiting the currently known fastest\nservers to minimize the expected queueing delay. We propose a bandit-based\nexploration policy that learns the service rates from observed job departures.\nUnlike the standard multi-armed bandit problem where only one out of a finite\nset of actions is optimal, here the optimal policy requires identifying the\noptimal fraction of incoming jobs to be sent to each server. We present a\nregret analysis and simulations to demonstrate the effectiveness of the\nproposed bandit-based exploration policy.",
    "descriptor": "",
    "authors": [
      "Tuhinangshu Choudhury",
      "Gauri Joshi",
      "Weina Wang",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04707"
  },
  {
    "id": "arXiv:2106.04708",
    "title": "Boolean Matrix Factorization via Nonnegative Auxiliary Optimization",
    "abstract": "A novel approach to Boolean matrix factorization (BMF) is presented. Instead\nof solving the BMF problem directly, this approach solves a nonnegative\noptimization problem with the constraint over an auxiliary matrix whose Boolean\nstructure is identical to the initial Boolean data. Then the solution of the\nnonnegative auxiliary optimization problem is thresholded to provide a solution\nfor the BMF problem. We provide the proofs for the equivalencies of the two\nsolution spaces under the existence of an exact solution. Moreover, the\nnonincreasing property of the algorithm is also proven. Experiments on\nsynthetic and real datasets are conducted to show the effectiveness and\ncomplexity of the algorithm compared to other current methods.",
    "descriptor": "",
    "authors": [
      "Duc P. Truong",
      "Erik Skau",
      "Derek Desantis",
      "Boian Alexandrov"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04708"
  },
  {
    "id": "arXiv:2106.04714",
    "title": "NRGNN: Learning a Label Noise-Resistant Graph Neural Network on Sparsely  and Noisily Labeled Graphs",
    "abstract": "Graph Neural Networks (GNNs) have achieved promising results for\nsemi-supervised learning tasks on graphs such as node classification. Despite\nthe great success of GNNs, many real-world graphs are often sparsely and\nnoisily labeled, which could significantly degrade the performance of GNNs, as\nthe noisy information could propagate to unlabeled nodes via graph structure.\nThus, it is important to develop a label noise-resistant GNN for\nsemi-supervised node classification. Though extensive studies have been\nconducted to learn neural networks with noisy labels, they mostly focus on\nindependent and identically distributed data and assume a large number of noisy\nlabels are available, which are not directly applicable for GNNs. Thus, we\ninvestigate a novel problem of learning a robust GNN with noisy and limited\nlabels. To alleviate the negative effects of label noise, we propose to link\nthe unlabeled nodes with labeled nodes of high feature similarity to bring more\nclean label information. Furthermore, accurate pseudo labels could be obtained\nby this strategy to provide more supervision and further reduce the effects of\nlabel noise. Our theoretical and empirical analysis verify the effectiveness of\nthese two strategies under mild conditions. Extensive experiments on real-world\ndatasets demonstrate the effectiveness of the proposed method in learning a\nrobust GNN with noisy and limited labels.",
    "descriptor": "",
    "authors": [
      "Enyan Dai",
      "Charu Aggarwal",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04714"
  },
  {
    "id": "arXiv:2106.04715",
    "title": "Measurable Monte Carlo Search Error Bounds",
    "abstract": "Monte Carlo planners can often return sub-optimal actions, even if they are\nguaranteed to converge in the limit of infinite samples. Known asymptotic\nregret bounds do not provide any way to measure confidence of a recommended\naction at the conclusion of search. In this work, we prove bounds on the\nsub-optimality of Monte Carlo estimates for non-stationary bandits and Markov\ndecision processes. These bounds can be directly computed at the conclusion of\nthe search and do not require knowledge of the true action-value. The presented\nbound holds for general Monte Carlo solvers meeting mild convergence\nconditions. We empirically test the tightness of the bounds through experiments\non a multi-armed bandit and a discrete Markov decision process for both a\nsimple solver and Monte Carlo tree search.",
    "descriptor": "\nComments: 9 pages, submitted to NeurIPS 2021\n",
    "authors": [
      "John Mern",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04715"
  },
  {
    "id": "arXiv:2106.04716",
    "title": "Labeled Data Generation with Inexact Supervision",
    "abstract": "The recent advanced deep learning techniques have shown the promising results\nin various domains such as computer vision and natural language processing. The\nsuccess of deep neural networks in supervised learning heavily relies on a\nlarge amount of labeled data. However, obtaining labeled data with target\nlabels is often challenging due to various reasons such as cost of labeling and\nprivacy issues, which challenges existing deep models. In spite of that, it is\nrelatively easy to obtain data with \\textit{inexact supervision}, i.e., having\nlabels/tags related to the target task. For example, social media platforms are\noverwhelmed with billions of posts and images with self-customized tags, which\nare not the exact labels for target classification tasks but are usually\nrelated to the target labels. It is promising to leverage these tags (inexact\nsupervision) and their relations with target classes to generate labeled data\nto facilitate the downstream classification tasks. However, the work on this is\nrather limited. Therefore, we study a novel problem of labeled data generation\nwith inexact supervision. We propose a novel generative framework named as\nADDES which can synthesize high-quality labeled data for target classification\ntasks by learning from data with inexact supervision and the relations between\ninexact supervision and target classes. Experimental results on image and text\ndatasets demonstrate the effectiveness of the proposed ADDES for generating\nrealistic labeled data from inexact supervision to facilitate the target\nclassification task.",
    "descriptor": "",
    "authors": [
      "Enyan Dai",
      "Kai Shu",
      "Yiwei Sun",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04716"
  },
  {
    "id": "arXiv:2106.04718",
    "title": "FastSeq: Make Sequence Generation Faster",
    "abstract": "Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.",
    "descriptor": "\nComments: ACL 2021 Demo Track\n",
    "authors": [
      "Yu Yan",
      "Fei Hu",
      "Jiusheng Chen",
      "Nikhil Bhendawade",
      "Ting Ye",
      "Yeyun Gong",
      "Nan Duan",
      "Desheng Cui",
      "Bingyu Chi",
      "Ruifei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04718"
  },
  {
    "id": "arXiv:2106.04720",
    "title": "Analysis of Attacker Behavior in Compromised Hosts During Command and  Control",
    "abstract": "Traditional reactive approach of blacklisting botnets fails to adapt to the\nrapidly evolving landscape of cyberattacks. An automated and proactive approach\nto detect and block botnet hosts will immensely benefit the industry.\nBehavioral analysis of botnet is shown to be effective against a wide variety\nof attack types. Current works, however, focus solely on analyzing network\ntraffic from and to the bots. In this work we take a different approach of\nanalyzing the chain of commands input by attackers in a compromised host. We\nhave deployed several honeypots to simulate Linux shells and allowed attackers\naccess to the shells to collect a large dataset of commands. We have further\ndeveloped an automated mechanism to analyze these data. For the automation we\nhave developed a system called CYbersecurity information Exchange with Privacy\n(CYBEX-P). Finally, we have done a sequential analysis on the dataset to show\nthat we can successfully predict attacker behavior from the shell commands\nwithout analyzing network traffic like previous works.",
    "descriptor": "",
    "authors": [
      "Farhan Sadique",
      "Shamik Sengupta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04720"
  },
  {
    "id": "arXiv:2106.04723",
    "title": "OODIn: An Optimised On-Device Inference Framework for Heterogeneous  Mobile Devices",
    "abstract": "Radical progress in the field of deep learning (DL) has led to unprecedented\naccuracy in diverse inference tasks. As such, deploying DL models across mobile\nplatforms is vital to enable the development and broad availability of the\nnext-generation intelligent apps. Nevertheless, the wide and optimised\ndeployment of DL models is currently hindered by the vast system heterogeneity\nof mobile devices, the varying computational cost of different DL models and\nthe variability of performance needs across DL applications. This paper\nproposes OODIn, a framework for the optimised deployment of DL apps across\nheterogeneous mobile devices. OODIn comprises a novel DL-specific software\narchitecture together with an analytical framework for modelling DL\napplications that: (1) counteract the variability in device resources and DL\nmodels by means of a highly parametrised multi-layer design; and (2) perform a\nprincipled optimisation of both model- and system-level parameters through a\nmulti-objective formulation, designed for DL inference apps, in order to adapt\nthe deployment to the user-specified performance requirements and device\ncapabilities. Quantitative evaluation shows that the proposed framework\nconsistently outperforms status-quo designs across heterogeneous devices and\ndelivers up to 4.3x and 3.5x performance gain over highly optimised platform-\nand model-aware designs respectively, while effectively adapting execution to\ndynamic changes in resource availability.",
    "descriptor": "\nComments: Accepted at the 7th IEEE International Conference on Smart Computing (SMARTCOMP), 2021\n",
    "authors": [
      "Stylianos I. Venieris",
      "Ioannis Panopoulos",
      "Iakovos S. Venieris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04723"
  },
  {
    "id": "arXiv:2106.04724",
    "title": "A space-time Trefftz discontinuous Galerkin method for the linear  Schr\u00f6dinger equation",
    "abstract": "A space-time Trefftz discontinuous Galerkin method for the Schr\\\"odinger\nequation with piecewise-constant potential is proposed and analyzed. Following\nthe spirit of Trefftz methods, trial and test spaces are spanned by\nnon-polynomial complex wave functions that satisfy the Schr\\\"odinger equation\nlocally on each element of the space-time mesh. This allows to significantly\nreduce the number of degrees of freedom in comparison with full polynomial\nspaces. We prove well-posedness and stability of the method, and, for the one-\nand two-dimensional cases, optimal, high-order, $h$-convergence error estimates\nin a skeleton norm. Some numerical experiments validate the theoretical results\npresented.",
    "descriptor": "",
    "authors": [
      "Sergio G\u00f3mez",
      "Andrea Moiola"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04724"
  },
  {
    "id": "arXiv:2106.04726",
    "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study  of the 2019 Indian Election on WhatsApp",
    "abstract": "WhatsApp is a popular chat application used by over 2 billion users\nworldwide. However, due to end-to-end encryption, there is currently no easy\nway to fact-check content on WhatsApp at scale. In this paper, we analyze the\nusefulness of a crowd-sourced system on WhatsApp through which users can submit\n\"tips\" containing messages they want fact-checked. We compare the tips sent to\na WhatsApp tipline run during the 2019 Indian national elections with the\nmessages circulating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, the\nanalysis suggests tiplines can be an effective source for discovering content\nto fact-check.",
    "descriptor": "",
    "authors": [
      "Ashkan Kazemi",
      "Kiran Garimella",
      "Gautam Kishore Shahi",
      "Devin Gaffney",
      "Scott A. Hale"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04726"
  },
  {
    "id": "arXiv:2106.04727",
    "title": "ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering  using Nearest-Neighbor Chain",
    "abstract": "This paper studies the hierarchical clustering problem, where the goal is to\nproduce a dendrogram that represents clusters at varying scales of a data set.\nWe propose the ParChain framework for designing parallel hierarchical\nagglomerative clustering (HAC) algorithms, and using the framework we obtain\nnovel parallel algorithms for the complete linkage, average linkage, and Ward's\nlinkage criteria. Compared to most previous parallel HAC algorithms, which\nrequire quadratic memory, our new algorithms require only linear memory, and\nare scalable to large data sets. ParChain is based on our parallelization of\nthe nearest-neighbor chain algorithm, and enables multiple clusters to be\nmerged on every round. We introduce two key optimizations that are critical for\nefficiency: a range query optimization that reduces the number of distance\ncomputations required when finding nearest neighbors of clusters, and a caching\noptimization that stores a subset of previously computed distances, which are\nlikely to be reused.\nExperimentally, we show that our highly-optimized implementations using 48\ncores with two-way hyper-threading achieve 5.8--110.1x speedup over\nstate-of-the-art parallel HAC algorithms and achieve 13.75--54.23x\nself-relative speedup. Compared to state-of-the-art algorithms, our algorithms\nrequire up to 237.3x less space. Our algorithms are able to scale to data set\nsizes with tens of millions of points, which existing algorithms are not able\nto handle.",
    "descriptor": "",
    "authors": [
      "Shangdi Yu",
      "Yiqiu Wang",
      "Yan Gu",
      "Laxman Dhulipala",
      "Julian Shun"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04727"
  },
  {
    "id": "arXiv:2106.04732",
    "title": "AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain  Adaptation",
    "abstract": "We extend semi-supervised learning to the problem of domain adaptation to\nlearn significantly higher-accuracy models that train on one data distribution\nand test on a different one. With the goal of generality, we introduce\nAdaMatch, a method that unifies the tasks of unsupervised domain adaptation\n(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation\n(SSDA). In an extensive experimental study, we compare its behavior with\nrespective state-of-the-art techniques from SSL, SSDA, and UDA on vision\nclassification tasks. We find AdaMatch either matches or significantly exceeds\nthe state-of-the-art in each case using the same hyper-parameters regardless of\nthe dataset or task. For example, AdaMatch nearly doubles the accuracy compared\nto that of the prior state-of-the-art on the UDA task for DomainNet and even\nexceeds the accuracy of the prior state-of-the-art obtained with pre-training\nby 6.4% when AdaMatch is trained completely from scratch. Furthermore, by\nproviding AdaMatch with just one labeled example per class from the target\ndomain (i.e., the SSDA setting), we increase the target accuracy by an\nadditional 6.1%, and with 5 labeled examples, by 13.6%.",
    "descriptor": "",
    "authors": [
      "David Berthelot",
      "Rebecca Roelofs",
      "Kihyuk Sohn",
      "Nicholas Carlini",
      "Alex Kurakin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04732"
  },
  {
    "id": "arXiv:2106.04735",
    "title": "Validating Static Warnings via Testing Code Fragments",
    "abstract": "Static analysis is an important approach for finding bugs and vulnerabilities\nin software. However, inspecting and confirming static warnings are challenging\nand time-consuming. In this paper, we present a novel solution that\nautomatically generates test cases based on static warnings to validate true\nand false positives. We designed a syntactic patching algorithm that can\ngenerate syntactically valid, semantic preserving executable code fragments\nfrom static warnings. We developed a build and testing system to automatically\ntest code fragments using fuzzers, KLEE and Valgrind. We evaluated our\ntechniques using 12 real-world C projects and 1955 warnings from two commercial\nstatic analysis tools. We successfully built 68.5% code fragments and generated\n1003 test cases. Through automatic testing, we identified 48 true positives and\n27 false positives, and 205 likely false positives. We matched 4 CVE and\nreal-world bugs using Helium, and they are only triggered by our tool but not\nother baseline tools. We found that testing code fragments is scalable and\nuseful; it can trigger bugs that testing entire programs or testing procedures\nfailed to trigger.",
    "descriptor": "\nComments: In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis July 11 to 17, 2021, Denmark. 13 pages\n",
    "authors": [
      "Ashwin Kallingal Joshy",
      "Xueyuan Chen",
      "Benjamin Steenhoek",
      "Wei Le"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.04735"
  },
  {
    "id": "arXiv:2106.04738",
    "title": "Optical Networks for Composable Data Centers",
    "abstract": "Composable data centers (DCs) have been proposed to enable greater\nefficiencies as the uptake of on-demand computing services grows. In this\narticle we give an overview of composable DCs by discussing their enabling\ntechnologies, benefits, challenges, and research directions. We then describe a\nnetwork for composable DCs that leverages optical communication technologies\nand components to implement a targeted design. Relative to the implementation\nof a generic design that requires a (high capacity) dedicated transceiver on\neach point-to-point link on a mesh optical fabric in a composable DC rack, the\ntargeted design can significantly reduce capital expenditure (by up to 34\ntimes) because fewer transceivers are used. This is achieved with little or no\ndegradation of expected performance in composable DCs.",
    "descriptor": "",
    "authors": [
      "Opeyemi O. Ajibola",
      "Taisir E. H. El-Gorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04738"
  },
  {
    "id": "arXiv:2106.04739",
    "title": "Learning subtree pattern importance for Weisfeiler-Lehmanbased graph  kernels",
    "abstract": "Graph is an usual representation of relational data, which are ubiquitous in\nmanydomains such as molecules, biological and social networks. A popular\napproach to learningwith graph structured data is to make use of graph kernels,\nwhich measure the similaritybetween graphs and are plugged into a kernel\nmachine such as a support vector machine.Weisfeiler-Lehman (WL) based graph\nkernels, which employ WL labeling scheme to extract subtree patterns and\nperform node embedding, are demonstrated to achieve great performance while\nbeing efficiently computable. However, one of the main drawbacks of ageneral\nkernel is the decoupling of kernel construction and learning process. For\nmoleculargraphs, usual kernels such as WL subtree, based on substructures of\nthe molecules, consider all available substructures having the same importance,\nwhich might not be suitable inpractice. In this paper, we propose a method to\nlearn the weights of subtree patterns in the framework of WWL kernels, the\nstate of the art method for graph classification task [14]. To overcome the\ncomputational issue on large scale data sets, we present an efficient learning\nalgorithm and also derive a generalization gap bound to show its convergence.\nFinally, through experiments on synthetic and real-world data sets, we\ndemonstrate the effectiveness of our proposed method for learning the weights\nof subtree patterns.",
    "descriptor": "\nComments: To appear Machine Learning\n",
    "authors": [
      "Dai Hai Nguyen",
      "Canh Hao Nguyen",
      "Hiroshi Mamitsuka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04739"
  },
  {
    "id": "arXiv:2106.04747",
    "title": "A Review of Human Evaluation for Style Transfer",
    "abstract": "This paper reviews and summarizes human evaluation practices described in 97\nstyle transfer papers with respect to three main evaluation aspects: style\ntransfer, meaning preservation, and fluency. In principle, evaluations by human\nraters should be the most reliable. However, in style transfer papers, we find\nthat protocols for human evaluations are often underspecified and not\nstandardized, which hampers the reproducibility of research in this field and\nprogress toward better human and automatic evaluation methods.",
    "descriptor": "\nComments: GEM 2021\n",
    "authors": [
      "Eleftheria Briakou",
      "Sweta Agrawal",
      "Ke Zhang",
      "Joel Tetreault",
      "Marine Carpuat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04747"
  },
  {
    "id": "arXiv:2106.04748",
    "title": "Online Optimization in Games via Control Theory: Connecting Regret,  Passivity and Poincar\u00e9 Recurrence",
    "abstract": "We present a novel control-theoretic understanding of online optimization and\nlearning in games, via the notion of passivity. Passivity is a fundamental\nconcept in control theory, which abstracts energy conservation and dissipation\nin physical systems. It has become a standard tool in analysis of general\nfeedback systems, to which game dynamics belong. Our starting point is to show\nthat all continuous-time Follow-the-Regularized-Leader (FTRL) dynamics, which\nincludes the well-known Replicator Dynamic, are lossless, i.e. it is passive\nwith no energy dissipation. Interestingly, we prove that passivity implies\nbounded regret, connecting two fundamental primitives of control theory and\nonline optimization.\nThe observation of energy conservation in FTRL inspires us to present a\nfamily of lossless learning dynamics, each of which has an underlying energy\nfunction with a simple gradient structure. This family is closed under convex\ncombination; as an immediate corollary, any convex combination of FTRL dynamics\nis lossless and thus has bounded regret. This allows us to extend the framework\nof Fox and Shamma (Games, 2013) to prove not just global asymptotic stability\nresults for game dynamics, but Poincar\\'e recurrence results as well.\nIntuitively, when a lossless game (e.g. graphical constant-sum game) is coupled\nwith lossless learning dynamic, their interconnection is also lossless, which\nresults in a pendulum-like energy-preserving recurrent behavior, generalizing\nthe results of Piliouras and Shamma (SODA, 2014) and Mertikopoulos,\nPapadimitriou and Piliouras (SODA, 2018).",
    "descriptor": "\nComments: In ICML 2021\n",
    "authors": [
      "Yun Kuen Cheung",
      "Georgios Piliouras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04748"
  },
  {
    "id": "arXiv:2106.04750",
    "title": "Cooperative Beamforming for Wireless Fronthaul and Access Links in  Ultra-Dense C-RANs with SWIPT: A First-Order Approach",
    "abstract": "This work studies multigroup multicasting transmission in cloud radio access\nnetworks (C-RANs) with simultaneous wireless information and power transfer,\nwhere densely packed remote radio heads (RRHs) cooperatively provide\ninformation and energy services for information users (IUs) and energy users\n(EUs), respectively. To maximize the weighted sum rate (WSR) of information\nservices while satisfying the energy harvesting levels at EUs, an optimization\nof joint beamforming design for the fronthaul and access links is formulated,\nwhich is however neither smooth nor convex and is indeed NP-hard. To tackle\nthis difficulty, the smooth and successive convex approximations are used to\ntransform the original problem into a sequence of convex problems, and two\nfirst-order algorithms are developed to find the initial feasible point and the\nnearly optimal solution, respectively. Moreover, an accelerated algorithm is\ndesigned to improve the convergence speed by exploiting both Nesterov and\nheavy-ball momentums. Numerical results demonstrate that the proposed\nfirst-order algorithms achieve almost the same WSR as that of traditional\nsecond-order approaches yet with much lower computational complexity, and the\nproposed scheme outperforms state-of-the-art competing schemes in terms of WSR.",
    "descriptor": "\nComments: To appear in IEEE Journal of Selected Topics in Signal Processing, 16 pages, 7 figures. arXiv admin note: text overlap with arXiv:2007.09920\n",
    "authors": [
      "Fangqing Tan",
      "Peiran Wu",
      "Yik-Chung Wu",
      "Minghua Xia"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04750"
  },
  {
    "id": "arXiv:2106.04751",
    "title": "Self-Supervised Graph Learning with Hyperbolic Embedding for Temporal  Health Event Prediction",
    "abstract": "Electronic Health Records (EHR) have been heavily used in modern healthcare\nsystems for recording patients' admission information to hospitals. Many\ndata-driven approaches employ temporal features in EHR for predicting specific\ndiseases, readmission times, or diagnoses of patients. However, most existing\npredictive models cannot fully utilize EHR data, due to an inherent lack of\nlabels in supervised training for some temporal events. Moreover, it is hard\nfor existing works to simultaneously provide generic and personalized\ninterpretability. To address these challenges, we first propose a hyperbolic\nembedding method with information flow to pre-train medical code\nrepresentations in a hierarchical structure. We incorporate these pre-trained\nrepresentations into a graph neural network to detect disease complications,\nand design a multi-level attention method to compute the contributions of\nparticular diseases and admissions, thus enhancing personalized\ninterpretability. We present a new hierarchy-enhanced historical prediction\nproxy task in our self-supervised learning framework to fully utilize EHR data\nand exploit medical domain knowledge. We conduct a comprehensive set of\nexperiments and case studies on widely used publicly available EHR datasets to\nverify the effectiveness of our model. The results demonstrate our model's\nstrengths in both predictive tasks and interpretable abilities.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Chang Lu",
      "Chandan K. Reddy",
      "Yue Ning"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04751"
  },
  {
    "id": "arXiv:2106.04753",
    "title": "On Sample Based Explanation Methods for NLP:Efficiency, Faithfulness,  and Semantic Evaluation",
    "abstract": "In the recent advances of natural language processing, the scale of the\nstate-of-the-art models and datasets is usually extensive, which challenges the\napplication of sample-based explanation methods in many aspects, such as\nexplanation interpretability, efficiency, and faithfulness. In this work, for\nthe first time, we can improve the interpretability of explanations by allowing\narbitrary text sequences as the explanation unit. On top of this, we implement\na hessian-free method with a model faithfulness guarantee. Finally, to compare\nour method with the others, we propose a semantic-based evaluation metric that\ncan better align with humans' judgment of explanations than the widely adopted\ndiagnostic or re-training measures. The empirical results on multiple real data\nsets demonstrate the proposed method's superior performance to popular\nexplanation techniques such as Influence Function or TracIn on semantic\nevaluation.",
    "descriptor": "\nComments: 13 pages; Accepted to ACL 2021\n",
    "authors": [
      "Wei Zhang",
      "Ziming Huang",
      "Yada Zhu",
      "Guangnan Ye",
      "Xiaodong Cui",
      "Fan Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04753"
  },
  {
    "id": "arXiv:2106.04757",
    "title": "BiFair: Training Fair Models with Bilevel Optimization",
    "abstract": "Prior studies have shown that, training machine learning models via empirical\nloss minimization to maximize a utility metric (e.g., accuracy), might yield\nmodels that make discriminatory predictions. To alleviate this issue, we\ndevelop a new training algorithm, named BiFair, which jointly minimizes for a\nutility, and a fairness loss of interest. Crucially, we do so without directly\nmodifying the training objective, e.g., by adding regularization terms. Rather,\nwe learn a set of weights on the training dataset, such that, training on the\nweighted dataset ensures both good utility, and fairness. The dataset weights\nare learned in concurrence to the model training, which is done by solving a\nbilevel optimization problem using a held-out validation dataset. Overall, this\napproach yields models with better fairness-utility trade-offs. Particularly,\nwe compare our algorithm with three other state-of-the-art fair training\nalgorithms over three real-world datasets, and demonstrate that, BiFair\nconsistently performs better, i.e., we reach to better values of a given\nfairness metric under same, or higher accuracy. Further, our algorithm is\nscalable. It is applicable both to simple models, such as logistic regression,\nas well as more complex models, such as deep neural networks, as evidenced by\nour experimental analysis.",
    "descriptor": "",
    "authors": [
      "Mustafa Safa Ozdayi",
      "Murat Kantarcioglu",
      "Rishabh Iyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.04757"
  },
  {
    "id": "arXiv:2106.04759",
    "title": "Communication-efficient SGD: From Local SGD to One-Shot Averaging",
    "abstract": "We consider speeding up stochastic gradient descent (SGD) by parallelizing it\nacross multiple workers. We assume the same data set is shared among $N$\nworkers, who can take SGD steps and coordinate with a central server. While it\nis possible to obtain a linear reduction in the variance by averaging all the\nstochastic gradients at every step, this requires a lot of communication\nbetween the workers and the server, which can dramatically reduce the gains\nfrom parallelism. The Local SGD method, proposed and analyzed in the earlier\nliterature, suggests machines should make many local steps between such\ncommunications. While the initial analysis of Local SGD showed it needs $\\Omega\n( \\sqrt{T} )$ communications for $T$ local gradient steps in order for the\nerror to scale proportionately to $1/(NT)$, this has been successively improved\nin a string of papers, with the state-of-the-art requiring $\\Omega \\left( N\n\\left( \\mbox{ polynomial in log } (T) \\right) \\right)$ communications. In this\npaper, we suggest a Local SGD scheme that communicates less overall by\ncommunicating less frequently as the number of iterations grows. Our analysis\nshows that this can achieve an error that scales as $1/(NT)$ with a number of\ncommunications that is completely independent of $T$. In particular, we show\nthat $\\Omega(N)$ communications are sufficient. Empirical evidence suggests\nthis bound is close to tight as we further show that $\\sqrt{N}$ or $N^{3/4}$\ncommunications fail to achieve linear speed-up in simulations. Moreover, we\nshow that under mild assumptions, the main of which is twice differentiability\non any neighborhood of the optimal solution, one-shot averaging which only uses\na single round of communication can also achieve the optimal convergence rate\nasymptotically.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2006.02582\n",
    "authors": [
      "Artin Spiridonoff",
      "Alex Olshevsky",
      "Ioannis Ch. Paschalidis"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04759"
  },
  {
    "id": "arXiv:2106.04761",
    "title": "Modeling and Analysis of Switched-Capacitor Converters as a Multi-port  Network for Covert Communication",
    "abstract": "Switched-capacitor (SC) DC-DC voltage converters are widely used in power\ndelivery and management of modern integrated circuits. Connected to a common\nsupply voltage, SC converters exhibit cross-regulation/coupling effects among\nloads connected to different SC converter stages due to the shared components\nsuch as switches, capacitors, and parasitic elements. The coupling effects\nbetween SC converter stages can potentially be used in covert communication,\nwhere two or more entities (e.g., loads) illegitimately establish a\ncommunication channel to exchange malicious information stealthily. To\nqualitatively analyze the coupling effects, a novel modeling technique is\nproposed based on the multi-port network theory. The fast and slow switching\nlimit (FSL and SSL) equivalent resistance concepts are used to analytically\ndetermine the impact of each design parameter such as switch resistance, flying\ncapacitance, switching frequency, and parasitic resistance. A three-stage 2:1\nSC converter supplying three different loads is considered as a case study to\nverify the proposed modeling technique.",
    "descriptor": "\nComments: 10 pages, 18 figures\n",
    "authors": [
      "Yerzhan Mustafa",
      "Sel\u00e7uk K\u00f6se"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04761"
  },
  {
    "id": "arXiv:2106.04763",
    "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A  Static-Adaptive Algorithm",
    "abstract": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
    "descriptor": "\nComments: 9 pages + appendix\n",
    "authors": [
      "MohammadJavad Azizi",
      "Branislav Kveton",
      "Mohammad Ghavamzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04763"
  },
  {
    "id": "arXiv:2106.04765",
    "title": "Predicting Deep Neural Network Generalization with Perturbation Response  Curves",
    "abstract": "The field of Deep Learning is rich with empirical evidence of human-like\nperformance on a variety of prediction tasks. However, despite these successes,\nthe recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020\ncompetition suggests that there is a need for more robust and efficient\nmeasures of network generalization. In this work, we propose a new framework\nfor evaluating the generalization capabilities of trained networks. We use\nperturbation response (PR) curves that capture the accuracy change of a given\nnetwork as a function of varying levels of training sample perturbation. From\nthese PR curves, we derive novel statistics that capture generalization\ncapability. Specifically, we introduce two new measures for accurately\npredicting generalization gaps: the Gi-score and Pal-score, that are inspired\nby the Gini coefficient and Palma ratio (measures of income inequality), that\naccurately predict generalization gaps. Using our framework applied to intra\nand inter class sample mixup, we attain better predictive scores than the\ncurrent state-of-the-art measures on a majority of tasks in the PGDL\ncompetition. In addition, we show that our framework and the proposed\nstatistics can be used to capture to what extent a trained network is invariant\nto a given parametric input transformation, such as rotation or translation.\nTherefore, these generalization gap prediction statistics also provide a useful\nmeans for selecting the optimal network architectures and hyperparameters that\nare invariant to a certain perturbation.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2104.03469\n",
    "authors": [
      "Yair Schiff",
      "Brian Quanz",
      "Payel Das",
      "Pin-Yu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04765"
  },
  {
    "id": "arXiv:2106.04766",
    "title": "Fundamental Privacy Limits in Bipartite Networks under Active Attacks",
    "abstract": "This work considers active deanonymization of bipartite networks. The\nscenario arises naturally in evaluating privacy in various applications such as\nsocial networks, mobility networks, and medical databases. For instance, in\nactive deanonymization of social networks, an anonymous victim is targeted by\nan attacker (e.g. the victim visits the attacker's website), and the attacker\nqueries her group memberships (e.g. by querying the browser history) to\ndeanonymize her. In this work, the fundamental limits of privacy, in terms of\nthe minimum number of queries necessary for deanonymization, is investigated. A\nstochastic model is considered, where i) the bipartite network of group\nmemberships is generated randomly, ii) the attacker has partial prior knowledge\nof the group memberships, and iii) it receives noisy responses to its real-time\nqueries. The bipartite network is generated based on linear and sublinear\npreferential attachment, and the stochastic block model. The victim's identity\nis chosen randomly based on a distribution modeling the users' risk of being\nthe victim (e.g. probability of visiting the website). An attack algorithm is\nproposed which builds upon techniques from communication with feedback, and its\nperformance, in terms of expected number of queries, is analyzed. Simulation\nresults are provided to verify the theoretical derivations.",
    "descriptor": "",
    "authors": [
      "Mahshad Shariatnasab",
      "Farhad Shirani",
      "Elza Erkip"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04766"
  },
  {
    "id": "arXiv:2106.04767",
    "title": "Ex uno plures: Splitting One Model into an Ensemble of Subnetworks",
    "abstract": "Monte Carlo (MC) dropout is a simple and efficient ensembling method that can\nimprove the accuracy and confidence calibration of high-capacity deep neural\nnetwork models. However, MC dropout is not as effective as more\ncompute-intensive methods such as deep ensembles. This performance gap can be\nattributed to the relatively poor quality of individual models in the MC\ndropout ensemble and their lack of diversity. These issues can in turn be\ntraced back to the coupled training and substantial parameter sharing of the\ndropout models. Motivated by this perspective, we propose a strategy to compute\nan ensemble of subnetworks, each corresponding to a non-overlapping dropout\nmask computed via a pruning strategy and trained independently. We show that\nthe proposed subnetwork ensembling method can perform as well as standard deep\nensembles in both accuracy and uncertainty estimates, yet with a computational\nefficiency similar to MC dropout. Lastly, using several computer vision\ndatasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally\ndemonstrate that subnetwork ensembling also consistently outperforms recently\nproposed approaches that efficiently ensemble neural networks.",
    "descriptor": "",
    "authors": [
      "Zhilu Zhang",
      "Vianne R. Gao",
      "Mert R. Sabuncu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04767"
  },
  {
    "id": "arXiv:2106.04770",
    "title": "Ghosts in Neural Networks: Existence, Structure and Role of  Infinite-Dimensional Null Space",
    "abstract": "Overparametrization has been remarkably successful for deep learning studies.\nThis study investigates an overlooked but important aspect of overparametrized\nneural networks, that is, the null components in the parameters of neural\nnetworks, or the ghosts. Since deep learning is not explicitly regularized,\ntypical deep learning solutions contain null components. In this paper, we\npresent a structure theorem of the null space for a general class of neural\nnetworks. Specifically, we show that any null element can be uniquely written\nby the linear combination of ridgelet transforms. In general, it is quite\ndifficult to fully characterize the null space of an arbitrarily given\noperator. Therefore, the structure theorem is a great advantage for\nunderstanding a complicated landscape of neural network parameters. As\napplications, we discuss the roles of ghosts on the generalization performance\nof deep learning.",
    "descriptor": "",
    "authors": [
      "Sho Sonoda",
      "Isao Ishikawa",
      "Masahiro Ikeda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04770"
  },
  {
    "id": "arXiv:2106.04771",
    "title": "Geospatial Reasoning with Shapefiles for Supporting Policy Decisions",
    "abstract": "Policies are authoritative assets that are present in multiple domains to\nsupport decision-making. They describe what actions are allowed or recommended\nwhen domain entities and their attributes satisfy certain criteria. It is\ncommon to find policies that contain geographical rules, including distance and\ncontainment relationships among named locations. These locations' polygons can\noften be found encoded in geospatial datasets. We present an approach to\ntransform data from geospatial datasets into Linked Data using the OWL, PROV-O,\nand GeoSPARQL standards, and to leverage this representation to support\nautomated ontology-based policy decisions. We applied our approach to\nlocation-sensitive radio spectrum policies to identify relationships between\nradio transmitters coordinates and policy-regulated regions in Census.gov\ndatasets. Using a policy evaluation pipeline that mixes OWL reasoning and\nGeoSPARQL, our approach implements the relevant geospatial relationships,\naccording to a set of requirements elicited by radio spectrum domain experts.",
    "descriptor": "\nComments: 4th International Workshop on Geospatial Linked Data (GeoLD 2021) at ESWC 2021\n",
    "authors": [
      "Henrique Santos",
      "James P. McCusker",
      "Deborah L. McGuinness"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.04771"
  },
  {
    "id": "arXiv:2106.04772",
    "title": "HyCA: A Hybrid Computing Architecture for Fault Tolerant Deep Learning",
    "abstract": "Hardware faults on the regular 2-D computing array of a typical deep learning\naccelerator (DLA) can lead to dramatic prediction accuracy loss. Prior\nredundancy design approaches typically have each homogeneous redundant\nprocessing element (PE) to mitigate faulty PEs for a limited region of the 2-D\ncomputing array rather than the entire computing array to avoid the excessive\nhardware overhead. However, they fail to recover the computing array when the\nnumber of faulty PEs in any region exceeds the number of redundant PEs in the\nsame region. The mismatch problem deteriorates when the fault injection rate\nrises and the faults are unevenly distributed. To address the problem, we\npropose a hybrid computing architecture (HyCA) for fault-tolerant DLAs. It has\na set of dot-production processing units (DPPUs) to recompute all the\noperations that are mapped to the faulty PEs despite the faulty PE locations.\nAccording to our experiments, HyCA shows significantly higher reliability,\nscalability, and performance with less chip area penalty when compared to the\nconventional redundancy approaches. Moreover, by taking advantage of the\nflexible recomputing, HyCA can also be utilized to scan the entire 2-D\ncomputing array and detect the faulty PEs effectively at runtime.",
    "descriptor": "",
    "authors": [
      "Dawen Xu",
      "Qianlong Wang",
      "Cheng Liu",
      "Cheng Chu",
      "Ying Wang",
      "Huawei Li",
      "Xiaowei Li",
      "Kwang-Ting Cheng"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2106.04772"
  },
  {
    "id": "arXiv:2106.04775",
    "title": "A 2020 taxonomy of algorithms inspired on living beings behavior",
    "abstract": "Taking the role of a computer naturalist, a journey is taken through bio\ninspired algorithms taking account on algorithms which are inspired on living\nbeing behaviors. A compilation of algorithms is made considering several\nreviews or surveys of bio-inspired heuristics and swarm intelligence until 2020\nyear. A classification is made considering kingdoms as used by biologists\ngenerating several branches for animalia, bacteria, plants, fungi and protista\nto develop a taxonomy.",
    "descriptor": "\nComments: a collection of algorithms names, 24 pages, two figures, 9 tables, a recompilation\n",
    "authors": [
      "Luis Torres-Trevi\u00f1o"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04775"
  },
  {
    "id": "arXiv:2106.04776",
    "title": "Uncovering Closed-form Governing Equations of Nonlinear Dynamics from  Videos",
    "abstract": "Distilling analytical models from data has the potential to advance our\nunderstanding and prediction of nonlinear dynamics. Although discovery of\ngoverning equations based on observed system states (e.g., trajectory time\nseries) has revealed success in a wide range of nonlinear dynamics, uncovering\nthe closed-form equations directly from raw videos still remains an open\nchallenge. To this end, we introduce a novel end-to-end unsupervised deep\nlearning framework to uncover the mathematical structure of equations that\ngoverns the dynamics of moving objects in videos. Such an architecture consists\nof (1) an encoder-decoder network that learns low-dimensional spatial/pixel\ncoordinates of the moving object, (2) a learnable Spatial-Physical\nTransformation component that creates mapping between the extracted\nspatial/pixel coordinates and the latent physical states of dynamics, and (3) a\nnumerical integrator-based sparse regression module that uncovers the\nparsimonious closed-form governing equations of learned physical states and,\nmeanwhile, serves as a constraint to the autoencoder. The efficacy of the\nproposed method is demonstrated by uncovering the governing equations of a\nvariety of nonlinear dynamical systems depicted by moving objects in videos.\nThe resulting computational framework enables discovery of parsimonious\ninterpretable model in a flexible and accessible sensing environment where only\nvideos are available.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Lele Luan",
      "Yang Liu",
      "Hao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04776"
  },
  {
    "id": "arXiv:2106.04777",
    "title": "A reversible system based on hybrid toggle radius-4 cellular automata  and its application as a block cipher",
    "abstract": "The dynamical system described herein uses a hybrid cellular automata (CA)\nmechanism to attain reversibility, and this approach is adapted to create a\nnovel block cipher algorithm called HCA. CA are widely used for modeling\ncomplex systems and employ an inherently parallel model. Therefore,\napplications derived from CA have a tendency to fit very well in the current\ncomputational paradigm where scalability and multi-threading potential are\nquite desirable characteristics. HCA model has recently received a patent by\nthe Brazilian agency INPI. Several evaluations and analyses performed on the\nmodel are presented here, such as theoretical discussions related to its\nreversibility and an analysis based on graph theory, which reduces HCA security\nto the well-known Hamiltonian cycle problem that belongs to the NP-complete\nclass. Finally, the cryptographic robustness of HCA is empirically evaluated\nthrough several tests, including avalanche property compliance and the NIST\nrandomness suite.",
    "descriptor": "\nComments: 34 pages, 12 figures\n",
    "authors": [
      "Everton R. Lira",
      "Heverton B. de Mac\u00eado",
      "Danielli A. Lima",
      "Leonardo Alt",
      "Gina M. B. Oliveira"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04777"
  },
  {
    "id": "arXiv:2106.04778",
    "title": "SHARP: Shape-Aware Reconstruction of People In Loose Clothing",
    "abstract": "3D human body reconstruction from monocular images is an interesting and\nill-posed problem in computer vision with wider applications in multiple\ndomains. In this paper, we propose SHARP, a novel end-to-end trainable network\nthat accurately recovers the detailed geometry and appearance of 3D people in\nloose clothing from a monocular image. We propose a sparse and efficient fusion\nof a parametric body prior with a non-parametric peeled depth map\nrepresentation of clothed models. The parametric body prior constraints our\nmodel in two ways: first, the network retains geometrically consistent body\nparts that are not occluded by clothing, and second, it provides a body shape\ncontext that improves prediction of the peeled depth maps. This enables SHARP\nto recover fine-grained 3D geometrical details with just L1 losses on the 2D\nmaps, given an input image. We evaluate SHARP on publicly available Cloth3D and\nTHuman datasets and report superior performance to state-of-the-art approaches.",
    "descriptor": "",
    "authors": [
      "Sai Sagar Jinka",
      "Rohan Chacko",
      "Astitva Srivastava",
      "Avinash Sharma",
      "P.J. Narayanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04778"
  },
  {
    "id": "arXiv:2106.04779",
    "title": "Point Cloud Upsampling via Disentangled Refinement",
    "abstract": "Point clouds produced by 3D scanning are often sparse, non-uniform, and\nnoisy. Recent upsampling approaches aim to generate a dense point set, while\nachieving both distribution uniformity and proximity-to-surface, and possibly\namending small holes, all in a single network. After revisiting the task, we\npropose to disentangle the task based on its multi-objective nature and\nformulate two cascaded sub-networks, a dense generator and a spatial refiner.\nThe dense generator infers a coarse but dense output that roughly describes the\nunderlying surface, while the spatial refiner further fine-tunes the coarse\noutput by adjusting the location of each point. Specifically, we design a pair\nof local and global refinement units in the spatial refiner to evolve a coarse\nfeature map. Also, in the spatial refiner, we regress a per-point offset vector\nto further adjust the coarse outputs in fine-scale. Extensive qualitative and\nquantitative results on both synthetic and real-scanned datasets demonstrate\nthe superiority of our method over the state-of-the-arts.",
    "descriptor": "\nComments: CVPR 2021, website this https URL\n",
    "authors": [
      "Ruihui Li",
      "Xianzhi Li",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04779"
  },
  {
    "id": "arXiv:2106.04781",
    "title": "Embedding Physics to Learn Spatiotemporal Dynamics from Sparse Data",
    "abstract": "Modeling nonlinear spatiotemporal dynamical systems has primarily relied on\npartial differential equations (PDEs) that are typically derived from first\nprinciples. However, the explicit formulation of PDEs for many underexplored\nprocesses, such as climate systems, biochemical reaction and epidemiology,\nremains uncertain or partially unknown, where very sparse measurement data is\nyet available. To tackle this challenge, we propose a novel deep learning\narchitecture that forcibly embedded known physics knowledge in a\nresidual-recurrent $\\Pi$-block network, to facilitate the learning of the\nspatiotemporal dynamics in a data-driven manner. The coercive embedding\nmechanism of physics, fundamentally different from physics-informed neural\nnetworks based on loss penalty, ensures the network to rigorously obey given\nphysics. Numerical experiments demonstrate that the resulting learning paradigm\nthat embeds physics possesses remarkable accuracy, robustness, interpretability\nand generalizability for learning spatiotemporal dynamics.",
    "descriptor": "\nComments: 18 pages. arXiv admin note: substantial text overlap with arXiv:2105.00557\n",
    "authors": [
      "Chengping Rao",
      "Hao Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.04781"
  },
  {
    "id": "arXiv:2106.04784",
    "title": "Accelerating Neural Architecture Search via Proxy Data",
    "abstract": "Despite the increasing interest in neural architecture search (NAS), the\nsignificant computational cost of NAS is a hindrance to researchers. Hence, we\npropose to reduce the cost of NAS using proxy data, i.e., a representative\nsubset of the target data, without sacrificing search performance. Even though\ndata selection has been used across various fields, our evaluation of existing\nselection methods for NAS algorithms offered by NAS-Bench-1shot1 reveals that\nthey are not always appropriate for NAS and a new selection method is\nnecessary. By analyzing proxy data constructed using various selection methods\nthrough data entropy, we propose a novel proxy data selection method tailored\nfor NAS. To empirically demonstrate the effectiveness, we conduct thorough\nexperiments across diverse datasets, search spaces, and NAS algorithms.\nConsequently, NAS algorithms with the proposed selection discover architectures\nthat are competitive with those obtained using the entire dataset. It\nsignificantly reduces the search cost: executing DARTS with the proposed\nselection requires only 40 minutes on CIFAR-10 and 7.5 hours on ImageNet with a\nsingle GPU. Additionally, when the architecture searched on ImageNet using the\nproposed selection is inversely transferred to CIFAR-10, a state-of-the-art\ntest error of 2.4\\% is yielded. Our code is available at\nhttps://github.com/nabk89/NAS-with-Proxy-data.",
    "descriptor": "\nComments: Accepted to IJCAI 2021\n",
    "authors": [
      "Byunggook Na",
      "Jisoo Mok",
      "Hyeokjun Choe",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04784"
  },
  {
    "id": "arXiv:2106.04791",
    "title": "Sentence Embeddings using Supervised Contrastive Learning",
    "abstract": "Sentence embeddings encode sentences in fixed dense vectors and have played\nan important role in various NLP tasks and systems. Methods for building\nsentence embeddings include unsupervised learning such as Quick-Thoughts and\nsupervised learning such as InferSent. With the success of pretrained NLP\nmodels, recent research shows that fine-tuning pretrained BERT on SNLI and\nMulti-NLI data creates state-of-the-art sentence embeddings, outperforming\nprevious sentence embeddings methods on various evaluation benchmarks. In this\npaper, we propose a new method to build sentence embeddings by doing supervised\ncontrastive learning. Specifically our method fine-tunes pretrained BERT on\nSNLI data, incorporating both supervised crossentropy loss and supervised\ncontrastive loss. Compared with baseline where fine-tuning is only done with\nsupervised cross-entropy loss similar to current state-of-the-art method SBERT,\nour supervised contrastive method improves 2.8% in average on Semantic Textual\nSimilarity (STS) benchmarks and 1.05% in average on various sentence transfer\ntasks.",
    "descriptor": "",
    "authors": [
      "Danqi Liao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04791"
  },
  {
    "id": "arXiv:2106.04794",
    "title": "Towards the Memorization Effect of Neural Networks in Adversarial  Training",
    "abstract": "Recent studies suggest that ``memorization'' is one important factor for\noverparameterized deep neural networks (DNNs) to achieve optimal performance.\nSpecifically, the perfectly fitted DNNs can memorize the labels of many\natypical samples, generalize their memorization to correctly classify test\natypical samples and enjoy better test performance. While, DNNs which are\noptimized via adversarial training algorithms can also achieve perfect training\nperformance by memorizing the labels of atypical samples, as well as the\nadversarially perturbed atypical samples. However, adversarially trained models\nalways suffer from poor generalization, with both relatively low clean accuracy\nand robustness on the test set. In this work, we study the effect of\nmemorization in adversarial trained DNNs and disclose two important findings:\n(a) Memorizing atypical samples is only effective to improve DNN's accuracy on\nclean atypical samples, but hardly improve their adversarial robustness and (b)\nMemorizing certain atypical samples will even hurt the DNN's performance on\ntypical samples. Based on these two findings, we propose Benign Adversarial\nTraining (BAT) which can facilitate adversarial training to avoid fitting\n``harmful'' atypical samples and fit as more ``benign'' atypical samples as\npossible. In our experiments, we validate the effectiveness of BAT, and show it\ncan achieve better clean accuracy vs. robustness trade-off than baseline\nmethods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.",
    "descriptor": "\nComments: Preprint, under submission\n",
    "authors": [
      "Han Xu",
      "Xiaorui Liu",
      "Wentao Wang",
      "Wenbiao Ding",
      "Zhongqin Wu",
      "Zitao Liu",
      "Anil Jain",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04794"
  },
  {
    "id": "arXiv:2106.04795",
    "title": "Harmless Overparametrization in Two-layer Neural Networks",
    "abstract": "Overparametrized neural networks, where the number of active parameters is\nlarger than the sample size, prove remarkably effective in modern deep learning\npractice. From the classical perspective, however, much fewer parameters are\nsufficient for optimal estimation and prediction, whereas overparametrization\ncan be harmful even in the presence of explicit regularization. To reconcile\nthis conflict, we present a generalization theory for overparametrized ReLU\nnetworks by incorporating an explicit regularizer based on the scaled variation\nnorm. Interestingly, this regularizer is equivalent to the ridge from the angle\nof gradient-based optimization, but is similar to the group lasso in terms of\ncontrolling model complexity. By exploiting this ridge-lasso duality, we show\nthat overparametrization is generally harmless to two-layer ReLU networks. In\nparticular, the overparametrized estimators are minimax optimal up to a\nlogarithmic factor. By contrast, we show that overparametrized random feature\nmodels suffer from the curse of dimensionality and thus are suboptimal.",
    "descriptor": "",
    "authors": [
      "Huiyuan Wang",
      "Wei Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04795"
  },
  {
    "id": "arXiv:2106.04799",
    "title": "Pretraining Representations for Data-Efficient Reinforcement Learning",
    "abstract": "Data efficiency is a key challenge for deep reinforcement learning. We\naddress this problem by using unlabeled data to pretrain an encoder which is\nthen finetuned on a small amount of task-specific data. To encourage learning\nrepresentations which capture diverse aspects of the underlying MDP, we employ\na combination of latent dynamics modelling and unsupervised goal-conditioned\nRL. When limited to 100k steps of interaction on Atari games (equivalent to two\nhours of human experience), our approach significantly surpasses prior work\ncombining offline representation pretraining with task-specific finetuning, and\ncompares favourably with other pretraining methods that require orders of\nmagnitude more data. Our approach shows particular promise when combined with\nlarger models as well as more diverse, task-aligned observational data --\napproaching human-level performance and data-efficiency on Atari in our best\nsetting. We provide code associated with this work at\nhttps://github.com/mila-iqia/SGI.",
    "descriptor": "",
    "authors": [
      "Max Schwarzer",
      "Nitarshan Rajkumar",
      "Michael Noukhovitch",
      "Ankesh Anand",
      "Laurent Charlin",
      "Devon Hjelm",
      "Philip Bachman",
      "Aaron Courville"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04799"
  },
  {
    "id": "arXiv:2106.04800",
    "title": "Diffusion Source Identification on Networks with Statistical Confidence",
    "abstract": "Diffusion source identification on networks is a problem of fundamental\nimportance in a broad class of applications, including rumor controlling and\nvirus identification. Though this problem has received significant recent\nattention, most studies have focused only on very restrictive settings and lack\ntheoretical guarantees for more realistic networks. We introduce a statistical\nframework for the study of diffusion source identification and develop a\nconfidence set inference approach inspired by hypothesis testing. Our method\nefficiently produces a small subset of nodes, which provably covers the source\nnode with any pre-specified confidence level without restrictive assumptions on\nnetwork structures. Moreover, we propose multiple Monte Carlo strategies for\nthe inference procedure based on network topology and the probabilistic\nproperties that significantly improve the scalability. To our knowledge, this\nis the first diffusion source identification method with a practically useful\ntheoretical guarantee on general networks. We demonstrate our approach via\nextensive synthetic experiments on well-known random network models and a\nmobility network between cities concerning the COVID-19 spreading.",
    "descriptor": "",
    "authors": [
      "Quinlan Dawkins",
      "Tianxi Li",
      "Haifeng Xu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04800"
  },
  {
    "id": "arXiv:2106.04802",
    "title": "Probabilistic task modelling for meta-learning",
    "abstract": "We propose probabilistic task modelling -- a generative probabilistic model\nfor collections of tasks used in meta-learning. The proposed model combines\nvariational auto-encoding and latent Dirichlet allocation to model each task as\na mixture of Gaussian distribution in an embedding space. Such modelling\nprovides an explicit representation of a task through its task-theme mixture.\nWe present an efficient approximation inference technique based on variational\ninference method for empirical Bayes parameter estimation. We perform empirical\nevaluations to validate the task uncertainty and task distance produced by the\nproposed method through correlation diagrams of the prediction accuracy on\ntesting tasks. We also carry out experiments of task selection in meta-learning\nto demonstrate how the task relatedness inferred from the proposed model help\nto facilitate meta-learning algorithms.",
    "descriptor": "\nComments: Accepted at UAI 2021\n",
    "authors": [
      "Cuong C. Nguyen",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04802"
  },
  {
    "id": "arXiv:2106.04803",
    "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
    "abstract": "Transformers have attracted increasing interests in computer vision, but they\nstill fall behind state-of-the-art convolutional networks. In this work, we\nshow that while Transformers tend to have larger model capacity, their\ngeneralization can be worse than convolutional networks due to the lack of the\nright inductive bias. To effectively combine the strengths from both\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\nmodels built from two key insights:(1) depthwise Convolution and self-Attention\ncan be naturally unified via simple relative attention; (2) vertically stacking\nconvolution layers and attention layers in a principled way is surprisingly\neffective in improving generalization, capacity and efficiency. Experiments\nshow that our CoAtNets achieve state-of-the-art performance under different\nresource constraints across various datasets. For example, CoAtNet achieves\n86.0% ImageNet top-1 accuracy without extra data, and 89.77% with extra JFT\ndata, outperforming prior arts of both convolutional networks and Transformers.\nNotably, when pre-trained with 13M images fromImageNet-21K, our CoAtNet\nachieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images\nfrom JFT while using 23x less data.",
    "descriptor": "",
    "authors": [
      "Zihang Dai",
      "Hanxiao Liu",
      "Quoc V. Le",
      "Mingxing Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04803"
  },
  {
    "id": "arXiv:2106.04804",
    "title": "EMFlow: Data Imputation in Latent Space via EM and Deep Flow Models",
    "abstract": "High dimensional incomplete data can be found in a wide range of systems. Due\nto the fact that most of the data mining techniques and machine learning\nalgorithms require complete observations, data imputation is vital for\ndown-stream analysis. In this work, we introduce an imputation approach, called\nEMFlow, that performs imputation in an latent space via an online version of\nExpectation-Maximization (EM) algorithm and connects the latent space and the\ndata space via the normalizing flow (NF). The inference of EMFlow is iterative,\ninvolving updating the parameters of online EM and NF alternatively. Extensive\nexperimental results on multivariate and image datasets show that the proposed\nEMFlow has superior performance to competing methods in terms of both\nimputation quality and convergence speed.",
    "descriptor": "",
    "authors": [
      "Qi Ma",
      "Sujit K. Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04804"
  },
  {
    "id": "arXiv:2106.04808",
    "title": "Blockchain for IoT Access Control: Recent Trends and Future Research  Directions",
    "abstract": "With the rapid development of wireless sensor networks, smart devices, and\ntraditional information and communication technologies, there is tremendous\ngrowth in the use of Internet of Things (IoT) applications and services in our\neveryday life. IoT systems deal with high volumes of data. This data can be\nparticularly sensitive, as it may include health, financial, location, and\nother highly personal information. Fine-grained security management in IoT\ndemands effective access control. Several proposals discuss access control for\nthe IoT, however, a limited focus is given to the emerging blockchain-based\nsolutions for IoT access control. In this paper, we review the recent trends\nand critical needs for blockchain-based solutions for IoT access control. We\nidentify several important aspects of blockchain, including decentralised\ncontrol, secure storage and sharing information in a trustless manner, for IoT\naccess control including their benefits and limitations. Finally, we note some\nfuture research directions on how to converge blockchain in IoT access control\nefficiently and effectively.",
    "descriptor": "",
    "authors": [
      "Shantanu Pal",
      "Ali Dorri",
      "Raja Jurdak"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04808"
  },
  {
    "id": "arXiv:2106.04810",
    "title": "Sleeping beauties and temporal evolution and of the coronavirus  literature",
    "abstract": "Temporal evolution of the coronavirus literature over the last thirty years\n(N=43,769) is analyzed along with its subdomain of SARS-CoV-2 articles\n(N=27,460) and the subdomain of reviews and meta-analytic studies (N=1,027).\n(i) The analyses on the subset of SARS-CoV-2 literature identified studies\npublished prior to 2020 that have now proven highly instrumental in the\ndevelopment of various clusters of publications linked to SARS-CoV-2. In\nparticular, the so-called sleeping beauties of the coronavirus literature with\nan awakening in 2020 were identified, i.e., previously published studies of\nthis literature that had remained relatively unnoticed for several years but\ngained sudden traction in 2020 in the wake of the SARS-CoV-2 outbreak. (ii) The\nsubset of 2020 SARS-CoV-2 articles is bibliographically distant from the rest\nof this literature published prior to 2020. Individual articles of the\nSARS-CoV-2 segment with a bridging role between the two bodies of articles\n(i.e., before and after 2020) are identifiable. (iii) Furthermore, the degree\nof bibliographic coupling within the 2020 SARS-CoV-2 cluster is much poorer\ncompared to the cluster of articles published prior to 2020. This could, in\npart, be explained by the higher diversity of topics that are studied in\nrelation to SARS-CoV-2 compared to the literature of coronaviruses published\nprior to the SARS-CoV-2 disease. This work demonstrates how scholarly efforts\nundertaken during peace time or prior to a disease outbreak could suddenly play\na critical role in prevention and mitigation of health disasters caused by new\ndiseases.",
    "descriptor": "",
    "authors": [
      "Milad Haghani",
      "Pegah Varamini"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.04810"
  },
  {
    "id": "arXiv:2106.04811",
    "title": "Benchmarking NetBASILISK: a Network Security Project for Science",
    "abstract": "Infrastructures supporting distributed scientific collaborations must address\ncompeting goals in both providing high-performance access to resources while\nsimultaneously securing the infrastructure against security threats. The\nNetBASILISK project is attempting to improve the security of such\ninfrastructures while not adversely impacting their performance. This paper\nwill present our work to create a benchmark and monitoring infrastructure that\nallows us to test for any degradation in transferring data into a NetBASILISK\nprotected site.",
    "descriptor": "\nComments: 12 pages, 4 figures, presented at vCHEP '21 Conference\n",
    "authors": [
      "Jem Guhit",
      "Edward Colone",
      "Shawn McKee",
      "Kris Steinhoff",
      "Katarina Thomas"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04811"
  },
  {
    "id": "arXiv:2106.04812",
    "title": "Phase Retrieval using Single-Instance Deep Generative Prior",
    "abstract": "Several deep learning methods for phase retrieval exist, but most of them\nfail on realistic data without precise support information. We propose a novel\nmethod based on single-instance deep generative prior that works well on\ncomplex-valued crystal data.",
    "descriptor": "",
    "authors": [
      "Kshitij Tayal",
      "Raunak Manekar",
      "Zhong Zhuang",
      "David Yang",
      "Vipin Kumar",
      "Felix Hofmann",
      "Ju Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.04812"
  },
  {
    "id": "arXiv:2106.04814",
    "title": "Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing",
    "abstract": "Abstract Meaning Representation (AMR) is a rooted, labeled, acyclic graph\nrepresenting the semantics of natural language. As previous works show,\nalthough AMR is designed for English at first, it can also represent semantics\nin other languages. However, they find that concepts in their predicted AMR\ngraphs are less specific. We argue that the misprediction of concepts is due to\nthe high relevance between English tokens and AMR concepts. In this work, we\nintroduce bilingual input, namely the translated texts as well as non-English\ntexts, in order to enable the model to predict more accurate concepts. Besides,\nwe also introduce an auxiliary task, requiring the decoder to predict the\nEnglish sequences at the same time. The auxiliary task can help the decoder\nunderstand what exactly the corresponding English tokens are. Our proposed\ncross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6\npoints on Smatch F1 score. The ablation study also demonstrates the efficacy of\nour proposed modules.",
    "descriptor": "\nComments: Findings of ACL 2021\n",
    "authors": [
      "Yitao Cai",
      "Zhe Lin",
      "Xiaojun Wan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04814"
  },
  {
    "id": "arXiv:2106.04815",
    "title": "ChaCha for Online AutoML",
    "abstract": "We propose the ChaCha (Champion-Challengers) algorithm for making an online\nchoice of hyperparameters in online learning settings. ChaCha handles the\nprocess of determining a champion and scheduling a set of `live' challengers\nover time based on sample complexity bounds. It is guaranteed to have sublinear\nregret after the optimal configuration is added into consideration by an\napplication-dependent oracle based on the champions. Empirically, we show that\nChaCha provides good performance across a wide array of datasets when\noptimizing over featurization and hyperparameter decisions.",
    "descriptor": "\nComments: 16 pages (including supplementary appendix). Appearing at ICML 2021\n",
    "authors": [
      "Qingyun Wu",
      "Chi Wang",
      "John Langford",
      "Paul Mineiro",
      "Marco Rossi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04815"
  },
  {
    "id": "arXiv:2106.04819",
    "title": "Contextual Recommendations and Low-Regret Cutting-Plane Algorithms",
    "abstract": "We consider the following variant of contextual linear bandits motivated by\nrouting applications in navigational engines and recommendation systems. We\nwish to learn a hidden $d$-dimensional value $w^*$. Every round, we are\npresented with a subset $\\mathcal{X}_t \\subseteq \\mathbb{R}^d$ of possible\nactions. If we choose (i.e. recommend to the user) action $x_t$, we obtain\nutility $\\langle x_t, w^* \\rangle$ but only learn the identity of the best\naction $\\arg\\max_{x \\in \\mathcal{X}_t} \\langle x, w^* \\rangle$. We design\nalgorithms for this problem which achieve regret $O(d\\log T)$ and $\\exp(O(d\n\\log d))$. To accomplish this, we design novel cutting-plane algorithms with\nlow \"regret\" -- the total distance between the true point $w^*$ and the\nhyperplanes the separation oracle returns. We also consider the variant where\nwe are allowed to provide a list of several recommendations. In this variant,\nwe give an algorithm with $O(d^2 \\log d)$ regret and list size\n$\\mathrm{poly}(d)$. Finally, we construct nearly tight algorithms for a weaker\nvariant of this problem where the learner only learns the identity of an action\nthat is better than the recommendation. Our results rely on new algorithmic\ntechniques in convex geometry (including a variant of Steiner's formula for the\ncentroid of a convex set) which may be of independent interest.",
    "descriptor": "",
    "authors": [
      "Sreenivas Gollapudi",
      "Guru Guruganesh",
      "Kostas Kollias",
      "Pasin Manurangsi",
      "Renato Paes Leme",
      "Jon Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04819"
  },
  {
    "id": "arXiv:2106.04823",
    "title": "Practical Machine Learning Safety: A Survey and Primer",
    "abstract": "The open-world deployment of Machine Learning (ML) algorithms in\nsafety-critical applications such as autonomous vehicles needs to address a\nvariety of ML vulnerabilities such as interpretability, verifiability, and\nperformance limitations. Research explores different approaches to improve ML\ndependability by proposing new models and training techniques to reduce\ngeneralization error, achieve domain adaptation, and detect outlier examples\nand adversarial attacks. In this paper, we review and organize practical ML\ntechniques that can improve the safety and dependability of ML algorithms and\ntherefore ML-based software. Our organization maps state-of-the-art ML\ntechniques to safety strategies in order to enhance the dependability of the ML\nalgorithm from different aspects, and discuss research gaps as well as\npromising solutions.",
    "descriptor": "",
    "authors": [
      "Sina Mohseni",
      "Haotao Wang",
      "Zhiding Yu",
      "Chaowei Xiao",
      "Zhangyang Wang",
      "Jay Yadawa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04823"
  },
  {
    "id": "arXiv:2106.04824",
    "title": "Statistical Classification via Robust Hypothesis Testing",
    "abstract": "In this letter, we consider multiple statistical classification problem where\na sequence of n independent and identically distributed observations, that are\ngenerated by one of M discrete sources, need to be classified. The source\ndistributions are not known, however one has access to labeled training\nsequences, of length N, from each source. We consider the case where the\nunknown source distributions are estimated from the training sequences, then\nthe estimates are used as nominal distributions in a robust hypothesis test.\nSpecifically, we consider the robust DGL test due to Devroye et al. and provide\nnon-asymptotic exponential bounds, that are functions of N{n, on the error\nprobability of classification.",
    "descriptor": "",
    "authors": [
      "H\u00fcseyin Af\u015fer"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04824"
  },
  {
    "id": "arXiv:2106.04826",
    "title": "Verification of a Merkle Patricia Tree Library Using F*",
    "abstract": "A Merkle tree is a data structure for representing a key-value store as a\ntree. Each node of a Merkle tree is equipped with a hash value computed from\nthose of their descendants. A Merkle tree is often used for representing a\nstate of a blockchain system since it can be used for efficiently auditing the\nstate in a trustless manner. Due to the safety-critical nature of blockchains,\nensuring the correctness of their implementation is paramount.\nWe show our formally verified implementation of the core part of Plebeia\nusing F*. Plebeia is a library to manipulate an extension of Merkle trees\n(called Plebeia trees). It is being implemented as a part of the storage system\nof the Tezos blockchain system. To this end, we gradually ported Plebeia to F*;\nthe OCaml code extracted from the modules ported to F* is linked with the\nunverified part of Plebeia. By this gradual porting process, we can obtain a\nworking code from our partially verified implementation of Plebeia; we\nconfirmed that the binary passes all the unit tests of Plebeia.\nMore specifically, we verified the following properties on the implementation\nof Plebeia: (1) Each tree-manipulating function preserves the invariants on the\ndata structure of a Plebeia tree and satisfies the functional requirements as a\nnested key-value store; (2) Each function for serializing/deserializing a\nPlebeia tree to/from the low-level storage is implemented correctly; and (3)\nThe hash function for a Plebeia tree is relatively collision-resistant with\nrespect to the cryptographic safety of the blake2b hash function. During\nporting Plebeia to F*, we found a bug in an old version of Plebeia, which was\noverlooked by the tests bundled with the original implementation. To the best\nof our knowledge, this is the first work that verifies a production-level\nimplementation of a Merkle-tree library by F*.",
    "descriptor": "",
    "authors": [
      "Sota Sato",
      "Ryotaro Banno",
      "Jun Furuse",
      "Kohei Suenaga",
      "Atsushi Igarashi"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.04826"
  },
  {
    "id": "arXiv:2106.04829",
    "title": "Temporal Averaging LSTM-based Channel Estimation Scheme for IEEE 802.11p  Standard",
    "abstract": "In vehicular communications, reliable channel estimation is critical for the\nsystem performance due to the doubly-dispersive nature of vehicular channels.\nIEEE 802.11p standard allocates insufficient pilots for accurate channel\ntracking. Consequently, conventional IEEE 802.11p estimators suffer from a\nconsiderable performance degradation, especially in high mobility scenarios.\nRecently, deep learning (DL) techniques have been employed for IEEE 802.11p\nchannel estimation. Nevertheless, these methods suffer either from performance\ndegradation in very high mobility scenarios or from large computational\ncomplexity. In this paper, these limitations are solved using a long short term\nmemory (LSTM)-based estimation. The proposed estimator employs an LSTM unit to\nestimate the channel, followed by temporal averaging (TA) processing as a noise\nalleviation technique. Moreover, the noise mitigation ratio is determined\nanalytically, thus validating the TA processing ability in improving the\noverall performance. Simulation results reveal the performance superiority of\nthe proposed schemes compared to recently proposed DL-based estimators, while\nrecording a significant reduction in the computational complexity.",
    "descriptor": "",
    "authors": [
      "Abdul Karim Gizzini",
      "Marwa Chafii",
      "Shahab Ehsanfar",
      "Raed M. Shubair"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04829"
  },
  {
    "id": "arXiv:2106.04830",
    "title": "Catchphrase: Automatic Detection of Cultural References",
    "abstract": "A snowclone is a customizable phrasal template that can be realized in\nmultiple, instantly recognized variants. For example, ``* is the new *\" (Orange\nis the new black, 40 is the new 30). Snowclones are extensively used in social\nmedia. In this paper, we study snowclones originating from pop-culture quotes;\nour goal is to automatically detect cultural references in text. We introduce a\nnew, publicly available data set of pop-culture quotes and their corresponding\nsnowclone usages and train models on them. We publish code for Catchphrase, an\ninternet browser plugin to automatically detect and mark references in\nreal-time, and examine its performance via a user study. Aside from assisting\npeople to better comprehend cultural references, we hope that detecting\nsnowclones can complement work on paraphrasing and help to tackle long-standing\nquestions in social science about the dynamics of information propagation.",
    "descriptor": "",
    "authors": [
      "Nir Sweed",
      "Dafna Shahaf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04830"
  },
  {
    "id": "arXiv:2106.04831",
    "title": "MICE: A Crosslinguistic Emotion Corpus in Malay, Indonesian, Chinese and  English",
    "abstract": "MICE is a corpus of emotion words in four languages which is currently\nworking progress. There are two sections to this study, Part I: Emotion word\ncorpus and Part II: Emotion word survey. In Part 1, the method of how the\nemotion data is culled for each of the four languages will be described and\nvery preliminary data will be presented. In total, we identified 3,750 emotion\nexpressions in Malay, 6,657 in Indonesian, 3,347 in Mandarin Chinese and 8,683\nin English. We are currently evaluating and double checking the corpus and\ndoing further analysis on the distribution of these emotion expressions. Part\nII Emotion word survey involved an online language survey which collected\ninformation on how speakers assigned the emotion words into basic emotion\ncategories, the rating for valence and intensity as well as biographical\ninformation of all the respondents.",
    "descriptor": "",
    "authors": [
      "Ng Bee Chin",
      "Yosephine Susanto",
      "Erik Cambria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04831"
  },
  {
    "id": "arXiv:2106.04832",
    "title": "Probing Multilingual Language Models for Discourse",
    "abstract": "Pre-trained multilingual language models have become an important building\nblock in multilingual natural language processing. In the present paper, we\ninvestigate a range of such models to find out how well they transfer\ndiscourse-level knowledge across languages. This is done with a systematic\nevaluation on a broader set of discourse-level tasks than has been previously\nbeen assembled. We find that the XLM-RoBERTa family of models consistently show\nthe best performance, by simultaneously being good monolingual models and\ndegrading relatively little in a zero-shot setting. Our results also indicate\nthat model distillation may hurt the ability of cross-lingual transfer of\nsentence representations, while language dissimilarity at most has a modest\neffect. We hope that our test suite, covering 5 tasks with a total of 22\nlanguages in 10 distinct families, will serve as a useful evaluation platform\nfor multilingual performance at and beyond the sentence level.",
    "descriptor": "\nComments: To be presented at RepL4NLP 2021\n",
    "authors": [
      "Murathan Kurfal\u0131",
      "Robert \u00d6stling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04832"
  },
  {
    "id": "arXiv:2106.04833",
    "title": "RealTranS: End-to-End Simultaneous Speech Translation with Convolutional  Weighted-Shrinking Transformer",
    "abstract": "End-to-end simultaneous speech translation (SST), which directly translates\nspeech in one language into text in another language in real-time, is useful in\nmany scenarios but has not been fully investigated. In this work, we propose\nRealTranS, an end-to-end model for SST. To bridge the modality gap between\nspeech and text, RealTranS gradually downsamples the input speech with\ninterleaved convolution and unidirectional Transformer layers for acoustic\nmodeling, and then maps speech features into text space with a\nweighted-shrinking operation and a semantic encoder. Besides, to improve the\nmodel performance in simultaneous scenarios, we propose a blank penalty to\nenhance the shrinking quality and a Wait-K-Stride-N strategy to allow local\nreranking during decoding. Experiments on public and widely-used datasets show\nthat RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end\nmodels as well as cascaded models in diverse latency settings.",
    "descriptor": "\nComments: Accepted by ACL2021 Findings\n",
    "authors": [
      "Xingshan Zeng",
      "Liangyou Li",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04833"
  },
  {
    "id": "arXiv:2106.04835",
    "title": "Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System",
    "abstract": "Recent work (Takanobu et al., 2020) proposed the system-wise evaluation on\ndialog systems and found that improvement on individual components (e.g., NLU,\npolicy) in prior work may not necessarily bring benefit to pipeline systems in\nsystem-wise evaluation. To improve the system-wise performance, in this paper,\nwe propose new joint system-wise optimization techniques for the pipeline\ndialog system. First, we propose a new data augmentation approach which\nautomates the labeling process for NLU training. Second, we propose a novel\nstochastic policy parameterization with Poisson distribution that enables\nbetter exploration and offers a principled way to compute policy gradient.\nThird, we propose a reward bonus to help policy explore successful dialogs. Our\napproaches outperform the competitive pipeline systems from Takanobu et al.\n(2020) by big margins of 12% success rate in automatic system-wise evaluation\nand of 16% success rate in human evaluation on the standard multi-domain\nbenchmark dataset MultiWOZ 2.1, and also outperform the recent state-of-the-art\nend-to-end trained model from DSTC9.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Zichuan Lin",
      "Jing Huang",
      "Bowen Zhou",
      "Xiaodong He",
      "Tengyu Ma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04835"
  },
  {
    "id": "arXiv:2106.04840",
    "title": "Tracking by Joint Local and Global Search: A Target-aware Attention  based Approach",
    "abstract": "Tracking-by-detection is a very popular framework for single object tracking\nwhich attempts to search the target object within a local search window for\neach frame. Although such local search mechanism works well on simple videos,\nhowever, it makes the trackers sensitive to extremely challenging scenarios,\nsuch as heavy occlusion and fast motion. In this paper, we propose a novel and\ngeneral target-aware attention mechanism (termed TANet) and integrate it with\ntracking-by-detection framework to conduct joint local and global search for\nrobust tracking. Specifically, we extract the features of target object patch\nand continuous video frames, then we concatenate and feed them into a decoder\nnetwork to generate target-aware global attention maps. More importantly, we\nresort to adversarial training for better attention prediction. The appearance\nand motion discriminator networks are designed to ensure its consistency in\nspatial and temporal views. In the tracking procedure, we integrate the\ntarget-aware attention with multiple trackers by exploring candidate search\nregions for robust tracking. Extensive experiments on both short-term and\nlong-term tracking benchmark datasets all validated the effectiveness of our\nalgorithm. The project page of this paper can be found at\n\\url{https://sites.google.com/view/globalattentiontracking/home/extend}.",
    "descriptor": "\nComments: Accepted by IEEE TNNLS 2021\n",
    "authors": [
      "Xiao Wang",
      "Jin Tang",
      "Bin Luo",
      "Yaowei Wang",
      "Yonghong Tian",
      "Feng Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04840"
  },
  {
    "id": "arXiv:2106.04844",
    "title": "Nonlinear Hawkes Processes in Time-Varying System",
    "abstract": "Hawkes processes are a class of point processes that have the ability to\nmodel the self- and mutual-exciting phenomena. Although the classic Hawkes\nprocesses cover a wide range of applications, their expressive ability is\nlimited due to three key hypotheses: parametric, linear and homogeneous. Recent\nwork has attempted to address these limitations separately. This work aims to\novercome all three assumptions simultaneously by proposing the flexible\nstate-switching Hawkes processes: a flexible, nonlinear and nonhomogeneous\nvariant where a state process is incorporated to interact with the point\nprocesses. The proposed model empowers Hawkes processes to be applied to\ntime-varying systems. For inference, we utilize the latent variable\naugmentation technique to design two efficient Bayesian inference algorithms:\nGibbs sampler and mean-field variational inference, with analytical iterative\nupdates to estimate the posterior. In experiments, our model achieves superior\nperformance compared to the state-of-the-art competitors.",
    "descriptor": "",
    "authors": [
      "Feng Zhou",
      "Quyu Kong",
      "Yixuan Zhang",
      "Cheng Feng",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04844"
  },
  {
    "id": "arXiv:2106.04847",
    "title": "UniKeyphrase: A Unified Extraction and Generation Framework for  Keyphrase Prediction",
    "abstract": "Keyphrase Prediction (KP) task aims at predicting several keyphrases that can\nsummarize the main idea of the given document. Mainstream KP methods can be\ncategorized into purely generative approaches and integrated models with\nextraction and generation. However, these methods either ignore the diversity\namong keyphrases or only weakly capture the relation across tasks implicitly.\nIn this paper, we propose UniKeyphrase, a novel end-to-end learning framework\nthat jointly learns to extract and generate keyphrases. In UniKeyphrase,\nstacked relation layer and bag-of-words constraint are proposed to fully\nexploit the latent semantic relation between extraction and generation in the\nview of model structure and training process, respectively. Experiments on KP\nbenchmarks demonstrate that our joint approach outperforms mainstream methods\nby a large margin.",
    "descriptor": "\nComments: 11pages, 6 figures, 6 tables, to be published in ACL 2021 findings\n",
    "authors": [
      "Huanqin Wu",
      "Wei Liu",
      "Lei Li",
      "Dan Nie",
      "Tao Chen",
      "Feng Zhang",
      "Di Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04847"
  },
  {
    "id": "arXiv:2106.04852",
    "title": "Deep Tiny Network for Recognition-Oriented Face Image Quality Assessment",
    "abstract": "Face recognition has made significant progress in recent years due to deep\nconvolutional neural networks (CNN). In many face recognition (FR) scenarios,\nface images are acquired from a sequence with huge intra-variations. These\nintra-variations, which are mainly affected by the low-quality face images,\ncause instability of recognition performance. Previous works have focused on\nad-hoc methods to select frames from a video or use face image quality\nassessment (FIQA) methods, which consider only a particular or combination of\nseveral distortions.\nIn this work, we present an efficient non-reference image quality assessment\nfor FR that directly links image quality assessment (IQA) and FR. More\nspecifically, we propose a new measurement to evaluate image quality without\nany reference. Based on the proposed quality measurement, we propose a deep\nTiny Face Quality network (tinyFQnet) to learn a quality prediction function\nfrom data.\nWe evaluate the proposed method for different powerful FR models on two\nclassical video-based (or template-based) benchmark: IJB-B and YTF. Extensive\nexperiments show that, although the tinyFQnet is much smaller than the others,\nthe proposed method outperforms state-of-the-art quality assessment methods in\nterms of effectiveness and efficiency.",
    "descriptor": "",
    "authors": [
      "Baoyun Peng",
      "Min Liu",
      "Heng Yang",
      "Zhaoning Zhang",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04852"
  },
  {
    "id": "arXiv:2106.04853",
    "title": "DravidianMultiModality: A Dataset for Multi-modal Sentiment Analysis in  Tamil and Malayalam",
    "abstract": "Human communication is inherently multimodal and asynchronous. Analyzing\nhuman emotions and sentiment is an emerging field of artificial intelligence.\nWe are witnessing an increasing amount of multimodal content in local languages\non social media about products and other topics. However, there are not many\nmultimodal resources available for under-resourced Dravidian languages. Our\nstudy aims to create a multimodal sentiment analysis dataset for the\nunder-resourced Tamil and Malayalam languages. First, we downloaded product or\nmovies review videos from YouTube for Tamil and Malayalam. Next, we created\ncaptions for the videos with the help of annotators. Then we labelled the\nvideos for sentiment, and verified the inter-annotator agreement using Fleiss's\nKappa. This is the first multimodal sentiment analysis dataset for Tamil and\nMalayalam by volunteer annotators.",
    "descriptor": "\nComments: 31\n",
    "authors": [
      "Bharathi Raja Chakravarthi",
      "Jishnu Parameswaran P.K",
      "Premjith B",
      "K.P Soman",
      "Rahul Ponnusamy",
      "Prasanna Kumar Kumaresan",
      "Kingston Pal Thamburaj",
      "John P. McCrae"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04853"
  },
  {
    "id": "arXiv:2106.04854",
    "title": "A Case Study: Using Genetic Algorithm for Job Scheduling Problem",
    "abstract": "Nowadays, DevOps pipelines of huge projects are getting more and more\ncomplex. Each job in the pipeline might need different requirements including\nspecific hardware specifications and dependencies. To achieve minimal makespan,\ndevelopers always apply as much machines as possible. Consequently, others may\nbe stalled for waiting resource released. Minimizing the makespan of each job\nusing a few resource is a challenging problem. In this study, it is aimed to 1)\nautomatically determine the priority of jobs to reduce the waiting time in the\nline, 2) automatically allocate the machine resource to each job. In this work,\nthe problem is formulated as a multi-objective optimization problem. We use GA\nalgorithm to automatically determine job priorities and resource demand for\nminimizing individual makespan and resource usage. Finally, the experimental\nresults show that our proposed priority list generation algorithm is more\neffective than current priority list producing method in the aspects of\nmakespan and allocated machine count.",
    "descriptor": "\nComments: 5 pages, 2 algorithm, Case study\n",
    "authors": [
      "Burak Ta\u011ftekin",
      "Mahiye Uluya\u011fmur \u00d6zt\u00fcrk",
      "Mert Kutay Sezer"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04854"
  },
  {
    "id": "arXiv:2106.04856",
    "title": "Strongly Sublinear Algorithms for Testing Pattern Freeness",
    "abstract": "Given a permutation $\\pi:[k] \\to [k]$, a function $f:[n] \\to \\mathbb{R}$\ncontains a $\\pi$-appearance if there exists $1 \\leq i_1 < i_2 < \\dots < i_k\n\\leq n$ such that for all $s,t \\in [k]$, it holds that $f(i_s) < f(i_t)$ if and\nonly if $\\pi(s) < \\pi(t)$. The function is $\\pi$-free if it has no\n$\\pi$-appearances. In this paper, we investigate the problem of testing whether\nan input function $f$ is $\\pi$-free or whether at least $\\varepsilon n$ values\nin $f$ need to be changed in order to make it $\\pi$-free. This problem is a\ngeneralization of the well-studied monotonicity testing and was first studied\nby Newman, Rabinovich, Rajendraprasad and Sohler (Random Structures and\nAlgorithms 2019). We show that for all constants $k \\in \\mathbb{N}$,\n$\\varepsilon \\in (0,1)$, and permutation $\\pi:[k] \\to [k]$, there is a\none-sided error $\\varepsilon$-testing algorithm for $\\pi$-freeness of functions\n$f:[n] \\to \\mathbb{R}$ that makes $\\tilde{O}(n^{o(1)})$ queries. We improve\nsignificantly upon the previous best upper bound $O(n^{1 - 1/(k-1)})$ by\nBen-Eliezer and Canonne (SODA 2018). Our algorithm is adaptive, while the\nearlier best upper bound is known to be tight for nonadaptive algorithms.\nHence, our results also show that adaptivity helps in testing freeness of order\npatterns.",
    "descriptor": "\nComments: 27 pages, 1 figure\n",
    "authors": [
      "Ilan Newman",
      "Nithin Varma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04856"
  },
  {
    "id": "arXiv:2106.04858",
    "title": "A non-standard numerical scheme for an age-of-infection epidemic model",
    "abstract": "We propose a numerical method for approximating integro-differential\nequations arising in age-of-infection epidemic models. The method is based on a\nnon-standard finite differences approximation of the integral term appearing in\nthe equation. The study of convergence properties and the analysis of the\nqualitative behavior of the numerical solution show that it preserves all the\nbasic properties of the continuous model with no restrictive conditions on the\nstep-length $h$ of integration and that it recovers the continuous dynamic as\n$h$ tends to zero.",
    "descriptor": "\nComments: 17 pages, 3 figures\n",
    "authors": [
      "Eleonora Messina",
      "Mario Pezzella",
      "Antonia Vecchio"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04858"
  },
  {
    "id": "arXiv:2106.04863",
    "title": "A Randomness Threshold for Online Bipartite Matching, via Lossless  Online Rounding",
    "abstract": "Over three decades ago, Karp, Vazirani and Vazirani (STOC'90) introduced the\nonline bipartite matching problem. They observed that deterministic algorithms'\ncompetitive ratio for this problem is no greater than $1/2$, and proved that\nrandomized algorithms can do better. A natural question thus arises: \\emph{how\nrandom is random}? i.e., how much randomness is needed to outperform\ndeterministic algorithms? The \\textsc{ranking} algorithm of Karp et\nal.~requires $\\tilde{O}(n)$ random bits, which, ignoring polylog terms,\nremained unimproved. On the other hand, Pena and Borodin (TCS'19) established a\nlower bound of $(1-o(1))\\log\\log n$ random bits for any $1/2+\\Omega(1)$\ncompetitive ratio.\nWe close this doubly-exponential gap, proving that, surprisingly, the lower\nbound is tight. In fact, we prove a \\emph{sharp threshold} of $(1\\pm\no(1))\\log\\log n$ random bits for the randomness necessary and sufficient to\noutperform deterministic algorithms for this problem, as well as its\nvertex-weighted generalization. This implies the same threshold for the advice\ncomplexity (nondeterminism) of these problems.\nSimilar to recent breakthroughs in the online matching literature, for\nedge-weighted matching (Fahrbach et al.~FOCS'20) and adwords (Huang et\nal.~FOCS'20), our algorithms break the barrier of $1/2$ by randomizing matching\nchoices over two neighbors. Unlike these works, our approach does not rely on\nthe recently-introduced OCS machinery, nor the more established randomized\nprimal-dual method. Instead, our work revisits a highly-successful online\ndesign technique, which was nonetheless under-utilized in the area of online\nmatching, namely (lossless) online rounding of fractional algorithms. While\nthis technique is known to be hopeless for online matching in general, we show\nthat it is nonetheless applicable to carefully designed fractional algorithms\nwith additional (non-convex) constraints.",
    "descriptor": "",
    "authors": [
      "Niv Buchbinder",
      "Joseph",
      "Naor",
      "David Wajc"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04863"
  },
  {
    "id": "arXiv:2106.04866",
    "title": "Planning for Novelty: Width-Based Algorithms for Common Problems in  Control, Planning and Reinforcement Learning",
    "abstract": "Width-based algorithms search for solutions through a general definition of\nstate novelty. These algorithms have been shown to result in state-of-the-art\nperformance in classical planning, and have been successfully applied to\nmodel-based and model-free settings where the dynamics of the problem are given\nthrough simulation engines. Width-based algorithms performance is understood\ntheoretically through the notion of planning width, providing polynomial\nguarantees on their runtime and memory consumption. To facilitate synergies\nacross research communities, this paper summarizes the area of width-based\nplanning, and surveys current and future research directions.",
    "descriptor": "",
    "authors": [
      "Nir Lipovetzky"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04866"
  },
  {
    "id": "arXiv:2106.04871",
    "title": "Congestion Control in the Cellular-V2X Sidelink",
    "abstract": "This paper presents a detailed quantitative evaluation of standardised\nDecentralised Congestion Control (DCC) and packet dropping mechanisms for\nCellular V2X (C-V2X). Based on the identified shortcomings, an Access layer DCC\nscheme, RRI adaptive, is then proposed. RRI adaptive accommodates the sidelink\nscheduling mechanism Sensing Based Semi-Persistent Scheduling (SB-SPS),\neliminating incompatibilities between current standards and the scheduling\nmechanism, to avoid unnecessary and reoccurring collisions. Two variants are\nproposed; one is an evolution of the ETSI Reactive DCC mechanism and the other\naligns with the 3GPP approach based on channel occupancy ratio (CR). Both\napproaches are compared with current ETSI and 3GPP standards and exhibit\nimproved performance. An evaluation of existing DCC standards and RRI Adaptive\nto meet the Quality of Service (QoS) requirements of vehicular cooperative\nawareness applications is also conducted.",
    "descriptor": "\nComments: 8 Pages, 7 Figures, 6 Tables\n",
    "authors": [
      "Brian McCarthy",
      "Aisling O'Driscoll"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04871"
  },
  {
    "id": "arXiv:2106.04873",
    "title": "AutoFT: Automatic Fine-Tune for Parameters Transfer Learning in  Click-Through Rate Prediction",
    "abstract": "Recommender systems are often asked to serve multiple recommendation\nscenarios or domains. Fine-tuning a pre-trained CTR model from source domains\nand adapting it to a target domain allows knowledge transferring. However,\noptimizing all the parameters of the pre-trained network may result in\nover-fitting if the target dataset is small and the number of parameters is\nlarge. This leads us to think of directly reusing parameters in the pre-trained\nmodel which represent more general features learned from multiple domains.\nHowever, the design of freezing or fine-tuning layers of parameters requires\nmuch manual effort since the decision highly depends on the pre-trained model\nand target instances. In this work, we propose an end-to-end transfer learning\nframework, called Automatic Fine-Tuning (AutoFT), for CTR prediction. AutoFT\nconsists of a field-wise transfer policy and a layer-wise transfer policy. The\nfield-wise transfer policy decides how the pre-trained embedding\nrepresentations are frozen or fine-tuned based on the given instance from the\ntarget domain. The layer-wise transfer policy decides how the high?order\nfeature representations are transferred layer by layer. Extensive experiments\non two public benchmark datasets and one private industrial dataset demonstrate\nthat AutoFT can significantly improve the performance of CTR prediction\ncompared with state-of-the-art transferring approaches.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Xiangli Yang",
      "Qing Liu",
      "Rong Su",
      "Ruiming Tang",
      "Zhirong Liu",
      "Xiuqiang He"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04873"
  },
  {
    "id": "arXiv:2106.04876",
    "title": "Recovering AES Keys with a Deep Cold Boot Attack",
    "abstract": "Cold boot attacks inspect the corrupted random access memory soon after the\npower has been shut down. While most of the bits have been corrupted, many\nbits, at random locations, have not. Since the keys in many encryption schemes\nare being expanded in memory into longer keys with fixed redundancies, the keys\ncan often be restored. In this work, we combine a novel cryptographic variant\nof a deep error correcting code technique with a modified SAT solver scheme to\napply the attack on AES keys. Even though AES consists of Rijndael S-box\nelements, that are specifically designed to be resistant to linear and\ndifferential cryptanalysis, our method provides a novel formalization of the\nAES key scheduling as a computational graph, which is implemented by a neural\nmessage passing network. Our results show that our methods outperform the state\nof the art attack methods by a very large margin.",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Itamar Zimerman",
      "Eliya Nachmani",
      "Lior Wolf"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04876"
  },
  {
    "id": "arXiv:2106.04880",
    "title": "Self-Improved Retrosynthetic Planning",
    "abstract": "Retrosynthetic planning is a fundamental problem in chemistry for finding a\npathway of reactions to synthesize a target molecule. Recently, search\nalgorithms have shown promising results for solving this problem by using deep\nneural networks (DNNs) to expand their candidate solutions, i.e., adding new\nreactions to reaction pathways. However, the existing works on this line are\nsuboptimal; the retrosynthetic planning problem requires the reaction pathways\nto be (a) represented by real-world reactions and (b) executable using\n\"building block\" molecules, yet the DNNs expand reaction pathways without fully\nincorporating such requirements. Motivated by this, we propose an end-to-end\nframework for directly training the DNNs towards generating reaction pathways\nwith the desirable properties. Our main idea is based on a self-improving\nprocedure that trains the model to imitate successful trajectories found by\nitself. We also propose a novel reaction augmentation scheme based on a forward\nreaction model. Our experiments demonstrate that our scheme significantly\nimproves the success rate of solving the retrosynthetic problem from 86.84% to\n96.32% while maintaining the performance of DNN for predicting valid reactions.",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Junsu Kim",
      "Sungsoo Ahn",
      "Hankook Lee",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.04880"
  },
  {
    "id": "arXiv:2106.04885",
    "title": "A Blockchain-Based Trust Management Framework with Verifiable  Interactions",
    "abstract": "There has been tremendous interest in the development of formal trust models\nand metrics through the use of analytics (e.g., Belief Theory and Bayesian\nmodels), logics (e.g., Epistemic and Subjective Logic) and other mathematical\nmodels. The choice of trust metric will depend on context, circumstance and\nuser requirements and there is no single best metric for use in all\ncircumstances. Where different users require different trust metrics to be\nemployed the trust score calculations should still be based on all available\ntrust evidence. Trust is normally computed using past experiences but, in\npractice (especially in centralised systems), the validity and accuracy of\nthese experiences are taken for granted. In this paper, we provide a formal\nframework and practical blockchain-based implementation that allows independent\ntrust providers to implement different trust metrics in a distributed manner\nwhile still allowing all trust providers to base their calculations on a common\nset of trust evidence. Further, our design allows experiences to be provably\nlinked to interactions without the need for a central authority. This leads to\nthe notion of evidence-based trust with provable interactions. Leveraging\nblockchain allows the trust providers to offer their services in a competitive\nmanner, charging fees while users are provided with payments for recording\nexperiences. Performance details of the blockchain implementation are provided.",
    "descriptor": "",
    "authors": [
      "Shantanu Pal",
      "Ambrose Hill",
      "Tahiry Rabehaja",
      "Michael Hitchens"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.04885"
  },
  {
    "id": "arXiv:2106.04887",
    "title": "Interaction-Grounded Learning",
    "abstract": "Consider a prosthetic arm, learning to adapt to its user's control signals.\nWe propose Interaction-Grounded Learning for this novel setting, in which a\nlearner's goal is to interact with the environment with no grounding or\nexplicit reward to optimize its policies. Such a problem evades common RL\nsolutions which require an explicit reward. The learning agent observes a\nmultidimensional context vector, takes an action, and then observes a\nmultidimensional feedback vector. This multidimensional feedback vector has no\nexplicit reward information. In order to succeed, the algorithm must learn how\nto evaluate the feedback vector to discover a latent reward signal, with which\nit can ground its policies without supervision. We show that in an\nInteraction-Grounded Learning setting, with certain natural assumptions, a\nlearner can discover the latent reward and ground its policy for successful\ninteraction. We provide theoretical guarantees and a proof-of-concept empirical\nevaluation to demonstrate the effectiveness of our proposed approach.",
    "descriptor": "\nComments: Published in ICML 2021\n",
    "authors": [
      "Tengyang Xie",
      "John Langford",
      "Paul Mineiro",
      "Ida Momennejad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04887"
  },
  {
    "id": "arXiv:2106.04890",
    "title": "A gradient based resolution strategy for a PDE-constrained optimization  approach for 3D-1D coupled problems",
    "abstract": "Coupled 3D-1D problems arise in many practical applications, in an attempt to\nreduce the computational burden in simulations where cylindrical inclusions\nwith a small section are embedded in a much larger domain. Nonetheless the\nresolution of such problems can be non trivial, both from a mathematical and a\ngeometrical standpoint. Indeed 3D-1D coupling requires to operate in non\nstandard function spaces, and, also, simulation geometries can be complex for\nthe presence of multiple intersecting domains. Recently, a PDE-constrained\noptimization based formulation has been proposed for such problems, proving a\nwell posed mathematical formulation and allowing for the use of non conforming\nmeshes for the discrete problem. Here an unconstrained optimization formulation\nof the problem is derived and an efficient gradient based solver is proposed\nfor such formulation. Some numerical tests on quite complex configurations are\ndiscussed to show the viability of the method.",
    "descriptor": "",
    "authors": [
      "Stefano Berrone",
      "Denise Grappein",
      "Stefano Scial\u00f2",
      "Fabio Vicini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04890"
  },
  {
    "id": "arXiv:2106.04895",
    "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online  Reinforcement Learning",
    "abstract": "Recent theoretical work studies sample-efficient reinforcement learning (RL)\nextensively in two settings: learning interactively in the environment (online\nRL), or learning from an offline dataset (offline RL). However, existing\nalgorithms and theories for learning near-optimal policies in these two\nsettings are rather different and disconnected. Towards bridging this gap, this\npaper initiates the theoretical study of policy finetuning, that is, online RL\nwhere the learner has additional access to a \"reference policy\" $\\mu$ close to\nthe optimal policy $\\pi_\\star$ in a certain sense. We consider the policy\nfinetuning problem in episodic Markov Decision Processes (MDPs) with $S$\nstates, $A$ actions, and horizon length $H$. We first design a sharp offline\nreduction algorithm -- which simply executes $\\mu$ and runs offline policy\noptimization on the collected dataset -- that finds an $\\varepsilon$\nnear-optimal policy within $\\widetilde{O}(H^3SC^\\star/\\varepsilon^2)$ episodes,\nwhere $C^\\star$ is the single-policy concentrability coefficient between $\\mu$\nand $\\pi_\\star$. This offline result is the first that matches the sample\ncomplexity lower bound in this setting, and resolves a recent open question in\noffline RL. We then establish an $\\Omega(H^3S\\min\\{C^\\star, A\\}/\\varepsilon^2)$\nsample complexity lower bound for any policy finetuning algorithm, including\nthose that can adaptively explore the environment. This implies that -- perhaps\nsurprisingly -- the optimal policy finetuning algorithm is either offline\nreduction or a purely online RL algorithm that does not use $\\mu$. Finally, we\ndesign a new hybrid offline/online algorithm for policy finetuning that\nachieves better sample complexity than both vanilla offline reduction and\npurely online RL algorithms, in a relaxed setting where $\\mu$ only satisfies\nconcentrability partially up to a certain time step.",
    "descriptor": "",
    "authors": [
      "Tengyang Xie",
      "Nan Jiang",
      "Huan Wang",
      "Caiming Xiong",
      "Yu Bai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04895"
  },
  {
    "id": "arXiv:2106.04897",
    "title": "Unsupervised Automatic Speech Recognition: A Review",
    "abstract": "Automatic Speech Recognition (ASR) systems can be trained to achieve\nremarkable performance given large amounts of manually transcribed speech, but\nlarge labeled data sets can be difficult or expensive to acquire for all\nlanguages of interest. In this paper, we review the research literature to\nidentify models and ideas that could lead to fully unsupervised ASR, including\nunsupervised segmentation of the speech signal, unsupervised mapping from\nspeech segments to text, and semi-supervised models with nominal amounts of\nlabeled examples. The objective of the study is to identify the limitations of\nwhat can be learned from speech data alone and to understand the minimum\nrequirements for speech recognition. Identifying these limitations would help\noptimize the resources and efforts in ASR development for low-resource\nlanguages.",
    "descriptor": "",
    "authors": [
      "Hanan Aldarmaki",
      "Asad Ullah",
      "Nazar Zaki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04897"
  },
  {
    "id": "arXiv:2106.04898",
    "title": "Continuous-discrete multiple target tracking with out-of-sequence  measurements",
    "abstract": "This paper derives the optimal Bayesian processing of an out-of-sequence\n(OOS) set of measurements in continuous-time for multiple target tracking. We\nconsider a multi-target system modelled in continuous time that is discretised\nat the time steps when we receive the measurements, which are distributed\naccording to the standard point target model. All information about this system\nat the sampled time steps is provided by the posterior density on the set of\nall trajectories. This density can be computed via the continuous-discrete\ntrajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an\nOOS measurement, the optimal Bayesian processing performs a retrodiction step\nthat adds trajectory information at the OOS measurement time stamp followed by\nan update step. After the OOS measurement update, the posterior remains in\nTPMBM form. We also provide a computationally lighter alternative based on a\ntrajectory Poisson multi-Bernoulli filter. The effectiveness of the two\napproaches to handle OOS measurements is evaluated via simulations.",
    "descriptor": "",
    "authors": [
      "\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez",
      "Wei Yi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04898"
  },
  {
    "id": "arXiv:2106.04900",
    "title": "Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks",
    "abstract": "Continuum mechanics simulators, numerically solving one or more partial\ndifferential equations, are essential tools in many areas of science and\nengineering, but their performance often limits application in practice. Recent\nmodern machine learning approaches have demonstrated their ability to\naccelerate spatio-temporal predictions, although, with only moderate accuracy\nin comparison. Here we introduce MultiScaleGNN, a novel multi-scale graph\nneural network model for learning to infer unsteady continuum mechanics.\nMultiScaleGNN represents the physical domain as an unstructured set of nodes,\nand it constructs one or more graphs, each of them encoding different scales of\nspatial resolution. Successive learnt message passing between these graphs\nimproves the ability of GNNs to capture and forecast the system state in\nproblems encompassing a range of length scales. Using graph representations,\nMultiScaleGNN can impose periodic boundary conditions as an inductive bias on\nthe edges in the graphs, and achieve independence to the nodes' positions. We\ndemonstrate this method on advection problems and incompressible fluid\ndynamics. Our results show that the proposed model can generalise from uniform\nadvection fields to high-gradient fields on complex domains at test time and\ninfer long-term Navier-Stokes solutions within a range of Reynolds numbers.\nSimulations obtained with MultiScaleGNN are between two and four orders of\nmagnitude faster than the ones on which it was trained.",
    "descriptor": "",
    "authors": [
      "Mario Lino",
      "Chris Cantwell",
      "Anil A. Bharath",
      "Stathi Fotiadis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2106.04900"
  },
  {
    "id": "arXiv:2106.04903",
    "title": "Fragmented and Valuable: Following Sentiment Changes in Food Tweets",
    "abstract": "We analysed sentiment and frequencies related to smell, taste and temperature\nexpressed by food tweets in the Latvian language. To get a better understanding\nof the role of smell, taste and temperature in the mental map of food\nassociations, we looked at such categories as 'tasty' and 'healthy', which\nturned out to be mutually exclusive. By analysing the occurrence frequency of\nwords associated with these categories, we discovered that food discourse\noverall was permeated by `tasty' while the category of 'healthy' was relatively\nsmall. Finally, we used the analysis of temporal dynamics to see if we can\ntrace seasonality or other temporal aspects in smell, taste and temperature as\nreflected in food tweets. Understanding the composition of social media content\nwith relation to smell, taste and temperature in food tweets allows us to\ndevelop our work further - on food culture/seasonality and its relation to\ntemperature, on our limited capacity to express smell-related sentiments, and\nthe lack of the paradigm of taste in discussing food healthiness.",
    "descriptor": "",
    "authors": [
      "Maija K\u0101le",
      "Mat\u012bss Rikters"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04903"
  },
  {
    "id": "arXiv:2106.04905",
    "title": "DGA-Net Dynamic Gaussian Attention Network for Sentence Semantic  Matching",
    "abstract": "Sentence semantic matching requires an agent to determine the semantic\nrelation between two sentences, where much recent progress has been made by the\nadvancement of representation learning techniques and inspiration of human\nbehaviors. Among all these methods, attention mechanism plays an essential role\nby selecting important parts effectively. However, current attention methods\neither focus on all the important parts in a static way or only select one\nimportant part at one attention step dynamically, which leaves a large space\nfor further improvement. To this end, in this paper, we design a novel Dynamic\nGaussian Attention Network (DGA-Net) to combine the advantages of current\nstatic and dynamic attention methods. More specifically, we first leverage\npre-trained language model to encode the input sentences and construct semantic\nrepresentations from a global perspective. Then, we develop a Dynamic Gaussian\nAttention (DGA) to dynamically capture the important parts and corresponding\nlocal contexts from a detailed perspective. Finally, we combine the global\ninformation and detailed local information together to decide the semantic\nrelation of sentences comprehensively and precisely. Extensive experiments on\ntwo popular sentence semantic matching tasks demonstrate that our proposed\nDGA-Net is effective in improving the ability of attention mechanism.",
    "descriptor": "\nComments: Accepted by CICAI2021\n",
    "authors": [
      "Kun Zhang",
      "Guangyi Lv",
      "Meng Wang",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04905"
  },
  {
    "id": "arXiv:2106.04906",
    "title": "Engineering-Economic Evaluation of Diffractive Non-Line-Of-Sight  Backhaul (e3nb): A Techno-economic Model for 3D Wireless Backhaul Assessment",
    "abstract": "Developing ways to affordably deliver broadband connectivity is one of the\nmajor issues of our time. In challenging deployment locations with irregular\nterrain, fiber optic or traditional Clear-Line-Of-Sight (CLOS) wireless links\ncan be uneconomical to deploy, resulting from the number of required towers\nmaking infrastructure deployment unviable. With the emergence of new research\nfocusing on developing wireless diffractive backhaul technologies to provide\ndiffractive Non-Line-Of-Sight (NLOS) links, this paper evaluates the\nengineering-economic implications of such approaches. To quantify different\ntechnology strategies, a Three-Dimensional (3D) techno-economic assessment\nframework is presented to help prioritize regions for future investment in\nbroadband connectivity, utilizing a combination of remote sensing and viewshed\ngeospatial techniques. Such a method is an essential evaluation step prior to\nbeginning detailed Radio Frequency (RF) Quality of Service engineering but has\nhitherto received less research attention in the literature. This framework is\napplied to assess both Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight\nstrategies for deployment in Peru, as well as the islands of Kalimantan and\nPapua, in Indonesia. The results find that a hybrid strategy combining the use\nof Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight links produces a 15-43\npercent cost-efficiency saving, relative to only using traditional\nClear-Line-Of-Sight wireless backhaul links. The codebase is released\nopensource via the Engineering-Economic Evaluation of Non-Line-of-Sight\nBackhaul (e3nb) repository.",
    "descriptor": "",
    "authors": [
      "Edward J. Oughton",
      "Erik Boch",
      "Julius Kusuma"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computers and Society (cs.CY)",
      "Emerging Technologies (cs.ET)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2106.04906"
  },
  {
    "id": "arXiv:2106.04907",
    "title": "FastZIP: Faster and More Secure Zero-Interaction Pairing",
    "abstract": "With the advent of the Internet of Things (IoT), establishing a secure\nchannel between smart devices becomes crucial. Recent research proposes\nzero-interaction pairing (ZIP), which enables pairing without user assistance\nby utilizing devices' physical context (e.g., ambient audio) to obtain a shared\nsecret key. The state-of-the-art ZIP schemes suffer from three limitations: (1)\nprolonged pairing time (i.e., minutes or hours), (2) vulnerability to\nbrute-force offline attacks on a shared key, and (3) susceptibility to attacks\ncaused by predictable context (e.g., replay attack) because they rely on\nlimited entropy of physical context to protect a shared key. We address these\nlimitations, proposing FastZIP, a novel ZIP scheme that significantly reduces\npairing time while preventing offline and predictable context attacks. In\nparticular, we adapt a recently introduced Fuzzy Password-Authenticated Key\nExchange (fPAKE) protocol and utilize sensor fusion, maximizing their\nadvantages. We instantiate FastZIP for intra-car device pairing to demonstrate\nits feasibility and show how the design of FastZIP can be adapted to other ZIP\nuse cases. We implement FastZIP and evaluate it by driving four cars for a\ntotal of 800 km. We achieve up to three times shorter pairing time compared to\nthe state-of-the-art ZIP schemes while assuring robust security with\nadversarial error rates below 0.5%.",
    "descriptor": "\nComments: ACM MobiSys '21 - Code and data at: this https URL\n",
    "authors": [
      "Mikhail Fomichev",
      "Julia Hesse",
      "Lars Almon",
      "Timm Lippert",
      "Jun Han",
      "Matthias Hollick"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04907"
  },
  {
    "id": "arXiv:2106.04908",
    "title": "Automatic Sexism Detection with Multilingual Transformer Models",
    "abstract": "Sexism has become an increasingly major problem on social networks during the\nlast years. The first shared task on sEXism Identification in Social neTworks\n(EXIST) at IberLEF 2021 is an international competition in the field of Natural\nLanguage Processing (NLP) with the aim to automatically identify sexism in\nsocial media content by applying machine learning methods. Thereby sexism\ndetection is formulated as a coarse (binary) classification problem and a\nfine-grained classification task that distinguishes multiple types of sexist\ncontent (e.g., dominance, stereotyping, and objectification). This paper\npresents the contribution of the AIT_FHSTP team at the EXIST2021 benchmark for\nboth tasks. To solve the tasks we applied two multilingual transformer models,\none based on multilingual BERT and one based on XLM-R. Our approach uses two\ndifferent strategies to adapt the transformers to the detection of sexist\ncontent: first, unsupervised pre-training with additional data and second,\nsupervised fine-tuning with additional and augmented data. For both tasks our\nbest model is XLM-R with unsupervised pre-training on the EXIST data and\nadditional datasets and fine-tuning on the provided dataset. The best run for\nthe binary classification (task 1) achieves a macro F1-score of 0.7752 and\nscores 5th rank in the benchmark; for the multiclass classification (task 2)\nour best submission scores 6th rank with a macro F1-score of 0.5589.",
    "descriptor": "\nComments: Technical Report to the AIT_FHSTP EXIST 2021 Challenge contribution (under review) this http URL\n",
    "authors": [
      "Sch\u00fctz Mina",
      "Boeck Jaqueline",
      "Liakhovets Daria",
      "Slijep\u010devi\u0107 Djordje",
      "Kirchknopf Armin",
      "Hecht Manuel",
      "Bogensperger Johannes",
      "Schlarb Sven",
      "Schindler Alexander",
      "Zeppelzauer Matthias"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04908"
  },
  {
    "id": "arXiv:2106.04911",
    "title": "Memory-based Optimization Methods for Model-Agnostic Meta-Learning",
    "abstract": "Recently, model-agnostic meta-learning (MAML) has garnered tremendous\nattention. However, stochastic optimization of MAML is still immature. Existing\nalgorithms for MAML are based on the ``episode\" idea by sampling a number of\ntasks and a number of data points for each sampled task at each iteration for\nupdating the meta-model. However, they either do not necessarily guarantee\nconvergence with a constant mini-batch size or require processing a larger\nnumber of tasks at every iteration, which is not viable for continual learning\nor cross-device federated learning where only a small number of tasks are\navailable per-iteration or per-round. This paper addresses these issues by (i)\nproposing efficient memory-based stochastic algorithms for MAML with a\ndiminishing convergence error, which only requires sampling a constant number\nof tasks and a constant number of examples per-task per-iteration; (ii)\nproposing communication-efficient distributed memory-based MAML algorithms for\npersonalized federated learning in both the cross-device (w/ client sampling)\nand the cross-silo (w/o client sampling) settings. The key novelty of the\nproposed algorithms is to maintain an individual personalized model (aka\nmemory) for each task besides the meta-model and only update them for the\nsampled tasks by a momentum method that incorporates historical updates at each\niteration. The theoretical results significantly improve the optimization\ntheory for MAML and the empirical results also corroborate the theory.",
    "descriptor": "",
    "authors": [
      "Bokun Wang",
      "Zhuoning Yuan",
      "Yiming Ying",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04911"
  },
  {
    "id": "arXiv:2106.04912",
    "title": "ClipGen: A Deep Generative Model for Clipart Vectorization and Synthesis",
    "abstract": "This paper presents a novel deep learning-based approach for automatically\nvectorizing and synthesizing the clipart of man-made objects. Given a raster\nclipart image and its corresponding object category (e.g., airplanes), the\nproposed method sequentially generates new layers, each of which is composed of\na new closed path filled with a single color. The final result is obtained by\ncompositing all layers together into a vector clipart image that falls into the\ntarget category. The proposed approach is based on an iterative generative\nmodel that (i) decides whether to continue synthesizing a new layer and (ii)\ndetermines the geometry and appearance of the new layer. We formulated a joint\nloss function for training our generative model, including the shape\nsimilarity, symmetry, and local curve smoothness losses, as well as vector\ngraphics rendering accuracy loss for synthesizing clipart recognizable by\nhumans. We also introduced a collection of man-made object clipart, ClipNet,\nwhich is composed of closed-path layers, and two designed preprocessing tasks\nto clean up and enrich the original raw clipart. To validate the proposed\napproach, we conducted several experiments and demonstrated its ability to\nvectorize and synthesize various clipart categories. We envision that our\ngenerative model can facilitate efficient and intuitive clipart designs for\nnovice users and graphic designers.",
    "descriptor": "\nComments: 15 pages, TVCG2021\n",
    "authors": [
      "I-Chao Shen",
      "Bing-Yu Chen"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.04912"
  },
  {
    "id": "arXiv:2106.04913",
    "title": "On Margin-Based Cluster Recovery with Oracle Queries",
    "abstract": "We study an active cluster recovery problem where, given a set of $n$ points\nand an oracle answering queries like \"are these two points in the same\ncluster?\", the task is to recover exactly all clusters using as few queries as\npossible. We begin by introducing a simple but general notion of margin between\nclusters that captures, as special cases, the margins used in previous work,\nthe classic SVM margin, and standard notions of stability for center-based\nclusterings. Then, under our margin assumptions we design algorithms that, in a\nvariety of settings, recover all clusters exactly using only $O(\\log n)$\nqueries. For the Euclidean case, $\\mathbb{R}^m$, we give an algorithm that\nrecovers arbitrary convex clusters, in polynomial time, and with a number of\nqueries that is lower than the best existing algorithm by $\\Theta(m^m)$\nfactors. For general pseudometric spaces, where clusters might not be convex or\nmight not have any notion of shape, we give an algorithm that achieves the\n$O(\\log n)$ query bound, and is provably near-optimal as a function of the\npacking number of the space. Finally, for clusterings realized by binary\nconcept classes, we give a combinatorial characterization of recoverability\nwith $O(\\log n)$ queries, and we show that, for many concept classes in\nEuclidean spaces, this characterization is equivalent to our margin condition.\nOur results show a deep connection between cluster margins and active cluster\nrecoverability.",
    "descriptor": "",
    "authors": [
      "Marco Bressan",
      "Nicol\u00f2 Cesa-Bianchi",
      "Silvio Lattanzi",
      "Andrea Paudice"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04913"
  },
  {
    "id": "arXiv:2106.04914",
    "title": "Exploiting Learned Symmetries in Group Equivariant Convolutions",
    "abstract": "Group Equivariant Convolutions (GConvs) enable convolutional neural networks\nto be equivariant to various transformation groups, but at an additional\nparameter and compute cost. We investigate the filter parameters learned by\nGConvs and find certain conditions under which they become highly redundant. We\nshow that GConvs can be efficiently decomposed into depthwise separable\nconvolutions while preserving equivariance properties and demonstrate improved\nperformance and data efficiency on two datasets. All code is publicly available\nat github.com/Attila94/SepGrouPy.",
    "descriptor": "",
    "authors": [
      "Attila Lengyel",
      "Jan C. van Gemert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04914"
  },
  {
    "id": "arXiv:2106.04916",
    "title": "Erratum: Leveraging Flexible Tree Matching to Repair Broken Locators in  Web Automation Scripts",
    "abstract": "Web applications are constantly evolving to integrate new features and fix\nreported bugs. Even an imperceptible change can sometimes entail significant\nmodifications of the Document Object Model (DOM), which is the underlying model\nused by browsers to render all the elements included in a web application.\nScripts that interact with web applications (e.g. web test scripts, crawlers,\nor robotic process automation) rely on this continuously evolving DOM which\nmeans they are often particularly fragile. More precisely, the major cause of\nbreakages observed in automation scripts are element locators, which are\nidentifiers used by automation scripts to navigate across the DOM. When the DOM\nevolves, these identifiers tend to break, thus causing the related scripts to\nno longer locate the intended target elements. For this reason, several\ncontributions explored the idea of automatically repairing broken locators on a\npage. These works attempt to repair a given broken locator by scanning all\nelements in the new DOM to find the most similar one. Unfortunately, this\napproach fails to scale when the complexity of web pages grows, leading either\nto long computation times or incorrect element repairs. This article,\ntherefore, adopts a different perspective on this problem by introducing a new\nlocator repair solution that leverages tree matching algorithms to relocate\nbroken locators. This solution, named Erratum, implements a holistic approach\nto reduce the element search space, which greatly eases the locator repair task\nand drastically improves repair accuracy. We compare the robustness of Erratum\non a large-scale benchmark composed of realistic and synthetic mutations\napplied to popular web applications currently deployed in production. Our\nempirical results demonstrate that Erratum outperforms the accuracy of WATER, a\nstate-of-the-art solution, by 67%.",
    "descriptor": "",
    "authors": [
      "Sacha Brisset",
      "Romain Rouvoy",
      "Lionel Seinturier",
      "Renaud Pawlak"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.04916"
  },
  {
    "id": "arXiv:2106.04919",
    "title": "Cervical Cytology Classification Using PCA & GWO Enhanced Deep Features  Selection",
    "abstract": "Cervical cancer is one of the most deadly and common diseases among women\nworldwide. It is completely curable if diagnosed in an early stage, but the\ntedious and costly detection procedure makes it unviable to conduct\npopulation-wise screening. Thus, to augment the effort of the clinicians, in\nthis paper, we propose a fully automated framework that utilizes Deep Learning\nand feature selection using evolutionary optimization for cytology image\nclassification. The proposed framework extracts Deep feature from several\nConvolution Neural Network models and uses a two-step feature reduction\napproach to ensure reduction in computation cost and faster convergence. The\nfeatures extracted from the CNN models form a large feature space whose\ndimensionality is reduced using Principal Component Analysis while preserving\n99% of the variance. A non-redundant, optimal feature subset is selected from\nthis feature space using an evolutionary optimization algorithm, the Grey Wolf\nOptimizer, thus improving the classification performance. Finally, the selected\nfeature subset is used to train an SVM classifier for generating the final\npredictions. The proposed framework is evaluated on three publicly available\nbenchmark datasets: Mendeley Liquid Based Cytology (4-class) dataset, Herlev\nPap Smear (7-class) dataset, and the SIPaKMeD Pap Smear (5-class) dataset\nachieving classification accuracies of 99.47%, 98.32% and 97.87% respectively,\nthus justifying the reliability of the approach. The relevant codes for the\nproposed approach can be found in:\nhttps://github.com/DVLP-CMATERJU/Two-Step-Feature-Enhancement",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Hritam Basak",
      "Rohit Kundu",
      "Sukanta Chakraborty",
      "Nibaran Das"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04919"
  },
  {
    "id": "arXiv:2106.04920",
    "title": "Towards Deep Industrial Transfer Learning for Anomaly Detection on Time  Series Data",
    "abstract": "Deep learning promises performant anomaly detection on time-variant datasets,\nbut greatly suffers from low availability of suitable training datasets and\nfrequently changing tasks. Deep transfer learning offers mitigation by letting\nalgorithms built upon previous knowledge from different tasks or locations. In\nthis article, a modular deep learning algorithm for anomaly detection on time\nseries datasets is presented that allows for an easy integration of such\ntransfer learning capabilities. It is thoroughly tested on a dataset from a\ndiscrete manufacturing process in order to prove its fundamental adequacy\ntowards deep industrial transfer learning - the transfer of knowledge in\nindustrial applications' special environment.",
    "descriptor": "\nComments: 8 pages, 5 figures, 5 tables. Accepted for publication at IEEE ETFA 2021\n",
    "authors": [
      "Benjamin Maschler",
      "Tim Knodel",
      "Michael Weyrich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04920"
  },
  {
    "id": "arXiv:2106.04921",
    "title": "Self-supervised Feature Enhancement: Applying Internal Pretext Task to  Supervised Learning",
    "abstract": "Traditional self-supervised learning requires CNNs using external pretext\ntasks (i.e., image- or video-based tasks) to encode high-level semantic visual\nrepresentations. In this paper, we show that feature transformations within\nCNNs can also be regarded as supervisory signals to construct the\nself-supervised task, called \\emph{internal pretext task}. And such a task can\nbe applied for the enhancement of supervised learning. Specifically, we first\ntransform the internal feature maps by discarding different channels, and then\ndefine an additional internal pretext task to identify the discarded channels.\nCNNs are trained to predict the joint labels generated by the combination of\nself-supervised labels and original labels. By doing so, we let CNNs know which\nchannels are missing while classifying in the hope to mine richer feature\ninformation. Extensive experiments show that our approach is effective on\nvarious models and datasets. And it's worth noting that we only incur\nnegligible computational overhead. Furthermore, our approach can also be\ncompatible with other methods to get better results.",
    "descriptor": "",
    "authors": [
      "Yuhang Yang",
      "Zilin Ding",
      "Xuan Cheng",
      "Xiaomin Wang",
      "Ming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04921"
  },
  {
    "id": "arXiv:2106.04922",
    "title": "Self-supervision of Feature Transformation for Further Improving  Supervised Learning",
    "abstract": "Self-supervised learning, which benefits from automatically constructing\nlabels through pre-designed pretext task, has recently been applied for\nstrengthen supervised learning. Since previous self-supervised pretext tasks\nare based on input, they may incur huge additional training overhead. In this\npaper we find that features in CNNs can be also used for self-supervision. Thus\nwe creatively design the \\emph{feature-based pretext task} which requires only\na small amount of additional training overhead. In our task we discard\ndifferent particular regions of features, and then train the model to\ndistinguish these different features. In order to fully apply our feature-based\npretext task in supervised learning, we also propose a novel learning framework\ncontaining multi-classifiers for further improvement. Original labels will be\nexpanded to joint labels via self-supervision of feature transformations. With\nmore semantic information provided by our self-supervised tasks, this approach\ncan train CNNs more effectively. Extensive experiments on various supervised\nlearning tasks demonstrate the accuracy improvement and wide applicability of\nour method.",
    "descriptor": "\nComments: 15 pages, 6 figures\n",
    "authors": [
      "Zilin Ding",
      "Yuhang Yang",
      "Xuan Cheng",
      "Xiaomin Wang",
      "Ming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04922"
  },
  {
    "id": "arXiv:2106.04927",
    "title": "A Bi-Level Framework for Learning to Solve Combinatorial Optimization on  Graphs",
    "abstract": "Combinatorial Optimization (CO) has been a long-standing challenging research\ntopic featured by its NP-hard nature. Traditionally such problems are\napproximately solved with heuristic algorithms which are usually fast but may\nsacrifice the solution quality. Currently, machine learning for combinatorial\noptimization (MLCO) has become a trending research topic, but most existing\nMLCO methods treat CO as a single-level optimization by directly learning the\nend-to-end solutions, which are hard to scale up and mostly limited by the\ncapacity of ML models given the high complexity of CO. In this paper, we\npropose a hybrid approach to combine the best of the two worlds, in which a\nbi-level framework is developed with an upper-level learning method to optimize\nthe graph (e.g. add, delete or modify edges in a graph), fused with a\nlower-level heuristic algorithm solving on the optimized graph. Such a bi-level\napproach simplifies the learning on the original hard CO and can effectively\nmitigate the demand for model capacity. The experiments and results on several\npopular CO problems like Directed Acyclic Graph scheduling, Graph Edit Distance\nand Hamiltonian Cycle Problem show its effectiveness over manually designed\nheuristics and single-level learning methods.",
    "descriptor": "",
    "authors": [
      "Runzhong Wang",
      "Zhigang Hua",
      "Gan Liu",
      "Jiayi Zhang",
      "Junchi Yan",
      "Feng Qi",
      "Shuang Yang",
      "Jun Zhou",
      "Xiaokang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.04927"
  },
  {
    "id": "arXiv:2106.04928",
    "title": "Reliable Adversarial Distillation with Unreliable Teachers",
    "abstract": "In ordinary distillation, student networks are trained with soft labels (SLs)\ngiven by pretrained teacher networks, and students are expected to improve upon\nteachers since SLs are stronger supervision than the original hard labels.\nHowever, when considering adversarial robustness, teachers may become\nunreliable and adversarial distillation may not work: teachers are pretrained\non their own adversarial data, and it is too demanding to require that teachers\nare also good at every adversarial data queried by students. Therefore, in this\npaper, we propose reliable introspective adversarial distillation (IAD) where\nstudents partially instead of fully trust their teachers. Specifically, IAD\ndistinguishes between three cases given a query of a natural data (ND) and the\ncorresponding adversarial data (AD): (a) if a teacher is good at AD, its SL is\nfully trusted; (b) if a teacher is good at ND but not AD, its SL is partially\ntrusted and the student also takes its own SL into account; (c) otherwise, the\nstudent only relies on its own SL. Experiments demonstrate the effectiveness of\nIAD for improving upon teachers in terms of adversarial robustness.",
    "descriptor": "",
    "authors": [
      "Jianing Zhu",
      "Jiangchao Yao",
      "Bo Han",
      "Jingfeng Zhang",
      "Tongliang Liu",
      "Gang Niu",
      "Jingren Zhou",
      "Jianliang Xu",
      "Hongxia Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04928"
  },
  {
    "id": "arXiv:2106.04935",
    "title": "Neural Supervised Domain Adaptation by Augmenting Pre-trained Models  with Random Units",
    "abstract": "Neural Transfer Learning (TL) is becoming ubiquitous in Natural Language\nProcessing (NLP), thanks to its high performance on many tasks, especially in\nlow-resourced scenarios. Notably, TL is widely used for neural domain\nadaptation to transfer valuable knowledge from high-resource to low-resource\ndomains. In the standard fine-tuning scheme of TL, a model is initially\npre-trained on a source domain and subsequently fine-tuned on a target domain\nand, therefore, source and target domains are trained using the same\narchitecture. In this paper, we show through interpretation methods that such\nscheme, despite its efficiency, is suffering from a main limitation. Indeed,\nalthough capable of adapting to new domains, pre-trained neurons struggle with\nlearning certain patterns that are specific to the target domain. Moreover, we\nshed light on the hidden negative transfer occurring despite the high\nrelatedness between source and target domains, which may mitigate the final\ngain brought by transfer learning. To address these problems, we propose to\naugment the pre-trained model with normalised, weighted and randomly\ninitialised units that foster a better adaptation while maintaining the\nvaluable source knowledge. We show that our approach exhibits significant\nimprovements to the standard fine-tuning scheme for neural domain adaptation\nfrom the news domain to the social media domain on four NLP tasks:\npart-of-speech tagging, chunking, named entity recognition and morphosyntactic\ntagging.",
    "descriptor": "",
    "authors": [
      "Sara Meftah",
      "Nasredine Semmar",
      "Youssef Tamaazousti",
      "Hassane Essafi",
      "Fatiha Sadat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04935"
  },
  {
    "id": "arXiv:2106.04938",
    "title": "Attacking Adversarial Attacks as A Defense",
    "abstract": "It is well known that adversarial attacks can fool deep neural networks with\nimperceptible perturbations. Although adversarial training significantly\nimproves model robustness, failure cases of defense still broadly exist. In\nthis work, we find that the adversarial attacks can also be vulnerable to small\nperturbations. Namely, on adversarially-trained models, perturbing adversarial\nexamples with a small random noise may invalidate their misled predictions.\nAfter carefully examining state-of-the-art attacks of various kinds, we find\nthat all these attacks have this deficiency to different extents. Enlightened\nby this finding, we propose to counter attacks by crafting more effective\ndefensive perturbations. Our defensive perturbations leverage the advantage\nthat adversarial training endows the ground-truth class with smaller local\nLipschitzness. By simultaneously attacking all the classes, the misled\npredictions with larger Lipschitzness can be flipped into correct ones. We\nverify our defensive perturbation with both empirical experiments and\ntheoretical analyses on a linear model. On CIFAR10, it boosts the\nstate-of-the-art model from 66.16% to 72.66% against the four attacks of\nAutoAttack, including 71.76% to 83.30% against the Square attack. On ImageNet,\nthe top-1 robust accuracy of FastAT is improved from 33.18% to 38.54% under the\n100-step PGD attack.",
    "descriptor": "",
    "authors": [
      "Boxi Wu",
      "Heng Pan",
      "Li Shen",
      "Jindong Gu",
      "Shuai Zhao",
      "Zhifeng Li",
      "Deng Cai",
      "Xiaofei He",
      "Wei Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04938"
  },
  {
    "id": "arXiv:2106.04939",
    "title": "Phraseformer: Multimodal Key-phrase Extraction using Transformer and  Graph Embedding",
    "abstract": "Background: Keyword extraction is a popular research topic in the field of\nnatural language processing. Keywords are terms that describe the most relevant\ninformation in a document. The main problem that researchers are facing is how\nto efficiently and accurately extract the core keywords from a document.\nHowever, previous keyword extraction approaches have utilized the text and\ngraph features, there is the lack of models that can properly learn and combine\nthese features in a best way.\nMethods: In this paper, we develop a multimodal Key-phrase extraction\napproach, namely Phraseformer, using transformer and graph embedding\ntechniques. In Phraseformer, each keyword candidate is presented by a vector\nwhich is the concatenation of the text and structure learning representations.\nPhraseformer takes the advantages of recent researches such as BERT and ExEm to\npreserve both representations. Also, the Phraseformer treats the key-phrase\nextraction task as a sequence labeling problem solved using classification\ntask.\nResults: We analyze the performance of Phraseformer on three datasets\nincluding Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we\ninvestigate the performance of different classifiers on Phraseformer method\nover Inspec dataset. Experimental results demonstrate the effectiveness of\nPhraseformer method over the three datasets used. Additionally, the Random\nForest classifier gain the highest F1-score among all classifiers.\nConclusions: Due to the fact that the combination of BERT and ExEm is more\nmeaningful and can better represent the semantic of words. Hence, Phraseformer\nsignificantly outperforms single-modality methods.",
    "descriptor": "",
    "authors": [
      "Narjes Nikzad-Khasmakhi",
      "Mohammad-Reza Feizi-Derakhshi",
      "Meysam Asgari-Chenaghlu",
      "Mohammad-Ali Balafar",
      "Ali-Reza Feizi-Derakhshi",
      "Taymaz Rahkar-Farshi",
      "Majid Ramezani",
      "Zoleikha Jahanbakhsh-Nagadeh",
      "Elnaz Zafarani-Moattar",
      "Mehrdad Ranjbar-Khadivi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04939"
  },
  {
    "id": "arXiv:2106.04941",
    "title": "Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach",
    "abstract": "Learning faithful graph representations as sets of vertex embeddings has\nbecome a fundamental intermediary step in a wide range of machine learning\napplications. We propose the systematic use of symmetric spaces in\nrepresentation learning, a class encompassing many of the previously used\nembedding targets. This enables us to introduce a new method, the use of\nFinsler metrics integrated in a Riemannian optimization scheme, that better\nadapts to dissimilar structures in the graph. We develop a tool to analyze the\nembeddings and infer structural properties of the data sets. For\nimplementation, we choose Siegel spaces, a versatile family of symmetric\nspaces. Our approach outperforms competitive baselines for graph reconstruction\ntasks on various synthetic and real-world datasets. We further demonstrate its\napplicability on two downstream tasks, recommender systems and node\nclassification.",
    "descriptor": "\nComments: 28 pages. Accepted at ICML 2021\n",
    "authors": [
      "Federico L\u00f3pez",
      "Beatrice Pozzetti",
      "Steve Trettel",
      "Michael Strube",
      "Anna Wienhard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.04941"
  },
  {
    "id": "arXiv:2106.04944",
    "title": "Non-Parametric Stochastic Sequential Assignment With Random Arrival  Times",
    "abstract": "We consider a problem wherein jobs arrive at random times and assume random\nvalues. Upon each job arrival, the decision-maker must decide immediately\nwhether or not to accept the job and gain the value on offer as a reward, with\nthe constraint that they may only accept at most $n$ jobs over some reference\ntime period. The decision-maker only has access to $M$ independent realisations\nof the job arrival process. We propose an algorithm, Non-Parametric Sequential\nAllocation (NPSA), for solving this problem. Moreover, we prove that the\nexpected reward returned by the NPSA algorithm converges in probability to\noptimality as $M$ grows large. We demonstrate the effectiveness of the\nalgorithm empirically on synthetic data and on public fraud-detection datasets,\nfrom where the motivation for this work is derived.",
    "descriptor": "\nComments: Accepted to IJCAI '21, full version with Supplementary Material\n",
    "authors": [
      "Danial Dervovic",
      "Parisa Hassanzadeh",
      "Samuel Assefa",
      "Prashant Reddy"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04944"
  },
  {
    "id": "arXiv:2106.04949",
    "title": "On the Analysis of the Second Order Time Filtered Backward Euler Method  for the EMAC formulation of Navier-Stokes Equations",
    "abstract": "This paper considers the backward Euler based linear time filtering method\nfor the EMAC formulation of the incompressible Navier-Stokes equations. The\ntime filtering is added as a modular step to the standard backward Euler code\nleading to a 2-step, unconditionally stable, second order linear method.\nDespite its success in conserving important physical quantities when the\ndivergence constraint is only weakly enforced, the EMAC formulation is unable\nto improve solutions of backward Euler discretized NSE. The combination of the\ntime filtering with the backward Euler discretized EMAC formulation of NSE\ngreatly increases numerical accuracy of solutions and still conserves energy,\nmomentum and angular momentum as EMAC does. Several numerical experiments are\nprovided that both verify the theoretical fidings and demonstrate superiority\nof the proposed method over the unfiltered case.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Medine Demir",
      "Aytekin \u00c7\u0131b\u0131k",
      "Song\u00fcl Kaya"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04949"
  },
  {
    "id": "arXiv:2106.04951",
    "title": "Information flow based defensive chain for data leakage detection and  prevention: a survey",
    "abstract": "Mobile and IoT applications have greatly enriched our daily life by providing\nconvenient and intelligent services. However, these smart applications have\nbeen a prime target of adversaries for stealing sensitive data. It poses a\ncrucial threat to users' identity security, financial security, or even life\nsecurity. Research communities and industries have proposed many Information\nFlow Control (IFC) techniques for data leakage detection and prevention,\nincluding secure modeling, type system, static analysis, dynamic analysis,\n\\textit{etc}. According to the application's development life cycle, although\nmost attacks are conducted during the application's execution phase, data\nleakage vulnerabilities have been introduced since the design phase. With a\nfocus on lifecycle protection, this survey reviews the recent representative\nworks adopted in different phases. We propose an information flow based\ndefensive chain, which provides a new framework to systematically understand\nvarious IFC techniques for data leakage detection and prevention in Mobile and\nIoT applications. In line with the phases of the application life cycle, each\nreviewed work is comprehensively studied in terms of technique, performance,\nand limitation. Research challenges and future directions are also pointed out\nby consideration of the integrity of the defensive chain.",
    "descriptor": "\nComments: 36 pages, 6 figures, 6 tables\n",
    "authors": [
      "Ning Xi",
      "Chao Chen",
      "Jun Zhang",
      "Cong Sun",
      "Shigang Liu",
      "Pengbin Feng",
      "Jianfeng Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04951"
  },
  {
    "id": "arXiv:2106.04953",
    "title": "Expectation Programming",
    "abstract": "Building on ideas from probabilistic programming, we introduce the concept of\nan expectation programming framework (EPF) that automates the calculation of\nexpectations. Analogous to a probabilistic program, an expectation program is\ncomprised of a mix of probabilistic constructs and deterministic calculations\nthat define a conditional distribution over its variables. However, the focus\nof the inference engine in an EPF is to directly estimate the resulting\nexpectation of the program return values, rather than approximate the\nconditional distribution itself. This distinction allows us to achieve\nsubstantial performance improvements over the standard probabilistic\nprogramming pipeline by tailoring the inference to the precise expectation we\ncare about. We realize a particular instantiation of our EPF concept by\nextending the probabilistic programming language Turing to allow so-called\ntarget-aware inference to be run automatically, and show that this leads to\nsignificant empirical gains compared to conventional posterior-based inference.",
    "descriptor": "",
    "authors": [
      "Tim Reichelt",
      "Adam Goli\u0144ski",
      "Luke Ong",
      "Tom Rainforth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.04953"
  },
  {
    "id": "arXiv:2106.04957",
    "title": "Real Time Egocentric Object Segmentation: THU-READ Labeling and  Benchmarking Results",
    "abstract": "Egocentric segmentation has attracted recent interest in the computer vision\ncommunity due to their potential in Mixed Reality (MR) applications. While most\nprevious works have been focused on segmenting egocentric human body parts\n(mainly hands), little attention has been given to egocentric objects. Due to\nthe lack of datasets of pixel-wise annotations of egocentric objects, in this\npaper we contribute with a semantic-wise labeling of a subset of 2124 images\nfrom the RGB-D THU-READ Dataset. We also report benchmarking results using\nThundernet, a real-time semantic segmentation network, that could allow future\nintegration with end-to-end MR applications.",
    "descriptor": "\nComments: Accepted for presentation at EPIC@CVPR2021 workshop\n",
    "authors": [
      "E. Gonzalez-Sosa",
      "G. Robledo",
      "D. Gonzalez-Morin",
      "P. Perez-Garcia",
      "A. Villegas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04957"
  },
  {
    "id": "arXiv:2106.04958",
    "title": "Unifying Behavioral and Response Diversity for Open-ended Learning in  Zero-sum Games",
    "abstract": "Measuring and promoting policy diversity is critical for solving games with\nstrong non-transitive dynamics where strategic cycles exist, and there is no\nconsistent winner (e.g., Rock-Paper-Scissors). With that in mind, maintaining a\npool of diverse policies via open-ended learning is an attractive solution,\nwhich can generate auto-curricula to avoid being exploited. However, in\nconventional open-ended learning algorithms, there are no widely accepted\ndefinitions for diversity, making it hard to construct and evaluate the diverse\npolicies. In this work, we summarize previous concepts of diversity and work\ntowards offering a unified measure of diversity in multi-agent open-ended\nlearning to include all elements in Markov games, based on both Behavioral\nDiversity (BD) and Response Diversity (RD). At the trajectory distribution\nlevel, we re-define BD in the state-action space as the discrepancies of\noccupancy measures. For the reward dynamics, we propose RD to characterize\ndiversity through the responses of policies when encountering different\nopponents. We also show that many current diversity measures fall in one of the\ncategories of BD or RD but not both. With this unified diversity measure, we\ndesign the corresponding diversity-promoting objective and population\neffectivity when seeking the best responses in open-ended learning. We validate\nour methods in both relatively simple games like matrix game, non-transitive\nmixture model, and the complex \\textit{Google Research Football} environment.\nThe population found by our methods reveals the lowest exploitability, highest\npopulation effectivity in matrix game and non-transitive mixture model, as well\nas the largest goal difference when interacting with opponents of various\nlevels in \\textit{Google Research Football}.",
    "descriptor": "\nComments: We investigate a new perspective on unifying diversity measures for open-ended learning in zero-sum games, which shapes an auto-curriculum to induce diverse yet effective behaviors\n",
    "authors": [
      "Xiangyu Liu",
      "Hangtian Jia",
      "Ying Wen",
      "Yaodong Yang",
      "Yujing Hu",
      "Yingfeng Chen",
      "Changjie Fan",
      "Zhipeng Hu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04958"
  },
  {
    "id": "arXiv:2106.04959",
    "title": "Auto-tagging of Short Conversational Sentences using Natural Language  Processing Methods",
    "abstract": "In this study, we aim to find a method to auto-tag sentences specific to a\ndomain. Our training data comprises short conversational sentences extracted\nfrom chat conversations between company's customer representatives and web site\nvisitors. We manually tagged approximately 14 thousand visitor inputs into ten\nbasic categories, which will later be used in a transformer-based language\nmodel with attention mechanisms for the ultimate goal of developing a chatbot\napplication that can produce meaningful dialogue. We considered three different\nstate-of-the-art models and reported their auto-tagging capabilities. We\nachieved the best performance with the bidirectional encoder representation\nfrom transformers (BERT) model. Implementation of the models used in these\nexperiments can be cloned from our GitHub repository and tested for similar\nauto-tagging problems without much effort.",
    "descriptor": "\nComments: in Turkish language\n",
    "authors": [
      "\u015e\u00fckr\u00fc Ozan",
      "D. Emre Ta\u015far"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04959"
  },
  {
    "id": "arXiv:2106.04963",
    "title": "Psycholinguistic Tripartite Graph Network for Personality Detection",
    "abstract": "Most of the recent work on personality detection from online posts adopts\nmultifarious deep neural networks to represent the posts and builds predictive\nmodels in a data-driven manner, without the exploitation of psycholinguistic\nknowledge that may unveil the connections between one's language usage and his\npsychological traits. In this paper, we propose a psycholinguistic\nknowledge-based tripartite graph network, TrigNet, which consists of a\ntripartite graph network and a BERT-based graph initializer. The graph network\ninjects structural psycholinguistic knowledge from LIWC, a computerized\ninstrument for psycholinguistic analysis, by constructing a heterogeneous\ntripartite graph. The graph initializer is employed to provide initial\nembeddings for the graph nodes. To reduce the computational cost in graph\nlearning, we further propose a novel flow graph attention network (GAT) that\nonly transmits messages between neighboring parties in the tripartite graph.\nBenefiting from the tripartite graph, TrigNet can aggregate post information\nfrom a psychological perspective, which is a novel way of exploiting domain\nknowledge. Extensive experiments on two datasets show that TrigNet outperforms\nthe existing state-of-art model by 3.47 and 2.10 points in average F1.\nMoreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%,\nrespectively, in comparison to the original GAT in our setting.",
    "descriptor": "\nComments: Accepted by ACL 2021\n",
    "authors": [
      "Tao Yang",
      "Feifan Yang",
      "Haolan Ouyang",
      "Xiaojun Quan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04963"
  },
  {
    "id": "arXiv:2106.04966",
    "title": "Towards Explainable Abnormal Infant Movements Identification: A  Body-part Based Prediction and Visualisation Framework",
    "abstract": "Providing early diagnosis of cerebral palsy (CP) is key to enhancing the\ndevelopmental outcomes for those affected. Diagnostic tools such as the General\nMovements Assessment (GMA), have produced promising results in early diagnosis,\nhowever these manual methods can be laborious.\nIn this paper, we propose a new framework for the automated classification of\ninfant body movements, based upon the GMA, which unlike previous methods, also\nincorporates a visualization framework to aid with interpretability. Our\nproposed framework segments extracted features to detect the presence of\nFidgety Movements (FMs) associated with the GMA spatiotemporally. These\nfeatures are then used to identify the body-parts with the greatest\ncontribution towards a classification decision and highlight the related\nbody-part segment providing visual feedback to the user.\nWe quantitatively compare the proposed framework's classification performance\nwith several other methods from the literature and qualitatively evaluate the\nvisualization's veracity. Our experimental results show that the proposed\nmethod performs more robustly than comparable techniques in this setting whilst\nsimultaneously providing relevant visual interpretability.",
    "descriptor": "\nComments: Proceedings of the 2021 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), accepted, 2021\n",
    "authors": [
      "Kevin D. McCay",
      "Edmond S. L. Ho",
      "Dimitrios Sakkos",
      "Wai Lok Woo",
      "Claire Marcroft",
      "Patricia Dulson",
      "Nicholas D. Embleton"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04966"
  },
  {
    "id": "arXiv:2106.04967",
    "title": "GP-ConvCNP: Better Generalization for Convolutional Conditional Neural  Processes on Time Series Data",
    "abstract": "Neural Processes (NPs) are a family of conditional generative models that are\nable to model a distribution over functions, in a way that allows them to\nperform predictions at test time conditioned on a number of context points. A\nrecent addition to this family, Convolutional Conditional Neural Processes\n(ConvCNP), have shown remarkable improvement in performance over prior art, but\nwe find that they sometimes struggle to generalize when applied to time series\ndata. In particular, they are not robust to distribution shifts and fail to\nextrapolate observed patterns into the future. By incorporating a Gaussian\nProcess into the model, we are able to remedy this and at the same time improve\nperformance within distribution. As an added benefit, the Gaussian Process\nreintroduces the possibility to sample from the model, a key feature of other\nmembers in the NP family.",
    "descriptor": "\nComments: UAI 2021\n",
    "authors": [
      "Jens Petersen",
      "Gregor K\u00f6hler",
      "David Zimmerer",
      "Fabian Isensee",
      "Paul F. J\u00e4ger",
      "Klaus H. Maier-Hein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04967"
  },
  {
    "id": "arXiv:2106.04968",
    "title": "Eight Reasons Why Cybersecurity on Novel Generations of Brain-Computer  Interfaces Must Be Prioritized",
    "abstract": "This article presents eight neural cyberattacks affecting spontaneous neural\nactivity, inspired by well-known cyberattacks from the computer science domain:\nNeural Flooding, Neural Jamming, Neural Scanning, Neural Selective Forwarding,\nNeural Spoofing, Neural Sybil, Neural Sinkhole and Neural Nonce. These\ncyberattacks are based on the exploitation of vulnerabilities existing in the\nnew generation of Brain-Computer Interfaces. After presenting their formal\ndefinitions, the cyberattacks have been implemented over a neuronal simulation.\nTo evaluate the impact of each cyberattack, they have been implemented in a\nConvolutional Neural Network (CNN) simulating a portion of a mouse's visual\ncortex. This implementation is based on existing literature indicating the\nsimilarities that CNNs have with neuronal structures from the visual cortex.\nSome conclusions are also provided, indicating that Neural Nonce and Neural\nJamming are the most impactful cyberattacks for short-term effects, while\nNeural Scanning and Neural Nonce are the most damaging for long-term effects.",
    "descriptor": "",
    "authors": [
      "Sergio L\u00f3pez Bernal",
      "Alberto Huertas Celdr\u00e1n",
      "Gregorio Mart\u00ednez P\u00e9rez"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04968"
  },
  {
    "id": "arXiv:2106.04969",
    "title": "Feedback Capacity Formulas of AGN Channels Driven by Nonstationary  Autoregressive Moving Average Noise",
    "abstract": "In this paper we derive closed-form formulas of feedback capacity and\nnonfeedback achievable rates, for Additive Gaussian Noise (AGN) channels driven\nby nonstationary autoregressive moving average (ARMA) noise (with unstable one\npoles and zeros), based on time-invariant feedback codes and channel input\ndistributions. From the analysis and simulations follows the surprising\nobservations, (i) the use of time-invariant channel input distributions gives\nrise to multiple regimes of capacity that depend on the parameters of the ARMA\nnoise, which may or may not use feedback, (ii) the more unstable the pole\n(resp. zero) of the ARMA noise the higher (resp. lower) the feedback capacity,\n(iii) certain conditions, known as detectability and stabilizability are\nnecessary and sufficient to ensure the feedback capacity formulas and\nnonfeedback achievable rates {\\it are independent of the initial state of the\nARMA noise}. Another surprizing observation is that Kim's \\cite{kim2010}\ncharacterization of feedback capacity which is developed for stable ARMA noise,\nif applied to the unstable ARMA noise, gives a lower value of feedback capacity\ncompared to our feedback capacity formula.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Stelios Louka",
      "Christos Kourtellaris",
      "Charalambos D. Charalambous"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04969"
  },
  {
    "id": "arXiv:2106.04970",
    "title": "Instantaneous Grammatical Error Correction with Shallow Aggressive  Decoding",
    "abstract": "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the\nonline inference efficiency of the Transformer for instantaneous Grammatical\nError Correction (GEC). SAD optimizes the online inference efficiency for GEC\nby two innovations: 1) it aggressively decodes as many tokens as possible in\nparallel instead of always decoding only one token in each step to improve\ncomputational parallelism; 2) it uses a shallow decoder instead of the\nconventional Transformer architecture with balanced encoder-decoder depth to\nreduce the computational cost during inference. Experiments in both English and\nChinese GEC benchmarks show that aggressive decoding could yield the same\npredictions as greedy decoding but with a significant speedup for online\ninference. Its combination with the shallow decoder could offer an even higher\nonline inference speedup over the powerful Transformer baseline without quality\nloss. Not only does our approach allow a single model to achieve the\nstate-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14\nand 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference\nspeedup over the Transformer-big model, but also it is easily adapted to other\nlanguages. Our code is available at\nhttps://github.com/AutoTemp/Shallow-Aggressive-Decoding.",
    "descriptor": "\nComments: Accepted by ACL2021 (main conference)\n",
    "authors": [
      "Xin Sun",
      "Tao Ge",
      "Furu Wei",
      "Houfeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04970"
  },
  {
    "id": "arXiv:2106.04972",
    "title": "Understanding Softmax Confidence and Uncertainty",
    "abstract": "It is often remarked that neural networks fail to increase their uncertainty\nwhen predicting on data far from the training distribution. Yet naively using\nsoftmax confidence as a proxy for uncertainty achieves modest success in tasks\nexclusively testing for this, e.g., out-of-distribution (OOD) detection. This\npaper investigates this contradiction, identifying two implicit biases that do\nencourage softmax confidence to correlate with epistemic uncertainty: 1)\nApproximately optimal decision boundary structure, and 2) Filtering effects of\ndeep networks. It describes why low-dimensional intuitions about softmax\nconfidence are misleading. Diagnostic experiments quantify reasons softmax\nconfidence can fail, finding that extrapolations are less to blame than overlap\nbetween training and OOD data in final-layer representations.\nPre-trained/fine-tuned networks reduce this overlap.",
    "descriptor": "",
    "authors": [
      "Tim Pearce",
      "Alexandra Brintrup",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04972"
  },
  {
    "id": "arXiv:2106.04973",
    "title": "Reachability Problems for Transmission Graphs",
    "abstract": "Let $P$ be a set of $n$ points in the plane where each point $p$ of $P$ is\nassociated with a radius $r_p>0$.The transmission graph $G=(P,E)$ of $P$ is\ndefined as the directed graph such that $E$ contains an edge from $p$ to $q$ if\nand only if $|pq|\\leq r_p$ for any two points $p$ and $q$ in $P$, where $|pq|$\ndenotes the Euclidean distance between $p$ and $q$. In this paper, we present a\ndata structure of size $O(n^{5/3})$ such that for any two points in $P$, we can\ncheck in $O(n^{2/3})$ time if there is a path in $G$ between the two points.\nThis is the first data structure for answering reachability queries whose\nperformance depends only on $n$ but not on the number of edges.",
    "descriptor": "\nComments: To appear in WADS2021\n",
    "authors": [
      "Shinwoo An",
      "Eunjin Oh"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.04973"
  },
  {
    "id": "arXiv:2106.04974",
    "title": "Grand Theft App: Digital Forensics of Vehicle Assistant Apps",
    "abstract": "Due to the increasing connectivity of modern vehicles, collected data is no\nlonger only stored in the vehicle itself but also transmitted to car\nmanufacturers and vehicle assistant apps. This development opens up new\npossibilities for digital forensics in criminal investigations involving modern\nvehicles. This paper deals with the digital forensic analysis of vehicle\nassistant apps of eight car manufacturers. We reconstruct the driver's\nactivities based on the data stored on the smartphones and in the\nmanufacturer's backend.\nFor this purpose, data of the Android and iOS apps of the car manufacturers\nAudi, BMW, Ford, Mercedes, Opel, Seat, Tesla, and Volkswagen were extracted\nfrom the smartphone and examined using digital forensic methods in accordance\nwith lawful government-approved forensics guidelines. Additionally,\nmanufacturer data was retrieved using Subject Access Requests. Using the\nextensive data gathered, we successfully reconstruct trips and refueling\nprocesses, determine parking positions and duration, and track the locking and\nunlocking of the vehicle.\nThese findings show that the digital forensic investigation of smartphone\napplications is a useful addition to vehicle forensics and should therefore be\ntaken into account in the strategic preparation of future digital forensic\ninvestigations.",
    "descriptor": "\nComments: This is the extended version of the Short Paper Grand Theft App: Digital Forensics of Vehicle Assistant Apps published at the 16th International Conference on Availability, Reliability and Security (ARES 2021)\n",
    "authors": [
      "Simon Ebbers",
      "Fabian Ising",
      "Christoph Saatjohann",
      "Sebastian Schinzel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04974"
  },
  {
    "id": "arXiv:2106.04976",
    "title": "A Compact Model for Scalable MTJ Simulation",
    "abstract": "This paper presents a physics-based modeling framework for the analysis and\ntransient simulation of circuits containing Spin-Transfer Torque (STT) Magnetic\nTunnel Junction (MTJ) devices. The framework provides the tools to analyze the\nstochastic behavior of MTJs and to generate Verilog-A compact models for their\nsimulation in large VLSI designs, addressing the need for an industry-ready\nmodel accounting for real-world reliability and scalability requirements.\nDevice dynamics are described by the Landau-Lifshitz-Gilbert-Slonczewsky\n(s-LLGS ) stochastic magnetization considering Voltage-Controlled Magnetic\nAnisotropy (VCMA) and the non-negligible statistical effects caused by thermal\nnoise. Model behavior is validated against the OOMMF magnetic simulator and its\nperformance is characterized on a 1-Mb 28 nm Magnetoresistive-RAM (MRAM) memory\nproduct.",
    "descriptor": "",
    "authors": [
      "Fernando Garc\u00eda-Redondo",
      "Pranay Prabhat",
      "Mudit Bhargava",
      "Cyrille Dray"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.04976"
  },
  {
    "id": "arXiv:2106.04979",
    "title": "Benchmarking the Nvidia GPU Lineage",
    "abstract": "For many, Graphics Processing Units (GPUs) provides a source of reliable\ncomputing power. Recently, Nvidia introduced its 9th generation HPC-grade GPUs,\nthe Ampere 100, claiming significant performance improvements over previous\ngenerations, particularly for AI-workloads, as well as introducing new\narchitectural features such as asynchronous data movement. But how well does\nthe A100 perform on non-AI benchmarks, and can we expect the A100 to deliver\nthe application improvements we have grown used to with previous GPU\ngenerations? In this paper, we benchmark the A100 GPU and compare it to four\nprevious generations of GPUs, with particular focus on empirically quantifying\nour derived performance expectations, and -- should those expectations be\nundelivered -- investigate whether the introduced data-movement features can\noffset any eventual loss in performance? We find that the A100 delivers less\nperformance increase than previous generations for the well-known Rodinia\nbenchmark suite; we show that some of these performance anomalies can be\nremedied through clever use of the new data-movement features, which we\nmicrobenchmark and demonstrate where (and more importantly, how) they should be\nused.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Martin Svedin",
      "Steven W. D. Chien",
      "Gibson Chikafa",
      "Niclas Jansson",
      "Artur Podobas"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.04979"
  },
  {
    "id": "arXiv:2106.04982",
    "title": "Cooperative Online Learning",
    "abstract": "In this preliminary (and unpolished) version of the paper, we study an\nasynchronous online learning setting with a network of agents. At each time\nstep, some of the agents are activated, requested to make a prediction, and pay\nthe corresponding loss. Some feedback is then revealed to these agents and is\nlater propagated through the network. We consider the case of full, bandit, and\nsemi-bandit feedback. In particular, we construct a reduction to delayed\nsingle-agent learning that applies to both the full and the bandit feedback\ncase and allows to obtain regret guarantees for both settings. We complement\nthese results with a near-matching lower bound.",
    "descriptor": "",
    "authors": [
      "Tommaso R. Cesari",
      "Riccardo Della Vecchia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04982"
  },
  {
    "id": "arXiv:2106.04984",
    "title": "Information Avoidance and Overvaluation in Sequential Decision Making  under Epistemic Constraints",
    "abstract": "Decision makers involved in the management of civil assets and systems\nusually take actions under constraints imposed by societal regulations. Some of\nthese constraints are related to epistemic quantities, as the probability of\nfailure events and the corresponding risks. Sensors and inspectors can provide\nuseful information supporting the control process (e.g. the maintenance process\nof an asset), and decisions about collecting this information should rely on an\nanalysis of its cost and value. When societal regulations encode an economic\nperspective that is not aligned with that of the decision makers, the Value of\nInformation (VoI) can be negative (i.e., information sometimes hurts), and\nalmost irrelevant information can even have a significant value (either\npositive or negative), for agents acting under these epistemic constraints. We\nrefer to these phenomena as Information Avoidance (IA) and Information\nOverValuation (IOV). In this paper, we illustrate how to assess VoI in\nsequential decision making under epistemic constraints (as those imposed by\nsocietal regulations), by modeling a Partially Observable Markov Decision\nProcesses (POMDP) and evaluating non optimal policies via Finite State\nControllers (FSCs). We focus on the value of collecting information at current\ntime, and on that of collecting sequential information, we illustrate how these\nvalues are related and we discuss how IA and IOV can occur in those settings.",
    "descriptor": "\nComments: submitted to Rel. Eng. Sys. Saf\n",
    "authors": [
      "Shuo Li",
      "Matteo Pozzi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04984"
  },
  {
    "id": "arXiv:2106.04985",
    "title": "Energy-Based Models for Code Generation under Compilability Constraints",
    "abstract": "Neural language models can be successfully trained on source code, leading to\napplications such as code completion. However, their versatile autoregressive\nself-supervision objective overlooks important global sequence-level features\nthat are present in the data such as syntactic correctness or compilability. In\nthis work, we pose the problem of learning to generate compilable code as\nconstraint satisfaction. We define an Energy-Based Model (EBM) representing a\npre-trained generative model with an imposed constraint of generating only\ncompilable sequences. We then use the KL-Adaptive Distributional Policy\nGradient algorithm (Khalifa et al., 2021) to train a generative model\napproximating the EBM. We conduct experiments showing that our proposed\napproach is able to improve compilability rates without sacrificing diversity\nand complexity of the generated samples.",
    "descriptor": "\nComments: Accepted for the First Workshop on Natural Language Processing for Programming, ACL 2021\n",
    "authors": [
      "Tomasz Korbak",
      "Hady Elsahar",
      "Marc Dymetman",
      "Germ\u00e1n Kruszewski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.04985"
  },
  {
    "id": "arXiv:2106.04986",
    "title": "Multistep Electric Vehicle Charging Station Occupancy Prediction using  Mixed LSTM Neural Networks",
    "abstract": "Public charging station occupancy prediction plays key importance in\ndeveloping a smart charging strategy to reduce electric vehicle (EV) operator\nand user inconvenience. However, existing studies are mainly based on\nconventional econometric or time series methodologies with limited accuracy. We\npropose a new mixed long short-term memory neural network incorporating both\nhistorical charging state sequences and time-related features for multistep\ndiscrete charging occupancy state prediction. Unlike the existing LSTM\nnetworks, the proposed model separates different types of features and handles\nthem differently with mixed neural network architecture. The model is compared\nto a number of state-of-the-art machine learning and deep learning approaches\nbased on the EV charging data obtained from the open data portal of the city of\nDundee, UK. The results show that the proposed method produces very accurate\npredictions (99.99% and 81.87% for 1 step (10 minutes) and 6 step (1 hour)\nahead, respectively, and outperforms the benchmark approaches significantly\n(+22.4% for one-step-ahead prediction and +6.2% for 6 steps ahead). A\nsensitivity analysis is conducted to evaluate the impact of the model\nparameters on prediction accuracy.",
    "descriptor": "",
    "authors": [
      "Tai-Yu Ma",
      "S\u00e9bastien Faye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04986"
  },
  {
    "id": "arXiv:2106.04989",
    "title": "CLCC: Contrastive Learning for Color Constancy",
    "abstract": "In this paper, we present CLCC, a novel contrastive learning framework for\ncolor constancy. Contrastive learning has been applied for learning\nhigh-quality visual representations for image classification. One key aspect to\nyield useful representations for image classification is to design illuminant\ninvariant augmentations. However, the illuminant invariant assumption conflicts\nwith the nature of the color constancy task, which aims to estimate the\nilluminant given a raw image. Therefore, we construct effective contrastive\npairs for learning better illuminant-dependent features via a novel raw-domain\ncolor augmentation. On the NUS-8 dataset, our method provides $17.5\\%$ relative\nimprovements over a strong baseline, reaching state-of-the-art performance\nwithout increasing model complexity. Furthermore, our method achieves\ncompetitive performance on the Gehler dataset with $3\\times$ fewer parameters\ncompared to top-ranking deep learning methods. More importantly, we show that\nour model is more robust to different scenes under close proximity of\nilluminants, significantly reducing $28.7\\%$ worst-case error in data-sparse\nregions.",
    "descriptor": "\nComments: Accepted at CVPR 2021. Our code is available at this https URL\n",
    "authors": [
      "Yi-Chen Lo",
      "Chia-Che Chang",
      "Hsuan-Chao Chiu",
      "Yu-Hao Huang",
      "Chia-Ping Chen",
      "Yu-Lin Chang",
      "Kevin Jou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04989"
  },
  {
    "id": "arXiv:2106.04990",
    "title": "It Takes Two to Tango: Mixup for Deep Metric Learning",
    "abstract": "Metric learning involves learning a discriminative representation such that\nembeddings of similar classes are encouraged to be close, while embeddings of\ndissimilar classes are pushed far apart. State-of-the-art methods focus mostly\non sophisticated loss functions or mining strategies. On the one hand, metric\nlearning losses consider two or more examples at a time. On the other hand,\nmodern data augmentation methods for classification consider two or more\nexamples at a time. The combination of the two ideas is under-studied.\nIn this work, we aim to bridge this gap and improve representations using\nmixup, which is a powerful data augmentation approach interpolating two or more\nexamples and corresponding target labels at a time. This task is challenging\nbecause, unlike classification, the loss functions used in metric learning are\nnot additive over examples, so the idea of interpolating target labels is not\nstraightforward. To the best of our knowledge, we are the first to investigate\nmixing examples and target labels for deep metric learning. We develop a\ngeneralized formulation that encompasses existing metric learning loss\nfunctions and modify it to accommodate for mixup, introducing Metric Mix, or\nMetrix. We show that mixing inputs, intermediate representations or embeddings\nalong with target labels significantly improves representations and outperforms\nstate-of-the-art metric learning methods on four benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Shashanka Venkataramanan",
      "Bill Psomas",
      "Yannis Avrithis",
      "Ewa Kijak",
      "Laurent Amsaleg",
      "Konstantinos Karantzalos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04990"
  },
  {
    "id": "arXiv:2106.04993",
    "title": "Initialization Matters: Regularizing Manifold-informed Initialization  for Neural Recommendation Systems",
    "abstract": "Proper initialization is crucial to the optimization and the generalization\nof neural networks. However, most existing neural recommendation systems\ninitialize the user and item embeddings randomly. In this work, we propose a\nnew initialization scheme for user and item embeddings called Laplacian\nEigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).\nLEPORID endows the embeddings with information regarding multi-scale\nneighborhood structures on the data manifold and performs adaptive\nregularization to compensate for high embedding variance on the tail of the\ndata distribution. Exploiting matrix sparsity, LEPORID embeddings can be\ncomputed efficiently. We evaluate LEPORID in a wide range of neural\nrecommendation models. In contrast to the recent surprising finding that the\nsimple K-nearest-neighbor (KNN) method often outperforms neural recommendation\nsystems, we show that existing neural systems initialized with LEPORID often\nperform on par or better than KNN. To maximize the effects of the\ninitialization, we propose the Dual-Loss Residual Recommendation (DLR2)\nnetwork, which, when initialized with LEPORID, substantially outperforms both\ntraditional and state-of-the-art neural recommender systems.",
    "descriptor": "",
    "authors": [
      "Yinan Zhang",
      "Boyang Li",
      "Yong Liu",
      "Hao Wang",
      "Chunyan Miao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04993"
  },
  {
    "id": "arXiv:2106.04995",
    "title": "Crosslingual Embeddings are Essential in UNMT for Distant Languages: An  English to IndoAryan Case Study",
    "abstract": "Recent advances in Unsupervised Neural Machine Translation (UNMT) have\nminimized the gap between supervised and unsupervised machine translation\nperformance for closely related language pairs. However, the situation is very\ndifferent for distant language pairs. Lack of lexical overlap and low syntactic\nsimilarities such as between English and Indo-Aryan languages leads to poor\ntranslation quality in existing UNMT systems. In this paper, we show that\ninitializing the embedding layer of UNMT models with cross-lingual embeddings\nshows significant improvements in BLEU score over existing approaches with\nembeddings randomly initialized. Further, static embeddings (freezing the\nembedding layer weights) lead to better gains compared to updating the\nembedding layer weights during training (non-static). We experimented using\nMasked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT\napproaches for three distant language pairs. The proposed cross-lingual\nembedding initialization yields BLEU score improvement of as much as ten times\nover the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our\nanalysis shows the importance of cross-lingual embedding, comparisons between\napproaches, and the scope of improvements in these systems.",
    "descriptor": "",
    "authors": [
      "Tamali Banerjee",
      "Rudra Murthy V",
      "Pushpak Bhattacharyya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04995"
  },
  {
    "id": "arXiv:2106.04996",
    "title": "Salient Positions based Attention Network for Image Classification",
    "abstract": "The self-attention mechanism has attracted wide publicity for its most\nimportant advantage of modeling long dependency, and its variations in computer\nvision tasks, the non-local block tries to model the global dependency of the\ninput feature maps. Gathering global contextual information will inevitably\nneed a tremendous amount of memory and computing resources, which has been\nextensively studied in the past several years. However, there is a further\nproblem with the self-attention scheme: is all information gathered from the\nglobal scope helpful for the contextual modelling? To our knowledge, few\nstudies have focused on the problem. Aimed at both questions this paper\nproposes the salient positions-based attention scheme SPANet, which is inspired\nby some interesting observations on the attention maps and affinity matrices\ngenerated in self-attention scheme. We believe these observations are\nbeneficial for better understanding of the self-attention. SPANet uses the\nsalient positions selection algorithm to select only a limited amount of\nsalient points to attend in the attention map computing. This approach will not\nonly spare a lot of memory and computing resources, but also try to distill the\npositive information from the transformation of the input feature maps. In the\nimplementation, considering the feature maps with channel high dimensions,\nwhich are completely different from the general visual image, we take the\nsquared power of the feature maps along the channel dimension as the saliency\nmetric of the positions. In general, different from the non-local block method,\nSPANet models the contextual information using only the selected positions\ninstead of all, along the channel dimension instead of space dimension. Our\nsource code is available at https://github.com/likyoo/SPANet.",
    "descriptor": "",
    "authors": [
      "Sheng Fang",
      "Kaiyu Li",
      "Zhe Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04996"
  },
  {
    "id": "arXiv:2106.05001",
    "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning  with Non-IID Data",
    "abstract": "A central challenge in training classification models in the real-world\nfederated system is learning with non-IID data. To cope with this, most of the\nexisting works involve enforcing regularization in local optimization or\nimproving the model aggregation scheme at the server. Other works also share\npublic datasets or synthesized samples to supplement the training of\nunder-represented classes or introduce a certain level of personalization.\nThough effective, they lack a deep understanding of how the data heterogeneity\naffects each layer of a deep classification model. In this paper, we bridge\nthis gap by performing an experimental analysis of the representations learned\nby different layers. Our observations are surprising: (1) there exists a\ngreater bias in the classifier than other layers, and (2) the classification\nperformance can be significantly improved by post-calibrating the classifier\nafter federated training. Motivated by the above findings, we propose a novel\nand simple algorithm called Classifier Calibration with Virtual Representations\n(CCVR), which adjusts the classifier using virtual representations sampled from\nan approximated gaussian mixture model. Experimental results demonstrate that\nCCVR achieves state-of-the-art performance on popular federated learning\nbenchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple\nyet effective method can shed some light on the future research of federated\nlearning with non-IID data.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Mi Luo",
      "Fei Chen",
      "Dapeng Hu",
      "Yifan Zhang",
      "Jian Liang",
      "Jiashi Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05001"
  },
  {
    "id": "arXiv:2106.05003",
    "title": "Dual-Modality Vehicle Anomaly Detection via Bilateral Trajectory Tracing",
    "abstract": "Traffic anomaly detection has played a crucial role in Intelligent\nTransportation System (ITS). The main challenges of this task lie in the highly\ndiversified anomaly scenes and variational lighting conditions. Although much\nwork has managed to identify the anomaly in homogenous weather and scene, few\nresolved to cope with complex ones. In this paper, we proposed a dual-modality\nmodularized methodology for the robust detection of abnormal vehicles. We\nintroduced an integrated anomaly detection framework comprising the following\nmodules: background modeling, vehicle tracking with detection, mask\nconstruction, Region of Interest (ROI) backtracking, and dual-modality tracing.\nConcretely, we employed background modeling to filter the motion information\nand left the static information for later vehicle detection. For the vehicle\ndetection and tracking module, we adopted YOLOv5 and multi-scale tracking to\nlocalize the anomalies. Besides, we utilized the frame difference and tracking\nresults to identify the road and obtain the mask. In addition, we introduced\nmultiple similarity estimation metrics to refine the anomaly period via\nbacktracking. Finally, we proposed a dual-modality bilateral tracing module to\nrefine the time further. The experiments conducted on the Track 4 testset of\nthe NVIDIA 2021 AI City Challenge yielded a result of 0.9302 F1-Score and\n3.4039 root mean square error (RMSE), indicating the effectiveness of our\nframework.",
    "descriptor": "\nComments: 9 pages, 5 figures, accepted to CVPRW 2021\n",
    "authors": [
      "Jingyuan Chen",
      "Guanchen Ding",
      "Yuchen Yang",
      "Wenwei Han",
      "Kangmin Xu",
      "Tianyi Gao",
      "Zhe Zhang",
      "Wanping Ouyang",
      "Hao Cai",
      "Zhenzhong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05003"
  },
  {
    "id": "arXiv:2106.05005",
    "title": "Relaxation Deferred Correction Methods and their Applications to  Residual Distribution Schemes",
    "abstract": "In [1] is proposed a simplified DeC method, that, when combined with the\nresidual distribution (RD) framework, allows to construct a high order,\nexplicit FE scheme with continuous approximation avoiding the inversion of the\nmass matrix for hyperbolic problems. In this paper, we close some open gaps in\nthe context of deferred correction (DeC) and their application within the RD\nframework. First, we demonstrate the connection between the DeC schemes and the\nRK methods. With this knowledge, DeC can be rewritten as a convex combination\nof explicit Euler steps, showing the connection to the strong stability\npreserving (SSP) framework. Then, we can apply the relaxation approach\nintroduced in [2] and construct entropy conservative/dissipative DeC (RDeC)\nmethods, using the entropy correction function proposed in [3].\n[1] R. Abgrall. High order schemes for hyperbolic problems using globally\ncontinuous approximation and avoiding mass matrices. Journal of Scientific\nComputing, 73(2):461--494, Dec 2017.\n[2] D. Ketcheson. Relaxation Runge--Kutta methods: Conservation and stability\nfor inner-product norms. SIAM Journal on Numerical Analysis, 57(6):2850--2870,\n2019.\n[3] R. Abgrall. A general framework to construct schemes satisfying\nadditional conservation relations. application to entropy conservative and\nentropy dissipative schemes. Journal of Computational Physics, 372:640--666,\n2018.",
    "descriptor": "",
    "authors": [
      "R\u00e9mi Abgrall",
      "Elise Le M\u00e9l\u00e9do",
      "Philipp \u00d6ffner",
      "Davide Torlo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05005"
  },
  {
    "id": "arXiv:2106.05006",
    "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack  Exchange Data",
    "abstract": "Most available semantic parsing datasets, comprising of pairs of natural\nutterances and logical forms, were collected solely for the purpose of training\nand evaluation of natural language understanding systems. As a result, they do\nnot contain any of the richness and variety of natural-occurring utterances,\nwhere humans ask about data they need or are curious about. In this work, we\nrelease SEDE, a dataset with 12,023 pairs of utterances and SQL queries\ncollected from real usage on the Stack Exchange website. We show that these\npairs contain a variety of real-world challenges which were rarely reflected so\nfar in any other semantic parsing dataset, propose an evaluation metric based\non comparison of partial query clauses that is more suitable for real-world\nqueries, and conduct experiments with strong baselines, showing a large gap\nbetween the performance on SEDE compared to other common datasets.",
    "descriptor": "\nComments: NLP4Prog 2021\n",
    "authors": [
      "Moshe Hazoom",
      "Vibhor Malik",
      "Ben Bogin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05006"
  },
  {
    "id": "arXiv:2106.05007",
    "title": "LTrack: Stealthy Tracking of Mobile Phones in LTE",
    "abstract": "We introduce LTrack, a new tracking attack on LTE that allows an attacker to\nstealthily extract user devices' (UEs) permanent identifiers (IMSI) and\nlocations. To remain stealthy, the localization of UEs in LTrack is fully\npassive. It relies on our new uplink/downlink sniffer implementation, which\nrecords both times of arrivals of LTE messages and contents of Timing Advance\ncommands, based on which LTrack calculates UE locations. LTrack is the first to\nshow the feasibility of passive UE's localization through an implementation on\na software-defined radio.\nPassive localization attacks reveal information about a UE's locations but\ncan at best link these locations to a UE's pseudonymous temporary identifier\n(TMSI), making tracking in dense areas challenging. LTrack overcomes this\nchallenge by introducing and implementing a new type of IMSI Catcher named IMSI\nExtractor. It extracts a UE's permanent identifier (IMSI) and binds it to its\ncurrent TMSI. Instead of relying on fake base stations like existing IMSI\nCatchers (which are detectable due to their output power), IMSI Extractor\nrelies on our uplink/downlink sniffer enhanced with surgical message\novershadowing. This makes our IMSI Extractor the stealthiest IMSI Catcher to\ndate.\nWe evaluate LTrack through a series of experiments and show that in\nline-of-sight conditions, the attacker can estimate the location of a phone\nwith less than 6m error in 90 of the cases. In addition, we successfully test\nour IMSI Extractor against a set of 17 modern smartphones connected to an\nindustry-grade LTE testbed.",
    "descriptor": "",
    "authors": [
      "Martin Kotuliak",
      "Simon Erni",
      "Patrick Leu",
      "Marc Roeschlin",
      "Srdjan Capkun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05007"
  },
  {
    "id": "arXiv:2106.05009",
    "title": "Network insensitivity to parameter noise via adversarial regularization",
    "abstract": "Neuromorphic neural network processors, in the form of compute-in-memory\ncrossbar arrays of memristors, or in the form of subthreshold analog and\nmixed-signal ASICs, promise enormous advantages in compute density and energy\nefficiency for NN-based ML tasks. However, these technologies are prone to\ncomputational non-idealities, due to process variation and intrinsic device\nphysics. This degrades the task performance of networks deployed to the\nprocessor, by introducing parameter noise into the deployed model. While it is\npossible to calibrate each device, or train networks individually for each\nprocessor, these approaches are expensive and impractical for commercial\ndeployment. Alternative methods are therefore needed to train networks that are\ninherently robust against parameter variation, as a consequence of network\narchitecture and parameters. We present a new adversarial network optimisation\nalgorithm that attacks network parameters during training, and promotes robust\nperformance during inference in the face of parameter variation. Our approach\nintroduces a regularization term penalising the susceptibility of a network to\nweight perturbation. We compare against previous approaches for producing\nparameter insensitivity such as dropout, weight smoothing and introducing\nparameter noise during training. We show that our approach produces models that\nare more robust to targeted parameter variation, and equally robust to random\nparameter variation. Our approach finds minima in flatter locations in the\nweight-loss landscape compared with other approaches, highlighting that the\nnetworks found by our technique are less sensitive to parameter perturbation.\nOur work provides an approach to deploy neural network architectures to\ninference devices that suffer from computational non-idealities, with minimal\nloss of performance. ...",
    "descriptor": "",
    "authors": [
      "Julian B\u00fccher",
      "Fynn Faber",
      "Dylan R. Muir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05009"
  },
  {
    "id": "arXiv:2106.05012",
    "title": "Bayesian Bellman Operators",
    "abstract": "We introduce a novel perspective on Bayesian reinforcement learning (RL);\nwhereas existing approaches infer a posterior over the transition distribution\nor Q-function, we characterise the uncertainty in the Bellman operator. Our\nBayesian Bellman operator (BBO) framework is motivated by the insight that when\nbootstrapping is introduced, model-free approaches actually infer a posterior\nover Bellman operators, not value functions. In this paper, we use BBO to\nprovide a rigorous theoretical analysis of model-free Bayesian RL to better\nunderstand its relationshipto established frequentist RL methodologies. We\nprove that Bayesian solutions are consistent with frequentist RL solutions,\neven when approximate inference isused, and derive conditions for which\nconvergence properties hold. Empirically, we demonstrate that algorithms\nderived from the BBO framework have sophisticated deep exploration properties\nthat enable them to solve continuous control tasks at which state-of-the-art\nregularised actor-critic algorithms fail catastrophically",
    "descriptor": "",
    "authors": [
      "Matthew Fellows",
      "Kristian Hartikainen",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05012"
  },
  {
    "id": "arXiv:2106.05016",
    "title": "Satellite- and Cache-assisted UAV: A Joint Cache Placement, Resource  Allocation, and Trajectory Optimization for 6G Aerial Networks",
    "abstract": "This paper considers LEO satellite- and cache-assisted UAV communications for\ncontent delivery in terrestrial networks, which shows great potential for\nnext-generation systems to provide ubiquitous connectivity and high capacity.\nSpecifically, caching is provided by the UAV to reduce backhaul congestion, and\nthe LEO satellite supports the UAV's backhaul link. In this context, we aim to\nmaximize the minimum achievable throughput per ground user (GU) by jointly\noptimizing cache placement, the UAV's resource allocation, and trajectory while\ncache capacity and flight time are limited. The formulated problem is\nchallenging to solve directly due to its non-convexity and combinatorial\nnature. To find a solution, the problem is decomposed into three sub-problems:\n(1) cache placement optimization with fixed UAV resources and trajectory,\nfollowed by (2) the UAV resources optimization with fixed cache placement\nvector and trajectory, and finally, (3) we optimize the UAV trajectory with\nfixed cache placement and UAV resources. Based on the solutions of\nsub-problems, an efficient alternating algorithm is proposed utilizing the\nblock coordinate descent (BCD) and successive convex approximation (SCA)\nmethods. Simulation results show that the max-min throughput and total\nachievable throughput enhancement can be achieved by applying our proposed\nalgorithm instead of other benchmark schemes.",
    "descriptor": "\nComments: 14 pages, 6 figures\n",
    "authors": [
      "Dinh-Hieu Tran",
      "Symeon Chatzinotas",
      "Bj\u00f6rn Ottersten"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.05016"
  },
  {
    "id": "arXiv:2106.05018",
    "title": "RLupus: Cooperation through emergent communication in The Werewolf  social deduction game",
    "abstract": "This paper focuses on the emergence of communication to support cooperation\nin environments modeled as social deduction games (SDG), that are games where\nplayers communicate freely to deduce each others' hidden intentions. We first\nstate the problem by giving a general formalization of SDG and a possible\nsolution framework based on reinforcement learning. Next, we focus on a\nspecific SDG, known as The Werewolf, and study if and how various forms of\ncommunication influence the outcome of the game. Experimental results show that\nintroducing a communication signal greatly increases the winning chances of a\nclass of players. We also study the effect of the signal's length and range on\nthe overall performance showing a non-linear relationship.",
    "descriptor": "",
    "authors": [
      "Nicolo' Brandizzi",
      "Davide Grossi",
      "Luca Iocchi"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.05018"
  },
  {
    "id": "arXiv:2106.05022",
    "title": "Adaptive Inference through Early-Exit Networks: Design, Challenges and  Directions",
    "abstract": "DNNs are becoming less and less over-parametrised due to recent advances in\nefficient model design, through careful hand-crafted or NAS-based methods.\nRelying on the fact that not all inputs require the same amount of computation\nto yield a confident prediction, adaptive inference is gaining attention as a\nprominent approach for pushing the limits of efficient deployment.\nParticularly, early-exit networks comprise an emerging direction for tailoring\nthe computation depth of each input sample at runtime, offering complementary\nperformance gains to other efficiency optimisations. In this paper, we\ndecompose the design methodology of early-exit networks to its key components\nand survey the recent advances in each one of them. We also position\nearly-exiting against other efficient inference solutions and provide our\ninsights on the current challenges and most promising future directions for\nresearch in the field.",
    "descriptor": "\nComments: Accepted at the 5th Annual International Workshop on Embedded and Mobile Deep Learning (EMDL), 2021\n",
    "authors": [
      "Stefanos Laskaridis",
      "Alexandros Kouris",
      "Nicholas D. Lane"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05022"
  },
  {
    "id": "arXiv:2106.05027",
    "title": "Scientometric engineering: Revealing spatiotemporal citation dynamics  via open eprints",
    "abstract": "With the ever-increasing speed and volume of knowledge production and\nconsumption, scholarly communication systems have been rapidly transformed into\ndigitised and networked open ecosystems, where preprint servers have played a\npivotal role. However, evidence is scarce regarding how this paradigm shift has\naffected the dynamics of collective attention on scientific knowledge. Herein,\nwe address this issue by investigating the citation dynamics of more than 1.5\nmillion eprints on arXiv, the most prominent and oldest eprint archive. The\ndiscipline-average citation history curves are estimated by applying a\nnonlinear regression model to the long-term citation data. The revealed\nspatiotemporal characteristics, including the growth and obsolescence patterns,\nare shown to vary across disciplines, reflecting the different publication and\ncitation practices. The results are used to develop a spatiotemporally\nnormalised citation index, called the $\\gamma$-index, with an approximately\nnormal distribution. It can be used to compare the citational impact of\nindividual papers across disciplines and time periods, providing a less biased\nmeasure of research impact than those widely used in the literature and in\npractice. Further, a stochastic model for the observed spatiotemporal citation\ndynamics is derived, reproducing both the Lognormal Law for the cumulative\ncitation distribution and the time trajectory of average citations in a unified\nformalism.",
    "descriptor": "\nComments: 1+25 pages, 6 figures for main text; 1+24 pages, 14 figures for supplementary information\n",
    "authors": [
      "Keisuke Okamura"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.05027"
  },
  {
    "id": "arXiv:2106.05036",
    "title": "Towards Defending against Adversarial Examples via Attack-Invariant  Features",
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. Their\nadversarial robustness can be improved by exploiting adversarial examples.\nHowever, given the continuously evolving attacks, models trained on seen types\nof adversarial examples generally cannot generalize well to unseen types of\nadversarial examples. To solve this problem, in this paper, we propose to\nremove adversarial noise by learning generalizable invariant features across\nattacks which maintain semantic classification information. Specifically, we\nintroduce an adversarial feature learning mechanism to disentangle invariant\nfeatures from adversarial noise. A normalization term has been proposed in the\nencoded space of the attack-invariant features to address the bias issue\nbetween the seen and unseen types of attacks. Empirical evaluations demonstrate\nthat our method could provide better protection in comparison to previous\nstate-of-the-art approaches, especially against unseen types of attacks and\nadaptive attacks.",
    "descriptor": "",
    "authors": [
      "Dawei Zhou",
      "Tongliang Liu",
      "Bo Han",
      "Nannan Wang",
      "Chunlei Peng",
      "Xinbo Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05036"
  },
  {
    "id": "arXiv:2106.05037",
    "title": "A general approach for Explanations in terms of Middle Level Features",
    "abstract": "Nowadays, it is growing interest to make Machine Learning (ML) systems more\nunderstandable and trusting to general users. Thus, generating explanations for\nML system behaviours that are understandable to human beings is a central\nscientific and technological issue addressed by the rapidly growing research\narea of eXplainable Artificial Intelligence (XAI). Recently, it is becoming\nmore and more evident that new directions to create better explanations should\ntake into account what a good explanation is to a human user, and consequently,\ndevelop XAI solutions able to provide user-centred explanations. This paper\nsuggests taking advantage of developing an XAI general approach that allows\nproducing explanations for an ML system behaviour in terms of different and\nuser-selected input features, i.e., explanations composed of input properties\nthat the human user can select according to his background knowledge and goals.\nTo this end, we propose an XAI general approach which is able: 1) to construct\nexplanations in terms of input features that represent more salient and\nunderstandable input properties for a user, which we call here Middle-Level\ninput Features (MLFs), 2) to be applied to different types of MLFs. We\nexperimentally tested our approach on two different datasets and using three\ndifferent types of MLFs. The results seem encouraging.",
    "descriptor": "\nComments: This work has been submitted to the Artificial Intelligence Journal for possible publication\n",
    "authors": [
      "Andrea Apicella",
      "Francesco Isgr\u00f2",
      "Roberto Prevete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05037"
  },
  {
    "id": "arXiv:2106.05039",
    "title": "AdaptOver : Adaptive Overshadowing of LTE signals",
    "abstract": "We introduce AdaptOver, a new LTE signal overshadowing attack that allows an\nadversary to reactively and adaptively overshadow any downlink message between\nthe network and the user equipment (UE). We demonstrate the impact of AdaptOver\nby using it to launch targeted Denial-of-Service (DoS) attacks on UEs. We\nimplement AdaptOver using a commercially available software-defined radio. Our\nexperiments demonstrate that our DoS attacks cause persistent connection loss\nlasting more than 12 hours for a wide range of smartphones. DoS attacks based\non AdaptOver are stealthier than attacks that relied on the use of fake base\nstations, and more persistent than existing overshadowing attacks, which caused\nconnection loss of only up to 9 minutes. Given that AdaptOver can reactively\novershadow any downlink message, its use is not limited to DoS attacks - it can\nbe used for a wide range of other attacks, e.g., to extract the IMSI from a UE\nin a stealthier manner than traditional IMSI catchers. We consider AdaptOver to\nbe an essential building block for many attacks against real-world LTE\nnetworks. In particular, any fake base station attack that makes use of spoofed\ndownlink messages can be ported to the presented attack method, causing a much\nmore reliable, persistent, and stealthy effect.",
    "descriptor": "",
    "authors": [
      "Simon Erni",
      "Patrick Leu",
      "Martin Kotuliak",
      "Marc R\u00f6schlin",
      "Srdjan \u010capkun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05039"
  },
  {
    "id": "arXiv:2106.05041",
    "title": "Fuzzy propositional configuration logics",
    "abstract": "In order to be able to characterize quantitative properties such as the\nuncertainty and reliability, we extended the weighted propositional\nconfiguration logic over a fuzzy setup. Fuzzy structures are suitable enough as\nthey rely on the idea that truth comes in degrees.",
    "descriptor": "",
    "authors": [
      "Paulina Paraponiari"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05041"
  },
  {
    "id": "arXiv:2106.05042",
    "title": "Polynomial magic! Hermite polynomials for private data generation",
    "abstract": "Kernel mean embedding is a useful tool to compare probability measures.\nDespite its usefulness, kernel mean embedding considers infinite-dimensional\nfeatures, which are challenging to handle in the context of differentially\nprivate data generation. A recent work proposes to approximate the kernel mean\nembedding of data distribution using finite-dimensional random features, where\nthe sensitivity of the features becomes analytically tractable. More\nimportantly, this approach significantly reduces the privacy cost, compared to\nother known privatization methods (e.g., DP-SGD), as the approximate kernel\nmean embedding of the data distribution is privatized only once and can then be\nrepeatedly used during training of a generator without incurring any further\nprivacy cost. However, the required number of random features is excessively\nhigh, often ten thousand to a hundred thousand, which worsens the sensitivity\nof the approximate kernel mean embedding. To improve the sensitivity, we\npropose to replace random features with Hermite polynomial features. Unlike the\nrandom features, the Hermite polynomial features are ordered, where the\nfeatures at the low orders contain more information on the distribution than\nthose at the high orders. Hence, a relatively low order of Hermite polynomial\nfeatures can more accurately approximate the mean embedding of the data\ndistribution compared to a significantly higher number of random features. As a\nresult, using the Hermite polynomial features, we significantly improve the\nprivacy-accuracy trade-off, reflected in the high quality and diversity of the\ngenerated data, when tested on several heterogeneous tabular datasets, as well\nas several image benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Mijung Park",
      "Margarita Vinaroz",
      "Mohammad-Amin Charusaie",
      "Frederik Harder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05042"
  },
  {
    "id": "arXiv:2106.05047",
    "title": "Salient Object Ranking with Position-Preserved Attention",
    "abstract": "Instance segmentation can detect where the objects are in an image, but hard\nto understand the relationship between them. We pay attention to a typical\nrelationship, relative saliency. A closely related task, salient object\ndetection, predicts a binary map highlighting a visually salient region while\nhard to distinguish multiple objects. Directly combining two tasks by\npost-processing also leads to poor performance. There is a lack of research on\nrelative saliency at present, limiting the practical applications such as\ncontent-aware image cropping, video summary, and image labeling.\nIn this paper, we study the Salient Object Ranking (SOR) task, which manages\nto assign a ranking order of each detected object according to its visual\nsaliency. We propose the first end-to-end framework of the SOR task and solve\nit in a multi-task learning fashion. The framework handles instance\nsegmentation and salient object ranking simultaneously. In this framework, the\nSOR branch is independent and flexible to cooperate with different detection\nmethods, so that easy to use as a plugin. We also introduce a\nPosition-Preserved Attention (PPA) module tailored for the SOR branch. It\nconsists of the position embedding stage and feature interaction stage.\nConsidering the importance of position in saliency comparison, we preserve\nabsolute coordinates of objects in ROI pooling operation and then fuse\npositional information with semantic features in the first stage. In the\nfeature interaction stage, we apply the attention mechanism to obtain\nproposals' contextualized representations to predict their relative ranking\norders. Extensive experiments have been conducted on the ASR dataset. Without\nbells and whistles, our proposed method outperforms the former state-of-the-art\nmethod significantly. The code will be released publicly available.",
    "descriptor": "",
    "authors": [
      "Hao Fang",
      "Daoxin Zhang",
      "Yi Zhang",
      "Minghao Chen",
      "Jiawei Li",
      "Yao Hu",
      "Deng Cai",
      "Xiaofei He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05047"
  },
  {
    "id": "arXiv:2106.05048",
    "title": "A class of boundary conditions for time-discrete Green-Naghdi equations  with bathymetry",
    "abstract": "This work is devoted to the structure of the time-discrete Green-Naghdi\nequations including bathymetry. We use the projection structure of the\nequations to characterize homogeneous and inhomogeneous boundary conditions for\nwhich the semi-discrete equations are well-posed. This structure allows us to\npropose efficient and robust numerical treatment of the boundary conditions\nthat ensures entropy stability of the scheme by construction. Numerical\nevidence is provided to illustrate that our approach is suitable for situations\nof practical interest that are not covered by existing theory.",
    "descriptor": "",
    "authors": [
      "Sebastian Noelle",
      "Martin Parisot",
      "Tabea Tscherpel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05048"
  },
  {
    "id": "arXiv:2106.05050",
    "title": "IChannels: Exploiting Current Management Mechanisms to Create Covert  Channels in Modern Processors",
    "abstract": "To operate efficiently across a wide range of workloads with varying power\nrequirements, a modern processor applies different current management\nmechanisms, which briefly throttle instruction execution while they adjust\nvoltage and frequency to accommodate for power-hungry instructions (PHIs) in\nthe instruction stream. Doing so 1) reduces the power consumption of non-PHI\ninstructions in typical workloads and 2) optimizes system voltage regulators'\ncost and area for the common use case while limiting current consumption when\nexecuting PHIs.\nHowever, these mechanisms may compromise a system's confidentiality\nguarantees. In particular, we observe that multilevel side-effects of\nthrottling mechanisms, due to PHI-related current management mechanisms, can be\ndetected by two different software contexts (i.e., sender and receiver) running\non 1) the same hardware thread, 2) co-located Simultaneous Multi-Threading\n(SMT) threads, and 3) different physical cores.\nBased on these new observations on current management mechanisms, we develop\na new set of covert channels, IChannels, and demonstrate them in real modern\nIntel processors (which span more than 70% of the entire client and server\nprocessor market). Our analysis shows that IChannels provides more than 24x the\nchannel capacity of state-of-the-art power management covert channels. We\npropose practical and effective mitigations to each covert channel in IChannels\nby leveraging the insights we gain through a rigorous characterization of real\nsystems.",
    "descriptor": "\nComments: To appear in ISCA 2021\n",
    "authors": [
      "Jawad Haj-Yahya",
      "Jeremie S. Kim",
      "A. Giray Yaglikci",
      "Ivan Puddu",
      "Lois Orosa",
      "Juan G\u00f3mez Luna",
      "Mohammed Alser",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05050"
  },
  {
    "id": "arXiv:2106.05052",
    "title": "Choice Logics and Their Computational Properties",
    "abstract": "Qualitative Choice Logic (QCL) and Conjunctive Choice Logic (CCL) are\nformalisms for preference handling, with especially QCL being well established\nin the field of AI. So far, analyses of these logics need to be done on a\ncase-by-case basis, albeit they share several common features. This calls for a\nmore general choice logic framework, with QCL and CCL as well as some of their\nderivatives being particular instantiations. We provide such a framework, which\nallows us, on the one hand, to easily define new choice logics and, on the\nother hand, to examine properties of different choice logics in a uniform\nsetting. In particular, we investigate strong equivalence, a core concept in\nnon-classical logics for understanding formula simplification, and\ncomputational complexity. Our analysis also yields new results for QCL and CCL.\nFor example, we show that the main reasoning task regarding preferred models is\n$\\Theta^p_2$-complete for QCL and CCL, while being $\\Delta^p_2$-complete for a\nnewly introduced choice logic.",
    "descriptor": "\nComments: This is an extended version of a paper of the same name to be published at IJCAI 2021\n",
    "authors": [
      "Michael Bernreiter",
      "Jan Maly",
      "Stefan Woltran"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05052"
  },
  {
    "id": "arXiv:2106.05057",
    "title": "Multiple Kernel Representation Learning on Networks",
    "abstract": "Learning representations of nodes in a low dimensional space is a crucial\ntask with numerous interesting applications in network analysis, including link\nprediction, node classification, and visualization. Two popular approaches for\nthis problem are matrix factorization and random walk-based models. In this\npaper, we aim to bring together the best of both worlds, towards learning node\nrepresentations. In particular, we propose a weighted matrix factorization\nmodel that encodes random walk-based information about nodes of the network.\nThe benefit of this novel formulation is that it enables us to utilize kernel\nfunctions without realizing the exact proximity matrix so that it enhances the\nexpressiveness of existing matrix decomposition methods with kernels and\nalleviates their computational complexities. We extend the approach with a\nmultiple kernel learning formulation that provides the flexibility of learning\nthe kernel as the linear combination of a dictionary of kernels in data-driven\nfashion. We perform an empirical evaluation on real-world networks, showing\nthat the proposed model outperforms baseline node embedding algorithms in\ndownstream machine learning tasks.",
    "descriptor": "\nComments: This manuscript is an extended version of the previous work entitled by \"Kernel Node Embeddings\". arXiv admin note: text overlap with arXiv:1909.03416\n",
    "authors": [
      "Abdulkadir Celikkanat",
      "Yanning Shen",
      "Fragkiskos D. Malliaros"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05057"
  },
  {
    "id": "arXiv:2106.05058",
    "title": "Towards Training Stronger Video Vision Transformers for  EPIC-KITCHENS-100 Action Recognition",
    "abstract": "With the recent surge in the research of vision transformers, they have\ndemonstrated remarkable potential for various challenging computer vision\napplications, such as image recognition, point cloud classification as well as\nvideo understanding. In this paper, we present empirical results for training a\nstronger video vision transformer on the EPIC-KITCHENS-100 Action Recognition\ndataset. Specifically, we explore training techniques for video vision\ntransformers, such as augmentations, resolutions as well as initialization,\netc. With our training recipe, a single ViViT model achieves the performance of\n47.4\\% on the validation set of EPIC-KITCHENS-100 dataset, outperforming what\nis reported in the original paper by 3.4%. We found that video transformers are\nespecially good at predicting the noun in the verb-noun action prediction task.\nThis makes the overall action prediction accuracy of video transformers notably\nhigher than convolutional ones. Surprisingly, even the best video transformers\nunderperform the convolutional networks on the verb prediction. Therefore, we\ncombine the video vision transformers and some of the convolutional video\nnetworks and present our solution to the EPIC-KITCHENS-100 Action Recognition\ncompetition.",
    "descriptor": "\nComments: CVPRW 2021, EPIC-KITCHENS-100 Competition Report\n",
    "authors": [
      "Ziyuan Huang",
      "Zhiwu Qing",
      "Xiang Wang",
      "Yutong Feng",
      "Shiwei Zhang",
      "Jianwen Jiang",
      "Zhurong Xia",
      "Mingqian Tang",
      "Nong Sang",
      "Marcelo H. Ang Jr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05058"
  },
  {
    "id": "arXiv:2106.05059",
    "title": "HEAP -- The autonomous walking excavator",
    "abstract": "The demand and the potential for automation in the construction sector is\nunmatched, particularly for increasing environmental sustainability, improving\nworker safety and reducing labor shortages. We have developed an autonomous\nwalking excavator - based one of the most versatile machines found on\nconstruction sites - as one way to begin fulfilling this potential. This\narticle describes the process of converting an off-the-shelf construction\nmachine into an autonomous robotic system. First we outline the necessary\nsensing equipment for full autonomy and the novel actuation of the legs, and\ncompare three different complementary actuation principles for the excavator's\narm. Second, we solve the state estimation problem for a general wheeled-legged\nrobot. Beside kinematic measurements, it includes GNSS-RTK, to absolutely\nreference the machine on a construction site. Third, we developed individual\ncontrollers for driving, chassis balancing and arm motions allowing for fully\nautonomous operation. Lastly, we highlight the machine's potential in four\ndifferent real-world applications, e.g. autonomous trench digging, autonomous\nassembly of dry stone walls, autonomous forestry work and semi-autonomous\nteleoperation. On top, we also share some development insights and possible\nfuture research directions.",
    "descriptor": "",
    "authors": [
      "Dominic Jud",
      "Simon Kerscher",
      "Martin Wermelinger",
      "Edo Jelavic",
      "Pascal Egli",
      "Philipp Leemann",
      "Gabriel Hottiger",
      "Marco Hutter"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05059"
  },
  {
    "id": "arXiv:2106.05061",
    "title": "Quickest change detection with unknown parameters: Constant complexity  and near optimality",
    "abstract": "We consider the quickest change detection problem where both the parameters\nof pre- and post- change distributions are unknown, which prevents the use of\nclassical simple hypothesis testing. Without additional assumptions, optimal\nsolutions are not tractable as they rely on some minimax and robust variant of\nthe objective. As a consequence, change points might be detected too late for\npractical applications (in economics, health care or maintenance for instance).\nAvailable constant complexity techniques typically solve a relaxed version of\nthe problem, deeply relying on very specific probability distributions and/or\nsome very precise additional knowledge. We consider a totally different\napproach that leverages the theoretical asymptotic properties of optimal\nsolutions to derive a new scalable approximate algorithm with near optimal\nperformance that runs~in~$\\mathcal{O}(1)$, adapted to even more complex\nMarkovian settings.",
    "descriptor": "",
    "authors": [
      "Firas Jarboui",
      "Viannet Perchet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05061"
  },
  {
    "id": "arXiv:2106.05065",
    "title": "Multi-layered Network Exploration via Random Walks: From Offline  Optimization to Online Learning",
    "abstract": "Multi-layered network exploration (MuLaNE) problem is an important problem\nabstracted from many applications. In MuLaNE, there are multiple network layers\nwhere each node has an importance weight and each layer is explored by a random\nwalk. The MuLaNE task is to allocate total random walk budget $B$ into each\nnetwork layer so that the total weights of the unique nodes visited by random\nwalks are maximized. We systematically study this problem from offline\noptimization to online learning. For the offline optimization setting where the\nnetwork structure and node weights are known, we provide greedy based\nconstant-ratio approximation algorithms for overlapping networks, and greedy or\ndynamic-programming based optimal solutions for non-overlapping networks. For\nthe online learning setting, neither the network structure nor the node weights\nare known initially. We adapt the combinatorial multi-armed bandit framework\nand design algorithms to learn random walk related parameters and node weights\nwhile optimizing the budget allocation in multiple rounds, and prove that they\nachieve logarithmic regret bounds. Finally, we conduct experiments on a\nreal-world social network dataset to validate our theoretical results.",
    "descriptor": "",
    "authors": [
      "Xutong Liu",
      "Jinhang Zuo",
      "Xiaowei Chen",
      "Wei Chen",
      "John C.S. Lui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05065"
  },
  {
    "id": "arXiv:2106.05066",
    "title": "Automating Induction by Reflection",
    "abstract": "Despite recent advances in automating theorem proving in full first-order\ntheories, inductive reasoning still poses a serious challenge to\nstate-of-the-art theorem provers. The reason for that is that in first-order\nlogic induction requires an infinite number of axioms, which is not a feasible\ninput to a computer-aided theorem prover requiring a finite input. Mathematical\npractice is to specify these infinite sets of axioms as axiom schemes.\nUnfortunately these schematic definitions cannot be formalized in first-order\nlogic, and therefore not supported as inputs for first-order theorem provers.\nIn this work we introduce a new method, inspired by the field of axiomatic\ntheories of truth, that allows to express schematic inductive definitions, in\nthe standard syntax of multi-sorted first-order logic. Further we test the\npractical feasibility of the method with state-of-the-art theorem provers,\ncomparing it to solvers' native techniques for handling induction.\nThis paper is an extended version of the LFMTP 21 submission with the same\ntitle.",
    "descriptor": "\nComments: LFMTP\n",
    "authors": [
      "Johannes Schoisswohl",
      "Laura Kovacs"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05066"
  },
  {
    "id": "arXiv:2106.05068",
    "title": "Offline Inverse Reinforcement Learning",
    "abstract": "The objective of offline RL is to learn optimal policies when a fixed\nexploratory demonstrations data-set is available and sampling additional\nobservations is impossible (typically if this operation is either costly or\nrises ethical questions). In order to solve this problem, off the shelf\napproaches require a properly defined cost function (or its evaluation on the\nprovided data-set), which are seldom available in practice. To circumvent this\nissue, a reasonable alternative is to query an expert for few optimal\ndemonstrations in addition to the exploratory data-set. The objective is then\nto learn an optimal policy w.r.t. the expert's latent cost function. Current\nsolutions either solve a behaviour cloning problem (which does not leverage the\nexploratory data) or a reinforced imitation learning problem (using a fixed\ncost function that discriminates available exploratory trajectories from expert\nones). Inspired by the success of IRL techniques in achieving state of the art\nimitation performances in online settings, we exploit GAN based data\naugmentation procedures to construct the first offline IRL algorithm. The\nobtained policies outperformed the aforementioned solutions on multiple OpenAI\ngym environments.",
    "descriptor": "",
    "authors": [
      "Firas Jarboui",
      "Vianney Perchet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05068"
  },
  {
    "id": "arXiv:2106.05074",
    "title": "Operationalizing Complex Causes:A Pragmatic View of Mediation",
    "abstract": "We examine the problem of causal response estimation for complex objects\n(e.g., text, images, genomics). In this setting, classical \\emph{atomic}\ninterventions are often not available (e.g., changes to characters, pixels, DNA\nbase-pairs). Instead, we only have access to indirect or \\emph{crude}\ninterventions (e.g., enrolling in a writing program, modifying a scene,\napplying a gene therapy). In this work, we formalize this problem and provide\nan initial solution. Given a collection of candidate mediators, we propose (a)\na two-step method for predicting the causal responses of crude interventions;\nand (b) a testing procedure to identify mediators of crude interventions. We\ndemonstrate, on a range of simulated and real-world-inspired examples, that our\napproach allows us to efficiently estimate the effect of crude interventions\nwith limited data from new treatment regimes.",
    "descriptor": "",
    "authors": [
      "Limor Gultchin",
      "David S. Watson",
      "Matt J. Kusner",
      "Ricardo Silva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05074"
  },
  {
    "id": "arXiv:2106.05075",
    "title": "On the Cover and Pombra Gaussian Feedback Capacity: Complete Sequential  Characterizations via a Sufficient Statistic",
    "abstract": "The main objective of this paper is to derive a new sequential\ncharacterization of the Cover and Pombra \\cite{cover-pombra1989}\ncharacterization of the $n-$finite block or transmission feedback information\n($n$-FTFI) capacity, which clarifies several issues of confusion and incorrect\ninterpretation of results in literature. The optimal channel input processes of\nthe new equivalent sequential characterizations are expressed as functionals of\na sufficient statistic and a Gaussian orthogonal innovations process. From the\nnew representations follows that the Cover and Pombra characterization of the\n$n-$FTFI capacity is expressed as a functional of two generalized matrix\ndifference Riccati equations (DRE) of filtering theory of Gaussian systems.\nThis contradicts results which are redundant in the literature, and illustrates\nthe fundamental complexity of the feedback capacity formula.",
    "descriptor": "\nComments: 6 pages. arXiv admin note: substantial text overlap with arXiv:2010.06226\n",
    "authors": [
      "Charalambos D. Charalambous",
      "Christos Kourtellaris",
      "Stelios Louka"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05075"
  },
  {
    "id": "arXiv:2106.05080",
    "title": "Learning Pseudo-Backdoors for Mixed Integer Programs",
    "abstract": "We propose a machine learning approach for quickly solving Mixed Integer\nPrograms (MIP) by learning to prioritize a set of decision variables, which we\ncall pseudo-backdoors, for branching that results in faster solution times.\nLearning-based approaches have seen success in the area of solving\ncombinatorial optimization problems by being able to flexibly leverage common\nstructures in a given distribution of problems. Our approach takes inspiration\nfrom the concept of strong backdoors, which corresponds to a small set of\nvariables such that only branching on these variables yields an optimal\nintegral solution and a proof of optimality. Our notion of pseudo-backdoors\ncorresponds to a small set of variables such that only branching on them leads\nto faster solve time (which can be solver dependent). A key advantage of\npseudo-backdoors over strong backdoors is that they are much amenable to\ndata-driven identification or prediction. Our proposed method learns to\nestimate the solver performance of a proposed pseudo-backdoor, using a labeled\ndataset collected on a set of training MIP instances. This model can then be\nused to identify high-quality pseudo-backdoors on new MIP instances from the\nsame distribution. We evaluate our method on the generalized independent set\nproblems and find that our approach can efficiently identify high-quality\npseudo-backdoors. In addition, we compare our learned approach against Gurobi,\na state-of-the-art MIP solver, demonstrating that our method can be used to\nimprove solver performance.",
    "descriptor": "\nComments: 2 pages, 1 page reference, SOCS extended abstract\n",
    "authors": [
      "Aaron Ferber",
      "Jialin Song",
      "Bistra Dilkina",
      "Yisong Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05080"
  },
  {
    "id": "arXiv:2106.05081",
    "title": "Global Context Enhanced Graph Neural Networks for Session-based  Recommendation",
    "abstract": "Session-based recommendation (SBR) is a challenging task, which aims at\nrecommending items based on anonymous behavior sequences. Almost all the\nexisting solutions for SBR model user preference only based on the current\nsession without exploiting the other sessions, which may contain both relevant\nand irrelevant item-transitions to the current session. This paper proposes a\nnovel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN)\nto exploit item transitions over all sessions in a more subtle manner for\nbetter inferring the user preference of the current session. Specifically,\nGCE-GNN learns two levels of item embeddings from session graph and global\ngraph, respectively: (i) Session graph, which is to learn the session-level\nitem embedding by modeling pairwise item-transitions within the current\nsession; and (ii) Global graph, which is to learn the global-level item\nembedding by modeling pairwise item-transitions over all sessions. In GCE-GNN,\nwe propose a novel global-level item representation learning layer, which\nemploys a session-aware attention mechanism to recursively incorporate the\nneighbors' embeddings of each node on the global graph. We also design a\nsession-level item representation learning layer, which employs a GNN on the\nsession graph to learn session-level item embeddings within the current\nsession. Moreover, GCE-GNN aggregates the learnt item representations in the\ntwo levels with a soft attention mechanism. Experiments on three benchmark\ndatasets demonstrate that GCE-GNN outperforms the state-of-the-art methods\nconsistently.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2011.10173\n",
    "authors": [
      "Ziyang Wang",
      "Wei Wei",
      "Gao Cong",
      "Xiao-Li Li",
      "Xian-Ling Mao",
      "Minghui Qiu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05081"
  },
  {
    "id": "arXiv:2106.05082",
    "title": "Agile wide-field imaging with selective high resolution",
    "abstract": "Wide-field and high-resolution (HR) imaging is essential for various\napplications such as aviation reconnaissance, topographic mapping and safety\nmonitoring. The existing techniques require a large-scale detector array to\ncapture HR images of the whole field, resulting in high complexity and heavy\ncost. In this work, we report an agile wide-field imaging framework with\nselective high resolution that requires only two detectors. It builds on the\nstatistical sparsity prior of natural scenes that the important targets locate\nonly at small regions of interests (ROI), instead of the whole field. Under\nthis assumption, we use a short-focal camera to image wide field with a certain\nlow resolution, and use a long-focal camera to acquire the HR images of ROI. To\nautomatically locate ROI in the wide field in real time, we propose an\nefficient deep-learning based multiscale registration method that is robust and\nblind to the large setting differences (focal, white balance, etc) between the\ntwo cameras. Using the registered location, the long-focal camera mounted on a\ngimbal enables real-time tracking of the ROI for continuous HR imaging. We\ndemonstrated the novel imaging framework by building a proof-of-concept setup\nwith only 1181 gram weight, and assembled it on an unmanned aerial vehicle for\nair-to-ground monitoring. Experiments show that the setup maintains\n120$^{\\circ}$ wide field-of-view (FOV) with selective 0.45$mrad$ instantaneous\nFOV.",
    "descriptor": "",
    "authors": [
      "Lintao Peng",
      "Liheng Bian",
      "Tiexin Liu",
      "Jun Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.05082"
  },
  {
    "id": "arXiv:2106.05086",
    "title": "Design and Implementation of 5G eHealth Systems, Technologies, Use Cases  and Future Challenges",
    "abstract": "Fifth generation (5G) aims to connect massive devices with even higher\nreliability, lower latency and even faster transmission speed, which are vital\nfor implementing the e-health systems. However, the current efforts on 5G\ne-health systems are still not enough to accomplish its full blueprint. In this\narticle, we first discuss the related technologies from physical layer, upper\nlayer and cross layer perspectives on designing the 5G e-health systems. We\nafterwards elaborate two use cases according to our implementations, i.e., 5G\ne-health systems for remote health and 5G e-health systems for Covid-19\npandemic containment. We finally envision the future research trends and\nchallenges of 5G e-health systems.",
    "descriptor": "",
    "authors": [
      "Di Zhang",
      "Joel J. P. C. Rodrigues",
      "Yunkai Zhai",
      "Takuro Sato"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05086"
  },
  {
    "id": "arXiv:2106.05087",
    "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion  Attacks in Deep RL",
    "abstract": "Evaluating the worst-case performance of a reinforcement learning (RL) agent\nunder the strongest/optimal adversarial perturbations on state observations\n(within some constraints) is crucial for understanding the robustness of RL\nagents. However, finding the optimal adversary is challenging, in terms of both\nwhether we can find the optimal attack and how efficiently we can find it.\nExisting works on adversarial RL either use heuristics-based methods that may\nnot find the strongest adversary, or directly train an RL-based adversary by\ntreating the agent as a part of the environment, which can find the optimal\nadversary but may become intractable in a large state space. In this paper, we\npropose a novel attacking algorithm which has an RL-based \"director\" searching\nfor the optimal policy perturbation, and an \"actor\" crafting state\nperturbations following the directions from the director (i.e. the actor\nexecutes targeted attacks). Our proposed algorithm, PA-AD, is theoretically\noptimal against an RL agent and significantly improves the efficiency compared\nwith prior RL-based works in environments with large or pixel state spaces.\nEmpirical results show that our proposed PA-AD universally outperforms\nstate-of-the-art attacking methods in a wide range of environments. Our method\ncan be easily applied to any RL algorithms to evaluate and improve their\nrobustness.",
    "descriptor": "",
    "authors": [
      "Yanchao Sun",
      "Ruijie Zheng",
      "Yongyuan Liang",
      "Furong Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05087"
  },
  {
    "id": "arXiv:2106.05091",
    "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via  Relabeling Experience and Unsupervised Pre-training",
    "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often\nbe difficult, involving meticulous design of reward functions that are\nsufficiently informative yet easy enough to provide. Human-in-the-loop RL\nmethods allow practitioners to instead interactively teach agents through\ntailored feedback; however, such approaches have been challenging to scale\nsince human feedback is very expensive. In this work, we aim to make this\nprocess more sample- and feedback-efficient. We present an off-policy,\ninteractive RL algorithm that capitalizes on the strengths of both feedback and\noff-policy learning. Specifically, we learn a reward model by actively querying\na teacher's preferences between two clips of behavior and use it to train an\nagent. To enable off-policy learning, we relabel all the agent's past\nexperience when its reward model changes. We additionally show that\npre-training our agents with unsupervised exploration substantially increases\nthe mileage of its queries. We demonstrate that our approach is capable of\nlearning tasks of higher complexity than previously considered by\nhuman-in-the-loop methods, including a variety of locomotion and robotic\nmanipulation skills. We also show that our method is able to utilize real-time\nhuman feedback to effectively prevent reward exploitation and learn new\nbehaviors that are difficult to specify with standard reward functions.",
    "descriptor": "\nComments: ICML 2021. First two authors contributed equally. Website: this https URL Code: this https URL\n",
    "authors": [
      "Kimin Lee",
      "Laura Smith",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05091"
  },
  {
    "id": "arXiv:2106.05093",
    "title": "Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation",
    "abstract": "We propose a new training objective named order-agnostic cross entropy (OaXE)\nfor fully non-autoregressive translation (NAT) models. OaXE improves the\nstandard cross-entropy loss to ameliorate the effect of word reordering, which\nis a common source of the critical multimodality problem in NAT. Concretely,\nOaXE removes the penalty for word order errors, and computes the cross entropy\nloss based on the best possible alignment between model predictions and target\ntokens. Since the log loss is very sensitive to invalid references, we leverage\ncross entropy initialization and loss truncation to ensure the model focuses on\na good part of the search space. Extensive experiments on major WMT benchmarks\nshow that OaXE substantially improves translation performance, setting new\nstate of the art for fully NAT models. Further analyses show that OaXE\nalleviates the multimodality problem by reducing token repetitions and\nincreasing prediction confidence. Our code, data, and trained models are\navailable at https://github.com/tencent-ailab/ICML21_OAXE.",
    "descriptor": "\nComments: ICML 2021 (Oral), Code at this https URL\n",
    "authors": [
      "Cunxiao Du",
      "Zhaopeng Tu",
      "Jing Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05093"
  },
  {
    "id": "arXiv:2106.05094",
    "title": "Semi-supervised lane detection with Deep Hough Transform",
    "abstract": "Current work on lane detection relies on large manually annotated datasets.\nWe reduce the dependency on annotations by leveraging massive cheaply available\nunlabelled data. We propose a novel loss function exploiting geometric\nknowledge of lanes in Hough space, where a lane can be identified as a local\nmaximum. By splitting lanes into separate channels, we can localize each lane\nvia simple global max-pooling. The location of the maximum encodes the layout\nof a lane, while the intensity indicates the the probability of a lane being\npresent. Maximizing the log-probability of the maximal bins helps neural\nnetworks find lanes without labels. On the CULane and TuSimple datasets, we\nshow that the proposed Hough Transform loss improves performance significantly\nby learning from large amounts of unlabelled images.",
    "descriptor": "\nComments: ICIP2021\n",
    "authors": [
      "Yancong Lin",
      "Silvia-Laura Pintea",
      "Jan van Gemert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05094"
  },
  {
    "id": "arXiv:2106.05095",
    "title": "ST++: Make Self-training Work Better for Semi-supervised Semantic  Segmentation",
    "abstract": "In this paper, we investigate if we could make the self-training -- a simple\nbut popular framework -- work better for semi-supervised segmentation. Since\nthe core issue in semi-supervised setting lies in effective and efficient\nutilization of unlabeled data, we notice that increasing the diversity and\nhardness of unlabeled data is crucial to performance improvement. Being aware\nof this fact, we propose to adopt the most plain self-training scheme coupled\nwith appropriate strong data augmentations on unlabeled data (namely ST) for\nthis task, which surprisingly outperforms previous methods under various\nsettings without any bells and whistles. Moreover, to alleviate the negative\nimpact of the wrongly pseudo labeled images, we further propose an advanced\nself-training framework (namely ST++), that performs selective re-training via\nselecting and prioritizing the more reliable unlabeled images. As a result, the\nproposed ST++ boosts the performance of semi-supervised model significantly and\nsurpasses existing methods by a large margin on the Pascal VOC 2012 and\nCityscapes benchmark. Overall, we hope this straightforward and simple\nframework will serve as a strong baseline or competitor for future works. Code\nis available at https://github.com/LiheYoung/ST-PlusPlus.",
    "descriptor": "\nComments: 16 pages, 10 figures\n",
    "authors": [
      "Lihe Yang",
      "Wei Zhuo",
      "Lei Qi",
      "Yinghuan Shi",
      "Yang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05095"
  },
  {
    "id": "arXiv:2106.05096",
    "title": "Multiple simultaneous solution representations in a population based  evolutionary algorithm",
    "abstract": "The representation used for solutions in optimization can have a significant\nimpact on the performance of the optimization method. Traditional population\nbased evolutionary methods have homogeneous populations where all solutions use\nthe same representation. If different representations are to be considered,\ndifferent runs are required to investigate the relative performance. In this\npaper, we illustrate the use of a population based evolutionary method, Fresa,\ninspired by the propagation of Strawberry plants, which allows for multiple\nrepresentations to co-exist in the population.\nFresa is implemented in the Julia language. Julia provides dynamic typing and\nmultiple dispatch. In multiple dispatch, the function invoked is determined,\ndynamically at run time, by the types of the arguments passed to it. This\nenables a generic implementation of key steps in the plant propagation\nalgorithm which allows for a heterogeneous population. The search procedure\nthen leads to a competition between representations automatically.\nA simple case study from the design of operating conditions for a batch\nreactor system is used to illustrate heterogeneous population based search.",
    "descriptor": "\nComments: 17 pages, 4 figures. Ancillary file written in org mode in Emacs contains all data for figures\n",
    "authors": [
      "Eric S. Fraga"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05096"
  },
  {
    "id": "arXiv:2106.05098",
    "title": "Computing Markov functions of Toeplitz matrices",
    "abstract": "We investigate the problem of approximating the matrix function $f(A)$ by\n$r(A)$, with $f$ a Markov function, $r$ a rational interpolant of $f$, and $A$\na symmetric Toeplitz matrix. In a first step, we obtain a new upper bound for\nthe relative interpolation error $1-r/f$ on the spectral interval of $A$. By\nminimizing this upper bound over all interpolation points, we obtain a new,\nsimple and sharp a priori bound for the relative interpolation error. We then\nconsider three different approaches of representing and computing the rational\ninterpolant $r$. Theoretical and numerical evidence is given that any of these\nmethods for a scalar argument allows to achieve high precision, even in the\npresence of finite precision arithmetic. We finally investigate the problem of\nefficiently evaluating $r(A)$, where it turns out that the relative error for a\nmatrix argument is only small if we use a partial fraction decomposition for\n$r$ following Antoulas and Mayo. An important role is played by a new stopping\ncriterion which ensures to automatically find the degree of $r$ leading to a\nsmall error, even in presence of finite precision arithmetic.",
    "descriptor": "",
    "authors": [
      "Bernhard Beckermann",
      "Joanna Bisch",
      "Robert Luce"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05098"
  },
  {
    "id": "arXiv:2106.05100",
    "title": "A Communication Layer for Integrated Sensors and Robotic ecology  Solutions to Ambient Intelligence",
    "abstract": "This paper presents a communication framework built to simplify the\nconstruction of robotic ecologies, i.e., networks of heterogeneous\ncomputational nodes interfaced with sensors, actuators, and mobile robots.\nBuilding integrated ambient intelligence (AmI) solutions out of such a wide\nrange of heterogeneous devices is a key requirement for a range of application\ndomains, such as home automation, logistic, security and Ambient Assisted\nLiving (AAL). This goal is challenging since these ecologies need to adapt to\nchanging environments and especially when they include tiny embedded devices\nwith limited computational resources. We discuss a number of requirements\ncharacterizing this type of systems and illustrate how they have been addressed\nin the design of the new communication framework. The most distinguishing\naspect of our frameworks is the transparency with which the same communication\nfeatures are offered across heterogeneous programming languages and operating\nsystems under a consistent API. Finally, we illustrate how the framework has\nbeen used to bind together and to support the operations of all the components\nof adaptive robotic ecologies in two real-world test-beds.",
    "descriptor": "",
    "authors": [
      "Giuseppe Amato",
      "Stefano Chessa",
      "Mauro Dragone",
      "Claudio Gennaro",
      "Claudio Vairo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05100"
  },
  {
    "id": "arXiv:2106.05102",
    "title": "Learning normal form autoencoders for data-driven discovery of  universal,parameter-dependent governing equations",
    "abstract": "Complex systems manifest a small number of instabilities and bifurcations\nthat are canonical in nature, resulting in universal pattern forming\ncharacteristics as a function of some parametric dependence. Such parametric\ninstabilities are mathematically characterized by their universal un-foldings,\nor normal form dynamics, whereby a parsimonious model can be used to represent\nthe dynamics. Although center manifold theory guarantees the existence of such\nlow-dimensional normal forms, finding them has remained a long standing\nchallenge. In this work, we introduce deep learning autoencoders to discover\ncoordinate transformations that capture the underlying parametric dependence of\na dynamical system in terms of its canonical normal form, allowing for a simple\nrepresentation of the parametric dependence and bifurcation structure. The\nautoencoder constrains the latent variable to adhere to a given normal form,\nthus allowing it to learn the appropriate coordinate transformation. We\ndemonstrate the method on a number of example problems, showing that it can\ncapture a diverse set of normal forms associated with Hopf, pitchfork,\ntranscritical and/or saddle node bifurcations. This method shows how normal\nforms can be leveraged as canonical and universal building blocks in deep\nlearning approaches for model discovery and reduced-order modeling.",
    "descriptor": "\nComments: 18 pages, 7 figures\n",
    "authors": [
      "Manu Kalia",
      "Steven L. Brunton",
      "Hil G.E. Meijer",
      "Christoph Brune",
      "J. Nathan Kutz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05102"
  },
  {
    "id": "arXiv:2106.05106",
    "title": "An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems  Using Extracted Ocular Features Based Neural Architecture",
    "abstract": "A user's eyes provide means for Human Computer Interaction (HCI) research as\nan important modal. The time to time scientific explorations of the eye has\nalready seen an upsurge of the benefits in HCI applications from gaze\nestimation to the measure of attentiveness of a user looking at a screen for a\ngiven time period. The eye tracking system as an assisting, interactive tool\ncan be incorporated by physically disabled individuals, fitted best for those\nwho have eyes as only a limited set of communication. The threefold objective\nof this paper is - 1. To introduce a neural network based architecture to\npredict users' gaze at 9 positions displayed in the 11.31{\\deg} visual range on\nthe screen, through a low resolution based system such as a webcam in real time\nby learning various aspects of eyes as an ocular feature set. 2.A collection of\ncoarsely supervised feature set obtained in real time which is also validated\nthrough the user case study presented in the paper for 21 individuals ( 17 men\nand 4 women ) from whom a 35k set of instances was derived with an accuracy\nscore of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability\nand underlying challenges of such systems. The experimental results verify the\nfeasibility and validity of the proposed eye gaze tracking model.",
    "descriptor": "",
    "authors": [
      "Atul Sahay",
      "Imon Mukherjee",
      "Kavi Arya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05106"
  },
  {
    "id": "arXiv:2106.05108",
    "title": "LB4OMP: A Dynamic Load Balancing Library for Multithreaded Applications",
    "abstract": "Exascale computing systems will exhibit high degrees of hierarchical\nparallelism, with thousands of computing nodes and hundreds of cores per node.\nEfficiently exploiting hierarchical parallelism is challenging due to load\nimbalance that arises at multiple levels.\nOpenMP is the most widely-used standard for expressing and exploiting the\never-increasing node-level parallelism.\nThe scheduling options in OpenMP are insufficient to address the load\nimbalance that arises during the execution of multithreaded applications.\nThe limited scheduling options in OpenMP hinder research on novel scheduling\ntechniques which require comparison with others from the literature.\nThis work introduces LB4OMP, an open-source dynamic load balancing library\nthat implements successful scheduling algorithms from the literature.\nLB4OMP is a research infrastructure designed to spur and support present and\nfuture scheduling research, for the benefit of multithreaded applications\nperformance.\nThrough an extensive performance analysis campaign, we assess the\neffectiveness and demystify the performance of all loop scheduling techniques\nin the library.\nWe show that, for numerous applications-systems pairs, the scheduling\ntechniques in LB4OMP outperform the scheduling options in OpenMP.\nNode-level load balancing using LB4OMP leads to reduced cross-node load\nimbalance and to improved MPI+OpenMP applications performance, which is\ncritical for Exascale computing.",
    "descriptor": "",
    "authors": [
      "Jonas H. M\u00fcller Kornd\u00f6rfer",
      "Ahmed Eleliemy",
      "Ali Mohammed",
      "Florina M. Ciorba"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.05108"
  },
  {
    "id": "arXiv:2106.05110",
    "title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
    "abstract": "Reinforcement learning (RL) has made a lot of advances for solving a single\nproblem in a given environment; but learning policies that generalize to unseen\nvariations of a problem remains challenging. To improve sample efficiency for\nlearning on such instances of a problem domain, we present Self-Paced Context\nEvaluation (SPaCE). Based on self-paced learning, \\spc automatically generates\n\\task curricula online with little computational overhead. To this end, SPaCE\nleverages information contained in state values during training to accelerate\nand improve training performance as well as generalization capabilities to new\ninstances from the same problem domain. Nevertheless, SPaCE is independent of\nthe problem domain at hand and can be applied on top of any RL agent with\nstate-value function approximation. We demonstrate SPaCE's ability to speed up\nlearning of different value-based RL agents on two environments, showing better\ngeneralization capabilities and up to 10x faster learning compared to naive\napproaches such as round robin or SPDRL, as the closest state-of-the-art\napproach.",
    "descriptor": "",
    "authors": [
      "Theresa Eimer",
      "Andr\u00e9 Biedenkapp",
      "Frank Hutter",
      "Marius Lindauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05110"
  },
  {
    "id": "arXiv:2106.05111",
    "title": "A Comparative Study on Neural Architectures and Training Methods for  Japanese Speech Recognition",
    "abstract": "End-to-end (E2E) modeling is advantageous for automatic speech recognition\n(ASR) especially for Japanese since word-based tokenization of Japanese is not\ntrivial, and E2E modeling is able to model character sequences directly. This\npaper focuses on the latest E2E modeling techniques, and investigates their\nperformances on character-based Japanese ASR by conducting comparative\nexperiments. The results are analyzed and discussed in order to understand the\nrelative advantages of long short-term memory (LSTM), and Conformer models in\ncombination with connectionist temporal classification, transducer, and\nattention-based loss functions. Furthermore, the paper investigates on\neffectivity of the recent training techniques such as data augmentation\n(SpecAugment), variational noise injection, and exponential moving average. The\nbest configuration found in the paper achieved the state-of-the-art character\nerror rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ)\neval1, eval2, and eval3 tasks, respectively. The system is also shown to be\ncomputationally efficient thanks to the efficiency of Conformer transducers.",
    "descriptor": "\nComments: to be published in INTERSPEECH2021\n",
    "authors": [
      "Shigeki Karita",
      "Yotaro Kubo",
      "Michiel Adriaan Unico Bacchiani",
      "Llion Jones"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05111"
  },
  {
    "id": "arXiv:2106.05113",
    "title": "More than meets the eye: Self-supervised depth reconstruction from brain  activity",
    "abstract": "In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages & depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.",
    "descriptor": "",
    "authors": [
      "Guy Gaziv",
      "Michal Irani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.05113"
  },
  {
    "id": "arXiv:2106.05115",
    "title": "Convergence of EBT method for a non-local model of cell proliferation  with discontinuous interaction kernel",
    "abstract": "We consider the EBT algorithm (a particle method) for the non-local equation\nwith discontinuous interaction kernel. The main difficulty lies in the low\nregularity of the kernel which is not Lipschitz continuous, thus preventing the\napplication of standard arguments. Therefore, we use the radial symmetry of the\nproblem instead and transform it using spherical coordinates. The resulting\nequation has a Lipschitz kernel with only one singularity at zero. We introduce\na new weighted flat norm and prove that the particle method converges in this\nnorm. We also comment on the two-dimensional case which requires the\napplication of the theory of measure spaces on general metric spaces and\npresent numerical simulations confirming the theoretical results. In a\ncompanion paper, we apply the Bayesian method to fit parameters to this model\nand study its theoretical properties.",
    "descriptor": "",
    "authors": [
      "Piotr Gwiazda",
      "B\u0142a\u017cej Miasojedow",
      "Jakub Skrzeczkowski",
      "Zuzanna Szyma\u0144ska"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.05115"
  },
  {
    "id": "arXiv:2106.05121",
    "title": "Grounding inductive biases in natural images:invariance stems from  variations in data",
    "abstract": "To perform well on unseen and potentially out-of-distribution samples, it is\ndesirable for machine learning models to have a predictable response with\nrespect to transformations affecting the factors of variation of the input.\nInvariance is commonly achieved through hand-engineered data augmentation, but\ndo standard data augmentations address transformations that explain variations\nin real data? While prior work has focused on synthetic data, we attempt here\nto characterize the factors of variation in a real dataset, ImageNet, and study\nthe invariance of both standard residual networks and the recently proposed\nvision transformer with respect to changes in these factors. We show standard\naugmentation relies on a precise combination of translation and scale, with\ntranslation recapturing most of the performance improvement -- despite the\n(approximate) translation invariance built in to convolutional architectures,\nsuch as residual networks. In fact, we found that scale and translation\ninvariance was similar across residual networks and vision transformer models\ndespite their markedly different inductive biases. We show the training data\nitself is the main source of invariance, and that data augmentation only\nfurther increases the learned invariances. Interestingly, the invariances\nbrought from the training process align with the ImageNet factors of variation\nwe found. Finally, we find that the main factors of variation in ImageNet\nmostly relate to appearance and are specific to each class.",
    "descriptor": "",
    "authors": [
      "Diane Bouchacourt",
      "Mark Ibrahim",
      "Ari S. Morcos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05121"
  },
  {
    "id": "arXiv:2106.05123",
    "title": "Pattern-defeating Quicksort",
    "abstract": "A new solution for the Dutch national flag problem is proposed, requiring no\nthree-way comparisons, which gives quicksort a proper worst-case runtime of\n$O(nk)$ for inputs with $k$ distinct elements. This is used together with other\nknown and novel techniques to construct a hybrid sort that is never\nsignificantly slower than regular quicksort while speeding up drastically for\nmany input distributions.",
    "descriptor": "\nComments: 20 pages, 10 figures\n",
    "authors": [
      "Orson R. L. Peters"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05123"
  },
  {
    "id": "arXiv:2106.05124",
    "title": "PCNet: A Structure Similarity Enhancement Method for Multispectral and  Multimodal Image Registration",
    "abstract": "Multispectral and multimodal image processing is important in the community\nof computer vision and computational photography. As the acquired multispectral\nand multimodal data are generally misaligned due to the alternation or movement\nof the image device, the image registration procedure is necessary. The\nregistration of multispectral or multimodal image is challenging due to the\nnon-linear intensity and gradient variation. To cope with this challenge, we\npropose the phase congruency network (PCNet), which is able to enhance the\nstructure similarity and alleviate the non-linear intensity and gradient\nvariation. The images can then be aligned using the similarity enhanced\nfeatures produced by the network. PCNet is constructed under the guidance of\nthe phase congruency prior. The network contains three trainable layers\naccompany with the modified learnable Gabor kernels according to the phase\ncongruency theory. Thanks to the prior knowledge, PCNet is extremely\nlight-weight and can be trained on quite a small amount of multispectral data.\nPCNet can be viewed to be fully convolutional and hence can take input of\narbitrary sizes. Once trained, PCNet is applicable on a variety of\nmultispectral and multimodal data such as RGB/NIR and flash/no-flash images\nwithout additional further tuning. Experimental results validate that PCNet\noutperforms current state-of-the-art registration algorithms, including the\ndeep-learning based ones that have the number of parameters hundreds times\ncompared to PCNet. Thanks to the similarity enhancement training, PCNet\noutperforms the original phase congruency algorithm with two-thirds less\nfeature channels.",
    "descriptor": "\nComments: 13 pages, 14 figures\n",
    "authors": [
      "Si-Yuan Cao",
      "Hui-Liang Shen",
      "Lun Luo",
      "Shu-Jie Chen",
      "Chunguang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05124"
  },
  {
    "id": "arXiv:2106.05126",
    "title": "Efficient Active Search for Combinatorial Optimization Problems",
    "abstract": "Recently numerous machine learning based methods for combinatorial\noptimization problems have been proposed that learn to construct solutions in a\nsequential decision process via reinforcement learning. While these methods can\nbe easily combined with search strategies like sampling and beam search, it is\nnot straightforward to integrate them into a high-level search procedure\noffering strong search guidance. Bello et al. (2016) propose active search,\nwhich adjusts the weights of a (trained) model with respect to a single\ninstance at test time using reinforcement learning. While active search is\nsimple to implement, it is not competitive with state-of-the-art methods\nbecause adjusting all model weights for each test instance is very time and\nmemory intensive. Instead of updating all model weights, we propose and\nevaluate three efficient active search strategies that only update a subset of\nparameters during the search. The proposed methods offer a simple way to\nsignificantly improve the search performance of a given model and outperform\nstate-of-the-art machine learning based methods on combinatorial problems, even\nsurpassing the well-known heuristic solver LKH3 on the capacitated vehicle\nrouting problem. Finally, we show that (efficient) active search enables\nlearned models to effectively solve instances that are much larger than those\nseen during training.",
    "descriptor": "",
    "authors": [
      "Andr\u00e9 Hottung",
      "Yeong-Dae Kwon",
      "Kevin Tierney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05126"
  },
  {
    "id": "arXiv:2106.05127",
    "title": "Deep Clustering based Fair Outlier Detection",
    "abstract": "In this paper, we focus on the fairness issues regarding unsupervised outlier\ndetection. Traditional algorithms, without a specific design for algorithmic\nfairness, could implicitly encode and propagate statistical bias in data and\nraise societal concerns. To correct such unfairness and deliver a fair set of\npotential outlier candidates, we propose Deep Clustering based Fair Outlier\nDetection (DCFOD) that learns a good representation for utility maximization\nwhile enforcing the learnable representation to be subgroup-invariant on the\nsensitive attribute. Considering the coupled and reciprocal nature between\nclustering and outlier detection, we leverage deep clustering to discover the\nintrinsic cluster structure and out-of-structure instances. Meanwhile, an\nadversarial training erases the sensitive pattern for instances for fairness\nadaptation. Technically, we propose an instance-level weighted representation\nlearning strategy to enhance the joint deep clustering and outlier detection,\nwhere the dynamic weight module re-emphasizes contributions of likely-inliers\nwhile mitigating the negative impact from outliers. Demonstrated by experiments\non eight datasets comparing to 17 outlier detection algorithms, our DCFOD\nmethod consistently achieves superior performance on both the outlier detection\nvalidity and two types of fairness notions in outlier detection.",
    "descriptor": "\nComments: To appear in KDD'2021\n",
    "authors": [
      "Hanyu Song",
      "Peizhao Li",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05127"
  },
  {
    "id": "arXiv:2106.05130",
    "title": "IoT Solution for Winter Survival of Indoor Plants",
    "abstract": "Not only does cold climate pose a problem for outdoor plants during winter in\nthe northern hemisphere, but for indoor plants as well: low sunlight, low\nhumidity, and simultaneous cold breezes from windows and heat from radiators\nall cause problems for indoor plants. People often treat their indoor plants\nlike mere decoration, which can often lead to health issues for the plant or\neven death of the plant, especially during winter. A plant monitoring system\nwas developed to solve this problem, collecting information on plants' indoor\nenvironmental conditions (light, humidity, and temperature) and providing this\ninformation in an accessible format for the user. Preliminary functional tests\nwere conducted in similar settings where the system would be used. In addition,\nthe concept was evaluated by interviewing an expert in the field of\nhorticulture.\nThe evaluation results indicate that this kind of system could prove useful;\nhowever, the tests indicated that the system requires further development to\nachieve more practical value and wider usage.",
    "descriptor": "\nComments: 10 pages, 2 tables, 4 figures\n",
    "authors": [
      "Md Saroar Jahan",
      "Jhuma kabir Mim",
      "Sampo Niittyviita",
      "Santeri Moberg",
      "Murad Ahmad",
      "Nijar Hossain"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/2106.05130"
  },
  {
    "id": "arXiv:2106.05131",
    "title": "Prior-Aware Distribution Estimation for Differential Privacy",
    "abstract": "Joint distribution estimation of a dataset under differential privacy is a\nfundamental problem for many privacy-focused applications, such as query\nanswering, machine learning tasks and synthetic data generation. In this work,\nwe examine the joint distribution estimation problem given two data points: 1)\ndifferentially private answers of a workload computed over private data and 2)\na prior empirical distribution from a public dataset. Our goal is to find a new\ndistribution such that estimating the workload using this distribution is as\naccurate as the differentially private answer, and the relative entropy, or KL\ndivergence, of this distribution is minimized with respect to the prior\ndistribution. We propose an approach based on iterative optimization for\nsolving this problem. An application of our solution won second place in the\nNIST 2020 Differential Privacy Temporal Map Challenge, Sprint 2.",
    "descriptor": "",
    "authors": [
      "Yuchao Tao",
      "Johes Bater",
      "Ashwin Machanavajjhala"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05131"
  },
  {
    "id": "arXiv:2106.05135",
    "title": "Regret and Cumulative Constraint Violation Analysis for Online Convex  Optimization with Long Term Constraints",
    "abstract": "This paper considers online convex optimization with long term constraints,\nwhere constraints can be violated in intermediate rounds, but need to be\nsatisfied in the long run. The cumulative constraint violation is used as the\nmetric to measure constraint violations, which excludes the situation that\nstrictly feasible constraints can compensate the effects of violated\nconstraints. A novel algorithm is first proposed and it achieves an\n$\\mathcal{O}(T^{\\max\\{c,1-c\\}})$ bound for static regret and an\n$\\mathcal{O}(T^{(1-c)/2})$ bound for cumulative constraint violation, where\n$c\\in(0,1)$ is a user-defined trade-off parameter, and thus has improved\nperformance compared with existing results. Both static regret and cumulative\nconstraint violation bounds are reduced to $\\mathcal{O}(\\log(T))$ when the loss\nfunctions are strongly convex, which also improves existing results. %In order\nto bound the regret with respect to any comparator sequence, In order to\nachieve the optimal regret with respect to any comparator sequence, another\nalgorithm is then proposed and it achieves the optimal\n$\\mathcal{O}(\\sqrt{T(1+P_T)})$ regret and an $\\mathcal{O}(\\sqrt{T})$ cumulative\nconstraint violation, where $P_T$ is the path-length of the comparator\nsequence. Finally, numerical simulations are provided to illustrate the\neffectiveness of the theoretical results.",
    "descriptor": "",
    "authors": [
      "Xinlei Yi",
      "Xiuxian Li",
      "Tao Yang",
      "Lihua Xie",
      "Tianyou Chai",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05135"
  },
  {
    "id": "arXiv:2106.05137",
    "title": "Bayesian Persuasion in Sequential Decision-Making",
    "abstract": "We study a dynamic model of Bayesian persuasion in sequential decision-making\nsettings. An informed principal observes an external parameter of the world and\nadvises an uninformed agent about actions to take over time. The agent takes\nactions in each time step based on the current state, the principal's\nadvice/signal, and beliefs about the external parameter. The action of the\nagent updates the state according to a stochastic process. The model arises\nnaturally in many applications, e.g., an app (the principal) can advice the\nuser (the agent) on possible choices between actions based on additional\nreal-time information the app has. We study the problem of designing a\nsignaling strategy from the principal's point of view. We show that the\nprincipal has an optimal strategy against a myopic agent, who only optimizes\ntheir rewards locally, and the optimal strategy can be computed in polynomial\ntime. In contrast, it is NP-hard to approximate an optimal policy against a\nfar-sighted agent. Further, we show that if the principal has the power to\nthreaten the agent by not providing future signals, then we can efficiently\ndesign a threat-based strategy. This strategy guarantees the principal's payoff\nas if playing against an agent who is far-sighted but myopic to future signals.",
    "descriptor": "",
    "authors": [
      "Jiarui Gan",
      "Rupak Majumdar",
      "Goran Radanovic",
      "Adish Singla"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.05137"
  },
  {
    "id": "arXiv:2106.05138",
    "title": "Second order scheme for self-similar solutions of a time-fractional  porous medium equation on the half-line",
    "abstract": "Nonlocality in time is an important property of systems in which their\npresent state depends on the history of the whole evolution. Combined with the\nnonlinearity of the process it poses serious difficulties in both analytical\nand numerical treatment.\nWe investigate a time-fractional porous medium equation that has proved to be\nimportant in many applications, notably in hydrology and material sciences. We\nshow that the solution of both free boundary Dirichlet, Neumann, and Robin\nproblems on the half-line satisfies a Volterra integral equation with\nnon-Lipschitz nonlinearity. Based on this result we prove existence,\nuniqueness, and construct a family of numerical methods that solve these\nequations outperforming the usual na\\\"ive finite difference approach. Moreover,\nwe prove the convergence of these methods and illustrate the theory with\nseveral numerical examples.",
    "descriptor": "",
    "authors": [
      "Hanna Okrasi\u0144ska-P\u0142ociniczak",
      "\u0141ukasz P\u0142ociniczak"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05138"
  },
  {
    "id": "arXiv:2106.05139",
    "title": "Pretrained Encoders are All You Need",
    "abstract": "Data-efficiency and generalization are key challenges in deep learning and\ndeep reinforcement learning as many models are trained on large-scale,\ndomain-specific, and expensive-to-label datasets. Self-supervised models\ntrained on large-scale uncurated datasets have shown successful transfer to\ndiverse settings. We investigate using pretrained image representations and\nspatio-temporal attention for state representation learning in Atari. We also\nexplore fine-tuning pretrained representations with self-supervised techniques,\ni.e., contrastive predictive coding, spatio-temporal contrastive learning, and\naugmentations. Our results show that pretrained representations are at par with\nstate-of-the-art self-supervised methods trained on domain-specific data.\nPretrained representations, thus, yield data and compute-efficient state\nrepresentations. https://github.com/PAL-ML/PEARL_v1",
    "descriptor": "",
    "authors": [
      "Mina Khan",
      "P Srivatsa",
      "Advait Rane",
      "Shriram Chenniappa",
      "Rishabh Anand",
      "Sherjil Ozair",
      "Pattie Maes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05139"
  },
  {
    "id": "arXiv:2106.05140",
    "title": "Linear Galerkin-Legendre spectral scheme for a degenerate nonlinear and  nonlocal parabolic equation arising in climatology",
    "abstract": "A special place in climatology is taken by the so-called conceptual climate\nmodels. These, relatively simple, sets of differential equations can\nsuccessfully describe single mechanisms of the climate. We focus on one family\nof such models based on the global energy balance. This gives rise to a\ndegenerate nonlocal parabolic nonlinear partial differential equation for the\nzonally averaged temperature. We construct a fully discrete numerical method\nthat has an optimal spectral accuracy in space and second order in time. Our\nscheme is based on Galerkin formulation of the Legendre basis expansion which\nis particularly convenient for this setting. By using extrapolation the\nnumerical scheme is linear even though the original equation is strongly\nnonlinear. We also test our theoretical result during various numerical\nsimulations that confirm the aforementioned accuracy of the scheme. All\nimplementations are coded in Julia programming language with the use of\nparallelization (multi-threading).",
    "descriptor": "",
    "authors": [
      "\u0141ukasz P\u0142ociniczak"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05140"
  },
  {
    "id": "arXiv:2106.05141",
    "title": "AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT",
    "abstract": "The success of Neural Machine Translation (NMT) largely depends on the\navailability of large bitext training corpora. Due to the lack of such large\ncorpora in low-resource language pairs, NMT systems often exhibit poor\nperformance. Extra relevant monolingual data often helps, but acquiring it\ncould be quite expensive, especially for low-resource languages. Moreover,\ndomain mismatch between bitext (train/test) and monolingual data might degrade\nthe performance. To alleviate such issues, we propose AUGVIC, a novel data\naugmentation framework for low-resource NMT which exploits the vicinal samples\nof the given bitext without using any extra monolingual data explicitly. It can\ndiversify the in-domain bitext data with finer level control. Through extensive\nexperiments on four low-resource language pairs comprising data from different\ndomains, we have shown that our method is comparable to the traditional\nback-translation that uses extra in-domain monolingual data. When we combine\nthe synthetic parallel data generated from AUGVIC with the ones from the extra\nmonolingual data, we achieve further improvements. We show that AUGVIC helps to\nattenuate the discrepancies between relevant and distant-domain monolingual\ndata in traditional back-translation. To understand the contributions of\ndifferent components of AUGVIC, we perform an in-depth framework analysis.",
    "descriptor": "\nComments: ACL-2021 accepted paper\n",
    "authors": [
      "Tasnim Mohiuddin",
      "M Saiful Bari",
      "Shafiq Joty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05141"
  },
  {
    "id": "arXiv:2106.05142",
    "title": "Neighborhood Contrastive Learning Applied to Online Patient Monitoring",
    "abstract": "Intensive care units (ICU) are increasingly looking towards machine learning\nfor methods to provide online monitoring of critically ill patients. In machine\nlearning, online monitoring is often formulated as a supervised learning\nproblem. Recently, contrastive learning approaches have demonstrated promising\nimprovements over competitive supervised benchmarks. These methods rely on\nwell-understood data augmentation techniques developed for image data which do\nnot apply to online monitoring. In this work, we overcome this limitation by\nsupplementing time-series data augmentation techniques with a novel contrastive\nlearning objective which we call neighborhood contrastive learning (NCL). Our\nobjective explicitly groups together contiguous time segments from each patient\nwhile maintaining state-specific information. Our experiments demonstrate a\nmarked improvement over existing work applying contrastive methods to medical\ntime-series.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Hugo Y\u00e8che",
      "Gideon Dresdner",
      "Francesco Locatello",
      "Matthias H\u00fcser",
      "Gunnar R\u00e4tsch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05142"
  },
  {
    "id": "arXiv:2106.05143",
    "title": "Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent  Resolution of Particle-Based Liquids",
    "abstract": "We present a novel up-resing technique for generating high-resolution liquids\nbased on scene flow estimation using deep neural networks. Our approach infers\nand synthesizes small- and large-scale details solely from a low-resolution\nparticle-based liquid simulation. The proposed network leverages neighborhood\ncontributions to encode inherent liquid properties throughout convolutions. We\nalso propose a particle-based approach to interpolate between liquids generated\nfrom varying simulation discretizations using a state-of-the-art bidirectional\noptical flow solver method for fluids in addition to a novel key-event\ntopological alignment constraint. In conjunction with the neighborhood\ncontributions, our loss formulation allows the inference model throughout\nepochs to reward important differences in regard to significant gaps in\nsimulation discretizations. Even when applied in an untested simulation setup,\nour approach is able to generate plausible high-resolution details. Using this\ninterpolation approach and the predicted displacements, our approach combines\nthe input liquid properties with the predicted motion to infer semi-Lagrangian\nadvection. We furthermore showcase how the proposed interpolation approach can\nfacilitate generating large simulation datasets with a subset of initial\ncondition parameters.",
    "descriptor": "\nComments: 14 pages, 18 figures, and 3 tables\n",
    "authors": [
      "Bruno Roy",
      "Pierre Poulin",
      "Eric Paquette"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05143"
  },
  {
    "id": "arXiv:2106.05144",
    "title": "Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting",
    "abstract": "In this paper, we explore and evaluate the use of ranking-based objective\nfunctions for learning simultaneously a word string and a word image encoder.\nWe consider retrieval frameworks in which the user expects a retrieval list\nranked according to a defined relevance score. In the context of a word\nspotting problem, the relevance score has been set according to the string edit\ndistance from the query string. We experimentally demonstrate the competitive\nperformance of the proposed model on query-by-string word spotting for both,\nhandwritten and real scene word images. We also provide the results for\nquery-by-example word spotting, although it is not the main focus of this work.",
    "descriptor": "\nComments: Accepted at ICDAR 2021\n",
    "authors": [
      "Pau Riba",
      "Adri\u00e0 Molina",
      "Lluis Gomez",
      "Oriol Ramos-Terrades",
      "Josep Llad\u00f3s"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05144"
  },
  {
    "id": "arXiv:2106.05145",
    "title": "Relative Clustering Coefficient",
    "abstract": "In this paper, we relatively extend the definition of global clustering\ncoefficient to another clustering, which we call it relative clustering\ncoefficient. The idea of this definition is to ignore the edges in the network\nthat the probability of having an edge is 0. Here, we also consider a model as\nan example that using relative clustering coefficient is better than global\nclustering coefficient for comparing networks and also checking the properties\nof the networks.",
    "descriptor": "",
    "authors": [
      "Elena Farahbakhsh Touli",
      "Oscar Lindberg"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.05145"
  },
  {
    "id": "arXiv:2106.05147",
    "title": "Helping results assessment by adding explainable elements to the deep  relevance matching model",
    "abstract": "In this paper we address the explainability of web search engines. We propose\ntwo explainable elements on the search engine result page: a visualization of\nquery term weights and a visualization of passage relevance. The idea is that\nsearch engines that indicate to the user why results are retrieved are valued\nhigher by users and gain user trust. We deduce the query term weights from the\nterm gating network in the Deep Relevance Matching Model (DRMM) and visualize\nthem as a doughnut chart. In addition, we train a passage-level ranker with\nDRMM that selects the most relevant passage from each document and shows it as\nsnippet on the result page. Next to the snippet we show a document thumbnail\nwith this passage highlighted. We evaluate the proposed interface in an online\nuser study, asking users to judge the explainability and assessability of the\ninterface. We found that users judge our proposed interface significantly more\nexplainable and easier to assess than a regular search engine result page.\nHowever, they are not significantly better in selecting the relevant documents\nfrom the top-5. This indicates that the explainability of the search engine\nresult page leads to a better user experience. Thus, we conclude that the\nproposed explainable elements are promising as visualization for search engine\nusers.",
    "descriptor": "\nComments: Published in the 3rd International Workshop on ExplainAble Recommendation and Search (EARS 2020), July 30, 2020 Xi'an, China\n",
    "authors": [
      "Ioannis Chios",
      "Suzan Verberne"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05147"
  },
  {
    "id": "arXiv:2106.05150",
    "title": "Scaling Up Graph Neural Networks Via Graph Coarsening",
    "abstract": "Scalability of graph neural networks remains one of the major challenges in\ngraph machine learning. Since the representation of a node is computed by\nrecursively aggregating and transforming representation vectors of its\nneighboring nodes from previous layers, the receptive fields grow\nexponentially, which makes standard stochastic optimization techniques\nineffective. Various approaches have been proposed to alleviate this issue,\ne.g., sampling-based methods and techniques based on pre-computation of graph\nfilters.\nIn this paper, we take a different approach and propose to use graph\ncoarsening for scalable training of GNNs, which is generic, extremely simple\nand has sublinear memory and time costs during training. We present extensive\ntheoretical analysis on the effect of using coarsening operations and provides\nuseful guidance on the choice of coarsening methods. Interestingly, our\ntheoretical analysis shows that coarsening can also be considered as a type of\nregularization and may improve the generalization. Finally, empirical results\non real world datasets show that, simply applying off-the-shelf coarsening\nmethods, we can reduce the number of nodes by up to a factor of ten without\ncausing a noticeable downgrade in classification accuracy.",
    "descriptor": "\nComments: KDD 2021\n",
    "authors": [
      "Zengfeng Huang",
      "Shengzhong Zhang",
      "Chong Xi",
      "Tang Liu",
      "Min Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.05150"
  },
  {
    "id": "arXiv:2106.05158",
    "title": "Verification of asymptotic homogenization method developed for periodic  architected materials in strain gradient continuum",
    "abstract": "Strain gradient theory is an accurate model for capturing size effects and\nlocalization phenomena. However, the challenge in identification of\ncorresponding constitutive parameters limits the practical application of such\ntheory. We present and utilize asymptotic homogenization herein. All parameters\nin rank four, five, and six tensors are determined with the demonstrated\ncomputational approach. Examples for epoxy carbon fiber composite, metal matrix\ncomposite, and aluminum foam illustrate the effectiveness and versatility of\nthe proposed method. The influences of volume fraction of matrix, the stack of\nRVEs, and the varying unit cell lengths on the identified parameters are\ninvestigated. The homogenization computational tool is applicable to a wide\nclass materials and makes use of open-source codes in FEniCS.",
    "descriptor": "",
    "authors": [
      "Hua Yang",
      "Bilen Emek Abali",
      "Wolfgang H. M\u00fcller",
      "Salma Barboura",
      "Jia Li"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.05158"
  },
  {
    "id": "arXiv:2106.05160",
    "title": "Case Studies on using Natural Language Processing Techniques in Customer  Relationship Management Software",
    "abstract": "How can a text corpus stored in a customer relationship management (CRM)\ndatabase be used for data mining and segmentation? In order to answer this\nquestion we inherited the state of the art methods commonly used in natural\nlanguage processing (NLP) literature, such as word embeddings, and deep\nlearning literature, such as recurrent neural networks (RNN). We used the text\nnotes from a CRM system which are taken by customer representatives of an\ninternet ads consultancy agency between years 2009 and 2020. We trained word\nembeddings by using the corresponding text corpus and showed that these word\nembeddings can not only be used directly for data mining but also be used in\nRNN architectures, which are deep learning frameworks built with long short\nterm memory (LSTM) units, for more comprehensive segmentation objectives. The\nresults prove that structured text data in a CRM can be used to mine out very\nvaluable information and any CRM can be equipped with useful NLP features once\nthe problem definitions are properly built and the solution methods are\nconveniently implemented.",
    "descriptor": "\nComments: Pre-print version of the article titled \"Case Studies on using Natural Language Processing Techniques in Customer Relationship Management Software\"\n",
    "authors": [
      "\u015e\u00fckr\u00fc Ozan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05160"
  },
  {
    "id": "arXiv:2106.05161",
    "title": "Interactive Modelling of Volumetric Musculoskeletal Anatomy",
    "abstract": "We present a new approach for modelling musculoskeletal anatomy. Unlike\nprevious methods, we do not model individual muscle shapes as geometric\nprimitives (polygonal meshes, NURBS etc.). Instead, we adopt a volumetric\nsegmentation approach where every point in our volume is assigned to a muscle,\nfat, or bone tissue. We provide an interactive modelling tool where the user\ncontrols the segmentation via muscle curves and we visualize the muscle shapes\nusing volumetric rendering. Muscle curves enable intuitive yet powerful control\nover the muscle shapes. This representation allows us to automatically handle\nintersections between different tissues (musclemuscle, muscle-bone, and\nmuscle-skin) during the modelling and automates computation of muscle fiber\nfields. We further introduce a novel algorithm for converting the volumetric\nmuscle representation into tetrahedral or surface geometry for use in\ndownstream tasks. Additionally, we introduce an interactive skeleton authoring\ntool that allows the users to create skeletal anatomy starting from only a skin\nmesh using a library of bone parts.",
    "descriptor": "\nComments: 13 pages, 20 figures, SIGGRAPH 2021\n",
    "authors": [
      "Rinat Abdrashitov",
      "Seungbae Bang",
      "David I.W. Levin",
      "Karan Singh",
      "Alec Jacobson"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.05161"
  },
  {
    "id": "arXiv:2106.05165",
    "title": "A Lyapunov-Based Methodology for Constrained Optimization with Bandit  Feedback",
    "abstract": "In a wide variety of applications including online advertising, contractual\nhiring, and wireless scheduling, the controller is constrained by a stringent\nbudget constraint on the available resources, which are consumed in a random\namount by each action, and a stochastic feasibility constraint that may impose\nimportant operational limitations on decision-making. In this work, we consider\na general model to address such problems, where each action returns a random\nreward, cost, and penalty from an unknown joint distribution, and the\ndecision-maker aims to maximize the total reward under a budget constraint $B$\non the total cost and a stochastic constraint on the time-average penalty. We\npropose a novel low-complexity algorithm based on Lyapunov optimization\nmethodology, named ${\\tt LyOn}$, and prove that it achieves $O(\\sqrt{B\\log B})$\nregret and $O(\\log B/B)$ constraint-violation. The low computational cost and\nsharp performance bounds of ${\\tt LyOn}$ suggest that Lyapunov-based algorithm\ndesign methodology can be effective in solving constrained bandit optimization\nproblems.",
    "descriptor": "",
    "authors": [
      "Semih Cayci",
      "Yilin Zheng",
      "Atilla Eryilmaz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05165"
  },
  {
    "id": "arXiv:2106.05166",
    "title": "Learning Multilingual Representation for Natural Language Understanding  with Enhanced Cross-Lingual Supervision",
    "abstract": "Recently, pre-training multilingual language models has shown great potential\nin learning multilingual representation, a crucial topic of natural language\nprocessing. Prior works generally use a single mixed attention (MA) module,\nfollowing TLM (Conneau and Lample, 2019), for attending to intra-lingual and\ncross-lingual contexts equivalently and simultaneously. In this paper, we\npropose a network named decomposed attention (DA) as a replacement of MA. The\nDA consists of an intra-lingual attention (IA) and a cross-lingual attention\n(CA), which model intralingual and cross-lingual supervisions respectively. In\naddition, we introduce a language-adaptive re-weighting strategy during\ntraining to further boost the model's performance. Experiments on various\ncross-lingual natural language understanding (NLU) tasks show that the proposed\narchitecture and learning strategy significantly improve the model's\ncross-lingual transferability.",
    "descriptor": "",
    "authors": [
      "Yinpeng Guo",
      "Liangyou Li",
      "Xin Jiang",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05166"
  },
  {
    "id": "arXiv:2106.05175",
    "title": "An Improved Bound for the Tree Conjecture in Network Creation Games",
    "abstract": "We study Nash equilibria in the network creation game of Fabrikant et\nal.[10]. In this game a vertex can buy an edge to another vertex for a cost of\n$\\alpha$, and the objective of each vertex is to minimize the sum of the costs\nof the edges it purchases plus the sum of the distances to every other vertex\nin the resultant network. A long-standing conjecture states that if $\\alpha\\ge\nn$ then every Nash equilibrium in the game is a spanning tree. We prove the\nconjecture holds for any $\\alpha>3n-3$.",
    "descriptor": "",
    "authors": [
      "Jack Dippel",
      "Adrian Vetta"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.05175"
  },
  {
    "id": "arXiv:2106.05177",
    "title": "Workflows Community Summit: Advancing the State-of-the-art of Scientific  Workflows Management Systems Research and Development",
    "abstract": "Scientific workflows are a cornerstone of modern scientific computing, and\nthey have underpinned some of the most significant discoveries of the last\ndecade. Many of these workflows have high computational, storage, and/or\ncommunication demands, and thus must execute on a wide range of large-scale\nplatforms, from large clouds to upcoming exascale HPC platforms. Workflows will\nplay a crucial role in the data-oriented and post-Moore's computing landscape\nas they democratize the application of cutting-edge research techniques,\ncomputationally intensive methods, and use of new computing platforms. As\nworkflows continue to be adopted by scientific projects and user communities,\nthey are becoming more complex. Workflows are increasingly composed of tasks\nthat perform computations such as short machine learning inference, multi-node\nsimulations, long-running machine learning model training, amongst others, and\nthus increasingly rely on heterogeneous architectures that include CPUs but\nalso GPUs and accelerators. The workflow management system (WMS) technology\nlandscape is currently segmented and presents significant barriers to entry due\nto the hundreds of seemingly comparable, yet incompatible, systems that exist.\nAnother fundamental problem is that there are conflicting theoretical bases and\nabstractions for a WMS. Systems that use the same underlying abstractions can\nlikely be translated between, which is not the case for systems that use\ndifferent abstractions. More information:\nhttps://workflowsri.org/summits/technical",
    "descriptor": "",
    "authors": [
      "Rafael Ferreira da Silva",
      "Henri Casanova",
      "Kyle Chard",
      "Tain\u00e3 Coleman",
      "Dan Laney",
      "Dong Ahn",
      "Shantenu Jha",
      "Dorran Howell",
      "Stian Soiland-Reys",
      "Ilkay Altintas",
      "Douglas Thain",
      "Rosa Filgueira",
      "Yadu Babuji",
      "Rosa M. Badia",
      "Bartosz Balis",
      "Silvina Caino-Lores",
      "Scott Callaghan",
      "Frederik Coppens",
      "Michael R. Crusoe",
      "Kaushik De",
      "Frank Di Natale",
      "Tu M. A. Do",
      "Bjoern Enders",
      "Thomas Fahringer",
      "Anne Fouilloux",
      "Grigori Fursin",
      "Alban Gaignard",
      "Alex Ganose",
      "Daniel Garijo",
      "Sandra Gesing",
      "Carole Goble",
      "Adil Hasan",
      "Sebastiaan Huber",
      "Daniel S. Katz",
      "Ulf Leser",
      "Douglas Lowe",
      "Bertram Ludaescher",
      "Ketan Maheshwari",
      "Maciej Malawski",
      "Rajiv Mayani",
      "Kshitij Mehta",
      "Andre Merzky",
      "Todd Munson",
      "Jonathan Ozik",
      "Lo\u00efc Pottier",
      "Sashko Ristov",
      "Mehdi Roozmeh",
      "Renan Souza",
      "Fr\u00e9d\u00e9ric Suter",
      "Benjamin Tovar",
      "Matteo Turilli"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.05177"
  },
  {
    "id": "arXiv:2106.05184",
    "title": "Tackling spam in the era of end-to-end encryption: A case study of  WhatsApp",
    "abstract": "WhatsApp is a popular messaging app used by over a billion users around the\nglobe. Due to this popularity, spam on WhatsApp is an important issue. Despite\nthis, the distribution of spam via WhatsApp remains understudied by\nresearchers, in part because of the end-to-end encryption offered by the\nplatform. This paper addresses this gap by studying spam on a dataset of 2.6\nmillion messages sent to 5,051 public WhatsApp groups in India over 300 days.\nFirst, we characterise spam content shared within public groups and find that\nnearly 1 in 10 messages is spam. We observe a wide selection of topics ranging\nfrom job ads to adult content, and find that spammers post both URLs and phone\nnumbers to promote material. Second, we inspect the nature of spammers\nthemselves. We find that spam is often disseminated by groups of phone numbers,\nand that spam messages are generally shared for longer duration than non-spam\nmessages. Finally, we devise content and activity based detection algorithms\nthat can counter spam.",
    "descriptor": "",
    "authors": [
      "Pushkal Agarwal",
      "Aravindh Raman",
      "Kiran Garimella",
      "Damilola Ibosiola",
      "Gareth Tyson",
      "Nishanth Sastry"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05184"
  },
  {
    "id": "arXiv:2106.05187",
    "title": "Geometry-Consistent Neural Shape Representation with Implicit  Displacement Fields",
    "abstract": "We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.",
    "descriptor": "\nComments: includes supplementary\n",
    "authors": [
      "Wang Yifan",
      "Lukas Rahmann",
      "Olga Sorkine-Hornung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05187"
  },
  {
    "id": "arXiv:2106.05188",
    "title": "Decentralised Approach for Multi Agent Path Finding",
    "abstract": "Multi Agent Path Finding (MAPF) requires identification of conflict free\npaths for agents which could be point-sized or with dimensions. In this paper,\nwe propose an approach for MAPF for spatially-extended agents. These find\napplication in real world problems like Convoy Movement Problem, Train\nScheduling etc. Our proposed approach, Decentralised Multi Agent Path Finding\n(DeMAPF), handles MAPF as a sequence of pathplanning and allocation problems\nwhich are solved by two sets of agents Travellers and Routers respectively,\nover multiple iterations. The approach being decentralised allows an agent to\nsolve the problem pertinent to itself, without being aware of other agents in\nthe same set. This allows the agents to be executed on independent machines,\nthereby leading to scalability to handle large sized problems. We prove, by\ncomparison with other distributed approaches, that the approach leads to a\nfaster convergence to a conflict-free solution, which may be suboptimal, with\nlesser memory requirement.",
    "descriptor": "",
    "authors": [
      "Shyni Thomas",
      "M. Narasimha Murty"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05188"
  },
  {
    "id": "arXiv:2106.05193",
    "title": "A Hybrid APM-CPGSO Approach for Constraint Satisfaction Problem Solving:  Application to Remote Sensing",
    "abstract": "Constraint satisfaction problem (CSP) has been actively used for modeling and\nsolving a wide range of complex real-world problems. However, it has been\nproven that developing efficient methods for solving CSP, especially for large\nproblems, is very difficult and challenging. Existing complete methods for\nproblem-solving are in most cases unsuitable. Therefore, proposing hybrid\nCSP-based methods for problem-solving has been of increasing interest in the\nlast decades. This paper aims at proposing a novel approach that combines\nincomplete and complete CSP methods for problem-solving. The proposed approach\ntakes advantage of the group search algorithm (GSO) and the constraint\npropagation (CP) methods to solve problems related to the remote sensing field.\nTo the best of our knowledge, this paper represents the first study that\nproposes a hybridization between an improved version of GSO and CP in the\nresolution of complex constraint-based problems. Experiments have been\nconducted for the resolution of object recognition problems in satellite\nimages. Results show good performances in terms of convergence and running time\nof the proposed CSP-based method compared to existing state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Zouhayra Ayadi",
      "Wadii Boulila",
      "Imed Riadh Farah"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05193"
  },
  {
    "id": "arXiv:2106.05203",
    "title": "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error  Feedback",
    "abstract": "Error feedback (EF), also known as error compensation, is an immensely\npopular convergence stabilization mechanism in the context of distributed\ntraining of supervised machine learning models enhanced by the use of\ncontractive communication compression mechanisms, such as Top-$k$. First\nproposed by Seide et al (2014) as a heuristic, EF resisted any theoretical\nunderstanding until recently [Stich et al., 2018, Alistarh et al., 2018].\nHowever, all existing analyses either i) apply to the single node setting only,\nii) rely on very strong and often unreasonable assumptions, such global\nboundedness of the gradients, or iterate-dependent assumptions that cannot be\nchecked a-priori and may not hold in practice, or iii) circumvent these issues\nvia the introduction of additional unbiased compressors, which increase the\ncommunication cost. In this work we fix all these deficiencies by proposing and\nanalyzing a new EF mechanism, which we call EF21, which consistently and\nsubstantially outperforms EF in practice. Our theoretical analysis relies on\nstandard assumptions only, works in the distributed heterogeneous data setting,\nand leads to better and more meaningful rates. In particular, we prove that\nEF21 enjoys a fast $O(1/T)$ convergence rate for smooth nonconvex problems,\nbeating the previous bound of $O(1/T^{2/3})$, which was shown a bounded\ngradients assumption. We further improve this to a fast linear rate for PL\nfunctions, which is the first linear convergence result for an EF-type method\nnot relying on unbiased compressors. Since EF has a large number of\napplications where it reigns supreme, we believe that our 2021 variant, EF21,\ncan a large impact on the practice of communication efficient distributed\nlearning.",
    "descriptor": "\nComments: 37 pages, 5 algorithms, 3 Theorems, 8 Lemmas, 15 Figures\n",
    "authors": [
      "Peter Richt\u00e1rik",
      "Igor Sokolov",
      "Ilyas Fatkhullin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05203"
  },
  {
    "id": "arXiv:2106.05209",
    "title": "Distilling Image Classifiers in Object Detectors",
    "abstract": "Knowledge distillation constitutes a simple yet effective way to improve the\nperformance of a compact student network by exploiting the knowledge of a more\npowerful teacher. Nevertheless, the knowledge distillation literature remains\nlimited to the scenario where the student and the teacher tackle the same task.\nHere, we investigate the problem of transferring knowledge not only across\narchitectures but also across tasks. To this end, we study the case of object\ndetection and, instead of following the standard detector-to-detector\ndistillation approach, introduce a classifier-to-detector knowledge transfer\nframework. In particular, we propose strategies to exploit the classification\nteacher to improve both the detector's recognition accuracy and localization\nperformance. Our experiments on several detectors with different backbones\ndemonstrate the effectiveness of our approach, allowing us to outperform the\nstate-of-the-art detector-to-detector distillation methods.",
    "descriptor": "",
    "authors": [
      "Shuxuan Guo",
      "Jose M. Alvarez",
      "Mathieu Salzmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05209"
  },
  {
    "id": "arXiv:2106.05210",
    "title": "Rethinking Space-Time Networks with Improved Memory Coverage for  Efficient Video Object Segmentation",
    "abstract": "This paper presents a simple yet effective approach to modeling space-time\ncorrespondences in the context of video object segmentation. Unlike most\nexisting approaches, we establish correspondences directly between frames\nwithout re-encoding the mask features for every object, leading to a highly\nefficient and robust framework. With the correspondences, every node in the\ncurrent query frame is inferred by aggregating features from the past in an\nassociative fashion. We cast the aggregation process as a voting problem and\nfind that the existing inner-product affinity leads to poor use of memory with\na small (fixed) subset of memory nodes dominating the votes, regardless of the\nquery. In light of this phenomenon, we propose using the negative squared\nEuclidean distance instead to compute the affinities. We validated that every\nmemory node now has a chance to contribute, and experimentally showed that such\ndiversified voting is beneficial to both memory efficiency and inference\naccuracy. The synergy of correspondence networks and diversified voting works\nexceedingly well, achieves new state-of-the-art results on both DAVIS and\nYouTubeVOS datasets while running significantly faster at 20+ FPS for multiple\nobjects without bells and whistles.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Ho Kei Cheng",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05210"
  },
  {
    "id": "arXiv:2106.05211",
    "title": "Near-Optimal Privacy-Utility Tradeoff in Genomic Studies Using Selective  SNP Hiding",
    "abstract": "Motivation: Researchers need a rich trove of genomic datasets that they can\nleverage to gain a better understanding of the genetic basis of the human\ngenome and identify associations between phenotypes and specific parts of DNA.\nHowever, sharing genomic datasets that include sensitive genetic or medical\ninformation of individuals can lead to serious privacy-related consequences if\ndata lands in the wrong hands. Restricting access to genomic datasets is one\nsolution, but this greatly reduces their usefulness for research purposes. To\nallow sharing of genomic datasets while addressing these privacy concerns,\nseveral studies propose privacy-preserving mechanisms for data sharing.\nDifferential privacy (DP) is one of such mechanisms that formalize rigorous\nmathematical foundations to provide privacy guarantees while sharing aggregated\nstatistical information about a dataset. However, it has been shown that the\noriginal privacy guarantees of DP-based solutions degrade when there are\ndependent tuples in the dataset, which is a common scenario for genomic\ndatasets (due to the existence of family members). Results: In this work, we\nintroduce a near-optimal mechanism to mitigate the vulnerabilities of the\ninference attacks on differentially private query results from genomic datasets\nincluding dependent tuples. We propose a utility-maximizing and\nprivacy-preserving approach for sharing statistics by hiding selective SNPs of\nthe family members as they participate in a genomic dataset. By evaluating our\nmechanism on a real-world genomic dataset, we empirically demonstrate that our\nproposed mechanism can achieve up to 40% better privacy than state-of-the-art\nDP-based solutions, while near-optimally minimizing the utility loss.",
    "descriptor": "\nComments: 9 pages, 9 figures\n",
    "authors": [
      "Nour Almadhoun Alserr",
      "Gulce Kale",
      "Onur Mutlu",
      "Oznur Tastan",
      "Erman Ayday"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05211"
  },
  {
    "id": "arXiv:2106.05215",
    "title": "A machine learning pipeline for aiding school identification from child  trafficking images",
    "abstract": "Child trafficking in a serious problem around the world. Every year there are\nmore than 4 million victims of child trafficking around the world, many of them\nfor the purposes of child sexual exploitation. In collaboration with UK Police\nand a non-profit focused on child abuse prevention, Global Emancipation\nNetwork, we developed a proof-of-concept machine learning pipeline to aid the\nidentification of children from intercepted images. In this work, we focus on\nimages that contain children wearing school uniforms to identify the school of\norigin. In the absence of a machine learning pipeline, this hugely time\nconsuming and labor intensive task is manually conducted by law enforcement\npersonnel. Thus, by automating aspects of the school identification process, we\nhope to significantly impact the speed of this portion of child identification.\nOur proposed pipeline consists of two machine learning models: i) to identify\nwhether an image of a child contains a school uniform in it, and ii)\nidentification of attributes of different school uniform items (such as\ncolor/texture of shirts, sweaters, blazers etc.). We describe the data\ncollection, labeling, model development and validation process, along with\nstrategies for efficient searching of schools using the model predictions.",
    "descriptor": "",
    "authors": [
      "Sumit Mukherjee",
      "Tina Sederholm",
      "Anthony C. Roman",
      "Ria Sankar",
      "Sherrie Caltagirone",
      "Juan Lavista Ferres"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05215"
  },
  {
    "id": "arXiv:2106.05218",
    "title": "Convergence of parallel overlapping domain decomposition methods for the  Helmholtz equation",
    "abstract": "We analyse parallel overlapping Schwarz domain decomposition methods for the\nHelmholtz equation, where the subdomain problems satisfy first-order absorbing\n(impedance) transmission conditions, and exchange of information between\nsubdomains is achieved using a partition of unity. We provide a novel analysis\nof this method at the PDE level (without discretization). First, we formulate\nthe method as a fixed point iteration, and show (in dimensions 1,2,3) that it\nis well-defined in a tensor product of appropriate local function spaces, each\nwith $L^2$ impedance boundary data. Given this, we then obtain a bound on the\nnorm of the fixed point operator in terms of the local norms of certain\nimpedance-to-impedance maps arising from local interactions between subdomains.\nThese bounds provide conditions under which (some power of) the fixed point\noperator is a contraction. In 2-d, for rectangular domains and strip-wise\ndomain decompositions (with each subdomain only overlapping its immediate\nneighbours), we present two techniques for verifying the assumptions on the\nimpedance-to-impedance maps which ensure power contractivity of the fixed point\noperator. The first is through semiclassical analysis, which gives rigorous\nestimates valid as the frequency tends to infinity. These results verify the\nrequired assumptions for sufficiently large overlap. For more realistic domain\ndecompositions, we directly compute the norms of the impedance-to-impedance\nmaps by solving certain canonical (local) eigenvalue problems. We give\nnumerical experiments that illustrate the theory. These also show that the\niterative method remains convergent and/or provides a good preconditioner in\ncases not covered by the theory, including for general domain decompositions,\nsuch as those obtained via automatic graph-partitioning software.",
    "descriptor": "",
    "authors": [
      "Shihua Gong",
      "Martin J.Gander",
      "Ivan G. Graham",
      "David Lafontaine",
      "Euan A. Spence"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05218"
  },
  {
    "id": "arXiv:2106.05220",
    "title": "Single-Server Private Linear Transformation: The Joint Privacy Case",
    "abstract": "This paper introduces the problem of Private Linear Transformation (PLT)\nwhich generalizes the problems of private information retrieval and private\nlinear computation. The PLT problem includes one or more remote server(s)\nstoring (identical copies of) $K$ messages and a user who wants to compute $L$\nindependent linear combinations of a $D$-subset of messages. The objective of\nthe user is to perform the computation by downloading minimum possible amount\nof information from the server(s), while protecting the identities of the $D$\nmessages required for the computation. In this work, we focus on the\nsingle-server setting of the PLT problem when the identities of the $D$\nmessages required for the computation must be protected jointly. We consider\ntwo different models, depending on whether the coefficient matrix of the\nrequired $L$ linear combinations generates a Maximum Distance Separable (MDS)\ncode. We prove that the capacity for both models is given by $L/(K-D+L)$, where\nthe capacity is defined as the supremum of all achievable download rates. Our\nconverse proofs are based on linear-algebraic and information-theoretic\narguments that establish connections between PLT schemes and linear codes. We\nalso present an achievability scheme for each of the models being considered.",
    "descriptor": "\nComments: 12 pages, 1 figure. arXiv admin note: text overlap with arXiv:2102.01665\n",
    "authors": [
      "Anoosheh Heidarzadeh",
      "Nahid Esmati",
      "Alex Sprintson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05220"
  },
  {
    "id": "arXiv:2106.05221",
    "title": "Multi-hop Graph Convolutional Network with High-order Chebyshev  Approximation for Text Reasoning",
    "abstract": "Graph convolutional network (GCN) has become popular in various natural\nlanguage processing (NLP) tasks with its superiority in long-term and\nnon-consecutive word interactions. However, existing single-hop graph reasoning\nin GCN may miss some important non-consecutive dependencies. In this study, we\ndefine the spectral graph convolutional network with the high-order dynamic\nChebyshev approximation (HDGCN), which augments the multi-hop graph reasoning\nby fusing messages aggregated from direct and long-term dependencies into one\nconvolutional layer. To alleviate the over-smoothing in high-order Chebyshev\napproximation, a multi-vote-based cross-attention (MVCAttn) with linear\ncomputation complexity is also proposed. The empirical results on four\ntransductive and inductive NLP tasks and the ablation study verify the efficacy\nof the proposed model. Our source code is available at\nhttps://github.com/MathIsAll/HDGCN-pytorch.",
    "descriptor": "",
    "authors": [
      "Shuoran Jiang",
      "Qingcai Chen",
      "Xin Liu",
      "Baotian Hu",
      "Lisai Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05221"
  },
  {
    "id": "arXiv:2106.05222",
    "title": "Single-Server Private Linear Transformation: The Individual Privacy Case",
    "abstract": "This paper considers the single-server Private Linear Transformation (PLT)\nproblem with individual privacy guarantees. In this problem, there is a user\nthat wishes to obtain $L$ independent linear combinations of a $D$-subset of\nmessages belonging to a dataset of $K$ messages stored on a single server. The\ngoal is to minimize the download cost while keeping the identity of each\nmessage required for the computation individually private. The individual\nprivacy requirement ensures that the identity of each individual message\nrequired for the computation is kept private. This is in contrast to the\nstricter notion of joint privacy that protects the entire set of identities of\nall messages used for the computation, including the correlations between these\nidentities. The notion of individual privacy captures a broad set of practical\napplications. For example, such notion is relevant when the dataset contains\ninformation about individuals, each of them requires privacy guarantees for\ntheir data access patterns. We focus on the setting in which the required\nlinear transformation is associated with a maximum distance separable (MDS)\nmatrix. In particular, we require that the matrix of coefficients pertaining to\nthe required linear combinations is the generator matrix of an MDS code. We\nestablish lower and upper bounds on the capacity of PLT with individual\nprivacy, where the capacity is defined as the supremum of all achievable\ndownload rates. We show that our bounds are tight under certain conditions.",
    "descriptor": "\nComments: 14 pages, 1 figure. arXiv admin note: substantial text overlap with arXiv:2102.01662\n",
    "authors": [
      "Anoosheh Heidarzadeh",
      "Nahid Esmati",
      "Alex Sprintson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05222"
  },
  {
    "id": "arXiv:2106.05223",
    "title": "Cross-Node Federated Graph Neural Network for Spatio-Temporal Data  Modeling",
    "abstract": "Vast amount of data generated from networks of sensors, wearables, and the\nInternet of Things (IoT) devices underscores the need for advanced modeling\ntechniques that leverage the spatio-temporal structure of decentralized data\ndue to the need for edge computation and licensing (data access) issues. While\nfederated learning (FL) has emerged as a framework for model training without\nrequiring direct data sharing and exchange, effectively modeling the complex\nspatio-temporal dependencies to improve forecasting capabilities still remains\nan open problem. On the other hand, state-of-the-art spatio-temporal\nforecasting models assume unfettered access to the data, neglecting constraints\non data sharing. To bridge this gap, we propose a federated spatio-temporal\nmodel -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly\nencodes the underlying graph structure using graph neural network (GNN)-based\narchitecture under the constraint of cross-node federated learning, which\nrequires that data in a network of nodes is generated locally on each node and\nremains decentralized. CNFGNN operates by disentangling the temporal dynamics\nmodeling on devices and spatial dynamics on the server, utilizing alternating\noptimization to reduce the communication cost, facilitating computations on the\nedge devices. Experiments on the traffic flow forecasting task show that CNFGNN\nachieves the best forecasting performance in both transductive and inductive\nlearning settings with no extra computation cost on edge devices, while\nincurring modest communication cost.",
    "descriptor": "\nComments: To be published in the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 21)\n",
    "authors": [
      "Chuizheng Meng",
      "Sirisha Rambhatla",
      "Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05223"
  },
  {
    "id": "arXiv:2106.05227",
    "title": "Understanding Privacy Attitudes and Concerns Towards Remote  Communications During the COVID-19 Pandemic",
    "abstract": "Since December 2019, the COVID-19 pandemic has caused people around the world\nto exercise social distancing, which has led to an abrupt rise in the adoption\nof remote communications for working, socializing, and learning from home. As\nremote communications will outlast the pandemic, it is crucial to protect\nusers' security and respect their privacy in this unprecedented setting, and\nthat requires a thorough understanding of their behaviors, attitudes, and\nconcerns toward various aspects of remote communications. To this end, we\nconducted an online study with 220 worldwide Prolific participants. We found\nthat privacy and security are among the most frequently mentioned factors\nimpacting participants' attitude and comfort level with conferencing tools and\nmeeting locations. Open-ended responses revealed that most participants lacked\nautonomy when choosing conferencing tools or using microphone/webcam in their\nremote meetings, which in several cases contradicted their personal privacy and\nsecurity preferences. Based on our findings, we distill several recommendations\non how employers, educators, and tool developers can inform and empower users\nto make privacy-protective decisions when engaging in remote communications.",
    "descriptor": "\nComments: To appear at the 17th Symposium on Usable Privacy and Security (SOUPS'21)\n",
    "authors": [
      "Pardis Emami-Naeini",
      "Tiona Francisco",
      "Tadayoshi Kohno",
      "Franziska Roesner"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.05227"
  },
  {
    "id": "arXiv:2106.05229",
    "title": "Intermittent Speech Recovery",
    "abstract": "A large number of Internet of Things (IoT) devices today are powered by\nbatteries, which are often expensive to maintain and may cause serious\nenvironmental pollution. To avoid these problems, researchers have begun to\nconsider the use of energy systems based on energy-harvesting units for such\ndevices. However, the power harvested from an ambient source is fundamentally\nsmall and unstable, resulting in frequent power failures during the operation\nof IoT applications involving, for example, intermittent speech signals and the\nstreaming of videos. This paper presents a deep-learning-based speech recovery\nsystem that reconstructs intermittent speech signals from self-powered IoT\ndevices. Our intermittent speech recovery system (ISR) consists of three\nstages: interpolation, recovery, and combination. The experimental results show\nthat our recovery system increases speech quality by up to 707.1%, while\nincreasing speech intelligibility by up to 92.1%. Most importantly, our ISR\nsystem also enhances the WER scores by up to 65.6%. To the best of our\nknowledge, this study is one of the first to reconstruct intermittent speech\nsignals from self-powered-sensing IoT devices. These promising results suggest\nthat even though self powered microphone devices function with weak energy\nsources, our ISR system can still maintain the performance of most\nspeech-signal-based applications.",
    "descriptor": "",
    "authors": [
      "Yu-Chen Lin",
      "Tsun-An Hsieh",
      "Kuo-Hsuan Hung",
      "Cheng Yu",
      "Harinath Garudadri",
      "Yu Tsao",
      "Tei-Wei Kuo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05229"
  },
  {
    "id": "arXiv:2106.05230",
    "title": "An ordinal CNN approach for the assessment of neurological damage in  Parkinson's disease patients",
    "abstract": "3D image scans are an assessment tool for neurological damage in Parkinson's\ndisease (PD) patients. This diagnosis process can be automatized to help\nmedical staff through Decision Support Systems (DSSs), and Convolutional Neural\nNetworks (CNNs) are good candidates, because they are effective when applied to\nspatial data. This paper proposes a 3D CNN ordinal model for assessing the\nlevel or neurological damage in PD patients. Given that CNNs need large\ndatasets to achieve acceptable performance, a data augmentation method is\nadapted to work with spatial data. We consider the Ordinal Graph-based\nOversampling via Shortest Paths (OGO-SP) method, which applies a gamma\nprobability distribution for inter-class data generation. A modification of\nOGO-SP is proposed, the OGO-SP-$\\beta$ algorithm, which applies the beta\ndistribution for generating synthetic samples in the inter-class region, a\nbetter suited distribution when compared to gamma. The evaluation of the\ndifferent methods is based on a novel 3D image dataset provided by the Hospital\nUniversitario 'Reina Sof\\'ia' (C\\'ordoba, Spain). We show how the ordinal\nmethodology improves the performance with respect to the nominal one, and how\nOGO-SP-$\\beta$ yields better performance than OGO-SP.",
    "descriptor": "\nComments: To be published in Expert Systems with Applications, 33 pages, 6 figures\n",
    "authors": [
      "Javier Barbero-G\u00f3mez",
      "Pedro-Antonio Guti\u00e9rrez",
      "V\u00edctor-Manuel Vargas",
      "Juan-Antonio Vallejo-Casas",
      "C\u00e9sar Herv\u00e1s-Mart\u00ednez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05230"
  },
  {
    "id": "arXiv:2106.05232",
    "title": "Realizing GANs via a Tunable Loss Function",
    "abstract": "We introduce a tunable GAN, called $\\alpha$-GAN, parameterized by $\\alpha \\in\n(0,\\infty]$, which interpolates between various $f$-GANs and Integral\nProbability Metric based GANs (under constrained discriminator set). We\nconstruct $\\alpha$-GAN using a supervised loss function, namely, $\\alpha$-loss,\nwhich is a tunable loss function capturing several canonical losses. We show\nthat $\\alpha$-GAN is intimately related to the Arimoto divergence, which was\nfirst proposed by \\\"{O}sterriecher (1996), and later studied by Liese and Vajda\n(2006). We posit that the holistic understanding that $\\alpha$-GAN introduces\nwill have practical benefits of addressing both the issues of vanishing\ngradients and mode collapse.",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Gowtham R. Kurri",
      "Tyler Sypherd",
      "Lalitha Sankar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05232"
  },
  {
    "id": "arXiv:2106.05233",
    "title": "Analysis of convolutional neural network image classifiers in a  hierarchical max-pooling model with additional local pooling",
    "abstract": "Image classification is considered, and a hierarchical max-pooling model with\nadditional local pooling is introduced. Here the additional local pooling\nenables the hierachical model to combine parts of the image which have a\nvariable relative distance towards each other. Various convolutional neural\nnetwork image classifiers are introduced and compared in view of their rate of\nconvergence. The finite sample size performance of the estimates is analyzed by\napplying them to simulated and real data.",
    "descriptor": "",
    "authors": [
      "Benjamin Walter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05233"
  },
  {
    "id": "arXiv:2106.05234",
    "title": "Do Transformers Really Perform Bad for Graph Representation?",
    "abstract": "The Transformer architecture has become a dominant choice in many domains,\nsuch as natural language processing and computer vision. Yet, it has not\nachieved competitive performance on popular leaderboards of graph-level\nprediction compared to mainstream GNN variants. Therefore, it remains a mystery\nhow Transformers could perform well for graph representation learning. In this\npaper, we solve this mystery by presenting Graphormer, which is built upon the\nstandard Transformer architecture, and could attain excellent results on a\nbroad range of graph representation learning tasks, especially on the recent\nOGB Large-Scale Challenge. Our key insight to utilizing Transformer in the\ngraph is the necessity of effectively encoding the structural information of a\ngraph into the model. To this end, we propose several simple yet effective\nstructural encoding methods to help Graphormer better model graph-structured\ndata. Besides, we mathematically characterize the expressive power of\nGraphormer and exhibit that with our ways of encoding the structural\ninformation of graphs, many popular GNN variants could be covered as the\nspecial cases of Graphormer.",
    "descriptor": "",
    "authors": [
      "Chengxuan Ying",
      "Tianle Cai",
      "Shengjie Luo",
      "Shuxin Zheng",
      "Guolin Ke",
      "Di He",
      "Yanming Shen",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05234"
  },
  {
    "id": "arXiv:2106.05236",
    "title": "Design and fabrication of solar powered remote controlled all terrain  sprayer and mower robot",
    "abstract": "Manual spraying of pesticides and herbicides to crops and weed inhibitors\nonto the field are quite laborious work to humans. Manual trimming of selected\nunwanted plants or harvested crops from the field is also difficult. Our\nproject proposes a multipurpose solar powered, flexible, Remote Controlled,\nsemi-automated spraying robot with 4 Degrees of Freedom (DoF) in spatial\nmovement, with an additional plant mowing equipment. The robot is designed to\nspray pesticide/insecticide directly onto individual lesions minimizing wastage\nor excess chemical spraying, hence making the system cost effective and also\nenvironment friendly. It is designed to cut down undesired plants selectively\nby remotely controlling the start and stop of the mowing system. Alternatively,\nit also serves the purpose of maintaining lawns and sports field made of grass.\nThe same system can be used for water spraying and mowing the grass to desired\nlevels, leading to proper maintenance of the field. The robot is designed to\nmove at 1.4m/s, with an effective spraying area of 0.98 sq. m. by the nozzle\nand an effective cutting area of 0.3 sq. m. by the mower, when stationary. The\nprototype has a battery back-up of 7.2hrs under minimum load conditions.",
    "descriptor": "\nComments: 104 Pages, 110 Figures, 13 Tables\n",
    "authors": [
      "Sri Tarun Ayyagari",
      "Sharan Kumar Kizhakke Erakkat",
      "Srikanth TS",
      "Manichandra Neerati"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05236"
  },
  {
    "id": "arXiv:2106.05237",
    "title": "Knowledge distillation: A good teacher is patient and consistent",
    "abstract": "There is a growing discrepancy in computer vision between large-scale models\nthat achieve state-of-the-art performance and models that are affordable in\npractical applications. In this paper we address this issue and significantly\nbridge the gap between these two types of models. Throughout our empirical\ninvestigation we do not aim to necessarily propose a new method, but strive to\nidentify a robust and effective recipe for making state-of-the-art large scale\nmodels affordable in practice. We demonstrate that, when performed correctly,\nknowledge distillation can be a powerful tool for reducing the size of large\nmodels without compromising their performance. In particular, we uncover that\nthere are certain implicit design choices, which may drastically affect the\neffectiveness of distillation. Our key contribution is the explicit\nidentification of these design choices, which were not previously articulated\nin the literature. We back up our findings by a comprehensive empirical study,\ndemonstrate compelling results on a wide range of vision datasets and, in\nparticular, obtain a state-of-the-art ResNet-50 model for ImageNet, which\nachieves 82.8\\% top-1 accuracy.",
    "descriptor": "\nComments: Lucas, Xiaohua, Am\\'elie, Larisa, and Alex contributed equally\n",
    "authors": [
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Am\u00e9lie Royer",
      "Larisa Markeeva",
      "Rohan Anil",
      "Alexander Kolesnikov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05237"
  },
  {
    "id": "arXiv:2106.05238",
    "title": "I Don't Need $\\mathbf{u}$: Identifiable Non-Linear ICA Without Side  Information",
    "abstract": "In this work we introduce a new approach for identifiable non-linear ICA\nmodels. Recently there has been a renaissance in identifiability results in\ndeep generative models, not least for non-linear ICA. These prior works,\nhowever, have assumed access to a sufficiently-informative auxiliary set of\nobservations, denoted $\\mathbf{u}$. We show here how identifiability can be\nobtained in the absence of this side-information, rendering possible\nfully-unsupervised identifiable non-linear ICA. While previous theoretical\nresults have established the impossibility of identifiable non-linear ICA in\nthe presence of infinitely-flexible universal function approximators, here we\nrely on the intrinsically-finite modelling capacity of any particular chosen\nparameterisation of a deep generative model. In particular, we focus on\ngenerative models which perform clustering in their latent space -- a model\nstructure which matches previous identifiable models, but with the learnt\nclustering providing a synthetic form of auxiliary information. We evaluate our\nproposals using VAEs, on synthetic and image datasets, and find that the\nlearned clusterings function effectively: deep generative models with latent\nclusterings are empirically identifiable, to the same degree as models which\nrely on side information.",
    "descriptor": "\nComments: 11 pages plus appendix\n",
    "authors": [
      "Matthew Willetts",
      "Brooks Paige"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05238"
  },
  {
    "id": "arXiv:2106.05239",
    "title": "XBNet : An Extremely Boosted Neural Network",
    "abstract": "Neural networks have proved to be very robust at processing unstructured data\nlike images, text, videos, and audio. However, it has been observed that their\nperformance is not up to the mark in tabular data; hence tree-based models are\npreferred in such scenarios. A popular model for tabular data is boosted trees,\na highly efficacious and extensively used machine learning method, and it also\nprovides good interpretability compared to neural networks. In this paper, we\ndescribe a novel architecture XBNet, which tries to combine tree-based models\nwith that of neural networks to create a robust architecture trained by using a\nnovel optimization technique, Boosted Gradient Descent for Tabular Data which\nincreases its interpretability and performance.",
    "descriptor": "",
    "authors": [
      "Tushar Sarkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05239"
  },
  {
    "id": "arXiv:2106.05244",
    "title": "Coopetition methodology for resource sharing in distributed OFDM-based  cognitive radio networks",
    "abstract": "In this paper, we present a distributed resource allocation mechanism in\ncognitive radio networks, based on a new coopeti-tion methodology, which\ncombines advantages of nodes competition and cooperation. We postulate that\nthis new method allows for fully distributed resource management between\ncognitive radio devices. The presented framework is generic, however, we\nconsider it for the application in OFDMA networks. Coopetition takes the best\nfrom cooperative and competitive problem formulation and provides the\nopportunity to control the balance between fairness and spectral efficiency\n(SE) of resource allocation. Simulation results confirm that coopetition allows\nfor efficient resource utilization, and may be used practically in wireless\ncognitive networks.",
    "descriptor": "\nComments: 12 pages, 13 figures\n",
    "authors": [
      "Marcin Parzy",
      "Hanna Bogucka"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.05244"
  },
  {
    "id": "arXiv:2106.05245",
    "title": "Local Algorithms for Finding Densely Connected Clusters",
    "abstract": "Local graph clustering is an important algorithmic technique for analysing\nmassive graphs, and has been widely applied in many research fields of data\nscience. While the objective of most (local) graph clustering algorithms is to\nfind a vertex set of low conductance, there has been a sequence of recent\nstudies that highlight the importance of the inter-connection between clusters\nwhen analysing real-world datasets. Following this line of research, in this\nwork we study local algorithms for finding a pair of vertex sets defined with\nrespect to their inter-connection and their relationship with the rest of the\ngraph. The key to our analysis is a new reduction technique that relates the\nstructure of multiple sets to a single vertex set in the reduced graph. Among\nmany potential applications, we show that our algorithms successfully recover\ndensely connected clusters in the Interstate Disputes Dataset and the US\nMigration Dataset.",
    "descriptor": "\nComments: This work is accepted at ICML'21 for a long presentation\n",
    "authors": [
      "Peter Macgregor",
      "He Sun"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05245"
  },
  {
    "id": "arXiv:2106.05249",
    "title": "What Would a Teacher Do? Predicting Future Talk Moves",
    "abstract": "Recent advances in natural language processing (NLP) have the ability to\ntransform how classroom learning takes place. Combined with the increasing\nintegration of technology in today's classrooms, NLP systems leveraging\nquestion answering and dialog processing techniques can serve as private tutors\nor participants in classroom discussions to increase student engagement and\nlearning. To progress towards this goal, we use the classroom discourse\nframework of academically productive talk (APT) to learn strategies that make\nfor the best learning experience. In this paper, we introduce a new task,\ncalled future talk move prediction (FTMP): it consists of predicting the next\ntalk move -- an utterance strategy from APT -- given a conversation history\nwith its corresponding talk moves. We further introduce a neural network model\nfor this task, which outperforms multiple baselines by a large margin. Finally,\nwe compare our model's performance on FTMP to human performance and show\nseveral similarities between the two.",
    "descriptor": "\nComments: 13 pages, 3 figures; To appear in Findings of ACL 2021\n",
    "authors": [
      "Ananya Ganesh",
      "Martha Palmer",
      "Katharina Kann"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05249"
  },
  {
    "id": "arXiv:2106.05250",
    "title": "The zero-rate threshold for adversarial bit-deletions is less than 1/2",
    "abstract": "We prove that there exists an absolute constant $\\delta>0$ such any binary\ncode $C\\subset\\{0,1\\}^N$ tolerating $(1/2-\\delta)N$ adversarial deletions must\nsatisfy $|C|\\le 2^{\\text{poly}\\log N}$ and thus have rate asymptotically\napproaching 0. This is the first constant fraction improvement over the trivial\nbound that codes tolerating $N/2$ adversarial deletions must have rate going to\n0 asymptotically. Equivalently, we show that there exists absolute constants\n$A$ and $\\delta>0$ such that any set $C\\subset\\{0,1\\}^N$ of $2^{\\log^A N}$\nbinary strings must contain two strings $c$ and $c'$ whose longest common\nsubsequence has length at least $(1/2+\\delta)N$. As an immediate corollary, we\nshow that $q$-ary codes tolerating a fraction $1-(1+2\\delta)/q$ of adversarial\ndeletions must also have rate approaching 0.\nOur techniques include string regularity arguments and a structural lemma\nthat classifies binary strings by their oscillation patterns. Leveraging these\ntools, we find in any large code two strings with similar oscillation patterns,\nwhich is exploited to find a long common subsequence.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Venkatesan Guruswami",
      "Xiaoyu He",
      "Ray Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.05250"
  },
  {
    "id": "arXiv:2106.05251",
    "title": "Bayesian Attention Belief Networks",
    "abstract": "Attention-based neural networks have achieved state-of-the-art results on a\nwide range of tasks. Most such models use deterministic attention while\nstochastic attention is less explored due to the optimization difficulties or\ncomplicated model design. This paper introduces Bayesian attention belief\nnetworks, which construct a decoder network by modeling unnormalized attention\nweights with a hierarchy of gamma distributions, and an encoder network by\nstacking Weibull distributions with a deterministic-upward-stochastic-downward\nstructure to approximate the posterior. The resulting auto-encoding networks\ncan be optimized in a differentiable way with a variational lower bound. It is\nsimple to convert any models with deterministic attention, including pretrained\nones, to the proposed Bayesian attention belief networks. On a variety of\nlanguage understanding tasks, we show that our method outperforms deterministic\nattention and state-of-the-art stochastic attention in accuracy, uncertainty\nestimation, generalization across domains, and robustness to adversarial\nattacks. We further demonstrate the general applicability of our method on\nneural machine translation and visual question answering, showing great\npotential of incorporating our method into various attention-related tasks.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Shujian Zhang",
      "Xinjie Fan",
      "Bo Chen",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05251"
  },
  {
    "id": "arXiv:2106.05256",
    "title": "URLTran: Improving Phishing URL Detection Using Transformers",
    "abstract": "Browsers often include security features to detect phishing web pages. In the\npast, some browsers evaluated an unknown URL for inclusion in a list of known\nphishing pages. However, as the number of URLs and known phishing pages\ncontinued to increase at a rapid pace, browsers started to include one or more\nmachine learning classifiers as part of their security services that aim to\nbetter protect end users from harm. While additional information could be used,\nbrowsers typically evaluate every unknown URL using some classifier in order to\nquickly detect these phishing pages. Early phishing detection used standard\nmachine learning classifiers, but recent research has instead proposed the use\nof deep learning models for the phishing URL detection task. Concurrently, text\nembedding research using transformers has led to state-of-the-art results in\nmany natural language processing tasks. In this work, we perform a\ncomprehensive analysis of transformer models on the phishing URL detection\ntask. We consider standard masked language model and additional domain-specific\npre-training tasks, and compare these models to fine-tuned BERT and RoBERTa\nmodels. Combining the insights from these experiments, we propose URLTran which\nuses transformers to significantly improve the performance of phishing URL\ndetection over a wide range of very low false positive rates (FPRs) compared to\nother deep learning-based methods. For example, URLTran yields a true positive\nrate (TPR) of 86.80% compared to 71.20% for the next best baseline at an FPR of\n0.01%, resulting in a relative improvement of over 21.9%. Further, we consider\nsome classical adversarial black-box phishing attacks such as those based on\nhomoglyphs and compound word splits to improve the robustness of URLTran. We\nconsider additional fine tuning with these adversarial samples and demonstrate\nthat URLTran can maintain low FPRs under these scenarios.",
    "descriptor": "",
    "authors": [
      "Pranav Maneriker",
      "Jack W. Stokes",
      "Edir Garcia Lazo",
      "Diana Carutasu",
      "Farid Tajaddodianfar",
      "Arun Gururajan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05256"
  },
  {
    "id": "arXiv:2106.05258",
    "title": "Generative Models as a Data Source for Multiview Representation Learning",
    "abstract": "Generative models are now capable of producing highly realistic images that\nlook nearly indistinguishable from the data on which they are trained. This\nraises the question: if we have good enough generative models, do we still need\ndatasets? We investigate this question in the setting of learning\ngeneral-purpose visual representations from a black-box generative model rather\nthan directly from data. Given an off-the-shelf image generator without any\naccess to its training data, we train representations from the samples output\nby this generator. We compare several representation learning methods that can\nbe applied to this setting, using the latent space of the generator to generate\nmultiple \"views\" of the same semantic content. We show that for contrastive\nmethods, this multiview data can naturally be used to identify positive pairs\n(nearby in latent space) and negative pairs (far apart in latent space). We\nfind that the resulting representations rival those learned directly from real\ndata, but that good performance requires care in the sampling strategy applied\nand the training method. Generative models can be viewed as a compressed and\norganized copy of a dataset, and we envision a future where more and more\n\"model zoos\" proliferate while datasets become increasingly unwieldy, missing,\nor private. This paper suggests several techniques for dealing with visual\nrepresentation learning in such a future. Code is released on our project page:\nhttps://ali-design.github.io/GenRep/",
    "descriptor": "",
    "authors": [
      "Ali Jahanian",
      "Xavier Puig",
      "Yonglong Tian",
      "Phillip Isola"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05258"
  },
  {
    "id": "arXiv:2106.05261",
    "title": "We Can Always Catch You: Detecting Adversarial Patched Objects WITH or  WITHOUT Signature",
    "abstract": "Recently, the object detection based on deep learning has proven to be\nvulnerable to adversarial patch attacks. The attackers holding a specially\ncrafted patch can hide themselves from the state-of-the-art person detectors,\ne.g., YOLO, even in the physical world. This kind of attack can bring serious\nsecurity threats, such as escaping from surveillance cameras. In this paper, we\ndeeply explore the detection problems about the adversarial patch attacks to\nthe object detection. First, we identify a leverageable signature of existing\nadversarial patches from the point of the visualization explanation. A fast\nsignature-based defense method is proposed and demonstrated to be effective.\nSecond, we design an improved patch generation algorithm to reveal the risk\nthat the signature-based way may be bypassed by the techniques emerging in the\nfuture. The newly generated adversarial patches can successfully evade the\nproposed signature-based defense. Finally, we present a novel\nsignature-independent detection method based on the internal content semantics\nconsistency rather than any attack-specific prior knowledge. The fundamental\nintuition is that the adversarial object can appear locally but disappear\nglobally in an input image. The experiments demonstrate that the\nsignature-independent method can effectively detect the existing and improved\nattacks. It has also proven to be a general method by detecting unforeseen and\neven other types of attacks without any attack-specific prior knowledge. The\ntwo proposed detection methods can be adopted in different scenarios, and we\nbelieve that combining them can offer a comprehensive protection.",
    "descriptor": "",
    "authors": [
      "Bin Liang",
      "Jiachun Li",
      "Jianjun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05261"
  },
  {
    "id": "arXiv:2106.05262",
    "title": "TempoRL: Learning When to Act",
    "abstract": "Reinforcement learning is a powerful approach to learn behaviour through\ninteractions with an environment. However, behaviours are usually learned in a\npurely reactive fashion, where an appropriate action is selected based on an\nobservation. In this form, it is challenging to learn when it is necessary to\nexecute new decisions. This makes learning inefficient, especially in\nenvironments that need various degrees of fine and coarse control. To address\nthis, we propose a proactive setting in which the agent not only selects an\naction in a state but also for how long to commit to that action. Our TempoRL\napproach introduces skip connections between states and learns a skip-policy\nfor repeating the same action along these skips. We demonstrate the\neffectiveness of TempoRL on a variety of traditional and deep RL environments,\nshowing that our approach is capable of learning successful policies up to an\norder of magnitude faster than vanilla Q-learning.",
    "descriptor": "\nComments: Accepted at ICML'21\n",
    "authors": [
      "Andr\u00e9 Biedenkapp",
      "Raghu Rajan",
      "Frank Hutter",
      "Marius Lindauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05262"
  },
  {
    "id": "arXiv:2106.05264",
    "title": "NeRF in detail: Learning to sample for view synthesis",
    "abstract": "Neural radiance fields (NeRF) methods have demonstrated impressive novel view\nsynthesis performance. The core approach is to render individual rays by\nquerying a neural network at points sampled along the ray to obtain the density\nand colour of the sampled points, and integrating this information using the\nrendering equation. Since dense sampling is computationally prohibitive, a\ncommon solution is to perform coarse-to-fine sampling.\nIn this work we address a clear limitation of the vanilla coarse-to-fine\napproach -- that it is based on a heuristic and not trained end-to-end for the\ntask at hand. We introduce a differentiable module that learns to propose\nsamples and their importance for the fine network, and consider and compare\nmultiple alternatives for its neural architecture. Training the proposal module\nfrom scratch can be unstable due to lack of supervision, so an effective\npre-training strategy is also put forward. The approach, named `NeRF in detail'\n(NeRF-ID), achieves superior view synthesis quality over NeRF and the\nstate-of-the-art on the synthetic Blender benchmark and on par or better\nperformance on the real LLFF-NeRF scenes. Furthermore, by leveraging the\npredicted sample importance, a 25% saving in computation can be achieved\nwithout significantly sacrificing the rendering quality.",
    "descriptor": "",
    "authors": [
      "Relja Arandjelovi\u0107",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05264"
  },
  {
    "id": "arXiv:2106.05266",
    "title": "Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in  Time",
    "abstract": "Estimating 3D hand and object pose from a single image is an extremely\nchallenging problem: hands and objects are often self-occluded during\ninteractions, and the 3D annotations are scarce as even humans cannot directly\nlabel the ground-truths from a single image perfectly. To tackle these\nchallenges, we propose a unified framework for estimating the 3D hand and\nobject poses with semi-supervised learning. We build a joint learning framework\nwhere we perform explicit contextual reasoning between hand and object\nrepresentations by a Transformer. Going beyond limited 3D annotations in a\nsingle image, we leverage the spatial-temporal consistency in large-scale\nhand-object videos as a constraint for generating pseudo labels in\nsemi-supervised learning. Our method not only improves hand pose estimation in\nchallenging real-world dataset, but also substantially improve the object pose\nwhich has fewer ground-truths per instance. By training with large-scale\ndiverse videos, our model also generalizes better across multiple out-of-domain\ndatasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object",
    "descriptor": "\nComments: CVPR 2021, Project page: this https URL\n",
    "authors": [
      "Shaowei Liu",
      "Hanwen Jiang",
      "Jiarui Xu",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05266"
  },
  {
    "id": "arXiv:2106.01475",
    "title": "A Reconfigurable Relay for Polarization Encoded QKD Networks",
    "abstract": "We propose a method for reconfiguring a relay node for polarization encoded\nquantum key distribution (QKD) networks. The relay can be switched between\ntrusted and untrusted modes to adapt to different network conditions, relay\ndistances, and security requirements. This not only extends the distance over\nwhich a QKD network operates but also enables point-to-multipoint (P2MP)\nnetwork topologies. The proposed architecture centralizes the expensive and\ndelicate single-photon detectors (SPDs) at the relay node with eased\nmaintenance and cooling while simplifying each user node so that it only needs\ncommercially available devices for low-cost qubit preparation.",
    "descriptor": "",
    "authors": [
      "Jing Wang",
      "Bernardo A. Huberman"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.01475"
  },
  {
    "id": "arXiv:2106.04619",
    "title": "Self-Supervised Learning with Data Augmentations Provably Isolates  Content from Style",
    "abstract": "Self-supervised representation learning has shown remarkable success in a\nnumber of domains. A common practice is to perform data augmentation via\nhand-crafted transformations intended to leave the semantics of the data\ninvariant. We seek to understand the empirical success of this approach from a\ntheoretical perspective. We formulate the augmentation process as a latent\nvariable model by postulating a partition of the latent representation into a\ncontent component, which is assumed invariant to augmentation, and a style\ncomponent, which is allowed to change. Unlike prior work on disentanglement and\nindependent component analysis, we allow for both nontrivial statistical and\ncausal dependencies in the latent space. We study the identifiability of the\nlatent representation based on pairs of views of the observations and prove\nsufficient conditions that allow us to identify the invariant content partition\nup to an invertible mapping in both generative and discriminative settings. We\nfind numerical simulations with dependent latent variables are consistent with\nour theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,\nvisually complex images with rich causal dependencies, which we use to study\nthe effect of data augmentations performed in practice.",
    "descriptor": "",
    "authors": [
      "Julius von K\u00fcgelgen",
      "Yash Sharma",
      "Luigi Gresele",
      "Wieland Brendel",
      "Bernhard Sch\u00f6lkopf",
      "Michel Besserve",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04619"
  },
  {
    "id": "arXiv:2106.04624",
    "title": "SpeechBrain: A General-Purpose Speech Toolkit",
    "abstract": "SpeechBrain is an open-source and all-in-one speech toolkit. It is designed\nto facilitate the research and development of neural speech processing\ntechnologies by being simple, flexible, user-friendly, and well-documented.\nThis paper describes the core architecture designed to support several tasks of\ncommon interest, allowing users to naturally conceive, compare and share novel\nspeech processing pipelines. SpeechBrain achieves competitive or\nstate-of-the-art performance in a wide range of speech benchmarks. It also\nprovides training recipes, pretrained models, and inference scripts for popular\nspeech datasets, as well as tutorials which allow anyone with basic Python\nproficiency to familiarize themselves with speech technologies.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Mirco Ravanelli",
      "Titouan Parcollet",
      "Peter Plantinga",
      "Aku Rouhe",
      "Samuele Cornell",
      "Loren Lugosch",
      "Cem Subakan",
      "Nauman Dawalatabad",
      "Abdelwahab Heba",
      "Jianyuan Zhong",
      "Ju-Chieh Chou",
      "Sung-Lin Yeh",
      "Szu-Wei Fu",
      "Chien-Feng Liao",
      "Elena Rastorgueva",
      "Fran\u00e7ois Grondin",
      "William Aris",
      "Hwidong Na",
      "Yan Gao",
      "Renato De Mori",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04624"
  },
  {
    "id": "arXiv:2106.04633",
    "title": "A Quantum Advantage for a Natural Streaming Problem",
    "abstract": "Data streaming, in which a large dataset is received as a \"stream\" of\nupdates, is an important model in the study of space-bounded computation.\nStarting with the work of Le Gall [SPAA `06], it has been known that quantum\nstreaming algorithms can use asymptotically less space than their classical\ncounterparts for certain problems. However, so far, all known examples of\nquantum advantages in streaming are for problems that are either specially\nconstructed for that purpose, or require many streaming passes over the input.\nWe give a one-pass quantum streaming algorithm for one of the best studied\nproblems in classical graph streaming - the triangle counting problem.\nAlmost-tight parametrized upper and lower bounds are known for this problem in\nthe classical setting; our algorithm uses polynomially less space in certain\nregions of the parameter space, resolving a question posed by Jain and Nayak in\n2014 on achieving quantum advantages for natural streaming problems.",
    "descriptor": "",
    "authors": [
      "John Kallaugher"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04633"
  },
  {
    "id": "arXiv:2106.04636",
    "title": "Automatically Differentiable Random Coefficient Logistic Demand  Estimation",
    "abstract": "We show how the random coefficient logistic demand (BLP) model can be phrased\nas an automatically differentiable moment function, including the incorporation\nof numerical safeguards proposed in the literature. This allows gradient-based\nfrequentist and quasi-Bayesian estimation using the Continuously Updating\nEstimator (CUE). Drawing from the machine learning literature, we outline\nhitherto under-utilized best practices in both frequentist and Bayesian\nestimation techniques. Our Monte Carlo experiments compare the performance of\nCUE, 2S-GMM, and LTE estimation. Preliminary findings indicate that the CUE\nestimated using LTE and frequentist optimization has a lower bias but higher\nMAE compared to the traditional 2-Stage GMM (2S-GMM) approach. We also find\nthat using credible intervals from MCMC sampling for the non-linear parameters\ntogether with frequentist analytical standard errors for the concentrated out\nlinear parameters provides empirical coverage closest to the nominal level. The\naccompanying admest Python package provides a platform for replication and\nextensibility.",
    "descriptor": "",
    "authors": [
      "Andrew Chia"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04636"
  },
  {
    "id": "arXiv:2106.04650",
    "title": "TED-net: Convolution-free T2T Vision Transformer-based Encoder-decoder  Dilation network for Low-dose CT Denoising",
    "abstract": "Low dose computed tomography is a mainstream for clinical applications.\nHow-ever, compared to normal dose CT, in the low dose CT (LDCT) images, there\nare stronger noise and more artifacts which are obstacles for practical\napplications. In the last few years, convolution-based end-to-end deep learning\nmethods have been widely used for LDCT image denoising. Recently, transformer\nhas shown superior performance over convolution with more feature interactions.\nYet its ap-plications in LDCT denoising have not been fully cultivated. Here,\nwe propose a convolution-free T2T vision transformer-based Encoder-decoder\nDilation net-work (TED-net) to enrich the family of LDCT denoising algorithms.\nThe model is free of convolution blocks and consists of a symmetric\nencoder-decoder block with sole transformer. Our model is evaluated on the\nAAPM-Mayo clinic LDCT Grand Challenge dataset, and results show outperformance\nover the state-of-the-art denoising methods.",
    "descriptor": "\nComments: 10 pages, manuscript for 12th International Workshop on Machine Learning in Medical Imaging\n",
    "authors": [
      "Dayang Wang",
      "Zhan Wu",
      "Hengyong Yu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.04650"
  },
  {
    "id": "arXiv:2106.04676",
    "title": "A hybrid discrete-continuum approach to model hydro-mechanical behaviour  of soil during desiccation",
    "abstract": "Desiccation cracking in clayey soils occurs when they lose moisture, leading\nto an increase in their compressibility and hydraulic conductivity and hence\nsignificant reduction of soil strength. The prediction of desiccation cracking\nin soils is challenging due to the lack of insights into the complex coupled\nhydro-mechanical process at the grain scale. In this paper, a new hybrid\ndiscrete-continuum numerical framework, capable of capturing hydro-mechanical\nbehaviour of soil at both grain and macro scales, is proposed for predicting\ndesiccation cracking in clayey soil. In this framework, a soil layer is\nrepresented by an assembly of DEM particles, each occupies an equivalent\ncontinuum space and carries physical properties governing unsaturated flow.\nThese particles move freely in the computational space following the discrete\nelement method (DEM), while their contact network and the continuum mixture\ntheory are used to model the unsaturated flow. The dependence of\nparticle-to-particle contact behaviour on water content is represented by a\ncohesive-frictional contact model, whose material properties are governed by\nthe water content. In parallel with the theoretical development is a series of\nexperiments on 3D soil desiccation cracking to determine essential properties\nand provide data for the validation of mechanical and physical behaviour. Very\ngood agreement in both physical behaviour (e.g. evolution of water content) and\nmechanical behaviour (e.g. occurrence and development of cracks, and\ndistribution of compressive and tensile strains) demonstrates that the proposed\nframework is capable of capturing the hydro-mechanical behaviour of soil during\ndesiccation. The capability of the proposed framework facilitates numerical\nexperiments for insights into the hydro-mechanical behaviour of unsaturated\nsoils that have not been possible before.",
    "descriptor": "",
    "authors": [
      "Khoa M. Tran",
      "Ha H. Bui",
      "Giang D. Nguyen"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2106.04676"
  },
  {
    "id": "arXiv:2106.04703",
    "title": "Categorical Data Structures for Technical Computing",
    "abstract": "Many mathematical objects can be represented as functors from\nfinitely-presented categories $\\mathsf{C}$ to $\\mathsf{Set}$. For instance,\ngraphs are functors to $\\mathsf{C}$ from the category with two parallel arrows.\nSuch functors are known informally as $\\mathsf{C}$-sets. In this paper, we\ndescribe and implement an extension of $\\mathsf{C}$-sets having data attributes\nwith fixed types, such as graphs with labeled vertices or real-valued edge\nweights. We call such structures \"acsets,\" short for \"attributed\n$\\mathsf{C}$-sets.\" Derived from previous work on algebraic databases, acsets\nare a joint generalization of graphs and data frames. They also encompass more\nelaborate graph-like objects such as wiring diagrams and Petri nets with rate\nconstants. We develop the mathematical theory of acsets and then describe a\ngeneric implementation in the Julia programming language, which uses advanced\nlanguage features to achieve performance comparable with specialized data\nstructures.",
    "descriptor": "\nComments: 26 pages, 7 figures\n",
    "authors": [
      "Evan Patterson",
      "Owen Lynch",
      "James Fairbanks"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Databases (cs.DB)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.04703"
  },
  {
    "id": "arXiv:2106.04729",
    "title": "Drones for Medical Delivery Considering Different Demands Classes: A  Markov Decision Process Approach for Managing Health Centers Dispatching  Medical Products",
    "abstract": "We consider the problem of optimizing the distribution operations of a hub\nusing drones to deliver medical supplies to different geographic regions.\nDrones are an innovative method with many benefits including low-contact\ndelivery thereby reducing the spread of pandemic and vaccine-preventable\ndiseases. While we focus on medical supply delivery for this work, it is\napplicable to drone delivery for many other applications, including food,\npostal items, and e-commerce delivery. In this paper, our goal is to address\ndrone delivery challenges by optimizing the distribution operations at a drone\nhub that dispatch drones to different geographic locations generating\nstochastic demands for medical supplies. By considering different geographic\nlocations, we consider different classes of demand that require different\nflight ranges, which is directly related to the amount of charge held in a\ndrone battery. We classify the stochastic demands based on their distance from\nthe drone hub, use a Markov decision process to model the problem, and perform\ncomputational tests using realistic data representing a prominent drone\ndelivery company. We solve the problem using a reinforcement learning method\nand show its high performance compared with the exact solution found using\ndynamic programming. Finally, we analyze the results and provide insights for\nmanaging the drone hub operations.",
    "descriptor": "",
    "authors": [
      "Amin Asadi",
      "Sarah Nurre Pinkley"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04729"
  },
  {
    "id": "arXiv:2106.04741",
    "title": "Marginalizable Density Models",
    "abstract": "Probability density models based on deep networks have achieved remarkable\nsuccess in modeling complex high-dimensional datasets. However, unlike kernel\ndensity estimators, modern neural models do not yield marginals or conditionals\nin closed form, as these quantities require the evaluation of seldom tractable\nintegrals. In this work, we present the Marginalizable Density Model\nApproximator (MDMA), a novel deep network architecture which provides closed\nform expressions for the probabilities, marginals and conditionals of any\nsubset of the variables. The MDMA learns deep scalar representations for each\nindividual variable and combines them via learned hierarchical tensor\ndecompositions into a tractable yet expressive CDF, from which marginals and\nconditional densities are easily obtained. We illustrate the advantage of exact\nmarginalizability in several tasks that are out of reach of previous deep\nnetwork-based density estimation models, such as estimating mutual information\nbetween arbitrary subsets of variables, inferring causality by testing for\nconditional independence, and inference with missing data without the need for\ndata imputation, outperforming state-of-the-art models on these tasks. The\nmodel also allows for parallelized sampling with only a logarithmic dependence\nof the time complexity on the number of variables.",
    "descriptor": "",
    "authors": [
      "Dar Gilboa",
      "Ari Pakman",
      "Thibault Vatter"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.04741"
  },
  {
    "id": "arXiv:2106.04758",
    "title": "Quantum Carry Lookahead Adders for NISQ and Quantum Image Processing",
    "abstract": "Progress in quantum hardware design is progressing toward machines of\nsufficient size to begin realizing quantum algorithms in disciplines such as\nencryption and physics. Quantum circuits for addition are crucial to realize\nmany quantum algorithms on these machines. Ideally, quantum circuits based on\nfault-tolerant gates and error-correcting codes should be used as they tolerant\nenvironmental noise. However, current machines called Noisy Intermediate Scale\nQuantum (NISQ) machines cannot support the overhead associated with\nfaulttolerant design. In response, low depth circuits such as quantum carry\nlookahead adders (QCLA)s have caught the attention of researchers. The risk for\nnoise errors and decoherence increase as the number of gate layers (or depth)\nin the circuit increases. This work presents an out-of-place QCLA based on\nClifford+T gates. The QCLAs optimized for T gate count and make use of a novel\nuncomputation gate to save T gates. We base our QCLAs on Clifford+T gates\nbecause they can eventually be made faulttolerant with error-correcting codes\nonce quantum hardware that can support fault-tolerant designs becomes\navailable. We focus on T gate cost as the T gate is significantly more costly\nto make faulttolerant than the other Clifford+T gates. The proposed QCLAs are\ncompared and shown to be superior to existing works in terms of T-count and\ntherefore the total number of quantum gates. Finally, we illustrate the\napplication of the proposed QCLAs in quantum image processing by presenting\nquantum circuits for bilinear interpolation.",
    "descriptor": "\nComments: 4 Pages, 2020 IEEE 38th International Conference on Computer Design (ICCD), Hartford, CT, USA, 2020\n",
    "authors": [
      "Himanshu Thapliyal",
      "Edgard Mu\u00f1oz-Coreas",
      "Vladislav Khalus"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2106.04758"
  },
  {
    "id": "arXiv:2106.04764",
    "title": "Semi-Supervised Training with Pseudo-Labeling for End-to-End Neural  Diarization",
    "abstract": "In this paper, we present a semi-supervised training technique using\npseudo-labeling for end-to-end neural diarization (EEND). The EEND system has\nshown promising performance compared with traditional clustering-based methods,\nespecially in the case of overlapping speech. However, to get a well-tuned\nmodel, EEND requires labeled data for all the joint speech activities of every\nspeaker at each time frame in a recording. In this paper, we explore a\npseudo-labeling approach that employs unlabeled data. First, we propose an\niterative pseudo-label method for EEND, which trains the model using unlabeled\ndata of a target condition. Then, we also propose a committee-based training\nmethod to improve the performance of EEND. To evaluate our proposed method, we\nconduct the experiments of model adaptation using labeled and unlabeled data.\nExperimental results on the CALLHOME dataset show that our proposed\npseudo-label achieved a 37.4% relative diarization error rate reduction\ncompared to a seed model. Moreover, we analyzed the results of semi-supervised\nadaptation with pseudo-labeling. We also show the effectiveness of our approach\non the third DIHARD dataset.",
    "descriptor": "\nComments: Accepted for Interspeech 2021\n",
    "authors": [
      "Yuki Takashima",
      "Yusuke Fujita",
      "Shota Horiguchi",
      "Shinji Watanabe",
      "Paola Garc\u00eda",
      "Kenji Nagamatsu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04764"
  },
  {
    "id": "arXiv:2106.04769",
    "title": "Submodular + Concave",
    "abstract": "It has been well established that first order optimization methods can\nconverge to the maximal objective value of concave functions and provide\nconstant factor approximation guarantees for (non-convex/non-concave)\ncontinuous submodular functions. In this work, we initiate the study of the\nmaximization of functions of the form $F(x) = G(x) +C(x)$ over a solvable\nconvex body $P$, where $G$ is a smooth DR-submodular function and $C$ is a\nsmooth concave function. This class of functions is a strict extension of both\nconcave and continuous DR-submodular functions for which no theoretical\nguarantee is known. We provide a suite of Frank-Wolfe style algorithms, which,\ndepending on the nature of the objective function (i.e., if $G$ and $C$ are\nmonotone or not, and non-negative or not) and on the nature of the set $P$\n(i.e., whether it is downward closed or not), provide $1-1/e$, $1/e$, or $1/2$\napproximation guarantees. We then use our algorithms to get a framework to\nsmoothly interpolate between choosing a diverse set of elements from a given\nground set (corresponding to the mode of a determinantal point process) and\nchoosing a clustered set of elements (corresponding to the maxima of a suitable\nconcave function). Additionally, we apply our algorithms to various functions\nin the above class (DR-submodular + concave) in both constrained and\nunconstrained settings, and show that our algorithms consistently outperform\nnatural baselines.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Siddharth Mitra",
      "Moran Feldman",
      "Amin Karbasi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04769"
  },
  {
    "id": "arXiv:2106.04805",
    "title": "Streaming Belief Propagation for Community Detection",
    "abstract": "The community detection problem requires to cluster the nodes of a network\ninto a small number of well-connected \"communities\". There has been substantial\nrecent progress in characterizing the fundamental statistical limits of\ncommunity detection under simple stochastic block models. However, in\nreal-world applications, the network structure is typically dynamic, with nodes\nthat join over time. In this setting, we would like a detection algorithm to\nperform only a limited number of updates at each node arrival. While standard\nvoting approaches satisfy this constraint, it is unclear whether they exploit\nthe network information optimally. We introduce a simple model for networks\ngrowing over time which we refer to as streaming stochastic block model\n(StSBM). Within this model, we prove that voting algorithms have fundamental\nlimitations. We also develop a streaming belief-propagation (StreamBP)\napproach, for which we prove optimality in certain regimes. We validate our\ntheoretical findings on synthetic and real data.",
    "descriptor": "\nComments: 36 pages, 13 figures\n",
    "authors": [
      "Yuchen Wu",
      "MohammadHossein Bateni",
      "Andre Linhares",
      "Filipe Miguel Goncalves de Almeida",
      "Andrea Montanari",
      "Ashkan Norouzi-Fard",
      "Jakab Tardos"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.04805"
  },
  {
    "id": "arXiv:2106.04822",
    "title": "Fast Computational Ghost Imaging using Unpaired Deep Learning and a  Constrained Generative Adversarial Network",
    "abstract": "The unpaired training can be the only option available for fast deep\nlearning-based ghost imaging, where obtaining a high signal-to-noise ratio\n(SNR) image copy of each low SNR ghost image could be practically\ntime-consuming and challenging. This paper explores the capabilities of deep\nlearning to leverage computational ghost imaging when there is a lack of paired\ntraining images. The deep learning approach proposed here enables fast ghost\nimaging through reconstruction of high SNR images from faint and hastily shot\nghost images using a constrained Wasserstein generative adversarial network. In\nthe proposed approach, the objective function is regularized to enforce the\ngeneration of faithful and relevant high SNR images to the ghost copies. This\nregularization measures the distance between reconstructed images and the faint\nghost images in a low-noise manifold generated by a shadow network. The\nperformance of the constrained network is shown to be particularly important\nfor ghost images with low SNR. The proposed pipeline is able to reconstruct\nhigh-quality images from the ghost images with SNR values not necessarily equal\nto the SNR of the training set.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Fatemeh Alishahi",
      "Amirhossein Mohajerin-Ariaei"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04822"
  },
  {
    "id": "arXiv:2106.04862",
    "title": "Bayesian Boosting for Linear Mixed Models",
    "abstract": "Boosting methods are widely used in statistical learning to deal with\nhigh-dimensional data due to their variable selection feature. However, those\nmethods lack straightforward ways to construct estimators for the precision of\nthe parameters such as variance or confidence interval, which can be achieved\nby conventional statistical methods like Bayesian inference. In this paper, we\npropose a new inference method \"BayesBoost\" that combines boosting and Bayesian\nfor linear mixed models to make the uncertainty estimation for the random\neffects possible on the one hand. On the other hand, the new method overcomes\nthe shortcomings of Bayesian inference in giving precise and unambiguous\nguidelines for the selection of covariates by benefiting from boosting\ntechniques. The implementation of Bayesian inference leads to the randomness of\nmodel selection criteria like the conditional AIC (cAIC), so we also propose a\ncAIC-based model selection criteria that focus on the stabilized regions\ninstead of the global minimum. The effectiveness of the new approach can be\nobserved via simulation and in a data example from the field of neurophysiology\nfocussing on the mechanisms in the brain while listening to unpleasant sounds.",
    "descriptor": "",
    "authors": [
      "Boyao Zhang",
      "Colin Griesbach",
      "Cora Kim",
      "Nadia M\u00fcller-Voggel",
      "Elisabeth Bergherr"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04862"
  },
  {
    "id": "arXiv:2106.04878",
    "title": "Deep Interaction between Masking and Mapping Targets for Single-Channel  Speech Enhancement",
    "abstract": "The most recent deep neural network (DNN) models exhibit impressive denoising\nperformance in the time-frequency (T-F) magnitude domain. However, the phase is\nalso a critical component of the speech signal that is easily overlooked. In\nthis paper, we propose a multi-branch dilated convolutional network (DCN) to\nsimultaneously enhance the magnitude and phase of noisy speech. A causal and\nrobust monaural speech enhancement system is achieved based on the\nmulti-objective learning framework of the complex spectrum and the ideal ratio\nmask (IRM) targets. In the process of joint learning, the intermediate\nestimation of IRM targets is used as a way of generating feature attention\nfactors to realize the information interaction between the two targets.\nMoreover, the proposed multi-scale dilated convolution enables the DCN model to\nhave a more efficient temporal modeling capability. Experimental results show\nthat compared with other state-of-the-art models, this model achieves better\nspeech quality and intelligibility with less computation.",
    "descriptor": "",
    "authors": [
      "Lu Zhang",
      "Mingjiang Wang",
      "Zehua Zhang",
      "Xuyi Zhuang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04878"
  },
  {
    "id": "arXiv:2106.04881",
    "title": "Fractal Structure and Generalization Properties of Stochastic  Optimization Algorithms",
    "abstract": "Understanding generalization in deep learning has been one of the major\nchallenges in statistical learning theory over the last decade. While recent\nwork has illustrated that the dataset and the training algorithm must be taken\ninto account in order to obtain meaningful generalization bounds, it is still\ntheoretically not clear which properties of the data and the algorithm\ndetermine the generalization performance. In this study, we approach this\nproblem from a dynamical systems theory perspective and represent stochastic\noptimization algorithms as random iterated function systems (IFS). Well studied\nin the dynamical systems literature, under mild assumptions, such IFSs can be\nshown to be ergodic with an invariant measure that is often supported on sets\nwith a fractal structure. As our main contribution, we prove that the\ngeneralization error of a stochastic optimization algorithm can be bounded\nbased on the `complexity' of the fractal structure that underlies its invariant\nmeasure. Leveraging results from dynamical systems theory, we show that the\ngeneralization error can be explicitly linked to the choice of the algorithm\n(e.g., stochastic gradient descent -- SGD), algorithm hyperparameters (e.g.,\nstep-size, batch-size), and the geometry of the problem (e.g., Hessian of the\nloss). We further specialize our results to specific problems (e.g.,\nlinear/logistic regression, one hidden-layered neural networks) and algorithms\n(e.g., SGD and preconditioned variants), and obtain analytical estimates for\nour bound.For modern neural networks, we develop an efficient algorithm to\ncompute the developed bound and support our theory with various experiments on\nneural networks.",
    "descriptor": "\nComments: 34 pages including Supplement, 4 Figures\n",
    "authors": [
      "Alexander Camuto",
      "George Deligiannidis",
      "Murat A. Erdogdu",
      "Mert G\u00fcrb\u00fczbalaban",
      "Umut \u015eim\u015fekli",
      "Lingjiong Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04881"
  },
  {
    "id": "arXiv:2106.04886",
    "title": "Fully differentiable model discovery",
    "abstract": "Model discovery aims at autonomously discovering differential equations\nunderlying a dataset. Approaches based on Physics Informed Neural Networks\n(PINNs) have shown great promise, but a fully-differentiable model which\nexplicitly learns the equation has remained elusive. In this paper we propose\nsuch an approach by combining neural network based surrogates with Sparse\nBayesian Learning (SBL). We start by reinterpreting PINNs as multitask models,\napplying multitask learning using uncertainty, and show that this leads to a\nnatural framework for including Bayesian regression techniques. We then\nconstruct a robust model discovery algorithm by using SBL, which we showcase on\nvarious datasets. Concurrently, the multitask approach allows the use of\nprobabilistic approximators, and we show a proof of concept using normalizing\nflows to directly learn a density model from single particle data. Our work\nexpands PINNs to various types of neural network architectures, and connects\nneural network-based surrogates to the rich field of Bayesian parameter\ninference.",
    "descriptor": "",
    "authors": [
      "Gert-Jan Both",
      "Remy Kusters"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04886"
  },
  {
    "id": "arXiv:2106.04892",
    "title": "A 2D front-tracking Lagrangian model for the modeling of anisotropic  grain growth",
    "abstract": "Grain growth is a well-known and complex phenomenon occurring during\nannealing of all polycrystalline materials. Its numerical modeling is a complex\ntask when anisotropy sources such as grain orientation and grain boundary\ninclination have to be taken into account. This article presents the\napplication of the front-tracking methodology ToRealMotion introduced in\nprevious works, to the context of anisotropic grain boundary motion at the\nmesoscopic scale. The new formulation of boundary migration can take into\naccount any source of anisotropy both at grain boundaries as well as at\nmultiple junctions (MJs) (intersection point of three or more grain\nboundaries). Special attention is given to the decomposition of high-order MJs\nfor which an algorithm is proposed based on local grain boundary energy\nminimisation. Numerical tests are provided using highly heterogeneous\nconfigurations, and comparisons with a recently developed Finite-Element\nLevel-Set (FE-LS) approach are given. Finally, the computational performance of\nthe model will be studied comparing the CPU-times obtained with the same model\nbut in an isotropic context.",
    "descriptor": "",
    "authors": [
      "Sebastian Florez",
      "Julien Fausty",
      "Karen Alvarado",
      "Brayan Murgas",
      "Marc Bernacki"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.04892"
  },
  {
    "id": "arXiv:2106.04923",
    "title": "Learning Domain Invariant Representations by Joint Wasserstein Distance  Minimization",
    "abstract": "Domain shifts in the training data are common in practical applications of\nmachine learning, they occur for instance when the data is coming from\ndifferent sources. Ideally, a ML model should work well independently of these\nshifts, for example, by learning a domain-invariant representation. Moreover,\nprivacy concerns regarding the source also require a domain-invariant\nrepresentation. In this work, we provide theoretical results that link domain\ninvariant representations -- measured by the Wasserstein distance on the joint\ndistributions -- to a practical semi-supervised learning objective based on a\ncross-entropy classifier and a novel domain critic. Quantitative experiments\ndemonstrate that the proposed approach is indeed able to practically learn such\nan invariant representation (between two domains), and the latter also supports\nmodels with higher predictive accuracy on both domains, comparing favorably to\nexisting techniques.",
    "descriptor": "\nComments: 20 pages including appendix. Under Review\n",
    "authors": [
      "L\u00e9o And\u00e9ol",
      "Yusei Kawakami",
      "Yuichiro Wada",
      "Takafumi Kanamori",
      "Klaus-Robert M\u00fcller",
      "Gr\u00e9goire Montavon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04923"
  },
  {
    "id": "arXiv:2106.04929",
    "title": "Fast and More Powerful Selective Inference for Sparse High-order  Interaction Model",
    "abstract": "Automated high-stake decision-making such as medical diagnosis requires\nmodels with high interpretability and reliability. As one of the interpretable\nand reliable models with good prediction ability, we consider Sparse High-order\nInteraction Model (SHIM) in this study. However, finding statistically\nsignificant high-order interactions is challenging due to the intrinsic high\ndimensionality of the combinatorial effects. Another problem in data-driven\nmodeling is the effect of \"cherry-picking\" a.k.a. selection bias. Our main\ncontribution is to extend the recently developed parametric programming\napproach for selective inference to high-order interaction models. Exhaustive\nsearch over the cherry tree (all possible interactions) can be daunting and\nimpractical even for a small-sized problem. We introduced an efficient pruning\nstrategy and demonstrated the computational efficiency and statistical power of\nthe proposed method using both synthetic and real data.",
    "descriptor": "",
    "authors": [
      "Diptesh Das",
      "Vo Nguyen Le Duy",
      "Hiroyuki Hanada",
      "Koji Tsuda",
      "Ichiro Takeuchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04929"
  },
  {
    "id": "arXiv:2106.04961",
    "title": "Spatio-Temporal Dual-Stream Neural Network for Sequential Whole-Body PET  Segmentation",
    "abstract": "Sequential whole-body 18F-Fluorodeoxyglucose (FDG) positron emission\ntomography (PET) scans are regarded as the imaging modality of choice for the\nassessment of treatment response in the lymphomas because they detect treatment\nresponse when there may not be changes on anatomical imaging. Any computerized\nanalysis of lymphomas in whole-body PET requires automatic segmentation of the\nstudies so that sites of disease can be quantitatively monitored over time.\nState-of-the-art PET image segmentation methods are based on convolutional\nneural networks (CNNs) given their ability to leverage annotated datasets to\nderive high-level features about the disease process. Such methods, however,\nfocus on PET images from a single time-point and discard information from other\nscans or are targeted towards specific organs and cannot cater for the multiple\nstructures in whole-body PET images. In this study, we propose a\nspatio-temporal 'dual-stream' neural network (ST-DSNN) to segment sequential\nwhole-body PET scans. Our ST-DSNN learns and accumulates image features from\nthe PET images done over time. The accumulated image features are used to\nenhance the organs / structures that are consistent over time to allow easier\nidentification of sites of active lymphoma. Our results show that our method\noutperforms the state-of-the-art PET image segmentation methods.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Kai-Chieh Liang",
      "Lei Bi",
      "Ashnil Kumar",
      "Michael Fulham",
      "Jinman Kim"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04961"
  },
  {
    "id": "arXiv:2106.04975",
    "title": "The dilemma of quantum neural networks",
    "abstract": "The core of quantum machine learning is to devise quantum models with good\ntrainability and low generalization error bound than their classical\ncounterparts to ensure better reliability and interpretability. Recent studies\nconfirmed that quantum neural networks (QNNs) have the ability to achieve this\ngoal on specific datasets. With this regard, it is of great importance to\nunderstand whether these advantages are still preserved on real-world tasks.\nThrough systematic numerical experiments, we empirically observe that current\nQNNs fail to provide any benefit over classical learning models. Concretely,\nour results deliver two key messages. First, QNNs suffer from the severely\nlimited effective model capacity, which incurs poor generalization on\nreal-world datasets. Second, the trainability of QNNs is insensitive to\nregularization techniques, which sharply contrasts with the classical scenario.\nThese empirical results force us to rethink the role of current QNNs and to\ndesign novel protocols for solving real-world problems with quantum advantages.",
    "descriptor": "",
    "authors": [
      "Yang Qian",
      "Xinbiao Wang",
      "Yuxuan Du",
      "Xingyao Wu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04975"
  },
  {
    "id": "arXiv:2106.04988",
    "title": "Optimal Inspection of Network Systems via Value of Information Analysis",
    "abstract": "This paper develops computable metrics to assign priorities for information\ncollection on network systems made up by binary components. Components are\nworth inspecting because their condition state is uncertain and the system\nfunctioning depends on it. The Value of Information (VoI) allows assessing the\nimpact of information in decision making under uncertainty, including the\nprecision of the observation, the available actions and the expected economic\nloss. Some VoI-based metrics for system-level and component-level maintenance\nactions, defined as \"global\" and \"local\" metrics, respectively, are introduced,\nanalyzed and applied to series and parallel systems. Their computationally\ncomplexity of applications to general networks is discussed and, to tame the\ncomplexity for the local metric assessment, a heuristic is presented and its\nperformance is compared on some case studies.",
    "descriptor": "\nComments: Submitted to Rel. Eng. and Sys. Saf\n",
    "authors": [
      "Chaochao Lin",
      "Junho Song",
      "Matteo Pozzi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04988"
  },
  {
    "id": "arXiv:2106.05010",
    "title": "Loss function based second-order Jensen inequality and its application  to particle variational inference",
    "abstract": "Bayesian model averaging, obtained as the expectation of a likelihood\nfunction by a posterior distribution, has been widely used for prediction,\nevaluation of uncertainty, and model selection. Various approaches have been\ndeveloped to efficiently capture the information in the posterior distribution;\none such approach is the optimization of a set of models simultaneously with\ninteraction to ensure the diversity of the individual models in the same way as\nensemble learning. A representative approach is particle variational inference\n(PVI), which uses an ensemble of models as an empirical approximation for the\nposterior distribution. PVI iteratively updates each model with a repulsion\nforce to ensure the diversity of the optimized models. However, despite its\npromising performance, a theoretical understanding of this repulsion and its\nassociation with the generalization ability remains unclear. In this paper, we\ntackle this problem in light of PAC-Bayesian analysis. First, we provide a new\nsecond-order Jensen inequality, which has the repulsion term based on the loss\nfunction. Thanks to the repulsion term, it is tighter than the standard Jensen\ninequality. Then, we derive a novel generalization error bound and show that it\ncan be reduced by enhancing the diversity of models. Finally, we derive a new\nPVI that optimizes the generalization error bound directly. Numerical\nexperiments demonstrate that the performance of the proposed PVI compares\nfavorably with existing methods in the experiment.",
    "descriptor": "",
    "authors": [
      "Futoshi Futami",
      "Tomoharu Iwata",
      "Naonori Ueda",
      "Issei Sato",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05010"
  },
  {
    "id": "arXiv:2106.05025",
    "title": "Towards a Framework for Nonlinear Predictive Control using  Derivative-Free Optimization",
    "abstract": "The use of derivative-based solvers to compute solutions to optimal control\nproblems with non-differentiable cost or dynamics often requires reformulations\nor relaxations that complicate the implementation or increase computational\ncomplexity. We present an initial framework for using the derivative-free Mesh\nAdaptive Direct Search (MADS) algorithm to solve Nonlinear Model Predictive\nControl problems with non-differentiable features without the need for\nreformulation. The MADS algorithm performs a structured search of the input\nspace by simulating selected system trajectories and computing the subsequent\ncost value. We propose handling the path constraints and the Lagrange cost term\nby augmenting the system dynamics with additional states to compute the\nviolation and cost value alongside the state trajectories, eliminating the need\nfor reconstructing the state trajectories in a separate phase. We demonstrate\nthe practicality of this framework by solving a robust rocket control problem,\nwhere the objective is to reach a target altitude as close as possible, given a\nsystem with uncertain parameters. This example uses a non-differentiable cost\nfunction and simulates two different system trajectories simultaneously, with\neach system having its own free final time.",
    "descriptor": "\nComments: Accepted for presentation at the 7th IFAC Conference on Nonlinear Model Predictive Control\n",
    "authors": [
      "Ian McInerney",
      "Lucian Nita",
      "Yuanbo Nie",
      "Alberto Oliveri",
      "Eric C. Kerrigan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05025"
  },
  {
    "id": "arXiv:2106.05064",
    "title": "Sharp Elements and the Scott Topology of Continuous Dcpos",
    "abstract": "Working constructively, we study continuous directed complete posets (dcpos)\nand the Scott topology. Our two primary novelties are a notion of intrinsic\napartness and a notion of sharp elements. Being apart is a positive formulation\nof being unequal, similar to how inhabitedness is a positive formulation of\nnonemptiness. To exemplify sharpness, we note that a lower real is sharp if and\nonly if it is located. Our first main result is that for a large class of\ncontinuous dcpos, the Bridges-V\\^i\\c{t}\\v{a} apartness topology and the Scott\ntopology coincide. Although we cannot expect a tight or cotransitive apartness\non nontrivial dcpos, we prove that the intrinsic apartness is both tight and\ncotransitive when restricted to the sharp elements of a continuous dcpo. These\ninclude the strongly maximal elements, as studied by Smyth and Heckmann. We\ndevelop the theory of strongly maximal elements highlighting its connection to\nsharpness and the Lawson topology. Finally, we illustrate the intrinsic\napartness, sharpness and strong maximality by considering several natural\nexamples of continuous dcpos: the Cantor and Baire domains, the partial\nDedekind reals, the lower reals and finally, an embedding of Cantor space into\nan exponential of lifted sets.",
    "descriptor": "",
    "authors": [
      "Tom de Jong"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05064"
  },
  {
    "id": "arXiv:2106.05109",
    "title": "Gaussian Mixture Estimation from Weighted Samples",
    "abstract": "We consider estimating the parameters of a Gaussian mixture density with a\ngiven number of components best representing a given set of weighted samples.\nWe adopt a density interpretation of the samples by viewing them as a discrete\nDirac mixture density over a continuous domain with weighted components. Hence,\nGaussian mixture fitting is viewed as density re-approximation. In order to\nspeed up computation, an expectation-maximization method is proposed that\nproperly considers not only the sample locations, but also the corresponding\nweights. It is shown that methods from literature do not treat the weights\ncorrectly, resulting in wrong estimates. This is demonstrated with simple\ncounterexamples. The proposed method works in any number of dimensions with the\nsame computational load as standard Gaussian mixture estimators for unweighted\nsamples.",
    "descriptor": "\nComments: 7 pages, 2 (10) figures\n",
    "authors": [
      "Daniel Frisch",
      "Uwe D. Hanebeck"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05109"
  },
  {
    "id": "arXiv:2106.05114",
    "title": "Mixture weights optimisation for Alpha-Divergence Variational Inference",
    "abstract": "This paper focuses on $\\alpha$-divergence minimisation methods for\nVariational Inference. More precisely, we are interested in algorithms\noptimising the mixture weights of any given mixture model, without any\ninformation on the underlying distribution of its mixture components\nparameters. The Power Descent, defined for all $\\alpha \\neq 1$, is one such\nalgorithm and we establish in our work the full proof of its convergence\ntowards the optimal mixture weights when $\\alpha <1$. Since the\n$\\alpha$-divergence recovers the widely-used forward Kullback-Leibler when\n$\\alpha \\to 1$, we then extend the Power Descent to the case $\\alpha = 1$ and\nshow that we obtain an Entropic Mirror Descent. This leads us to investigate\nthe link between Power Descent and Entropic Mirror Descent: first-order\napproximations allow us to introduce the Renyi Descent, a novel algorithm for\nwhich we prove an $O(1/N)$ convergence rate. Lastly, we compare numerically the\nbehavior of the unbiased Power Descent and of the biased Renyi Descent and we\ndiscuss the potential advantages of one algorithm over the other.",
    "descriptor": "",
    "authors": [
      "Kam\u00e9lia Daudel",
      "Randal Douc"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05114"
  },
  {
    "id": "arXiv:2106.05132",
    "title": "A multi-stage GAN for multi-organ chest X-ray image generation and  segmentation",
    "abstract": "Multi-organ segmentation of X-ray images is of fundamental importance for\ncomputer aided diagnosis systems. However, the most advanced semantic\nsegmentation methods rely on deep learning and require a huge amount of labeled\nimages, which are rarely available due to both the high cost of human resources\nand the time required for labeling. In this paper, we present a novel\nmulti-stage generation algorithm based on Generative Adversarial Networks\n(GANs) that can produce synthetic images along with their semantic labels and\ncan be used for data augmentation. The main feature of the method is that,\nunlike other approaches, generation occurs in several stages, which simplifies\nthe procedure and allows it to be used on very small datasets. The method has\nbeen evaluated on the segmentation of chest radiographic images, showing\npromising results. The multistage approach achieves state-of-the-art and, when\nvery few images are used to train the GANs, outperforms the corresponding\nsingle-stage approach.",
    "descriptor": "",
    "authors": [
      "Giorgio Ciano",
      "Paolo Andreini",
      "Tommaso Mazzierli",
      "Monica Bianchini",
      "Franco Scarselli"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05132"
  },
  {
    "id": "arXiv:2106.05134",
    "title": "Quantum Annealing for Automated Feature Selection in Stress Detection",
    "abstract": "We present a novel methodology for automated feature subset selection from a\npool of physiological signals using Quantum Annealing (QA). As a case study, we\nwill investigate the effectiveness of QA-based feature selection techniques in\nselecting the optimal feature subset for stress detection. Features are\nextracted from four signal sources: foot EDA, hand EDA, ECG, and respiration.\nThe proposed method embeds the feature variables extracted from the\nphysiological signals in a binary quadratic model. The bias of the feature\nvariable is calculated using the Pearson correlation coefficient between the\nfeature variable and the target variable. The weight of the edge connecting the\ntwo feature variables is calculated using the Pearson correlation coefficient\nbetween two feature variables in the binary quadratic model. Subsequently,\nD-Wave's clique sampler is used to sample cliques from the binary quadratic\nmodel. The underlying solution is then re-sampled to obtain multiple good\nsolutions and the clique with the lowest energy is returned as the optimal\nsolution. The proposed method is compared with commonly used feature selection\ntechniques for stress detection. Results indicate that QA-based feature subset\nselection performed equally as that of classical techniques. However, under\ndata uncertainty conditions such as limited training data, the performance of\nquantum annealing for selecting optimum features remained unaffected, whereas a\nsignificant decrease in performance is observed with classical feature\nselection techniques. Preliminary results show the promise of quantum annealing\nin optimizing the training phase of a machine learning classifier, especially\nunder data uncertainty conditions.",
    "descriptor": "\nComments: 5 Pages, 2nd International Workshop on Quantum Computing: Circuits Systems Automation and Applications (QC-CSAA) in conjuction with the ISVLSI 2021\n",
    "authors": [
      "Rajdeep Kumar Nath",
      "Himanshu Thapliyal",
      "Travis S. Humble"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05134"
  },
  {
    "id": "arXiv:2106.05146",
    "title": "An arbitrary order and pointwise divergence-free finite element scheme  for the incompressible 3D Navier-Stokes equations",
    "abstract": "In this paper we introduce a new discretization of the incompressible\nNavier-Stokes equations. We use the Lamb identity for the advection term $(u\n\\cdot \\nabla)u$ and the general idea allows a lot of freedom in the treatment\nof the non-linear term. The main advantage of this scheme is that the\ndivergence of the fluid velocity is pointwise zero at the discrete level. This\nexactness allows for exactly conserved quantities and pressure robustness.\nDiscrete spaces consist of piecewise polynomials, they may be taken of\narbitrary order and are already implemented in most libraries. Although the\nnonlinear term may be implemented as is, most proofs here are done for the\nlinearized equation. The whole problem is expressed in the finite element\nexterior calculus framework. We also present numerical simulations, our codes\nare written with the FEniCS computing platform, version 2019.1.0.",
    "descriptor": "\nComments: 30 pages, 8 figures\n",
    "authors": [
      "M. Hanot"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05146"
  },
  {
    "id": "arXiv:2106.05148",
    "title": "Time-Frequency Phase Retrieval for Audio -- The Effect of Transform  Parameters",
    "abstract": "In audio processing applications, phase retrieval (PR) is often performed\nfrom the magnitude of short-time Fourier transform (STFT) coefficients.\nAlthough PR performance has been observed to depend on the considered STFT\nparameters and audio data, the extent of this dependence has not been\nsystematically evaluated yet. To address this, we studied the performance of\nthree PR algorithms for various types of audio content and various STFT\nparameters such as redundancy, time-frequency ratio, and the type of window.\nThe quality of PR was studied in terms of objective difference grade and\nsignal-to-noise ratio of the STFT magnitude, to provide auditory- and\nsignal-based quality assessments. Our results show that PR quality improved\nwith increasing redundancy, with a strong relevance of the time-frequency\nratio. The effect of the audio content was smaller but still observable. The\neffect of the window was only significant for one of the PR algorithms.\nInterestingly, for a good PR quality, each of the three algorithms required a\ndifferent set of parameters, demonstrating the relevance of individual\nparameter sets for a fair comparison across PR algorithms. Based on these\nresults, we developed guidelines for optimizing STFT parameters for a given\napplication.",
    "descriptor": "\nComments: Accepted for publication as a regular paper in the IEEE Transactions on Signal Processing\n",
    "authors": [
      "Andr\u00e9s Marafioti",
      "Nicki Holighaus",
      "Piotr Majdak"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05148"
  },
  {
    "id": "arXiv:2106.05152",
    "title": "Rethink Transfer Learning in Medical Image Classification",
    "abstract": "Transfer learning (TL) with deep convolutional neural networks (DCNNs) has\nproved successful in medical image classification (MIC). However, the current\npractice is puzzling, as MIC typically relies only on low- and/or mid-level\nfeatures that are learned in the bottom layers of DCNNs. Following this\nintuition, we question the current strategies of TL in MIC. In this paper, we\nperform careful experimental comparisons between shallow and deep networks for\nclassification on two chest x-ray datasets, using different TL strategies. We\nfind that deep models are not always favorable, and finetuning truncated deep\nmodels almost always yields the best performance, especially in data-poor\nregimes.\nProject webpage:\nhttps://github.com/sun-umn/Transfer-Learning-in-Medical-Imaging\nKeywords: Transfer learning, Medical image classification, Feature hierarchy,\nMedical imaging, Evaluation metrics, Imbalanced data",
    "descriptor": "\nComments: Under review for MICCAI 2021\n",
    "authors": [
      "Le Peng",
      "Hengyue Liang",
      "Taihui Li",
      "Ju Sun"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05152"
  },
  {
    "id": "arXiv:2106.05181",
    "title": "Condition Integration Memory Network: An Interpretation of the Meaning  of the Neuronal Design",
    "abstract": "This document introduces a hypothesized framework on the functional nature of\nprimitive neural network. It discusses such an idea that the activity of\nneurons and synapses can symbolically reenact the dynamic changes in the world\nand enable an adaptive system of behavior. More specifically, the network\nachieves these without participating in an algorithmic structure. When a\nneuron's activation represents some symbolic element in the environment, each\nof its synapses can indicate a potential change to the element and its future\nstate. The efficacy of a synaptic connection further specifies the element's\nparticular probability for, or contribution to, such a change. A neuron's\nactivation is transformed to its postsynaptic targets as it fires, resulting in\na chronological shift of the represented elements. As the inherent function of\nsummation in a neuron integrates the various presynaptic contributions, the\nneural network mimics the collective causal relationship of events in the\nobserved environment.",
    "descriptor": "\nComments: 39 pages, 6 figures\n",
    "authors": [
      "Cheng Qian"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.05181"
  },
  {
    "id": "arXiv:2106.05186",
    "title": "Information theoretic analysis of computational models as a tool to  understand the neural basis of behaviors",
    "abstract": "One of the greatest research challenges of this century is to understand the\nneural basis for how behavior emerges in brain-body-environment systems. To\nthis end, research has flourished along several directions but have\npredominantly focused on the brain. While there is in an increasing acceptance\nand focus on including the body and environment in studying the neural basis of\nbehavior, animal researchers are often limited by technology or tools.\nComputational models provide an alternative framework within which one can\nstudy model systems where ground-truth can be measured and interfered with.\nThese models act as a hypothesis generation framework that would in turn guide\nexperimentation. Furthermore, the ability to intervene as we please, allows us\nto conduct in-depth analysis of these models in a way that cannot be performed\nin natural systems. For this purpose, information theory is emerging as a\npowerful tool that can provide insights into the operation of these\nbrain-body-environment models. In this work, I provide an introduction, a\nreview and discussion to make a case for how information theoretic analysis of\ncomputational models is a potent research methodology to help us better\nunderstand the neural basis of behavior.",
    "descriptor": "",
    "authors": [
      "Madhavun Candadai"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Information Theory (cs.IT)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.05186"
  },
  {
    "id": "arXiv:2106.05190",
    "title": "DPER: Efficient Parameter Estimation for Randomly Missing Data",
    "abstract": "The missing data problem has been broadly studied in the last few decades and\nhas various applications in different areas such as statistics or\nbioinformatics. Even though many methods have been developed to tackle this\nchallenge, most of those are imputation techniques that require multiple\niterations through the data before yielding convergence. In addition, such\napproaches may introduce extra biases and noises to the estimated parameters.\nIn this work, we propose novel algorithms to find the maximum likelihood\nestimates (MLEs) for a one-class/multiple-class randomly missing data set under\nsome mild assumptions. As the computation is direct without any imputation, our\nalgorithms do not require multiple iterations through the data, thus promising\nto be less time-consuming than other methods while maintaining superior\nestimation performance. We validate these claims by empirical results on\nvarious data sets of different sizes and release all codes in a GitHub\nrepository to contribute to the research community related to this problem.",
    "descriptor": "\nComments: 28 pages, 3 tables, 40 references\n",
    "authors": [
      "Thu Nguyen",
      "Khoi Minh Nguyen-Duy",
      "Duy Ho Minh Nguyen",
      "Binh T. Nguyen",
      "Bruce Alan Wade"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05190"
  },
  {
    "id": "arXiv:2106.05194",
    "title": "DIGRAC: Digraph Clustering with Flow Imbalance",
    "abstract": "Node clustering is a powerful tool in the analysis of networks. Here, we\nintroduce a graph neural network framework with a novel scalable Directed Mixed\nPath Aggregation(DIMPA) scheme to obtain node embeddings for directed networks\nin a self-supervised manner, including a novel probabilistic imbalance loss.\nThe method is end-to-end in combining embedding generation and clustering\nwithout an intermediate step. In contrast to standard approaches in the\nliterature, in this paper, directionality is not treated as a nuisance, but\nrather contains the main signal. In particular, we leverage the recently\nintroduced cut flow imbalance measure, which is tightly related to\ndirectionality; cut flow imbalance is optimized without resorting to spectral\nmethods or cluster labels. Experimental results on synthetic data, in the form\nof directed stochastic block models and real-world data at different scales,\ndemonstrate that our method attains state-of-the-art results on directed\nclustering, for a wide range of noise and sparsity levels, as well as graph\nstructures.",
    "descriptor": "\nComments: 33 pages (10 pages for main text)\n",
    "authors": [
      "Yixuan He",
      "Gesine Reinert",
      "Mihai Cucuringu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05194"
  },
  {
    "id": "arXiv:2106.05200",
    "title": "Independent mechanism analysis, a new concept?",
    "abstract": "Independent component analysis provides a principled framework for\nunsupervised representation learning, with solid theory on the identifiability\nof the latent code that generated the data, given only observations of mixtures\nthereof. Unfortunately, when the mixing is nonlinear, the model is provably\nnonidentifiable, since statistical independence alone does not sufficiently\nconstrain the problem. Identifiability can be recovered in settings where\nadditional, typically observed variables are included in the generative\nprocess. We investigate an alternative path and consider instead including\nassumptions reflecting the principle of independent causal mechanisms exploited\nin the field of causality. Specifically, our approach is motivated by thinking\nof each source as independently influencing the mixing process. This gives rise\nto a framework which we term independent mechanism analysis. We provide\ntheoretical and empirical evidence that our approach circumvents a number of\nnonidentifiability issues arising in nonlinear blind source separation.",
    "descriptor": "",
    "authors": [
      "Luigi Gresele",
      "Julius von K\u00fcgelgen",
      "Vincent Stimper",
      "Bernhard Sch\u00f6lkopf",
      "Michel Besserve"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05200"
  },
  {
    "id": "arXiv:2106.05206",
    "title": "Avoiding Traps in Nonconvex Problems",
    "abstract": "Iterative projection methods may become trapped at non-solutions when the\nconstraint sets are nonconvex. Two kinds of parameters are available to help\navoid this behavior and this study gives examples of both. The first kind of\nparameter, called a hyperparameter, includes any kind of parameter that appears\nin the definition of the iteration rule itself. The second kind comprises\nmetric parameters in the definition of the constraint sets, a feature that\narises when the problem to be solved has two or more kinds of variables.\nThrough examples we show the importance of properly tuning both kinds of\nparameters and offer heuristic interpretations of the observed behavior.",
    "descriptor": "",
    "authors": [
      "Sean Deyo",
      "Veit Elser"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05206"
  },
  {
    "id": "arXiv:2106.05214",
    "title": "Implicit field learning for unsupervised anomaly detection in medical  images",
    "abstract": "We propose a novel unsupervised out-of-distribution detection method for\nmedical images based on implicit fields image representations. In our approach,\nan auto-decoder feed-forward neural network learns the distribution of healthy\nimages in the form of a mapping between spatial coordinates and probabilities\nover a proxy for tissue types. At inference time, the learnt distribution is\nused to retrieve, from a given test image, a restoration, i.e. an image\nmaximally consistent with the input one but belonging to the healthy\ndistribution. Anomalies are localized using the voxel-wise probability\npredicted by our model for the restored image. We tested our approach in the\ntask of unsupervised localization of gliomas on brain MR images and compared it\nto several other VAE-based anomaly detection methods. Results show that the\nproposed technique substantially outperforms them (average DICE 0.640 vs 0.518\nfor the best performing VAE-based alternative) while also requiring\nconsiderably less computing time.",
    "descriptor": "\nComments: 10 pages, 3 figures. Accepted for publication in MICCAI 2021\n",
    "authors": [
      "Sergio Naval Marimont",
      "Giacomo Tarroni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05214"
  },
  {
    "id": "arXiv:2106.05241",
    "title": "Multi-Facet Clustering Variational Autoencoders",
    "abstract": "Work in deep clustering focuses on finding a single partition of data.\nHowever, high-dimensional data, such as images, typically feature multiple\ninteresting characteristics one could cluster over. For example, images of\nobjects against a background could be clustered over the shape of the object\nand separately by the colour of the background. In this paper, we introduce\nMulti-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of\nvariational autoencoders with a hierarchy of latent variables, each with a\nMixture-of-Gaussians prior, that learns multiple clusterings simultaneously,\nand is trained fully unsupervised and end-to-end. MFCVAE uses a\nprogressively-trained ladder architecture which leads to highly stable\nperformance. We provide novel theoretical results for optimising the ELBO\nanalytically with respect to the categorical variational posterior\ndistribution, and corrects earlier influential theoretical work. On image\nbenchmarks, we demonstrate that our approach separates out and clusters over\ndifferent aspects of the data in a disentangled manner. We also show other\nadvantages of our model: the compositionality of its latent space and that it\nprovides controlled generation of samples.",
    "descriptor": "\nComments: main text: 15 pages, appendices: 33 pages, 23 figures\n",
    "authors": [
      "Fabian Falck",
      "Haoting Zhang",
      "Matthew Willetts",
      "George Nicholson",
      "Christopher Yau",
      "Christopher C Holmes"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05241"
  },
  {
    "id": "arXiv:2106.05260",
    "title": "Sirius: A Mutual Information Tool for Exploratory Visualization of Mixed  Data",
    "abstract": "Data scientists across disciplines are increasingly in need of exploratory\nanalysis tools for data sets with a high volume of features. We expand upon\ngraph mining approaches for exploratory analysis of high-dimensional data to\nintroduce Sirius, a visualization package for researchers to explore feature\nrelationships among mixed data types using mutual information and network\nbackbone sparsification. Visualizations of feature relationships aid data\nscientists in finding meaningful dependence among features, which can engender\nfurther analysis for feature selection, feature extraction, projection,\nidentification of proxy variables, or insight into temporal variation at the\nmacro scale. Graph mining approaches for feature analysis exist, such as\nassociation networks of binary features, or correlation networks of\nquantitative features, but mixed data types present a unique challenge for\ndeveloping comprehensive feature networks for exploratory analysis. Using an\ninformation theoretic approach, Sirius supports heterogeneous data sets\nconsisting of binary, continuous quantitative, and discrete categorical data\ntypes, and provides a user interface exploring feature pairs with high mutual\ninformation scores. We leverage a backbone sparsification approach from network\ntheory as a dimensionality reduction technique, which probabilistically trims\nedges according to the local network context. Sirius is an open source Python\npackage and Django web application for exploratory visualization, which can be\ndeployed in data analysis pipelines. The Sirius codebase and exemplary data\nsets can be found at: https://github.com/compstorylab/sirius",
    "descriptor": "\nComments: 15 pages, 9 figures\n",
    "authors": [
      "Jane L. Adams",
      "Todd F. Deluca",
      "Christopher M. Danforth",
      "Peter S. Dodds",
      "Yuhang Zheng",
      "Konstantinos Anastasakis",
      "Boyoon Choi",
      "Allison Min",
      "Michael M. Bessey"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05260"
  },
  {
    "id": "arXiv:2106.05265",
    "title": "Efficient input placement for the optimal control of network moments",
    "abstract": "In this paper, we study the optimal control of the mean and variance of the\nnetwork state vector. We develop an algorithm to optimize the control input\nplacement subject to constraints on the state, which must be achieved at a\ngiven time threshold; seeking an input placement which moves the moment at\nminimum cost. First, we solve the state-selection problem for a number of\nvariants of the first and second moment, and find solutions related to the\neigenvalues of the systems' Gramian matrices. Our algorithm then uses this\ninformation to find a locally optimal input placement. This is a Generalization\nof the Projected Gradient Method (GPGM). We solve the problem for some common\nversions of these moments, including the mean state and versions of the second\nmoment which induce discord, repel from a certain state, or encourage\nconvergence. We then perform simulations, and discuss a measure of centrality\nbased on the system flux -- a measure which describes what nodes are most\nimportant to optimal control of the average state.",
    "descriptor": "",
    "authors": [
      "Philip Solimine",
      "Anke Meyer-Baese"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Social and Information Networks (cs.SI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05265"
  },
  {
    "id": "arXiv:1507.01101",
    "title": "Utility Optimal Thread Assignment and Resource Allocation in  Multi-Server Systems",
    "abstract": "Comments: 17 pages",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Pan Lai",
      "Rui Fan",
      "Xiao Zhang",
      "Wei Zhang",
      "Fang Liu",
      "Joey Tianyi Zhou"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/1507.01101"
  },
  {
    "id": "arXiv:1804.06679",
    "title": "Understanding Neural Networks and Individual Neuron Importance via  Information-Ordered Cumulative Ablation",
    "abstract": "Comments: 12 pages; accepted for publication in IEEE Transactions on Neural Networks and Learning Systems",
    "descriptor": "\nComments: 12 pages; accepted for publication in IEEE Transactions on Neural Networks and Learning Systems\n",
    "authors": [
      "Rana Ali Amjad",
      "Kairen Liu",
      "Bernhard C. Geiger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1804.06679"
  },
  {
    "id": "arXiv:1811.00606",
    "title": "DeepTileBars: Visualizing Term Distribution for Neural Information  Retrieval",
    "abstract": "DeepTileBars: Visualizing Term Distribution for Neural Information  Retrieval",
    "descriptor": "",
    "authors": [
      "Zhiwen Tang",
      "Grace Hui Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/1811.00606"
  },
  {
    "id": "arXiv:1905.13308",
    "title": "Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for  Investigating Learned Representations",
    "abstract": "Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for  Investigating Learned Representations",
    "descriptor": "",
    "authors": [
      "Jesse A. Livezey",
      "Ahyeon Hwang",
      "Jacob Yeung",
      "Kristofer E. Bouchard"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/1905.13308"
  },
  {
    "id": "arXiv:1907.13227",
    "title": "Compiling With Classical Connectives",
    "abstract": "Compiling With Classical Connectives",
    "descriptor": "",
    "authors": [
      "Paul Downen",
      "Zena M. Ariola"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/1907.13227"
  },
  {
    "id": "arXiv:1909.12038",
    "title": "Dimensionwise Separable 2-D Graph Convolution for Unsupervised and  Semi-Supervised Learning on Graphs",
    "abstract": "Comments: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21)",
    "descriptor": "\nComments: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21)\n",
    "authors": [
      "Qimai Li",
      "Xiaotong Zhang",
      "Han Liu",
      "Quanyu Dai",
      "Xiao-Ming Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1909.12038"
  },
  {
    "id": "arXiv:1910.06515",
    "title": "Numerically stable coded matrix computations via circulant and rotation  matrix embeddings",
    "abstract": "Comments: 39 pages, 3 figures",
    "descriptor": "\nComments: 39 pages, 3 figures\n",
    "authors": [
      "Aditya Ramamoorthy",
      "Li Tang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1910.06515"
  },
  {
    "id": "arXiv:1910.09417",
    "title": "Maximum Probability Theorem: A Framework for Probabilistic Learning",
    "abstract": "Maximum Probability Theorem: A Framework for Probabilistic Learning",
    "descriptor": "",
    "authors": [
      "Amir Emad Marvasti",
      "Ehsan Emad Marvasti",
      "Hassan Foroosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.09417"
  },
  {
    "id": "arXiv:1912.00009",
    "title": "MSTDP: A More Biologically Plausible Learning",
    "abstract": "Comments: 9 pages, 3 figures",
    "descriptor": "\nComments: 9 pages, 3 figures\n",
    "authors": [
      "Shiyuan Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1912.00009"
  },
  {
    "id": "arXiv:1912.00753",
    "title": "Corpus-Level End-to-End Exploration for Interactive Systems",
    "abstract": "Comments: Accepted into AAAI 2020",
    "descriptor": "\nComments: Accepted into AAAI 2020\n",
    "authors": [
      "Zhiwen Tang",
      "Grace Hui Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1912.00753"
  },
  {
    "id": "arXiv:1912.11444",
    "title": "An algorithm to evaluate the spectral expansion",
    "abstract": "An algorithm to evaluate the spectral expansion",
    "descriptor": "",
    "authors": [
      "Hau-Wen Huang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1912.11444"
  },
  {
    "id": "arXiv:2001.03919",
    "title": "Rethinking Class Relations: Absolute-relative Supervised and  Unsupervised Few-shot Learning",
    "abstract": "Comments: IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021",
    "descriptor": "\nComments: IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021\n",
    "authors": [
      "Hongguang Zhang",
      "Piotr Koniusz",
      "Songlei Jian",
      "Hongdong Li",
      "Philip H. S. Torr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2001.03919"
  },
  {
    "id": "arXiv:2001.05849",
    "title": "Application of Deep Learning in Generating Desired Design Options:  Experiments Using Synthetic Training Dataset",
    "abstract": "Comments: 10 pages, 12 figures, 1 table",
    "descriptor": "\nComments: 10 pages, 12 figures, 1 table\n",
    "authors": [
      "Zohreh Shaghaghian",
      "Wei Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2001.05849"
  },
  {
    "id": "arXiv:2002.01619",
    "title": "Monocular 3D Object Detection with Decoupled Structured Polygon  Estimation and Height-Guided Depth Estimation",
    "abstract": "Comments: 11 pages, 8 figures, AAAI2020",
    "descriptor": "\nComments: 11 pages, 8 figures, AAAI2020\n",
    "authors": [
      "Yingjie Cai",
      "Buyu Li",
      "Zeyu Jiao",
      "Hongsheng Li",
      "Xingyu Zeng",
      "Xiaogang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2002.01619"
  },
  {
    "id": "arXiv:2003.01176",
    "title": "Deep Survival Machines: Fully Parametric Survival Regression and  Representation Learning for Censored Data with Competing Risks",
    "abstract": "Comments: Also appeared in NeurIPS 2019 Workshop on Machine Learning for Healthcare (ML4H)",
    "descriptor": "\nComments: Also appeared in NeurIPS 2019 Workshop on Machine Learning for Healthcare (ML4H)\n",
    "authors": [
      "Chirag Nagpal",
      "Xinyu Rachel Li",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.01176"
  },
  {
    "id": "arXiv:2003.04938",
    "title": "A Mean-Field Game Approach to Equilibrium Pricing in Solar Renewable  Energy Certificate Markets",
    "abstract": "Comments: 40 pages, 8 figures",
    "descriptor": "\nComments: 40 pages, 8 figures\n",
    "authors": [
      "Arvind Shrivats",
      "Dena Firoozi",
      "Sebastian Jaimungal"
    ],
    "subjectives": [
      "Mathematical Finance (q-fin.MF)",
      "Theoretical Economics (econ.TH)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2003.04938"
  },
  {
    "id": "arXiv:2004.00101",
    "title": "Crowdsourced Labeling for Worker-Task Specialization Model",
    "abstract": "Comments: To appear at IEEE International Symposium on Information Theory (ISIT) 2021",
    "descriptor": "\nComments: To appear at IEEE International Symposium on Information Theory (ISIT) 2021\n",
    "authors": [
      "Doyeon Kim",
      "Hye Won Chung"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.00101"
  },
  {
    "id": "arXiv:2004.07141",
    "title": "From Understanding Genetic Drift to a Smart-Restart Parameter-less  Compact Genetic Algorithm",
    "abstract": "Comments: 4 figures. Extended version of a paper appearing at GECCO 2020",
    "descriptor": "\nComments: 4 figures. Extended version of a paper appearing at GECCO 2020\n",
    "authors": [
      "Benjamin Doerr",
      "Weijie Zheng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2004.07141"
  },
  {
    "id": "arXiv:2004.08694",
    "title": "Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation",
    "abstract": "Comments: Some of the results in the paper were incorrect",
    "descriptor": "\nComments: Some of the results in the paper were incorrect\n",
    "authors": [
      "Kaustubh D. Dhole",
      "Christopher D. Manning"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2004.08694"
  },
  {
    "id": "arXiv:2004.11827",
    "title": "A model reduction approach for inverse problems with operator valued  data",
    "abstract": "A model reduction approach for inverse problems with operator valued  data",
    "descriptor": "",
    "authors": [
      "J\u00fcrgen D\u00f6lz",
      "Herbert Egger",
      "Matthias Schlottbom"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2004.11827"
  },
  {
    "id": "arXiv:2005.02828",
    "title": "CS-TSSOS: Correlative and term sparsity for large-scale polynomial  optimization",
    "abstract": "Comments: 28 pages, 8 figures, 8 tables",
    "descriptor": "\nComments: 28 pages, 8 figures, 8 tables\n",
    "authors": [
      "Jie Wang",
      "Victor Magron",
      "Jean B. Lasserre",
      "Ngoc Hoang Anh Mai"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2005.02828"
  },
  {
    "id": "arXiv:2005.05376",
    "title": "Provably insecure group authentication: Not all security proofs are what  they claim to be",
    "abstract": "Comments: The previous versions of the paper contained an incorrect description of the ICICS 2019 scheme. This has been corrected. The attack has also been changed so that it applies to the correct version of the scheme. The main conclusions are unchanged",
    "descriptor": "\nComments: The previous versions of the paper contained an incorrect description of the ICICS 2019 scheme. This has been corrected. The attack has also been changed so that it applies to the correct version of the scheme. The main conclusions are unchanged\n",
    "authors": [
      "Chris J Mitchell"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2005.05376"
  },
  {
    "id": "arXiv:2005.12469",
    "title": "CARPe Posterum: A Convolutional Approach for Real-time Pedestrian Path  Prediction",
    "abstract": "Comments: AAAI-21 Camera Ready",
    "descriptor": "\nComments: AAAI-21 Camera Ready\n",
    "authors": [
      "Mat\u00edas Mendieta",
      "Hamed Tabkhi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2005.12469"
  },
  {
    "id": "arXiv:2006.01419",
    "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for  Sample-Efficient Exploration",
    "abstract": "Comments: Accepted to Proceedings of the 38th International Conference on Machine Learning",
    "descriptor": "\nComments: Accepted to Proceedings of the 38th International Conference on Machine Learning\n",
    "authors": [
      "Seungyul Han",
      "Youngchul Sung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.01419"
  },
  {
    "id": "arXiv:2006.03185",
    "title": "Balancing Reinforcement Learning Training Experiences in Interactive  Information Retrieval",
    "abstract": "Comments: Accepted by SIGIR 2020",
    "descriptor": "\nComments: Accepted by SIGIR 2020\n",
    "authors": [
      "Limin Chen",
      "Zhiwen Tang",
      "Grace Hui Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2006.03185"
  },
  {
    "id": "arXiv:2006.04088",
    "title": "An Efficient Framework for Clustered Federated Learning",
    "abstract": "Comments: Preliminary results appeared at NeurIPS 2020",
    "descriptor": "\nComments: Preliminary results appeared at NeurIPS 2020\n",
    "authors": [
      "Avishek Ghosh",
      "Jichan Chung",
      "Dong Yin",
      "Kannan Ramchandran"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.04088"
  },
  {
    "id": "arXiv:2006.04710",
    "title": "The Lipschitz Constant of Self-Attention",
    "abstract": "The Lipschitz Constant of Self-Attention",
    "descriptor": "",
    "authors": [
      "Hyunjik Kim",
      "George Papamakarios",
      "Andriy Mnih"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.04710"
  },
  {
    "id": "arXiv:2006.07311",
    "title": "Predicting cell phone adoption metrics using satellite imagery",
    "abstract": "Predicting cell phone adoption metrics using satellite imagery",
    "descriptor": "",
    "authors": [
      "Edward J. Oughton",
      "Jatin Mathur"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2006.07311"
  },
  {
    "id": "arXiv:2006.08573",
    "title": "Neural Ensemble Search for Uncertainty Estimation and Dataset Shift",
    "abstract": "Comments: Preliminary version of this work was accepted for oral presentation at ICML 2020 Workshop on Uncertainty & Robustness in Deep Learning",
    "descriptor": "\nComments: Preliminary version of this work was accepted for oral presentation at ICML 2020 Workshop on Uncertainty & Robustness in Deep Learning\n",
    "authors": [
      "Sheheryar Zaidi",
      "Arber Zela",
      "Thomas Elsken",
      "Chris Holmes",
      "Frank Hutter",
      "Yee Whye Teh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.08573"
  },
  {
    "id": "arXiv:2006.10259",
    "title": "On Path Integration of Grid Cells: Group Representation and Isotropic  Scaling",
    "abstract": "On Path Integration of Grid Cells: Group Representation and Isotropic  Scaling",
    "descriptor": "",
    "authors": [
      "Ruiqi Gao",
      "Jianwen Xie",
      "Xue-Xin Wei",
      "Song-Chun Zhu",
      "Ying Nian Wu"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.10259"
  },
  {
    "id": "arXiv:2006.10412",
    "title": "Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning",
    "abstract": "Comments: Published in the 38th International Conference on Machine Learning (ICML 2021)",
    "descriptor": "\nComments: Published in the 38th International Conference on Machine Learning (ICML 2021)\n",
    "authors": [
      "Arrasy Rahman",
      "Niklas H\u00f6pner",
      "Filippos Christianos",
      "Stefano V. Albrecht"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.10412"
  },
  {
    "id": "arXiv:2006.12135",
    "title": "Learning to Generate Noise for Multi-Attack Robustness",
    "abstract": "Comments: Accepted to ICML 2021. Code available at this https URL",
    "descriptor": "\nComments: Accepted to ICML 2021. Code available at this https URL\n",
    "authors": [
      "Divyam Madaan",
      "Jinwoo Shin",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.12135"
  },
  {
    "id": "arXiv:2006.14222",
    "title": "Stochastic Subset Selection for Efficient Training and Inference of  Neural Networks",
    "abstract": "Comments: 19 pages",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Bruno Andreis",
      "A. Tuan Nguyen",
      "Seanie Lee",
      "Juho Lee",
      "Eunho Yang",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.14222"
  },
  {
    "id": "arXiv:2006.14378",
    "title": "A Canonical Transform for Strengthening the Local $L^p$-Type Universal  Approximation Property",
    "abstract": "Comments: 8 pages + 12 page appendix",
    "descriptor": "\nComments: 8 pages + 12 page appendix\n",
    "authors": [
      "Anastasis Kratsios",
      "Behnoosh Zamanlooy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Functional Analysis (math.FA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.14378"
  },
  {
    "id": "arXiv:2007.01755",
    "title": "Learning to Discover Multi-Class Attentional Regions for Multi-Label  Image Recognition",
    "abstract": "Comments: 13 pages, Accepted by IEEE TIP (5-Jun-2021)",
    "descriptor": "\nComments: 13 pages, Accepted by IEEE TIP (5-Jun-2021)\n",
    "authors": [
      "Bin-Bin Gao",
      "Hong-Yu Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.01755"
  },
  {
    "id": "arXiv:2007.04833",
    "title": "Towards Open-World Recommendation: An Inductive Model-based  Collaborative Filtering Approach",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Qitian Wu",
      "Hengrui Zhang",
      "Xiaofeng Gao",
      "Junchi Yan",
      "Hongyuan Zha"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.04833"
  },
  {
    "id": "arXiv:2007.06172",
    "title": "Bottom-up mechanism and improved contract net protocol for the dynamic  task planning of heterogeneous Earth observation resources",
    "abstract": "Comments: 14 pages, 11 figures.This work has been submitted to the IEEE for possible publication",
    "descriptor": "\nComments: 14 pages, 11 figures.This work has been submitted to the IEEE for possible publication\n",
    "authors": [
      "Baoju Liu",
      "Min Deng",
      "Guohua Wu",
      "Xinyu Pei",
      "Haifeng Li",
      "Witold Pedrycz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2007.06172"
  },
  {
    "id": "arXiv:2007.08445",
    "title": "Hierarchical Interaction Networks with Rethinking Mechanism for  Document-level Sentiment Analysis",
    "abstract": "Comments: 17 pages, accepted by ECML-PKDD 2020",
    "descriptor": "\nComments: 17 pages, accepted by ECML-PKDD 2020\n",
    "authors": [
      "Lingwei Wei",
      "Dou Hu",
      "Wei Zhou",
      "Xuehai Tang",
      "Xiaodan Zhang",
      "Xin Wang",
      "Jizhong Han",
      "Songlin Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2007.08445"
  },
  {
    "id": "arXiv:2008.01801",
    "title": "On the Sobolev and $L^p$-Stability of the $L^2$-projection",
    "abstract": "On the Sobolev and $L^p$-Stability of the $L^2$-projection",
    "descriptor": "",
    "authors": [
      "Lars Diening",
      "Johannes Storn",
      "Tabea Tscherpel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2008.01801"
  },
  {
    "id": "arXiv:2008.03130",
    "title": "Convolutional Complex Knowledge Graph Embeddings",
    "abstract": "Convolutional Complex Knowledge Graph Embeddings",
    "descriptor": "",
    "authors": [
      "Caglar Demir",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.03130"
  },
  {
    "id": "arXiv:2008.07063",
    "title": "To Bag is to Prune",
    "abstract": "Comments: added references; corrected typos; added NN discussions and results, new experiments and discussion on double descent",
    "descriptor": "\nComments: added references; corrected typos; added NN discussions and results, new experiments and discussion on double descent\n",
    "authors": [
      "Philippe Goulet Coulombe"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2008.07063"
  },
  {
    "id": "arXiv:2009.01673",
    "title": "Approximate Generalized Inverses with Iterative Refinement for  $\u03b5$-Accurate Preconditioning of Singular Systems",
    "abstract": "Comments: Submitted to SIAM Journal on Matrix Analysis and Applications (SIMAX); fixed minor errors in the theorems, propositions, etc.; fixed broken ref",
    "descriptor": "\nComments: Submitted to SIAM Journal on Matrix Analysis and Applications (SIMAX); fixed minor errors in the theorems, propositions, etc.; fixed broken ref\n",
    "authors": [
      "Xiangmin Jiao",
      "Qiao Chen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2009.01673"
  },
  {
    "id": "arXiv:2009.02934",
    "title": "On prefix palindromic length of automatic words",
    "abstract": "Comments: revised version, to appear in Theoret. Comput. Sci",
    "descriptor": "\nComments: revised version, to appear in Theoret. Comput. Sci\n",
    "authors": [
      "Anna E. Frid",
      "Enzo Laborde",
      "Jarkko Peltom\u00e4ki"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2009.02934"
  },
  {
    "id": "arXiv:2009.04861",
    "title": "Massively Parallel and Asynchronous Tsetlin Machine Architecture  Supporting Almost Constant-Time Scaling",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "K. Darshana Abeyrathna",
      "Bimal Bhattarai",
      "Morten Goodwin",
      "Saeed Gorji",
      "Ole-Christoffer Granmo",
      "Lei Jiao",
      "Rupsa Saha",
      "Rohan K. Yadav"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.04861"
  },
  {
    "id": "arXiv:2009.05683",
    "title": "MACE: A Flexible Framework for Membership Privacy Estimation in  Generative Models",
    "abstract": "MACE: A Flexible Framework for Membership Privacy Estimation in  Generative Models",
    "descriptor": "",
    "authors": [
      "Xiyang Liu",
      "Yixi Xu",
      "Shruti Tople",
      "Sumit Mukherjee",
      "Juan Lavista Ferres"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.05683"
  },
  {
    "id": "arXiv:2009.13139",
    "title": "Preventing pressure oscillations does not fix local linear stability  issues of entropy-based split-form high-order schemes",
    "abstract": "Preventing pressure oscillations does not fix local linear stability  issues of entropy-based split-form high-order schemes",
    "descriptor": "",
    "authors": [
      "Hendrik Ranocha",
      "Gregor J Gassner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2009.13139"
  },
  {
    "id": "arXiv:2009.14404",
    "title": "Learning to Reflect and to Beamform for Intelligent Reflecting Surface  with Implicit Channel Estimation",
    "abstract": "Comments: To appear in IEEE Journal of Selected Areas in Communications",
    "descriptor": "\nComments: To appear in IEEE Journal of Selected Areas in Communications\n",
    "authors": [
      "Tao Jiang",
      "Hei Victor Cheng",
      "Wei Yu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2009.14404"
  },
  {
    "id": "arXiv:2010.02810",
    "title": "Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech  to Standard German Text Corpus",
    "abstract": "Comments: 8 pages, 0 figures",
    "descriptor": "\nComments: 8 pages, 0 figures\n",
    "authors": [
      "Michel Pl\u00fcss",
      "Lukas Neukom",
      "Christian Scheller",
      "Manfred Vogel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.02810"
  },
  {
    "id": "arXiv:2010.03110",
    "title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for  Causal Representation Learning",
    "abstract": "Comments: International Conference on Machine Learning, PMLR 139, 2021",
    "descriptor": "\nComments: International Conference on Machine Learning, PMLR 139, 2021\n",
    "authors": [
      "Sumedh A. Sontakke",
      "Arash Mehrjou",
      "Laurent Itti",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2010.03110"
  },
  {
    "id": "arXiv:2010.03792",
    "title": "The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive  Experiments and a Paradox Concerning Logging Policy",
    "abstract": "The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive  Experiments and a Paradox Concerning Logging Policy",
    "descriptor": "",
    "authors": [
      "Masahiro Kato",
      "Shota Yasui",
      "Kenichiro McAlinn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.03792"
  },
  {
    "id": "arXiv:2010.03957",
    "title": "Transformers for Modeling Physical Systems",
    "abstract": "Comments: 22 pages, 14 figures, 3 appendices",
    "descriptor": "\nComments: 22 pages, 14 figures, 3 appendices\n",
    "authors": [
      "Nicholas Geneva",
      "Nicholas Zabaras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2010.03957"
  },
  {
    "id": "arXiv:2010.05170",
    "title": "What causes the test error? Going beyond bias-variance via ANOVA",
    "abstract": "What causes the test error? Going beyond bias-variance via ANOVA",
    "descriptor": "",
    "authors": [
      "Licong Lin",
      "Edgar Dobriban"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2010.05170"
  },
  {
    "id": "arXiv:2010.06217",
    "title": "TM-NET: Deep Generative Networks for Textured Meshes",
    "abstract": "TM-NET: Deep Generative Networks for Textured Meshes",
    "descriptor": "",
    "authors": [
      "Lin Gao",
      "Tong Wu",
      "Yu-Jie Yuan",
      "Ming-Xian Lin",
      "Yu-Kun Lai",
      "Hao Zhang"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2010.06217"
  },
  {
    "id": "arXiv:2010.07139",
    "title": "Fairness for Freshness: Optimal Age of Information Based OFDMA  Scheduling with Minimal Knowledge",
    "abstract": "Comments: Accepted on 05.06.2021 by the IEEE Transactions on Wireless Communications for publication",
    "descriptor": "\nComments: Accepted on 05.06.2021 by the IEEE Transactions on Wireless Communications for publication\n",
    "authors": [
      "Bin Han",
      "Yao Zhu",
      "Zhiyuan Jiang",
      "Muxia Sun",
      "Hans D. Schotten"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2010.07139"
  },
  {
    "id": "arXiv:2010.08925",
    "title": "Implementing Agent-Based Systems via Computability Logic CL2",
    "abstract": "Comments: 11 pages. This is a revised version and some errors are fixed. arXiv admin note: substantial text overlap with arXiv:1909.07036",
    "descriptor": "\nComments: 11 pages. This is a revised version and some errors are fixed. arXiv admin note: substantial text overlap with arXiv:1909.07036\n",
    "authors": [
      "Keehang Kwon"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2010.08925"
  },
  {
    "id": "arXiv:2010.11038",
    "title": "Influence-Augmented Online Planning for Complex Environments",
    "abstract": "Comments: NeurIPS2020 - results have been updated after fixing minor bugs in the code",
    "descriptor": "\nComments: NeurIPS2020 - results have been updated after fixing minor bugs in the code\n",
    "authors": [
      "Jinke He",
      "Miguel Suau",
      "Frans A. Oliehoek"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.11038"
  },
  {
    "id": "arXiv:2010.12951",
    "title": "Y-Vector: Multiscale Waveform Encoder for Speaker Embedding",
    "abstract": "Comments: Accepted by Interspeech 2021",
    "descriptor": "\nComments: Accepted by Interspeech 2021\n",
    "authors": [
      "Ge Zhu",
      "Fei Jiang",
      "Zhiyao Duan"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2010.12951"
  },
  {
    "id": "arXiv:2010.13632",
    "title": "Black-box density function estimation using recursive partitioning",
    "abstract": "Comments: International Conference on Machine Learning (ICML) 2021",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Erik Bodin",
      "Zhenwen Dai",
      "Neill D. F. Campbell",
      "Carl Henrik Ek"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.13632"
  },
  {
    "id": "arXiv:2011.02511",
    "title": "Offline Reinforcement Learning from Human Feedback in Real-World  Sequence-to-Sequence Tasks",
    "abstract": "Comments: 5th Workshop on Structured Prediction for NLP at ACL 2021 Previously named \"Learning from Human Feedback: Challenges for Real-World Reinforcement Learning in NLP\" and presented at Challenges of Real-World RL Workshop at NeurIPS 2020",
    "descriptor": "\nComments: 5th Workshop on Structured Prediction for NLP at ACL 2021 Previously named \"Learning from Human Feedback: Challenges for Real-World Reinforcement Learning in NLP\" and presented at Challenges of Real-World RL Workshop at NeurIPS 2020\n",
    "authors": [
      "Julia Kreutzer",
      "Stefan Riezler",
      "Carolin Lawrence"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.02511"
  },
  {
    "id": "arXiv:2011.06455",
    "title": "Optimal governance and implementation of vaccination programmes to  contain the COVID-19 pandemic",
    "abstract": "Comments: 15 pages, 1 figure; published in Royal Society Open Science",
    "descriptor": "\nComments: 15 pages, 1 figure; published in Royal Society Open Science\n",
    "authors": [
      "Mahendra Piraveenan",
      "Shailendra Sawleshwarkar",
      "Michael Walsh",
      "Iryna Zablotska",
      "Samit Bhattacharyya",
      "Habib Hassan Farooqui",
      "Tarun Bhatnagar",
      "Anup Karan",
      "Manoj Murhekar",
      "Sanjay Zodpey",
      "K. S. Mallikarjuna Rao",
      "Philippa Pattison",
      "Albert Zomaya",
      "Matjaz Perc"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Physics and Society (physics.soc-ph)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2011.06455"
  },
  {
    "id": "arXiv:2011.07248",
    "title": "Self Normalizing Flows",
    "abstract": "Self Normalizing Flows",
    "descriptor": "",
    "authors": [
      "T. Anderson Keller",
      "Jorn W.T. Peters",
      "Priyank Jaini",
      "Emiel Hoogeboom",
      "Patrick Forr\u00e9",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.07248"
  },
  {
    "id": "arXiv:2011.09756",
    "title": "Active Inference and Behavior Trees for Reactive Action Planning and  Execution in Robotics",
    "abstract": "Active Inference and Behavior Trees for Reactive Action Planning and  Execution in Robotics",
    "descriptor": "",
    "authors": [
      "Corrado Pezzato",
      "Carlos Hernandez",
      "Stefan Bonhof",
      "Martijn Wisse"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.09756"
  },
  {
    "id": "arXiv:2011.09769",
    "title": "Data-Driven Robust Optimization using Unsupervised Deep Learning",
    "abstract": "Data-Driven Robust Optimization using Unsupervised Deep Learning",
    "descriptor": "",
    "authors": [
      "Marc Goerigk",
      "Jannis Kurtz"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.09769"
  },
  {
    "id": "arXiv:2011.09996",
    "title": "A Stable High-order Tuner for General Convex Functions",
    "abstract": "Comments: 6 pages, 3 figures. This work has been accepted for publication at IEEE Control Systems Letters",
    "descriptor": "\nComments: 6 pages, 3 figures. This work has been accepted for publication at IEEE Control Systems Letters\n",
    "authors": [
      "Jos\u00e9 M. Moreu",
      "Anuradha M. Annaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2011.09996"
  },
  {
    "id": "arXiv:2011.11844",
    "title": "Densely connected multidilated convolutional networks for dense  prediction tasks",
    "abstract": "Comments: Accepted to CVPR 2021. arXiv admin note: text overlap with arXiv:2010.01733",
    "descriptor": "\nComments: Accepted to CVPR 2021. arXiv admin note: text overlap with arXiv:2010.01733\n",
    "authors": [
      "Naoya Takahashi",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.11844"
  },
  {
    "id": "arXiv:2011.11884",
    "title": "SMG: A Shuffling Gradient-Based Method with Momentum",
    "abstract": "Comments: The 38th International Conference on Machine Learning (ICML 2021)",
    "descriptor": "\nComments: The 38th International Conference on Machine Learning (ICML 2021)\n",
    "authors": [
      "Trang H. Tran",
      "Lam M. Nguyen",
      "Quoc Tran-Dinh"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.11884"
  },
  {
    "id": "arXiv:2011.12663",
    "title": "Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval",
    "abstract": "Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval",
    "descriptor": "",
    "authors": [
      "Frederik Warburg",
      "Martin J\u00f8rgensen",
      "Javier Civera",
      "S\u00f8ren Hauberg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.12663"
  },
  {
    "id": "arXiv:2011.12902",
    "title": "Adversarial Evaluation of Multimodal Models under Realistic Gray Box  Assumption",
    "abstract": "Adversarial Evaluation of Multimodal Models under Realistic Gray Box  Assumption",
    "descriptor": "",
    "authors": [
      "Ivan Evtimov",
      "Russel Howes",
      "Brian Dolhansky",
      "Hamed Firooz",
      "Cristian Canton Ferrer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2011.12902"
  },
  {
    "id": "arXiv:2012.00567",
    "title": "Boosting Adversarial Attacks on Neural Networks with Better Optimizer",
    "abstract": "Boosting Adversarial Attacks on Neural Networks with Better Optimizer",
    "descriptor": "",
    "authors": [
      "Heng Yin",
      "Hengwei Zhang",
      "Jindong Wang",
      "Ruiyu Dou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.00567"
  },
  {
    "id": "arXiv:2012.01721",
    "title": "Learning Class-Transductive Intent Representations for Zero-shot Intent  Detection",
    "abstract": "Comments: IJCAI-2021",
    "descriptor": "\nComments: IJCAI-2021\n",
    "authors": [
      "Qingyi Si",
      "Yuanxin Liu",
      "Peng Fu",
      "Zheng Lin",
      "Jiangnan Li",
      "Weiping Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01721"
  },
  {
    "id": "arXiv:2012.04830",
    "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic  Imaging Modalities: A Survey",
    "abstract": "Comments: 15 pages, 13 figures",
    "descriptor": "\nComments: 15 pages, 13 figures\n",
    "authors": [
      "Xiaoqing Zhang",
      "Yan Hu",
      "Jiansheng Fang",
      "Yanwu Xu",
      "Risa Higashita",
      "Jiang Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.04830"
  },
  {
    "id": "arXiv:2012.05628",
    "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models  for Other Languages",
    "abstract": "Comments: Findings of ACL 2021 Camera Ready",
    "descriptor": "\nComments: Findings of ACL 2021 Camera Ready\n",
    "authors": [
      "Wietse de Vries",
      "Malvina Nissim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.05628"
  },
  {
    "id": "arXiv:2012.07463",
    "title": "Parameter-Efficient Transfer Learning with Diff Pruning",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Demi Guo",
      "Alexander M. Rush",
      "Yoon Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.07463"
  },
  {
    "id": "arXiv:2012.11654",
    "title": "Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for  Deep ReLU Networks",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Quynh Nguyen",
      "Marco Mondelli",
      "Guido Montufar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.11654"
  },
  {
    "id": "arXiv:2012.12109",
    "title": "Enhance Convolutional Neural Networks with Noise Incentive Block",
    "abstract": "Enhance Convolutional Neural Networks with Noise Incentive Block",
    "descriptor": "",
    "authors": [
      "Menghan Xia",
      "Yi Wang",
      "Chu Han",
      "Tien-Tsin Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.12109"
  },
  {
    "id": "arXiv:2012.14210",
    "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index  Sizes",
    "abstract": "Comments: Published at ACL 2021",
    "descriptor": "\nComments: Published at ACL 2021\n",
    "authors": [
      "Nils Reimers",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.14210"
  },
  {
    "id": "arXiv:2012.15573",
    "title": "Coreference Reasoning in Machine Reading Comprehension",
    "abstract": "Comments: Accepted at ACL-IJCNLP 2021",
    "descriptor": "\nComments: Accepted at ACL-IJCNLP 2021\n",
    "authors": [
      "Mingzhu Wu",
      "Nafise Sadat Moosavi",
      "Dan Roth",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.15573"
  },
  {
    "id": "arXiv:2012.15671",
    "title": "Vocabulary Learning via Optimal Transport for Machine Translation",
    "abstract": "Comments: Accepted by ACL 2021",
    "descriptor": "\nComments: Accepted by ACL 2021\n",
    "authors": [
      "Jingjing Xu",
      "Hao Zhou",
      "Chun Gan",
      "Zaixiang Zheng",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.15671"
  },
  {
    "id": "arXiv:2101.00379",
    "title": "Investigating Memorization of Conspiracy Theories in Text Generation",
    "abstract": "Comments: ACL 2021 Findings",
    "descriptor": "\nComments: ACL 2021 Findings\n",
    "authors": [
      "Sharon Levy",
      "Michael Saxon",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2101.00379"
  },
  {
    "id": "arXiv:2101.01404",
    "title": "Domain Generalization for Document Authentication against Practical  Recapturing Attacks",
    "abstract": "Comments: 13 pages, 14 figures, 8 tables",
    "descriptor": "\nComments: 13 pages, 14 figures, 8 tables\n",
    "authors": [
      "Changsheng Chen",
      "Shuzheng Zhang",
      "Fengbo Lan",
      "Jiwu Huang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2101.01404"
  },
  {
    "id": "arXiv:2101.01827",
    "title": "On the Computational Complexity of the Secure State-Reconstruction  Problem",
    "abstract": "On the Computational Complexity of the Secure State-Reconstruction  Problem",
    "descriptor": "",
    "authors": [
      "Yanwen Mao",
      "Aritra Mitra",
      "Shreyas Sundaram",
      "Paulo Tabuada"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2101.01827"
  },
  {
    "id": "arXiv:2101.02404",
    "title": "Modeling massive highly-multivariate nonstationary spatial data with the  basis graphical lasso",
    "abstract": "Modeling massive highly-multivariate nonstationary spatial data with the  basis graphical lasso",
    "descriptor": "",
    "authors": [
      "Mitchell Krock",
      "William Kleiber",
      "Dorit Hammerling",
      "Stephen Becker"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.02404"
  },
  {
    "id": "arXiv:2101.02555",
    "title": "Explainable AI and Adoption of Financial Algorithmic Advisors: an  Experimental Study",
    "abstract": "Comments: accepted: AIES '21",
    "descriptor": "\nComments: accepted: AIES '21\n",
    "authors": [
      "Daniel Ben David",
      "Yehezkel S. Resheff",
      "Talia Tron"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.02555"
  },
  {
    "id": "arXiv:2101.04702",
    "title": "Cross-Modal Contrastive Learning for Text-to-Image Generation",
    "abstract": "Comments: CVPR 2021",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Han Zhang",
      "Jing Yu Koh",
      "Jason Baldridge",
      "Honglak Lee",
      "Yinfei Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.04702"
  },
  {
    "id": "arXiv:2101.05917",
    "title": "DiffPD: Differentiable Projective Dynamics",
    "abstract": "DiffPD: Differentiable Projective Dynamics",
    "descriptor": "",
    "authors": [
      "Tao Du",
      "Kui Wu",
      "Pingchuan Ma",
      "Sebastien Wah",
      "Andrew Spielberg",
      "Daniela Rus",
      "Wojciech Matusik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2101.05917"
  },
  {
    "id": "arXiv:2101.06778",
    "title": "A Safe Hierarchical Planning Framework for Complex Driving Scenarios  based on Reinforcement Learning",
    "abstract": "A Safe Hierarchical Planning Framework for Complex Driving Scenarios  based on Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Jinning Li",
      "Liting Sun",
      "Jianyu Chen",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2101.06778"
  },
  {
    "id": "arXiv:2101.07560",
    "title": "A doubly relaxed minimal-norm Gauss-Newton method for underdetermined  nonlinear least-squares problems",
    "abstract": "A doubly relaxed minimal-norm Gauss-Newton method for underdetermined  nonlinear least-squares problems",
    "descriptor": "",
    "authors": [
      "Federica Pes",
      "Giuseppe Rodriguez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2101.07560"
  },
  {
    "id": "arXiv:2101.08539",
    "title": "Orthogonal Least Squares Based Fast Feature Selection for Linear  Classification",
    "abstract": "Orthogonal Least Squares Based Fast Feature Selection for Linear  Classification",
    "descriptor": "",
    "authors": [
      "Sikai Zhang",
      "Zi-Qiang Lang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.08539"
  },
  {
    "id": "arXiv:2101.08578",
    "title": "MLPF: Efficient machine-learned particle-flow reconstruction using graph  neural networks",
    "abstract": "Comments: 15 pages, 10 figures",
    "descriptor": "\nComments: 15 pages, 10 figures\n",
    "authors": [
      "Joosep Pata",
      "Javier Duarte",
      "Jean-Roch Vlimant",
      "Maurizio Pierini",
      "Maria Spiropulu"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.08578"
  },
  {
    "id": "arXiv:2101.11228",
    "title": "GaitGraph: Graph Convolutional Network for Skeleton-Based Gait  Recognition",
    "abstract": "Comments: 5 pages, 2 figures",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Torben Teepe",
      "Ali Khan",
      "Johannes Gilg",
      "Fabian Herzog",
      "Stefan H\u00f6rmann",
      "Gerhard Rigoll"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.11228"
  },
  {
    "id": "arXiv:2101.11500",
    "title": "A Deterministic Algorithm for the Discrete Logarithm Problem in a  Semigroup",
    "abstract": "A Deterministic Algorithm for the Discrete Logarithm Problem in a  Semigroup",
    "descriptor": "",
    "authors": [
      "Simran Tinani",
      "Joachim Rosenthal"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/2101.11500"
  },
  {
    "id": "arXiv:2102.00223",
    "title": "Performance Measurements within Asynchronous Task-based Runtime Systems:  A Double White Dwarf Merger as an Application",
    "abstract": "Performance Measurements within Asynchronous Task-based Runtime Systems:  A Double White Dwarf Merger as an Application",
    "descriptor": "",
    "authors": [
      "Patrick Diehl",
      "Dominic Marcello",
      "Parsa Amini",
      "Hartmut Kaiser",
      "Sagiv Shiber",
      "Geoffrey C. Clayton",
      "Juhan Frank",
      "Gregor Dai\u00df",
      "Dirk Pfl\u00fcger",
      "David Eder",
      "Alice Koniges",
      "Kevin Huck"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2102.00223"
  },
  {
    "id": "arXiv:2102.01623",
    "title": "Adversarial Tracking Control via Strongly Adaptive Online Learning with  Memory",
    "abstract": "Adversarial Tracking Control via Strongly Adaptive Online Learning with  Memory",
    "descriptor": "",
    "authors": [
      "Zhiyu Zhang",
      "Ashok Cutkosky",
      "Ioannis Ch. Paschalidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.01623"
  },
  {
    "id": "arXiv:2102.03509",
    "title": "Robust normalizing flows using Bernstein-type polynomials",
    "abstract": "Robust normalizing flows using Bernstein-type polynomials",
    "descriptor": "",
    "authors": [
      "Sameera Ramasinghe",
      "Kasun Fernando",
      "Salman Khan",
      "Nick Barnes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.03509"
  },
  {
    "id": "arXiv:2102.03583",
    "title": "Algorithms for Linearly Recurrent Sequences of Truncated Polynomials",
    "abstract": "Comments: 8 pages, ISSAC 2021",
    "descriptor": "\nComments: 8 pages, ISSAC 2021\n",
    "authors": [
      "Seung Gyu Hyun",
      "Vincent Neiger",
      "\u00c9ric Schost"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2102.03583"
  },
  {
    "id": "arXiv:2102.03786",
    "title": "EMA2S: An End-to-End Multimodal Articulatory-to-Speech System",
    "abstract": "EMA2S: An End-to-End Multimodal Articulatory-to-Speech System",
    "descriptor": "",
    "authors": [
      "Yu-Wen Chen",
      "Kuo-Hsuan Hung",
      "Shang-Yi Chuang",
      "Jonathan Sherman",
      "Wen-Chin Huang",
      "Xugang Lu",
      "Yu Tsao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2102.03786"
  },
  {
    "id": "arXiv:2102.04500",
    "title": "Learning Diagonal Gaussian Mixture Models and Incomplete Tensor  Decompositions",
    "abstract": "Comments: 24 pages",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Bingni Guo",
      "Jiawang Nie",
      "Zi Yang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2102.04500"
  },
  {
    "id": "arXiv:2102.05095",
    "title": "Is Space-Time Attention All You Need for Video Understanding?",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Gedas Bertasius",
      "Heng Wang",
      "Lorenzo Torresani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.05095"
  },
  {
    "id": "arXiv:2102.05509",
    "title": "Robustness in Compressed Neural Networks for Object Detection",
    "abstract": "Comments: IJCNN 2021",
    "descriptor": "\nComments: IJCNN 2021\n",
    "authors": [
      "Sebastian Cygert",
      "Andrzej Czy\u017cewski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.05509"
  },
  {
    "id": "arXiv:2102.06986",
    "title": "How Framelets Enhance Graph Neural Networks",
    "abstract": "Comments: 24 pages, 17 figures, 8 tables, ICML2021",
    "descriptor": "\nComments: 24 pages, 17 figures, 8 tables, ICML2021\n",
    "authors": [
      "Xuebin Zheng",
      "Bingxin Zhou",
      "Junbin Gao",
      "Yu Guang Wang",
      "Pietro Lio",
      "Ming Li",
      "Guido Montufar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2102.06986"
  },
  {
    "id": "arXiv:2102.07376",
    "title": "On the Rate of Error Growth in Time for Numerical Solutions of Nonlinear  Dispersive Wave Equations",
    "abstract": "On the Rate of Error Growth in Time for Numerical Solutions of Nonlinear  Dispersive Wave Equations",
    "descriptor": "",
    "authors": [
      "Hendrik Ranocha",
      "Manuel Quezada de Luna",
      "David I. Ketcheson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2102.07376"
  },
  {
    "id": "arXiv:2102.07541",
    "title": "WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points",
    "abstract": "Comments: Published at ICML 2021",
    "descriptor": "\nComments: Published at ICML 2021\n",
    "authors": [
      "Albert No",
      "TaeHo Yoon",
      "Sehyun Kwon",
      "Ernest K. Ryu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.07541"
  },
  {
    "id": "arXiv:2102.07686",
    "title": "Does the Adam Optimizer Exacerbate Catastrophic Forgetting?",
    "abstract": "Comments: 9 pages in main text + 3 pages of references + 16 pages of appendices, 6 figures in main text + 21 figures in appendices, 6 tables in appendices; source code available at this https URL",
    "descriptor": "\nComments: 9 pages in main text + 3 pages of references + 16 pages of appendices, 6 figures in main text + 21 figures in appendices, 6 tables in appendices; source code available at this https URL\n",
    "authors": [
      "Dylan R. Ashley",
      "Sina Ghiassian",
      "Richard S. Sutton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.07686"
  },
  {
    "id": "arXiv:2102.07826",
    "title": "Controlling False Discovery Rates under Cross-Sectional Correlations",
    "abstract": "Controlling False Discovery Rates under Cross-Sectional Correlations",
    "descriptor": "",
    "authors": [
      "Junpei Komiyama",
      "Masaya Abe",
      "Kei Nakagawa",
      "Kenichiro McAlinn"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.07826"
  },
  {
    "id": "arXiv:2102.08329",
    "title": "Rate-Distortion Theoretic Model Compression: Successive Refinement for  Pruning",
    "abstract": "Comments: 26 pages, 9 figures, 9 tables",
    "descriptor": "\nComments: 26 pages, 9 figures, 9 tables\n",
    "authors": [
      "Berivan Isik",
      "Albert No",
      "Tsachy Weissman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.08329"
  },
  {
    "id": "arXiv:2102.09972",
    "title": "Implicit Regularization in Tensor Factorization",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Noam Razin",
      "Asaf Maman",
      "Nadav Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.09972"
  },
  {
    "id": "arXiv:2102.10472",
    "title": "Learning Neural Network Subspaces",
    "abstract": "Learning Neural Network Subspaces",
    "descriptor": "",
    "authors": [
      "Mitchell Wortsman",
      "Maxwell Horton",
      "Carlos Guestrin",
      "Ali Farhadi",
      "Mohammad Rastegari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.10472"
  },
  {
    "id": "arXiv:2102.11174",
    "title": "Linear Transformers Are Secretly Fast Weight Programmers",
    "abstract": "Linear Transformers Are Secretly Fast Weight Programmers",
    "descriptor": "",
    "authors": [
      "Imanol Schlag",
      "Kazuki Irie",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11174"
  },
  {
    "id": "arXiv:2102.11409",
    "title": "On Feature Collapse and Deep Kernel Learning for Single Forward Pass  Uncertainty",
    "abstract": "On Feature Collapse and Deep Kernel Learning for Single Forward Pass  Uncertainty",
    "descriptor": "",
    "authors": [
      "Joost van Amersfoort",
      "Lewis Smith",
      "Andrew Jesson",
      "Oscar Key",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11409"
  },
  {
    "id": "arXiv:2102.12033",
    "title": "Self-Diagnosing GAN: Diagnosing Underrepresented Samples in Generative  Adversarial Networks",
    "abstract": "Self-Diagnosing GAN: Diagnosing Underrepresented Samples in Generative  Adversarial Networks",
    "descriptor": "",
    "authors": [
      "Jinhee Lee",
      "Haeri Kim",
      "Youngkyu Hong",
      "Hye Won Chung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.12033"
  },
  {
    "id": "arXiv:2102.12353",
    "title": "Nonlinear Invariant Risk Minimization: A Causal Approach",
    "abstract": "Nonlinear Invariant Risk Minimization: A Causal Approach",
    "descriptor": "",
    "authors": [
      "Chaochao Lu",
      "Yuhuai Wu",
      "Jo\u015be Miguel Hern\u00e1ndez-Lobato",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.12353"
  },
  {
    "id": "arXiv:2102.12825",
    "title": "Revisiting Optimal Resilience of Fast Byzantine Consensus",
    "abstract": "Revisiting Optimal Resilience of Fast Byzantine Consensus",
    "descriptor": "",
    "authors": [
      "Petr Kuznetsov",
      "Andrei Tonkikh",
      "Yan X Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.12825"
  },
  {
    "id": "arXiv:2102.13037",
    "title": "SPINN: Sparse, Physics-based, and Interpretable Neural Networks for PDEs",
    "abstract": "Comments: 58 pages, 32 figures, 1 table",
    "descriptor": "\nComments: 58 pages, 32 figures, 1 table\n",
    "authors": [
      "Amuthan A. Ramabathiran",
      "Prabhu Ramachandran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2102.13037"
  },
  {
    "id": "arXiv:2103.01710",
    "title": "Autobahn: Automorphism-based Graph Neural Nets",
    "abstract": "Comments: 9 pages, 4 figures",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Erik Henning Thiede",
      "Wenda Zhou",
      "Risi Kondor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.01710"
  },
  {
    "id": "arXiv:2103.02749",
    "title": "Introduction to Periodic Geometry and Topology",
    "abstract": "Comments: 40 pages, 21 figures. The second version contains minor amendments, especially in algorithmic section 8",
    "descriptor": "\nComments: 40 pages, 21 figures. The second version contains minor amendments, especially in algorithmic section 8\n",
    "authors": [
      "Olga Anosova",
      "Vitaliy Kurlin"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2103.02749"
  },
  {
    "id": "arXiv:2103.03111",
    "title": "Robust Binary Neural Network Operation from 233 K to 398 K via Gate  Stack and Bias Optimization of Ferroelectric FinFET Synapses",
    "abstract": "Comments: Accepted to be published in IEEE EDL",
    "descriptor": "\nComments: Accepted to be published in IEEE EDL\n",
    "authors": [
      "Sourav De",
      "Hoang-Hiep Le",
      "Bo-Han Qiu",
      "Md. Aftab Baig",
      "Po-Jung Sung",
      "Chung Jun Su",
      "Yao-Jen Lee",
      "Darsen D. Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2103.03111"
  },
  {
    "id": "arXiv:2103.03363",
    "title": "Koopman Operator Based Modeling for Quadrotor Control on $SE(3)$",
    "abstract": "Comments: 7 pages, 4 figures",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Vrushabh Zinage",
      "Efstathios Bakolas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.03363"
  },
  {
    "id": "arXiv:2103.03452",
    "title": "FedDR -- Randomized Douglas-Rachford Splitting Algorithms for Nonconvex  Federated Composite Optimization",
    "abstract": "Comments: 38 pages, and 12 figures",
    "descriptor": "\nComments: 38 pages, and 12 figures\n",
    "authors": [
      "Quoc Tran-Dinh",
      "Nhan H. Pham",
      "Dzung T. Phan",
      "Lam M. Nguyen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.03452"
  },
  {
    "id": "arXiv:2103.03567",
    "title": "Thermodynamic topology optimization including plasticity",
    "abstract": "Thermodynamic topology optimization including plasticity",
    "descriptor": "",
    "authors": [
      "Miriam Kick",
      "Philipp Junker"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2103.03567"
  },
  {
    "id": "arXiv:2103.04985",
    "title": "Significance tests of feature relevance for a blackbox learner",
    "abstract": "Significance tests of feature relevance for a blackbox learner",
    "descriptor": "",
    "authors": [
      "Ben Dai",
      "Xiaotong Shen",
      "Wei Pan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2103.04985"
  },
  {
    "id": "arXiv:2103.05462",
    "title": "Generating Reliable Process Event Streams and Time Series Data based on  Neural Networks",
    "abstract": "Generating Reliable Process Event Streams and Time Series Data based on  Neural Networks",
    "descriptor": "",
    "authors": [
      "Tobias Herbert",
      "Juergen Mangler",
      "Stefanie Rinderle-Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.05462"
  },
  {
    "id": "arXiv:2103.07309",
    "title": "Vectorial Parameterizations of Pose",
    "abstract": "Comments: 12 pages, 5 figures",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Timothy D. Barfoot",
      "James R. Forbes",
      "Gabriele M. T. D'Eleuterio"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.07309"
  },
  {
    "id": "arXiv:2103.09108",
    "title": "Is it Enough to Optimize CNN Architectures on ImageNet?",
    "abstract": "Is it Enough to Optimize CNN Architectures on ImageNet?",
    "descriptor": "",
    "authors": [
      "Lukas Tuggener",
      "J\u00fcrgen Schmidhuber",
      "Thilo Stadelmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.09108"
  },
  {
    "id": "arXiv:2103.09815",
    "title": "TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL",
    "abstract": "TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Romac",
      "R\u00e9my Portelas",
      "Katja Hofmann",
      "Pierre-Yves Oudeyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.09815"
  },
  {
    "id": "arXiv:2103.10710",
    "title": "Sparse Algorithms for Markovian Gaussian Processes",
    "abstract": "Comments: Appearing in the 24th International Conference on Artificial Intelligence and Statistics (AISTATS) 2021",
    "descriptor": "\nComments: Appearing in the 24th International Conference on Artificial Intelligence and Statistics (AISTATS) 2021\n",
    "authors": [
      "William J. Wilkinson",
      "Arno Solin",
      "Vincent Adam"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.10710"
  },
  {
    "id": "arXiv:2103.12011",
    "title": "Open Domain Question Answering over Tables via Dense Retrieval",
    "abstract": "Comments: NAACL 2021 camera ready",
    "descriptor": "\nComments: NAACL 2021 camera ready\n",
    "authors": [
      "Jonathan Herzig",
      "Thomas M\u00fcller",
      "Syrine Krichene",
      "Julian Martin Eisenschlos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2103.12011"
  },
  {
    "id": "arXiv:2103.14465",
    "title": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers",
    "abstract": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers",
    "descriptor": "",
    "authors": [
      "Kamil Bujel",
      "Helen Yannakoudakis",
      "Marek Rei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.14465"
  },
  {
    "id": "arXiv:2103.15999",
    "title": "Audio classification of the content of food containers and drinking  glasses",
    "abstract": "Comments: Camera-ready version. Paper accepted to EUSIPCO21. 5 pages, 4 figures, 3 tables. Minor improvements to the paper presentation",
    "descriptor": "\nComments: Camera-ready version. Paper accepted to EUSIPCO21. 5 pages, 4 figures, 3 tables. Minor improvements to the paper presentation\n",
    "authors": [
      "Santiago Donaher",
      "Alessio Xompero",
      "Andrea Cavallaro"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2103.15999"
  },
  {
    "id": "arXiv:2103.16483",
    "title": "Benchmarking Representation Learning for Natural World Image Collections",
    "abstract": "Comments: CVPR 2021",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Grant Van Horn",
      "Elijah Cole",
      "Sara Beery",
      "Kimberly Wilber",
      "Serge Belongie",
      "Oisin Mac Aodha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.16483"
  },
  {
    "id": "arXiv:2103.16809",
    "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech:  Two-stage Sequence-to-Sequence Training",
    "abstract": "Comments: Accepted by Interspeech 2021",
    "descriptor": "\nComments: Accepted by Interspeech 2021\n",
    "authors": [
      "Kun Zhou",
      "Berrak Sisman",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2103.16809"
  },
  {
    "id": "arXiv:2104.00768",
    "title": "Radar Target Detection aided by Reconfigurable Intelligent Surfaces",
    "abstract": "Comments: Accepted with Minor revisions on the IEEE Signal Processing Letters; updated version with supplementary material provided",
    "descriptor": "\nComments: Accepted with Minor revisions on the IEEE Signal Processing Letters; updated version with supplementary material provided\n",
    "authors": [
      "Stefano Buzzi",
      "Emanuele Grossi",
      "Marco Lops",
      "Luca Venturino"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2104.00768"
  },
  {
    "id": "arXiv:2104.02021",
    "title": "Intent Detection and Slot Filling for Vietnamese",
    "abstract": "Comments: To appear in Proceedings of INTERSPEECH 2021; The first two authors contributed equally to this work",
    "descriptor": "\nComments: To appear in Proceedings of INTERSPEECH 2021; The first two authors contributed equally to this work\n",
    "authors": [
      "Mai Hoang Dao",
      "Thinh Hung Truong",
      "Dat Quoc Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.02021"
  },
  {
    "id": "arXiv:2104.02867",
    "title": "Affordance Transfer Learning for Human-Object Interaction Detection",
    "abstract": "Comments: Accepted to CVPR2021; add a new but important ablated experiment in appendix(union box verb representation);",
    "descriptor": "\nComments: Accepted to CVPR2021; add a new but important ablated experiment in appendix(union box verb representation);\n",
    "authors": [
      "Zhi Hou",
      "Baosheng Yu",
      "Yu Qiao",
      "Xiaojiang Peng",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.02867"
  },
  {
    "id": "arXiv:2104.06569",
    "title": "Preventing Manipulation Attack in Local Differential Privacy using  Verifiable Randomization Mechanism",
    "abstract": "Comments: accepted by DBSec 2021",
    "descriptor": "\nComments: accepted by DBSec 2021\n",
    "authors": [
      "Fumiyuki Kato",
      "Yang Cao",
      "Masatoshi Yoshikawa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.06569"
  },
  {
    "id": "arXiv:2104.07491",
    "title": "Cross-domain Speech Recognition with Unsupervised Character-level  Distribution Matching",
    "abstract": "Comments: Accepted to INTERSPEECH 2021; code available at this https URL",
    "descriptor": "\nComments: Accepted to INTERSPEECH 2021; code available at this https URL\n",
    "authors": [
      "Wenxin Hou",
      "Jindong Wang",
      "Xu Tan",
      "Tao Qin",
      "Takahiro Shinozaki"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.07491"
  },
  {
    "id": "arXiv:2104.07508",
    "title": "Minimizing privilege for building HPC containers",
    "abstract": "Comments: 13 pages, 11 figures. Revision 2: clarifications, corrections of some minor errors",
    "descriptor": "\nComments: 13 pages, 11 figures. Revision 2: clarifications, corrections of some minor errors\n",
    "authors": [
      "Reid Priedhorsky",
      "R. Shane Canon",
      "Timothy Randles",
      "Andrew J. Younge"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2104.07508"
  },
  {
    "id": "arXiv:2104.10267",
    "title": "On reduction and normalization in the computational core",
    "abstract": "On reduction and normalization in the computational core",
    "descriptor": "",
    "authors": [
      "Claudia Faggian",
      "Giulio Guerrieri",
      "Ugo de'Liguoro",
      "Riccardo Treglia"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2104.10267"
  },
  {
    "id": "arXiv:2104.10611",
    "title": "Programmable 3D snapshot microscopy with Fourier convolutional networks",
    "abstract": "Comments: Rewritten and updated with DLMD results for conference submission",
    "descriptor": "\nComments: Rewritten and updated with DLMD results for conference submission\n",
    "authors": [
      "Diptodip Deb",
      "Zhenfei Jiao",
      "Alex B. Chen",
      "Misha B. Ahrens",
      "Kaspar Podgorski",
      "Srinivas C. Turaga"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.10611"
  },
  {
    "id": "arXiv:2104.10851",
    "title": "Continuous Learning and Adaptation with Membrane Potential and  Activation Threshold Homeostasis",
    "abstract": "Comments: 20 pages",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Alexander Hadjiivanov"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.10851"
  },
  {
    "id": "arXiv:2104.10858",
    "title": "All Tokens Matter: Token Labeling for Training Better Vision  Transformers",
    "abstract": "All Tokens Matter: Token Labeling for Training Better Vision  Transformers",
    "descriptor": "",
    "authors": [
      "Zihang Jiang",
      "Qibin Hou",
      "Li Yuan",
      "Daquan Zhou",
      "Yujun Shi",
      "Xiaojie Jin",
      "Anran Wang",
      "Jiashi Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.10858"
  },
  {
    "id": "arXiv:2104.11127",
    "title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network",
    "abstract": "Comments: 5 pages, 2 figures. Accepted to Interspeech 2021",
    "descriptor": "\nComments: 5 pages, 2 figures. Accepted to Interspeech 2021\n",
    "authors": [
      "Janne Pylkk\u00f6nen",
      "Antti Ukkonen",
      "Juho Kilpikoski",
      "Samu Tamminen",
      "Hannes Heikinheimo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.11127"
  },
  {
    "id": "arXiv:2105.02606",
    "title": "Analysis and Improvement of Heterogeneous Hardware Support in Docker  Images",
    "abstract": "Comments: 21st International Conference on Distributed Applications and Interoperable Systems",
    "descriptor": "\nComments: 21st International Conference on Distributed Applications and Interoperable Systems\n",
    "authors": [
      "Panagiotis Gkikopoulos",
      "Valerio Schiavoni",
      "Josef Spillner"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2105.02606"
  },
  {
    "id": "arXiv:2105.02639",
    "title": "De Finetti's Theorem in Categorical Probability",
    "abstract": "Comments: 31 pages, 1 figure. v2: More accurate abstract and some typos fixed",
    "descriptor": "\nComments: 31 pages, 1 figure. v2: More accurate abstract and some typos fixed\n",
    "authors": [
      "Tobias Fritz",
      "Tom\u00e1\u0161 Gonda",
      "Paolo Perrone"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2105.02639"
  },
  {
    "id": "arXiv:2105.03928",
    "title": "Which transformer architecture fits my data? A vocabulary bottleneck in  self-attention",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Noam Wies",
      "Yoav Levine",
      "Daniel Jannai",
      "Amnon Shashua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.03928"
  },
  {
    "id": "arXiv:2105.04339",
    "title": "DefSent: Sentence Embeddings using Definition Sentences",
    "abstract": "Comments: Accepted at ACL-IJCNLP 2021 main conference",
    "descriptor": "\nComments: Accepted at ACL-IJCNLP 2021 main conference\n",
    "authors": [
      "Hayato Tsukagoshi",
      "Ryohei Sasano",
      "Koichi Takeda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.04339"
  },
  {
    "id": "arXiv:2105.08645",
    "title": "CoTexT: Multi-task Learning with Code-Text Transformer",
    "abstract": "CoTexT: Multi-task Learning with Code-Text Transformer",
    "descriptor": "",
    "authors": [
      "Long Phan",
      "Hieu Tran",
      "Daniel Le",
      "Hieu Nguyen",
      "James Anibal",
      "Alec Peltekian",
      "Yanfang Ye"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2105.08645"
  },
  {
    "id": "arXiv:2105.08873",
    "title": "Mahalanobis distance-based robust approaches against false data  injection attacks on dynamic power state estimation",
    "abstract": "Mahalanobis distance-based robust approaches against false data  injection attacks on dynamic power state estimation",
    "descriptor": "",
    "authors": [
      "Jing Lin",
      "Kaiqi Xiong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2105.08873"
  },
  {
    "id": "arXiv:2105.09046",
    "title": "Music Generation using Three-layered LSTM",
    "abstract": "Music Generation using Three-layered LSTM",
    "descriptor": "",
    "authors": [
      "Vaishali Ingale",
      "Anush Mohan",
      "Divit Adlakha",
      "Krishan Kumar",
      "Mohit Gupta"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2105.09046"
  },
  {
    "id": "arXiv:2105.09313",
    "title": "Approximation Algorithms For The Dispersion Problems in a Metric Space",
    "abstract": "Comments: 9. arXiv admin note: text overlap with arXiv:2105.09217",
    "descriptor": "\nComments: 9. arXiv admin note: text overlap with arXiv:2105.09217\n",
    "authors": [
      "Pawan K. Mishra",
      "Gautam K. Das"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2105.09313"
  },
  {
    "id": "arXiv:2105.09501",
    "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine  Translation",
    "abstract": "Comments: ACL2021",
    "descriptor": "\nComments: ACL2021\n",
    "authors": [
      "Xiao Pan",
      "Mingxuan Wang",
      "Liwei Wu",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.09501"
  },
  {
    "id": "arXiv:2105.09534",
    "title": "Generalizing Non-Punctuality for Timed Temporal Logic with Freeze  Quantifiers",
    "abstract": "Generalizing Non-Punctuality for Timed Temporal Logic with Freeze  Quantifiers",
    "descriptor": "",
    "authors": [
      "Shankara Narayanan Krishna",
      "Khushraj Madnani",
      "Manuel Mazo Jr.",
      "Paritosh K. Pandya"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2105.09534"
  },
  {
    "id": "arXiv:2105.09632",
    "title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical  Notes: An Initial Study",
    "abstract": "Comments: 12 pages, 2 figures, 2 tables, SmartPhil 2020-First Workshop on Smart Personal Health Interfaces, Associated to ACM IUI 2020",
    "descriptor": "\nComments: 12 pages, 2 figures, 2 tables, SmartPhil 2020-First Workshop on Smart Personal Health Interfaces, Associated to ACM IUI 2020\n",
    "authors": [
      "Danilo Dessi",
      "Rim Helaoui",
      "Vivek Kumar",
      "Diego Reforgiato Recupero",
      "Daniele Riboni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.09632"
  },
  {
    "id": "arXiv:2105.10042",
    "title": "A Streaming End-to-End Framework For Spoken Language Understanding",
    "abstract": "Comments: Accepted at IJCAI 2021",
    "descriptor": "\nComments: Accepted at IJCAI 2021\n",
    "authors": [
      "Nihal Potdar",
      "Anderson R. Avila",
      "Chao Xing",
      "Dong Wang",
      "Yiran Cao",
      "Xiao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2105.10042"
  },
  {
    "id": "arXiv:2105.11802",
    "title": "Bias-Robust Bayesian Optimization via Dueling Bandits",
    "abstract": "Bias-Robust Bayesian Optimization via Dueling Bandits",
    "descriptor": "",
    "authors": [
      "Johannes Kirschner",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.11802"
  },
  {
    "id": "arXiv:2105.11928",
    "title": "VerLoc: Verifiable Localization in Decentralized Systems",
    "abstract": "VerLoc: Verifiable Localization in Decentralized Systems",
    "descriptor": "",
    "authors": [
      "Katharina Kohls",
      "Claudia Diaz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.11928"
  },
  {
    "id": "arXiv:2105.13910",
    "title": "A Holistic Approach to Enhanced Security and Privacy in Digital Health  Passports",
    "abstract": "Comments: 21 pages, to be published at the IWAPS workshop in connection with the ARES conference 2021",
    "descriptor": "\nComments: 21 pages, to be published at the IWAPS workshop in connection with the ARES conference 2021\n",
    "authors": [
      "Tore Kasper Frederiksen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.13910"
  },
  {
    "id": "arXiv:2105.14111",
    "title": "Objective Robustness in Deep Reinforcement Learning",
    "abstract": "Comments: small revisions, corrected figure for ablation",
    "descriptor": "\nComments: small revisions, corrected figure for ablation\n",
    "authors": [
      "Jack Koch",
      "Lauro Langosco",
      "Jacob Pfau",
      "James Le",
      "Lee Sharkey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.14111"
  },
  {
    "id": "arXiv:2105.14139",
    "title": "On a class of data-driven combinatorial optimization problems under  uncertainty: a distributionally robust approach",
    "abstract": "On a class of data-driven combinatorial optimization problems under  uncertainty: a distributionally robust approach",
    "descriptor": "",
    "authors": [
      "Sergey S. Ketkov",
      "Andrei S. Shilov",
      "Oleg A. Prokopyev"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2105.14139"
  },
  {
    "id": "arXiv:2105.14314",
    "title": "Automatic CT Segmentation from Bounding Box Annotations using  Convolutional Neural Networks",
    "abstract": "Automatic CT Segmentation from Bounding Box Annotations using  Convolutional Neural Networks",
    "descriptor": "",
    "authors": [
      "Yuanpeng Liu",
      "Qinglei Hui",
      "Zhiyi Peng",
      "Shaolin Gong",
      "Dexing Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.14314"
  },
  {
    "id": "arXiv:2105.14422",
    "title": "Periodic-GP: Learning Periodic World with Gaussian Process Bandits",
    "abstract": "Periodic-GP: Learning Periodic World with Gaussian Process Bandits",
    "descriptor": "",
    "authors": [
      "Hengrui Cai",
      "Zhihao Cen",
      "Ling Leng",
      "Rui Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2105.14422"
  },
  {
    "id": "arXiv:2105.14642",
    "title": "An iterative Jacobi-like algorithm to compute a few sparse  eigenvalue-eigenvector pairs",
    "abstract": "An iterative Jacobi-like algorithm to compute a few sparse  eigenvalue-eigenvector pairs",
    "descriptor": "",
    "authors": [
      "Cristian Rusu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2105.14642"
  },
  {
    "id": "arXiv:2105.14818",
    "title": "Redacting Transactions from Execute-Order-Validate Blockchains",
    "abstract": "Comments: IEEE International Conference on Blockchain and Cryptocurrency 2021 conference: icbc2021.ieee-icbc.org",
    "descriptor": "\nComments: IEEE International Conference on Blockchain and Cryptocurrency 2021 conference: icbc2021.ieee-icbc.org\n",
    "authors": [
      "Yacov Manevich",
      "Artem Barger",
      "Gal Assa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.14818"
  },
  {
    "id": "arXiv:2106.00116",
    "title": "Effect of large-scale pre-training on full and few-shot transfer  learning for natural and medical images",
    "abstract": "Comments: Preprint. Under review",
    "descriptor": "\nComments: Preprint. Under review\n",
    "authors": [
      "Mehdi Cherti",
      "Jenia Jitsev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.00116"
  },
  {
    "id": "arXiv:2106.00120",
    "title": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep  Probabilistic Models",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1811.06622",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1811.06622\n",
    "authors": [
      "Daniel T. Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00120"
  },
  {
    "id": "arXiv:2106.00771",
    "title": "SWIPT with Intelligent Reflecting Surfaces under Spatial Correlation",
    "abstract": "Comments: IEEE Wireless Communications Letters",
    "descriptor": "\nComments: IEEE Wireless Communications Letters\n",
    "authors": [
      "Constantinos Psomas",
      "Ioannis Krikidis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.00771"
  },
  {
    "id": "arXiv:2106.00891",
    "title": "High-Quality Diversification for Task-Oriented Dialogue Systems",
    "abstract": "Comments: Accepted by ACL-IJCNLP 2021 (Findings of ACL)",
    "descriptor": "\nComments: Accepted by ACL-IJCNLP 2021 (Findings of ACL)\n",
    "authors": [
      "Zhiwen Tang",
      "Hrishikesh Kulkarni",
      "Grace Hui Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.00891"
  },
  {
    "id": "arXiv:2106.01084",
    "title": "Asymptotic Characterisation of Regularised Zero-Forcing Receiver for  Imperfect and Correlated Massive MIMO Systems with Optimal Power Allocation",
    "abstract": "Asymptotic Characterisation of Regularised Zero-Forcing Receiver for  Imperfect and Correlated Massive MIMO Systems with Optimal Power Allocation",
    "descriptor": "",
    "authors": [
      "Ayed M. Alrashdi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.01084"
  },
  {
    "id": "arXiv:2106.01140",
    "title": "semopy 2: A Structural Equation Modeling Package with Random Effects in  Python",
    "abstract": "semopy 2: A Structural Equation Modeling Package with Random Effects in  Python",
    "descriptor": "",
    "authors": [
      "Georgy Meshcheryakov",
      "Anna A. Igolkina",
      "Maria G. Samsonova"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2106.01140"
  },
  {
    "id": "arXiv:2106.01720",
    "title": "Hybrid coupling of finite element and boundary element methods using  Nitsche's method and the Calderon projection",
    "abstract": "Hybrid coupling of finite element and boundary element methods using  Nitsche's method and the Calderon projection",
    "descriptor": "",
    "authors": [
      "Timo Betcke",
      "Micha\u0142 Bosy",
      "Erik Burman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.01720"
  },
  {
    "id": "arXiv:2106.01927",
    "title": "A Comparison for Anti-noise Robustness of Deep Learning Classification  Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to  Visual Transformer and Performer",
    "abstract": "A Comparison for Anti-noise Robustness of Deep Learning Classification  Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to  Visual Transformer and Performer",
    "descriptor": "",
    "authors": [
      "Ao Chen",
      "Chen Li",
      "Haoyuan Chen",
      "Hechen Yang",
      "Peng Zhao",
      "Weiming Hu",
      "Wanli Liu",
      "Shuojia Zou",
      "Marcin Grzegorzek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.01927"
  },
  {
    "id": "arXiv:2106.01978",
    "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in  Conversations",
    "abstract": "Comments: 11 pages, accepted by ACL-IJCNLP 2021",
    "descriptor": "\nComments: 11 pages, accepted by ACL-IJCNLP 2021\n",
    "authors": [
      "Dou Hu",
      "Lingwei Wei",
      "Xiaoyong Huai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.01978"
  },
  {
    "id": "arXiv:2106.01981",
    "title": "ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose",
    "abstract": "ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose",
    "descriptor": "",
    "authors": [
      "Boris N. Oreshkin",
      "Florent Bocquelet",
      "F\u00e9lix G. Harvey",
      "Bay Raitt",
      "Dominic Laflamme"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01981"
  },
  {
    "id": "arXiv:2106.02040",
    "title": "Towards Learning to Play Piano with Dexterous Hands and Touch",
    "abstract": "Towards Learning to Play Piano with Dexterous Hands and Touch",
    "descriptor": "",
    "authors": [
      "Huazhe Xu",
      "Yuping Luo",
      "Shaoxiong Wang",
      "Trevor Darrell",
      "Roberto Calandra"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02040"
  },
  {
    "id": "arXiv:2106.02185",
    "title": "Functional observers with linear error dynamics for discrete-time  nonlinear systems, with application to fault diagnosis",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2101.11148",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2101.11148\n",
    "authors": [
      "Sunjeev Venkateswaran",
      "Benjamin A. Wilhite",
      "Costas Kravaris"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02185"
  },
  {
    "id": "arXiv:2106.02282",
    "title": "Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn  Text-to-SQL",
    "abstract": "Comments: 12 pages, 3 figures, accepted to Findings of ACL 2021",
    "descriptor": "\nComments: 12 pages, 3 figures, accepted to Findings of ACL 2021\n",
    "authors": [
      "Zhi Chen",
      "Lu Chen",
      "Hanqi Li",
      "Ruisheng Cao",
      "Da Ma",
      "Mengyue Wu",
      "Kai Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02282"
  },
  {
    "id": "arXiv:2106.02351",
    "title": "SOLQ: Segmenting Objects by Learning Queries",
    "abstract": "Comments: Tech Report.Code is available at this https URL",
    "descriptor": "\nComments: Tech Report.Code is available at this https URL\n",
    "authors": [
      "Bin Dong",
      "Fangao Zeng",
      "Tiancai Wang",
      "Xiangyu Zhang",
      "Yichen Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02351"
  },
  {
    "id": "arXiv:2106.02382",
    "title": "Annotation Curricula to Implicitly Train Non-Expert Annotators",
    "abstract": "Annotation Curricula to Implicitly Train Non-Expert Annotators",
    "descriptor": "",
    "authors": [
      "Ji-Ung Lee",
      "Jan-Christoph Klie",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02382"
  },
  {
    "id": "arXiv:2106.02452",
    "title": "Proving Equivalence Between Complex Expressions Using Graph-to-Sequence  Neural Models",
    "abstract": "Comments: 10 pages (24 including references and appendices), 8 figures, 17 tables. arXiv admin note: substantial text overlap with arXiv:2002.06799. Updated to include funding acknowledgement",
    "descriptor": "\nComments: 10 pages (24 including references and appendices), 8 figures, 17 tables. arXiv admin note: substantial text overlap with arXiv:2002.06799. Updated to include funding acknowledgement\n",
    "authors": [
      "Steve Kommrusch",
      "Th\u00e9o Barollet",
      "Louis-No\u00ebl Pouchet"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02452"
  },
  {
    "id": "arXiv:2106.02713",
    "title": "Learning Curves for SGD on Structured Features",
    "abstract": "Comments: Fixed Typo in A.7",
    "descriptor": "\nComments: Fixed Typo in A.7\n",
    "authors": [
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02713"
  },
  {
    "id": "arXiv:2106.02875",
    "title": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease  Progression",
    "abstract": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease  Progression",
    "descriptor": "",
    "authors": [
      "Zhaozhi Qian",
      "William R. Zame",
      "Lucas M. Fleuren",
      "Paul Elbers",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02875"
  },
  {
    "id": "arXiv:2106.02927",
    "title": "A Framework for Dynamic Optimal Next-Hop Selection and RF Interface  Setting in IoT with the Same Source Requests",
    "abstract": "A Framework for Dynamic Optimal Next-Hop Selection and RF Interface  Setting in IoT with the Same Source Requests",
    "descriptor": "",
    "authors": [
      "Monireh Allah Gholi Ghasri",
      "Ali Mohammad Afshin Hemmatyar"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02927"
  },
  {
    "id": "arXiv:2106.02989",
    "title": "Exploring the Disproportion Between Scientific Productivity and  Knowledge Amount",
    "abstract": "Comments: Luoyi Fu and Huquan Kang contribute equally to the work. Xinbing Wang and Chenghu Zhou both are corresponding authors",
    "descriptor": "\nComments: Luoyi Fu and Huquan Kang contribute equally to the work. Xinbing Wang and Chenghu Zhou both are corresponding authors\n",
    "authors": [
      "Luoyi Fu",
      "Huquan Kang",
      "Jianghao Wang",
      "Ling Yao",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.02989"
  },
  {
    "id": "arXiv:2106.03180",
    "title": "Transformer in Convolutional Neural Networks",
    "abstract": "Transformer in Convolutional Neural Networks",
    "descriptor": "",
    "authors": [
      "Yun Liu",
      "Guolei Sun",
      "Yu Qiu",
      "Le Zhang",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03180"
  },
  {
    "id": "arXiv:2106.03181",
    "title": "Transient Chaos in BERT",
    "abstract": "Comments: 11 pages, 5 figures",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Katsuma Inoue",
      "Soh Ohara",
      "Yasuo Kuniyoshi",
      "Kohei Nakajima"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2106.03181"
  },
  {
    "id": "arXiv:2106.03457",
    "title": "Mechanism Design for Facility Location Problems: A Survey",
    "abstract": "Comments: To appear in IJCAI 2021 (Survey Track)",
    "descriptor": "\nComments: To appear in IJCAI 2021 (Survey Track)\n",
    "authors": [
      "Hau Chan",
      "Aris Filos-Ratsikas",
      "Bo Li",
      "Minming Li",
      "Chenhao Wang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03457"
  },
  {
    "id": "arXiv:2106.03760",
    "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with  Applications to Multi-Task Learning",
    "abstract": "DSelect-k: Differentiable Selection in the Mixture of Experts with  Applications to Multi-Task Learning",
    "descriptor": "",
    "authors": [
      "Hussein Hazimeh",
      "Zhe Zhao",
      "Aakanksha Chowdhery",
      "Maheswaran Sathiamoorthy",
      "Yihua Chen",
      "Rahul Mazumder",
      "Lichan Hong",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03760"
  },
  {
    "id": "arXiv:2106.03787",
    "title": "Concave Utility Reinforcement Learning: the Mean-field Game viewpoint",
    "abstract": "Concave Utility Reinforcement Learning: the Mean-field Game viewpoint",
    "descriptor": "",
    "authors": [
      "Matthieu Geist",
      "Julien P\u00e9rolat",
      "Mathieu Lauri\u00e8re",
      "Romuald Elie",
      "Sarah Perrin",
      "Olivier Bachem",
      "R\u00e9mi Munos",
      "Olivier Pietquin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.03787"
  },
  {
    "id": "arXiv:2106.03790",
    "title": "Multi-armed Bandit Requiring Monotone Arm Sequences",
    "abstract": "Multi-armed Bandit Requiring Monotone Arm Sequences",
    "descriptor": "",
    "authors": [
      "Ningyuan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03790"
  },
  {
    "id": "arXiv:2106.03893",
    "title": "Rethinking Graph Transformers with Spectral Attention",
    "abstract": "Rethinking Graph Transformers with Spectral Attention",
    "descriptor": "",
    "authors": [
      "Devin Kreuzer",
      "Dominique Beaini",
      "William L. Hamilton",
      "Vincent L\u00e9tourneau",
      "Prudencio Tossou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03893"
  },
  {
    "id": "arXiv:2106.03905",
    "title": "AutoPtosis",
    "abstract": "AutoPtosis",
    "descriptor": "",
    "authors": [
      "Abdullah Aleem",
      "Manoj Prabhakar Nallabothula",
      "Pete Setabutr",
      "Joelle A. Hallak",
      "Darvin Yi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03905"
  },
  {
    "id": "arXiv:2106.03958",
    "title": "Exploiting Language Relatedness for Low Web-Resource Language Model  Adaptation: An Indic Languages Study",
    "abstract": "Comments: Accepted to ACL-IJCNLP 2021",
    "descriptor": "\nComments: Accepted to ACL-IJCNLP 2021\n",
    "authors": [
      "Yash Khemchandani",
      "Sarvesh Mehtani",
      "Vaidehi Patil",
      "Abhijeet Awasthi",
      "Partha Talukdar",
      "Sunita Sarawagi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03958"
  },
  {
    "id": "arXiv:2106.04008",
    "title": "Widening Access to Applied Machine Learning with TinyML",
    "abstract": "Comments: Understanding the underpinnings of the TinyML edX course series: this https URL",
    "descriptor": "\nComments: Understanding the underpinnings of the TinyML edX course series: this https URL\n",
    "authors": [
      "Vijay Janapa Reddi",
      "Brian Plancher",
      "Susan Kennedy",
      "Laurence Moroney",
      "Pete Warden",
      "Anant Agarwal",
      "Colby Banbury",
      "Massimo Banzi",
      "Matthew Bennett",
      "Benjamin Brown",
      "Sharad Chitlangia",
      "Radhika Ghosal",
      "Sarah Grafman",
      "Rupert Jaeger",
      "Srivatsan Krishnan",
      "Maximilian Lam",
      "Daniel Leiker",
      "Cara Mann",
      "Mark Mazumder",
      "Dominic Pajak",
      "Dhilan Ramaprasad",
      "J. Evan Smith",
      "Matthew Stewart",
      "Dustin Tingley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04008"
  },
  {
    "id": "arXiv:2106.04149",
    "title": "Understanding (Generalized) Label Smoothing when Learning with Noisy  Labels",
    "abstract": "Comments: Under Review",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Jiaheng Wei",
      "Hangyu Liu",
      "Tongliang Liu",
      "Gang Niu",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04149"
  },
  {
    "id": "arXiv:2106.04166",
    "title": "Incorporating NODE with Pre-trained Neural Differential Operator for  Learning Dynamics",
    "abstract": "Comments: 15 pages, 12 figures, 3 tables",
    "descriptor": "\nComments: 15 pages, 12 figures, 3 tables\n",
    "authors": [
      "Shiqi Gong",
      "Qi Meng",
      "Yue Wang",
      "Lijun Wu",
      "Wei Chen",
      "Zhi-Ming Ma",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04166"
  },
  {
    "id": "arXiv:2106.04216",
    "title": "A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021",
    "abstract": "Comments: To be published in proceedings of the 17th International Conference on Parsing Technologies",
    "descriptor": "\nComments: To be published in proceedings of the 17th International Conference on Parsing Technologies\n",
    "authors": [
      "Mark Anderson",
      "Carlos G\u00f3mez Rodr\u00edguez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04216"
  },
  {
    "id": "arXiv:2106.04277",
    "title": "Operating Tor Relays at Universities: Experiences and Considerations",
    "abstract": "Operating Tor Relays at Universities: Experiences and Considerations",
    "descriptor": "",
    "authors": [
      "Christoph D\u00f6pmann",
      "Matthias Marx",
      "Hannes Federrath",
      "Florian Tschorsch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04277"
  },
  {
    "id": "arXiv:2106.04280",
    "title": "Optimizing a Binary Intelligent Reflecting Surface for OFDM  Communications under Mutual Coupling",
    "abstract": "Comments: 6 pages, 6 figures",
    "descriptor": "\nComments: 6 pages, 6 figures\n",
    "authors": [
      "Emil Bj\u00f6rnson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04280"
  },
  {
    "id": "arXiv:2106.04292",
    "title": "Principled Hyperedge Prediction with Structural Spectral Features and  Neural Networks",
    "abstract": "Principled Hyperedge Prediction with Structural Spectral Features and  Neural Networks",
    "descriptor": "",
    "authors": [
      "Changlin Wan",
      "Muhan Zhang",
      "Wei Hao",
      "Sha Cao",
      "Pan Li",
      "Chi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04292"
  },
  {
    "id": "arXiv:2106.04383",
    "title": "Using a New Nonlinear Gradient Method for Solving Large Scale Convex  Optimization Problems with an Application on Arabic Medical Text",
    "abstract": "Using a New Nonlinear Gradient Method for Solving Large Scale Convex  Optimization Problems with an Application on Arabic Medical Text",
    "descriptor": "",
    "authors": [
      "Jaafar Hammoud",
      "Ali Eisa",
      "Natalia Dobrenko",
      "Natalia Gusarova"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04383"
  },
  {
    "id": "arXiv:2106.04403",
    "title": "SynthRef: Generation of Synthetic Referring Expressions for Object  Segmentation",
    "abstract": "Comments: Accepted as poster at the NAACL 2021 Visually Grounded Interaction and Language (ViGIL) Workshop. 4 pages. Project website: this https URL",
    "descriptor": "\nComments: Accepted as poster at the NAACL 2021 Visually Grounded Interaction and Language (ViGIL) Workshop. 4 pages. Project website: this https URL\n",
    "authors": [
      "Ioannis Kazakos",
      "Carles Ventura",
      "Miriam Bellver",
      "Carina Silberer",
      "Xavier Giro-i-Nieto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.04403"
  },
  {
    "id": "arXiv:2106.04410",
    "title": "Error Mitigation for Deep Quantum Optimization Circuits by Leveraging  Problem Symmetries",
    "abstract": "Comments: minor updates (additional citation, clarified language)",
    "descriptor": "\nComments: minor updates (additional citation, clarified language)\n",
    "authors": [
      "Ruslan Shaydulin",
      "Alexey Galda"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.04410"
  },
  {
    "id": "arXiv:2106.04434",
    "title": "SDGMNet: Statistic-based Dynamic Gradient Modulation for Local  Descriptor Learning",
    "abstract": "SDGMNet: Statistic-based Dynamic Gradient Modulation for Local  Descriptor Learning",
    "descriptor": "",
    "authors": [
      "Jiayi Ma",
      "Yuxin Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04434"
  },
  {
    "id": "arXiv:2106.04441",
    "title": "CLTR: An End-to-End, Transformer-Based System for Cell Level Table  Retrieval and Table Question Answering",
    "abstract": "CLTR: An End-to-End, Transformer-Based System for Cell Level Table  Retrieval and Table Question Answering",
    "descriptor": "",
    "authors": [
      "Feifei Pan",
      "Mustafa Canim",
      "Michael Glass",
      "Alfio Gliozzo",
      "Peter Fox"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04441"
  },
  {
    "id": "arXiv:2106.04480",
    "title": "There Is No Turning Back: A Self-Supervised Approach for  Reversibility-Aware Reinforcement Learning",
    "abstract": "There Is No Turning Back: A Self-Supervised Approach for  Reversibility-Aware Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Nathan Grinsztajn",
      "Johan Ferret",
      "Olivier Pietquin",
      "Philippe Preux",
      "Matthieu Geist"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04480"
  },
  {
    "id": "arXiv:2106.04515",
    "title": "Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in  North Carolina",
    "abstract": "Comments: 12 pages, 6 figures, 7 tables, to be published in ACM-BCB 2021",
    "descriptor": "\nComments: 12 pages, 6 figures, 7 tables, to be published in ACM-BCB 2021\n",
    "authors": [
      "Christopher Whitfield",
      "Yang Liu",
      "Mohad Anwar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04515"
  },
  {
    "id": "arXiv:2106.04549",
    "title": "KIGLIS: Smart Networks for Smart Cities",
    "abstract": "KIGLIS: Smart Networks for Smart Cities",
    "descriptor": "",
    "authors": [
      "Daniel Bogdoll",
      "Patrick Matalla",
      "Christoph F\u00fcllner",
      "Christian Raack",
      "Shi Li",
      "Tobias K\u00e4fer",
      "Stefan Orf",
      "Marc Ren\u00e9 Zofka",
      "Finn Sartoris",
      "Christoph Schweikert",
      "Thomas Pfeiffer",
      "Andr\u00e9 Richter",
      "Sebastian Randel",
      "Rene Bonk"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04549"
  }
]