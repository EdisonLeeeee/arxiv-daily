[
  {
    "id": "arXiv:2106.03850",
    "title": "Multi-Task Hierarchical Learning Based Network Traffic Analytics",
    "abstract": "Classifying network traffic is the basis for important network applications.\nPrior research in this area has faced challenges on the availability of\nrepresentative datasets, and many of the results cannot be readily reproduced.\nSuch a problem is exacerbated by emerging data-driven machine learning based\napproaches. To address this issue, we present(N et)2databasewith three open\ndatasets containing nearly 1.3M labeled flows in total, with a comprehensive\nlist of flow features, for there search community1. We focus on broad aspects\nin network traffic analysis, including both malware detection and application\nclassification. As we continue to grow them, we expect the datasets to serve as\na common ground for AI driven, reproducible research on network flow analytics.\nWe release the datasets publicly and also introduce a Multi-Task Hierarchical\nLearning (MTHL)model to perform all tasks in a single model. Our results show\nthat MTHL is capable of accurately performing multiple tasks with hierarchical\nlabeling with a dramatic reduction in training time.",
    "descriptor": "\nComments: 6 pages, 2 figures, 1 table. arXiv admin note: substantial text overlap with arXiv:2004.13006\n",
    "authors": [
      "Onur Barut",
      "Yan Luo",
      "Tong Zhang",
      "Weigang Li",
      "Peilong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03850"
  },
  {
    "id": "arXiv:2106.03851",
    "title": "Impact of data-splits on generalization: Identifying COVID-19 from cough  and context",
    "abstract": "Rapidly scaling screening, testing and quarantine has shown to be an\neffective strategy to combat the COVID-19 pandemic. We consider the application\nof deep learning techniques to distinguish individuals with COVID from\nnon-COVID by using data acquirable from a phone. Using cough and context\n(symptoms and meta-data) represent such a promising approach. Several\nindependent works in this direction have shown promising results. However, none\nof them report performance across clinically relevant data splits.\nSpecifically, the performance where the development and test sets are split in\ntime (retrospective validation) and across sites (broad validation). Although\nthere is meaningful generalization across these splits the performance\nsignificantly varies (up to 0.1 AUC score). In addition, we study the\nperformance of symptomatic and asymptomatic individuals across these three\nsplits. Finally, we show that our model focuses on meaningful features of the\ninput, cough bouts for cough and relevant symptoms for context. The code and\ncheckpoints are available at https://github.com/WadhwaniAI/cough-against-covid",
    "descriptor": "\nComments: Published as a workshop paper at ICLR 2021 AI for Public Health Workshop and ICLR 20201 Machine Learning for Preventing and Combating Pandemics Workshop\n",
    "authors": [
      "Makkunda Sharma",
      "Nikhil Shenoy",
      "Jigar Doshi",
      "Piyush Bagad",
      "Aman Dalmia",
      "Parag Bhamare",
      "Amrita Mahale",
      "Saurabh Rane",
      "Neeraj Agrawal",
      "Rahul Panicker"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03851"
  },
  {
    "id": "arXiv:2106.03852",
    "title": "Generalized Circular One-Way Jumping Finite Automata",
    "abstract": "A discontinuous model of computation called one-way jumping finite automata\nwas defined by H. Chigahara et. al. This model was a restricted version of the\nmodel jumping finite automata. One-way jumping finite automata change their\nstates after deleting a letter of an input and jump only in one direction.\nAllowing a state to delete a subword instead of a letter, we define a new model\ngeneralized circular one-way jumping finite automata. These automata work on an\ninput in a circular manner. Similar to one-way jumping finite automata,\ngeneralized circular one-way jumping finite automata also jump only in one\ndirection. We show that this newly defined model is powerful than one-way\njumping finite automata. We define new variants(right and left) of the model\ngeneralized circular one-way jumping finite automata and compare them. We also\ncompare the newly defined model with Chomsky hierarchy. Finally, we explore\nclosure properties of the model.",
    "descriptor": "\nComments: 18 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2106.02937\n",
    "authors": [
      "Ujjwal Kumar Mishra",
      "Kalpana Mahalingam",
      "Rama Raghavan"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.03852"
  },
  {
    "id": "arXiv:2106.03853",
    "title": "DisTop: Discovering a Topological representation to learn diverse and  rewarding skills",
    "abstract": "The optimal way for a deep reinforcement learning (DRL) agent to explore is\nto learn a set of skills that achieves a uniform distribution of states.\nFollowing this,we introduce DisTop, a new model that simultaneously learns\ndiverse skills and focuses on improving rewarding skills. DisTop progressively\nbuilds a discrete topology of the environment using an unsupervised contrastive\nloss, a growing network and a goal-conditioned policy. Using this topology, a\nstate-independent hierarchical policy can select where the agent has to keep\ndiscovering skills in the state space. In turn, the newly visited states allows\nan improved learnt representation and the learning loop continues. Our\nexperiments emphasize that DisTop is agnostic to the ground state\nrepresentation and that the agent can discover the topology of its environment\nwhether the states are high-dimensional binary data, images, or proprioceptive\ninputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks\nwith state-of-the-art algorithms on both single-task dense rewards and diverse\nskill discovery. By combining these two aspects, we showthat DisTop achieves\nstate-of-the-art performance in comparison with hierarchical reinforcement\nlearning (HRL) when rewards are sparse. We believe DisTop opens new\nperspectives by showing that bottom-up skill discovery combined with\nrepresentation learning can unlock the exploration challenge in DRL.",
    "descriptor": "",
    "authors": [
      "Arthur Aubret",
      "Laetitia matignon",
      "Salima Hassas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03853"
  },
  {
    "id": "arXiv:2106.03856",
    "title": "High Order Impedance Boundary Condition for the Three-dimensional  Scattering Problem in Electromagnetism",
    "abstract": "In this paper, we propose a variational formulation with the use of high\norder impedance boundary condition (HOIBC) to solve the scattering problem. We\nstudy the existence and uniqueness of the solution. Then, a discretization of\nthis formulation is done with Rao-Wilton-Glisson (RWG). We give validations of\nthe HOIBC obtained with a 3D MoM code that show the improvement in accuracy\nover the standard impedance boundary condition (SIBC) computations.",
    "descriptor": "\nComments: 5 pages, 5 figures. arXiv admin note: substantial text overlap with arXiv:2106.03486\n",
    "authors": [
      "Soumaya Oueslati",
      "Christian Daveau",
      "Abil Aubakirov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03856"
  },
  {
    "id": "arXiv:2106.03873",
    "title": "Measuring Conversational Uptake: A Case Study on Student-Teacher  Interactions",
    "abstract": "In conversation, uptake happens when a speaker builds on the contribution of\ntheir interlocutor by, for example, acknowledging, repeating or reformulating\nwhat they have said. In education, teachers' uptake of student contributions\nhas been linked to higher student achievement. Yet measuring and improving\nteachers' uptake at scale is challenging, as existing methods require expensive\nannotation by experts. We propose a framework for computationally measuring\nuptake, by (1) releasing a dataset of student-teacher exchanges extracted from\nUS math classroom transcripts annotated for uptake by experts; (2) formalizing\nuptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next\nutterance classification; (3) conducting a linguistically-motivated comparison\nof different unsupervised measures and (4) correlating these measures with\neducational outcomes. We find that although repetition captures a significant\npart of uptake, pJSD outperforms repetition-based baselines, as it is capable\nof identifying a wider range of uptake phenomena like question answering and\nreformulation. We apply our uptake measure to three different educational\ndatasets with outcome indicators. Unlike baseline measures, pJSD correlates\nsignificantly with instruction quality in all three, providing evidence for its\ngeneralizability and for its potential to serve as an automated professional\ndevelopment tool for teachers.",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Dorottya Demszky",
      "Jing Liu",
      "Zid Mancenido",
      "Julie Cohen",
      "Heather Hill",
      "Dan Jurafsky",
      "Tatsunori Hashimoto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03873"
  },
  {
    "id": "arXiv:2106.03885",
    "title": "Differentiable Multiple Shooting Layers",
    "abstract": "We detail a novel class of implicit neural models. Leveraging time-parallel\nmethods for differential equations, Multiple Shooting Layers (MSLs) seek\nsolutions of initial value problems via parallelizable root-finding algorithms.\nMSLs broadly serve as drop-in replacements for neural ordinary differential\nequations (Neural ODEs) with improved efficiency in number of function\nevaluations (NFEs) and wall-clock inference time. We develop the algorithmic\nframework of MSLs, analyzing the different choices of solution methods from a\ntheoretical and computational perspective. MSLs are showcased in long horizon\noptimal control of ODEs and PDEs and as latent models for sequence generation.\nFinally, we investigate the speedups obtained through application of MSL\ninference in neural controlled differential equations (Neural CDEs) for time\nseries classification of medical data.",
    "descriptor": "",
    "authors": [
      "Stefano Massaroli",
      "Michael Poli",
      "Sho Sonoda",
      "Taji Suzuki",
      "Jinkyoo Park",
      "Atsushi Yamashita",
      "Hajime Asama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03885"
  },
  {
    "id": "arXiv:2106.03893",
    "title": "Rethinking Graph Transformers with Spectral Attention",
    "abstract": "In recent years, the Transformer architecture has proven to be very\nsuccessful in sequence processing, but its application to other data\nstructures, such as graphs, has remained limited due to the difficulty of\nproperly defining positions. Here, we present the $\\textit{Spectral Attention\nNetwork}$ (SAN), which uses a learned positional encoding (LPE) that can take\nadvantage of the full Laplacian spectrum to learn the position of each node in\na given graph. This LPE is then added to the node features of the graph and\npassed to a fully-connected Transformer. By leveraging the full spectrum of the\nLaplacian, our model is theoretically powerful in distinguishing graphs, and\ncan better detect similar sub-structures from their resonance. Further, by\nfully connecting the graph, the Transformer does not suffer from\nover-squashing, an information bottleneck of most GNNs, and enables better\nmodeling of physical phenomenons such as heat transfer and electric\ninteraction. When tested empirically on a set of 4 standard datasets, our model\nperforms on par or better than state-of-the-art GNNs, and outperforms any\nattention-based model by a wide margin, becoming the first fully-connected\narchitecture to perform well on graph benchmarks.",
    "descriptor": "",
    "authors": [
      "Devin Kreuzer",
      "Dominique Beaini",
      "William L. Hamilton",
      "Vincent L\u00e9tourneau",
      "Prudencio Tossou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03893"
  },
  {
    "id": "arXiv:2106.03894",
    "title": "Differentiable Quality Diversity",
    "abstract": "Quality diversity (QD) is a growing branch of stochastic optimization\nresearch that studies the problem of generating an archive of solutions that\nmaximize a given objective function but are also diverse with respect to a set\nof specified measure functions. However, even when these functions are\ndifferentiable, QD algorithms treat them as \"black boxes\", ignoring gradient\ninformation. We present the differentiable quality diversity (DQD) problem, a\nspecial case of QD, where both the objective and measure functions are first\norder differentiable. We then present MAP-Elites via Gradient Arborescence\n(MEGA), a DQD algorithm that leverages gradient information to efficiently\nexplore the joint range of the objective and measure functions. Results in two\nQD benchmark domains and in searching the latent space of a StyleGAN show that\nMEGA significantly outperforms state-of-the-art QD algorithms, highlighting\nDQD's promise for efficient quality diversity optimization when gradient\ninformation is available. Source code is available at\nhttps://github.com/icaros-usc/dqd.",
    "descriptor": "",
    "authors": [
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03894"
  },
  {
    "id": "arXiv:2106.03895",
    "title": "SIGTYP 2021 Shared Task: Robust Spoken Language Identification",
    "abstract": "While language identification is a fundamental speech and language processing\ntask, for many languages and language families it remains a challenging task.\nFor many low-resource and endangered languages this is in part due to resource\navailability: where larger datasets exist, they may be single-speaker or have\ndifferent domains than desired application scenarios, demanding a need for\ndomain and speaker-invariant language identification systems. This year's\nshared task on robust spoken language identification sought to investigate just\nthis scenario: systems were to be trained on largely single-speaker speech from\none domain, but evaluated on data in other domains recorded from speakers under\ndifferent recording circumstances, mimicking realistic low-resource scenarios.\nWe see that domain and speaker mismatch proves very challenging for current\nmethods which can perform above 95% accuracy in-domain, which domain adaptation\ncan address to some degree, but that these conditions merit further\ninvestigation to make spoken language identification accessible in many\nscenarios.",
    "descriptor": "\nComments: The first three authors contributed equally\n",
    "authors": [
      "Elizabeth Salesky",
      "Badr M. Abdullah",
      "Sabrina J. Mielke",
      "Elena Klyachko",
      "Oleg Serikov",
      "Edoardo Ponti",
      "Ritesh Kumar",
      "Ryan Cotterell",
      "Ekaterina Vylomova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03895"
  },
  {
    "id": "arXiv:2106.03899",
    "title": "Multi-task Transformation Learning for Robust Out-of-Distribution  Detection",
    "abstract": "Detecting out-of-distribution (OOD) samples plays a key role in open-world\nand safety-critical applications such as autonomous systems and healthcare.\nSelf-supervised representation learning techniques (e.g., contrastive learning\nand pretext learning) are well suited for learning representation that can\nidentify OOD samples. In this paper, we propose a simple framework that\nleverages multi-task transformation learning for training effective\nrepresentation for OOD detection which outperforms state-of-the-art OOD\ndetection performance and robustness on several image datasets. We empirically\nobserve that the OOD performance depends on the choice of data transformations\nwhich itself depends on the in-domain training set. To address this problem, we\npropose a simple mechanism for selecting the transformations automatically and\nmodulate their effect on representation learning without requiring any OOD\ntraining samples. We characterize the criteria for a desirable OOD detector for\nreal-world applications and demonstrate the efficacy of our proposed technique\nagainst a diverse range of the state-of-the-art OOD detection techniques.",
    "descriptor": "",
    "authors": [
      "Sina Mohseni",
      "Arash Vahdat",
      "Jay Yadawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03899"
  },
  {
    "id": "arXiv:2106.03903",
    "title": "PILOT: Introducing Transformers for Probabilistic Sound Event  Localization",
    "abstract": "Sound event localization aims at estimating the positions of sound sources in\nthe environment with respect to an acoustic receiver (e.g. a microphone array).\nRecent advances in this domain most prominently focused on utilizing deep\nrecurrent neural networks. Inspired by the success of transformer architectures\nas a suitable alternative to classical recurrent neural networks, this paper\nintroduces a novel transformer-based sound event localization framework, where\ntemporal dependencies in the received multi-channel audio signals are captured\nvia self-attention mechanisms. Additionally, the estimated sound event\npositions are represented as multivariate Gaussian variables, yielding an\nadditional notion of uncertainty, which many previously proposed deep\nlearning-based systems designed for this application do not provide. The\nframework is evaluated on three publicly available multi-source sound event\nlocalization datasets and compared against state-of-the-art methods in terms of\nlocalization error and event detection accuracy. It outperforms all competing\nsystems on all datasets with statistical significant differences in\nperformance.",
    "descriptor": "\nComments: Accepted at INTERSPEECH 2021\n",
    "authors": [
      "Christopher Schymura",
      "Benedikt B\u00f6nninghoff",
      "Tsubasa Ochiai",
      "Marc Delcroix",
      "Keisuke Kinoshita",
      "Tomohiro Nakatani",
      "Shoko Araki",
      "Dorothea Kolossa"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03903"
  },
  {
    "id": "arXiv:2106.03904",
    "title": "When in Doubt: Neural Non-Parametric Uncertainty Quantification for  Epidemic Forecasting",
    "abstract": "Accurate and trustworthy epidemic forecasting is an important problem that\nhas impact on public health planning and disease mitigation. Most existing\nepidemic forecasting models disregard uncertainty quantification, resulting in\nmis-calibrated predictions. Recent works in deep neural models for\nuncertainty-aware time-series forecasting also have several limitations; e.g.\nit is difficult to specify meaningful priors in Bayesian NNs, while methods\nlike deep ensembling are computationally expensive in practice. In this paper,\nwe fill this important gap. We model the forecasting task as a probabilistic\ngenerative process and propose a functional neural process model called EPIFNP,\nwhich directly models the probability density of the forecast value. EPIFNP\nleverages a dynamic stochastic correlation graph to model the correlations\nbetween sequences in a non-parametric way, and designs different stochastic\nlatent variables to capture functional uncertainty from different perspectives.\nOur extensive experiments in a real-time flu forecasting setting show that\nEPIFNP significantly outperforms previous state-of-the-art models in both\naccuracy and calibration metrics, up to 2.5x in accuracy and 2.4x in\ncalibration. Additionally, due to properties of its generative process,EPIFNP\nlearns the relations between the current season and similar patterns of\nhistorical seasons,enabling interpretable forecasts. Beyond epidemic\nforecasting, the EPIFNP can be of independent interest for advancing principled\nuncertainty quantification in deep sequential models for predictive analytics",
    "descriptor": "\nComments: 16 pages, 12 Figures\n",
    "authors": [
      "Harshavardhan Kamarthi",
      "Lingkai Kong",
      "Alexander Rodr\u00edguez",
      "Chao Zhang",
      "B. Aditya Prakash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03904"
  },
  {
    "id": "arXiv:2106.03906",
    "title": "Learning to Guide a Saturation-Based Theorem Prover",
    "abstract": "Traditional automated theorem provers have relied on manually tuned\nheuristics to guide how they perform proof search. Recently, however, there has\nbeen a surge of interest in the design of learning mechanisms that can be\nintegrated into theorem provers to improve their performance automatically. In\nthis work, we introduce TRAIL, a deep learning-based approach to theorem\nproving that characterizes core elements of saturation-based theorem proving\nwithin a neural framework. TRAIL leverages (a) an effective graph neural\nnetwork for representing logical formulas, (b) a novel neural representation of\nthe state of a saturation-based theorem prover in terms of processed clauses\nand available actions, and (c) a novel representation of the inference\nselection process as an attention-based action policy. We show through a\nsystematic analysis that these components allow TRAIL to significantly\noutperform previous reinforcement learning-based theorem provers on two\nstandard benchmark datasets (up to 36% more theorems proved). In addition, to\nthe best of our knowledge, TRAIL is the first reinforcement learning-based\napproach to exceed the performance of a state-of-the-art traditional theorem\nprover on a standard theorem proving benchmark (solving up to 17% more\nproblems).",
    "descriptor": "",
    "authors": [
      "Ibrahim Abdelaziz",
      "Maxwell Crouse",
      "Bassem Makni",
      "Vernon Austil",
      "Cristina Cornelio",
      "Shajith Ikbal",
      "Pavan Kapanipathi",
      "Ndivhuwo Makondo",
      "Kavitha Srinivas",
      "Michael Witbrock",
      "Achille Fokoue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.03906"
  },
  {
    "id": "arXiv:2106.03907",
    "title": "Deep Proxy Causal Learning and its Application to Confounded Bandit  Policy Evaluation",
    "abstract": "Proxy causal learning (PCL) is a method for estimating the causal effect of\ntreatments on outcomes in the presence of unobserved confounding, using proxies\n(structured side information) for the confounder. This is achieved via\ntwo-stage regression: in the first stage, we model relations among the\ntreatment and proxies; in the second stage, we use this model to learn the\neffect of treatment on the outcome, given the context provided by the proxies.\nPCL guarantees recovery of the true causal effect, subject to identifiability\nconditions. We propose a novel method for PCL, the deep feature proxy variable\nmethod (DFPV), to address the case where the proxies, treatments, and outcomes\nare high-dimensional and have nonlinear complex relationships, as represented\nby deep neural network features. We show that DFPV outperforms recent\nstate-of-the-art PCL methods on challenging synthetic benchmarks, including\nsettings involving high dimensional image data. Furthermore, we show that PCL\ncan be applied to off-policy evaluation for the confounded bandit problem, in\nwhich DFPV also exhibits competitive performance.",
    "descriptor": "",
    "authors": [
      "Liyuan Xu",
      "Heishiro Kanagawa",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03907"
  },
  {
    "id": "arXiv:2106.03911",
    "title": "XIRL: Cross-embodiment Inverse Reinforcement Learning",
    "abstract": "We investigate the visual cross-embodiment imitation setting, in which agents\nlearn policies from videos of other agents (such as humans) demonstrating the\nsame task, but with stark differences in their embodiments -- shape, actions,\nend-effector dynamics, etc. In this work, we demonstrate that it is possible to\nautomatically discover and learn vision-based reward functions from\ncross-embodiment demonstration videos that are robust to these differences.\nSpecifically, we present a self-supervised method for Cross-embodiment Inverse\nReinforcement Learning (XIRL) that leverages temporal cycle-consistency\nconstraints to learn deep visual embeddings that capture task progression from\noffline videos of demonstrations across multiple expert agents, each performing\nthe same task differently due to embodiment differences. Prior to our work,\nproducing rewards from self-supervised embeddings has typically required\nalignment with a reference trajectory, which may be difficult to acquire. We\nshow empirically that if the embeddings are aware of task-progress, simply\ntaking the negative distance between the current state and goal state in the\nlearned embedding space is useful as a reward for training policies with\nreinforcement learning. We find our learned reward function not only works for\nembodiments seen during training, but also generalizes to entirely new\nembodiments. We also find that XIRL policies are more sample efficient than\nbaselines, and in some cases exceed the sample efficiency of the same agent\ntrained with ground truth sparse rewards.",
    "descriptor": "",
    "authors": [
      "Kevin Zakka",
      "Andy Zeng",
      "Pete Florence",
      "Jonathan Tompson",
      "Jeannette Bohg",
      "Debidatta Dwibedi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03911"
  },
  {
    "id": "arXiv:2106.03917",
    "title": "Fine-grained Out-of-Distribution Detection with Mixup Outlier Exposure",
    "abstract": "Enabling out-of-distribution (OOD) detection for DNNs is critical for their\nsafe and reliable operation in the \"open world\". Unfortunately, current works\nin both methodology and evaluation focus on rather contrived detection\nproblems, and only consider a coarse level of granularity w.r.t.: 1) the\nin-distribution (ID) classes, and 2) the OOD data's \"closeness\" to the ID data.\nWe posit that such settings may be poor approximations of many real-world tasks\nthat are naturally fine-grained (e.g., bird species classification), and thus\nthe reported detection abilities may be over-estimates. Differently, in this\nwork we make granularity a top priority and focus on fine-grained OOD\ndetection. We start by carefully constructing five novel fine-grained test\nenvironments in which existing methods are shown to have difficulties. We then\npropose a new DNN training algorithm, Mixup Outlier Exposure (MixupOE), which\nleverages an outlier distribution and principles from vicinal risk\nminimization. Finally, we perform extensive experiments and analyses in our\ncustom test environments and demonstrate that MixupOE can consistently improve\nfine-grained detection performance, establishing a strong baseline in these\nmore realistic and challenging OOD detection settings.",
    "descriptor": "",
    "authors": [
      "Jingyang Zhang",
      "Nathan Inkawhich",
      "Yiran Chen",
      "Hai Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03917"
  },
  {
    "id": "arXiv:2106.03919",
    "title": "Learning to Detect Multi-Modal Grasps for Dexterous Grasping in Dense  Clutter",
    "abstract": "Grasping arbitrary objects in densely cluttered novel environments is a\ncrucial skill for robots. Though many existing systems enable two-finger\nparallel-jaw grippers to pick items from clutter, these grippers cannot perform\nmultiple types of grasps. However, multi-modal grasping with multi-finger\ngrippers could much more effectively clear objects of varying sizes from\ncluttered scenes. We propose an approach to multi-model grasp detection that\njointly predicts the probabilities that several types of grasps succeed at a\ngiven grasp pose. Given a partial point cloud of a scene, the algorithm\nproposes a set of feasible grasp candidates, then estimates the probabilities\nthat a grasp of each type would succeed at each candidate pose. Predicting\ngrasp success probabilities directly from point clouds makes our approach\nagnostic to the number and placement of depth sensors at execution time. We\nevaluate our system both in simulation and on a real robot with a Robotiq\n3-Finger Adaptive Gripper. We compare our network against several baselines\nthat perform fewer types of grasps. Our experiments show that a system that\nexplicitly models grasp type achieves an object retrieval rate 8.5% higher in a\ncomplex cluttered environment than our highest-performing baseline.",
    "descriptor": "\nComments: IROS 2021 submission\n",
    "authors": [
      "Matt Corsaro",
      "Stefanie Tellex",
      "George Konidaris"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03919"
  },
  {
    "id": "arXiv:2106.03921",
    "title": "Measuring and Improving BERT's Mathematical Abilities by Predicting the  Order of Reasoning",
    "abstract": "Imagine you are in a supermarket. You have two bananas in your basket and\nwant to buy four apples. How many fruits do you have in total? This seemingly\nstraightforward question can be challenging for data-driven language models,\neven if trained at scale. However, we would expect such generic language models\nto possess some mathematical abilities in addition to typical linguistic\ncompetence. Towards this goal, we investigate if a commonly used language\nmodel, BERT, possesses such mathematical abilities and, if so, to what degree.\nFor that, we fine-tune BERT on a popular dataset for word math problems,\nAQuA-RAT, and conduct several tests to understand learned representations\nbetter. Since we teach models trained on natural language to do formal\nmathematics, we hypothesize that such models would benefit from training on\nsemi-formal steps that explain how math results are derived. To better\naccommodate such training, we also propose new pretext tasks for learning\nmathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or\nNROP). With this new model, we achieve significantly better outcomes than\ndata-driven baselines and even on-par with more tailored models. We also show\nhow to reduce positional bias in such models.",
    "descriptor": "\nComments: The paper has been accepted to the ACL-IJCNLP 2021 conference\n",
    "authors": [
      "Piotr Pi\u0119kos",
      "Henryk Michalewski",
      "Mateusz Malinowski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03921"
  },
  {
    "id": "arXiv:2106.03922",
    "title": "Interactive Label Cleaning with Example-based Explanations",
    "abstract": "We tackle sequential learning under label noise in applications where a human\nsupervisor can be queried to relabel suspicious examples. Existing approaches\nare flawed, in that they only relabel incoming examples that look\n``suspicious'' to the model. As a consequence, those mislabeled examples that\nelude (or don't undergo) this cleaning step end up tainting the training data\nand the model with no further chance of being cleaned. We propose Cincer, a\nnovel approach that cleans both new and past data by identifying pairs of\nmutually incompatible examples. Whenever it detects a suspicious example,\nCincer identifies a counter-example in the training set that -- according to\nthe model -- is maximally incompatible with the suspicious example, and asks\nthe annotator to relabel either or both examples, resolving this possible\ninconsistency. The counter-examples are chosen to be maximally incompatible, so\nto serve as explanations of the model' suspicion, and highly influential, so to\nconvey as much information as possible if relabeled. Cincer achieves this by\nleveraging an efficient and robust approximation of influence functions based\non the Fisher information matrix (FIM). Our extensive empirical evaluation\nshows that clarifying the reasons behind the model's suspicions by cleaning the\ncounter-examples helps acquiring substantially better data and models,\nespecially when paired with our FIM approximation.",
    "descriptor": "",
    "authors": [
      "Stefano Teso",
      "Andrea Bontempelli",
      "Fausto Giunchiglia",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03922"
  },
  {
    "id": "arXiv:2106.03923",
    "title": "Acoustic Power for Swarms of Microscopic Robots",
    "abstract": "Ultrasound can power implanted medical devices. This paper evaluates its\nfeasibility for microscopic robots in tissue that mechanically harvest energy\nusing pistons. At these sizes, viscous drag dominates the piston motion and\nacoustic attenuation limits how far power can reach. Combining these factors\nshows that frequencies around 100kHz can deliver hundreds of picowatts to\nwell-separated micron-size robots in low-attenuation tissues within about 10cm\nof the skin. However, applications of microscopic robots could involve large\nnumbers, in which case the robots themselves significantly increase acoustic\nattenuation. Robots can mitigate this attenuation using cooperative swarm\nbehaviors, with trade-offs among individual power, group performance and the\ncomplexity of the robot controllers. With such mitigating behaviors, acoustic\npower can be useful for swarms of a few hundred billion robots in the body,\nthat each use tens of picowatts, on average, and can tolerate significant\nvariability in available power, e.g, as robots in the bloodstream move from\nnear the skin to deep within the body, or from low- to high-attenuation tissue\nsuch as the lungs.",
    "descriptor": "",
    "authors": [
      "Tad Hogg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03923"
  },
  {
    "id": "arXiv:2106.03924",
    "title": "News consumption and social media regulations policy",
    "abstract": "Users online tend to consume information adhering to their system of beliefs\nand to ignore dissenting information. During the COVID-19 pandemic, users get\nexposed to a massive amount of information about a new topic having a high\nlevel of uncertainty. In this paper, we analyze two social media that enforced\nopposite moderation methods, Twitter and Gab, to assess the interplay between\nnews consumption and content regulation concerning COVID-19. We compare the two\nplatforms on about three million pieces of content analyzing user interaction\nwith respect to news articles. We first describe users' consumption patterns on\nthe two platforms focusing on the political leaning of news outlets. Finally,\nwe characterize the echo chamber effect by modeling the dynamics of users'\ninteraction networks. Our results show that the presence of moderation pursued\nby Twitter produces a significant reduction of questionable content, with a\nconsequent affiliation towards reliable sources in terms of engagement and\ncomments. Conversely, the lack of clear regulation on Gab results in the\ntendency of the user to engage with both types of content, showing a slight\npreference for the questionable ones which may account for a\ndissing/endorsement behavior. Twitter users show segregation towards reliable\ncontent with a uniform narrative. Gab, instead, offers a more heterogeneous\nstructure where users, independently of their leaning, follow people who are\nslightly polarized towards questionable news.",
    "descriptor": "",
    "authors": [
      "Gabriele Etta",
      "Matteo Cinelli",
      "Alessandro Galeazzi",
      "Carlo Michele Valensise",
      "Mauro Conti",
      "Walter Quattrociocchi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.03924"
  },
  {
    "id": "arXiv:2106.03926",
    "title": "Reconciling Rewards with Predictive State Representations",
    "abstract": "Predictive state representations (PSRs) are models of controlled non-Markov\nobservation sequences which exhibit the same generative process governing POMDP\nobservations without relying on an underlying latent state. In that respect, a\nPSR is indistinguishable from the corresponding POMDP. However, PSRs\nnotoriously ignore the notion of rewards, which undermines the general utility\nof PSR models for control, planning, or reinforcement learning. Therefore, we\ndescribe a sufficient and necessary accuracy condition which determines whether\na PSR is able to accurately model POMDP rewards, we show that rewards can be\napproximated even when the accuracy condition is not satisfied, and we find\nthat a non-trivial number of POMDPs taken from a well-known third-party\nrepository do not satisfy the accuracy condition. We propose reward-predictive\nstate representations (R-PSRs), a generalization of PSRs which accurately\nmodels both observations and rewards, and develop value iteration for R-PSRs.\nWe show that there is a mismatch between optimal POMDP policies and the optimal\nPSR policies derived from approximate rewards. On the other hand, optimal R-PSR\npolicies perfectly match optimal POMDP policies, reconfirming R-PSRs as\naccurate state-less generative models of observations and rewards.",
    "descriptor": "\nComments: IJCAI 2021\n",
    "authors": [
      "Andrea Baisero",
      "Christopher Amato"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03926"
  },
  {
    "id": "arXiv:2106.03927",
    "title": "Improving Social Welfare While Preserving Autonomy via a Pareto Mediator",
    "abstract": "Machine learning algorithms often make decisions on behalf of agents with\nvaried and sometimes conflicting interests. In domains where agents can choose\nto take their own action or delegate their action to a central mediator, an\nopen question is how mediators should take actions on behalf of delegating\nagents. The main existing approach uses delegating agents to punish\nnon-delegating agents in an attempt to get all agents to delegate, which tends\nto be costly for all. We introduce a Pareto Mediator which aims to improve\noutcomes for delegating agents without making any of them worse off. Our\nexperiments in random normal form games, a restaurant recommendation game, and\na reinforcement learning sequential social dilemma show that the Pareto\nMediator greatly increases social welfare. Also, even when the Pareto Mediator\nis based on an incorrect model of agent utility, performance gracefully\ndegrades to the pre-intervention level, due to the individual autonomy\npreserved by the voluntary mediator.",
    "descriptor": "",
    "authors": [
      "Stephen McAleer",
      "John Lanier",
      "Michael Dennis",
      "Pierre Baldi",
      "Roy Fox"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.03927"
  },
  {
    "id": "arXiv:2106.03931",
    "title": "Closed-Form Analytical Results for Maximum Entropy Reinforcement  Learning",
    "abstract": "We introduce a mapping between Maximum Entropy Reinforcement Learning (MaxEnt\nRL) and Markovian processes conditioned on rare events. In the long time limit,\nthis mapping allows us to derive analytical expressions for the optimal policy,\ndynamics and initial state distributions for the general case of stochastic\ndynamics in MaxEnt RL. We find that soft-$\\mathcal{Q}$ functions in MaxEnt RL\ncan be obtained from the Perron-Frobenius eigenvalue and the corresponding left\neigenvector of a regular, non-negative matrix derived from the underlying\nMarkov Decision Process (MDP). The results derived lead to novel algorithms for\nmodel-based and model-free MaxEnt RL, which we validate by numerical\nsimulations. The mapping established in this work opens further avenues for the\napplication of novel analytical and computational approaches to problems in\nMaxEnt RL. We make our code available at:\nhttps://github.com/argearriojas/maxent-rl-mdp-scripts",
    "descriptor": "",
    "authors": [
      "Argenis Arriojas",
      "Stas Tiomkin",
      "Rahul V. Kulkarni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03931"
  },
  {
    "id": "arXiv:2106.03932",
    "title": "How to Design a Three-Stage Architecture for Audio-Visual Active Speaker  Detection in the Wild",
    "abstract": "Successful active speaker detection requires a three-stage pipeline: (i)\naudio-visual encoding for all speakers in the clip, (ii) inter-speaker relation\nmodeling between a reference speaker and the background speakers within each\nframe, and (iii) temporal modeling for the reference speaker. Each stage of\nthis pipeline plays an important role for the final performance of the created\narchitecture. Based on a series of controlled experiments, this work presents\nseveral practical guidelines for audio-visual active speaker detection.\nCorrespondingly, we present a new architecture called ASDNet, which achieves a\nnew state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%\noutperforming the second best with a large margin of 4.7%. Our code and\npretrained models are publicly available.",
    "descriptor": "",
    "authors": [
      "Okan K\u00f6p\u00fckl\u00fc",
      "Maja Taseska",
      "Gerhard Rigoll"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03932"
  },
  {
    "id": "arXiv:2106.03934",
    "title": "Offline Policy Comparison under Limited Historical Agent-Environment  Interactions",
    "abstract": "We address the challenge of policy evaluation in real-world applications of\nreinforcement learning systems where the available historical data is limited\ndue to ethical, practical, or security considerations. This constrained\ndistribution of data samples often leads to biased policy evaluation estimates.\nTo remedy this, we propose that instead of policy evaluation, one should\nperform policy comparison, i.e. to rank the policies of interest in terms of\ntheir value based on available historical data. In addition we present the\nLimited Data Estimator (LDE) as a simple method for evaluating and comparing\npolicies from a small number of interactions with the environment. According to\nour theoretical analysis, the LDE is shown to be statistically reliable on\npolicy comparison tasks under mild assumptions on the distribution of the\nhistorical data. Additionally, our numerical experiments compare the LDE to\nother policy evaluation methods on the task of policy ranking and demonstrate\nits advantage in various settings.",
    "descriptor": "",
    "authors": [
      "Anton Dereventsov",
      "Joseph D. Daws Jr.",
      "Clayton Webster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03934"
  },
  {
    "id": "arXiv:2106.03937",
    "title": "Byakto Speech: Real-time long speech synthesis with convolutional neural  network: Transfer learning from English to Bangla",
    "abstract": "Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts",
    "descriptor": "",
    "authors": [
      "Zabir Al Nazi",
      "Sayed Mohammed Tasmimul Huda"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03937"
  },
  {
    "id": "arXiv:2106.03941",
    "title": "Progressive Multi-scale Fusion Network for RGB-D Salient Object  Detection",
    "abstract": "Salient object detection(SOD) aims at locating the most significant object\nwithin a given image. In recent years, great progress has been made in applying\nSOD on many vision tasks. The depth map could provide additional spatial prior\nand boundary cues to boost the performance. Combining the depth information\nwith image data obtained from standard visual cameras has been widely used in\nrecent SOD works, however, introducing depth information in a suboptimal fusion\nstrategy may have negative influence in the performance of SOD. In this paper,\nwe discuss about the advantages of the so-called progressive multi-scale fusion\nmethod and propose a mask-guided feature aggregation module(MGFA). The proposed\nframework can effectively combine the two features of different modalities and,\nfurthermore, alleviate the impact of erroneous depth features, which are\ninevitably caused by the variation of depth quality. We further introduce a\nmask-guided refinement module(MGRM) to complement the high-level semantic\nfeatures and reduce the irrelevant features from multi-scale fusion, leading to\nan overall refinement of detection. Experiments on five challenging benchmarks\ndemonstrate that the proposed method outperforms 11 state-of-the-art methods\nunder different evaluation metrics.",
    "descriptor": "",
    "authors": [
      "Guangyu Ren",
      "Yanchu Xie",
      "Tianhong Dai",
      "Tania Stathaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03941"
  },
  {
    "id": "arXiv:2106.03943",
    "title": "Near-Optimal Dispersion on Arbitrary Anonymous Graphs",
    "abstract": "Given an undirected, anonymous, port-labeled graph of $n$ memory-less nodes,\n$m$ edges, and degree $\\Delta$, we consider the problem of dispersing $k\\leq n$\nrobots (or tokens) positioned initially arbitrarily on one or more nodes of the\ngraph to exactly $k$ different nodes of the graph, one on each node. The\nobjective is to simultaneously minimize time to achieve dispersion and memory\nrequirement at each robot. If all $k$ robots are positioned initially on a\nsingle node, depth first search (DFS) traversal solves this problem in\n$O(\\min\\{m,k\\Delta\\})$ time with $\\Theta(\\log(k+\\Delta))$ bits at each robot.\nHowever, if robots are positioned initially on multiple nodes, the best\npreviously known algorithm solves this problem in $O(\\min\\{m,k\\Delta\\}\\cdot\n\\log \\ell)$ time storing $\\Theta(\\log(k+\\Delta))$ bits at each robot, where\n$\\ell\\leq k/2$ is the number of multiplicity nodes in the initial\nconfiguration. In this paper, we present a novel multi-source DFS traversal\nalgorithm solving this problem in $O(\\min\\{m,k\\Delta\\})$ time with\n$\\Theta(\\log(k+\\Delta))$ bits at each robot, improving the time bound of the\nbest previously known algorithm by $O(\\log \\ell)$ and matching asymptotically\nthe single-source DFS traversal bounds. This is the first algorithm for\ndispersion that is optimal in both time and memory in arbitrary anonymous\ngraphs of constant degree, $\\Delta=O(1)$. Furthermore, the result holds in both\nsynchronous and asynchronous settings.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Ajay D. Kshemkalyani",
      "Gokarna Sharma"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03943"
  },
  {
    "id": "arXiv:2106.03947",
    "title": "TENGraD: Time-Efficient Natural Gradient Descent with Exact Fisher-Block  Inversion",
    "abstract": "This work proposes a time-efficient Natural Gradient Descent method, called\nTENGraD, with linear convergence guarantees. Computing the inverse of the\nneural network's Fisher information matrix is expensive in NGD because the\nFisher matrix is large. Approximate NGD methods such as KFAC attempt to improve\nNGD's running time and practical application by reducing the Fisher matrix\ninversion cost with approximation. However, the approximations do not reduce\nthe overall time significantly and lead to less accurate parameter updates and\nloss of curvature information. TENGraD improves the time efficiency of NGD by\ncomputing Fisher block inverses with a computationally efficient covariance\nfactorization and reuse method. It computes the inverse of each block exactly\nusing the Woodbury matrix identity to preserve curvature information while\nadmitting (linear) fast convergence rates. Our experiments on image\nclassification tasks for state-of-the-art deep neural architecture on CIFAR-10,\nCIFAR-100, and Fashion-MNIST show that TENGraD significantly outperforms\nstate-of-the-art NGD methods and often stochastic gradient descent in\nwall-clock time.",
    "descriptor": "",
    "authors": [
      "Saeed Soori",
      "Bugra Can",
      "Baourun Mu",
      "Mert G\u00fcrb\u00fczbalaban",
      "Maryam Mehri Dehnavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03947"
  },
  {
    "id": "arXiv:2106.03952",
    "title": "Predicting Different Types of Subtle Toxicity in Unhealthy Online  Conversations",
    "abstract": "This paper investigates the use of machine learning models for the\nclassification of unhealthy online conversations containing one or more forms\nof subtler abuse, such as hostility, sarcasm, and generalization. We leveraged\na public dataset of 44K online comments containing healthy and unhealthy\ncomments labeled with seven forms of subtle toxicity. We were able to\ndistinguish between these comments with a top micro F1-score, macro F1-score,\nand ROC-AUC of 88.76%, 67.98%, and 0.71, respectively. Hostile comments were\neasier to detect than other types of unhealthy comments. We also conducted a\nsentiment analysis which revealed that most types of unhealthy comments were\nassociated with a slight negative sentiment, with hostile comments being the\nmost negative ones.",
    "descriptor": "",
    "authors": [
      "Shlok Gilda",
      "Mirela Silva",
      "Luiz Giovanini",
      "Daniela Oliveira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03952"
  },
  {
    "id": "arXiv:2106.03953",
    "title": "Neural Abstractive Unsupervised Summarization of Online News Discussions",
    "abstract": "Summarization has usually relied on gold standard summaries to train\nextractive or abstractive models. Social media brings a hurdle to summarization\ntechniques since it requires addressing a multi-document multi-author approach.\nWe address this challenging task by introducing a novel method that generates\nabstractive summaries of online news discussions. Our method extends a\nBERT-based architecture, including an attention encoding that fed comments'\nlikes during the training stage. To train our model, we define a task which\nconsists of reconstructing high impact comments based on popularity (likes).\nAccordingly, our model learns to summarize online discussions based on their\nmost relevant comments. Our novel approach provides a summary that represents\nthe most relevant aspects of a news item that users comment on, incorporating\nthe social context as a source of information to summarize texts in online\nsocial networks. Our model is evaluated using ROUGE scores between the\ngenerated summary and each comment on the thread. Our model, including the\nsocial attention encoding, significantly outperforms both extractive and\nabstractive summarization methods based on such evaluation.",
    "descriptor": "",
    "authors": [
      "Ignacio Tampe Palma",
      "Marcelo Mendoza",
      "Evangelos Milios"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03953"
  },
  {
    "id": "arXiv:2106.03954",
    "title": "Evaluating Meta-Feature Selection for the Algorithm Recommendation  Problem",
    "abstract": "With the popularity of Machine Learning (ML) solutions, algorithms and data\nhave been released faster than the capacity of processing them. In this\ncontext, the problem of Algorithm Recommendation (AR) is receiving a\nsignificant deal of attention recently. This problem has been addressed in the\nliterature as a learning task, often as a Meta-Learning problem where the aim\nis to recommend the best alternative for a specific dataset. For such, datasets\nencoded by meta-features are explored by ML algorithms that try to learn the\nmapping between meta-representations and the best technique to be used. One of\nthe challenges for the successful use of ML is to define which features are the\nmost valuable for a specific dataset since several meta-features can be used,\nwhich increases the meta-feature dimension. This paper presents an empirical\nanalysis of Feature Selection and Feature Extraction in the meta-level for the\nAR problem. The present study was focused on three criteria: predictive\nperformance, dimensionality reduction, and pipeline runtime. As we verified,\napplying Dimensionality Reduction (DR) methods did not improve predictive\nperformances in general. However, DR solutions reduced about 80% of the\nmeta-features, obtaining pretty much the same performance as the original setup\nbut with lower runtimes. The only exception was PCA, which presented about the\nsame runtime as the original meta-features. Experimental results also showed\nthat various datasets have many non-informative meta-features and that it is\npossible to obtain high predictive performance using around 20% of the original\nmeta-features. Therefore, due to their natural trend for high dimensionality,\nDR methods should be used for Meta-Feature Selection and Meta-Feature\nExtraction.",
    "descriptor": "",
    "authors": [
      "Geand Trindade Pereira",
      "Moises Rocha dos Santos",
      "Andre Carlos Ponce de Leon Ferreira de Carvalho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03954"
  },
  {
    "id": "arXiv:2106.03955",
    "title": "Correcting Momentum in Temporal Difference Learning",
    "abstract": "A common optimization tool used in deep reinforcement learning is momentum,\nwhich consists in accumulating and discounting past gradients, reapplying them\nat each iteration. We argue that, unlike in supervised learning, momentum in\nTemporal Difference (TD) learning accumulates gradients that become doubly\nstale: not only does the gradient of the loss change due to parameter updates,\nthe loss itself changes due to bootstrapping. We first show that this\nphenomenon exists, and then propose a first-order correction term to momentum.\nWe show that this correction term improves sample efficiency in policy\nevaluation by correcting target value drift. An important insight of this work\nis that deep RL methods are not always best served by directly importing\ntechniques from the supervised setting.",
    "descriptor": "\nComments: NeurIPS Deep RL Workshop 2020\n",
    "authors": [
      "Emmanuel Bengio",
      "Joelle Pineau",
      "Doina Precup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03955"
  },
  {
    "id": "arXiv:2106.03956",
    "title": "Novel View Video Prediction Using a Dual Representation",
    "abstract": "We address the problem of novel view video prediction; given a set of input\nvideo clips from a single/multiple views, our network is able to predict the\nvideo from a novel view. The proposed approach does not require any priors and\nis able to predict the video from wider angular distances, upto 45 degree, as\ncompared to the recent studies predicting small variations in viewpoint.\nMoreover, our method relies only onRGB frames to learn a dual representation\nwhich is used to generate the video from a novel viewpoint. The dual\nrepresentation encompasses a view-dependent and a global representation which\nincorporates complementary details to enable novel view video prediction. We\ndemonstrate the effectiveness of our framework on two real world datasets:\nNTU-RGB+D and CMU Panoptic. A comparison with the State-of-the-art novel view\nvideo prediction methods shows an improvement of 26.1% in SSIM, 13.6% in PSNR,\nand 60% inFVD scores without using explicit priors from target views.",
    "descriptor": "\nComments: Accepted in ICIP 2021\n",
    "authors": [
      "Sarah Shiraz",
      "Krishna Regmi",
      "Shruti Vyas",
      "Yogesh S. Rawat",
      "Mubarak Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03956"
  },
  {
    "id": "arXiv:2106.03958",
    "title": "Exploiting Language Relatedness for Low Web-Resource Language Model  Adaptation: An Indic Languages Study",
    "abstract": "Recent research in multilingual language models (LM) has demonstrated their\nability to effectively handle multiple languages in a single model. This holds\npromise for low web-resource languages (LRL) as multilingual models can enable\ntransfer of supervision from high resource languages to LRLs. However,\nincorporating a new language in an LM still remains a challenge, particularly\nfor languages with limited corpora and in unseen scripts. In this paper we\nargue that relatedness among languages in a language family may be exploited to\novercome some of the corpora limitations of LRLs, and propose RelateLM. We\nfocus on Indian languages, and exploit relatedness along two dimensions: (1)\nscript (since many Indic scripts originated from the Brahmic script), and (2)\nsentence structure. RelateLM uses transliteration to convert the unseen script\nof limited LRL text into the script of a Related Prominent Language (RPL)\n(Hindi in our case). While exploiting similar sentence structures, RelateLM\nutilizes readily available bilingual dictionaries to pseudo translate RPL text\ninto LRL corpora. Experiments on multiple real-world benchmark datasets provide\nvalidation to our hypothesis that using a related language as pivot, along with\ntransliteration and pseudo translation based data augmentation, can be an\neffective way to adapt LMs for LRLs, rather than direct training or pivoting\nthrough English.",
    "descriptor": "",
    "authors": [
      "Yash Khemchandani",
      "Sarvesh Mehtani",
      "Vaidehi Patil",
      "Abhijeet Awasthi",
      "Partha Talukdar",
      "Sunita Sarawagi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03958"
  },
  {
    "id": "arXiv:2106.03959",
    "title": "Generative Flows with Invertible Attentions",
    "abstract": "Flow-based generative models have shown excellent ability to explicitly learn\nthe probability density function of data via a sequence of invertible\ntransformations. Yet, modeling long-range dependencies over normalizing flows\nremains understudied. To fill the gap, in this paper, we introduce two types of\ninvertible attention mechanisms for generative flow models. To be precise, we\npropose map-based and scaled dot-product attention for unconditional and\nconditional generative flow models. The key idea is to exploit split-based\nattention mechanisms to learn the attention weights and input representations\non every two splits of flow feature maps. Our method provides invertible\nattention modules with tractable Jacobian determinants, enabling seamless\nintegration of it at any positions of the flow-based models. The proposed\nattention mechanism can model the global data dependencies, leading to more\ncomprehensive flow models. Evaluation on multiple generation tasks demonstrates\nthat the introduced attention flow idea results in efficient flow models and\ncompares favorably against the state-of-the-art unconditional and conditional\ngenerative flow methods.",
    "descriptor": "",
    "authors": [
      "Rhea Sanjay Sukthanker",
      "Zhiwu Huang",
      "Suryansh Kumar",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03959"
  },
  {
    "id": "arXiv:2106.03962",
    "title": "Amortized Generation of Sequential Counterfactual Explanations for  Black-box Models",
    "abstract": "Explainable machine learning (ML) has gained traction in recent years due to\nthe increasing adoption of ML-based systems in many sectors. Counterfactual\nexplanations (CFEs) provide ``what if'' feedback of the form ``if an input\ndatapoint were $x'$ instead of $x$, then an ML-based system's output would be\n$y'$ instead of $y$.'' CFEs are attractive due to their actionable feedback,\namenability to existing legal frameworks, and fidelity to the underlying ML\nmodel. Yet, current CFE approaches are single shot -- that is, they assume $x$\ncan change to $x'$ in a single time period. We propose a novel\nstochastic-control-based approach that generates sequential CFEs, that is, CFEs\nthat allow $x$ to move stochastically and sequentially across intermediate\nstates to a final state $x'$. Our approach is model agnostic and black box.\nFurthermore, calculation of CFEs is amortized such that once trained, it\napplies to multiple datapoints without the need for re-optimization. In\naddition to these primary characteristics, our approach admits optional\ndesiderata such as adherence to the data manifold, respect for causal\nrelations, and sparsity -- identified by past research as desirable properties\nof CFEs. We evaluate our approach using three real-world datasets and show\nsuccessful generation of sequential CFEs that respect other counterfactual\ndesiderata.",
    "descriptor": "\nComments: 19 pages, 3 figures, 4 tables\n",
    "authors": [
      "Sahil Verma",
      "Keegan Hines",
      "John P. Dickerson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03962"
  },
  {
    "id": "arXiv:2106.03965",
    "title": "A highly scalable repository of waveform and vital signs data from  bedside monitoring devices",
    "abstract": "The advent of cost effective cloud computing over the past decade and\never-growing accumulation of high-fidelity clinical data in a modern hospital\nsetting is leading to new opportunities for translational medicine. Machine\nlearning is driving the appetite of the research community for various types of\nsignal data such as patient vitals. Health care systems, however, are ill\nsuited for massive processing of large volumes of data. In addition, due to the\nsheer magnitude of the data being collected, it is not feasible to retain all\nof the data in health care systems in perpetuity. This gold mine of information\ngets purged periodically thereby losing invaluable future research\nopportunities. We have developed a highly scalable solution that: a) siphons\noff patient vital data on a nightly basis from on-premises bio-medical systems\nto a cloud storage location as a permanent archive, b) reconstructs the\ndatabase in the cloud, c) generates waveforms, alarms and numeric data in a\nresearch-ready format, and d) uploads the processed data to a storage location\nin the cloud ready for research.\nThe data is de-identified and catalogued such that it can be joined with\nElectronic Medical Records (EMR) and other ancillary data types such as\nelectroencephalogram (EEG), radiology, video monitoring etc. This technique\neliminates the research burden from health care systems. This highly scalable\nsolution is used to process high density patient monitoring data aggregated by\nthe Philips Patient Information Center iX (PIC iX) hospital surveillance system\nfor archival storage in the Philips Data Warehouse Connect enterprise-level\ndatabase. The solution is part of a broader platform that supports a secure\nhigh performance clinical data science platform.",
    "descriptor": "\nComments: 12 pages, 2 figures\n",
    "authors": [
      "Sanjay Malunjkar",
      "Susan Weber",
      "Somalee Datta"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Computers and Society (cs.CY)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03965"
  },
  {
    "id": "arXiv:2106.03969",
    "title": "Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models",
    "abstract": "We consider the problem of learning a tree-structured Ising model from data,\nsuch that subsequent predictions computed using the model are accurate.\nConcretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small\nsets of variables $S$ are accurate. Since its introduction more than 50 years\nago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood\ntree, has been the benchmark algorithm for learning tree-structured graphical\nmodels. A bound on the sample complexity of the Chow-Liu algorithm with respect\nto the prediction-centric local total variation loss was shown in [BK19]. While\nthose results demonstrated that it is possible to learn a useful model even\nwhen recovering the true underlying graph is impossible, their bound depends on\nthe maximum strength of interactions and thus does not achieve the\ninformation-theoretic optimum. In this paper, we introduce a new algorithm that\ncarefully combines elements of the Chow-Liu algorithm with tree metric\nreconstruction methods to efficiently and optimally learn tree Ising models\nunder a prediction-centric loss. Our algorithm is robust to model\nmisspecification and adversarial corruptions. In contrast, we show that the\ncelebrated Chow-Liu algorithm can be arbitrarily suboptimal.",
    "descriptor": "\nComments: 49 pages, 3 figures\n",
    "authors": [
      "Enric Boix-Adsera",
      "Guy Bresler",
      "Frederic Koehler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.03969"
  },
  {
    "id": "arXiv:2106.03973",
    "title": "Generating Hypothetical Events for Abductive Inference",
    "abstract": "Abductive reasoning starts from some observations and aims at finding the\nmost plausible explanation for these observations. To perform abduction, humans\noften make use of temporal and causal inferences, and knowledge about how some\nhypothetical situation can result in different outcomes. This work offers the\nfirst study of how such knowledge impacts the Abductive NLI task -- which\nconsists in choosing the more likely explanation for given observations. We\ntrain a specialized language model LMI that is tasked to generate what could\nhappen next from a hypothetical scenario that evolves from a given event. We\nthen propose a multi-task model MTL to solve the Abductive NLI task, which\npredicts a plausible explanation by a) considering different possible events\nemerging from candidate hypotheses -- events generated by LMI -- and b)\nselecting the one that is most similar to the observed outcome. We show that\nour MTL model improves over prior vanilla pre-trained LMs fine-tuned on\nAbductive NLI. Our manual evaluation and analysis suggest that learning about\npossible next events from different hypothetical scenarios supports abductive\ninference.",
    "descriptor": "\nComments: Proceedings of The Tenth Joint Conference on Lexical and Computational Semantics (STARSEM 2021)\n",
    "authors": [
      "Debjit Paul",
      "Anette Frank"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03973"
  },
  {
    "id": "arXiv:2106.03974",
    "title": "A Nonlinear Observability Analysis of Ambient Wind Estimation with  Uncalibrated Sensors, Inspired by Insect Neural Encoding",
    "abstract": "Estimating the direction of ambient fluid flow is key for many flying or\nswimming animals and robots, but can only be accomplished through indirect\nmeasurements and active control. Recent work with tethered flying insects\nindicates that their sensory representation of orientation, apparent flow,\ndirection of movement, and control is represented by a 2-dimensional angular\nencoding in the central brain. This representation simplifies sensory\nintegration by projecting the direction (but not scale) of measurements with\ndifferent units onto a universal polar coordinate frame. To align these angular\nmeasurements with one another and the motor system does, however, require a\ncalibration of angular gain and offset for each sensor. This calibration could\nchange with time due to changes in the environment or physical structure. The\ncircumstances under which small robots and animals with angular sensors and\nchanging calibrations could self-calibrate and estimate the direction of\nambient fluid flow while moving remains an open question. Here, a methodical\nnonlinear observability analysis is presented to address this. The analysis\nshows that it is mathematically feasible to continuously estimate flow\ndirection and perform regular self-calibrations by adopting frequent changes in\ncourse (or active prevention thereof) and orientation, and requires fusion and\ntemporal differentiation of three sensory measurements: apparent flow,\norientation (or its derivative), and direction of motion (or its derivative).\nThese conclusions are consistent with the zigzagging trajectories exhibited by\nmany plume tracking organisms, suggesting that perhaps flow estimation is a\nsecondary driver of their trajectory structure.",
    "descriptor": "\nComments: 8 pages, 3 figures, submitted to CDC 2021\n",
    "authors": [
      "Floris van Breugel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03974"
  },
  {
    "id": "arXiv:2106.03982",
    "title": "Expressivity of Emergent Language is a Trade-off between Contextual  Complexity and Unpredictability",
    "abstract": "Researchers are now using deep learning models to explore the emergence of\nlanguage in various language games, where simulated agents interact and develop\nan emergent language to solve a task. Although it is quite intuitive that\ndifferent types of language games posing different communicative challenges\nmight require emergent languages which encode different levels of information,\nthere is no existing work exploring the expressivity of the emergent languages.\nIn this work, we propose a definition of partial order between expressivity\nbased on the generalisation performance across different language games. We\nalso validate the hypothesis that expressivity of emergent languages is a\ntrade-off between the complexity and unpredictability of the context those\nlanguages are used in. Our second novel contribution is introducing contrastive\nloss into the implementation of referential games. We show that using our\ncontrastive loss alleviates the collapse of message types seen using standard\nreferential loss functions.",
    "descriptor": "\nComments: 17 pages, 11 figures, 2 tables\n",
    "authors": [
      "Shangmin Guo",
      "Yi Ren",
      "Kory Mathewson",
      "Simon Kirby",
      "Stefano V. Albrecht",
      "Kenny Smith"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03982"
  },
  {
    "id": "arXiv:2106.03983",
    "title": "Investigating Transfer Learning in Multilingual Pre-trained Language  Models through Chinese Natural Language Inference",
    "abstract": "Multilingual transformers (XLM, mT5) have been shown to have remarkable\ntransfer skills in zero-shot settings. Most transfer studies, however, rely on\nautomatically translated resources (XNLI, XQuAD), making it hard to discern the\nparticular linguistic knowledge that is being transferred, and the role of\nexpert annotated monolingual datasets when developing task-specific models. We\ninvestigate the cross-lingual transfer abilities of XLM-R for Chinese and\nEnglish natural language inference (NLI), with a focus on the recent\nlarge-scale Chinese dataset OCNLI. To better understand linguistic transfer, we\ncreated 4 categories of challenge and adversarial tasks (totaling 17 new\ndatasets) for Chinese that build on several well-known resources for English\n(e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on\nEnglish NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our\nchallenge categories, they perform as well/better than the best monolingual\nmodels, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro\ndrop). These results, however, come with important caveats: cross-lingual\nmodels often perform best when trained on a mixture of English and high-quality\nmonolingual NLI data (OCNLI), and are often hindered by automatically\ntranslated resources (XNLI-zh). For many phenomena, all models continue to\nstruggle, highlighting the need for our new diagnostics to help benchmark\nChinese and cross-lingual models. All new datasets/code are released at\nhttps://github.com/huhailinguist/ChineseNLIProbing.",
    "descriptor": "\nComments: accepted to ACL Findings 2021\n",
    "authors": [
      "Hai Hu",
      "He Zhou",
      "Zuoyu Tian",
      "Yiwen Zhang",
      "Yina Ma",
      "Yanting Li",
      "Yixin Nie",
      "Kyle Richardson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03983"
  },
  {
    "id": "arXiv:2106.03987",
    "title": "Weakly Supervised Volumetric Image Segmentation with Deformed Templates",
    "abstract": "There are many approaches that use weak-supervision to train networks to\nsegment 2D images. By contrast, existing 3D approaches rely on full-supervision\nof a subset of 2D slices of the 3D image volume. In this paper, we propose an\napproach that is truly weakly-supervised in the sense that we only need to\nprovide a sparse set of 3D point on the surface of target objects, an easy task\nthat can be quickly done. We use the 3D points to deform a 3D template so that\nit roughly matches the target object outlines and we introduce an architecture\nthat exploits the supervision provided by coarse template to train a network to\nfind accurate boundaries.\nWe evaluate the performance of our approach on Computed Tomography (CT),\nMagnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets.\nWe will show that it outperforms a more traditional approach to\nweak-supervision in 3D at a reduced supervision cost.",
    "descriptor": "\nComments: 13 Pages\n",
    "authors": [
      "Udaranga Wickramasinghe",
      "Pascal Fua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03987"
  },
  {
    "id": "arXiv:2106.03988",
    "title": "Towards Learning Geometric Transformations through Play: An AR-powered  approach",
    "abstract": "Despite the excessive developments of architectural parametric platforms,\nparametric design is often interpreted as an architectural style rather than a\ncomputational method. Also, the problem is still a lack of knowledge and skill\nabout the technical application of parametric design in architectural\nmodelling. Students often dive into utilizing complex digital modelling without\nhaving a competent pedagogical context to learn algorithmic thinking and the\ncorresponding logic behind digital and parametric modelling. The insufficient\nskills and superficial knowledge often result in utilizing the modelling\nsoftware through trial and error, not taking full advantage of what it has to\noffer. Geometric transformations as the fundamental functions of parametric\nmodelling is explored in this study to anchor learning essential components in\nparametric modelling. Students need to understand the differences between\nvariables, parameters, functions and their relations. Fologram, an Augmented\nReality tool, is utilized in this study to learn geometric transformation and\nits components in an intuitive way. A LEGO set is used as an editable physical\nmodel to improve spatial skill through hand movement beside an instant feedback\nin the physical environment.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Zohreh Shaghaghian",
      "Wei Yan",
      "Dezhen Song"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.03988"
  },
  {
    "id": "arXiv:2106.03993",
    "title": "Lexicon Learning for Few-Shot Neural Sequence Modeling",
    "abstract": "Sequence-to-sequence transduction is the core problem in language processing\napplications as diverse as semantic parsing, machine translation, and\ninstruction following. The neural network models that provide the dominant\nsolution to these problems are brittle, especially in low-resource settings:\nthey fail to generalize correctly or systematically from small datasets. Past\nwork has shown that many failures of systematic generalization arise from\nneural models' inability to disentangle lexical phenomena from syntactic ones.\nTo address this, we augment neural decoders with a lexical translation\nmechanism that generalizes existing copy mechanisms to incorporate learned,\ndecontextualized, token-level translation rules. We describe how to initialize\nthis mechanism using a variety of lexicon learning algorithms, and show that it\nimproves systematic generalization on a diverse set of sequence modeling tasks\ndrawn from cognitive science, formal semantics, and machine translation.",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Ekin Aky\u00fcrek",
      "Jacob Andreas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03993"
  },
  {
    "id": "arXiv:2106.03996",
    "title": "Occode: an end-to-end machine learning pipeline for transcription of  historical population censuses",
    "abstract": "Machine learning approaches achieve high accuracy for text recognition and\nare therefore increasingly used for the transcription of handwritten historical\nsources. However, using machine learning in production requires a streamlined\nend-to-end machine learning pipeline that scales to the dataset size, and a\nmodel that achieves high accuracy with few manual transcriptions. In addition,\nthe correctness of the model results must be verified. This paper describes our\nlessons learned developing, tuning, and using the Occode end-to-end machine\nlearning pipeline for transcribing 7,3 million rows with handwritten occupation\ncodes in the Norwegian 1950 population census. We achieve an accuracy of 97%\nfor the automatically transcribed codes, and we send 3% of the codes for manual\nverification. We verify that the occupation code distribution found in our\nresult matches the distribution found in our training data which should be\nrepresentative for the census as a whole. We believe our approach and lessons\nlearned are useful for other transcription projects that plan to use machine\nlearning in production. The source code is available at:\nhttps://github.com/uit-hdl/rhd-codes",
    "descriptor": "",
    "authors": [
      "Bj\u00f8rn-Richard Pedersen",
      "Einar Holsb\u00f8",
      "Trygve Andersen",
      "Nikita Shvetsov",
      "Johan Ravn",
      "Hilde Leikny Sommerseth",
      "Lars Ailo Bongo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03996"
  },
  {
    "id": "arXiv:2106.03998",
    "title": "Asynchronous Distributed Optimization with Redundancy in Cost Functions",
    "abstract": "This paper considers the problem of asynchronous distributed multi-agent\noptimization on server-based system architecture. In this problem, each agent\nhas a local cost, and the goal for the agents is to collectively find a minimum\nof their aggregate cost. A standard algorithm to solve this problem is the\niterative distributed gradient-descent (DGD) method being implemented\ncollaboratively by the server and the agents. In the synchronous setting, the\nalgorithm proceeds from one iteration to the next only after all the agents\ncomplete their expected communication with the server. However, such synchrony\ncan be expensive and even infeasible in real-world applications. We show that\nwaiting for all the agents is unnecessary in many applications of distributed\noptimization, including distributed machine learning, due to redundancy in the\ncost functions (or {\\em data}). Specifically, we consider a generic notion of\nredundancy named $(r,\\epsilon)$-redundancy implying solvability of the original\nmulti-agent optimization problem with $\\epsilon$ accuracy, despite the removal\nof up to $r$ (out of total $n$) agents from the system. We present an\nasynchronous DGD algorithm where in each iteration the server only waits for\n(any) $n-r$ agents, instead of all the $n$ agents. Assuming\n$(r,\\epsilon)$-redundancy, we show that our asynchronous algorithm converges to\nan approximate solution with error that is linear in $\\epsilon$ and $r$.\nMoreover, we also present a generalization of our algorithm to tolerate some\nByzantine faulty agents in the system. Finally, we demonstrate the improved\ncommunication efficiency of our algorithm through experiments on MNIST and\nFashion-MNIST using the benchmark neural network LeNet.",
    "descriptor": "\nComments: 37 pages, 4 figures. Related to our prior work on Byzantine fault-tolerance distributed optimization in redundancy in cost functions (doi:10.1145/3382734.3405748 and arXiv:2101.09337)\n",
    "authors": [
      "Shuo Liu",
      "Nirupam Gupta",
      "Nitin H. Vaidya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03998"
  },
  {
    "id": "arXiv:2106.04003",
    "title": "Double Descent and Other Interpolation Phenomena in GANs",
    "abstract": "We study overparameterization in generative adversarial networks (GANs) that\ncan interpolate the training data. We show that overparameterization can\nimprove generalization performance and accelerate the training process. We\nstudy the generalization error as a function of latent space dimension and\nidentify two main behaviors, depending on the learning setting. First, we show\nthat overparameterized generative models that learn distributions by minimizing\na metric or $f$-divergence do not exhibit double descent in generalization\nerrors; specifically, all the interpolating solutions achieve the same\ngeneralization error. Second, we develop a new pseudo-supervised learning\napproach for GANs where the training utilizes pairs of fabricated (noise)\ninputs in conjunction with real output samples. Our pseudo-supervised setting\nexhibits double descent (and in some cases, triple descent) of generalization\nerrors. We combine pseudo-supervision with overparameterization (i.e., overly\nlarge latent space dimension) to accelerate training while performing better,\nor close to, the generalization performance without pseudo-supervision. While\nour analysis focuses mostly on linear GANs, we also apply important insights\nfor improving generalization of nonlinear, multilayer GANs.",
    "descriptor": "",
    "authors": [
      "Lorenzo Luzi",
      "Yehuda Dar",
      "Richard Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04003"
  },
  {
    "id": "arXiv:2106.04004",
    "title": "Task-Generic Hierarchical Human Motion Prior using VAEs",
    "abstract": "A deep generative model that describes human motions can benefit a wide range\nof fundamental computer vision and graphics tasks, such as providing robustness\nto video-based human pose estimation, predicting complete body movements for\nmotion capture systems during occlusions, and assisting key frame animation\nwith plausible movements. In this paper, we present a method for learning\ncomplex human motions independent of specific tasks using a combined global and\nlocal latent space to facilitate coarse and fine-grained modeling.\nSpecifically, we propose a hierarchical motion variational autoencoder (HM-VAE)\nthat consists of a 2-level hierarchical latent space. While the global latent\nspace captures the overall global body motion, the local latent space enables\nto capture the refined poses of the different body parts. We demonstrate the\neffectiveness of our hierarchical motion variational autoencoder in a variety\nof tasks including video-based human pose estimation, motion completion from\npartial observations, and motion synthesis from sparse key-frames. Even though,\nour model has not been trained for any of these tasks specifically, it provides\nsuperior performance than task-specific alternatives. Our general-purpose human\nmotion prior model can fix corrupted human body animations and generate\ncomplete movements from incomplete observations.",
    "descriptor": "",
    "authors": [
      "Jiaman Li",
      "Ruben Villegas",
      "Duygu Ceylan",
      "Jimei Yang",
      "Zhengfei Kuang",
      "Hao Li",
      "Yajie Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.04004"
  },
  {
    "id": "arXiv:2106.04007",
    "title": "Self-Supervised Structure-from-Motion through Tightly-Coupled Depth and  Egomotion Networks",
    "abstract": "Much recent literature has formulated structure-from-motion (SfM) as a\nself-supervised learning problem where the goal is to jointly learn neural\nnetwork models of depth and egomotion through view synthesis. Herein, we\naddress the open problem of how to optimally couple the depth and egomotion\nnetwork components. Toward this end, we introduce several notions of coupling,\ncategorize existing approaches, and present a novel tightly-coupled approach\nthat leverages the interdependence of depth and egomotion at training and at\ninference time. Our approach uses iterative view synthesis to recursively\nupdate the egomotion network input, permitting contextual information to be\npassed between the components without explicit weight sharing. Through\nsubstantial experiments, we demonstrate that our approach promotes consistency\nbetween the depth and egomotion predictions at test time, improves\ngeneralization on new data, and leads to state-of-the-art accuracy on indoor\nand outdoor depth and egomotion evaluation benchmarks.",
    "descriptor": "\nComments: Submitted to NeurIPS 2021\n",
    "authors": [
      "Brandon Wagstaff",
      "Valentin Peretroukhin",
      "Jonathan Kelly"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04007"
  },
  {
    "id": "arXiv:2106.04008",
    "title": "Widening Access to Applied Machine Learning with TinyML",
    "abstract": "Broadening access to both computational and educational resources is critical\nto diffusing machine-learning (ML) innovation. However, today, most ML\nresources and experts are siloed in a few countries and organizations. In this\npaper, we describe our pedagogical approach to increasing access to applied ML\nthrough a massive open online course (MOOC) on Tiny Machine Learning (TinyML).\nWe suggest that TinyML, ML on resource-constrained embedded devices, is an\nattractive means to widen access because TinyML both leverages low-cost and\nglobally accessible hardware, and encourages the development of complete,\nself-contained applications, from data collection to deployment. To this end, a\ncollaboration between academia (Harvard University) and industry (Google)\nproduced a four-part MOOC that provides application-oriented instruction on how\nto develop solutions using TinyML. The series is openly available on the edX\nMOOC platform, has no prerequisites beyond basic programming, and is designed\nfor learners from a global variety of backgrounds. It introduces pupils to\nreal-world applications, ML algorithms, data-set engineering, and the ethical\nconsiderations of these technologies via hands-on programming and deployment of\nTinyML applications in both the cloud and their own microcontrollers. To\nfacilitate continued learning, community building, and collaboration beyond the\ncourses, we launched a standalone website, a forum, a chat, and an optional\ncourse-project competition. We also released the course materials publicly,\nhoping they will inspire the next generation of ML practitioners and educators\nand further broaden access to cutting-edge ML technologies.",
    "descriptor": "\nComments: Understanding the underpinnings of the TinyML edX course series: this https URL\n",
    "authors": [
      "Vijay Janapa Reddi",
      "Brian Plancher",
      "Susan Kennedy",
      "Laurence Moroney",
      "Pete Warden",
      "Anant Agarwal",
      "Colby Banbury",
      "Massimo Banzi",
      "Matthew Bennett",
      "Benjamin Brown",
      "Sharad Chitlangia",
      "Radhika Ghosal",
      "Sarah Grafman",
      "Rupert Jaeger",
      "Srivatsan Krishnan",
      "Maximilian Lam",
      "Daniel Leiker",
      "Cara Mann",
      "Mark Mazumder",
      "Dominic Pajak",
      "Dhilan Ramaprasad",
      "J. Evan Smith",
      "Matthew Stewart",
      "Dustin Tingley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04008"
  },
  {
    "id": "arXiv:2106.04009",
    "title": "Rotating spiders and reflecting dogs: a class conditional approach to  learning data augmentation distributions",
    "abstract": "Building invariance to non-meaningful transformations is essential to\nbuilding efficient and generalizable machine learning models. In practice, the\nmost common way to learn invariance is through data augmentation. There has\nbeen recent interest in the development of methods that learn distributions on\naugmentation transformations from the training data itself. While such\napproaches are beneficial since they are responsive to the data, they ignore\nthe fact that in many situations the range of transformations to which a model\nneeds to be invariant changes depending on the particular class input belongs\nto. For example, if a model needs to be able to predict whether an image\ncontains a starfish or a dog, we may want to apply random rotations to starfish\nimages during training (since these do not have a preferred orientation), but\nwe would not want to do this to images of dogs. In this work we introduce a\nmethod by which we can learn class conditional distributions on augmentation\ntransformations. We give a number of examples where our methods learn different\nnon-meaningful transformations depending on class and further show how our\nmethod can be used as a tool to probe the symmetries intrinsic to a potentially\ncomplex dataset.",
    "descriptor": "\nComments: 10 pages, 6 figures, submitted to NeurIPS 2021\n",
    "authors": [
      "Scott Mahan",
      "Henry Kvinge",
      "Tim Doster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04009"
  },
  {
    "id": "arXiv:2106.04010",
    "title": "FEAR: A Simple Lightweight Method to Rank Architectures",
    "abstract": "The fundamental problem in Neural Architecture Search (NAS) is to efficiently\nfind high-performing architectures from a given search space. We propose a\nsimple but powerful method which we call FEAR, for ranking architectures in any\nsearch space. FEAR leverages the viewpoint that neural networks are powerful\nnon-linear feature extractors. First, we train different architectures in the\nsearch space to the same training or validation error. Then, we compare the\nusefulness of the features extracted by each architecture. We do so with a\nquick training keeping most of the architecture frozen. This gives fast\nestimates of the relative performance. We validate FEAR on Natsbench topology\nsearch space on three different datasets against competing baselines and show\nstrong ranking correlation especially compared to recently proposed zero-cost\nmethods. FEAR particularly excels at ranking high-performance architectures in\nthe search space. When used in the inner loop of discrete search algorithms\nlike random search, FEAR can cut down the search time by approximately 2.4X\nwithout losing accuracy. We additionally empirically study very recently\nproposed zero-cost measures for ranking and find that they breakdown in ranking\nperformance as training proceeds and also that data-agnostic ranking scores\nwhich ignore the dataset do not generalize across dissimilar datasets.",
    "descriptor": "\nComments: 31 pages, 8 figures\n",
    "authors": [
      "Debadeepta Dey",
      "Shital Shah",
      "Sebastien Bubeck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04010"
  },
  {
    "id": "arXiv:2106.04011",
    "title": "JANUS: Parallel Tempered Genetic Algorithm Guided by Deep Neural  Networks for Inverse Molecular Design",
    "abstract": "Inverse molecular design, i.e., designing molecules with specific target\nproperties, can be posed as an optimization problem. High-dimensional\noptimization tasks in the natural sciences are commonly tackled via\npopulation-based metaheuristic optimization algorithms such as evolutionary\nalgorithms. However, expensive property evaluation, which is often required,\ncan limit the widespread use of such approaches as the associated cost can\nbecome prohibitive. Herein, we present JANUS, a genetic algorithm that is\ninspired by parallel tempering. It propagates two populations, one for\nexploration and another for exploitation, improving optimization by reducing\nexpensive property evaluations. Additionally, JANUS is augmented by a deep\nneural network that approximates molecular properties via active learning for\nenhanced sampling of the chemical space. Our method uses the SELFIES molecular\nrepresentation and the STONED algorithm for the efficient generation of\nstructures, and outperforms other generative models in common inverse molecular\ndesign tasks achieving state-of-the-art performance.",
    "descriptor": "\nComments: 20 pages, 12 figures, 4 tables. Comments are welcome! (code will be uploaded when paper is formally published)\n",
    "authors": [
      "AkshatKumar Nigam",
      "Robert Pollice",
      "Alan Aspuru-Guzik"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04011"
  },
  {
    "id": "arXiv:2106.04015",
    "title": "Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep  Learning",
    "abstract": "High-quality estimates of uncertainty and robustness are crucial for numerous\nreal-world applications, especially for deep learning which underlies many\ndeployed ML systems. The ability to compare techniques for improving these\nestimates is therefore very important for research and practice alike. Yet,\ncompetitive comparisons of methods are often lacking due to a range of reasons,\nincluding: compute availability for extensive tuning, incorporation of\nsufficiently many baselines, and concrete documentation for reproducibility. In\nthis paper we introduce Uncertainty Baselines: high-quality implementations of\nstandard and state-of-the-art deep learning methods on a variety of tasks. As\nof this writing, the collection spans 19 methods across 9 tasks, each with at\nleast 5 metrics. Each baseline is a self-contained experiment pipeline with\neasily reusable and extendable components. Our goal is to provide immediate\nstarting points for experimentation with new methods or applications.\nAdditionally we provide model checkpoints, experiment outputs as Python\nnotebooks, and leaderboards for comparing results. Code available at\nhttps://github.com/google/uncertainty-baselines.",
    "descriptor": "",
    "authors": [
      "Zachary Nado",
      "Neil Band",
      "Mark Collier",
      "Josip Djolonga",
      "Michael W. Dusenberry",
      "Sebastian Farquhar",
      "Angelos Filos",
      "Marton Havasi",
      "Rodolphe Jenatton",
      "Ghassen Jerfel",
      "Jeremiah Liu",
      "Zelda Mariet",
      "Jeremy Nixon",
      "Shreyas Padhy",
      "Jie Ren",
      "Tim G. J. Rudner",
      "Yeming Wen",
      "Florian Wenzel",
      "Kevin Murphy",
      "D. Sculley",
      "Balaji Lakshminarayanan",
      "Jasper Snoek",
      "Yarin Gal",
      "Dustin Tran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04015"
  },
  {
    "id": "arXiv:2106.04016",
    "title": "Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question  Answering",
    "abstract": "Disfluencies is an under-studied topic in NLP, even though it is ubiquitous\nin human conversation. This is largely due to the lack of datasets containing\ndisfluencies. In this paper, we present a new challenge question answering\ndataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual\ndisfluencies in previously fluent questions. Disfl-QA contains a variety of\nchallenging disfluencies that require a more comprehensive understanding of the\ntext than what was necessary in prior datasets. Experiments show that the\nperformance of existing state-of-the-art question answering models degrades\nsignificantly when tested on Disfl-QA in a zero-shot setting.We show data\naugmentation methods partially recover the loss in performance and also\ndemonstrate the efficacy of using gold data for fine-tuning. We argue that we\nneed large-scale disfluency datasets in order for NLP models to be robust to\nthem. The dataset is publicly available at:\nhttps://github.com/google-research-datasets/disfl-qa.",
    "descriptor": "\nComments: Findings of ACL 2021\n",
    "authors": [
      "Aditya Gupta",
      "Jiacheng Xu",
      "Shyam Upadhyay",
      "Diyi Yang",
      "Manaal Faruqui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04016"
  },
  {
    "id": "arXiv:2106.04021",
    "title": "Hybrid Method Based on NARX models and Machine Learning for Pattern  Recognition",
    "abstract": "This work presents a novel technique that integrates the methodologies of\nmachine learning and system identification to solve multiclass problems. Such\nan approach allows to extract and select sets of representative features with\nreduced dimensionality, as well as predicts categorical outputs. The efficiency\nof the method was tested by running case studies investigated in machine\nlearning, obtaining better absolute results when compared with classical\nclassification algorithms.",
    "descriptor": "\nComments: In English. SBAI 2021 - Brazilian Symposium on Intelligent Automation (SBAI - Simposio Brasileiro de Automacao Inteligente). 6 pages. 4 figures\n",
    "authors": [
      "P. H. O. Silva",
      "A. S. Cerqueira",
      "E. G. Nepomuceno"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04021"
  },
  {
    "id": "arXiv:2106.04024",
    "title": "Manifold Topology Divergence: a Framework for Comparing Data Manifolds",
    "abstract": "We develop a framework for comparing data manifolds, aimed, in particular,\ntowards the evaluation of deep generative models. We describe a novel tool,\nCross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional\nspace, tracks multiscale topology spacial discrepancies between manifolds on\nwhich the distributions are concentrated. Based on the Cross-Barcode, we\nintroduce the Manifold Topology Divergence score (MTop-Divergence) and apply it\nto assess the performance of deep generative models in various domains: images,\n3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,\nCIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate\nthat the MTop-Divergence accurately detects various degrees of mode-dropping,\nintra-mode collapse, mode invention, and image disturbance. Our algorithm\nscales well (essentially linearly) with the increase of the dimension of the\nambient high-dimensional space. It is one of the first TDA-based practical\nmethodologies that can be applied universally to datasets of different sizes\nand dimensions, including the ones on which the most recent GANs in the visual\ndomain are trained. The proposed method is domain agnostic and does not rely on\npre-trained networks.",
    "descriptor": "",
    "authors": [
      "Serguei Barannikov",
      "Ilya Trofimov",
      "Grigorii Sotnikov",
      "Ekaterina Trimbach",
      "Alexander Korotin",
      "Alexander Filippov",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04024"
  },
  {
    "id": "arXiv:2106.04025",
    "title": "SpaceMeshLab: Spatial Context Memoization and Meshgrid Atrous  Convolution Consensus for Semantic Segmentation",
    "abstract": "Semantic segmentation networks adopt transfer learning from image\nclassification networks which occurs a shortage of spatial context information.\nFor this reason, we propose Spatial Context Memoization (SpaM), a bypassing\nbranch for spatial context by retaining the input dimension and constantly\ncommunicating its spatial context and rich semantic information mutually with\nthe backbone network. Multi-scale context information for semantic segmentation\nis crucial for dealing with diverse sizes and shapes of target objects in the\ngiven scene. Conventional multi-scale context scheme adopts multiple effective\nreceptive fields by multiple dilation rates or pooling operations, but often\nsuffer from misalignment problem with respect to the target pixel. To this end,\nwe propose Meshgrid Atrous Convolution Consensus (MetroCon^2) which brings\nmulti-scale scheme into fine-grained multi-scale object context using\nconvolutions with meshgrid-like scattered dilation rates. SpaceMeshLab\n(ResNet-101 + SpaM + MetroCon^2) achieves 82.0% mIoU in Cityscapes test and\n53.5% mIoU on Pascal-Context validation set.",
    "descriptor": "\nComments: 5 pages, 3 figures, 4 tables. To appear in the proceedings of the 28th IEEE International Conference on Image Processing (IEEE - ICIP), September 19-22, 2021, Anchorage, Alaska, USA\n",
    "authors": [
      "Taehun Kim",
      "Jinseong Kim",
      "Daijin Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04025"
  },
  {
    "id": "arXiv:2106.04026",
    "title": "Subject-Independent Brain-Computer Interface for Decoding High-Level  Visual Imagery Tasks",
    "abstract": "Brain-computer interface (BCI) is used for communication between humans and\ndevices by recognizing status and intention of humans. Communication between\nhumans and a drone using electroencephalogram (EEG) signals is one of the most\nchallenging issues in the BCI domain. In particular, the control of drone\nswarms (the direction and formation) has more advantages compared to the\ncontrol of a drone. The visual imagery (VI) paradigm is that subjects visually\nimagine specific objects or scenes. Reduction of the variability among EEG\nsignals of subjects is essential for practical BCI-based systems. In this\nstudy, we proposed the subepoch-wise feature encoder (SEFE) to improve the\nperformances in the subject-independent tasks by using the VI dataset. This\nstudy is the first attempt to demonstrate the possibility of generalization\namong subjects in the VI-based BCI. We used the leave-one-subject-out\ncross-validation for evaluating the performances. We obtained higher\nperformances when including our proposed module than excluding our proposed\nmodule. The DeepConvNet with SEFE showed the highest performance of 0.72 among\nsix different decoding models. Hence, we demonstrated the feasibility of\ndecoding the VI dataset in the subject-independent task with robust\nperformances by using our proposed module.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Dae-Hyeok Lee",
      "Dong-Kyun Han",
      "Sung-Jin Kim",
      "Ji-Hoon Jeong",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.04026"
  },
  {
    "id": "arXiv:2106.04028",
    "title": "Deep Learning Statistical Arbitrage",
    "abstract": "Statistical arbitrage identifies and exploits temporal price differences\nbetween similar assets. We propose a unifying conceptual framework for\nstatistical arbitrage and develop a novel deep learning solution, which finds\ncommonality and time-series patterns from large panels in a data-driven and\nflexible way. First, we construct arbitrage portfolios of similar assets as\nresidual portfolios from conditional latent asset pricing factors. Second, we\nextract the time series signals of these residual portfolios with one of the\nmost powerful machine learning time-series solutions, a convolutional\ntransformer. Last, we use these signals to form an optimal trading policy, that\nmaximizes risk-adjusted returns under constraints. We conduct a comprehensive\nempirical comparison study with daily large cap U.S. stocks. Our optimal\ntrading strategy obtains a consistently high out-of-sample Sharpe ratio and\nsubstantially outperforms all benchmark approaches. It is orthogonal to common\nrisk factors, and exploits asymmetric local trend and reversion patterns. Our\nstrategies remain profitable after taking into account trading frictions and\ncosts. Our findings suggest a high compensation for arbitrageurs to enforce the\nlaw of one price.",
    "descriptor": "",
    "authors": [
      "Jorge Guijarro-Ordonez",
      "Markus Pelger",
      "Greg Zanotti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Portfolio Management (q-fin.PM)"
    ],
    "url": "https://arxiv.org/abs/2106.04028"
  },
  {
    "id": "arXiv:2106.04029",
    "title": "Mission Level Uncertainty in Multi-Agent Resource Allocation",
    "abstract": "In recent years, a significant research effort has been devoted to the design\nof distributed protocols for the control of multi-agent systems, as the scale\nand limited communication bandwidth characteristic of such systems render\ncentralized control impossible. Given the strict operating conditions, it is\nunlikely that every agent in a multi-agent system will have local information\nthat is consistent with the true system state. Yet, the majority of works in\nthe literature assume that agents share perfect knowledge of their environment.\nThis paper focuses on understanding the impact that inconsistencies in agents'\nlocal information can have on the performance of multi-agent systems. More\nspecifically, we consider the design of multi-agent operations under a game\ntheoretic lens where individual agents are assigned utilities that guide their\nlocal decision making. We provide a tractable procedure for designing utilities\nthat optimize the efficiency of the resulting collective behavior (i.e., price\nof anarchy) for classes of set covering games where the extent of the\ninformation inconsistencies is known. In the setting where the extent of the\ninformational inconsistencies is not known, we show -- perhaps surprisingly --\nthat underestimating the level of uncertainty leads to better price of anarchy\nthan overestimating it.",
    "descriptor": "",
    "authors": [
      "Rohit Konda",
      "Rahul Chandan",
      "Jason R. Marden"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04029"
  },
  {
    "id": "arXiv:2106.04030",
    "title": "Supporting Multiparty Signing over Named Data Networking",
    "abstract": "Modern digitally controlled systems require multiparty authentication and\nauthorization to meet the desired security requirement. This paper describes\nthe design and development of NDN-MPS, an automated solution to support\nmultiparty signature signing and verification for NDN-enabled applications.\nNDN-MPS suggests several changes and extensions to the existing NDN security\nsolutions. First, it introduces a new type of trust schema to support signing\nand verification for multiple signers under complex policies such as threshold\nschemes. Second, it extends the NDN signature format to accommodate\nmultisignature schemes such as BLS signature. Third, it introduces a signature\ncollection protocol to solicit signatures securely from multiple signers. We\nfurther evaluate NDN-MPS by assessing its security properties and measuring its\nperformance.",
    "descriptor": "",
    "authors": [
      "Zhiyi Zhang",
      "Siqi Liu",
      "Randy King",
      "Lixia Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04030"
  },
  {
    "id": "arXiv:2106.04031",
    "title": "Balancing Asymptotic and Transient Efficiency Guarantees in Set Covering  Games",
    "abstract": "Game theoretic approaches have gained traction as a robust methodology for\ndesigning distributed local algorithms that induce a desired overall system\nconfiguration in a multi-agent setting. However, much of the emphasis in these\napproaches is on providing asymptotic guarantees on the performance of network\nof agents, and there is a gap in the study of efficiency guarantees along\ntransients of these distributed algorithms. Therefore, in this paper, we study\nthe transient efficiency guarantees of a natural game-theoretic algorithm in\nthe class of set covering games, which have been used to model a variety of\napplications. Our main results characterize the optimal utility design that\nmaximizes the guaranteed efficiency along the transient of the natural\ndynamics. Furthermore, we characterize the Pareto-optimal frontier with regards\nto guaranteed efficiency in the transient and the asymptote under a class of\ngame-theoretic designs. Surprisingly, we show that there exists an extreme\ntrade-off between the long-term and short-term guarantees in that an\nasymptotically optimal game-theoretic design can perform arbitrarily bad in the\ntransient.",
    "descriptor": "",
    "authors": [
      "Rohit Konda",
      "Rahul Chandan",
      "David Grimsman",
      "Jason R. Marden"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.04031"
  },
  {
    "id": "arXiv:2106.04033",
    "title": "Sample Complexity of Tree Search Configuration: Cutting Planes and  Beyond",
    "abstract": "Cutting-plane methods have enabled remarkable successes in integer\nprogramming over the last few decades. State-of-the-art solvers integrate a\nmyriad of cutting-plane techniques to speed up the underlying tree-search\nalgorithm used to find optimal solutions. In this paper we prove the first\nguarantees for learning high-performing cut-selection policies tailored to the\ninstance distribution at hand using samples. We first bound the sample\ncomplexity of learning cutting planes from the canonical family of\nChv\\'atal-Gomory cuts. Our bounds handle any number of waves of any number of\ncuts and are fine tuned to the magnitudes of the constraint coefficients. Next,\nwe prove sample complexity bounds for more sophisticated cut selection policies\nthat use a combination of scoring rules to choose from a family of cuts.\nFinally, beyond the realm of cutting planes for integer programming, we develop\na general abstraction of tree search that captures key components such as node\nselection and variable selection. For this abstraction, we bound the sample\ncomplexity of learning a good policy for building the search tree.",
    "descriptor": "",
    "authors": [
      "Maria-Florina Balcan",
      "Siddharth Prasad",
      "Tuomas Sandholm",
      "Ellen Vitercik"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04033"
  },
  {
    "id": "arXiv:2106.04034",
    "title": "GSGP-CUDA -- a CUDA framework for Geometric Semantic Genetic Programming",
    "abstract": "Geometric Semantic Genetic Programming (GSGP) is a state-of-the-art machine\nlearning method based on evolutionary computation. GSGP performs search\noperations directly at the level of program semantics, which can be done more\nefficiently then operating at the syntax level like most GP systems. Efficient\nimplementations of GSGP in C++ exploit this fact, but not to its full\npotential. This paper presents GSGP-CUDA, the first CUDA implementation of GSGP\nand the most efficient, which exploits the intrinsic parallelism of GSGP using\nGPUs. Results show speedups greater than 1,000X relative to the\nstate-of-the-art sequential implementation.",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Leonardo Trujillo",
      "Jose Manuel Mu\u00f1oz Contreras",
      "Daniel E Hernandez",
      "Mauro Castelli",
      "Juan J Tapia"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2106.04034"
  },
  {
    "id": "arXiv:2106.04037",
    "title": "Online Algorithms for Network Robustness under Connectivity Constraints",
    "abstract": "In this paper, we present algorithms for designing networks that are robust\nto node failures with minimal or limited number of links. We present algorithms\nfor both the static network setting and the dynamic network setting; setting\nwhere new nodes can arrive in the future. For the static setting, we present\nalgorithms for constructing the optimal network in terms of the number of links\nused for a given node size and the number of nodes that can fail. We then\nconsider the dynamic setting where it is disruptive to remove any of the older\nlinks. For this setting, we present online algorithms for two cases: (i) when\nthe number of nodes that can fail remains constant and (ii) when only the\nproportion of the nodes that can fail remains constant. We show that the\nproposed algorithm for the first case saves nearly $3/4$th of the total\npossible links at any point of time. We then present algorithms for various\nlevels of the fraction of the nodes that can fail and characterize their link\nusage. We show that when $1/2$ the number of nodes can fail at any point of\ntime, the proposed algorithm saves nearly $1/2$ of the total possible links at\nany point of time. We show that when the number of nodes that can fail is\nlimited to the fraction $1/(2m)$ ($m \\in \\mathbb{N}$), the proposed algorithm\nsaves nearly as much as $(1-1/2m)$ of the total possible links at any point of\ntime. We also show that when the number of nodes that can fail at any point of\ntime is $1/2$ of the number of nodes plus $n$, $n \\in \\mathbb{N}$, the number\nof links saved by the proposed algorithm reduces only linearly in $n$. We\nconjecture that the saving ratio achieved by the algorithms we present is\noptimal for the dynamic setting.",
    "descriptor": "",
    "authors": [
      "Deepan Muthirayan",
      "Pramod P. Khargonekar"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04037"
  },
  {
    "id": "arXiv:2106.04043",
    "title": "Dilated Convolution based CSI Feedback Compression for Massive MIMO  Systems",
    "abstract": "Although the frequency-division duplex (FDD) massive multiple-input\nmultiple-output (MIMO) system can offer high spectral and energy efficiency, it\nrequires to feedback the downlink channel state information (CSI) from users to\nthe base station (BS), in order to fulfill the precoding design at the BS.\nHowever, the large dimension of CSI matrices in the massive MIMO system makes\nthe CSI feedback very challenging, and it is urgent to compress the feedback\nCSI. To this end, this paper proposes a novel dilated convolution based CSI\nfeedback network, namely DCRNet. Specifically, the dilated convolutions are\nused to enhance the receptive field (RF) of the proposed DCRNet without\nincreasing the convolution size. Moreover, advanced encoder and decoder blocks\nare designed to improve the reconstruction performance and reduce computational\ncomplexity as well. Numerical results are presented to show the superiority of\nthe proposed DCRNet over the conventional networks. In particular, the proposed\nDCRNet can achieve almost the state-of-the-arts (SOTA) performance with much\nlower floating point operations (FLOPs). The open source code and checkpoint of\nthis work are available at https://github.com/recusant7/DCRNet.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Shunpu Tang",
      "Junjuan Xia",
      "Lisheng Fan",
      "Xianfu Lei",
      "Wei Xu",
      "Arumugam Nallanathan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04043"
  },
  {
    "id": "arXiv:2106.04048",
    "title": "H-ModQuad: Modular Multi-Rotors with 4, 5, and 6 Controllable DOF",
    "abstract": "Traditional aerial vehicles are usually custom-designed for specific tasks.\nAlthough they offer an efficient solution, they are not always able to adapt to\nchanges in the task specification, e.g., increasing the payload. This applies\nto quadrotors, having a maximum payload and only four controllable degrees of\nfreedom, limiting their adaptability to the task's variations. We propose a\nversatile modular robotic system that can increase its payload and degrees of\nfreedom by assembling heterogeneous modules; we call it H-ModQuad. It consists\nof cuboid modules propelled by quadrotors with tilted propellers that can\ngenerate forces in different directions. By connecting different types of\nmodules, an H-ModQuad can increase its controllable degrees of freedom from 4\nto 5 and 6. We model the general structure and propose three controllers, one\nfor each number of controllable degrees of freedom. We extend the concept of\nthe actuation ellipsoid to find the best reference orientation that can\nmaximize the performance of the structure. Our approach is validated with\nexperiments using actual robots, showing the independence of the translation\nand orientation of a structure.",
    "descriptor": "\nComments: 6 pages plus reference, ICRA 2021\n",
    "authors": [
      "Jiawei Xu",
      "Diego S. D'Antonio",
      "David Salda\u00f1a"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04048"
  },
  {
    "id": "arXiv:2106.04051",
    "title": "Graph-MLP: Node Classification without Message Passing in Graph",
    "abstract": "Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing\nwith non-Euclidean structural data. Both spatial-based and spectral-based GNNs\nare relying on adjacency matrix to guide message passing among neighbors during\nfeature aggregation. Recent works have mainly focused on powerful message\npassing modules, however, in this paper, we show that none of the message\npassing modules is necessary. Instead, we propose a pure\nmultilayer-perceptron-based framework, Graph-MLP with the supervision signal\nleveraging graph structure, which is sufficient for learning discriminative\nnode representation. In model-level, Graph-MLP only includes multi-layer\nperceptrons, activation function, and layer normalization. In the loss level,\nwe design a neighboring contrastive (NContrast) loss to bridge the gap between\nGNNs and MLPs by utilizing the adjacency information implicitly. This design\nallows our model to be lighter and more robust when facing large-scale graph\ndata and corrupted adjacency information. Extensive experiments prove that even\nwithout adjacency information in testing phase, our framework can still reach\ncomparable and even superior performance against the state-of-the-art models in\nthe graph node classification task.",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Yang Hu",
      "Haoxuan You",
      "Zhecan Wang",
      "Zhicheng Wang",
      "Erjin Zhou",
      "Yue Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.04051"
  },
  {
    "id": "arXiv:2106.04053",
    "title": "Discriminative Triad Matching and Reconstruction for Weakly Referring  Expression Grounding",
    "abstract": "In this paper, we are tackling the weakly-supervised referring expression\ngrounding task, for the localization of a referent object in an image according\nto a query sentence, where the mapping between image regions and queries are\nnot available during the training stage. In traditional methods, an object\nregion that best matches the referring expression is picked out, and then the\nquery sentence is reconstructed from the selected region, where the\nreconstruction difference serves as the loss for back-propagation. The existing\nmethods, however, conduct both the matching and the reconstruction\napproximately as they ignore the fact that the matching correctness is unknown.\nTo overcome this limitation, a discriminative triad is designed here as the\nbasis to the solution, through which a query can be converted into one or\nmultiple discriminative triads in a very scalable way. Based on the\ndiscriminative triad, we further propose the triad-level matching and\nreconstruction modules which are lightweight yet effective for the\nweakly-supervised training, making it three times lighter and faster than the\nprevious state-of-the-art methods. One important merit of our work is its\nsuperior performance despite the simple and neat design. Specifically, the\nproposed method achieves a new state-of-the-art accuracy when evaluated on\nRefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is\n4.17%, 4.08% and 7.8% higher than the previous one, respectively.",
    "descriptor": "\nComments: TPAMI\n",
    "authors": [
      "Mingjie Sun",
      "Jimin Xiao",
      "Eng Gee Lim",
      "Si Liu",
      "John Y. Goulermas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.04053"
  },
  {
    "id": "arXiv:2106.04054",
    "title": "Affinity Attention Graph Neural Network for Weakly Supervised Semantic  Segmentation",
    "abstract": "Weakly supervised semantic segmentation is receiving great attention due to\nits low human annotation cost. In this paper, we aim to tackle bounding box\nsupervised semantic segmentation, i.e., training accurate semantic segmentation\nmodels using bounding box annotations as supervision. To this end, we propose\nAffinity Attention Graph Neural Network ($A^2$GNN). Following previous\npractices, we first generate pseudo semantic-aware seeds, which are then formed\ninto semantic graphs based on our newly proposed affinity Convolutional Neural\nNetwork (CNN). Then the built graphs are input to our $A^2$GNN, in which an\naffinity attention layer is designed to acquire the short- and long- distance\ninformation from soft graph edges to accurately propagate semantic labels from\nthe confident seeds to the unlabeled pixels. However, to guarantee the\nprecision of the seeds, we only adopt a limited number of confident pixel seed\nlabels for $A^2$GNN, which may lead to insufficient supervision for training.\nTo alleviate this issue, we further introduce a new loss function and a\nconsistency-checking mechanism to leverage the bounding box constraint, so that\nmore reliable guidance can be included for the model optimization. Experiments\nshow that our approach achieves new state-of-the-art performances on Pascal VOC\n2012 datasets (val: 76.5\\%, test: 75.2\\%). More importantly, our approach can\nbe readily applied to bounding box supervised instance segmentation task or\nother weakly supervised semantic segmentation tasks, with state-of-the-art or\ncomparable performance among almot all weakly supervised tasks on PASCAL VOC or\nCOCO dataset. Our source code will be available at\nhttps://github.com/zbf1991/A2GNN.",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TAPMI 2021)\n",
    "authors": [
      "Bingfeng Zhang",
      "Jimin Xiao",
      "Jianbo Jiao",
      "Yunchao Wei",
      "Yao Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04054"
  },
  {
    "id": "arXiv:2106.04060",
    "title": "Self-supervised and Supervised Joint Training for Resource-rich Machine  Translation",
    "abstract": "Self-supervised pre-training of text representations has been successfully\napplied to low-resource Neural Machine Translation (NMT). However, it usually\nfails to achieve notable gains on resource-rich NMT. In this paper, we propose\na joint training approach, $F_2$-XEnDec, to combine self-supervised and\nsupervised learning to optimize NMT models. To exploit complementary\nself-supervised signals for supervised learning, NMT models are trained on\nexamples that are interbred from monolingual and parallel sentences through a\nnew process called crossover encoder-decoder. Experiments on two resource-rich\ntranslation benchmarks, WMT'14 English-German and WMT'14 English-French,\ndemonstrate that our approach achieves substantial improvements over several\nstrong baseline methods and obtains a new state of the art of 46.19 BLEU on\nEnglish-French when incorporating back translation. Results also show that our\napproach is capable of improving model robustness to input perturbations such\nas code-switching noise which frequently appears on social media.",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Yong Cheng",
      "Wei Wang",
      "Lu Jiang",
      "Wolfgang Macherey"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04060"
  },
  {
    "id": "arXiv:2106.04066",
    "title": "Semantically Controllable Scene Generation with Guidance of Explicit  Knowledge",
    "abstract": "Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.",
    "descriptor": "\nComments: 10 pages, 6 figures, Submitted to NeurIPS 2021\n",
    "authors": [
      "Wenhao Ding",
      "Bo Li",
      "Kim Ji Eun",
      "Ding Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04066"
  },
  {
    "id": "arXiv:2106.04067",
    "title": "LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution  Homography Estimation",
    "abstract": "Cross-resolution image alignment is a key problem in multiscale gigapixel\nphotography, which requires to estimate homography matrix using images with\nlarge resolution gap. Existing deep homography methods concatenate the input\nimages or features, neglecting the explicit formulation of correspondences\nbetween them, which leads to degraded accuracy in cross-resolution challenges.\nIn this paper, we consider the cross-resolution homography estimation as a\nmultimodal problem, and propose a local transformer network embedded within a\nmultiscale structure to explicitly learn correspondences between the multimodal\ninputs, namely, input images with different resolutions. The proposed local\ntransformer adopts a local attention map specifically for each position in the\nfeature. By combining the local transformer with the multiscale structure, the\nnetwork is able to capture long-short range correspondences efficiently and\naccurately. Experiments on both the MS-COCO dataset and the real-captured\ncross-resolution dataset show that the proposed network outperforms existing\nstate-of-the-art feature-based and deep-learning-based homography estimation\nmethods, and is able to accurately align images under $10\\times$ resolution\ngap.",
    "descriptor": "",
    "authors": [
      "Ruizhi Shao",
      "Gaochang Wu",
      "Yuemei Zhou",
      "Ying Fu",
      "Lu Fang",
      "Yebin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04067"
  },
  {
    "id": "arXiv:2106.04072",
    "title": "Coarse-to-Fine Curriculum Learning",
    "abstract": "When faced with learning challenging new tasks, humans often follow sequences\nof steps that allow them to incrementally build up the necessary skills for\nperforming these new tasks. However, in machine learning, models are most often\ntrained to solve the target tasks directly.Inspired by human learning, we\npropose a novel curriculum learning approach which decomposes challenging tasks\ninto sequences of easier intermediate goals that are used to pre-train a model\nbefore tackling the target task. We focus on classification tasks, and design\nthe intermediate tasks using an automatically constructed label hierarchy. We\ntrain the model at each level of the hierarchy, from coarse labels to fine\nlabels, transferring acquired knowledge across these levels. For instance, the\nmodel will first learn to distinguish animals from objects, and then use this\nacquired knowledge when learning to classify among more fine-grained classes\nsuch as cat, dog, car, and truck. Most existing curriculum learning algorithms\nfor supervised learning consist of scheduling the order in which the training\nexamples are presented to the model. In contrast, our approach focuses on the\noutput space of the model. We evaluate our method on several established\ndatasets and show significant performance gains especially on classification\nproblems with many labels. We also evaluate on a new synthetic dataset which\nallows us to study multiple aspects of our method.",
    "descriptor": "",
    "authors": [
      "Otilia Stretcu",
      "Emmanouil Antonios Platanios",
      "Tom M. Mitchell",
      "Barnab\u00e1s P\u00f3czos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04072"
  },
  {
    "id": "arXiv:2106.04073",
    "title": "Salvage of Supervision in Weakly Supervised Detection",
    "abstract": "Weakly supervised object detection (WSOD) has recently attracted much\nattention. However, the method, performance and speed gaps between WSOD and\nfully supervised detection prevent WSOD from being applied in real-world tasks.\nTo bridge the gaps, this paper proposes a new framework, Salvage of Supervision\n(SoS), with the key idea being to harness every potentially useful supervisory\nsignal in WSOD: the weak image-level labels, the pseudo-labels, and the power\nof semi-supervised object detection. This paper shows that each type of\nsupervisory signal brings in notable improvements, outperforms existing WSOD\nmethods (which mainly use only the weak labels) by large margins. The proposed\nSoS-WSOD method achieves 64.4 $m\\text{AP}_{50}$ on VOC2007, 61.9\n$m\\text{AP}_{50}$ on VOC2012 and 16.4 $m\\text{AP}_{50:95}$ on MS-COCO, and also\nhas fast inference speed. Ablations and visualization further verify the\neffectiveness of SoS.",
    "descriptor": "",
    "authors": [
      "Lin Sui",
      "Chen-Lin Zhang",
      "Jianxin Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04073"
  },
  {
    "id": "arXiv:2106.04074",
    "title": "Optimized Rate-Profiling for PAC Codes",
    "abstract": "The polarization-adjusted convolutional (PAC) codes concatenate the polar\ntransform and the convolutional transform to improve the decoding performance\nof the finite-length polar codes, where the rate-profile is used to construct\nthe PAC codes by setting the positions of frozen bits. However, the optimal\nrateprofile method of PAC codes is still unknown. In this paper, an optimized\nrate-profile algorithm of PAC codes is proposed. First, we propose the\nnormalized compression factor (NCF) to quantify the transmission efficiency of\nuseful information, showing that the distribution of useful information that\nneeds to be transmitted after the convolutional transform should be adaptive to\nthe capacity profile after finite-length polar transform. This phenomenon\nindicates that the PAC code improves the transmission efficiency of useful\ninformation, which leads to a better decoding performance than the polar codes\nwith the same length. Then, we propose a novel rate-profile method of PAC\ncodes, where a quadratic optimization model is established and the Euclidean\nnorm of the NCF spectrum is adopted to construct the objective function.\nFinally, a heuristic bit-swapping strategy is designed to search for the frozen\nset with high objective function values, where the search space is limited by\nconsidering the only bits with medium Hamming weight of the row index.\nSimulation results show that the PAC codes with the proposed optimized\nrate-profile construction have better decoding performance than the PAC codes\nwith the originally proposed Reed-Muller design construction.",
    "descriptor": "",
    "authors": [
      "He Sun",
      "Emanuele Viterbo",
      "Rongke Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04074"
  },
  {
    "id": "arXiv:2106.04075",
    "title": "Multi-Agent Cooperative Bidding Games for Multi-Objective Optimization  in e-Commercial Sponsored Search",
    "abstract": "Bid optimization for online advertising from single advertiser's perspective\nhas been thoroughly investigated in both academic research and industrial\npractice. However, existing work typically assume competitors do not change\ntheir bids, i.e., the wining price is fixed, leading to poor performance of the\nderived solution. Although a few studies use multi-agent reinforcement learning\nto set up a cooperative game, they still suffer the following drawbacks: (1)\nThey fail to avoid collusion solutions where all the advertisers involved in an\nauction collude to bid an extremely low price on purpose. (2) Previous works\ncannot well handle the underlying complex bidding environment, leading to poor\nmodel convergence. This problem could be amplified when handling multiple\nobjectives of advertisers which are practical demands but not considered by\nprevious work. In this paper, we propose a novel multi-objective cooperative\nbid optimization formulation called Multi-Agent Cooperative bidding Games\n(MACG). MACG sets up a carefully designed multi-objective optimization\nframework where different objectives of advertisers are incorporated. A global\nobjective to maximize the overall profit of all advertisements is added in\norder to encourage better cooperation and also to protect self-bidding\nadvertisers. To avoid collusion, we also introduce an extra platform revenue\nconstraint. We analyze the optimal functional form of the bidding formula\ntheoretically and design a policy network accordingly to generate auction-level\nbids. Then we design an efficient multi-agent evolutionary strategy for model\noptimization. Offline experiments and online A/B tests conducted on the Taobao\nplatform indicate both single advertiser's objective and global profit have\nbeen significantly improved compared to state-of-art methods.",
    "descriptor": "",
    "authors": [
      "Ziyu Guan",
      "Hongchang Wu",
      "Qingyu Cao",
      "Hao Liu",
      "Wei Zhao",
      "Sheng Li",
      "Cai Xu",
      "Guang Qiu",
      "Jian Xu",
      "Bo Zheng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.04075"
  },
  {
    "id": "arXiv:2106.04080",
    "title": "RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation",
    "abstract": "To date, most abstractive summarisation models have relied on variants of the\nnegative log-likelihood (NLL) as their training objective. In some cases,\nreinforcement learning has been added to train the models with an objective\nthat is closer to their evaluation measures (e.g. ROUGE). However, the reward\nfunction to be used within the reinforcement learning approach can play a key\nrole for performance and is still partially unexplored. For this reason, in\nthis paper, we propose two reward functions for the task of abstractive\nsummarisation: the first function, referred to as RwB-Hinge, dynamically\nselects the samples for the gradient update. The second function, nicknamed\nRISK, leverages a small pool of strong candidates to inform the reward. In the\nexperiments, we probe the proposed approach by fine-tuning an NLL pre trained\nmodel over nine summarisation datasets of diverse size and nature. The\nexperimental results show a consistent improvement over the negative\nlog-likelihood baselines.",
    "descriptor": "\nComments: 5th Workshop on Structured Prediction for NLP; held in conjunction with ACL-IJCNLP 2021\n",
    "authors": [
      "Jacob Parnell",
      "Inigo Jauregi Unanue",
      "Massimo Piccardi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04080"
  },
  {
    "id": "arXiv:2106.04081",
    "title": "Insight from NLP Analysis: COVID-19 Vaccines Sentiments on Social Media",
    "abstract": "Social media is an appropriate source for analyzing public attitudes towards\nthe COVID-19 vaccine and various brands. Nevertheless, there are few relevant\nstudies. In the research, we collected tweet posts by the UK and US residents\nfrom the Twitter API during the pandemic and designed experiments to answer\nthree main questions concerning vaccination. To get the dominant sentiment of\nthe civics, we performed sentiment analysis by VADER and proposed a new method\nthat can count the individual's influence. This allows us to go a step further\nin sentiment analysis and explain some of the fluctuations in the data\nchanging. The results indicated that celebrities could lead the opinion shift\non social media in vaccination progress. Moreover, at the peak, nearly 40\\% of\nthe population in both countries have a negative attitude towards COVID-19\nvaccines. Besides, we investigated how people's opinions toward different\nvaccine brands are. We found that the Pfizer vaccine enjoys the most popular\namong people. By applying the sentiment analysis tool, we discovered most\npeople hold positive views toward the COVID-19 vaccine manufactured by most\nbrands. In the end, we carried out topic modelling by using the LDA model. We\nfound residents in the two countries are willing to share their views and\nfeelings concerning the vaccine. Several death cases have occurred after\nvaccination. Due to these negative events, US residents are more worried about\nthe side effects and safety of the vaccine.",
    "descriptor": "",
    "authors": [
      "Tao Na",
      "Wei Cheng",
      "Dongming Li",
      "Wanyu Lu",
      "Hongjiang Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.04081"
  },
  {
    "id": "arXiv:2106.04086",
    "title": "Complexity classification of counting graph homomorphisms modulo a prime  number",
    "abstract": "Counting graph homomorphisms and its generalizations such as the Counting\nConstraint Satisfaction Problem (CSP), its variations, and counting problems in\ngeneral have been intensively studied since the pioneering work of Valiant.\nWhile the complexity of exact counting of graph homomorphisms (Dyer and\nGreenhill, 2000) and the counting CSP (Bulatov, 2013, and Dyer and Richerby,\n2013) is well understood, counting modulo some natural number has attracted\nconsiderable interest as well. In their 2015 paper Faben and Jerrum suggested a\nconjecture stating that counting homomorphisms to a fixed graph H modulo a\nprime number is hard whenever it is hard to count exactly, unless H has\nautomorphisms of certain kind. In this paper we confirm this conjecture. As a\npart of this investigation we develop techniques that widen the spectrum of\nreductions available for modular counting and apply to the general CSP rather\nthan being limited to graph homomorphisms.",
    "descriptor": "",
    "authors": [
      "Andrei A.Bulatov",
      "Amirhossein Kazeminia"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.04086"
  },
  {
    "id": "arXiv:2106.04088",
    "title": "Householder-Absolute Neural Layers For High Variability and Deep  Trainability",
    "abstract": "We propose a new architecture for artificial neural networks called\nHouseholder-absolute neural layers, or Han-layers for short, that use\nHouseholder reflectors as weight matrices and the absolute-value function for\nactivation. Han-layers, functioning as fully connected layers, are motivated by\nrecent results on neural-network variability and are designed to increase\nactivation ratio and reduce the chance of Collapse to Constants. Neural\nnetworks constructed chiefly from Han-layers are called HanNets. By\nconstruction, HanNets enjoy a theoretical guarantee that vanishing or exploding\ngradient never occurs. We conduct several proof-of-concept experiments. Some\nsurprising results obtained on styled test problems suggest that, under certain\nconditions, HanNets exhibit an unusual ability to produce nearly perfect\nsolutions unattainable by fully connected networks. Experiments on regression\ndatasets show that HanNets can significantly reduce the number of model\nparameters while maintaining or improving the level of generalization accuracy.\nIn addition, by adding a few Han-layers into the pre-classification FC-layer of\na convolutional neural network, we are able to quickly improve a\nstate-of-the-art result on CIFAR10 dataset. These proof-of-concept results are\nsufficient to necessitate further studies on HanNets to understand their\ncapacities and limits, and to exploit their potentials in real-world\napplications.",
    "descriptor": "",
    "authors": [
      "Yueyao Yu",
      "Yin Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04088"
  },
  {
    "id": "arXiv:2106.04090",
    "title": "Variational AutoEncoder for Reference based Image Super-Resolution",
    "abstract": "In this paper, we propose a novel reference based image super-resolution\napproach via Variational AutoEncoder (RefVAE). Existing state-of-the-art\nmethods mainly focus on single image super-resolution which cannot perform well\non large upsampling factors, e.g., 8$\\times$. We propose a reference based\nimage super-resolution, for which any arbitrary image can act as a reference\nfor super-resolution. Even using random map or low-resolution image itself, the\nproposed RefVAE can transfer the knowledge from the reference to the\nsuper-resolved images. Depending upon different references, the proposed method\ncan generate different versions of super-resolved images from a hidden\nsuper-resolution space. Besides using different datasets for some standard\nevaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space\nchallenge and have provided results of the randomness evaluation of our\napproach. Compared to other state-of-the-art methods, our approach achieves\nhigher diverse scores.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Zhi-Song Liu",
      "Wan-Chi Siu",
      "Li-Wen Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.04090"
  },
  {
    "id": "arXiv:2106.04092",
    "title": "Online Learning Robust Control of Nonlinear Dynamical Systems",
    "abstract": "In this work we address the problem of the online robust control of nonlinear\ndynamical systems perturbed by disturbance. We study the problem of attenuation\nof the total cost over a duration $T$ in response to the disturbances. We\nconsider the setting where the cost function (at a particular time) is a\ngeneral continuous function and adversarial, the disturbance is adversarial and\nbounded at any point of time. Our goal is to design a controller that can learn\nand adapt to achieve a certain level of attenuation. We analyse two cases (i)\nwhen the system is known and (ii) when the system is unknown. We measure the\nperformance of the controller by the deviation of the controller's cost for a\nsequence of cost functions with respect to an attenuation $\\gamma$, $R^p_t$. We\npropose an online controller and present guarantees for the metric $R^p_t$ when\nthe maximum possible attenuation is given by $\\overline{\\gamma}$, which is a\nsystem constant. We show that when the controller has preview of the cost\nfunctions and the disturbances for a short duration of time and the system is\nknown $R^p_T(\\gamma) = O(1)$ when $\\gamma \\geq \\gamma_c$, where $\\gamma_c =\n\\mathcal{O}(\\overline{\\gamma})$. We then show that when the system is unknown\nthe proposed controller with a preview of the cost functions and the\ndisturbances for a short horizon achieves $R^p_T(\\gamma) = \\mathcal{O}(N) +\n\\mathcal{O}(1) + \\mathcal{O}((T-N)g(N))$, when $\\gamma \\geq \\gamma_c$, where\n$g(N)$ is the accuracy of a given nonlinear estimator and $N$ is the duration\nof the initial estimation period. We also characterize the lower bound on the\nrequired prediction horizon for these guarantees to hold in terms of the system\nconstants.",
    "descriptor": "",
    "authors": [
      "Deepan Muthirayan",
      "Pramod P. Khargonekar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04092"
  },
  {
    "id": "arXiv:2106.04094",
    "title": "Game-Theoretic Model Predictive Control with Data-Driven Identification  of Vehicle Model for Head-to-Head Autonomous Racing",
    "abstract": "Resolving edge-cases in autonomous driving, head-to-head autonomous racing is\ngetting a lot of attention from the industry and academia. In this study, we\npropose a game-theoretic model predictive control (MPC) approach for\nhead-to-head autonomous racing and data-driven model identification method. For\nthe practical estimation of nonlinear model parameters, we adopted the\nhyperband algorithm, which is used for neural model training in machine\nlearning. The proposed controller comprises three modules: 1) game-based\nopponents' trajectory predictor, 2) high-level race strategy planner, and 3)\nMPC-based low-level controller. The game-based predictor was designed to\npredict the future trajectories of competitors. Based on the prediction\nresults, the high-level race strategy planner plans several behaviors to\nrespond to various race circumstances. Finally, the MPC-based controller\ncomputes the optimal control commands to follow the trajectories. The proposed\napproach was validated under various racing circumstances in an official\nsimulator of the Indy Autonomous Challenge. The experimental results show that\nthe proposed method can effectively overtake competitors, while driving through\nthe track as quickly as possible without collisions.",
    "descriptor": "\nComments: 6 pages, 7 figures, ICRA workshop on Opportunities and Challenges with Autonomous Racing, 31 May, 2021(accepted)\n",
    "authors": [
      "Chanyoung Jung",
      "Seungwook Lee",
      "Hyunki Seong",
      "Andrea Finazzi",
      "David Hyunchul Shim"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04094"
  },
  {
    "id": "arXiv:2106.04095",
    "title": "Diverse Part Discovery: Occluded Person Re-identification with  Part-Aware Transformer",
    "abstract": "Occluded person re-identification (Re-ID) is a challenging task as persons\nare frequently occluded by various obstacles or other persons, especially in\nthe crowd scenario. To address these issues, we propose a novel end-to-end\nPart-Aware Transformer (PAT) for occluded person Re-ID through diverse part\ndiscovery via a transformer encoderdecoder architecture, including a pixel\ncontext based transformer encoder and a part prototype based transformer\ndecoder. The proposed PAT model enjoys several merits. First, to the best of\nour knowledge, this is the first work to exploit the transformer\nencoder-decoder architecture for occluded person Re-ID in a unified deep model.\nSecond, to learn part prototypes well with only identity labels, we design two\neffective mechanisms including part diversity and part discriminability.\nConsequently, we can achieve diverse part discovery for occluded person Re-ID\nin a weakly supervised manner. Extensive experimental results on six\nchallenging benchmarks for three tasks (occluded, partial and holistic Re-ID)\ndemonstrate that our proposed PAT performs favorably against stat-of-the-art\nmethods.",
    "descriptor": "\nComments: Accepted by CVPR 2021\n",
    "authors": [
      "Yulin Li",
      "Jianfeng He",
      "Tianzhu Zhang",
      "Xiang Liu",
      "Yongdong Zhang",
      "Feng Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04095"
  },
  {
    "id": "arXiv:2106.04096",
    "title": "Linear Convergence of Entropy-Regularized Natural Policy Gradient with  Linear Function Approximation",
    "abstract": "Natural policy gradient (NPG) methods with function approximation achieve\nimpressive empirical success in reinforcement learning problems with large\nstate-action spaces. However, theoretical understanding of their convergence\nbehaviors remains limited in the function approximation setting. In this paper,\nwe perform a finite-time analysis of NPG with linear function approximation and\nsoftmax parameterization, and prove for the first time that widely used entropy\nregularization method, which encourages exploration, leads to linear\nconvergence rate. We adopt a Lyapunov drift analysis to prove the convergence\nresults and explain the effectiveness of entropy regularization in improving\nthe convergence rates.",
    "descriptor": "",
    "authors": [
      "Semih Cayci",
      "Niao He",
      "R. Srikant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04096"
  },
  {
    "id": "arXiv:2106.04097",
    "title": "A Sequence Selection Bound for the Capacity of the Nonlinear Fiber  Channel",
    "abstract": "A novel technique to optimize the input distribution and compute a lower\nbound for the capacity of the nonlinear optical fiber channel is proposed. The\ntechnique improves previous bounds obtained with the additive white Gaussian\nnoise decoding metric.",
    "descriptor": "\nComments: The manuscript has been submitted for publication to the european conference on optical communication (ECOC) 2021\n",
    "authors": [
      "Stella Civelli",
      "Enrico Forestieri",
      "Alexey Lotsmanov",
      "Dmitry Razdoburdin",
      "Marco Secondini"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04097"
  },
  {
    "id": "arXiv:2106.04098",
    "title": "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language  Model",
    "abstract": "Recently, there is an effort to extend fine-grained entity typing by using a\nricher and ultra-fine set of types, and labeling noun phrases including\npronouns and nominal nouns instead of just named entity mentions. A key\nchallenge for this ultra-fine entity typing task is that human annotated data\nare extremely scarce, and the annotation ability of existing distant or weak\nsupervision approaches is very limited. To remedy this problem, in this paper,\nwe propose to obtain training data for ultra-fine entity typing by using a BERT\nMasked Language Model (MLM). Given a mention in a sentence, our approach\nconstructs an input for the BERT MLM so that it predicts context dependent\nhypernyms of the mention, which can be used as type labels. Experimental\nresults demonstrate that, with the help of these automatically generated\nlabels, the performance of an ultra-fine entity typing model can be improved\nsubstantially. We also show that our approach can be applied to improve\ntraditional fine-grained entity typing after performing simple type mapping.",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Hongliang Dai",
      "Yangqiu Song",
      "Haixun Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04098"
  },
  {
    "id": "arXiv:2106.04102",
    "title": "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage  and Quality",
    "abstract": "We release a new benchmark for lexical substitution, the task of finding\nappropriate substitutes for a target word in a context. To assist humans with\nwriting, lexical substitution systems can suggest words that humans cannot\neasily think of. However, existing benchmarks depend on human recall as the\nonly source of data, and therefore lack coverage of the substitutes that would\nbe most helpful to humans. Furthermore, annotators often provide substitutes of\nlow quality, which are not actually appropriate in the given context. We\ncollect higher-coverage and higher-quality data by framing lexical substitution\nas a classification problem, guided by the intuition that it is easier for\nhumans to judge the appropriateness of candidate substitutes than conjure them\nfrom memory. To this end, we use a context-free thesaurus to produce candidates\nand rely on human judgement to determine contextual appropriateness. Compared\nto the previous largest benchmark, our Swords benchmark has 4.1x more\nsubstitutes per target word for the same level of quality, and its substitutes\nare 1.5x more appropriate (based on human judgement) for the same number of\nsubstitutes.",
    "descriptor": "\nComments: Published as a conference paper at NAACL 2021\n",
    "authors": [
      "Mina Lee",
      "Chris Donahue",
      "Alexander Iyabor",
      "Robin Jia",
      "Percy Liang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04102"
  },
  {
    "id": "arXiv:2106.04104",
    "title": "Design of Low-Artifact Interpolation Kernels by Means of Computer  Algebra",
    "abstract": "We present a number of new piecewise-polynomial kernels for image\ninterpolation. The kernels are constructed by optimizing a measure of\ninterpolation quality based on the magnitude of anisotropic artifacts. The\nkernel design process is performed symbolically using Mathematica computer\nalgebra system. Experimental evaluation involving 14 image quality assessment\nmethods demonstrates that our results compare favorably with the existing\nlinear interpolators.",
    "descriptor": "\nComments: 22 pages, 6 figures\n",
    "authors": [
      "Peter Karpov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Symbolic Computation (cs.SC)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.04104"
  },
  {
    "id": "arXiv:2106.04105",
    "title": "Entropic Independence in High-Dimensional Expanders: Modified  Log-Sobolev Inequalities for Fractionally Log-Concave Polynomials and the  Ising Model",
    "abstract": "We introduce a notion called entropic independence for distributions $\\mu$\ndefined on pure simplicial complexes, i.e., subsets of size $k$ of a ground set\nof elements. Informally, we call a background measure $\\mu$ entropically\nindependent if for any (possibly randomly chosen) set $S$, the relative entropy\nof an element of $S$ drawn uniformly at random carries at most $O(1/k)$\nfraction of the relative entropy of $S$, a constant multiple of its ``share of\nentropy.'' Entropic independence is the natural analog of spectral\nindependence, another recently established notion, if one replaces variance by\nentropy.\nIn our main result, we show that $\\mu$ is entropically independent exactly\nwhen a transformed version of the generating polynomial of $\\mu$ can be upper\nbounded by its linear tangent, a property implied by concavity of the said\ntransformation. We further show that this concavity is equivalent to spectral\nindependence under arbitrary external fields, an assumption that also goes by\nthe name of fractional log-concavity. Our result can be seen as a new tool to\nestablish entropy contraction from the much simpler variance contraction\ninequalities. A key differentiating feature of our result is that we make no\nassumptions on marginals of $\\mu$ or the degrees of the underlying graphical\nmodel when $\\mu$ is based on one. We leverage our results to derive tight\nmodified log-Sobolev inequalities for multi-step down-up walks on fractionally\nlog-concave distributions. As our main application, we establish the tight\nmixing time of $O(n\\log n)$ for Glauber dynamics on Ising models with\ninteraction matrix of operator norm smaller than $1$, improving upon the prior\nquadratic dependence on $n$.",
    "descriptor": "",
    "authors": [
      "Nima Anari",
      "Vishesh Jain",
      "Frederic Koehler",
      "Huy Tuan Pham",
      "Thuy-Duong Vuong"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Mathematical Physics (math-ph)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.04105"
  },
  {
    "id": "arXiv:2106.04108",
    "title": "Fully Transformer Networks for Semantic Image Segmentation",
    "abstract": "Transformers have shown impressive performance in various natural language\nprocessing and computer vision tasks, due to the capability of modeling\nlong-range dependencies. Recent progress has demonstrated to combine such\ntransformers with CNN-based semantic image segmentation models is very\npromising. However, it is not well studied yet on how well a pure transformer\nbased approach can achieve for image segmentation. In this work, we explore a\nnovel framework for semantic image segmentation, which is encoder-decoder based\nFully Transformer Networks (FTN). Specifically, we first propose a Pyramid\nGroup Transformer (PGT) as the encoder for progressively learning hierarchical\nfeatures, while reducing the computation complexity of the standard visual\ntransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse\nsemantic-level and spatial-level information from multiple levels of the PGT\nencoder for semantic image segmentation. Surprisingly, this simple baseline can\nachieve new state-of-the-art results on multiple challenging semantic\nsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The\nsource code will be released upon the publication of this work.",
    "descriptor": "",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Fangjian Lin",
      "Shengwei Tian",
      "Guodong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04108"
  },
  {
    "id": "arXiv:2106.04110",
    "title": "A self consistent theory of Gaussian Processes captures feature learning  effects in finite CNNs",
    "abstract": "Deep neural networks (DNNs) in the infinite width/channel limit have received\nmuch attention recently, as they provide a clear analytical window to deep\nlearning via mappings to Gaussian Processes (GPs). Despite its theoretical\nappeal, this viewpoint lacks a crucial ingredient of deep learning in finite\nDNNs, laying at the heart of their success -- feature learning. Here we\nconsider DNNs trained with noisy gradient descent on a large training set and\nderive a self consistent Gaussian Process theory accounting for strong\nfinite-DNN and feature learning effects. Applying this to a toy model of a\ntwo-layer linear convolutional neural network (CNN) shows good agreement with\nexperiments. We further identify, both analytical and numerically, a sharp\ntransition between a feature learning regime and a lazy learning regime in this\nmodel. Strong finite-DNN effects are also derived for a non-linear two-layer\nfully connected network. Our self consistent theory provides a rich and\nversatile analytical framework for studying feature learning and other non-lazy\neffects in finite DNNs.",
    "descriptor": "\nComments: 9 pages of main text, 23 pages of appendices, 5 figures total\n",
    "authors": [
      "Gadi Naveh",
      "Zohar Ringel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04110"
  },
  {
    "id": "arXiv:2106.04112",
    "title": "Harnessing Unrecognizable Faces for Face Recognition",
    "abstract": "The common implementation of face recognition systems as a cascade of a\ndetection stage and a recognition or verification stage can cause problems\nbeyond failures of the detector. When the detector succeeds, it can detect\nfaces that cannot be recognized, no matter how capable the recognition system.\nRecognizability, a latent variable, should therefore be factored into the\ndesign and implementation of face recognition systems. We propose a measure of\nrecognizability of a face image that leverages a key empirical observation: an\nembedding of face images, implemented by a deep neural network trained using\nmostly recognizable identities, induces a partition of the hypersphere whereby\nunrecognizable identities cluster together. This occurs regardless of the\nphenomenon that causes a face to be unrecognizable, it be optical or motion\nblur, partial occlusion, spatial quantization, poor illumination. Therefore, we\nuse the distance from such an \"unrecognizable identity\" as a measure of\nrecognizability, and incorporate it in the design of the over-all system. We\nshow that accounting for recognizability reduces error rate of single-image\nface recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification\nbenchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based\nrecognition on the IJB-C benchmark.",
    "descriptor": "",
    "authors": [
      "Siqi Deng",
      "Yuanjun Xiong",
      "Meng Wang",
      "Wei Xia",
      "Stefano Soatto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04112"
  },
  {
    "id": "arXiv:2106.04113",
    "title": "Self-supervised Graph-level Representation Learning with Local and  Global Structure",
    "abstract": "This paper studies unsupervised/self-supervised whole-graph representation\nlearning, which is critical in many tasks such as molecule properties\nprediction in drug and material discovery. Existing methods mainly focus on\npreserving the local similarity structure between different graph instances but\nfail to discover the global semantic structure of the entire data set. In this\npaper, we propose a unified framework called Local-instance and Global-semantic\nLearning (GraphLoG) for self-supervised whole-graph representation learning.\nSpecifically, besides preserving the local similarities, GraphLoG introduces\nthe hierarchical prototypes to capture the global semantic clusters. An\nefficient online expectation-maximization (EM) algorithm is further developed\nfor learning the model. We evaluate GraphLoG by pre-training it on massive\nunlabeled graphs followed by fine-tuning on downstream tasks. Extensive\nexperiments on both chemical and biological benchmark data sets demonstrate the\neffectiveness of the proposed approach.",
    "descriptor": "\nComments: Accepted as Short Talk at International Conference on Machine Learning (ICML), 2021\n",
    "authors": [
      "Minghao Xu",
      "Hang Wang",
      "Bingbing Ni",
      "Hongyu Guo",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04113"
  },
  {
    "id": "arXiv:2106.04114",
    "title": "What Data Augmentation Do We Need for Deep-Learning-Based Finance?",
    "abstract": "The main task we consider is portfolio construction in a speculative market,\na fundamental problem in modern finance. While various empirical works now\nexist to explore deep learning in finance, the theory side is almost\nnon-existent. In this work, we focus on developing a theoretical framework for\nunderstanding the use of data augmentation for deep-learning-based approaches\nto quantitative finance. The proposed theory clarifies the role and necessity\nof data augmentation for finance; moreover, our theory motivates a simple\nalgorithm of injecting a random noise of strength $\\sqrt{|r_{t-1}|}$ to the\nobserved return $r_{t}$. This algorithm is shown to work well in practice.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Liu Ziyin",
      "Kentaro Minami",
      "Kentaro Imajo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "General Finance (q-fin.GN)",
      "Portfolio Management (q-fin.PM)"
    ],
    "url": "https://arxiv.org/abs/2106.04114"
  },
  {
    "id": "arXiv:2106.04117",
    "title": "The best of both worlds: stochastic and adversarial episodic MDPs with  unknown transition",
    "abstract": "We consider the best-of-both-worlds problem for learning an episodic Markov\nDecision Process through $T$ episodes, with the goal of achieving\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret when the losses are adversarial and\nsimultaneously $\\mathcal{O}(\\text{polylog}(T))$ regret when the losses are\n(almost) stochastic. Recent work by [Jin and Luo, 2020] achieves this goal when\nthe fixed transition is known, and leaves the case of unknown transition as a\nmajor open question. In this work, we resolve this open problem by using the\nsame Follow-the-Regularized-Leader ($\\text{FTRL}$) framework together with a\nset of new techniques. Specifically, we first propose a loss-shifting trick in\nthe $\\text{FTRL}$ analysis, which greatly simplifies the approach of [Jin and\nLuo, 2020] and already improves their results for the known transition case.\nThen, we extend this idea to the unknown transition case and develop a novel\nanalysis which upper bounds the transition estimation error by (a fraction of)\nthe regret itself in the stochastic setting, a key property to ensure\n$\\mathcal{O}(\\text{polylog}(T))$ regret.",
    "descriptor": "",
    "authors": [
      "Tiancheng Jin",
      "Longbo Huang",
      "Haipeng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04117"
  },
  {
    "id": "arXiv:2106.04119",
    "title": "LaserShark: Establishing Fast, Bidirectional Communication into  Air-Gapped Systems",
    "abstract": "Physical isolation, so called air-gapping, is an effective method for\nprotecting security-critical computers and networks. While it might be possible\nto introduce malicious code through the supply chain, insider attacks, or\nsocial engineering, communicating with the outside world is prevented.\nDifferent approaches to breach this essential line of defense have been\ndeveloped based on electromagnetic, acoustic, and optical communication\nchannels. However, all of these approaches are limited in either data rate or\ndistance, and frequently offer only exfiltration of data. We present a novel\napproach to infiltrate data to and exfiltrate data from air-gapped systems\nwithout any additional hardware on-site. By aiming lasers at already built-in\nLEDs and recording their response, we are the first to enable a long-distance\n(25m), bidirectional, and fast (18.2kbps in & 100kbps out) covert communication\nchannel. The approach can be used against any office device that operates LEDs\nat the CPU's GPIO interface.",
    "descriptor": "",
    "authors": [
      "Niclas K\u00fchnapfel",
      "Stefan Preu\u00dfler",
      "Maximilian Noppel",
      "Thomas Schneider",
      "Konrad Rieck",
      "Christian Wressnegger"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04119"
  },
  {
    "id": "arXiv:2106.04120",
    "title": "Throughput Analysis of UAV-assisted CellularNetworks by Mat\u00e9rn  Hardcore Point Process",
    "abstract": "Unmanned aerial vehicles (UAVs) are expected to coexist with conventional\nterrestrial cellular networks and become an important component to support high\nrate transmissions. This paper presents an analytical framework for evaluating\nthe throughput performance of a downlink two-tier heterogeneous network.\nConsidering the minimum distance constraint among UAVs, Mat\\'{e}rn hardcore\npoint process (MHP) is utilized to model the locations of UAVs. The locations\nof terrestrial base stations (BSs) are modeled by Poisson point process (PPP).\nTools of stochastic geometry are invoked to derive tractable expressions for\naverage data rates of users. With the analytical results, we discuss the\noptimal combinations of UAVs' height and power control factor. The result shows\nthat an appropriate power control factor can effectively maximize UAV users'\naverage data rate as well as guaranteeing the BS users' performance under our\nproposed model.",
    "descriptor": "",
    "authors": [
      "Mengbing Liu",
      "Guangji Chen",
      "Ling Qiu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04120"
  },
  {
    "id": "arXiv:2106.04121",
    "title": "Multi-dataset Pretraining: A Unified Model for Semantic Segmentation",
    "abstract": "Collecting annotated data for semantic segmentation is time-consuming and\nhard to scale up. In this paper, we for the first time propose a unified\nframework, termed as Multi-Dataset Pretraining, to take full advantage of the\nfragmented annotations of different datasets. The highlight is that the\nannotations from different domains can be efficiently reused and consistently\nboost performance for each specific domain. This is achieved by first\npretraining the network via the proposed pixel-to-prototype contrastive loss\nover multiple datasets regardless of their taxonomy labels, and followed by\nfine-tuning the pretrained model over specific dataset as usual. In order to\nbetter model the relationship among images and classes from different datasets,\nwe extend the pixel level embeddings via cross dataset mixing and propose a\npixel-to-class sparse coding strategy that explicitly models the pixel-class\nsimilarity over the manifold embedding space. In this way, we are able to\nincrease intra-class compactness and inter-class separability, as well as\nconsidering inter-class similarity across different datasets for better\ntransferability. Experiments conducted on several benchmarks demonstrate its\nsuperior performance. Notably, MDP consistently outperforms the pretrained\nmodels over ImageNet by a considerable margin, while only using less than 10%\nsamples for pretraining.",
    "descriptor": "",
    "authors": [
      "Bowen Shi",
      "Xiaopeng Zhang",
      "Haohang Xu",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04121"
  },
  {
    "id": "arXiv:2106.04122",
    "title": "CloudChain: A Cloud Blockchain Using Shared Memory Consensus and RDMA",
    "abstract": "Blockchain technologies can enable secure computing environments among\nmistrusting parties. Permissioned blockchains are particularly enlightened by\ncompanies, enterprises, and government agencies due to their efficiency,\ncustomizability, and governance-friendly features. Obviously, seamlessly fusing\nblockchain and cloud computing can significantly benefit permissioned\nblockchains; nevertheless, most blockchains implemented on clouds are\noriginally designed for loosely-coupled networks where nodes communicate\nasynchronously, failing to take advantages of the closely-coupled nature of\ncloud servers. In this paper, we propose an innovative cloud-oriented\nblockchain -- CloudChain, which is a modularized three-layer system composed of\nthe network layer, consensus layer, and blockchain layer. CloudChain is based\non a shared-memory model where nodes communicate synchronously by direct memory\naccesses. We realize the shared-memory model with the Remote Direct Memory\nAccess technology, based on which we propose a shared-memory consensus\nalgorithm to ensure presistence and liveness, the two crucial blockchain\nsecurity properties countering Byzantine nodes. We also implement a CloudChain\nprototype based on a RoCEv2-based testbed to experimentally validate our\ndesign, and the results verify the feasibility and efficiency of CloudChain.",
    "descriptor": "\nComments: 12 pages, 8 figures, journal paper\n",
    "authors": [
      "Minghui Xu",
      "Shuo Liu",
      "Dongxiao Yu",
      "Xiuzhen Cheng",
      "Shaoyong Guo",
      "Jiguo Yu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.04122"
  },
  {
    "id": "arXiv:2106.04127",
    "title": "Left Ventricle Contouring in Cardiac Images Based on Deep Reinforcement  Learning",
    "abstract": "Medical image segmentation is one of the important tasks of computer-aided\ndiagnosis in medical image analysis. Since most medical images have the\ncharacteristics of blurred boundaries and uneven intensity distribution,\nthrough existing segmentation methods, the discontinuity within the target area\nand the discontinuity of the target boundary are likely to lead to rough or\neven erroneous boundary delineation. In this paper, we propose a new iterative\nrefined interactive segmentation method for medical images based on agent\nreinforcement learning, which focuses on the problem of target segmentation\nboundaries. We model the dynamic process of drawing the target contour in a\ncertain order as a Markov Decision Process (MDP) based on a deep reinforcement\nlearning method. In the dynamic process of continuous interaction between the\nagent and the image, the agent tracks the boundary point by point in order\nwithin a limited length range until the contour of the target is completely\ndrawn. In this process, the agent can quickly improve the segmentation\nperformance by exploring an interactive policy in the image. The method we\nproposed is simple and effective. At the same time, we evaluate our method on\nthe cardiac MRI scan data set. Experimental results show that our method has a\nbetter segmentation effect on the left ventricle in a small number of medical\nimage data sets, especially in terms of segmentation boundaries, this method is\nbetter than existing methods. Based on our proposed method, the dynamic\ngeneration process of the predicted contour trajectory of the left ventricle\nwill be displayed online at https://github.com/H1997ym/LV-contour-trajectory.",
    "descriptor": "",
    "authors": [
      "Sixing Yin",
      "Yameng Han",
      "Shufang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04127"
  },
  {
    "id": "arXiv:2106.04128",
    "title": "Conversational Fashion Image Retrieval via Multiturn Natural Language  Feedback",
    "abstract": "We study the task of conversational fashion image retrieval via multiturn\nnatural language feedback. Most previous studies are based on single-turn\nsettings. Existing models on multiturn conversational fashion image retrieval\nhave limitations, such as employing traditional models, and leading to\nineffective performance. We propose a novel framework that can effectively\nhandle conversational fashion image retrieval with multiturn natural language\nfeedback texts. One characteristic of the framework is that it searches for\ncandidate images based on exploitation of the encoded reference image and\nfeedback text information together with the conversation history. Furthermore,\nthe image fashion attribute information is leveraged via a mutual attention\nstrategy. Since there is no existing fashion dataset suitable for the multiturn\nsetting of our task, we derive a large-scale multiturn fashion dataset via\nadditional manual annotation efforts on an existing single-turn dataset. The\nexperiments show that our proposed model significantly outperforms existing\nstate-of-the-art methods.",
    "descriptor": "\nComments: Accepted by SIGIR 2021\n",
    "authors": [
      "Yifei Yuan",
      "Wai Lam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04128"
  },
  {
    "id": "arXiv:2106.04133",
    "title": "Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention",
    "abstract": "Emotion recognition from speech is a challenging task. Re-cent advances in\ndeep learning have led bi-directional recur-rent neural network (Bi-RNN) and\nattention mechanism as astandard method for speech emotion recognition,\nextractingand attending multi-modal features - audio and text, and thenfusing\nthem for downstream emotion classification tasks. Inthis paper, we propose a\nsimple yet efficient neural networkarchitecture to exploit both acoustic and\nlexical informationfrom speech. The proposed framework using multi-scale\ncon-volutional layers (MSCNN) to obtain both audio and text hid-den\nrepresentations. Then, a statistical pooling unit (SPU)is used to further\nextract the features in each modality. Be-sides, an attention module can be\nbuilt on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the\nperfor-mance. Extensive experiments show that the proposed modeloutperforms\nprevious state-of-the-art methods on IEMOCAPdataset with four emotion\ncategories (i.e., angry, happy, sadand neutral) in both weighted accuracy (WA)\nand unweightedaccuracy (UA), with an improvement of 5.0% and 5.2% respectively\nunder the ASR setting.",
    "descriptor": "\nComments: First two authors contributed equally.Accepted by ICASSP 2021\n",
    "authors": [
      "Zixuan Peng",
      "Yu Lu",
      "Shengfeng Pan",
      "Yunfeng Liu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04133"
  },
  {
    "id": "arXiv:2106.04134",
    "title": "Cheap and Good? Simple and Effective Data Augmentation for Low Resource  Machine Reading",
    "abstract": "We propose a simple and effective strategy for data augmentation for\nlow-resource machine reading comprehension (MRC). Our approach first pretrains\nthe answer extraction components of a MRC system on the augmented data that\ncontains approximate context of the correct answers, before training it on the\nexact answer spans. The approximate context helps the QA method components in\nnarrowing the location of the answers. We demonstrate that our simple strategy\nsubstantially improves both document retrieval and answer extraction\nperformance by providing larger context of the answers and additional training\ndata. In particular, our method significantly improves the performance of BERT\nbased retriever (15.12\\%), and answer extractor (4.33\\% F1) on TechQA, a\ncomplex, low-resource MRC task. Further, our data augmentation strategy yields\nsignificant improvements of up to 3.9\\% exact match (EM) and 2.7\\% F1 for\nanswer extraction on PolicyQA, another practical but moderate sized QA dataset\nthat also contains long answer spans.",
    "descriptor": "\nComments: 5 pages, 1 figure, SIGIR 2021\n",
    "authors": [
      "Hoang Van",
      "Vikas Yadav",
      "Mihai Surdeanu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04134"
  },
  {
    "id": "arXiv:2106.04139",
    "title": "Image Deformation Estimation via Multi-Objective Optimization",
    "abstract": "The free-form deformation model can represent a wide range of non-rigid\ndeformations by manipulating a control point lattice over the image. However,\ndue to a large number of parameters, it is challenging to fit the free-form\ndeformation model directly to the deformed image for deformation estimation\nbecause of the complexity of the fitness landscape. In this paper, we cast the\nregistration task as a multi-objective optimization problem (MOP) according to\nthe fact that regions affected by each control point overlap with each other.\nSpecifically, by partitioning the template image into several regions and\nmeasuring the similarity of each region independently, multiple objectives are\nbuilt and deformation estimation can thus be realized by solving the MOP with\noff-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a\ncoarse-to-fine strategy is realized by image pyramid combined with control\npoint mesh subdivision. Specifically, the optimized candidate solutions of the\ncurrent image level are inherited by the next level, which increases the\nability to deal with large deformation. Also, a post-processing procedure is\nproposed to generate a single output utilizing the Pareto optimal solutions.\nComparative experiments on both synthetic and real-world images show the\neffectiveness and usefulness of our deformation estimation method.",
    "descriptor": "",
    "authors": [
      "Takumi Nakane",
      "Xuequan Lu",
      "Haoran Xie",
      "Chao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04139"
  },
  {
    "id": "arXiv:2106.04140",
    "title": "Broadcasted Residual Learning for Efficient Keyword Spotting",
    "abstract": "Keyword spotting is an important research field because it plays a key role\nin device wake-up and user interaction on smart devices. However, it is\nchallenging to minimize errors while operating efficiently in devices with\nlimited resources such as mobile phones. We present a broadcasted residual\nlearning method to achieve high accuracy with small model size and\ncomputational load. Our method configures most of the residual functions as 1D\ntemporal convolution while still allows 2D convolution together using a\nbroadcasted-residual connection that expands temporal output to\nfrequency-temporal dimension. This residual mapping enables the network to\neffectively represent useful audio features with much less computation than\nconventional convolutional neural networks. We also propose a novel network\narchitecture, Broadcasting-residual network (BC-ResNet), based on broadcasted\nresidual learning and describe how to scale up the model according to the\ntarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%\ntop-1 accuracy on Google speech command datasets v1 and v2, respectively, and\nconsistently outperform previous approaches, using fewer computations and\nparameters.",
    "descriptor": "\nComments: Proceedings of INTERSPEECH 2021\n",
    "authors": [
      "Byeonggeun Kim",
      "Simyung Chang",
      "Jinkyu Lee",
      "Dooyong Sung"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04140"
  },
  {
    "id": "arXiv:2106.04144",
    "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic  Segmentation",
    "abstract": "Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Gabriel Tjio",
      "Ping Liu",
      "Joey Tianyi Zhou",
      "Rick Siow Mong Goh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04144"
  },
  {
    "id": "arXiv:2106.04146",
    "title": "Risk Ranked Recall: Collision Safety Metric for Object Detection Systems  in Autonomous Vehicles",
    "abstract": "Commonly used metrics for evaluation of object detection systems (precision,\nrecall, mAP) do not give complete information about their suitability of use in\nsafety critical tasks, like obstacle detection for collision avoidance in\nAutonomous Vehicles (AV). This work introduces the Risk Ranked Recall ($R^3$)\nmetrics for object detection systems. The $R^3$ metrics categorize objects\nwithin three ranks. Ranks are assigned based on an objective cyber-physical\nmodel for the risk of collision. Recall is measured for each rank.",
    "descriptor": "\nComments: Cyber-Physical Systems and Internet-of-Things 2021\n",
    "authors": [
      "Ayoosh Bansal",
      "Jayati Singh",
      "Micaela Verucchi",
      "Marco Caccamo",
      "Lui Sha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04146"
  },
  {
    "id": "arXiv:2106.04148",
    "title": "RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting",
    "abstract": "Time series forecasting is a relevant task that is performed in several\nreal-world scenarios such as product sales analysis and prediction of energy\ndemand. Given their accuracy performance, currently, Recurrent Neural Networks\n(RNNs) are the models of choice for this task. Despite their success in time\nseries forecasting, less attention has been paid to make the RNNs trustworthy.\nFor example, RNNs can not naturally provide an uncertainty measure to their\npredictions. This could be extremely useful in practice in several cases e.g.\nto detect when a prediction might be completely wrong due to an unusual pattern\nin the time series. Whittle Sum-Product Networks (WSPNs), prominent deep\ntractable probabilistic circuits (PCs) for time series, can assist an RNN with\nproviding meaningful probabilities as uncertainty measure. With this aim, we\npropose RECOWN, a novel architecture that employs RNNs and a discriminant\nvariant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a\nLog-Likelihood Ratio Score as better estimation of uncertainty that is tailored\nto time series and Whittle likelihoods. In our experiments, we show that\nRECOWNs are accurate and trustworthy time series predictors, able to \"know when\nthey do not know\".",
    "descriptor": "",
    "authors": [
      "Nils Thoma",
      "Zhongjie Yu",
      "Fabrizio Ventola",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04148"
  },
  {
    "id": "arXiv:2106.04149",
    "title": "Understanding (Generalized) Label Smoothing whenLearning with Noisy  Labels",
    "abstract": "Label smoothing (LS) is an arising learning paradigm that uses the positively\nweighted average of both the hard training labels and uniformly distributed\nsoft labels. It was shown that LS serves as a regularizer for training data\nwith hard labels and therefore improves the generalization of the model. Later\nit was reported LS even helps with improving robustness when learning with\nnoisy labels. However, we observe that the advantage of LS vanishes when we\noperate in a high label noise regime. Puzzled by the observation, we proceeded\nto discover that several proposed learning-with-noisy-labels solutions in the\nliterature instead relate more closely to negative label smoothing (NLS), which\ndefines as using a negative weight to combine the hard and soft labels! We show\nthat NLS functions substantially differently from LS in their achieved model\nconfidence. To differentiate the two cases, we will call LS the positive label\nsmoothing (PLS), and this paper unifies PLS and NLS into generalized label\nsmoothing (GLS). We provide understandings for the properties of GLS when\nlearning with noisy labels. Among other established properties, we\ntheoretically show NLS is considered more beneficial when the label noise rates\nare high. We provide experimental results to support our findings too.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Jiaheng Wei",
      "Hangyu Liu",
      "Tongliang Liu",
      "Gang Niu",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04149"
  },
  {
    "id": "arXiv:2106.04150",
    "title": "Few-Shot Action Localization without Knowing Boundaries",
    "abstract": "Learning to localize actions in long, cluttered, and untrimmed videos is a\nhard task, that in the literature has typically been addressed assuming the\navailability of large amounts of annotated training samples for each class --\neither in a fully-supervised setting, where action boundaries are known, or in\na weakly-supervised setting, where only class labels are known for each video.\nIn this paper, we go a step further and show that it is possible to learn to\nlocalize actions in untrimmed videos when a) only one/few trimmed examples of\nthe target action are available at test time, and b) when a large collection of\nvideos with only class label annotation (some trimmed and some weakly annotated\nuntrimmed ones) are available for training; with no overlap between the classes\nused during training and testing. To do so, we propose a network that learns to\nestimate Temporal Similarity Matrices (TSMs) that model a fine-grained\nsimilarity pattern between pairs of videos (trimmed or untrimmed), and uses\nthem to generate Temporal Class Activation Maps (TCAMs) for seen or unseen\nclasses. The TCAMs serve as temporal attention mechanisms to extract\nvideo-level representations of untrimmed videos, and to temporally localize\nactions at test time. To the best of our knowledge, we are the first to propose\na weakly-supervised, one/few-shot action localization network that can be\ntrained in an end-to-end fashion. Experimental results on THUMOS14 and\nActivityNet1.2 datasets, show that our method achieves performance comparable\nor better to state-of-the-art fully-supervised, few-shot learning methods.",
    "descriptor": "\nComments: ICMR21 Camera ready; link to code: this https URL\n",
    "authors": [
      "Ting-Ting Xie",
      "Christos Tzelepis",
      "Fan Fu",
      "Ioannis Patras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04150"
  },
  {
    "id": "arXiv:2106.04151",
    "title": "Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain  Adaptation",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned\nfrom a well-labeled source domain to an unlabeled target domain. Recently,\nadversarial domain adaptation with two distinct classifiers (bi-classifier) has\nbeen introduced into UDA which is effective to align distributions between\ndifferent domains. Previous bi-classifier adversarial learning methods only\nfocus on the similarity between the outputs of two distinct classifiers.\nHowever, the similarity of the outputs cannot guarantee the accuracy of target\nsamples, i.e., target samples may match to wrong categories even if the\ndiscrepancy between two classifiers is small. To challenge this issue, in this\npaper, we propose a cross-domain gradient discrepancy minimization (CGDM)\nmethod which explicitly minimizes the discrepancy of gradients generated by\nsource samples and target samples. Specifically, the gradient gives a cue for\nthe semantic information of target samples so it can be used as a good\nsupervision to improve the accuracy of target samples. In order to compute the\ngradient signal of target samples, we further obtain target pseudo labels\nthrough a clustering-based self-supervised learning. Extensive experiments on\nthree widely used UDA datasets show that our method surpasses many previous\nstate-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.",
    "descriptor": "\nComments: Accepted to CVPR 2021, Codes are avaliable at this https URL\n",
    "authors": [
      "Zhekai Du",
      "Jingjing Li",
      "Hongzu Su",
      "Lei Zhu",
      "Ke Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.04151"
  },
  {
    "id": "arXiv:2106.04152",
    "title": "PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for  Reinforcement Learning",
    "abstract": "Learning good feature representations is important for deep reinforcement\nlearning (RL). However, with limited experience, RL often suffers from data\ninefficiency for training. For un-experienced or less-experienced trajectories\n(i.e., state-action sequences), the lack of data limits the use of them for\nbetter feature learning. In this work, we propose a novel method, dubbed\nPlayVirtual, which augments cycle-consistent virtual trajectories to enhance\nthe data efficiency for RL feature representation learning. Specifically,\nPlayVirtual predicts future states based on the current state and action by a\ndynamics model and then predicts the previous states by a backward dynamics\nmodel, which forms a trajectory cycle. Based on this, we augment the actions to\ngenerate a large amount of virtual state-action trajectories. Being free of\ngroudtruth state supervision, we enforce a trajectory to meet the cycle\nconsistency constraint, which can significantly enhance the data efficiency. We\nvalidate the effectiveness of our designs on the Atari and DeepMind Control\nSuite benchmarks. Our method outperforms the current state-of-the-art methods\nby a large margin on both benchmarks.",
    "descriptor": "",
    "authors": [
      "Tao Yu",
      "Cuiling Lan",
      "Wenjun Zeng",
      "Mingxiao Feng",
      "Zhibo Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04152"
  },
  {
    "id": "arXiv:2106.04155",
    "title": "Review Polarity-wise Recommender",
    "abstract": "Utilizing review information to enhance recommendation, the de facto\nreview-involved recommender systems, have received increasing interests over\nthe past few years. Thereinto, one advanced branch is to extract salient\naspects from textual reviews (i.e., the item attributes that users express) and\ncombine them with the matrix factorization technique. However, existing\napproaches all ignore the fact that semantically different reviews often\ninclude opposite aspect information. In particular, positive reviews usually\nexpress aspects that users prefer, while negative ones describe aspects that\nusers reject. As a result, it may mislead the recommender systems into making\nincorrect decisions pertaining to user preference modeling. Towards this end,\nin this paper, we propose a Review Polarity-wise Recommender model, dubbed as\nRPR, to discriminately treat reviews with different polarities. To be specific,\nin this model, positive and negative reviews are separately gathered and\nutilized to model the user-preferred and user-rejected aspects, respectively.\nBesides, in order to overcome the imbalance problem of semantically different\nreviews, we also develop an aspect-aware importance weighting approach to align\nthe aspect importance for these two kinds of reviews. Extensive experiments\nconducted on eight benchmark datasets have demonstrated the superiority of our\nmodel as compared to a series of state-of-the-art review-involved baselines.\nMoreover, our method can provide certain explanations to the real-world rating\nprediction scenarios.",
    "descriptor": "",
    "authors": [
      "Han Liu",
      "Yangyang Guo",
      "Jianhua Yin",
      "Zan Gao",
      "Liqiang Nie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04155"
  },
  {
    "id": "arXiv:2106.04156",
    "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral  Contrastive Loss",
    "abstract": "Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.",
    "descriptor": "",
    "authors": [
      "Jeff Z. HaoChen",
      "Colin Wei",
      "Adrien Gaidon",
      "Tengyu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04156"
  },
  {
    "id": "arXiv:2106.04159",
    "title": "Fast Federated Learning in the Presence of Arbitrary Device  Unavailability",
    "abstract": "Federated Learning (FL) coordinates with numerous heterogeneous devices to\ncollaboratively train a shared model while preserving user privacy. Despite its\nmultiple advantages, FL faces new challenges. One challenge arises when devices\ndrop out of the training process beyond the control of the central server. In\nthis case, the convergence of popular FL algorithms such as FedAvg is severely\ninfluenced by the straggling devices. To tackle this challenge, we study\nfederated learning algorithms under arbitrary device unavailability and propose\nan algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our\nalgorithm efficiently avoids excessive latency induced by inactive devices, and\ncorrects the gradient bias using the memorized latest updates from the devices.\nWe prove that MIFA achieves minimax optimal convergence rates on non-i.i.d.\ndata for both strongly convex and non-convex smooth functions. We also provide\nan explicit characterization of the improvement over baseline algorithms\nthrough a case study, and validate the results by numerical experiments on\nreal-world datasets.",
    "descriptor": "",
    "authors": [
      "Xinran Gu",
      "Kaixuan Huang",
      "Jingzhao Zhang",
      "Longbo Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04159"
  },
  {
    "id": "arXiv:2106.04165",
    "title": "Neural Hybrid Automata: Learning Dynamics with Multiple Modes and  Stochastic Transitions",
    "abstract": "Effective control and prediction of dynamical systems often require\nappropriate handling of continuous-time and discrete, event-triggered\nprocesses. Stochastic hybrid systems (SHSs), common across engineering domains,\nprovide a formalism for dynamical systems subject to discrete, possibly\nstochastic, state jumps and multi-modal continuous-time flows. Despite the\nversatility and importance of SHSs across applications, a general procedure for\nthe explicit learning of both discrete events and multi-mode continuous\ndynamics remains an open problem. This work introduces Neural Hybrid Automata\n(NHAs), a recipe for learning SHS dynamics without a priori knowledge on the\nnumber of modes and inter-modal transition dynamics. NHAs provide a systematic\ninference method based on normalizing flows, neural differential equations and\nself-supervision. We showcase NHAs on several tasks, including mode recovery\nand flow learning in systems with stochastic transitions, and end-to-end\nlearning of hierarchical robot controllers.",
    "descriptor": "",
    "authors": [
      "Michael Poli",
      "Stefano Massaroli",
      "Luca Scimeca",
      "Seong Joon Oh",
      "Sanghyuk Chun",
      "Atsushi Yamashita",
      "Hajime Asama",
      "Jinkyoo Park",
      "Animesh Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04165"
  },
  {
    "id": "arXiv:2106.04166",
    "title": "Incorporating NODE with Pre-trained Neural Differential Operator for  Learning Dynamics",
    "abstract": "Learning dynamics governed by differential equations is crucial for\npredicting and controlling the systems in science and engineering. Neural\nOrdinary Differential Equation (NODE), a deep learning model integrated with\ndifferential equations, learns the dynamics directly from the samples on the\ntrajectory and shows great promise in the scientific field. However, the\ntraining of NODE highly depends on the numerical solver, which can amplify\nnumerical noise and be unstable, especially for ill-conditioned dynamical\nsystems. In this paper, to reduce the reliance on the numerical solver, we\npropose to enhance the supervised signal in learning dynamics. Specifically,\nbeyond learning directly from the trajectory samples, we pre-train a neural\ndifferential operator (NDO) to output an estimation of the derivatives to serve\nas an additional supervised signal. The NDO is pre-trained on a class of\nsymbolic functions, and it learns the mapping between the trajectory samples of\nthese functions to their derivatives. We provide theoretical guarantee on that\nthe output of NDO can well approximate the ground truth derivatives by proper\ntuning the complexity of the library. To leverage both the trajectory signal\nand the estimated derivatives from NDO, we propose an algorithm called\nNDO-NODE, in which the loss function contains two terms: the fitness on the\ntrue trajectory samples and the fitness on the estimated derivatives that are\noutput by the pre-trained NDO. Experiments on various of dynamics show that our\nproposed NDO-NODE can consistently improve the forecasting accuracy.",
    "descriptor": "\nComments: 15 pages, 12 figures, 3 tables\n",
    "authors": [
      "Shiqi Gong",
      "Qi Meng",
      "Yue Wang",
      "Lijun Wu",
      "Wei Chen",
      "Zhi-Ming Ma",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04166"
  },
  {
    "id": "arXiv:2106.04169",
    "title": "On Improving Adversarial Transferability of Vision Transformers",
    "abstract": "Vision transformers (ViTs) process input images as sequences of patches via\nself-attention; a radically different architecture than convolutional neural\nnetworks (CNNs). This makes it interesting to study the adversarial feature\nspace of ViT models and their transferability. In particular, we observe that\nadversarial patterns found via conventional adversarial attacks show very low\nblack-box transferability even for large ViT models. However, we show that this\nphenomenon is only due to the sub-optimal attack procedures that do not\nleverage the true representation potential of ViTs. A deep ViT is composed of\nmultiple blocks, with a consistent architecture comprising of self-attention\nand feed-forward layers, where each block is capable of independently producing\na class token. Formulating an attack using only the last class token\n(conventional approach) does not directly leverage the discriminative\ninformation stored in the earlier tokens, leading to poor adversarial\ntransferability of ViTs. Using the compositional nature of ViT models, we\nenhance the transferability of existing attacks by introducing two novel\nstrategies specific to the architecture of ViT models. (i) Self-Ensemble: We\npropose a method to find multiple discriminative pathways by dissecting a\nsingle ViT model into an ensemble of networks. This allows explicitly utilizing\nclass-specific information at each ViT block. (ii) Token Refinement: We then\npropose to refine the tokens to further enhance the discriminative capacity at\neach block of ViT. Our token refinement systematically combines the class\ntokens with structural information preserved within the patch tokens. An\nadversarial attack, when applied to such refined tokens within the ensemble of\nclassifiers found in a single vision transformer, has significantly higher\ntransferability.",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Muzammal Naseer",
      "Kanchana Ranasinghe",
      "Salman Khan",
      "Fahad Shahbaz Khan",
      "Fatih Porikli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04169"
  },
  {
    "id": "arXiv:2106.04172",
    "title": "Contention-based Grant-free Transmission with Extremely Sparse  Orthogonal Pilot Scheme",
    "abstract": "Due to the limited number of traditional orthogonal pilots, pilot collision\nwill severely degrade the performance of contention-based grant-free\ntransmission. To alleviate the pilot collision and exploit the spatial degree\nof freedom as much as possible, an extremely sparse orthogonal pilot scheme is\nproposed for uplink grant-free transmission. The proposed sparse pilot is used\nto perform active user detection and estimate the spatial channel. Then,\ninter-user interference suppression is performed by spatially combining the\nreceived data symbols using the estimated spatial channel. After that, the\nestimation and compensation of wireless channel and time/frequency offset are\nperformed utilizing the geometric characteristics of combined data symbols. The\ntask of pilot is much lightened, so that the extremely sparse orthogonal pilot\ncan occupy minimized resources, and the number of orthogonal pilots can be\nincreased significantly, which greatly reduces the probability of pilot\ncollision. The numerical results show that the proposed extremely sparse\northogonal pilot scheme significantly improves the performance in\nhigh-overloading grant-free scenario.",
    "descriptor": "\nComments: 6 pages;This paper has been submitted to IEEE VTC 2021-FALL\n",
    "authors": [
      "Zhifeng Yuan",
      "Zhigang Li",
      "Weimin Li",
      "Yihua Ma"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04172"
  },
  {
    "id": "arXiv:2106.04174",
    "title": "Interpretable and Low-Resource Entity Matching via Decoupling Feature  Learning from Decision Making",
    "abstract": "Entity Matching (EM) aims at recognizing entity records that denote the same\nreal-world object. Neural EM models learn vector representation of entity\ndescriptions and match entities end-to-end. Though robust, these methods\nrequire many resources for training, and lack of interpretability. In this\npaper, we propose a novel EM framework that consists of Heterogeneous\nInformation Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple\nfeature representation from matching decision. Using self-supervised learning\nand mask mechanism in pre-trained language modeling, HIF learns the embeddings\nof noisy attribute values by inter-attribute attention with unlabeled data.\nUsing a set of comparison features and a limited amount of annotated data, KAT\nInduction learns an efficient decision tree that can be interpreted by\ngenerating entity matching rules whose structure is advocated by domain\nexperts. Experiments on 6 public datasets and 3 industrial datasets show that\nour method is highly efficient and outperforms SOTA EM models in most cases.\nOur codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT.",
    "descriptor": "",
    "authors": [
      "Zijun Yao",
      "Chengjiang Li",
      "Tiansi Dong",
      "Xin Lv",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li",
      "Yichi Zhang",
      "Zelin Dai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04174"
  },
  {
    "id": "arXiv:2106.04175",
    "title": "Highly accurate digital traffic recording as a basis for future mobility  research: Methods and concepts of the research project HDV-Mess",
    "abstract": "The research project HDV-Mess aims at a currently missing, but very crucial\ncomponent for addressing important challenges in the field of connected and\nautomated driving on public roads. The goal is to record traffic events at\nvarious relevant locations with high accuracy and to collect real traffic data\nas a basis for the development and validation of current and future sensor\ntechnologies as well as automated driving functions. For this purpose, it is\nnecessary to develop a concept for a mobile modular system of measuring\nstations for highly accurate traffic data acquisition, which enables a\ntemporary installation of a sensor and communication infrastructure at\ndifferent locations. Within this paper, we first discuss the project goals\nbefore we present our traffic detection concept using mobile modular\nintelligent transport systems stations (ITS-Ss). We then explain the approaches\nfor data processing of sensor raw data to refined trajectories, data\ncommunication, and data validation.",
    "descriptor": "",
    "authors": [
      "Laurent Kloeker",
      "Fabian Thomsen",
      "Lutz Eckstein",
      "Philip Trettner",
      "Tim Elsner",
      "Julius Nehring-Wirxel",
      "Kersten Schuster",
      "Leif Kobbelt",
      "Michael Hoesch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04175"
  },
  {
    "id": "arXiv:2106.04178",
    "title": "White Paper Assistance: A Step Forward Beyond the Shortcut Learning",
    "abstract": "The promising performances of CNNs often overshadow the need to examine\nwhether they are doing in the way we are actually interested. We show through\nexperiments that even over-parameterized models would still solve a dataset by\nrecklessly leveraging spurious correlations, or so-called 'shortcuts'. To\ncombat with this unintended propensity, we borrow the idea of printer test page\nand propose a novel approach called White Paper Assistance. Our proposed method\ninvolves the white paper to detect the extent to which the model has preference\nfor certain characterized patterns and alleviates it by forcing the model to\nmake a random guess on the white paper. We show the consistent accuracy\nimprovements that are manifest in various architectures, datasets and\ncombinations with other techniques. Experiments have also demonstrated the\nversatility of our approach on fine-grained recognition, imbalanced\nclassification and robustness to corruptions.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Xuan Cheng",
      "Tianshu Xie",
      "Xiaomin Wang",
      "Jiali Deng",
      "Minghui Liu",
      "Ming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04178"
  },
  {
    "id": "arXiv:2106.04179",
    "title": "Deterministic $(1+\\varepsilon)$-Approximate Maximum Matching with  $\\mathsf{poly}(1/\\varepsilon)$ Passes in the Semi-Streaming Model",
    "abstract": "We present a deterministic $(1+\\varepsilon)$-approximate maximum matching\nalgorithm in $\\mathsf{poly}(1/\\varepsilon)$ passes in the semi-streaming model,\nsolving the long-standing open problem of breaking the exponential barrier in\nthe dependence on $1/\\varepsilon$. Our algorithm exponentially improves on the\nwell-known randomized $(1/\\varepsilon)^{O(1/\\varepsilon)}$-pass algorithm from\nthe seminal work by McGregor [APPROX05], the recent deterministic algorithm by\nTirodkar with the same pass complexity [FSTTCS18], as well as the deterministic\n$\\log n \\cdot \\mathsf{poly}(1/\\varepsilon)$-pass algorithm by Ahn and Guha\n[ICALP11].",
    "descriptor": "",
    "authors": [
      "Manuela Fischer",
      "Slobodan Mitrovi\u0107",
      "Jara Uitto"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04179"
  },
  {
    "id": "arXiv:2106.04180",
    "title": "Image2Point: 3D Point-Cloud Understanding with Pretrained 2D ConvNets",
    "abstract": "3D point-clouds and 2D images are different visual representations of the\nphysical world. While human vision can understand both representations,\ncomputer vision models designed for 2D image and 3D point-cloud understanding\nare quite different. Our paper investigates the potential for transferability\nbetween these two representations by empirically investigating whether this\napproach works, what factors affect the transfer performance, and how to make\nit work even better. We discovered that we can indeed use the same neural net\nmodel architectures to understand both images and point-clouds. Moreover, we\ncan transfer pretrained weights from image models to point-cloud models with\nminimal effort. Specifically, based on a 2D ConvNet pretrained on an image\ndataset, we can transfer the image model to a point-cloud model by\n\\textit{inflating} 2D convolutional filters to 3D then finetuning its input,\noutput, and optionally normalization layers. The transferred model can achieve\ncompetitive performance on 3D point-cloud classification, indoor and driving\nscene segmentation, even beating a wide range of point-cloud models that adopt\ntask-specific architectures and use a variety of tricks.",
    "descriptor": "\nComments: The code is avaliable at: \\url{this https URL}\n",
    "authors": [
      "Chenfeng Xu",
      "Shijia Yang",
      "Bohan Zhai",
      "Bichen Wu",
      "Xiangyu Yue",
      "Wei Zhan",
      "Peter Vajda",
      "Kurt Keutzer",
      "Masayoshi Tomizuka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04180"
  },
  {
    "id": "arXiv:2106.04181",
    "title": "The Randomness of Input Data Spaces is an A Priori Predictor for  Generalization",
    "abstract": "Over-parameterized models can perfectly learn various types of data\ndistributions, however, generalization error is usually lower for real data in\ncomparison to artificial data. This suggests that the properties of data\ndistributions have an impact on generalization capability. This work focuses on\nthe search space defined by the input data and assumes that the correlation\nbetween labels of neighboring input values influences generalization. If\ncorrelation is low, the randomness of the input data space is high leading to\nhigh generalization error. We suggest to measure the randomness of an input\ndata space using Maurer's universal. Results for synthetic classification tasks\nand common image classification benchmarks (MNIST, CIFAR10, and Microsoft's\ncats vs. dogs data set) find a high correlation between the randomness of input\ndata spaces and the generalization error of deep neural networks for binary\nclassification problems.",
    "descriptor": "",
    "authors": [
      "Martin Briesch",
      "Dominik Sobania",
      "Franz Rothlauf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04181"
  },
  {
    "id": "arXiv:2106.04182",
    "title": "Fast voltage boosters to improve transient stability of power systems  with 100% of grid-forming VSC-based generation",
    "abstract": "Grid-forming voltage source converter (GF-VSC) has been identified as the key\ntechnology for the operation of future converter-dominated power systems. Among\nmany other issues, transient stability of this type of power systems remains an\nopen topic of research because it is still a key limiting factor for stressed\npower systems. Previous studies have proposed control strategies for GF-VSC to\nimprove transient stability of this type of systems by suitable\ncurrent-limitation algorithms and/or control of active-power injections. As an\nalternative, this paper proposes two fast voltage boosters to improve transient\nstability of power systems with 100% of GF-VSC-based generation with virtual\nsynchronous machine (VSM). One control strategy uses local measurements,\nwhereas the other one uses global measurements of the frequency of the centre\nof inertia (COI). Both strategies improve transient stability of this type of\nsystems significantly. The advantage of using fast voltage boosters for this\npurpose is that the set points linked to frequency/active-power injection (i.e\nset points linked to the primary energy source of the VSCs) will not be\nmodified. Furthermore, strategies such as current-limitation, active-power\ncontrol and fast voltage controllers for transient stability improvement are\ncompatible and complementary.",
    "descriptor": "",
    "authors": [
      "R\u00e9gulo E. \u00c1vila-Mart\u00ednez",
      "Javier Renedo",
      "Luis Rouco",
      "Aurelio Garc\u00eda-Cerrada",
      "Lukas Sigrist",
      "Taoufik Qoria",
      "Xavier Guillaud"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04182"
  },
  {
    "id": "arXiv:2106.04185",
    "title": "LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from  Video using Pose and Lighting Normalization",
    "abstract": "In this paper, we present a video-based learning framework for animating\npersonalized 3D talking faces from audio. We introduce two training-time data\nnormalizations that significantly improve data sample efficiency. First, we\nisolate and represent faces in a normalized space that decouples 3D geometry,\nhead pose, and texture. This decomposes the prediction problem into regressions\nover the 3D face shape and the corresponding 2D texture atlas. Second, we\nleverage facial symmetry and approximate albedo constancy of skin to isolate\nand remove spatio-temporal lighting variations. Together, these normalizations\nallow simple networks to generate high fidelity lip-sync videos under novel\nambient illumination while training with just a single speaker-specific video.\nFurther, to stabilize temporal dynamics, we introduce an auto-regressive\napproach that conditions the model on its previous visual state. Human ratings\nand objective metrics demonstrate that our method outperforms contemporary\nstate-of-the-art audio-driven video reenactment benchmarks in terms of realism,\nlip-sync and visual quality scores. We illustrate several applications enabled\nby our framework.",
    "descriptor": "\nComments: Accepted to IEEE CVPR 2021. Brief demo video available at: this https URL\n",
    "authors": [
      "Avisek Lahiri",
      "Vivek Kwatra",
      "Christian Frueh",
      "John Lewis",
      "Chris Bregler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04185"
  },
  {
    "id": "arXiv:2106.04186",
    "title": "What training reveals about neural network complexity",
    "abstract": "This work explores the hypothesis that the complexity of the function a deep\nneural network (NN) is learning can be deduced by how fast its weights change\nduring training. Our analysis provides evidence for this supposition by\nrelating the network's distribution of Lipschitz constants (i.e., the norm of\nthe gradient at different regions of the input space) during different training\nintervals with the behavior of the stochastic training procedure. We first\nobserve that the average Lipschitz constant close to the training data affects\nvarious aspects of the parameter trajectory, with more complex networks having\na longer trajectory, bigger variance, and often veering further from their\ninitialization. We then show that NNs whose biases are trained more steadily\nhave bounded complexity even in regions of the input space that are far from\nany training point. Finally, we find that steady training with Dropout implies\na training- and data-dependent generalization bound that grows\npoly-logarithmically with the number of parameters. Overall, our results\nsupport the hypothesis that good training behavior can be a useful bias towards\ngood generalization.",
    "descriptor": "\nComments: 31 pages, 8 figures\n",
    "authors": [
      "Andreas Loukas",
      "Marinos Poiitis",
      "Stefanie Jegelka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04186"
  },
  {
    "id": "arXiv:2106.04188",
    "title": "Stability and Generalization of Bilevel Programming in Hyperparameter  Optimization",
    "abstract": "Recently, the (gradient-based) bilevel programming framework is widely used\nin hyperparameter optimization and has achieved excellent performance\nempirically. Previous theoretical work mainly focuses on its optimization\nproperties, while leaving the analysis on generalization largely open. This\npaper attempts to address the issue by presenting an expectation bound w.r.t.\nthe validation set based on uniform stability. Our results can explain some\nmysterious behaviours of the bilevel programming in practice, for instance,\noverfitting to the validation set. We also present an expectation bound for the\nclassical cross-validation algorithm. Our results suggest that gradient-based\nalgorithms can be better than cross-validation under certain conditions in a\ntheoretical perspective. Furthermore, we prove that regularization terms in\nboth the outer and inner levels can relieve the overfitting problem in\ngradient-based algorithms. In experiments on feature learning and data\nreweighting for noisy labels, we corroborate our theoretical findings.",
    "descriptor": "",
    "authors": [
      "Fan Bao",
      "Guoqiang Wu",
      "Chongxuan Li",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04188"
  },
  {
    "id": "arXiv:2106.04191",
    "title": "FPT Algorithms to Compute the Elimination Distance to Bipartite Graphs  and More",
    "abstract": "For a hereditary graph class $\\mathcal{H}$, the $\\mathcal{H}$-elimination\ndistance of a graph $G$ is the minimum number of rounds needed to reduce $G$ to\na member of $\\mathcal{H}$ by removing one vertex from each connected component\nin each round. The $\\mathcal{H}$-treewidth of a graph $G$ is the minimum, taken\nover all vertex sets $X$ for which each connected component of $G - X$ belongs\nto $\\mathcal{H}$, of the treewidth of the graph obtained from $G$ by replacing\nthe neighborhood of each component of $G-X$ by a clique and then removing $V(G)\n\\setminus X$. These parameterizations recently attracted interest because they\nare simultaneously smaller than the graph-complexity measures treedepth and\ntreewidth, respectively, and the vertex-deletion distance to $\\mathcal{H}$. For\nthe class $\\mathcal{H}$ of bipartite graphs, we present non-uniform\nfixed-parameter tractable algorithms for testing whether the\n$\\mathcal{H}$-elimination distance or $\\mathcal{H}$-treewidth of a graph is at\nmost $k$. Along the way, we also provide such algorithms for all graph classes\n$\\mathcal{H}$ defined by a finite set of forbidden induced subgraphs.",
    "descriptor": "\nComments: 14 pages, to appear at WG 2021\n",
    "authors": [
      "Bart M.P. Jansen",
      "Jari J.H. de Kroon"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04191"
  },
  {
    "id": "arXiv:2106.04192",
    "title": "Realistic Evaluation Principles for Cross-document Coreference  Resolution",
    "abstract": "We point out that common evaluation practices for cross-document coreference\nresolution have been unrealistically permissive in their assumed settings,\nyielding inflated results. We propose addressing this issue via two evaluation\nmethodology principles. First, as in other tasks, models should be evaluated on\npredicted mentions rather than on gold mentions. Doing this raises a subtle\nissue regarding singleton coreference clusters, which we address by decoupling\nthe evaluation of mention detection from that of coreference linking. Second,\nwe argue that models should not exploit the synthetic topic structure of the\nstandard ECB+ dataset, forcing models to confront the lexical ambiguity\nchallenge, as intended by the dataset creators. We demonstrate empirically the\ndrastic impact of our more realistic evaluation principles on a competitive\nmodel, yielding a score which is 33 F1 lower compared to evaluating by prior\nlenient practices.",
    "descriptor": "\nComments: *SEM 2021\n",
    "authors": [
      "Arie Cattan",
      "Alon Eirew",
      "Gabriel Stanovsky",
      "Mandar Joshi",
      "Ido Dagan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04192"
  },
  {
    "id": "arXiv:2106.04195",
    "title": "Learning by Distillation: A Self-Supervised Learning Framework for  Optical Flow Estimation",
    "abstract": "We present DistillFlow, a knowledge distillation approach to learning optical\nflow. DistillFlow trains multiple teacher models and a student model, where\nchallenging transformations are applied to the input of the student model to\ngenerate hallucinated occlusions as well as less confident predictions. Then, a\nself-supervised learning framework is constructed: confident predictions from\nteacher models are served as annotations to guide the student model to learn\noptical flow for those less confident predictions. The self-supervised learning\nframework enables us to effectively learn optical flow from unlabeled data, not\nonly for non-occluded pixels, but also for occluded pixels. DistillFlow\nachieves state-of-the-art unsupervised learning performance on both KITTI and\nSintel datasets. Our self-supervised pre-trained model also provides an\nexcellent initialization for supervised fine-tuning, suggesting an alternate\ntraining paradigm in contrast to current supervised learning methods that\nhighly rely on pre-training on synthetic data. At the time of writing, our\nfine-tuned models ranked 1st among all monocular methods on the KITTI 2015\nbenchmark, and outperform all published methods on the Sintel Final benchmark.\nMore importantly, we demonstrate the generalization capability of DistillFlow\nin three aspects: framework generalization, correspondence generalization and\ncross-dataset generalization.",
    "descriptor": "\nComments: TPAMI 2021\n",
    "authors": [
      "Pengpeng Liu",
      "Michael R. Lyu",
      "Irwin King",
      "Jia Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04195"
  },
  {
    "id": "arXiv:2106.04201",
    "title": "On the nonexistence of FO-continuous path and tree-decompositions",
    "abstract": "Bojanczyk and Pilipczuk showed in their celebrated article \"Definability\nequals recognizability for graphs of bounded treewidth\" (LICS 2016) that\nmonadic second-order logic can define tree-decompositions in graphs of bounded\ntreewidth. This raises the question whether such decompositions can already be\ndefined in first-order logic (FO).\nWe start by introducing the notion of tree-decompositions of bounded span,\nwhich restricts the diameter of the subtree consisting of the bags containing a\nsame node of the structure. Having a bounded span is a natural property of\ntree-decompositions when dealing with FO, since equality of nodes cannot in\ngeneral be recovered in FO when it doesn't hold. In particular, it encompasses\nthe notion of domino tree-decompositions.\nWe show that path-decompositions of bounded span are not FO-continuous, in\nthe sense that there exist arbitrarily FO-similar graphs of bounded pathwidth\nwhich do not possess FO-similar path-decompositions of bounded span. Then, we\nshow that tree-decompositions of bounded span are not FO-continuous either.",
    "descriptor": "",
    "authors": [
      "Julien Grange"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.04201"
  },
  {
    "id": "arXiv:2106.04202",
    "title": "Model Predictive Robot-Environment Interaction Control for Mobile  Manipulation Tasks",
    "abstract": "Modern, torque-controlled service robots can regulate contact forces when\ninteracting with their environment. Model Predictive Control (MPC) is a\npowerful method to solve the underlying control problem, allowing to plan for\nwhole-body motions while including different constraints imposed by the robot\ndynamics or its environment. However, an accurate model of the\nrobot-environment is needed to achieve a satisfying closed-loop performance.\nCurrently, this necessity undermines the performance and generality of MPC in\nmanipulation tasks. In this work, we combine an MPC-based whole-body controller\nwith two adaptive schemes, derived from online system identification and\nadaptive control. As a result, we enable a general mobile manipulator to\ninteract with unknown environments, without any need for re-tuning parameters\nor pre-modeling the interacting objects. In combination with the MPC\ncontroller, the two adaptive approaches are validated and benchmarked with a\nball-balancing manipulator in door opening and object lifting tasks.",
    "descriptor": "\nComments: IEEE International Conference on Robotics and Automation (ICRA) 2021\n",
    "authors": [
      "Maria Vittoria Minniti",
      "Ruben Grandia",
      "Kevin F\u00e4h",
      "Farbod Farshidian",
      "Marco Hutter"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04202"
  },
  {
    "id": "arXiv:2106.04203",
    "title": "On the Outage Capacity of the Massive MIMO Diversity Channel",
    "abstract": "We consider the massive Multiple Input Multiple Output (MIMO) diversity\nchannel affected by independent and identically distributed Rayleigh fading,\nwith linear processing at both transmitter and receiver sides, and analyze the\noutage capacity for large number of antennas. We first discuss the classical\nSingle Input Multiple Output (SIMO) diversity channel that uses Maximal Ratio\nCombining (MRC) or Selection Combining (SC). For MRC, a numerical computation\nand a Gaussian Approximation (GA) are considered, whereas for SC an exact\nevaluation is possible. The analysis is then straightforwardly extended to the\nMultiple Input Single Output (MISO) system that uses Maximal Ratio Transmission\n(MRT) or transmit antenna selection. The general Multiple Input Multiple Output\n(MIMO) system that pursues full diversity is finally considered, with both\noptimal linear processing and simple antenna selection at both transmitter and\nreceiver. If the number of antennas is sufficiently large on at least one side,\nthe outage capacity of each considered diversity channel approaches that of a\nsuitable reference Additive White Gaussian Noise (AWGN) channel with properly\ndefined Signal-to-Noise Ratio (SNR), which provides a performance benchmark.\nThis conclusion is valid for large but realistic number of antennas compatible\nwith the assumption of independent fading.",
    "descriptor": "",
    "authors": [
      "Marco Martalo",
      "Riccardo Raheli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04203"
  },
  {
    "id": "arXiv:2106.04205",
    "title": "Micro BTB: A High Performance and Lightweight Last-Level Branch Target  Buffer for Servers",
    "abstract": "High-performance branch target buffers (BTBs) and the L1I cache are key to\nhigh-performance front-end. Modern branch predictors are highly accurate, but\nwith an increase in code footprint in modern-day server workloads, BTB and L1I\nmisses are still frequent. Recent industry trend shows usage of large BTBs\n(100s of KB per core) that provide performance closer to the ideal BTB along\nwith a decoupled front-end that provides efficient fetch-directed L1I\ninstruction prefetching. On the other hand, techniques proposed by academia,\nlike BTB prefetching and using retire order stream for learning, fail to\nprovide significant performance with modern-day processor cores that are deeper\nand wider.\nWe solve the problem fundamentally by increasing the storage density of the\nlast-level BTB. We observe that not all branch instructions require a full\nbranch target address. Instead, we can store the branch target as a branch\noffset, relative to the branch instruction. Using branch offset enables the BTB\nto store multiple branches per entry. We reduce the BTB storage in half, but we\nobserve that it increases skewness in the BTB. We propose a skewed indexed and\ncompressed last-level BTB design called MicroBTB (MBTB) that stores multiple\nbranches per BTB entry. We evaluate MBTB on 100 industry-provided server\nworkloads. A 4K-entry MBTB provides 17.61% performance improvement compared to\nan 8K-entry baseline BTB design with a storage savings of 47.5KB per core.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Vishal Gupta",
      "Biswabandan Panda"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2106.04205"
  },
  {
    "id": "arXiv:2106.04206",
    "title": "Efficient Sampling in POMDPs with Lipschitz Bandits for Motion Planning  in Continuous Spaces",
    "abstract": "Decision making under uncertainty can be framed as a partially observable\nMarkov decision process (POMDP). Finding exact solutions of POMDPs is generally\ncomputationally intractable, but the solution can be approximated by\nsampling-based approaches. These sampling-based POMDP solvers rely on\nmulti-armed bandit (MAB) heuristics, which assume the outcomes of different\nactions to be uncorrelated. In some applications, like motion planning in\ncontinuous spaces, similar actions yield similar outcomes. In this paper, we\nutilize variants of MAB heuristics that make Lipschitz continuity assumptions\non the outcomes of actions to improve the efficiency of sampling-based planning\napproaches. We demonstrate the effectiveness of this approach in the context of\nmotion planning for automated driving.",
    "descriptor": "\nComments: In Proceedings of the IEEE Intelligent Vehicle Symposium 2021\n",
    "authors": [
      "\u00d6mer \u015eahin Ta\u015f",
      "Felix Hauser",
      "Martin Lauer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04206"
  },
  {
    "id": "arXiv:2106.04207",
    "title": "Cooperative Stochastic Multi-agent Multi-armed Bandits Robust to  Adversarial Corruptions",
    "abstract": "We study the problem of stochastic bandits with adversarial corruptions in\nthe cooperative multi-agent setting, where $V$ agents interact with a common\n$K$-armed bandit problem, and each pair of agents can communicate with each\nother to expedite the learning process. In the problem, the rewards are\nindependently sampled from distributions across all agents and rounds, but they\nmay be corrupted by an adversary. Our goal is to minimize both the overall\nregret and communication cost across all agents. We first show that an additive\nterm of corruption is unavoidable for any algorithm in this problem. Then, we\npropose a new algorithm that is agnostic to the level of corruption. Our\nalgorithm not only achieves near-optimal regret in the stochastic setting, but\nalso obtains a regret with an additive term of corruption in the corrupted\nsetting, while maintaining efficient communication. The algorithm is also\napplicable for the single-agent corruption problem, and achieves a high\nprobability regret that removes the multiplicative dependence of $K$ on\ncorruption level. Our result of the single-agent case resolves an open question\nfrom Gupta et al. [2019].",
    "descriptor": "",
    "authors": [
      "Junyan Liu",
      "Shuai Li",
      "Dapeng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04207"
  },
  {
    "id": "arXiv:2106.04208",
    "title": "Grapevine Winter Pruning Automation: On Potential Pruning Points  Detection through 2D Plant Modeling using Grapevine Segmentation",
    "abstract": "Grapevine winter pruning is a complex task, that requires skilled workers to\nexecute it correctly. The complexity of this task is also the reason why it is\ntime consuming. Considering that this operation takes about 80-120 hours/ha to\nbe completed, and therefore is even more crucial in large-size vineyards, an\nautomated system can help to speed up the process. To this end, this paper\npresents a novel multidisciplinary approach that tackles this challenging task\nby performing object segmentation on grapevine images, used to create a\nrepresentative model of the grapevine plants. Second, a set of potential\npruning points is generated from this plant representation. We will describe\n(a) a methodology for data acquisition and annotation, (b) a neural network\nfine-tuning for grapevine segmentation, (c) an image processing based method\nfor creating the representative model of grapevines, starting from the inferred\nsegmentation and (d) potential pruning points detection and localization, based\non the plant model which is a simplification of the grapevine structure. With\nthis approach, we are able to identify a significant set of potential pruning\npoints on the canes, that can be used, with further selection, to derive the\nfinal set of the real pruning points.",
    "descriptor": "",
    "authors": [
      "Miguel Fernandes",
      "Antonello Scaldaferri",
      "Giuseppe Fiameni",
      "Tao Teng",
      "Matteo Gatti",
      "Stefano Poni",
      "Claudio Semini",
      "Darwin Caldwell",
      "Fei Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04208"
  },
  {
    "id": "arXiv:2106.04209",
    "title": "MindReader: Recommendation over Knowledge Graph Entities with Explicit  User Ratings",
    "abstract": "Knowledge Graphs (KGs) have been integrated in several models of\nrecommendation to augment the informational value of an item by means of its\nrelated entities in the graph. Yet, existing datasets only provide explicit\nratings on items and no information is provided about user opinions of other\n(non-recommendable) entities. To overcome this limitation, we introduce a new\ndataset, called the MindReader, providing explicit user ratings both for items\nand for KG entities. In this first version, the MindReader dataset provides\nmore than 102 thousands explicit ratings collected from 1,174 real users on\nboth items and entities from a KG in the movie domain. This dataset has been\ncollected through an online interview application that we also release open\nsource. As a demonstration of the importance of this new dataset, we present a\ncomparative study of the effect of the inclusion of ratings on non-item KG\nentities in a variety of state-of-the-art recommendation models. In particular,\nwe show that most models, whether designed specifically for graph data or not,\nsee improvements in recommendation quality when trained on explicit non-item\nratings. Moreover, for some models, we show that non-item ratings can\neffectively replace item ratings without loss of recommendation quality. This\nfinding, thanks also to an observed greater familiarity of users towards common\nKG entities than towards long-tail items, motivates the use of KG entities for\nboth warm and cold-start recommendations.",
    "descriptor": "",
    "authors": [
      "Anders H. Brams",
      "Anders L. Jakobsen",
      "Theis E. Jendal",
      "Matteo Lissandrini",
      "Peter Dolog",
      "Katja Hose"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04209"
  },
  {
    "id": "arXiv:2106.04210",
    "title": "Defining definition: a Text mining Approach to Define Innovative  Technological Fields",
    "abstract": "One of the first task of an innovative project is delineating the scope of\nthe project itself or of the product/service to be developed. A wrong scope\ndefinition can determine (in the worst case) project failure. A good scope\ndefinition become even more relevant in technological intensive innovation\nprojects, nowadays characterized by a highly dynamic multidisciplinary,\nturbulent and uncertain environment. In these cases, the boundaries of the\nproject are not easily detectable and it is difficult to decide what it is\nin-scope and out-of-scope. The present work proposes a tool for the scope\ndelineation process, that automatically define an innovative technological\nfield or a new technology. The tool is based on Text Mining algorithm that\nexploits Elsevier's Scopus abstracts in order to the extract relevant data to\ndefine a technological scope. The automatic definition tool is then applied on\nfour case studies: Artificial Intelligence and Data Science. The results show\nhow the tool can provide many crucial information in the definition process of\na technological field. In particular for the target technological field (or\ntechnology), it provides the definition and other elements related to the\ntarget.",
    "descriptor": "",
    "authors": [
      "Vito Giordano",
      "Filippo Chiarello",
      "Elena Cervelli"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04210"
  },
  {
    "id": "arXiv:2106.04215",
    "title": "On the use of automatically generated synthetic image datasets for  benchmarking face recognition",
    "abstract": "The availability of large-scale face datasets has been key in the progress of\nface recognition. However, due to licensing issues or copyright infringement,\nsome datasets are not available anymore (e.g. MS-Celeb-1M). Recent advances in\nGenerative Adversarial Networks (GANs), to synthesize realistic face images,\nprovide a pathway to replace real datasets by synthetic datasets, both to train\nand benchmark face recognition (FR) systems. The work presented in this paper\nprovides a study on benchmarking FR systems using a synthetic dataset. First,\nwe introduce the proposed methodology to generate a synthetic dataset, without\nthe need for human intervention, by exploiting the latent structure of a\nStyleGAN2 model with multiple controlled factors of variation. Then, we confirm\nthat (i) the generated synthetic identities are not data subjects from the\nGAN's training dataset, which is verified on a synthetic dataset with 10K+\nidentities; (ii) benchmarking results on the synthetic dataset are a good\nsubstitution, often providing error rates and system ranking similar to the\nbenchmarking on the real dataset.",
    "descriptor": "\nComments: 11 pages, Accepted for publication in the 2021 International Joint Conference on Biometrics (IJCB 2021)\n",
    "authors": [
      "Laurent Colbois",
      "Tiago de Freitas Pereira",
      "S\u00e9bastien Marcel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04215"
  },
  {
    "id": "arXiv:2106.04216",
    "title": "A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021",
    "abstract": "We evaluate three leading dependency parser systems from different paradigms\non a small yet diverse subset of languages in terms of their\naccuracy-efficiency Pareto front. As we are interested in efficiency, we\nevaluate core parsers without pretrained language models (as these are\ntypically huge networks and would constitute most of the compute time) or other\naugmentations that can be transversally applied to any of them. Biaffine\nparsing emerges as a well-balanced default choice, with sequence-labelling\nparsing being preferable if inference speed (but not training energy cost) is\nthe priority.",
    "descriptor": "\nComments: To be published in proceedings of the 17th International Conference on Parsing Technologies\n",
    "authors": [
      "Mar Anderson",
      "Carlos G\u00f3mez Rodr\u00edguez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04216"
  },
  {
    "id": "arXiv:2106.04217",
    "title": "Dynamic Sparse Training for Deep Reinforcement Learning",
    "abstract": "Deep reinforcement learning has achieved significant success in many\ndecision-making tasks in various fields. However, it requires a large training\ntime of dense neural networks to obtain a good performance. This hinders its\napplicability on low-resource devices where memory and computation are strictly\nconstrained. In a step towards enabling deep reinforcement learning agents to\nbe applied to low-resource devices, in this work, we propose for the first time\nto dynamically train deep reinforcement learning agents with sparse neural\nnetworks from scratch. We adopt the evolution principles of dynamic sparse\ntraining in the reinforcement learning paradigm and introduce a training\nalgorithm that optimizes the sparse topology and the weight values jointly to\ndynamically fit the incoming data. Our approach is easy to be integrated into\nexisting deep reinforcement learning algorithms and has many favorable\nadvantages. First, it allows for significant compression of the network size\nwhich reduces the memory and computation costs substantially. This would\naccelerate not only the agent inference but also its training process. Second,\nit speeds up the agent learning process and allows for reducing the number of\nrequired training steps. Third, it can achieve higher performance than training\nthe dense counterpart network. We evaluate our approach on OpenAI gym\ncontinuous control tasks. The experimental results show the effectiveness of\nour approach in achieving higher performance than one of the state-of-art\nbaselines with a 50\\% reduction in the network size and floating-point\noperations (FLOPs). Moreover, our proposed approach can reach the same\nperformance achieved by the dense network with a 40-50\\% reduction in the\nnumber of training steps.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Ghada Sokar",
      "Elena Mocanu",
      "Decebal Constantin Mocanu",
      "Mykola Pechenizkiy",
      "Peter Stone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04217"
  },
  {
    "id": "arXiv:2106.04219",
    "title": "Time-series Imputation of Temporally-occluded Multiagent Trajectories",
    "abstract": "In multiagent environments, several decision-making individuals interact\nwhile adhering to the dynamics constraints imposed by the environment. These\ninteractions, combined with the potential stochasticity of the agents'\ndecision-making processes, make such systems complex and interesting to study\nfrom a dynamical perspective. Significant research has been conducted on\nlearning models for forward-direction estimation of agent behaviors, for\nexample, pedestrian predictions used for collision-avoidance in self-driving\ncars. However, in many settings, only sporadic observations of agents may be\navailable in a given trajectory sequence. For instance, in football, subsets of\nplayers may come in and out of view of broadcast video footage, while\nunobserved players continue to interact off-screen. In this paper, we study the\nproblem of multiagent time-series imputation, where available past and future\nobservations of subsets of agents are used to estimate missing observations for\nother agents. Our approach, called the Graph Imputer, uses forward- and\nbackward-information in combination with graph networks and variational\nautoencoders to enable learning of a distribution of imputed trajectories. We\nevaluate our approach on a dataset of football matches, using a projective\ncamera module to train and evaluate our model for the off-screen player state\nestimation setting. We illustrate that our method outperforms several\nstate-of-the-art approaches, including those hand-crafted for football.",
    "descriptor": "",
    "authors": [
      "Shayegan Omidshafiei",
      "Daniel Hennes",
      "Marta Garnelo",
      "Eugene Tarassov",
      "Zhe Wang",
      "Romuald Elie",
      "Jerome T. Connor",
      "Paul Muller",
      "Ian Graham",
      "William Spearman",
      "Karl Tuyls"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.04219"
  },
  {
    "id": "arXiv:2106.04221",
    "title": "Multi-output Gaussian Processes for Uncertainty-aware Recommender  Systems",
    "abstract": "Recommender systems are often designed based on a collaborative filtering\napproach, where user preferences are predicted by modelling interactions\nbetween users and items. Many common approaches to solve the collaborative\nfiltering task are based on learning representations of users and items,\nincluding simple matrix factorization, Gaussian process latent variable models,\nand neural-network based embeddings. While matrix factorization approaches fail\nto model nonlinear relations, neural networks can potentially capture such\ncomplex relations with unprecedented predictive power and are highly scalable.\nHowever, neither of them is able to model predictive uncertainties. In\ncontrast, Gaussian Process based models can generate a predictive distribution,\nbut cannot scale to large amounts of data. In this manuscript, we propose a\nnovel approach combining the representation learning paradigm of collaborative\nfiltering with multi-output Gaussian processes in a joint framework to generate\nuncertainty-aware recommendations. We introduce an efficient strategy for model\ntraining and inference, resulting in a model that scales to very large and\nsparse datasets and achieves competitive performance in terms of classical\nmetrics quantifying the reconstruction error. In addition to accurately\npredicting user preferences, our model also provides meaningful uncertainty\nestimates about that prediction.",
    "descriptor": "",
    "authors": [
      "Yinchong Yang",
      "Florian Buettner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04221"
  },
  {
    "id": "arXiv:2106.04222",
    "title": "A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags  for Low Resource UD Parsing",
    "abstract": "We evaluate the efficacy of predicted UPOS tags as input features for\ndependency parsers in lower resource settings to evaluate how treebank size\naffects the impact tagging accuracy has on parsing performance. We do this for\nreal low resource universal dependency treebanks, artificially low resource\ndata with varying treebank sizes, and for very small treebanks with varying\namounts of augmented data. We find that predicted UPOS tags are somewhat\nhelpful for low resource treebanks, especially when fewer fully-annotated trees\nare available. We also find that this positive impact diminishes as the amount\nof data increases.",
    "descriptor": "\nComments: To be published in proceedings of the 16th International Conference on Parsing Technologies. Earlier versions were rejected at the 16th Conference of the European Chapter of the Association for Computational Linguistics and the 23rd Nordic Conference on Computational Linguistics\n",
    "authors": [
      "Mark Anderson",
      "Mathieu Dehouck",
      "Carlos G\u00f3mez Rodr\u00edguez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04222"
  },
  {
    "id": "arXiv:2106.04223",
    "title": "Outage Performance of Multi-UAV Relaying-based Imperfect Hardware Hybrid  Satellite-Terrestrial Networks",
    "abstract": "In this paper, we consider an imperfect hardware hybrid satellite-terrestrial\nnetwork (HSTN) where the satellite communication with a ground user equipment\n(UE) is aided by the multiple amplify-and-forward (AF) three-dimensional ($3$D)\nmobile unmanned aerial vehicle (UAV) relays. Herein, we consider that all\ntransceiver nodes are corrupted by the radio frequency hardware impairments\n(RFHI). Further, a stochastic mixed mobility (MM) model is employed to\ncharacterize the instantaneous location of $3$D mobile UAV relays in a\ncylindrical cell with UE lying at its center on ground plane. Taking into\naccount the aggregate RFHI model for satellite and UAV relay transceivers and\nthe random $3$D distances-based path loss for UAV relay-UE links, we\ninvestigate the outage probability (OP) and corresponding asymptotic outage\nbehaviour of the system under an opportunistic relay selection scheme in a\nunified form for shadowed-Rician satellite links' channels and\nNakagami-\\emph{m} as well as Rician terrestrial links' channels. We corroborate\ntheoretical analysis by simulations.",
    "descriptor": "\nComments: 12 pages, 3 figures, Submitted to IEEE for possible journal publication\n",
    "authors": [
      "Pankaj K. Sharma",
      "Deepika Gupta"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04223"
  },
  {
    "id": "arXiv:2106.04224",
    "title": "Improved Online Correlated Selection",
    "abstract": "This paper studies the online correlated selection (OCS) problem introduced\nby Fahrbach, Huang, Tao, and Zadimoghaddam (2020) to get the first\nedge-weighted online bipartite matching algorithm that breaks the $0.5$\nbarrier. Suppose that we receive a pair of elements in each round and select\none of them. Can we select with negative correlation to be more effective than\nindependent random selections? Our contributions are threefold. For semi-OCS,\nwhich considers the probability that an element remains unselected after\nappearing in $k$ rounds, we give an optimal algorithm that minimizes this\nprobability for all $k$. It leads to $0.536$-competitive unweighted and\nvertex-weighted online bipartite matching algorithms that randomize over only\ntwo options in each round, improving the previous 0.508-competitive ratio by\nFahrbach et al. (2020). Further, we give the first multi-way semi-OCS that\nallows an arbitrary number of elements with arbitrary masses in each round. As\nan application, it rounds the Balance algorithm in unweighted and\nvertex-weighted online bipartite matching to get a $0.593$-competitive ratio.\nThis is the first algorithm other than Ranking whose competitive ratio is\nbeyond the $0.5 + \\epsilon$ regime. Finally, we study OCS, which further\nconsiders the probability that an element is unselected in any subset of\nrounds. We prove that the optimal \"level of negative correlation\" is between\n$0.167$ and $0.25$, improving the previous bounds of $0.109$ and $1$ by\nFahrbach et al. (2020). Our OCS gives a $0.519$-competitive edge-weighted\nonline bipartite matching algorithm, improving the previous $0.508$-competitive\nratio by Fahrbach et al. (2020).",
    "descriptor": "",
    "authors": [
      "Ruiquan Gao",
      "Zhongtian He",
      "Zhiyi Huang",
      "Zipei Nie",
      "Bijun Yuan",
      "Yan Zhong"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04224"
  },
  {
    "id": "arXiv:2106.04225",
    "title": "On the role of feedback in visual processing: a predictive coding  perspective",
    "abstract": "Brain-inspired machine learning is gaining increasing consideration,\nparticularly in computer vision. Several studies investigated the inclusion of\ntop-down feedback connections in convolutional networks; however, it remains\nunclear how and when these connections are functionally helpful. Here we\naddress this question in the context of object recognition under noisy\nconditions. We consider deep convolutional networks (CNNs) as models of\nfeed-forward visual processing and implement Predictive Coding (PC) dynamics\nthrough feedback connections (predictive feedback) trained for reconstruction\nor classification of clean images. To directly assess the computational role of\npredictive feedback in various experimental situations, we optimize and\ninterpret the hyper-parameters controlling the network's recurrent dynamics.\nThat is, we let the optimization process determine whether top-down connections\nand predictive coding dynamics are functionally beneficial. Across different\nmodel depths and architectures (3-layer CNN, ResNet18, and EfficientNetB0) and\nagainst various types of noise (CIFAR100-C), we find that the network\nincreasingly relies on top-down predictions as the noise level increases; in\ndeeper networks, this effect is most prominent at lower layers. In addition,\nthe accuracy of the network implementing PC dynamics significantly increases\nover time-steps, compared to its equivalent forward network. All in all, our\nresults provide novel insights relevant to Neuroscience by confirming the\ncomputational role of feedback connections in sensory systems, and to Machine\nLearning by revealing how these can improve the robustness of current vision\nmodels.",
    "descriptor": "\nComments: 'Andrea Alamia' and 'Milad Mozafari' contributed equally to this work\n",
    "authors": [
      "Andrea Alamia",
      "Milad Mozafari",
      "Bhavin Choksi",
      "Rufin VanRullen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.04225"
  },
  {
    "id": "arXiv:2106.04230",
    "title": "Demystifying the Performance of Bluetooth Mesh: Experimental Evaluation  and Optimization",
    "abstract": "Mesh connectivity is attractive for Internet-of- Things (IoT) applications\nfrom various perspectives. The recent Bluetooth mesh specification provides a\nfull-stack mesh networking solution, potentially for thousands of nodes.\nAlthough Bluetooth mesh has been adopted for various IoT applications, its\nperformance aspects are not extensively investigated in literature. This paper\nprovides an experimental evaluation of Bluetooth mesh (using Nordic nRF52840\ndevices) with an emphasis on those aspects which are not well-investigated in\nliterature. Such aspects include evaluation of unicast and group modes,\nperformance under different traffic patterns, impact of message segmentation,\nand most importantly, latency performance for perfect reliability. The paper\nalso investigates performance enhancement of Bluetooth mesh based on different\ntechniques including parametric adjustments, extended advertisements\n(introduced in Bluetooth 5.0), power control, and customized relaying. Results\nprovide insights into system-level performance of Bluetooth mesh while\nclarifying various important issues identified in recent studies.",
    "descriptor": "\nComments: To appear in IEEE Wireless Days 2021\n",
    "authors": [
      "Adnan Aijaz",
      "Aleksandar Stanoev",
      "Dominic London",
      "Victor Marot"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04230"
  },
  {
    "id": "arXiv:2106.04232",
    "title": "Giving Commands to a Self-Driving Car: How to Deal with Uncertain  Situations?",
    "abstract": "Current technology for autonomous cars primarily focuses on getting the\npassenger from point A to B. Nevertheless, it has been shown that passengers\nare afraid of taking a ride in self-driving cars. One way to alleviate this\nproblem is by allowing the passenger to give natural language commands to the\ncar. However, the car can misunderstand the issued command or the visual\nsurroundings which could lead to uncertain situations. It is desirable that the\nself-driving car detects these situations and interacts with the passenger to\nsolve them. This paper proposes a model that detects uncertain situations when\na command is given and finds the visual objects causing it. Optionally, a\nquestion generated by the system describing the uncertain objects is included.\nWe argue that if the car could explain the objects in a human-like way,\npassengers could gain more confidence in the car's abilities. Thus, we\ninvestigate how to (1) detect uncertain situations and their underlying causes,\nand (2) how to generate clarifying questions for the passenger. When evaluating\non the Talk2Car dataset, we show that the proposed model, \\acrfull{pipeline},\nimproves \\gls{m:ambiguous-absolute-increase} in terms of $IoU_{.5}$ compared to\nnot using \\gls{pipeline}. Furthermore, we designed a referring expression\ngenerator (REG) \\acrfull{reg_model} tailored to a self-driving car setting\nwhich yields a relative improvement of \\gls{m:meteor-relative} METEOR and\n\\gls{m:rouge-relative} ROUGE-l compared with state-of-the-art REG models, and\nis three times faster.",
    "descriptor": "\nComments: Accepted in Engineering Applications of Artificial Intelligence (EAAI) journal\n",
    "authors": [
      "Thierry Deruyttere",
      "Victor Milewski",
      "Marie-Francine Moens"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04232"
  },
  {
    "id": "arXiv:2106.04233",
    "title": "Towards interval uncertainty propagation control in bivariate  aggregation processes and the introduction of width-limited interval-valued  overlap functions",
    "abstract": "Overlap functions are a class of aggregation functions that measure the\noverlapping degree between two values. Interval-valued overlap functions were\ndefined as an extension to express the overlapping of interval-valued data, and\nthey have been usually applied when there is uncertainty regarding the\nassignment of membership degrees. The choice of a total order for intervals can\nbe significant, which motivated the recent developments on interval-valued\naggregation functions and interval-valued overlap functions that are increasing\nto a given admissible order, that is, a total order that refines the usual\npartial order for intervals. Also, width preservation has been considered on\nthese recent works, in an intent to avoid the uncertainty increase and\nguarantee the information quality, but no deeper study was made regarding the\nrelation between the widths of the input intervals and the output interval,\nwhen applying interval-valued functions, or how one can control such\nuncertainty propagation based on this relation. Thus, in this paper we: (i)\nintroduce and develop the concepts of width-limited interval-valued functions\nand width limiting functions, presenting a theoretical approach to analyze the\nrelation between the widths of the input and output intervals of bivariate\ninterval-valued functions, with special attention to interval-valued\naggregation functions; (ii) introduce the concept of $(a,b)$-ultramodular\naggregation functions, a less restrictive extension of one-dimension convexity\nfor bivariate aggregation functions, which have an important predictable\nbehaviour with respect to the width when extended to the interval-valued\ncontext; (iii) define width-limited interval-valued overlap functions, taking\ninto account a function that controls the width of the output interval; (iv)\npresent and compare three construction methods for these width-limited\ninterval-valued overlap functions.",
    "descriptor": "\nComments: submitted\n",
    "authors": [
      "Tiago da Cruz Asmus",
      "Gra\u00e7aliz Pereira Dimuro",
      "Benjam\u00edn Bedregal",
      "Jos\u00e9 Antonio Sanz",
      "Radko Mesiar",
      "Humberto Bustince"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04233"
  },
  {
    "id": "arXiv:2106.04235",
    "title": "Definitions of intent suitable for algorithms",
    "abstract": "Intent modifies an actor's culpability of many types wrongdoing. Autonomous\nAlgorithmic Agents have the capability of causing harm, and whilst their\ncurrent lack of legal personhood precludes them from committing crimes, it is\nuseful for a number of parties to understand under what type of intentional\nmode an algorithm might transgress. From the perspective of the creator or\nowner they would like ensure that their algorithms never intend to cause harm\nby doing things that would otherwise be labelled criminal if committed by a\nlegal person. Prosecutors might have an interest in understanding whether the\nactions of an algorithm were internally intended according to a transparent\ndefinition of the concept. The presence or absence of intention in the\nalgorithmic agent might inform the court as to the complicity of its owner.\nThis article introduces definitions for direct, oblique (or indirect) and\nulterior intent which can be used to test for intent in an algorithmic actor.",
    "descriptor": "",
    "authors": [
      "Hal Ashton"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04235"
  },
  {
    "id": "arXiv:2106.04240",
    "title": "The Medkit-Learn(ing) Environment: Medical Decision Modelling through  Simulation",
    "abstract": "Understanding decision-making in clinical environments is of paramount\nimportance if we are to bring the strengths of machine learning to ultimately\nimprove patient outcomes. Several factors including the availability of public\ndata, the intrinsically offline nature of the problem, and the complexity of\nhuman decision making, has meant that the mainstream development of algorithms\nis often geared towards optimal performance in tasks that do not necessarily\ntranslate well into the medical regime; often overlooking more niche issues\ncommonly associated with the area. We therefore present a new benchmarking\nsuite designed specifically for medical sequential decision making: the\nMedkit-Learn(ing) Environment, a publicly available Python package providing\nsimple and easy access to high-fidelity synthetic medical data. While providing\na standardised way to compare algorithms in a realistic medical setting we\nemploy a generating process that disentangles the policy and environment\ndynamics to allow for a range of customisations, thus enabling systematic\nevaluation of algorithms' robustness against specific challenges prevalent in\nhealthcare.",
    "descriptor": "",
    "authors": [
      "Alex J. Chan",
      "Ioana Bica",
      "Alihan Huyuk",
      "Daniel Jarrett",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04240"
  },
  {
    "id": "arXiv:2106.04243",
    "title": "Parameter Inference with Bifurcation Diagrams",
    "abstract": "Estimation of parameters in differential equation models can be achieved by\napplying learning algorithms to quantitative time-series data. However,\nsometimes it is only possible to measure qualitative changes of a system in\nresponse to a controlled condition. In dynamical systems theory, such change\npoints are known as \\textit{bifurcations} and lie on a function of the\ncontrolled condition called the \\textit{bifurcation diagram}. In this work, we\npropose a gradient-based semi-supervised approach for inferring the parameters\nof differential equations that produce a user-specified bifurcation diagram.\nThe cost function contains a supervised error term that is minimal when the\nmodel bifurcations match the specified targets and an unsupervised bifurcation\nmeasure which has gradients that push optimisers towards bifurcating parameter\nregimes. The gradients can be computed without the need to differentiate\nthrough the operations of the solver that was used to compute the diagram. We\ndemonstrate parameter inference with minimal models which explore the space of\nsaddle-node and pitchfork diagrams and the genetic toggle switch from synthetic\nbiology. Furthermore, the cost landscape allows us to organise models in terms\nof topological and geometric equivalence.",
    "descriptor": "\nComments: Under review at NeurIPS 2021\n",
    "authors": [
      "Gregory Szep",
      "Neil Dalchau",
      "Attila Csikasz-Nagy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.04243"
  },
  {
    "id": "arXiv:2106.04247",
    "title": "Private Counting from Anonymous Messages: Near-Optimal Accuracy with  Vanishing Communication Overhead",
    "abstract": "Differential privacy (DP) is a formal notion for quantifying the privacy loss\nof algorithms. Algorithms in the central model of DP achieve high accuracy but\nmake the strongest trust assumptions whereas those in the local DP model make\nthe weakest trust assumptions but incur substantial accuracy loss. The shuffled\nDP model (Bittau et al., 2017; Erlingsson et al., 2019; Cheu et al., 2019) has\nrecently emerged as a feasible middle ground between the central and local\nmodels, providing stronger trust assumptions than the former while promising\nhigher accuracies than the latter. In this paper, we obtain practical\ncommunication-efficient algorithms in the shuffled DP model for two basic\naggregation primitives used in machine learning: 1) binary summation, and 2)\nhistograms over a moderate number of buckets. Our algorithms achieve accuracy\nthat is arbitrarily close to that of central DP algorithms with an expected\ncommunication per user essentially matching what is needed without any privacy\nconstraints! We demonstrate the practicality of our algorithms by\nexperimentally comparing their performance to several widely-used protocols\nsuch as Randomized Response (Warner, 1965) and RAPPOR (Erlingsson et al.,\n2014).",
    "descriptor": "\nComments: Originally appeared in ICML'20. This version contains a correction of calculation errors in Theorem 13 of the ICML'20 version\n",
    "authors": [
      "Badih Ghazi",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Rasmus Pagh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04247"
  },
  {
    "id": "arXiv:2106.04248",
    "title": "Converged Reconfigurable Intelligent Surface and Mobile Edge Computing  for Space Information Networks",
    "abstract": "Space information networks (SIN) are facing an ever-increasing thirst for\nhigh-speed and high-capacity seamless data transmission due to the integration\nof ground, air, and space communications. However, this imposes a new paradigm\non the architecture design of the integrated SIN. Recently, reconfigurable\nintelligent surfaces (RISs) and mobile edge computing (MEC) are the most\npromising techniques, conceived to improve communication and computation\ncapability by reconfiguring the wireless propagation environment and\noffloading. Hence, converging RISs and MEC in SIN is becoming an effort to reap\nthe double benefits of computation and communication. In this article, we\npropose an RIS-assisted collaborative MEC architecture for SIN and discuss its\nimplementation. Then we present its potential benefits, major challenges, and\nfeasible applications. Subsequently, we study different cases to evaluate the\nsystem data rate and latency. Finally, we conclude with a list of open issues\nin this research area.",
    "descriptor": "",
    "authors": [
      "Xuelin Cao",
      "Bo Yang",
      "Chongwen Huang",
      "Chau Yuen",
      "Yan Zhang",
      "Dusit Niyato",
      "Zhu Han"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04248"
  },
  {
    "id": "arXiv:2106.04251",
    "title": "Constructing invariant tori using guaranteed Euler method",
    "abstract": "We show here how, using Euler's integration method and an associated function\nbounding the error in function of time, one can generate structures closely\nsurrounding the invariant tori of dynamical systems. Such structures are\nconstructed from a finite number of balls of $\\mathbb{R}^n$ and encompass the\ndeformations of the tori when small perturbations of the flow of the system\noccur.",
    "descriptor": "\nComments: 15 pages, 7 figures. arXiv admin note: text overlap with arXiv:2012.09310\n",
    "authors": [
      "Jawher Jerray",
      "Laurent Fribourg"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04251"
  },
  {
    "id": "arXiv:2106.04252",
    "title": "Meta-Learning to Compositionally Generalize",
    "abstract": "Natural language is compositional; the meaning of a sentence is a function of\nthe meaning of its parts. This property allows humans to create and interpret\nnovel sentences, generalizing robustly outside their prior experience. Neural\nnetworks have been shown to struggle with this kind of generalization, in\nparticular performing poorly on tasks designed to assess compositional\ngeneralization (i.e. where training and testing distributions differ in ways\nthat would be trivial for a compositional strategy to resolve). Their poor\nperformance on these tasks may in part be due to the nature of supervised\nlearning which assumes training and testing data to be drawn from the same\ndistribution. We implement a meta-learning augmented version of supervised\nlearning whose objective directly optimizes for out-of-distribution\ngeneralization. We construct pairs of tasks for meta-learning by sub-sampling\nexisting training data. Each pair of tasks is constructed to contain relevant\nexamples, as determined by a similarity metric, in an effort to inhibit models\nfrom memorizing their input. Experimental results on the COGS and SCAN datasets\nshow that our similarity-driven meta-learning can improve generalization\nperformance.",
    "descriptor": "\nComments: ACL2021 Camera Ready\n",
    "authors": [
      "Henry Conklin",
      "Bailin Wang",
      "Kenny Smith",
      "Ivan Titov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04252"
  },
  {
    "id": "arXiv:2106.04254",
    "title": "Coresets for Classification -- Simplified and Strengthened",
    "abstract": "We give relative error coresets for training linear classifiers with a broad\nclass of loss functions, including the logistic loss and hinge loss. Our\nconstruction achieves $(1\\pm \\epsilon)$ relative error with $\\tilde O(d \\cdot\n\\mu_y(X)^2/\\epsilon^2)$ points, where $\\mu_y(X)$ is a natural complexity\nmeasure of the data matrix $X \\in \\mathbb{R}^{n \\times d}$ and label vector $y\n\\in \\{-1,1\\}^n$, introduced in by Munteanu et al. 2018. Our result is based on\nsubsampling data points with probabilities proportional to their $\\ell_1$\n$Lewis$ $weights$. It significantly improves on existing theoretical bounds and\nperforms well in practice, outperforming uniform subsampling along with other\nimportance sampling methods. Our sampling distribution does not depend on the\nlabels, so can be used for active learning. It also does not depend on the\nspecific loss function, so a single coreset can be used in multiple training\nscenarios.",
    "descriptor": "",
    "authors": [
      "Tung Mai",
      "Anup B. Rao",
      "Cameron Musco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04254"
  },
  {
    "id": "arXiv:2106.04257",
    "title": "Augmenting Teleportation in Virtual Reality With Discrete Rotation  Angles",
    "abstract": "Locomotion is one of the most essential interaction tasks in virtual reality\n(VR) with teleportation being widely accepted as the state-of-the-art\nlocomotion technique at the time of this writing. A major draw-back of\nteleportation is the accompanying physical rotation that is necessary to adjust\nthe users' orientation either before or after teleportation. This is a limiting\nfactor for tethered head-mounted displays (HMDs) and static body postures and\ncan induce additional simulator sickness for HMDs with three degrees-of-freedom\n(DOF) due to missing parallax cues. To avoid physical rotation, previous work\nproposed discrete rotation at fixed intervals (InPlace) as a controller-based\ntechnique with low simulator sickness, yet the impact of varying intervals on\nspatial disorientation, user presence and performance remains to be explored.\nAn unevaluated technique found in commercial VR games is reorientation during\nthe teleportation process (TeleTurn), which prevents physical rotation but\npotentially increases interaction time due to its continuous orientation\nselection. In an exploratory user study, where participants were free to apply\nboth techniques, we evaluated the impact of rotation parameters of either\ntechnique on user performance and preference. Our results indicate that\ndiscrete InPlace rotation introduced no significant spatial disorientation,\nwhile user presence scores were increased. Discrete TeleTurn and teleportation\nwithout rotation was ranked higher and achieved a higher presence score than\ncontinuous TeleTurn, which is the current state-of-the-art found in VR games.\nBased on observations, that participants avoided TeleTurn rotation when\ndiscrete InPlace rotation was available, we distilled guidelines for designing\nteleportation without physical rotation.",
    "descriptor": "\nComments: 14 pages, 2 figures, 4 tables\n",
    "authors": [
      "Dennis Wolf",
      "Michael Rietzler",
      "Laura Bottner",
      "Enrico Rukzio"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04257"
  },
  {
    "id": "arXiv:2106.04258",
    "title": "Interpretable agent communication from scratch(with a generic visual  processor emerging on the side)",
    "abstract": "As deep networks begin to be deployed as autonomous agents, the issue of how\nthey can communicate with each other becomes important. Here, we train two deep\nnets from scratch to perform realistic referent identification through\nunsupervised emergent communication. We show that the largely interpretable\nemergent protocol allows the nets to successfully communicate even about object\ntypes they did not see at training time. The visual representations induced as\na by-product of our training regime, moreover, show comparable quality, when\nre-used as generic visual features, to a recent self-supervised learning model.\nOur results provide concrete evidence of the viability of (interpretable)\nemergent deep net communication in a more realistic scenario than previously\nconsidered, as well as establishing an intriguing link between this field and\nself-supervised visual learning.",
    "descriptor": "\nComments: 9 pages main text, 13 pages total\n",
    "authors": [
      "Roberto Dess\u00ec",
      "Eugene Kharitonov",
      "Marco Baroni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.04258"
  },
  {
    "id": "arXiv:2106.04260",
    "title": "Provably Robust Detection of Out-of-distribution Data (almost) for free",
    "abstract": "When applying machine learning in safety-critical systems, a reliable\nassessment of the uncertainy of a classifier is required. However, deep neural\nnetworks are known to produce highly overconfident predictions on\nout-of-distribution (OOD) data and even if trained to be non-confident on OOD\ndata one can still adversarially manipulate OOD data so that the classifer\nagain assigns high confidence to the manipulated samples. In this paper we\npropose a novel method where from first principles we combine a certifiable OOD\ndetector with a standard classifier into an OOD aware classifier. In this way\nwe achieve the best of two worlds: certifiably adversarially robust OOD\ndetection, even for OOD samples close to the in-distribution, without loss in\nprediction accuracy and close to state-of-the-art OOD detection performance for\nnon-manipulated OOD data. Moreover, due to the particular construction our\nclassifier provably avoids the asymptotic overconfidence problem of standard\nneural networks.",
    "descriptor": "",
    "authors": [
      "Alexander Meinke",
      "Julian Bitterwolf",
      "Matthias Hein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04260"
  },
  {
    "id": "arXiv:2106.04262",
    "title": "Question Generation for Adaptive Education",
    "abstract": "Intelligent and adaptive online education systems aim to make high-quality\neducation available for a diverse range of students. However, existing systems\nusually depend on a pool of hand-made questions, limiting how fine-grained and\nopen-ended they can be in adapting to individual students. We explore targeted\nquestion generation as a controllable sequence generation task. We first show\nhow to fine-tune pre-trained language models for deep knowledge tracing\n(LM-KT). This model accurately predicts the probability of a student answering\na question correctly, and generalizes to questions not seen in training. We\nthen use LM-KT to specify the objective and data for training a model to\ngenerate questions conditioned on the student and target difficulty. Our\nresults show we succeed at generating novel, well-calibrated language\ntranslation questions for second language learners from a real online education\nplatform.",
    "descriptor": "\nComments: 10 pages, 3 figures, ACL 2021\n",
    "authors": [
      "Megha Srivastava",
      "Noah Goodman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04262"
  },
  {
    "id": "arXiv:2106.04263",
    "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight  Sharing, and Dynamic Weight",
    "abstract": "Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity.",
    "descriptor": "",
    "authors": [
      "Qi Han",
      "Zejia Fan",
      "Qi Dai",
      "Lei Sun",
      "Ming-Ming Cheng",
      "Jiaying Liu",
      "Jingdong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04263"
  },
  {
    "id": "arXiv:2106.04265",
    "title": "Towards Social Role-Based Interruptibility Management",
    "abstract": "Managing individuals' attention and interruptibility is still a challenging\ntask in the field of human-computer interaction. Individuals' intrinsic\ninterruptibility preferences are often established for and across different\nsocial roles and life domains, which have not yet been captured by modeling\nshort-term opportunities alone. This paper investigates the applicability of\nsocial role theory and boundary management as theoretical underpinnings for\nanalyzing social roles and their associated interruptibility preferences. We\nconducted an in-the-wild study with 16 participants for five weeks to collect\nindividuals' social roles, interruptibility preferences, application usage and\nspatio-temporal information. A paired t-test shows that interruptibility models\nare significantly improved by incorporating individuals' self-reported social\nroles, achieving a F1 score of 0.73 for classifying 4 different\ninterruptibility preferences. We design and evaluate social role classification\nmodels based on spatio-temporal and application based features. We then\ncombined social role and interruptibility classifiers in a novel two-stage\ninterruptibility model that first infers individuals' social roles to finally\npredict individuals' interruptibility preferences. The two-stage\ninterruptibility model achieves a F1 score of 0.70. Finally, we examine the\ninfluence of multi-device data on social role and interruptibility\nclassification performances. Our findings break new grounds and provide new\ninsights for the design of future interruption management systems.",
    "descriptor": "\nComments: 24 pages, 9 figures, first submitted on August 2020\n",
    "authors": [
      "Christoph Anderson",
      "Judith Simone Heinisch",
      "Shohreh Deldari",
      "Flora D. Salim",
      "Sandra Ohly",
      "Klaus David",
      "Veljko Pejovic"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04265"
  },
  {
    "id": "arXiv:2106.04267",
    "title": "Supervised Machine Learning with Plausible Deniability",
    "abstract": "We study the question of how well machine learning (ML) models trained on a\ncertain data set provide privacy for the training data, or equivalently,\nwhether it is possible to reverse-engineer the training data from a given ML\nmodel. While this is easy to answer negatively in the most general case, it is\ninteresting to note that the protection extends over non-recoverability towards\nplausible deniability: Given an ML model $f$, we show that one can take a set\nof purely random training data, and from this define a suitable ``learning\nrule'' that will produce a ML model that is exactly $f$. Thus, any speculation\nabout which data has been used to train $f$ is deniable upon the claim that any\nother data could have led to the same results. We corroborate our theoretical\nfinding with practical examples, and open source implementations of how to find\nthe learning rules for a chosen set of raining data.",
    "descriptor": "",
    "authors": [
      "Stefan Rass",
      "Sandra K\u00f6nig",
      "Jasmin Wachter",
      "Manuel Egger",
      "Manuel Hobisch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04267"
  },
  {
    "id": "arXiv:2106.04269",
    "title": "HPRNet: Hierarchical Point Regression for Whole-Body Human Pose  Estimation",
    "abstract": "In this paper, we present a new bottom-up one-stage method for whole-body\npose estimation, which we name \"hierarchical point regression,\" or HPRNet for\nshort, referring to the network that implements this method. To handle the\nscale variance among different body parts, we build a hierarchical point\nrepresentation of body parts and jointly regress them. Unlike the existing\ntwo-stage methods, our method predicts whole-body pose in a constant time\nindependent of the number of people in an image. On the COCO WholeBody dataset,\nHPRNet significantly outperforms all previous bottom-up methods on the keypoint\ndetection of all whole-body parts (i.e. body, foot, face and hand); it also\nachieves state-of-the-art results in the face (75.4 AP) and hand (50.4 AP)\nkeypoint detection. Code and models are available at\nhttps://github.com/nerminsamet/HPRNet.git.",
    "descriptor": "\nComments: under review\n",
    "authors": [
      "Nermin Samet",
      "Emre Akbas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04269"
  },
  {
    "id": "arXiv:2106.04274",
    "title": "A Synchronized Reprojection-based Model for 3D Human Pose Estimation",
    "abstract": "3D human pose estimation is still a challenging problem despite the large\namount of work that has been done in this field. Generally, most methods\ndirectly use neural networks and ignore certain constraints (e.g., reprojection\nconstraints and joint angle and bone length constraints). This paper proposes a\nweakly supervised GAN-based model for 3D human pose estimation that considers\n3D information along with 2D information simultaneously, in which a\nreprojection network is employed to learn the mapping of the distribution from\n3D poses to 2D poses. In particular, we train the reprojection network and the\ngenerative adversarial network synchronously. Furthermore, inspired by the\ntypical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,\nwhich is added into the discriminator's input to impose joint angle and bone\nlength constraints. The experimental results on Human3.6M show that our method\noutperforms state-of-the-art methods by approximately 5.1\\%.",
    "descriptor": "",
    "authors": [
      "Yicheng Deng",
      "Cheng Sun",
      "Yongqi Sun",
      "Jiahui Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04274"
  },
  {
    "id": "arXiv:2106.04275",
    "title": "Raw Waveform Encoder with Multi-Scale Globally Attentive Locally  Recurrent Networks for End-to-End Speech Recognition",
    "abstract": "End-to-end speech recognition generally uses hand-engineered acoustic\nfeatures as input and excludes the feature extraction module from its joint\noptimization. To extract learnable and adaptive features and mitigate\ninformation loss, we propose a new encoder that adopts globally attentive\nlocally recurrent (GALR) networks and directly takes raw waveform as input. We\nobserve improved ASR performance and robustness by applying GALR on different\nwindow lengths to aggregate fine-grain temporal information into multi-scale\nacoustic features. Experiments are conducted on a benchmark dataset AISHELL-2\nand two large-scale Mandarin speech corpus of 5,000 hours and 21,000 hours.\nWith faster speed and comparable model size, our proposed multi-scale GALR\nwaveform encoder achieved consistent character error rate reductions (CERRs)\nfrom 7.9% to 28.1% relative over strong baselines, including Conformer and\nTDNN-Conformer. In particular, our approach demonstrated notable robustness\nthan the traditional handcrafted features and outperformed the baseline\nMFCC-based TDNN-Conformer model by a 15.2% CERR on a music-mixed real-world\nspeech test set.",
    "descriptor": "\nComments: Accepted in Interspeech 2021\n",
    "authors": [
      "Max W. Y. Lam",
      "Jun Wang",
      "Chao Weng",
      "Dan Su",
      "Dong Yu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04275"
  },
  {
    "id": "arXiv:2106.04277",
    "title": "Operating Tor Relays at Universities: Experiences and Considerations",
    "abstract": "In today's digital society, the Tor network has become an indispensable tool\nfor individuals to protect their privacy on the Internet. Operated by\nvolunteers, relay servers constitute the core component of Tor and are used to\ngeographically escape surveillance. It is therefore essential to have a large,\nyet diverse set of relays. In this work, we analyze the contribution of\neducational institutions to the Tor network and report on our experience of\noperating exit relays at a university. Taking Germany as an example (but\narguing that the global situation is similar), we carry out a quantitative\nstudy and find that universities contribute negligible amounts of relays and\nbandwidth. Since many universities all over the world have excellent conditions\nthat render them perfect places to host Tor (exit) relays, we encourage other\ninterested people and institutions to join. To this end, we discuss and resolve\ncommon concerns and provide lessons learned.",
    "descriptor": "",
    "authors": [
      "Christoph D\u00f6pmann",
      "Matthias Marx",
      "Hannes Federrath",
      "Florian Tschorsch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04277"
  },
  {
    "id": "arXiv:2106.04279",
    "title": "Staircase Attention for Recurrent Processing of Sequences",
    "abstract": "Attention mechanisms have become a standard tool for sequence modeling tasks,\nin particular by stacking self-attention layers over the entire input sequence\nas in the Transformer architecture. In this work we introduce a novel attention\nprocedure called staircase attention that, unlike self-attention, operates\nacross the sequence (in time) recurrently processing the input by adding\nanother step of processing. A step in the staircase comprises of backward\ntokens (encoding the sequence so far seen) and forward tokens (ingesting a new\npart of the sequence), or an extreme Ladder version with a forward step of zero\nthat simply repeats the Transformer on each step of the ladder, sharing the\nweights. We thus describe a family of such models that can trade off\nperformance and compute, by either increasing the amount of recurrence through\ntime, the amount of sequential processing via recurrence in depth, or both.\nStaircase attention is shown to be able to solve tasks that involve tracking\nthat conventional Transformers cannot, due to this recurrence. Further, it is\nshown to provide improved modeling power for the same size model (number of\nparameters) compared to self-attentive Transformers on large language modeling\nand dialogue tasks, yielding significant perplexity gains.",
    "descriptor": "",
    "authors": [
      "Da Ju",
      "Stephen Roller",
      "Sainbayar Sukhbaatar",
      "Jason Weston"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04279"
  },
  {
    "id": "arXiv:2106.04280",
    "title": "Optimizing a Binary Intelligent Reflecting Surface for OFDM  Communications under Mutual Coupling",
    "abstract": "An intelligent reflecting surface (IRS) can greatly improve the channel\nquality over a frequency-flat channel, if it is configured to reflect the\nincident signal as a beam towards the receiver. However, the fundamental\nlimitations of the IRS technology become apparent over practical\nfrequency-selective channels, where the same configuration must be used over\nthe entire bandwidth. In this paper, we consider a wideband orthogonal\nfrequency-division multiplexing (OFDM) system that is supported by a fairly\nrealistic IRS setup with two unbalanced states per element and also mutual\ncoupling. We describe the simulation setup considered in the IEEE Signal\nProcessing Cup 2021, propose a low-complexity solution for channel estimation\nand IRS configuration, and evaluate it on that setup.",
    "descriptor": "\nComments: 6 pages, 6 figures\n",
    "authors": [
      "Emil Bj\u00f6rnson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04280"
  },
  {
    "id": "arXiv:2106.04283",
    "title": "NWT: Towards natural audio-to-video generation with representation  learning",
    "abstract": "In this work we introduce NWT, an expressive speech-to-video model. Unlike\napproaches that use domain-specific intermediate representations such as pose\nkeypoints, NWT learns its own latent representations, with minimal assumptions\nabout the audio and video content. To this end, we propose a novel discrete\nvariational autoencoder with adversarial loss, dVAE-Adv, which learns a new\ndiscrete latent representation we call Memcodes. Memcodes are straightforward\nto implement, require no additional loss terms, are stable to train compared\nwith other approaches, and show evidence of interpretability. To predict on the\nMemcode space, we use an autoregressive encoder-decoder model conditioned on\naudio. Additionally, our model can control latent attributes in the generated\nvideo that are not annotated in the data. We train NWT on clips from HBO's Last\nWeek Tonight with John Oliver. NWT consistently scores above other approaches\nin Mean Opinion Score (MOS) on tests of overall video naturalness, facial\nnaturalness and expressiveness, and lipsync quality. This work sets a strong\nbaseline for generalized audio-to-video synthesis. Samples are available at\nhttps://next-week-tonight.github.io/NWT/.",
    "descriptor": "",
    "authors": [
      "Rayhane Mama",
      "Marc S. Tyndel",
      "Hashiam Kadhim",
      "Cole Clifford",
      "Ragavan Thurairatnam"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04283"
  },
  {
    "id": "arXiv:2106.04284",
    "title": "LLAMA: The Low Level Abstraction For Memory Access",
    "abstract": "The performance gap between CPU and memory widens continuously. Choosing the\nbest memory layout for each hardware architecture is increasingly important as\nmore and more programs become memory bound. For portable codes that run across\nheterogeneous hardware architectures, the choice of the memory layout for data\nstructures is therefore ideally decoupled from the rest of a program. This can\nbe accomplished via a zero-runtime-overhead abstraction layer, underneath which\nmemory layouts can be freely exchanged.\nWe present the C++ library LLAMA, which provides such a data structure\nabstraction layer with example implementations for multidimensional arrays of\nnested, structured data. LLAMA provides fully C++ compliant methods for\ndefining and switching custom memory layouts for user-defined data types.\nProviding two close-to-life examples, we show that the LLAMA-generated AoS\n(Array of Struct) and SoA (Struct of Array) layouts produce identical code with\nthe same performance characteristics as manually written data structures.\nLLAMA's layout-aware copy routines can significantly speed up transfer and\nreshuffling of data between layouts compared with naive element-wise copying.\nThe library is fully extensible with third-party allocators and allows users to\nsupport their own memory layouts with custom mappings.",
    "descriptor": "\nComments: 32 pages, 7 figures, 10 listings\n",
    "authors": [
      "Bernhard Manfred Gruber",
      "Guilherme Amadio",
      "Jakob Blomer",
      "Alexander Matthes",
      "Ren\u00e9 Widera",
      "Michael Bussmann"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2106.04284"
  },
  {
    "id": "arXiv:2106.04289",
    "title": "Morphing tree drawings using a small 3D grid",
    "abstract": "Morphing between two planar drawings using an extra dimension is relatively\nnew area of research. Also, the morphs with polynomially-bounded resolution has\ngained a lot of attention in recent years. Recent results use linear number of\nmorphing steps to morph between two planar straight-line grid drawings of an\n$n$-vertex tree while maintaining the resolution. In this paper, we bring\ntogether both the paradigms to develop an algorithm that morphs between two\nplanar straight-line grid drawings of an $n$-vertex tree $T$ in sublinear\nnumber of morphing steps such that each intermediate morphing step produces a\n$3D$ straight-line crossing free grid drawing of $T$ while maintaining the\nresolution. To be precise, we first devise an algorithm that takes\n$\\mathcal{O}(n)$ morphing steps but uses a grid of small volume. We then use\nthis algorithm to devise a better algorithm that takes $\\mathcal{O}(\\sqrt{n}\n\\log n)$ morphing steps. We also ensure that each intermediate drawing lies in\na $3D$ grid of polynomially-bounded volume. To the best of our knowledge, this\nis the first morphing algorithm that takes sublinear number of morphing steps\nwhile maintaining a polynomially-bounded grid size, and thus the resolution.",
    "descriptor": "\nComments: 47 pages, preliminary version\n",
    "authors": [
      "Elena Arseneva",
      "Rahul Gangopadhyay",
      "Aleksandra Istomina"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.04289"
  },
  {
    "id": "arXiv:2106.04292",
    "title": "Principled Hyperedge Prediction with Structural Spectral Features and  Neural Networks",
    "abstract": "Hypergraph offers a framework to depict the multilateral relationships in\nreal-world complex data. Predicting higher-order relationships, i.e hyperedge,\nbecomes a fundamental problem for the full understanding of complicated\ninteractions. The development of graph neural network (GNN) has greatly\nadvanced the analysis of ordinary graphs with pair-wise relations. However,\nthese methods could not be easily extended to the case of hypergraph. In this\npaper, we generalize the challenges of GNN in representing higher-order data in\nprinciple, which are edge- and node-level ambiguities. To overcome the\nchallenges, we present \\textbf{SNALS} that utilizes bipartite graph neural\nnetwork with structural features to collectively tackle the two ambiguity\nissues. SNALS captures the joint interactions of a hyperedge by its local\nenvironment, which is retrieved by collecting the spectrum information of their\nconnections. As a result, SNALS achieves nearly 30% performance increase\ncompared with most recent GNN-based models. In addition, we applied SNALS to\npredict genetic higher-order interactions on 3D genome organization data. SNALS\nshowed consistently high prediction accuracy across different chromosomes, and\ngenerated novel findings on 4-way gene interaction, which is further validated\nby existing literature.",
    "descriptor": "",
    "authors": [
      "Changlin Wan",
      "Muhan Zhang",
      "Wei Hao",
      "Sha Cao",
      "Pan Li",
      "Chi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04292"
  },
  {
    "id": "arXiv:2106.04293",
    "title": "Modeling Uplink Coverage Performance in Hybrid Satellite-Terrestrial  Networks",
    "abstract": "Once deemed a far-fetched vision, emerging deployments of massive satellite\nconstellations will soon offer true global coverage. When these constellations\noverlay the evolving terrestrial networks, a new hybrid continuum is formed\nwhich has the potential to provide uninterrupted coverage. In this paper, we\nprovide an analytic framework for the uplink coverage probability in hybrid\nsatellite-terrestrial networks. The framework extends well-developed\nterrestrial models that utilize tools from stochastic geometry by incorporating\nan additional layer that fits the emerging next generation\nsatellite-terrestrial networks. The paper captures the impact of both (i) the\nconstellation size, and (ii) the terrestrial base station density on the\ncoverage of the uplink traffic which is dominant in applications relying on\nwireless sensor networks, such as the Internet of Things. This framework\nprovides insights that guide the design of hybrid network infrastructures for a\ndesired quality of service.",
    "descriptor": "",
    "authors": [
      "Bassel Al Homssi",
      "Akram Al-Hourani"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04293"
  },
  {
    "id": "arXiv:2106.04298",
    "title": "Unsupervised Word Segmentation from Discrete Speech Units in  Low-Resource Settings",
    "abstract": "When documenting oral-languages, Unsupervised Word Segmentation (UWS) from\nspeech is a useful, yet challenging, task. It can be performed from phonetic\ntranscriptions, or in the absence of these, from the output of unsupervised\nspeech discretization models. These discretization models are trained using raw\nspeech only, producing discrete speech units which can be applied for\ndownstream (text-based) tasks. In this paper we compare five of these models:\nthree Bayesian and two neural approaches, with regards to the exploitability of\nthe produced units for UWS. Two UWS models are experimented with and we report\nresults for Finnish, Hungarian, Mboshi, Romanian and Russian in a low-resource\nsetting (using only 5k sentences). Our results suggest that neural models for\nspeech discretization are difficult to exploit in our setting, and that it\nmight be necessary to adapt them to limit sequence length. We obtain our best\nUWS results by using the SHMM and H-SHMM Bayesian models, which produce high\nquality, yet compressed, discrete representations of the input speech signal.",
    "descriptor": "",
    "authors": [
      "Marcely Zanon Boito",
      "Bolaji Yusuf",
      "Lucas Ondel",
      "Aline Villavicencio",
      "Laurent Besacier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04298"
  },
  {
    "id": "arXiv:2106.04300",
    "title": "A Unified Generative Framework for Aspect-Based Sentiment Analysis",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms,\ntheir corresponding sentiment polarities, and the opinion terms. There exist\nseven subtasks in ABSA. Most studies only focus on the subsets of these\nsubtasks, which leads to various complicated ABSA models while hard to solve\nthese subtasks in a unified framework. In this paper, we redefine every subtask\ntarget as a sequence mixed by pointer indexes and sentiment class indexes,\nwhich converts all ABSA subtasks into a unified generative formulation. Based\non the unified formulation, we exploit the pre-training sequence-to-sequence\nmodel BART to solve all ABSA subtasks in an end-to-end framework. Extensive\nexperiments on four ABSA datasets for seven subtasks demonstrate that our\nframework achieves substantial performance gain and provides a real unified\nend-to-end solution for the whole ABSA subtasks, which could benefit multiple\ntasks.",
    "descriptor": "\nComments: Accepted by ACL 2021 (long paper)\n",
    "authors": [
      "Hang Yan",
      "Junqi Dai",
      "Tuo ji",
      "Xipeng Qiu",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04300"
  },
  {
    "id": "arXiv:2106.04302",
    "title": "Obtaining Better Static Word Embeddings Using Contextual Embedding  Models",
    "abstract": "The advent of contextual word embeddings -- representations of words which\nincorporate semantic and syntactic information from their context -- has led to\ntremendous improvements on a wide variety of NLP tasks. However, recent\ncontextual models have prohibitively high computational cost in many use-cases\nand are often hard to interpret. In this work, we demonstrate that our proposed\ndistillation method, which is a simple extension of CBOW-based training, allows\nto significantly improve computational efficiency of NLP applications, while\noutperforming the quality of existing static embeddings trained from scratch as\nwell as those distilled from previously proposed methods. As a side-effect, our\napproach also allows a fair comparison of both contextual and static embeddings\nvia standard lexical evaluation tasks.",
    "descriptor": "\nComments: ACL 2021 accept\n",
    "authors": [
      "Prakhar Gupta",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04302"
  },
  {
    "id": "arXiv:2106.04306",
    "title": "Residual Feedback Learning for Contact-Rich Manipulation Tasks with  Uncertainty",
    "abstract": "While classic control theory offers state of the art solutions in many\nproblem scenarios, it is often desired to improve beyond the structure of such\nsolutions and surpass their limitations. To this end, \\emph{\\gls{rpl}} offers a\nformulation to improve existing controllers with reinforcement learning (RL) by\nlearning an additive \"residual\" to the output of a given controller. However,\nthe applicability of such an approach highly depends on the structure of the\ncontroller. Often, internal feedback signals of the controller limit an RL\nalgorithm to adequately change the policy and, hence, learn the task. We\npropose a new formulation that addresses these limitations by also modifying\nthe feedback signals to the controller with an RL policy and show superior\nperformance of our approach on a contact-rich peg-insertion task under position\nand orientation uncertainty. In addition, we use a recent impedance control\narchitecture as control framework and show the difficulties of standard RPL.\nFurthermore, we introduce an adaptive curriculum for the given task to\ngradually increase the task difficulty in terms of position and orientation\nuncertainty. A video showing the results can be found at\nhttps://youtu.be/SAZm_Krze7U .",
    "descriptor": "",
    "authors": [
      "Alireza Ranjbar",
      "Ngo Anh Vien",
      "Hanna Ziesche",
      "Joschka Boedecker",
      "Gerhard Neumann"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.04306"
  },
  {
    "id": "arXiv:2106.04310",
    "title": "Principled Data Completion of Network Constraints for Day Ahead Auctions  in Power Markets",
    "abstract": "Network constraints play a key role in the price finding mechanism for\nEuropean Power Markets, but historical data is very sparse and usually\ninsufficient for many quantitative applications. We reconstruct the constraints\ndata, known as the Power Transmission Distribution Factors (PTDFs) and\nRemaining Available Margins (RAMs), by first recovering the underlying time\ndependent signals known as the Generation Shift Keys (GSKs) and Phase Angles\n(PAs), and the electricity grid characteristics, via a mathematical\noptimisation problem. This is solved by exploiting marginal convexity in\ncertain subspaces via alternating minimisation. The GSKs and PAs are then\nmapped to the PTDFs and RAMs, using the grid structure. Our reconstruction\nachieves good in-sample and out-of-sample relative errors for the PTDFs and\nRAMs. We further show that our model outperforms the naive approach, and that\nthe reconstructed GSKs and PAs recover specific structure.",
    "descriptor": "",
    "authors": [
      "Ioan Alexandru Puiu",
      "Raphael Andreas Hauser"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.04310"
  },
  {
    "id": "arXiv:2106.04311",
    "title": "Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time  Curvatures",
    "abstract": "Knowledge Graph (KG) completion has been excessively studied with a massive\nnumber of models proposed for the Link Prediction (LP) task. The main\nlimitation of such models is their insensitivity to time. Indeed, the temporal\naspect of stored facts is often ignored. To this end, more and more works\nconsider time as a parameter to complete KGs. In this paper, we first\ndemonstrate that, by simply increasing the number of negative samples, the\nrecent AttH model can achieve competitive or even better performance than the\nstate-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further\npropose Hercules, a time-aware extension of AttH model, which defines the\ncurvature of a Riemannian manifold as the product of both relation and time.\nOur experiments show that both Hercules and AttH achieve competitive or new\nstate-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore,\none should raise awareness when learning TKGs representations to identify\nwhether time truly boosts performances.",
    "descriptor": "\nComments: Accepted to Findings of ACL 2021 (Long Paper)\n",
    "authors": [
      "Sebastien Montella",
      "Lina Rojas-Barahona",
      "Johannes Heinecke"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04311"
  },
  {
    "id": "arXiv:2106.04314",
    "title": "A Perspective on Time towards Wireless 6G",
    "abstract": "With the advent of 5G technology, the notion of latency got a prominent role\nin wireless connectivity, serving as a proxy term for addressing the\nrequirements for real-time communication. As wireless systems evolve towards\n6G, the ambition to immerse the digital into the physical reality will\nincrease. Besides making the real-time requirements more stringent, this\nimmersion will bring the notions of time, simultaneity, presence, and causality\nto a new level of complexity. A growing body of research points out that\nlatency is insufficient to parameterize all real-time requirements. Notably,\none such requirement that received a significant attention is information\nfreshness, defined through the Age of Information (AoI) and its derivatives.\nThe objective of this article is to investigate the general notion of timing in\nwireless communication systems and networks and its relation to effective\ninformation generation, processing, transmission, and reconstruction at the\nsenders and receivers. We establish a general statistical framework of timing\nrequirements in wireless communication systems, which subsumes both latency and\nAoI. The framework is made by associating a timing component with the two basic\nstatistical operations, decision and estimation. We first use the framework to\npresent a representative sample of the existing works that deal with timing in\nwireless communication. Next, it is shown how the framework can be used with\ndifferent communication models of increasing complexity, starting from the\nbasic Shannon one-way communication model and arriving to communication models\nfor consensus, distributed learning, and inference. Overall, this paper fills\nan important gap in the literature by providing a systematic treatment of\nvarious timing measures in wireless communication and sets the basis for design\nand optimization for the next-generation real-time systems.",
    "descriptor": "",
    "authors": [
      "Petar Popovski",
      "Federico Chiariotti",
      "Kaibin Huang",
      "Anders E. Kal\u00f8r",
      "Marios Kountouris",
      "Nikolaos Pappas",
      "Beatriz Soret"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04314"
  },
  {
    "id": "arXiv:2106.04315",
    "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
    "abstract": "For robots to work alongside humans and perform in unstructured environments,\nthey must learn new motion skills and adapt them to unseen situations on the\nfly. This demands learning models that capture relevant motion patterns, while\noffering enough flexibility to adapt the encoded skills to new requirements,\nsuch as dynamic obstacle avoidance. We introduce a Riemannian manifold\nperspective on this problem, and propose to learn a Riemannian manifold from\nhuman demonstrations on which geodesics are natural motion skills. We realize\nthis with a variational autoencoder (VAE) over the space of position and\norientations of the robot end-effector. Geodesic motion skills let a robot plan\nmovements from and to arbitrary points on the data manifold. They also provide\na straightforward method to avoid obstacles by redefining the ambient metric in\nan online fashion. Moreover, geodesics naturally exploit the manifold resulting\nfrom multiple--mode tasks to design motions that were not explicitly\ndemonstrated previously. We test our learning framework using a 7-DoF robotic\nmanipulator, where the robot satisfactorily learns and reproduces realistic\nskills featuring elaborated motion patterns, avoids previously unseen\nobstacles, and generates novel movements in multiple-mode settings.",
    "descriptor": "",
    "authors": [
      "Hadi Beik-Mohammadi",
      "S\u00f8ren Hauberg",
      "Georgios Arvanitidis",
      "Gerhard Neumann",
      "Leonel Rozo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04315"
  },
  {
    "id": "arXiv:2106.04316",
    "title": "Exploration and preference satisfaction trade-off in reward-free  learning",
    "abstract": "Biological agents have meaningful interactions with their environment despite\nthe absence of a reward signal. In such instances, the agent can learn\npreferred modes of behaviour that lead to predictable states -- necessary for\nsurvival. In this paper, we pursue the notion that this learnt behaviour can be\na consequence of reward-free preference learning that ensures an appropriate\ntrade-off between exploration and preference satisfaction. For this, we\nintroduce a model-based Bayesian agent equipped with a preference learning\nmechanism (pepper) using conjugate priors. These conjugate priors are used to\naugment the expected free energy planner for learning preferences over states\n(or outcomes) across time. Importantly, our approach enables the agent to learn\npreferences that encourage adaptive behaviour at test time. We illustrate this\nin the OpenAI Gym FrozenLake and the 3D mini-world environments -- with and\nwithout volatility. Given a constant environment, these agents learn confident\n(i.e., precise) preferences and act to satisfy them. Conversely, in a volatile\nsetting, perpetual preference uncertainty maintains exploratory behaviour. Our\nexperiments suggest that learnable (reward-free) preferences entail a trade-off\nbetween exploration and preference satisfaction. Pepper offers a\nstraightforward framework suitable for designing adaptive agents when reward\nfunctions cannot be predefined as in real environments.",
    "descriptor": "",
    "authors": [
      "Noor Sajid",
      "Panagiotis Tigas",
      "Alexey Zakharov",
      "Zafeirios Fountas",
      "Karl Friston"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.04316"
  },
  {
    "id": "arXiv:2106.04319",
    "title": "Breaking the Limits of Message Passing Graph Neural Networks",
    "abstract": "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear\ncomplexity with respect to the number of nodes when applied to sparse graphs,\nthey have been widely implemented and still raise a lot of interest even though\ntheir theoretical expressive power is limited to the first order\nWeisfeiler-Lehman test (1-WL). In this paper, we show that if the graph\nconvolution supports are designed in spectral-domain by a non-linear custom\nfunction of eigenvalues and masked with an arbitrary large receptive field, the\nMPNN is theoretically more powerful than the 1-WL test and experimentally as\npowerful as a 3-WL existing models, while remaining spatially localized.\nMoreover, by designing custom filter functions, outputs can have various\nfrequency components that allow the convolution process to learn different\nrelationships between a given input graph signal and its associated properties.\nSo far, the best 3-WL equivalent graph neural networks have a computational\ncomplexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$,\nconsider non-local update mechanism and do not provide the spectral richness of\noutput profile. The proposed method overcomes all these aforementioned problems\nand reaches state-of-the-art results in many downstream tasks.",
    "descriptor": "\nComments: 18 pages, 6 figures\n",
    "authors": [
      "Muhammet Balcilar",
      "Pierre H\u00e9roux",
      "Benoit Ga\u00fcz\u00e8re",
      "Pascal Vasseur",
      "S\u00e9bastien Adam",
      "Paul Honeine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04319"
  },
  {
    "id": "arXiv:2106.04324",
    "title": "Contrastive Representation Learning for Hand Shape Estimation",
    "abstract": "This work presents improvements in monocular hand shape estimation by\nbuilding on top of recent advances in unsupervised learning. We extend momentum\ncontrastive learning and contribute a structured collection of hand images,\nwell suited for visual representation learning, which we call HanCo. We find\nthat the representation learned by established contrastive learning methods can\nbe improved significantly by exploiting advanced background removal techniques\nand multi-view information. These allow us to generate more diverse instance\npairs than those obtained by augmentations commonly used in exemplar based\napproaches. Our method leads to a more suitable representation for the hand\nshape estimation task and shows a 4.7% reduction in mesh error and a 3.6%\nimprovement in F-score compared to an ImageNet pretrained baseline. We make our\nbenchmark dataset publicly available, to encourage further research into this\ndirection.",
    "descriptor": "",
    "authors": [
      "Christian Zimmermann",
      "Max Argus",
      "Thomas Brox"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04324"
  },
  {
    "id": "arXiv:2106.04332",
    "title": "Progressive Spatio-Temporal Bilinear Network with Monte Carlo Dropout  for Landmark-based Facial Expression Recognition with Uncertainty Estimation",
    "abstract": "Deep neural networks have been widely used for feature learning in facial\nexpression recognition systems. However, small datasets and large intra-class\nvariability can lead to overfitting. In this paper, we propose a method which\nlearns an optimized compact network topology for real-time facial expression\nrecognition utilizing localized facial landmark features. Our method employs a\nspatio-temporal bilinear layer as backbone to capture the motion of facial\nlandmarks during the execution of a facial expression effectively. Besides, it\ntakes advantage of Monte Carlo Dropout to capture the model's uncertainty which\nis of great importance to analyze and treat uncertain cases. The performance of\nour method is evaluated on three widely used datasets and it is comparable to\nthat of video-based state-of-the-art methods while it has much less complexity.",
    "descriptor": "\nComments: 6 pages, 3 figures, 3 tables\n",
    "authors": [
      "Negar Heidari",
      "Alexandros Iosifidis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Complexity (cs.CC)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04332"
  },
  {
    "id": "arXiv:2106.04335",
    "title": "Reinforced Few-Shot Acquisition Function Learning for Bayesian  Optimization",
    "abstract": "Bayesian optimization (BO) conventionally relies on handcrafted acquisition\nfunctions (AFs) to sequentially determine the sample points. However, it has\nbeen widely observed in practice that the best-performing AF in terms of regret\ncan vary significantly under different types of black-box functions. It has\nremained a challenge to design one AF that can attain the best performance over\na wide variety of black-box functions. This paper aims to attack this challenge\nthrough the perspective of reinforced few-shot AF learning (FSAF).\nSpecifically, we first connect the notion of AFs with Q-functions and view a\ndeep Q-network (DQN) as a surrogate differentiable AF. While it serves as a\nnatural idea to combine DQN and an existing few-shot learning method, we\nidentify that such a direct combination does not perform well due to severe\noverfitting, which is particularly critical in BO due to the need of a\nversatile sampling policy. To address this, we present a Bayesian variant of\nDQN with the following three features: (i) It learns a distribution of\nQ-networks as AFs based on the Kullback-Leibler regularization framework. This\ninherently provides the uncertainty required in sampling for BO and mitigates\noverfitting. (ii) For the prior of the Bayesian DQN, we propose to use a demo\npolicy induced by an off-the-shelf AF for better training stability. (iii) On\nthe meta-level, we leverage the meta-loss of Bayesian model-agnostic\nmeta-learning, which serves as a natural companion to the proposed FSAF.\nMoreover, with the proper design of the Q-networks, FSAF is general-purpose in\nthat it is agnostic to the dimension and the cardinality of the input domain.\nThrough extensive experiments, we demonstrate that the FSAF achieves comparable\nor better regrets than the state-of-the-art benchmarks on a wide variety of\nsynthetic and real-world test functions.",
    "descriptor": "\nComments: 21 pages, 8 figures\n",
    "authors": [
      "Bing-Jing Hsieh",
      "Ping-Chun Hsieh",
      "Xi Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04335"
  },
  {
    "id": "arXiv:2106.04336",
    "title": "Efficient Online Learning for Dynamic k-Clustering",
    "abstract": "We study dynamic clustering problems from the perspective of online learning.\nWe consider an online learning problem, called \\textit{Dynamic $k$-Clustering},\nin which $k$ centers are maintained in a metric space over time (centers may\nchange positions) such as a dynamically changing set of $r$ clients is served\nin the best possible way. The connection cost at round $t$ is given by the\n\\textit{$p$-norm} of the vector consisting of the distance of each client to\nits closest center at round $t$, for some $p\\geq 1$ or $p = \\infty$. We present\na \\textit{$\\Theta\\left( \\min(k,r) \\right)$-regret} polynomial-time online\nlearning algorithm and show that, under some well-established computational\ncomplexity conjectures, \\textit{constant-regret} cannot be achieved in\npolynomial-time. In addition to the efficient solution of Dynamic\n$k$-Clustering, our work contributes to the long line of research on\ncombinatorial online learning.",
    "descriptor": "",
    "authors": [
      "Dimitris Fotakis",
      "Georgios Piliouras",
      "Stratis Skoulakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04336"
  },
  {
    "id": "arXiv:2106.04337",
    "title": "A novel partially-decoupled translational parallel manipulator with  symbolic kinematics, singularity identification and workspace determination",
    "abstract": "This paper presents a novel three-degree-of-freedom (3-DOF) translational\nparallel manipulator (TPM) by using a topological design method of parallel\nmechanism (PM) based on position and orientation characteristic (POC)\nequations. The proposed PM is only composed of lower-mobility joints and\nactuated prismatic joints, together with the investigations on three kinematic\nissues of importance. The first aspect pertains to geometric modeling of the\nTPM in connection with its topological characteristics, such as the POC, degree\nof freedom and coupling degree, from which its symbolic direct kinematic\nsolutions are readily obtained. Moreover, the decoupled properties of\ninput-output motions are directly evaluated without Jacobian analysis.\nSequentially, based upon the inverse kinematics, the singular configurations of\nthe TPM are identified, wherein the singular surfaces are visualized by means\nof a Gr{\\\"o}bner based elimination operation. Finally, the workspace of the TPM\nis evaluated with a geometric approach. This 3-DOF TPM features less joints and\nlinks compared with the well-known Delta robot, which reduces the structural\ncomplexity. Its symbolic direct kinematics and partially-decoupled property\nwill ease path planning and dynamic analysis. The TPM can be used for\nmanufacturing large work pieces.",
    "descriptor": "",
    "authors": [
      "Huiping Shen",
      "Yinan Zhao",
      "Ju Li",
      "Guanglei Wu",
      "Damien Chablat"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04337"
  },
  {
    "id": "arXiv:2106.04340",
    "title": "Interpolation and Model Checking for Nonlinear Arithmetic",
    "abstract": "We present a new model-based interpolation procedure for satisfiability\nmodulo theories (SMT). The procedure uses a new mode of interaction with the\nSMT solver that we call solving modulo a model. This either extends a given\npartial model into a full model for a set of assertions or returns an\nexplanation (a model interpolant) when no solution exists. This mode of\ninteraction fits well into the model-constructing satisfiability (MCSAT)\nframework of SMT. We use it to develop an interpolation procedure for any\nMCSAT-supported theory. In particular, this method leads to an effective\ninterpolation procedure for nonlinear real arithmetic. We evaluate the new\nprocedure by integrating it into a model checker and comparing it with\nstate-of-art model-checking tools for nonlinear arithmetic.",
    "descriptor": "\nComments: To be published in CAV 2021\n",
    "authors": [
      "Dejan Jovanovi\u0107",
      "Bruno Dutertre"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2106.04340"
  },
  {
    "id": "arXiv:2106.04341",
    "title": "Revealing drivers and risks for power grid frequency stability with  explainable AI",
    "abstract": "Stable operation of the electrical power system requires the power grid\nfrequency to stay within strict operational limits. With millions of consumers\nand thousands of generators connected to a power grid, detailed human-build\nmodels can no longer capture the full dynamics of this complex system. Modern\nmachine learning algorithms provide a powerful alternative for system modelling\nand prediction, but the intrinsic black-box character of many models impedes\nscientific insights and poses severe security risks. Here, we show how\neXplainable AI (XAI) alleviates these problems by revealing critical\ndependencies and influences on the power grid frequency. We accurately predict\nfrequency stability indicators (such as RoCoF and Nadir) for three major\nEuropean synchronous areas and identify key features that determine the power\ngrid stability. Load ramps, specific generation ramps but also prices and\nforecast errors are central to understand and stabilize the power grid.",
    "descriptor": "\nComments: 26 pages, 19 figures\n",
    "authors": [
      "Johannes Kruse",
      "Benjamin Sch\u00e4fer",
      "Dirk Witthaut"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2106.04341"
  },
  {
    "id": "arXiv:2106.04345",
    "title": "An Intelligent Hybrid Model for Identity Document Classification",
    "abstract": "Digitization, i.e., the process of converting information into a digital\nformat, may provide various opportunities (e.g., increase in productivity,\ndisaster recovery, and environmentally friendly solutions) and challenges for\nbusinesses. In this context, one of the main challenges would be to accurately\nclassify numerous scanned documents uploaded every day by customers as usual\nbusiness processes. For example, processes in banking (e.g., applying for\nloans) or the Government Registry of BDM (Births, Deaths, and Marriages)\napplications may involve uploading several documents such as a driver's license\nand passport. There are not many studies available to address the challenge as\nan application of image classification. Although some studies are available\nwhich used various methods, a more accurate model is still required. The\ncurrent study has proposed a robust fusion model to define the type of identity\ndocuments accurately. The proposed approach is based on two different methods\nin which images are classified based on their visual features and text\nfeatures. A novel model based on statistics and regression has been proposed to\ncalculate the confidence level for the feature-based classifier. A fuzzy-mean\nfusion model has been proposed to combine the classifier results based on their\nconfidence score. The proposed approach has been implemented using Python and\nexperimentally validated on synthetic and real-world datasets. The performance\nof the proposed model is evaluated using the Receiver Operating Characteristic\n(ROC) curve analysis.",
    "descriptor": "",
    "authors": [
      "Nouna Khandan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.04345"
  },
  {
    "id": "arXiv:2106.04350",
    "title": "Nonsmooth Implicit Differentiation for Machine Learning and Optimization",
    "abstract": "In view of training increasingly complex learning architectures, we establish\na nonsmooth implicit function theorem with an operational calculus. Our result\napplies to most practical problems (i.e., definable problems) provided that a\nnonsmooth form of the classical invertibility condition is fulfilled. This\napproach allows for formal subdifferentiation: for instance, replacing\nderivatives by Clarke Jacobians in the usual differentiation formulas is fully\njustified for a wide class of nonsmooth problems. Moreover this calculus is\nentirely compatible with algorithmic differentiation (e.g., backpropagation).\nWe provide several applications such as training deep equilibrium networks,\ntraining neural nets with conic optimization layers, or hyperparameter-tuning\nfor nonsmooth Lasso-type models. To show the sharpness of our assumptions, we\npresent numerical experiments showcasing the extremely pathological gradient\ndynamics one can encounter when applying implicit algorithmic differentiation\nwithout any hypothesis.",
    "descriptor": "",
    "authors": [
      "J\u00e9r\u00f4me Bolte",
      "Tam Le",
      "Edouard Pauwels",
      "Antonio Silveti-Falls"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04350"
  },
  {
    "id": "arXiv:2106.04351",
    "title": "Si microring resonator crossbar array for on-chip inference and training  of optical neural network",
    "abstract": "Deep learning is one of the most advancing technologies in various fields.\nFacing the limits of the current electronics platform, optical neural networks\n(ONNs) based on Si programmable photonic integrated circuits (PICs) have\nattracted considerable attention as a novel deep learning scheme with\noptical-domain matrix-vector multiplication (MVM). However, most of the\nproposed Si programmable PICs for ONNs have several drawbacks such as low\nscalability, high power consumption, and lack of frameworks for training. To\naddress these issues, we have proposed a microring resonator (MRR) crossbar\narray as a Si programmable PIC for an ONN. In this article, we present a\nprototype of a fully integrated 4 ${\\rm \\times}$ 4 MRR crossbar array and\ndemonstrated a simple MVM and classification task. Moreover, we propose on-chip\nbackpropagation using the transpose matrix operation of the MRR crossbar array,\nenabling the on-chip training of the ONN. The proposed ONN scheme can establish\na scalable, power-efficient deep learning accelerator for applications in both\ninference and training tasks.",
    "descriptor": "\nComments: 18 pages, 5 figures for main manuscript. 20 pages, 16 figures for supplementary information\n",
    "authors": [
      "Shuhei Ohno",
      "Kasidit Toprasertpong",
      "Shinichi Takagi",
      "Mitsuru Takenaka"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2106.04351"
  },
  {
    "id": "arXiv:2106.04363",
    "title": "Stability of Self-Configuring Large Multiport Interferometers",
    "abstract": "Realistic multiport interferometers (beamsplitter meshes) are sensitive to\ncomponent imperfections, and this sensitivity increases with size.\nSelf-configuration techniques can be employed to correct these imperfections,\nbut not all techniques are equal. This paper highlights the importance of\nalgorithmic stability in self-configuration. Naive approaches based on\nsequentially setting matrix elements are unstable and perform poorly for large\nmeshes, while techniques based on power ratios perform well in all cases, even\nin the presence of large errors. Based on this insight, we propose a\nself-configuration scheme for triangular meshes that requires only external\ndetectors and works without prior knowledge of the component imperfections.\nThis scheme extends to the rectangular mesh by adding a single array of\ndetectors along the diagonal.",
    "descriptor": "\nComments: 13 pages, 12 figures\n",
    "authors": [
      "Ryan Hamerly",
      "Saumil Bandyopadhyay",
      "Dirk Englund"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2106.04363"
  },
  {
    "id": "arXiv:2106.04364",
    "title": "countBF: A General-purpose High Accuracy and Space Efficient Counting  Bloom Filter",
    "abstract": "Bloom Filter is a probabilistic data structure for the membership query, and\nit has been intensely experimented in various fields to reduce memory\nconsumption and enhance a system's performance. Bloom Filter is classified into\ntwo key categories: counting Bloom Filter (CBF), and non-counting Bloom Filter.\nCBF has a higher false positive probability than standard Bloom Filter (SBF),\ni.e., CBF uses a higher memory footprint than SBF. But CBF can address the\nissue of the false negative probability. Notably, SBF is also false negative\nfree, but it cannot support delete operations like CBF. To address these\nissues, we present a novel counting Bloom Filter based on SBF and 2D Bloom\nFilter, called countBF. countBF uses a modified murmur hash function to enhance\nits various requirements, which is experimentally evaluated. Our experimental\nresults show that countBF uses $1.96\\times$ and $7.85\\times$ less memory than\nSBF and CBF respectively, while preserving lower false positive probability and\nexecution time than both SBF and CBF. The overall accuracy of countBF is\n$99.999921$, and it proves the superiority of countBF over SBF and CBF. Also,\nwe compare with other state-of-the-art counting Bloom Filters.",
    "descriptor": "\nComments: Submitted to IEEE Conference for possible publication\n",
    "authors": [
      "Sabuzima Nayak",
      "Ripon Patgiri"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04364"
  },
  {
    "id": "arXiv:2106.04365",
    "title": "robustBF: A High Accuracy and Memory Efficient 2D Bloom Filter",
    "abstract": "Bloom Filter is an important probabilistic data structure to reduce memory\nconsumption for membership filters. It is applied in diverse domains such as\nComputer Networking, Network Security and Privacy, IoT, Edge Computing, Cloud\nComputing, Big Data, and Biometrics. But Bloom Filter has an issue of the false\npositive probability. To address this issue, we propose a novel robust Bloom\nFilter, robustBF for short. robustBF is a 2D Bloom Filter, capable of filtering\nmillions of data with high accuracy without compromising the performance. Our\nproposed system is presented in two-fold. Firstly, we modify the murmur hash\nfunction, and test all modified hash functions for improvements and select the\nbest-modified hash function experimentally. Secondly, we embed the modified\nhash functions in 2D Bloom Filter. Our experimental results show that robustBF\nis better than standard Bloom Filter and counting Bloom Filter in every aspect.\nrobustBF exhibits nearly zero false positive probability with more than\n$10\\times$ and $44\\times$ lower memory consumption than standard Bloom filter\nand counting Bloom Filter, respectively.",
    "descriptor": "\nComments: Submitted to IEEE conference\n",
    "authors": [
      "Ripon Patgiri"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04365"
  },
  {
    "id": "arXiv:2106.04372",
    "title": "Segmentation and ABCD rule extraction for skin tumors classification",
    "abstract": "During the last years, computer vision-based diagnosis systems have been\nwidely used in several hospitals and dermatology clinics, aiming at the early\ndetection of malignant melanoma tumor, which is among the most frequent types\nof skin cancer. In this work, we present an automated diagnosis system based on\nthe ABCD rule used in clinical diagnosis in order to discriminate benign from\nmalignant skin lesions. First, to reduce the influence of small structures, a\npreprocessing step based on morphological and fast marching schemes is used. In\nthe second step, an unsupervised approach for lesion segmentation is proposed.\nIterative thresholding is applied to initialize level set automatically. As the\ndetection of an automated border is an important step for the correctness of\nsubsequent phases in the computerized melanoma recognition systems, we compare\nits accuracy with growcut and mean shift algorithms, and discuss how these\nresults may influence in the following steps: the feature extraction and the\nfinal lesion classification. Relying on visual diagnosis four features:\nAsymmetry (A), Border (B), Color (C) and Diversity (D) are computed and used to\nconstruct a classification module based on artificial neural network for the\nrecognition of malignant melanoma. This framework has been tested on a\ndermoscopic database [16] of 320 images. The classification results show an\nincreasing true detection rate and a decreasing false positive rate.",
    "descriptor": "",
    "authors": [
      "Mahammed Messadi",
      "Hocine Cherifi",
      "Abdelhafid Bessaid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04372"
  },
  {
    "id": "arXiv:2106.04376",
    "title": "ConSTR: A Contextual Search Term Recommender",
    "abstract": "In this demo paper, we present ConSTR, a novel Contextual Search Term\nRecommender that utilises the user's interaction context for search term\nrecommendation and literature retrieval. ConSTR integrates a two-layered\nrecommendation interface: the first layer suggests terms with respect to a\nuser's current search term, and the second layer suggests terms based on the\nusers' previous search activities (interaction context). For the demonstration,\nConSTR is built on the arXiv, an academic repository consisting of 1.8 million\ndocuments.",
    "descriptor": "\nComments: 2 pages, 2 figures, accepted demo paper at JCDL 2021\n",
    "authors": [
      "Thomas Kr\u00e4mer",
      "Zeljko Carevic",
      "Dwaipayan Roy",
      "Claus-Peter Klas",
      "Philipp Mayr"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04376"
  },
  {
    "id": "arXiv:2106.04378",
    "title": "Adaptive Machine Unlearning",
    "abstract": "Data deletion algorithms aim to remove the influence of deleted data points\nfrom trained models at a cheaper computational cost than fully retraining those\nmodels. However, for sequences of deletions, most prior work in the non-convex\nsetting gives valid guarantees only for sequences that are chosen independently\nof the models that are published. If people choose to delete their data as a\nfunction of the published models (because they don't like what the models\nreveal about them, for example), then the update sequence is adaptive. In this\npaper, we give a general reduction from deletion guarantees against adaptive\nsequences to deletion guarantees against non-adaptive sequences, using\ndifferential privacy and its connection to max information. Combined with ideas\nfrom prior work which give guarantees for non-adaptive deletion sequences, this\nleads to extremely flexible algorithms able to handle arbitrary model classes\nand training methodologies, giving strong provable deletion guarantees for\nadaptive deletion sequences. We show in theory how prior work for non-convex\nmodels fails against adaptive deletion sequences, and use this intuition to\ndesign a practical attack against the SISA algorithm of Bourtoule et al. [2021]\non CIFAR-10, MNIST, Fashion-MNIST.",
    "descriptor": "",
    "authors": [
      "Varun Gupta",
      "Christopher Jung",
      "Seth Neel",
      "Aaron Roth",
      "Saeed Sharifi-Malvajerdi",
      "Chris Waites"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04378"
  },
  {
    "id": "arXiv:2106.04379",
    "title": "Learning Markov State Abstractions for Deep Reinforcement Learning",
    "abstract": "The fundamental assumption of reinforcement learning in Markov decision\nprocesses (MDPs) is that the relevant decision process is, in fact, Markov.\nHowever, when MDPs have rich observations, agents typically learn by way of an\nabstract state representation, and such representations are not guaranteed to\npreserve the Markov property. We introduce a novel set of conditions and prove\nthat they are sufficient for learning a Markov abstract state representation.\nWe then describe a practical training procedure that combines inverse model\nestimation and temporal contrastive learning to learn an abstraction that\napproximately satisfies these conditions. Our novel training objective is\ncompatible with both online and offline training: it does not require a reward\nsignal, but agents can capitalize on reward information when available. We\nempirically evaluate our approach on a visual gridworld domain and a set of\ncontinuous control benchmarks. Our approach learns representations that capture\nthe underlying structure of the domain and lead to improved sample efficiency\nover state-of-the-art deep reinforcement learning with visual features -- often\nmatching or exceeding the performance achieved with hand-designed compact state\ninformation.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Cameron Allen",
      "Neev Parikh",
      "Omer Gottesman",
      "George Konidaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04379"
  },
  {
    "id": "arXiv:2106.04382",
    "title": "Proof methods for robust low-rank matrix recovery",
    "abstract": "Low-rank matrix recovery problems arise naturally as mathematical\nformulations of various inverse problems, such as matrix completion, blind\ndeconvolution, and phase retrieval. Over the last two decades, a number of\nworks have rigorously analyzed the reconstruction performance for such\nscenarios, giving rise to a rather general understanding of the potential and\nthe limitations of low-rank matrix models in sensing problems. In this article,\nwe compare the two main proof techniques that have been paving the way to a\nrigorous analysis, discuss their potential and limitations, and survey their\nsuccessful applications. On the one hand, we review approaches based on descent\ncone analysis, showing that they often lead to strong guarantees even in the\npresence of adversarial noise, but face limitations when it comes to structured\nobservations. On the other hand, we discuss techniques using approximate dual\ncertificates and the golfing scheme, which are often better suited to deal with\npractical measurement structures, but sometimes lead to weaker guarantees.\nLastly, we review recent progress towards analyzing descent cones also for\nstructured scenarios -- exploiting the idea of splitting the cones into\nmultiple parts that are analyzed via different techniques.",
    "descriptor": "\nComments: 39 pages, 6 figures\n",
    "authors": [
      "Tim Fuchs",
      "David Gross",
      "Peter Jung",
      "Felix Krahmer",
      "Richard Kueng",
      "Dominik St\u00f6ger"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04382"
  },
  {
    "id": "arXiv:2106.04384",
    "title": "Incentive Mechanism for Privacy-Preserving Federated Learning",
    "abstract": "Federated learning (FL) is an emerging paradigm for machine learning, in\nwhich data owners can collaboratively train a model by sharing gradients\ninstead of their raw data. Two fundamental research problems in FL are\nincentive mechanism and privacy protection. The former focuses on how to\nincentivize data owners to participate in FL. The latter studies how to protect\ndata owners' privacy while maintaining high utility of trained models. However,\nincentive mechanism and privacy protection in FL have been studied separately\nand no work solves both problems at the same time. In this work, we address the\ntwo problems simultaneously by an FL-Market that incentivizes data owners'\nparticipation by providing appropriate payments and privacy protection.\nFL-Market enables data owners to obtain compensation according to their privacy\nloss quantified by local differential privacy (LDP). Our insight is that, by\nmeeting data owners' personalized privacy preferences and providing appropriate\npayments, we can (1) incentivize privacy risk-tolerant data owners to set\nlarger privacy parameters (i.e., gradients with less noise) and (2) provide\npreferred privacy protection for privacy risk-averse data owners. To achieve\nthis, we design a personalized LDP-based FL framework with a deep\nlearning-empowered auction mechanism for incentivizing trading gradients with\nless noise and optimal aggregation mechanisms for model updates. Our\nexperiments verify the effectiveness of the proposed framework and mechanisms.",
    "descriptor": "",
    "authors": [
      "Shuyuan Zheng",
      "Yang Cao",
      "Masatoshi Yoshikawa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.04384"
  },
  {
    "id": "arXiv:2106.04385",
    "title": "Property-Aware Robot Object Manipulation: a Generative Approach",
    "abstract": "When transporting an object, we unconsciously adapt our movement to its\nproperties, for instance by slowing down when the item is fragile. The most\nrelevant features of an object are immediately revealed to a human observer by\nthe way the handling occurs, without any need for verbal description. It would\ngreatly facilitate collaboration to enable humanoid robots to perform movements\nthat convey similar intuitive cues to the observers. In this work, we focus on\nhow to generate robot motion adapted to the hidden properties of the\nmanipulated objects, such as their weight and fragility. We explore the\npossibility of leveraging Generative Adversarial Networks to synthesize new\nactions coherent with the properties of the object. The use of a generative\napproach allows us to create new and consistent motion patterns, without the\nneed of collecting a large number of recorded human-led demonstrations.\nBesides, the informative content of the actions is preserved. Our results show\nthat Generative Adversarial Nets can be a powerful tool for the generation of\nnovel and meaningful transportation actions, which result effectively modulated\nas a function of the object weight and the carefulness required in its\nhandling.",
    "descriptor": "\nComments: Accepted for publication in the Proceedings of the IEEE International Conference on Development and Learning (ICDL) 2021 - 11th ICDL-EPIROB 7 pages, 5 figures\n",
    "authors": [
      "Luca Garello",
      "Linda Lastrico",
      "Francesco Rea",
      "Fulvio Mastrogiovanni",
      "Nicoletta Noceti",
      "Alessandra Sciutti"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04385"
  },
  {
    "id": "arXiv:2106.04387",
    "title": "Multi-frame sequence generator of 4D human body motion",
    "abstract": "We examine the problem of generating temporally and spatially dense 4D human\nbody motion. On the one hand generative modeling has been extensively studied\nas a per time-frame static fitting problem for dense 3D models such as mesh\nrepresentations, where the temporal aspect is left out of the generative model.\nOn the other hand, temporal generative models exist for sparse human models\nsuch as marker-based capture representations, but have not to our knowledge\nbeen extended to dense 3D shapes. We propose to bridge this gap with a\ngenerative auto-encoder-based framework, which encodes morphology, global\nlocomotion including translation and rotation, and multi-frame temporal motion\nas a single latent space vector. To assess its generalization and factorization\nabilities, we train our model on a cyclic locomotion subset of AMASS,\nleveraging the dense surface models it provides for an extensive set of motion\ncaptures. Our results validate the ability of the model to reconstruct 4D\nsequences of human locomotions within a low error bound, and the meaningfulness\nof latent space interpolation between latent vectors representing different\nmulti-frame sequences and locomotion types. We also illustrate the benefits of\nthe approach for 4D human motion prediction of future frames from initial human\nlocomotion frames, showing promising abilities of our model to learn realistic\nspatio-temporal features of human motion. We show that our model allows for\ndata completion of both spatially and temporally sparse data.",
    "descriptor": "",
    "authors": [
      "Marsot Mathieu",
      "Wuhrer Stefanie",
      "Franco Jean-Sebastien",
      "Durocher Stephane"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04387"
  },
  {
    "id": "arXiv:2106.04392",
    "title": "Signal Transformer: Complex-valued Attention and Meta-Learning for  Signal Recognition",
    "abstract": "Deep neural networks have been shown as a class of useful tools for\naddressing signal recognition issues in recent years, especially for\nidentifying the nonlinear feature structures of signals. However, this power of\nmost deep learning techniques heavily relies on an abundant amount of training\ndata, so the performance of classic neural nets decreases sharply when the\nnumber of training data samples is small or unseen data are presented in the\ntesting phase. This calls for an advanced strategy, i.e., model-agnostic\nmeta-learning (MAML), which is able to capture the invariant representation of\nthe data samples or signals. In this paper, inspired by the special structure\nof the signal, i.e., real and imaginary parts consisted in practical\ntime-series signals, we propose a Complex-valued Attentional MEta Learner\n(CAMEL) for the problem of few-shot signal recognition by leveraging attention\nand meta-learning in the complex domain. To the best of our knowledge, this is\nalso the first complex-valued MAML that can find the first-order stationary\npoints of general nonconvex problems with theoretical convergence guarantees.\nExtensive experiments results showcase the superiority of the proposed CAMEL\ncompared with the state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Yihong Dong",
      "Ying Peng",
      "Muqiao Yang",
      "Songtao Lu",
      "Qingjiang Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04392"
  },
  {
    "id": "arXiv:2106.04398",
    "title": "Learning compositional structures for semantic graph parsing",
    "abstract": "AM dependency parsing is a method for neural semantic graph parsing that\nexploits the principle of compositionality. While AM dependency parsers have\nbeen shown to be fast and accurate across several graphbanks, they require\nexplicit annotations of the compositional tree structures for training. In the\npast, these were obtained using complex graphbank-specific heuristics written\nby experts. Here we show how they can instead be trained directly on the graphs\nwith a neural latent-variable model, drastically reducing the amount and\ncomplexity of manual heuristics. We demonstrate that our model picks up on\nseveral linguistic phenomena on its own and achieves comparable accuracy to\nsupervised training, greatly facilitating the use of AM dependency parsing for\nnew sembanks.",
    "descriptor": "\nComments: Accepted at the 5th Workshop on Structured Prediction for NLP (this http URL)\n",
    "authors": [
      "Jonas Groschwitz",
      "Meaghan Fowlie",
      "Alexander Koller"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04398"
  },
  {
    "id": "arXiv:2106.04399",
    "title": "Flow Network based Generative Models for Non-Iterative Diverse Candidate  Generation",
    "abstract": "This paper is about the problem of learning a stochastic policy for\ngenerating an object (like a molecular graph) from a sequence of actions, such\nthat the probability of generating an object is proportional to a given\npositive reward for that object. Whereas standard return maximization tends to\nconverge to a single return-maximizing sequence, there are cases where we would\nlike to sample a diverse set of high-return solutions. These arise, for\nexample, in black-box function optimization when few rounds are possible, each\nwith large batches of queries, where the batches should be diverse, e.g., in\nthe design of new molecules. One can also see this as a problem of\napproximately converting an energy function to a generative distribution. While\nMCMC methods can achieve that, they are expensive and generally only perform\nlocal exploration. Instead, training a generative policy amortizes the cost of\nsearch during training and yields to fast generation. Using insights from\nTemporal Difference learning, we propose GFlowNet, based on a view of the\ngenerative process as a flow network, making it possible to handle the tricky\ncase where different trajectories can yield the same final state, e.g., there\nare many ways to sequentially add atoms to generate some molecular graph. We\ncast the set of trajectories as a flow and convert the flow consistency\nequations into a learning objective, akin to the casting of the Bellman\nequations into Temporal Difference methods. We prove that any global minimum of\nthe proposed objectives yields a policy which samples from the desired\ndistribution, and demonstrate the improved performance and diversity of\nGFlowNet on a simple domain where there are many modes to the reward function,\nand on a molecule synthesis task.",
    "descriptor": "\nComments: Submitted to NeurIPS 2021\n",
    "authors": [
      "Emmanuel Bengio",
      "Moksh Jain",
      "Maksym Korablyov",
      "Doina Precup",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04399"
  },
  {
    "id": "arXiv:2106.04400",
    "title": "CSRNet: Cascaded Selective Resolution Network for Real-time Semantic  Segmentation",
    "abstract": "Real-time semantic segmentation has received considerable attention due to\ngrowing demands in many practical applications, such as autonomous vehicles,\nrobotics, etc. Existing real-time segmentation approaches often utilize feature\nfusion to improve segmentation accuracy. However, they fail to fully consider\nthe feature information at different resolutions and the receptive fields of\nthe networks are relatively limited, thereby compromising the performance. To\ntackle this problem, we propose a light Cascaded Selective Resolution Network\n(CSRNet) to improve the performance of real-time segmentation through multiple\ncontext information embedding and enhanced feature aggregation. The proposed\nnetwork builds a three-stage segmentation system, which integrates feature\ninformation from low resolution to high resolution and achieves feature\nrefinement progressively. CSRNet contains two critical modules: the Shorted\nPyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The\nSPFM is a computationally efficient module to incorporate the global context\ninformation and significantly enlarge the receptive field at each stage. The\nSRM is designed to fuse multi-resolution feature maps with various receptive\nfields, which assigns soft channel attentions across the feature maps and helps\nto remedy the problem caused by multi-scale objects. Comprehensive experiments\non two well-known datasets demonstrate that the proposed CSRNet effectively\nimproves the performance for real-time segmentation.",
    "descriptor": "",
    "authors": [
      "Jingjing Xiong",
      "Lai-Man Po",
      "Wing-Yin Yu",
      "Chang Zhou",
      "Pengfei Xian",
      "Weifeng Ou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04400"
  },
  {
    "id": "arXiv:2106.04403",
    "title": "SynthRef: Generation of Synthetic Referring Expressions for Object  Segmentation",
    "abstract": "Recent advances in deep learning have brought significant progress in visual\ngrounding tasks such as language-guided video object segmentation. However,\ncollecting large datasets for these tasks is expensive in terms of annotation\ntime, which represents a bottleneck. To this end, we propose a novel method,\nnamely SynthRef, for generating synthetic referring expressions for target\nobjects in an image (or video frame), and we also present and disseminate the\nfirst large-scale dataset with synthetic referring expressions for video object\nsegmentation. Our experiments demonstrate that by training with our synthetic\nreferring expressions one can improve the ability of a model to generalize\nacross different datasets, without any additional annotation cost. Moreover,\nour formulation allows its application to any object detection or segmentation\ndataset.",
    "descriptor": "\nComments: Accepted as poster at the NAACL 2021 Visually Grounded Interaction and Language (ViGIL) Workshop. 4 pages. Project website: this https URL\n",
    "authors": [
      "Ioannis Kazakos",
      "Carles Ventura",
      "Miriam Bellver",
      "Carina Silberer",
      "Xavier Giro-i-Nieto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.04403"
  },
  {
    "id": "arXiv:2106.04404",
    "title": "The Struggle with Academic Plagiarism: Approaches based on Semantic  Similarity",
    "abstract": "Academic plagiarism is a serious problem nowadays. Due to the existence of\ninexhaustible sources of digital information, today it is easier to plagiarize\nmore than ever before. The good thing is that plagiarism detection techniques\nhave improved and are powerful enough to detect attempts of plagiarism in\neducation. We are now witnessing efficient plagiarism detection software in\naction, such as Turnitin, iThenticate or SafeAssign. In the introduction we\nexplore software that is used within the Croatian academic community for\nplagiarism detection in universities and/or in scientific journals. The\nquestion is: is this enough? Current software has proven to be successful,\nhowever the problem of identifying paraphrasing or obfuscation plagiarism\nremains unresolved. In this paper we present a report of how semantic\nsimilarity measures can be used in the plagiarism detection task.",
    "descriptor": "\nComments: 6 pages, 1 figure, 34 references\n",
    "authors": [
      "Tedo Vrbanec",
      "Ana Mestrovic"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04404"
  },
  {
    "id": "arXiv:2106.04405",
    "title": "Federated Neural Collaborative Filtering",
    "abstract": "In this work, we present a federated version of the state-of-the-art Neural\nCollaborative Filtering (NCF) approach for item recommendations. The system,\nnamed FedNCF, allows learning without requiring users to expose or transmit\ntheir raw data. Experimental validation shows that FedNCF achieves comparable\nrecommendation quality to the original NCF system. Although federated learning\n(FL) enables learning without raw data transmission, recent attacks showed that\nFL alone does not eliminate privacy concerns. To overcome this challenge, we\nintegrate a privacy-preserving enhancement with a secure aggregation scheme\nthat satisfies the security requirements against an honest-but-curious (HBC)\nentity, without affecting the quality of the original model. Finally, we\ndiscuss the peculiarities observed in the application of FL in a collaborative\nfiltering (CF) task as well as we evaluate the privacy-preserving mechanism in\nterms of computational cost.",
    "descriptor": "",
    "authors": [
      "Vasileios Perifanis",
      "Pavlos S. Efraimidis"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04405"
  },
  {
    "id": "arXiv:2106.04408",
    "title": "HieRec: Hierarchical User Interest Modeling for Personalized News  Recommendation",
    "abstract": "User interest modeling is critical for personalized news recommendation.\nExisting news recommendation methods usually learn a single user embedding for\neach user from their previous behaviors to represent their overall interest.\nHowever, user interest is usually diverse and multi-grained, which is difficult\nto be accurately modeled by a single user embedding. In this paper, we propose\na news recommendation method with hierarchical user interest modeling, named\nHieRec. Instead of a single user embedding, in our method each user is\nrepresented in a hierarchical interest tree to better capture their diverse and\nmulti-grained interest in news. We use a three-level hierarchy to represent 1)\noverall user interest; 2) user interest in coarse-grained topics like sports;\nand 3) user interest in fine-grained topics like football. Moreover, we propose\na hierarchical user interest matching framework to match candidate news with\ndifferent levels of user interest for more accurate user interest targeting.\nExtensive experiments on two real-world datasets validate our method can\neffectively improve the performance of user modeling for personalized news\nrecommendation.",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Peiru Yang",
      "Yang Yu",
      "Xing Xie",
      "Yongfeng Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04408"
  },
  {
    "id": "arXiv:2106.04411",
    "title": "Fair Feature Distillation for Visual Recognition",
    "abstract": "Fairness is becoming an increasingly crucial issue for computer vision,\nespecially in the human-related decision systems. However, achieving\nalgorithmic fairness, which makes a model produce indiscriminative outcomes\nagainst protected groups, is still an unresolved problem. In this paper, we\ndevise a systematic approach which reduces algorithmic biases via feature\ndistillation for visual recognition tasks, dubbed as MMD-based Fair\nDistillation (MFD). While the distillation technique has been widely used in\ngeneral to improve the prediction accuracy, to the best of our knowledge, there\nhas been no explicit work that also tries to improve fairness via distillation.\nFurthermore, We give a theoretical justification of our MFD on the effect of\nknowledge distillation and fairness. Throughout the extensive experiments, we\nshow our MFD significantly mitigates the bias against specific minorities\nwithout any loss of the accuracy on both synthetic and real-world face\ndatasets.",
    "descriptor": "",
    "authors": [
      "Sangwon Jung",
      "Donggyu Lee",
      "Taeeon Lee",
      "Taesup Moon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04411"
  },
  {
    "id": "arXiv:2106.04413",
    "title": "Stochastic Whitening Batch Normalization",
    "abstract": "Batch Normalization (BN) is a popular technique for training Deep Neural\nNetworks (DNNs). BN uses scaling and shifting to normalize activations of\nmini-batches to accelerate convergence and improve generalization. The recently\nproposed Iterative Normalization (IterNorm) method improves these properties by\nwhitening the activations iteratively using Newton's method. However, since\nNewton's method initializes the whitening matrix independently at each training\nstep, no information is shared between consecutive steps. In this work, instead\nof exact computation of whitening matrix at each time step, we estimate it\ngradually during training in an online fashion, using our proposed Stochastic\nWhitening Batch Normalization (SWBN) algorithm. We show that while SWBN\nimproves the convergence rate and generalization of DNNs, its computational\noverhead is less than that of IterNorm. Due to the high efficiency of the\nproposed method, it can be easily employed in most DNN architectures with a\nlarge number of layers. We provide comprehensive experiments and comparisons\nbetween BN, IterNorm, and SWBN layers to demonstrate the effectiveness of the\nproposed technique in conventional (many-shot) image classification and\nfew-shot classification tasks.",
    "descriptor": "\nComments: Accepted to the Main Conference of CVPR 2021\n",
    "authors": [
      "Shengdong Zhang",
      "Ehsan Nezhadarya",
      "Homa Fashandi",
      "Jiayi Liu",
      "Darin Graham",
      "Mohak Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04413"
  },
  {
    "id": "arXiv:2106.04415",
    "title": "Exploring Periodicity and Interactivity in Multi-Interest Framework for  Sequential Recommendation",
    "abstract": "Sequential recommendation systems alleviate the problem of information\noverload, and have attracted increasing attention in the literature. Most prior\nworks usually obtain an overall representation based on the user's behavior\nsequence, which can not sufficiently reflect the multiple interests of the\nuser. To this end, we propose a novel method called PIMI to mitigate this\nissue. PIMI can model the user's multi-interest representation effectively by\nconsidering both the periodicity and interactivity in the item sequence.\nSpecifically, we design a periodicity-aware module to utilize the time interval\ninformation between user's behaviors. Meanwhile, an ingenious graph is proposed\nto enhance the interactivity between items in user's behavior sequence, which\ncan capture both global and local item features. Finally, a multi-interest\nextraction module is applied to describe user's multiple interests based on the\nobtained item representation. Extensive experiments on two real-world datasets\nAmazon and Taobao show that PIMI outperforms state-of-the-art methods\nconsistently.",
    "descriptor": "\nComments: 8 pages, 5 figures, to be published in IJCAI-2021\n",
    "authors": [
      "Gaode Chen",
      "Xinghua Zhang",
      "Yanyan Zhao",
      "Cong Xue",
      "Ji Xiang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04415"
  },
  {
    "id": "arXiv:2106.04419",
    "title": "Asymmetrical Bi-RNN for pedestrian trajectory encoding",
    "abstract": "Pedestrian motion behavior involves a combination of individual goals and\nsocial interactions with other agents. In this article, we present a\nnon-symmetrical bidirectional recurrent neural network architecture called\nU-RNN as a sequence encoder and evaluate its relevance to replace LSTMs for\nvarious forecasting models. Experimental results on the Trajnet++ benchmark\nshow that the U-LSTM variant can yield better results regarding every available\nmetric (ADE, FDE, Collision rate) than common LSTMs sequence encoders for a\nvariety of approaches and interaction modules.\nOur implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is\navailable at:\ngithub.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Rapha\u00ebl Rozenberg",
      "Joseph Gesnouin",
      "Fabien Moutarde"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04419"
  },
  {
    "id": "arXiv:2106.04420",
    "title": "Back2Future: Leveraging Backfill Dynamics for Improving Real-time  Predictions in Future",
    "abstract": "In real-time forecasting in public health, data collection is a non-trivial\nand demanding task. Often after initially released, it undergoes several\nrevisions later (maybe due to human or technical constraints) - as a result, it\nmay take weeks until the data reaches to a stable value. This so-called\n'backfill' phenomenon and its effect on model performance has been barely\nstudied in the prior literature. In this paper, we introduce the multi-variate\nbackfill problem using COVID-19 as the motivating example. We construct a\ndetailed dataset composed of relevant signals over the past year of the\npandemic. We then systematically characterize several patterns in backfill\ndynamics and leverage our observations for formulating a novel problem and\nneural framework Back2Future that aims to refines a given model's predictions\nin real-time. Our extensive experiments demonstrate that our method refines the\nperformance of top models for COVID-19 forecasting, in contrast to non-trivial\nbaselines, yielding 18% improvement over baselines, enabling us obtain a new\nSOTA performance. In addition, we show that our model improves model evaluation\ntoo; hence policy-makers can better understand the true accuracy of forecasting\nmodels in real-time.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Harshavardhan Kamarthi",
      "Alexander Rodr\u00edguez",
      "B. Aditya Prakash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.04420"
  },
  {
    "id": "arXiv:2106.04423",
    "title": "PANACEA cough sound-based diagnosis of COVID-19 for the DiCOVA 2021  Challenge",
    "abstract": "The COVID-19 pandemic has led to the saturation of public health services\nworldwide. In this scenario, the early diagnosis of SARS-Cov-2 infections can\nhelp to stop or slow the spread of the virus and to manage the demand upon\nhealth services. This is especially important when resources are also being\nstretched by heightened demand linked to other seasonal diseases, such as the\nflu. In this context, the organisers of the DiCOVA 2021 challenge have\ncollected a database with the aim of diagnosing COVID-19 through the use of\ncoughing audio samples. This work presents the details of the automatic system\nfor COVID-19 detection from cough recordings presented by team PANACEA. This\nteam consists of researchers from two European academic institutions and one\ncompany: EURECOM (France), University of Granada (Spain), and Biometric Vox\nS.L. (Spain). We developed several systems based on established signal\nprocessing and machine learning methods. Our best system employs a Teager\nenergy operator cepstral coefficients (TECCs) based frontend and Light gradient\nboosting machine (LightGBM) backend. The AUC obtained by this system on the\ntest set is 76.31% which corresponds to a 10% improvement over the official\nbaseline.",
    "descriptor": "\nComments: Accepted in INTERSPEECH 2021\n",
    "authors": [
      "Madhu R. Kamble",
      "Jose A. Gonzalez-Lopez",
      "Teresa Grau",
      "Juan M. Espin",
      "Lorenzo Cascioli",
      "Yiqing Huang",
      "Alejandro Gomez-Alanis",
      "Jose Patino",
      "Roberto Font",
      "Antonio M. Peinado",
      "Angel M. Gomez",
      "Nicholas Evans",
      "Maria A. Zuluaga",
      "Massimiliano Todisco"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.04423"
  },
  {
    "id": "arXiv:2106.04426",
    "title": "Hash Layers For Large Sparse Models",
    "abstract": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
    "descriptor": "",
    "authors": [
      "Stephen Roller",
      "Sainbayar Sukhbaatar",
      "Arthur Szlam",
      "Jason Weston"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04426"
  },
  {
    "id": "arXiv:2106.04427",
    "title": "On the relation between statistical learning and perceptual distances",
    "abstract": "It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationship between the probability distribution of\nthe data, perceptual distances, and unsupervised machine learning. To this end,\nwe show that perceptual sensitivity is correlated with the probability of an\nimage in its close neighborhood. We also explore the relation between distances\ninduced by autoencoders and the probability distribution of the data used for\ntraining them, as well as how these induced distances are correlated with human\nperception. Finally, we discuss why perceptual distances might not lead to\nnoticeable gains in performance over standard Euclidean distances in common\nimage processing tasks except when data is scarce and the perceptual distance\nprovides regularization.",
    "descriptor": "",
    "authors": [
      "Alexander Hepburn",
      "Valero Laparra",
      "Raul Santos-Rodriguez",
      "Johannes Ball\u00e9",
      "Jes\u00fas Malo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.04427"
  },
  {
    "id": "arXiv:2106.04428",
    "title": "Noise Conditional Flow Model for Learning the Super-Resolution Space",
    "abstract": "Fundamentally, super-resolution is ill-posed problem because a low-resolution\nimage can be obtained from many high-resolution images. Recent studies for\nsuper-resolution cannot create diverse super-resolution images. Although SRFlow\ntried to account for ill-posed nature of the super-resolution by predicting\nmultiple high-resolution images given a low-resolution image, there is room to\nimprove the diversity and visual quality. In this paper, we propose Noise\nConditional flow model for Super-Resolution, NCSR, which increases the visual\nquality and diversity of images through noise conditional layer. To learn more\ndiverse data distribution, we add noise to training data. However, low-quality\nimages are resulted from adding noise. We propose the noise conditional layer\nto overcome this phenomenon. The noise conditional layer makes our model\ngenerate more diverse images with higher visual quality than other works.\nFurthermore, we show that this layer can overcome data distribution mismatch, a\nproblem that arises in normalizing flow models. With these benefits, NCSR\noutperforms baseline in diversity and visual quality and achieves better visual\nquality than traditional GAN-based models. We also get outperformed scores at\nNTIRE 2021 challenge.",
    "descriptor": "\nComments: Final CVPR2021 workshop version\n",
    "authors": [
      "Younggeun Kim",
      "Donghee Son"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04428"
  },
  {
    "id": "arXiv:2106.04432",
    "title": "Lifts for Voronoi cells of lattices",
    "abstract": "Many polytopes arising in polyhedral combinatorics are linear projections of\nhigher-dimensional polytopes with significantly fewer facets. Such lifts may\nyield compressed representations of polytopes, which are typically used to\nconstruct small-size linear programs. Motivated by algorithmic implications for\nthe closest vector problem, we study lifts of Voronoi cells of lattices.\nWe construct an explicit $d$-dimensional lattice such that every lift of the\nrespective Voronoi cell has $2^{\\Omega(d / \\log d)}$ facets. On the positive\nside, we show that Voronoi cells of $d$-dimensional root lattices and their\ndual lattices have lifts with $O(d)$ and $O(d \\log d)$ facets, respectively. We\nobtain similar results for spectrahedral lifts.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Matthias Schymura",
      "Ina Seidel",
      "Stefan Weltge"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04432"
  },
  {
    "id": "arXiv:2106.04434",
    "title": "SDGMNet: Statistic-based Dynamic Gradient Modulation for Local  Descriptor Learning",
    "abstract": "Modifications on triplet loss that rescale the back-propagated gradients of\nspecial pairs have made significant progress on local descriptor learning.\nHowever, current gradient modulation strategies are mainly static so that they\nwould suffer from changes of training phases or datasets. In this paper, we\npropose a dynamic gradient modulation, named SDGMNet, to improve triplet loss\nfor local descriptor learning. The core of our method is formulating modulation\nfunctions with statistical characteristics which are estimated dynamically.\nFirstly, we perform deep analysis on back propagation of general triplet-based\nloss and introduce included angle for distance measure. On this basis,\nauto-focus modulation is employed to moderate the impact of statistically\nuncommon individual pairs in stochastic gradient descent optimization;\nprobabilistic margin cuts off the gradients of proportional Siamese pairs that\nare believed to reach the optimum; power adjustment balances the total weights\nof negative pairs and positive pairs. Extensive experiments demonstrate that\nour novel descriptor surpasses previous state-of-the-arts on standard\nbenchmarks including patch verification, matching and retrieval tasks.",
    "descriptor": "",
    "authors": [
      "Jiayi Ma",
      "Yuxin Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04434"
  },
  {
    "id": "arXiv:2106.04435",
    "title": "Enhancing Robustness of Neural Networks through Fourier Stabilization",
    "abstract": "Despite the considerable success of neural networks in security settings such\nas malware detection, such models have proved vulnerable to evasion attacks, in\nwhich attackers make slight changes to inputs (e.g., malware) to bypass\ndetection. We propose a novel approach, \\emph{Fourier stabilization}, for\ndesigning evasion-robust neural networks with binary inputs. This approach,\nwhich is complementary to other forms of defense, replaces the weights of\nindividual neurons with robust analogs derived using Fourier analytic tools.\nThe choice of which neurons to stabilize in a neural network is then a\ncombinatorial optimization problem, and we propose several methods for\napproximately solving it. We provide a formal bound on the per-neuron drop in\naccuracy due to Fourier stabilization, and experimentally demonstrate the\neffectiveness of the proposed approach in boosting robustness of neural\nnetworks in several detection settings. Moreover, we show that our approach\neffectively composes with adversarial training.",
    "descriptor": "\nComments: Full version of an ICML 2021 paper\n",
    "authors": [
      "Netanel Raviv",
      "Aidan Kelley",
      "Michael Guo",
      "Yevgeny Vorobeychik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04435"
  },
  {
    "id": "arXiv:2106.04437",
    "title": "Adversarial Training for Machine Reading Comprehension with Virtual  Embeddings",
    "abstract": "Adversarial training (AT) as a regularization method has proved its\neffectiveness on various tasks. Though there are successful applications of AT\non some NLP tasks, the distinguishing characteristics of NLP tasks have not\nbeen exploited. In this paper, we aim to apply AT on machine reading\ncomprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing\na novel adversarial training method called PQAT that perturbs the embedding\nmatrix instead of word vectors. To differentiate the roles of passages and\nquestions, PQAT uses additional virtual P/Q-embedding matrices to gather the\nglobal perturbations of words from passages and questions separately. We test\nthe method on a wide range of MRC tasks, including span-based extractive RC and\nmultiple-choice RC. The results show that adversarial training is effective\nuniversally, and PQAT further improves the performance.",
    "descriptor": "\nComments: Accepted to *SEM 2021 workshop at ACL 2021\n",
    "authors": [
      "Ziqing Yang",
      "Yiming Cui",
      "Chenglei Si",
      "Wanxiang Che",
      "Ting Liu",
      "Shijin Wang",
      "Guoping Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04437"
  },
  {
    "id": "arXiv:2106.04441",
    "title": "CLTR: An End-to-End, Transformer-Based System for Cell Level  TableRetrieval and Table Question Answering",
    "abstract": "We present the first end-to-end, transformer-based table question answering\n(QA) system that takes natural language questions and massive table corpus as\ninputs to retrieve the most relevant tables and locate the correct table cells\nto answer the question. Our system, CLTR, extends the current state-of-the-art\nQA over tables model to build an end-to-end table QA architecture. This system\nhas successfully tackled many real-world table QA problems with a simple,\nunified pipeline. Our proposed system can also generate a heatmap of candidate\ncolumns and rows over complex tables and allow users to quickly identify the\ncorrect cells to answer questions. In addition, we introduce two new\nopen-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural\nlanguage questions over 76,242 tables. The benchmarks are designed to validate\nCLTR as well as accommodate future table retrieval and end-to-end table QA\nresearch and experiments. Our experiments demonstrate that our system is the\ncurrent state-of-the-art model on the table retrieval task and produces\npromising results for end-to-end table QA.",
    "descriptor": "",
    "authors": [
      "Feifei Pan",
      "Mustafa Canim",
      "Michael Glass",
      "Alfio Gliozzo",
      "Peter Fox"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04441"
  },
  {
    "id": "arXiv:2106.04443",
    "title": "Robust Generalization despite Distribution Shift via Minimum  Discriminating Information",
    "abstract": "Training models that perform well under distribution shifts is a central\nchallenge in machine learning. In this paper, we introduce a modeling framework\nwhere, in addition to training data, we have partial structural knowledge of\nthe shifted test distribution. We employ the principle of minimum\ndiscriminating information to embed the available prior knowledge, and use\ndistributionally robust optimization to account for uncertainty due to the\nlimited samples. By leveraging large deviation results, we obtain explicit\ngeneralization bounds with respect to the unknown shifted distribution. Lastly,\nwe demonstrate the versatility of our framework by demonstrating it on two\nrather distinct applications: (1) training classifiers on systematically biased\ndata and (2) off-policy evaluation in Markov Decision Processes.",
    "descriptor": "\nComments: 23 pages, 4 figures\n",
    "authors": [
      "Tobias Sutter",
      "Andreas Krause",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04443"
  },
  {
    "id": "arXiv:2106.04447",
    "title": "Reading StackOverflow Encourages Cheating: Adding Question Text Improves  Extractive Code Generation",
    "abstract": "Answering a programming question using only its title is difficult as salient\ncontextual information is omitted. Based on this observation, we present a\ncorpus of over 40,000 StackOverflow question texts to be used in conjunction\nwith their corresponding intents from the CoNaLa dataset (Yin et al., 2018).\nUsing both the intent and question body, we use BART to establish a baseline\nBLEU score of 34.35 for this new task. We find further improvements of $2.8\\%$\nby combining the mined CoNaLa data with the labeled data to achieve a 35.32\nBLEU score. We evaluate prior state-of-the-art CoNaLa models with this\nadditional data and find that our proposed method of using the body and mined\ndata beats the BLEU score of the prior state-of-the-art by $71.96\\%$. Finally,\nwe perform ablations to demonstrate that BART is an unsupervised multimodal\nlearner and examine its extractive behavior. The code and data can be found\nhttps://github.com/gabeorlanski/stackoverflow-encourages-cheating.",
    "descriptor": "\nComments: To be published in ACL-IJCNLP NLP4Prog workshop. (The First Workshop on Natural Language Processing for Programming)\n",
    "authors": [
      "Gabriel Orlanski",
      "Alex Gittens"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04447"
  },
  {
    "id": "arXiv:2106.04449",
    "title": "Computation Offloading at Field Level: Motivation and Break-Even Point  Calculation",
    "abstract": "Smart manufacturing has the objective of creating highly flexible and\nresource optimized industrial plants. Furthermore, the improvement of product\nquality is another important target. These requirements implicate more complex\ncontrol algo-rithms. Processing these algorithms may exceed the capabilities of\nresource constrained devices, such as programmable logic controllers (PLCs). In\nthis case, the necessity for computation offloading is given. Due to the fact\nthat industrial plants are currently designed for a life-cycle-time of more\nthan ten years, in a realistic smart manufacturing scenario, these devices have\nto be considered. Therefore, we investigate the impact of complex algorithms on\nconventional PLCs by simulating them with a load generator. In addition, we\npropose a realistic factory scenario including benchmarks for both wireline and\nwireless communication systems. Thus, their round-trip time (RTT) is measured\nwith and without additional load on the network. With the help of these\ninvestigations, break-even points for the application of computation offloading\nof two typical PLCs of Siemens S7 series can be calculated.",
    "descriptor": "",
    "authors": [
      "Michael Gundall",
      "Christopher Huber",
      "Hans D. Schotten"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04449"
  },
  {
    "id": "arXiv:2106.04461",
    "title": "North Carolina COVID-19 Agent-Based Model Framework for Hospitalization  Forecasting Overview, Design Concepts, and Details Protocol",
    "abstract": "This Overview, Design Concepts, and Details Protocol (ODD) provides a\ndetailed description of an agent-based model (ABM) that was developed to\nsimulate hospitalizations during the COVID-19 pandemic. Using the descriptions\nof submodels, provided parameters, and the links to data sources, modelers will\nbe able to replicate the creation and results of this model.",
    "descriptor": "",
    "authors": [
      "Kasey Jones",
      "Emily Hadley",
      "Sandy Preiss",
      "Caroline Kery",
      "Peter Baumgartner",
      "Marie Stoner",
      "Sarah Rhea"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.04461"
  },
  {
    "id": "arXiv:2106.04462",
    "title": "Muddling Label Regularization: Deep Learning for Tabular Datasets",
    "abstract": "Deep Learning (DL) is considered the state-of-the-art in computer vision,\nspeech recognition and natural language processing. Until recently, it was also\nwidely accepted that DL is irrelevant for learning tasks on tabular data,\nespecially in the small sample regime where ensemble methods are acknowledged\nas the gold standard. We present a new end-to-end differentiable method to\ntrain a standard FFNN. Our method, \\textbf{Muddling labels for Regularization}\n(\\texttt{MLR}), penalizes memorization through the generation of uninformative\nlabels and the application of a differentiable close-form regularization scheme\non the last hidden layer during training. \\texttt{MLR} outperforms classical NN\nand the gold standard (GBDT, RF) for regression and classification tasks on\nseveral datasets from the UCI database and Kaggle covering a large range of\nsample sizes and feature to sample ratios. Researchers and practitioners can\nuse \\texttt{MLR} on its own as an off-the-shelf \\DL{} solution or integrate it\ninto the most advanced ML pipelines.",
    "descriptor": "\nComments: 20 pages, 7 tables\n",
    "authors": [
      "Karim Lounici",
      "Katia Meziani",
      "Benjamin Riu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04462"
  },
  {
    "id": "arXiv:2106.04465",
    "title": "Detecting Anomalous Event Sequences with Temporal Point Processes",
    "abstract": "Automatically detecting anomalies in event data can provide substantial value\nin domains such as healthcare, DevOps, and information security. In this paper,\nwe frame the problem of detecting anomalous continuous-time event sequences as\nout-of-distribution (OoD) detection for temporal point processes (TPPs). First,\nwe show how this problem can be approached using goodness-of-fit (GoF) tests.\nWe then demonstrate the limitations of popular GoF statistics for TPPs and\npropose a new test that addresses these shortcomings. The proposed method can\nbe combined with various TPP models, such as neural TPPs, and is easy to\nimplement. In our experiments, we show that the proposed statistic excels at\nboth traditional GoF testing, as well as at detecting anomalies in simulated\nand real-world data.",
    "descriptor": "",
    "authors": [
      "Oleksandr Shchur",
      "Ali Caner T\u00fcrkmen",
      "Tim Januschowski",
      "Jan Gasthaus",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04465"
  },
  {
    "id": "arXiv:2106.04467",
    "title": "Private Multi-Group Aggregation",
    "abstract": "We study the differentially private multi group aggregation (PMGA) problem.\nThis setting involves a single server and $n$ users. Each user belongs to one\nof $k$ distinct groups and holds a discrete value. The goal is to design\nschemes that allow the server to find the aggregate (sum) of the values in each\ngroup (with high accuracy) under communication and local differential privacy\nconstraints. The privacy constraint guarantees that the user's group remains\nprivate. This is motivated by applications where a user's group can reveal\nsensitive information, such as his religious and political beliefs, health\ncondition, or race. We propose a novel scheme, dubbed Query and Aggregate\n(Q\\&A) for PMGA. The novelty of Q\\&A is that it is an interactive aggregation\nscheme. In Q\\&A, each user is assigned a random query matrix, to which he sends\nthe server an answer based on his group and value. We characterize the Q\\&A\nscheme's performance in terms of accuracy (MSE), privacy, and communication. We\ncompare Q\\&A to the Randomized Group (RG) scheme, which is non-interactive and\nadapts existing randomized response schemes to the PMGA setting. We observe\nthat typically Q\\&A outperforms RG, in terms of privacy vs. utility, in the\nhigh privacy regime.",
    "descriptor": "\nComments: Short video explaining part of the results: this https URL\n",
    "authors": [
      "Carolina Naim",
      "Rafael G. L. D'Oliveira",
      "Salim El Rouayheb"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04467"
  },
  {
    "id": "arXiv:2106.04468",
    "title": "Localization Threats in Next-Generation Wireless Networks",
    "abstract": "The impact of localization systems in our daily lives is increasing. As\nnext-generation networks will introduce hyperconnectivity with the emerging\napplications, this impact will undoubtedly further increase, proliferating the\nimportance of the location information's reliability. As society becomes more\ndependent on this information in terms of the products and services, security\nsolutions will have to be enriched to provide countermeasures sufficiently\nadvanced to ever-evolving threats, forcing the joint design of communication\nand localization systems. This paper envisions integrated communication and\nlocalization systems by focusing on localization security. Also, conventional\nand next-generation attacks on localization are discussed along with an\nefficient attack detection method and test-bed-based demonstration,\nhighlighting the need for effective countermeasures.",
    "descriptor": "\nComments: 7 pages, 5 figures, 1 table, 15 references\n",
    "authors": [
      "Caner Goztepe",
      "Saliha Buyukcorak",
      "Gunes Karabulut Kurt",
      "Halim Yanikomeroglu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04468"
  },
  {
    "id": "arXiv:2106.04471",
    "title": "Interpreting Deep Learning based Cerebral Palsy Prediction with Channel  Attention",
    "abstract": "Early prediction of cerebral palsy is essential as it leads to early\ntreatment and monitoring. Deep learning has shown promising results in\nbiomedical engineering thanks to its capacity of modelling complicated data\nwith its non-linear architecture. However, due to their complex structure, deep\nlearning models are generally not interpretable by humans, making it difficult\nfor clinicians to rely on the findings. In this paper, we propose a channel\nattention module for deep learning models to predict cerebral palsy from\ninfants' body movements, which highlights the key features (i.e. body joints)\nthe model identifies as important, thereby indicating why certain diagnostic\nresults are found. To highlight the capacity of the deep network in modelling\ninput features, we utilize raw joint positions instead of hand-crafted\nfeatures. We validate our system with a real-world infant movement dataset. Our\nproposed channel attention module enables the visualization of the vital joints\nto this disease that the network considers. Our system achieves 91.67%\naccuracy, suppressing other state-of-the-art deep learning methods.",
    "descriptor": "",
    "authors": [
      "Manli Zhu",
      "Qianhui Men",
      "Edmond S. L. Ho",
      "Howard Leung",
      "Hubert P. H. Shum"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.04471"
  },
  {
    "id": "arXiv:2106.04475",
    "title": "Globular weak $\u03c9$-categories as models of a type theory",
    "abstract": "We study the dependent type theory CaTT, introduced by Finster and Mimram,\nwhich presents the theory of weak $\\omega$-categories, following the idea that\ntype theories can be considered as presentations of generalized algebraic\ntheories. Our main contribution is a formal proof that the models of this type\ntheory correspond precisely to weak $\\omega$-categories, as defined by\nMaltsiniotis, by generalizing a definition proposed by Grothendieck for weak\n$\\omega$-groupoids: Those are defined as suitable presheaves over a\ncat-coherator, which is a category encoding structure expected to be found in\nan $\\omega$-category. This comparison is established by proving the initiality\nconjecture for the type theory CaTT, in a way which suggests the possible\ngeneralization to a nerve theorem for a certain class of dependent type\ntheories",
    "descriptor": "",
    "authors": [
      "Thibaut Benjamin",
      "Eric Finster",
      "Samuel Mimram"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2106.04475"
  },
  {
    "id": "arXiv:2106.04476",
    "title": "One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task  Learning on Semantic Parsing Datasets",
    "abstract": "Semantic parsers map natural language utterances to meaning representations.\nThe lack of a single standard for meaning representations led to the creation\nof a plethora of semantic parsing datasets. To unify different datasets and\ntrain a single model for them, we investigate the use of Multi-Task Learning\n(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,\nOvernight, AMR). We find that an MTL architecture that shares the entire\nnetwork across datasets yields competitive or better parsing accuracies than\nthe single-task baselines, while reducing the total number of parameters by\n68%. We further provide evidence that MTL has also better compositional\ngeneralization than single-task models. We also present a comparison of task\nsampling methods and propose a competitive alternative to widespread\nproportional sampling strategies.",
    "descriptor": "",
    "authors": [
      "Marco Damonte",
      "Emilio Monti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04476"
  },
  {
    "id": "arXiv:2106.04477",
    "title": "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary  Monocular Cameras",
    "abstract": "Synthesizing novel views of dynamic humans from stationary monocular cameras\nis a popular scenario. This is particularly attractive as it does not require\nstatic scenes, controlled environments, or specialized hardware. In contrast to\ntechniques that exploit multi-view observations to constrain the modeling,\ngiven a single fixed viewpoint only, the problem of modeling the dynamic scene\nis significantly more under-constrained and ill-posed. In this paper, we\nintroduce Neural Motion Consensus Flow (MoCo-Flow), a representation that\nmodels the dynamic scene using a 4D continuous time-variant function. The\nproposed representation is learned by an optimization which models a dynamic\nscene that minimizes the error of rendering all observation images. At the\nheart of our work lies a novel optimization formulation, which is constrained\nby a motion consensus regularization on the motion flow. We extensively\nevaluate MoCo-Flow on several datasets that contain human motions of varying\ncomplexity, and compare, both qualitatively and quantitatively, to several\nbaseline methods and variants of our methods. Pretrained model, code, and data\nwill be released for research purposes upon paper acceptance.",
    "descriptor": "",
    "authors": [
      "Xuelin Chen",
      "Weiyu Li",
      "Daniel Cohen-Or",
      "Niloy J. Mitra",
      "Baoquan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04477"
  },
  {
    "id": "arXiv:2106.04480",
    "title": "There Is No Turning Back: A Self-Supervised Approach for  Reversibility-Aware Reinforcement Learning",
    "abstract": "We propose to learn to distinguish reversible from irreversible actions for\nbetter informed decision-making in Reinforcement Learning (RL). From\ntheoretical considerations, we show that approximate reversibility can be\nlearned through a simple surrogate task: ranking randomly sampled trajectory\nevents in chronological order. Intuitively, pairs of events that are always\nobserved in the same order are likely to be separated by an irreversible\nsequence of actions. Conveniently, learning the temporal order of events can be\ndone in a fully self-supervised way, which we use to estimate the reversibility\nof actions from experience, without any priors. We propose two different\nstrategies that incorporate reversibility in RL agents, one strategy for\nexploration (RAE) and one strategy for control (RAC). We demonstrate the\npotential of reversibility-aware agents in several environments, including the\nchallenging Sokoban game. In synthetic tasks, we show that we can learn control\npolicies that never fail and reduce to zero the side-effects of interactions,\neven without access to the reward function.",
    "descriptor": "",
    "authors": [
      "Nathan Grinsztajn",
      "Johan Ferret",
      "Olivier Pietquin",
      "Philippe Preux",
      "Matthieu Geist"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04480"
  },
  {
    "id": "arXiv:2106.04483",
    "title": "On the Linear Capacity of Conditional Disclosure of Secrets",
    "abstract": "Conditional disclosure of secrets (CDS) is the problem of disclosing as\nefficiently as possible, one secret from Alice and Bob to Carol if and only if\nthe inputs at Alice and Bob satisfy some function $f$. The information\ntheoretic capacity of CDS is the maximum number of bits of the secret that can\nbe securely disclosed per bit of total communication. All CDS instances, where\nthe capacity is the highest and is equal to $1/2$, are recently characterized\nthrough a noise and signal alignment approach and are described using a graph\nrepresentation of the function $f$. In this work, we go beyond the best case\nscenarios and further develop the alignment approach to characterize the linear\ncapacity of a class of CDS instances to be $(\\rho-1)/(2\\rho)$, where $\\rho$ is\na covering parameter of the graph representation of $f$.",
    "descriptor": "\nComments: 19 pages, 8 figures\n",
    "authors": [
      "Zhou Li",
      "Hua Sun"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04483"
  },
  {
    "id": "arXiv:2106.04484",
    "title": "Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused  Interventions",
    "abstract": "Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.",
    "descriptor": "\nComments: ACL 2021. Our code and data are available at this https URL\n",
    "authors": [
      "Daniel Rosenberg",
      "Itai Gat",
      "Amir Feder",
      "Roi Reichart"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04484"
  },
  {
    "id": "arXiv:2106.04486",
    "title": "Sketch-Based Streaming Anomaly Detection in Dynamic Graphs",
    "abstract": "Given a stream of graph edges from a dynamic graph, how can we assign anomaly\nscores to edges and subgraphs in an online manner, for the purpose of detecting\nunusual behavior, using constant time and memory? For example, in intrusion\ndetection, existing work seeks to detect either anomalous edges or anomalous\nsubgraphs, but not both. In this paper, we first extend the count-min sketch\ndata structure to a higher-order sketch. This higher-order sketch has the\nuseful property of preserving the dense subgraph structure (dense subgraphs in\nthe input turn into dense submatrices in the data structure). We then propose\nfour online algorithms that utilize this enhanced data structure, which (a)\ndetect both edge and graph anomalies; (b) process each edge and graph in\nconstant memory and constant update time per newly arriving edge, and; (c)\noutperform state-of-the-art baselines on four real-world datasets. Our method\nis the first streaming approach that incorporates dense subgraph search to\ndetect graph anomalies in constant memory and time.",
    "descriptor": "",
    "authors": [
      "Siddharth Bhatia",
      "Mohit Wadhwa",
      "Philip S. Yu",
      "Bryan Hooi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04486"
  },
  {
    "id": "arXiv:2106.04487",
    "title": "The Fast Kernel Transform",
    "abstract": "Kernel methods are a highly effective and widely used collection of modern\nmachine learning algorithms. A fundamental limitation of virtually all such\nmethods are computations involving the kernel matrix that naively scale\nquadratically (e.g., constructing the kernel matrix and matrix-vector\nmultiplication) or cubically (solving linear systems) with the size of the data\nset $N.$ We propose the Fast Kernel Transform (FKT), a general algorithm to\ncompute matrix-vector multiplications (MVMs) for datasets in moderate\ndimensions with quasilinear complexity. Typically, analytically grounded fast\nmultiplication methods require specialized development for specific kernels. In\ncontrast, our scheme is based on auto-differentiation and automated symbolic\ncomputations that leverage the analytical structure of the underlying kernel.\nThis allows the FKT to be easily applied to a broad class of kernels, including\nGaussian, Matern, and Rational Quadratic covariance functions and physically\nmotivated Green's functions, including those of the Laplace and Helmholtz\nequations. Furthermore, the FKT maintains a high, quantifiable, and\ncontrollable level of accuracy -- properties that many acceleration methods\nlack. We illustrate the efficacy and versatility of the FKT by providing timing\nand accuracy benchmarks and by applying it to scale the stochastic neighborhood\nembedding (t-SNE) and Gaussian processes to large real-world data sets.",
    "descriptor": "",
    "authors": [
      "John Paul Ryan",
      "Sebastian Ament",
      "Carla P. Gomes",
      "Anil Damle"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04487"
  },
  {
    "id": "arXiv:2106.04488",
    "title": "Low-Rank Subspaces in GANs",
    "abstract": "The latent space of a Generative Adversarial Network (GAN) has been shown to\nencode rich semantics within some subspaces. To identify these subspaces,\nresearchers typically analyze the statistical information from a collection of\nsynthesized data, and the identified subspaces tend to control image attributes\nglobally (i.e., manipulating an attribute causes the change of an entire\nimage). By contrast, this work introduces low-rank subspaces that enable more\nprecise control of GAN generation. Concretely, given an arbitrary image and a\nregion of interest (e.g., eyes of face images), we manage to relate the latent\nspace to the image region with the Jacobian matrix and then use low-rank\nfactorization to discover steerable latent subspaces. There are three\ndistinguishable strengths of our approach that can be aptly called LowRankGAN.\nFirst, compared to analytic algorithms in prior work, our low-rank\nfactorization of Jacobians is able to find the low-dimensional representation\nof attribute manifold, making image editing more precise and controllable.\nSecond, low-rank factorization naturally yields a null space of attributes such\nthat moving the latent code within it only affects the outer region of\ninterest. Therefore, local image editing can be simply achieved by projecting\nan attribute vector into the null space without relying on a spatial mask as\nexisting methods do. Third, our method can robustly work with a local region\nfrom one image for analysis yet well generalize to other images, making it much\neasy to use in practice. Extensive experiments on state-of-the-art GAN models\n(including StyleGAN2 and BigGAN) trained on various datasets demonstrate the\neffectiveness of our LowRankGAN.",
    "descriptor": "",
    "authors": [
      "Jiapeng Zhu",
      "Ruili Feng",
      "Yujun Shen",
      "Deli Zhao",
      "Zhengjun Zha",
      "Jingren Zhou",
      "Qifeng Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04488"
  },
  {
    "id": "arXiv:2106.04489",
    "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared  Hypernetworks",
    "abstract": "State-of-the-art parameter-efficient fine-tuning methods rely on introducing\nadapter modules between the layers of a pretrained language model. However,\nsuch modules are trained separately for each task and thus do not enable\nsharing information across tasks. In this paper, we show that we can learn\nadapter parameters for all layers and tasks by generating them using shared\nhypernetworks, which condition on task, adapter position, and layer id in a\ntransformer model. This parameter-efficient multi-task learning framework\nallows us to achieve the best of both worlds by sharing knowledge across tasks\nvia hypernetworks while enabling the model to adapt to each individual task\nthrough task-specific adapters. Experiments on the well-known GLUE benchmark\nshow improved performance in multi-task learning while adding only 0.29%\nparameters per task. We additionally demonstrate substantial performance\nimprovements in few-shot domain generalization across a variety of tasks. Our\ncode is publicly available in https://github.com/rabeehk/hyperformer.",
    "descriptor": "\nComments: accepted in ACL, 2021\n",
    "authors": [
      "Rabeeh Karimi Mahabadi",
      "Sebastian Ruder",
      "Mostafa Dehghani",
      "James Henderson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04489"
  },
  {
    "id": "arXiv:2106.04493",
    "title": "A Deep Value-network Based Approach for Multi-Driver Order Dispatching",
    "abstract": "Recent works on ride-sharing order dispatching have highlighted the\nimportance of taking into account both the spatial and temporal dynamics in the\ndispatching process for improving the transportation system efficiency. At the\nsame time, deep reinforcement learning has advanced to the point where it\nachieves superhuman performance in a number of fields. In this work, we propose\na deep reinforcement learning based solution for order dispatching and we\nconduct large scale online A/B tests on DiDi's ride-dispatching platform to\nshow that the proposed method achieves significant improvement on both total\ndriver income and user experience related metrics. In particular, we model the\nride dispatching problem as a Semi Markov Decision Process to account for the\ntemporal aspect of the dispatching actions. To improve the stability of the\nvalue iteration with nonlinear function approximators like neural networks, we\npropose Cerebellar Value Networks (CVNet) with a novel distributed state\nrepresentation layer. We further derive a regularized policy evaluation scheme\nfor CVNet that penalizes large Lipschitz constant of the value network for\nadditional robustness against adversarial perturbation and noises. Finally, we\nadapt various transfer learning methods to CVNet for increased learning\nadaptability and efficiency across multiple cities. We conduct extensive\noffline simulations based on real dispatching data as well as online AB tests\nthrough the DiDi's platform. Results show that CVNet consistently outperforms\nother recently proposed dispatching methods. We finally show that the\nperformance can be further improved through the efficient use of transfer\nlearning.",
    "descriptor": "\nComments: KDD 2019 Oral\n",
    "authors": [
      "Xiaocheng Tang",
      "Zhiwei Qin",
      "Fan Zhang",
      "Zhaodong Wang",
      "Zhe Xu",
      "Yintai Ma",
      "Hongtu Zhu",
      "Jieping Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04493"
  },
  {
    "id": "arXiv:2106.04494",
    "title": "Optimization of Service Addition in Multilevel Index Model for Edge  Computing",
    "abstract": "With the development of Edge Computing and Artificial Intelligence (AI)\ntechnologies, edge devices are witnessed to generate data at unprecedented\nvolume. The Edge Intelligence (EI) has led to the emergence of edge devices in\nvarious application domains. The EI can provide efficient services to\ndelay-sensitive applications, where the edge devices are deployed as edge nodes\nto host the majority of execution, which can effectively manage services and\nimprove service discovery efficiency. The multilevel index model is a\nwell-known model used for indexing service, such a model is being introduced\nand optimized in the edge environments to efficiently services discovery whilst\nmanaging large volumes of data. However, effectively updating the multilevel\nindex model by adding new services timely and precisely in the dynamic Edge\nComputing environments is still a challenge. Addressing this issue, this paper\nproposes a designated key selection method to improve the efficiency of adding\nservices in the multilevel index models. Our experimental results show that in\nthe partial index and the full index of multilevel index model, our method\nreduces the service addition time by around 84% and 76%, respectively when\ncompared with the original key selection method and by around 78% and 66%,\nrespectively when compared with the random selection method. Our proposed\nmethod significantly improves the service addition efficiency in the multilevel\nindex model, when compared with existing state-of-the-art key selection\nmethods, without compromising the service retrieval stability to any notable\nlevel.",
    "descriptor": "",
    "authors": [
      "Jiayan Gu",
      "Yan Wu",
      "Ashiq Anjum",
      "John Panneerselvam",
      "Yao Lu",
      "Bo Yuan"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.04494"
  },
  {
    "id": "arXiv:2106.04496",
    "title": "Towards a Theoretical Framework of Out-of-Distribution Generalization",
    "abstract": "Generalization to out-of-distribution (OOD) data, or domain generalization,\nis one of the central problems in modern machine learning. Recently, there is a\nsurge of attempts to propose algorithms for OOD that mainly build upon the idea\nof extracting invariant features. Although intuitively reasonable, theoretical\nunderstanding of what kind of invariance can guarantee OOD generalization is\nstill limited, and generalization to arbitrary out-of-distribution is clearly\nimpossible. In this work, we take the first step towards rigorous and\nquantitative definitions of 1) what is OOD; and 2) what does it mean by saying\nan OOD problem is learnable. We also introduce a new concept of expansion\nfunction, which characterizes to what extent the variance is amplified in the\ntest domains over the training domains, and therefore give a quantitative\nmeaning of invariant features. Based on these, we prove OOD generalization\nerror bounds. It turns out that OOD generalization largely depends on the\nexpansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),\nany OOD learning algorithm without a model selection module is incomplete. Our\ntheory naturally induces a model selection criterion. Extensive experiments on\nbenchmark OOD datasets demonstrate that our model selection criterion has a\nsignificant advantage over baselines.",
    "descriptor": "",
    "authors": [
      "Haotian Ye",
      "Chuanlong Xie",
      "Tianle Cai",
      "Ruichen Li",
      "Zhenguo Li",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04496"
  },
  {
    "id": "arXiv:2106.04499",
    "title": "Towards Practical Credit Assignment for Deep Reinforcement Learning",
    "abstract": "Credit assignment is a fundamental problem in reinforcement learning, the\nproblem of measuring an action's influence on future rewards. Improvements in\ncredit assignment methods have the potential to boost the performance of RL\nalgorithms on many tasks, but thus far have not seen widespread adoption.\nRecently, a family of methods called Hindsight Credit Assignment (HCA) was\nproposed, which explicitly assign credit to actions in hindsight based on the\nprobability of the action having led to an observed outcome. This approach is\nappealing as a means to more efficient data usage, but remains a largely\ntheoretical idea applicable to a limited set of tabular RL tasks, and it is\nunclear how to extend HCA to Deep RL environments. In this work, we explore the\nuse of HCA-style credit in a deep RL context. We first describe the limitations\nof existing HCA algorithms in deep RL, then propose several\ntheoretically-justified modifications to overcome them. Based on this\nexploration, we present a new algorithm, Credit-Constrained Advantage\nActor-Critic (C2A2C), which ignores policy updates for actions which don't\naffect future outcomes based on credit in hindsight, while updating the policy\nas normal for those that do. We find that C2A2C outperforms Advantage\nActor-Critic (A2C) on the Arcade Learning Environment (ALE) benchmark, showing\nbroad improvements over A2C and motivating further work on credit-constrained\nupdate rules for deep RL methods.",
    "descriptor": "\nComments: 9 pages plus 7 page appendix\n",
    "authors": [
      "Vyacheslav Alipov",
      "Riley Simmons-Edler",
      "Nikita Putintsev",
      "Pavel Kalinin",
      "Dmitry Vetrov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04499"
  },
  {
    "id": "arXiv:2106.04502",
    "title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections  to Weight-Sharing",
    "abstract": "Tuning hyperparameters is a crucial but arduous part of the machine learning\npipeline. Hyperparameter optimization is even more challenging in federated\nlearning, where models are learned over a distributed network of heterogeneous\ndevices; here, the need to keep data on device and perform local training makes\nit difficult to efficiently train and evaluate configurations. In this work, we\ninvestigate the problem of federated hyperparameter tuning. We first identify\nkey challenges and show how standard approaches may be adapted to form\nbaselines for the federated setting. Then, by making a novel connection to the\nneural architecture search technique of weight-sharing, we introduce a new\nmethod, FedEx, to accelerate federated hyperparameter tuning that is applicable\nto widely-used federated optimization methods such as FedAvg and recent\nvariants. Theoretically, we show that a FedEx variant correctly tunes the\non-device learning rate in the setting of online convex optimization across\ndevices. Empirically, we show that FedEx can outperform natural baselines for\nfederated hyperparameter tuning by several percentage points on the\nShakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using\nthe same training budget.",
    "descriptor": "",
    "authors": [
      "Mikhail Khodak",
      "Renbo Tu",
      "Tian Li",
      "Liam Li",
      "Maria-Florina Balcan",
      "Virginia Smith",
      "Ameet Talwalkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04502"
  },
  {
    "id": "arXiv:2106.04506",
    "title": "Cyberbullying Detection Using Deep Neural Network from Social Media  Comments in Bangla Language",
    "abstract": "Cyberbullying or Online harassment detection on social media for various\nmajor languages is currently being given a good amount of focus by researchers\nworldwide. Being the seventh most speaking language in the world and increasing\nusage of online platform among the Bengali speaking people urge to find\neffective detection technique to handle the online harassment. In this paper,\nwe have proposed binary and multiclass classification model using hybrid neural\nnetwork for bully expression detection in Bengali language. We have used 44,001\nusers comments from popular public Facebook pages, which fall into five classes\n- Non-bully, Sexual, Threat, Troll and Religious. We have examined the\nperformance of our proposed models from different perspective. Our binary\nclassification model gives 87.91% accuracy, whereas introducing ensemble\ntechnique after neural network for multiclass classification, we got 85%\naccuracy.",
    "descriptor": "\nComments: 9 pages, 9 figures, 3 tables\n",
    "authors": [
      "Md Faisal Ahmed",
      "Zalish Mahmud",
      "Zarin Tasnim Biash",
      "Ahmed Ann Noor Ryen",
      "Arman Hossain",
      "Faisal Bin Ashraf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.04506"
  },
  {
    "id": "arXiv:2106.04511",
    "title": "Designing Toxic Content Classification for a Diversity of Perspectives",
    "abstract": "In this work, we demonstrate how existing classifiers for identifying toxic\ncomments online fail to generalize to the diverse concerns of Internet users.\nWe survey 17,280 participants to understand how user expectations for what\nconstitutes toxic content differ across demographics, beliefs, and personal\nexperiences. We find that groups historically at-risk of harassment - such as\npeople who identify as LGBTQ+ or young adults - are more likely to to flag a\nrandom comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who\nhave personally experienced harassment in the past. Based on our findings, we\nshow how current one-size-fits-all toxicity classification algorithms, like the\nPerspective API from Jigsaw, can improve in accuracy by 86% on average through\npersonalized model tuning. Ultimately, we highlight current pitfalls and new\ndesign directions that can improve the equity and efficacy of toxic content\nclassifiers for all users.",
    "descriptor": "",
    "authors": [
      "Deepak Kumar",
      "Patrick Gage Kelley",
      "Sunny Consolvo",
      "Joshua Mason",
      "Elie Bursztein",
      "Zakir Durumeric",
      "Kurt Thomas",
      "Michael Bailey"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04511"
  },
  {
    "id": "arXiv:2106.04512",
    "title": "Formal Verification of a Map Merging Protocol in the Multi-Agent  Programming Contest",
    "abstract": "Communication is a critical part of enabling multi-agent systems to\ncooperate. This means that applying formal methods to protocols governing\ncommunication within multi-agent systems provides useful confidence in its\nreliability. In this paper, we describe the formal verification of a complex\ncommunication protocol that coordinates agents merging maps of their\nenvironment. The protocol was used by the LFC team in the 2019 edition of the\nMulti-Agent Programming Contest (MAPC). Our specification of the protocol is\nwritten in Communicating Sequential Processes (CSP), which is well-suited\napproach to specifying agent communication protocols due to its focus on\nconcurrent communicating systems. We validate the specification's behaviour\nusing five scenarios where the correct behaviour is known, and verify that\neventually all the maps have merged.",
    "descriptor": "\nComments: EMAS 2021 Workshop Version\n",
    "authors": [
      "Matt Luckcuck",
      "Rafael C. Cardoso"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.04512"
  },
  {
    "id": "arXiv:2106.04513",
    "title": "Identifying Linked Fraudulent Activities Using GraphConvolution Network",
    "abstract": "In this paper, we present a novel approach to identify linked fraudulent\nactivities or actors sharing similar attributes, using Graph Convolution\nNetwork (GCN). These linked fraudulent activities can be visualized as graphs\nwith abstract concepts like relationships and interactions, which makes GCNs an\nideal solution to identify the graph edges which serve as links between\nfraudulent nodes. Traditional approaches like community detection require\nstrong links between fraudulent attempts like shared attributes to find\ncommunities and the supervised solutions require large amount of training data\nwhich may not be available in fraud scenarios and work best to provide binary\nseparation between fraudulent and non fraudulent activities. Our approach\novercomes the drawbacks of traditional methods as GCNs simply learn\nsimilarities between fraudulent nodes to identify clusters of similar attempts\nand require much smaller dataset to learn. We demonstrate our results on linked\naccounts with both strong and weak links to identify fraud rings with high\nconfidence. Our results outperform label propagation community detection and\nsupervised GBTs algorithms in terms of solution quality and computation time.",
    "descriptor": "",
    "authors": [
      "Sharmin Pathan",
      "Vyom Shrivastava"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04513"
  },
  {
    "id": "arXiv:2106.04514",
    "title": "GearV: A Two-Gear Hypervisor for Mixed-Criticality IoT Systems",
    "abstract": "This paper presents GearV, a two-gear lightweight hypervisor architecture to\naddress the some known challenges. By dividing hypervisor into some partitions,\nand dividing scheduling policies into Gear1 and Gear2 respectively, GearV\ncreates a consolidated platform to run best-effort system and safety-critical\nsystem simultaneously with managed engineering effort. The two-gears\narchitecture also simplifies retrofitting the virtualization systems. We\nbelieve that GearV can serves as a reasonable hypervisor architecture for the\nmix-critical IoT systems.",
    "descriptor": "\nComments: 12 pages, 8 figures, 11 tables\n",
    "authors": [
      "Kaiwen Long",
      "Chong Xing",
      "Yuebin Qi",
      "Pei Zhang",
      "Changsong Wu",
      "Wenxiao Fang",
      "Jing Tan",
      "Jie Chen",
      "Shiming Zhang",
      "Zuosheng Wang",
      "Zuanmin Liu",
      "Cao Liang",
      "Jiaxiang Xu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2106.04514"
  },
  {
    "id": "arXiv:2106.04515",
    "title": "Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in  North Carolina",
    "abstract": "Coronavirus disease (COVID-19) pandemic has changed various aspects of\npeople's lives and behaviors. At this stage, there are no other ways to control\nthe natural progression of the disease than adopting mitigation strategies such\nas wearing masks, watching distance, and washing hands. Moreover, at this time\nof social distancing, social media plays a key role in connecting people and\nproviding a platform for expressing their feelings. In this study, we tap into\nsocial media to surveil the uptake of mitigation and detection strategies, and\ncapture issues and concerns about the pandemic. In particular, we explore the\nresearch question, \"how much can be learned regarding the public uptake of\nmitigation strategies and concerns about COVID-19 pandemic by using natural\nlanguage processing on Reddit posts?\" After extracting COVID-related posts from\nthe four largest subreddit communities of North Carolina over six months, we\nperformed NLP-based preprocessing to clean the noisy data. We employed a custom\nNamed-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)\nmethod for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',\nand 'testing' are the most prevalent named-entities for \"Personal Protective\nEquipment\", \"symptoms\", and \"testing\" categories, respectively. We also\nobserved that the most discussed topics are related to testing, masks, and\nemployment. The mitigation measures are the most prevalent theme of discussion\nacross all subreddits.",
    "descriptor": "\nComments: 12 pages, 6 figures, 7 tables, to be published in ACM-BCB 2021\n",
    "authors": [
      "Christopher Whitfield",
      "Yang Liu",
      "Mohad Anwar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04515"
  },
  {
    "id": "arXiv:2106.04516",
    "title": "Launchpad: A Programming Model for Distributed Machine Learning Research",
    "abstract": "A major driver behind the success of modern machine learning algorithms has\nbeen their ability to process ever-larger amounts of data. As a result, the use\nof distributed systems in both research and production has become increasingly\nprevalent as a means to scale to this growing data. At the same time, however,\ndistributing the learning process can drastically complicate the implementation\nof even simple algorithms. This is especially problematic as many machine\nlearning practitioners are not well-versed in the design of distributed\nsystems, let alone those that have complicated communication topologies. In\nthis work we introduce Launchpad, a programming model that simplifies the\nprocess of defining and launching distributed systems that is specifically\ntailored towards a machine learning audience. We describe our framework, its\ndesign philosophy and implementation, and give a number of examples of common\nlearning algorithms whose designs are greatly simplified by this approach.",
    "descriptor": "",
    "authors": [
      "Fan Yang",
      "Gabriel Barth-Maron",
      "Piotr Sta\u0144czyk",
      "Matthew Hoffman",
      "Siqi Liu",
      "Manuel Kroiss",
      "Aedan Pope",
      "Alban Rrustemi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.04516"
  },
  {
    "id": "arXiv:2106.04517",
    "title": "Assessing Open Interfaces and Protocols of PLCs for Computation  Offloading at Field Level",
    "abstract": "Programmable logic controllers (PLCs) are the core element of industrial\nplants in todays deployments. They read sensor values, execute control\nalgorithms, and write output values. Furthermore, industrial plants have\nlifetimes of one or more decades. Thus, in a realistic Industry 4.0 scenario,\nthese devices have to be integrated in novel systems. In order to apply\nadvanced concepts and technologies, such as computation offloading, which\nrequires data exchange between PLCs and edge cloud, we investigate open\ncommunication interfaces of two typical PLCs of Siemens S7 series. Hence, each\nof the interfaces is analyzed based on plug & play capability, if metadata is\nprovided, protocol efficiency, and performance. For the latter, the smallest\npossible update time for each of the interfaces will be measured.",
    "descriptor": "",
    "authors": [
      "Michael Gundall",
      "Hans Dieter Schotten"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.04517"
  },
  {
    "id": "arXiv:2106.04520",
    "title": "MViT: Mask Vision Transformer for Facial Expression Recognition in the  wild",
    "abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging\ntask in computer vision due to variant backgrounds, low-quality facial images,\nand the subjectiveness of annotators. These uncertainties make it difficult for\nneural networks to learn robust features on limited-scale datasets. Moreover,\nthe networks can be easily distributed by the above factors and perform\nincorrect decisions. Recently, vision transformer (ViT) and data-efficient\nimage transformers (DeiT) present their significant performance in traditional\nclassification tasks. The self-attention mechanism makes transformers obtain a\nglobal receptive field in the first layer which dramatically enhances the\nfeature extraction capability. In this work, we first propose a novel pure\ntransformer-based mask vision transformer (MViT) for FER in the wild, which\nconsists of two modules: a transformer-based mask generation network (MGN) to\ngenerate a mask that can filter out complex backgrounds and occlusion of face\nimages, and a dynamic relabeling module to rectify incorrect labels in FER\ndatasets in the wild. Extensive experimental results demonstrate that our MViT\noutperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with\n89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable\nresult on AffectNet-8 with 61.40%.",
    "descriptor": "\nComments: 11 pages, 6 figures, conference, 5 tables\n",
    "authors": [
      "Hanting Li",
      "Mingzhe Sui",
      "Feng Zhao",
      "Zhengjun Zha",
      "Feng Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04520"
  },
  {
    "id": "arXiv:2106.04521",
    "title": "An App for Visual Exploration, Discovery, and Sharing of Poncelet  3-Periodic Phenomena",
    "abstract": "We describe a browser-based application built for the real-time visualization\nof the beauteous dynamic geometry of Poncelet 3-periodic families. The focus is\non highly responsive, visually smooth, \"live\" experimentation with old and new\nphenomena involving loci of triangle centers and/or metric invariants. Another\nfocus is on the production of beautiful color-filled images of loci. Once a\nlive browser-based simulation is defined, it can be easily shared with\ncolleagues and/or inserted as links in publications, eliminating the need of\ntime-consuming video production and uploads.",
    "descriptor": "\nComments: 19 pages, 20 figures\n",
    "authors": [
      "Iverton Darlan",
      "Dan Reznik"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computational Geometry (cs.CG)",
      "Dynamical Systems (math.DS)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2106.04521"
  },
  {
    "id": "arXiv:2106.04525",
    "title": "A critical look at the current train/test split in machine learning",
    "abstract": "The randomized or cross-validated split of training and testing sets has been\nadopted as the gold standard of machine learning for decades. The establishment\nof these split protocols are based on two assumptions: (i)-fixing the dataset\nto be eternally static so we could evaluate different machine learning\nalgorithms or models; (ii)-there is a complete set of annotated data available\nto researchers or industrial practitioners. However, in this article, we intend\nto take a closer and critical look at the split protocol itself and point out\nits weakness and limitation, especially for industrial applications. In many\nreal-world problems, we must acknowledge that there are numerous situations\nwhere assumption (ii) does not hold. For instance, for interdisciplinary\napplications like drug discovery, it often requires real lab experiments to\nannotate data which poses huge costs in both time and financial considerations.\nIn other words, it can be very difficult or even impossible to satisfy\nassumption (ii). In this article, we intend to access this problem and\nreiterate the paradigm of active learning, and investigate its potential on\nsolving problems under unconventional train/test split protocols. We further\npropose a new adaptive active learning architecture (AAL) which involves an\nadaptation policy, in comparison with the traditional active learning that only\nunidirectionally adds data points to the training pool. We primarily justify\nour points by extensively investigating an interdisciplinary drug-protein\nbinding problem. We additionally evaluate AAL on more conventional machine\nlearning benchmarking datasets like CIFAR-10 to demonstrate the\ngeneralizability and efficacy of the new framework.",
    "descriptor": "",
    "authors": [
      "Jimin Tan",
      "Jianan Yang",
      "Sai Wu",
      "Gang Chen",
      "Jake Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04525"
  },
  {
    "id": "arXiv:2106.04527",
    "title": "LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised  Classification",
    "abstract": "Semi-supervised learning has received a lot of recent attention as it\nalleviates the need for large amounts of labelled data which can often be\nexpensive, requires expert knowledge and be time consuming to collect. Recent\ndevelopments in deep semi-supervised classification have reached unprecedented\nperformance and the gap between supervised and semi-supervised learning is\never-decreasing. This improvement in performance has been based on the\ninclusion of numerous technical tricks, strong augmentation techniques and\ncostly optimisation schemes with multi-term loss functions. We propose a new\nframework, LaplaceNet, for deep semi-supervised classification that has a\ngreatly reduced model complexity. We utilise a hybrid energy-neural network\nwhere graph based pseudo-labels, generated by minimising the graphical\nLaplacian, are used to iteratively improve a neural-network backbone. Our model\noutperforms state-of-the-art methods for deep semi-supervised classification,\nover several benchmark datasets. Furthermore, we consider the application of\nstrong-augmentations to neural networks theoretically and justify the use of a\nmulti-sampling approach for semi-supervised learning. We demonstrate, through\nrigorous experimentation, that a multi-sampling augmentation approach improves\ngeneralisation and reduces the sensitivity of the network to augmentation.",
    "descriptor": "",
    "authors": [
      "Philip Sellars",
      "Angelica I. Aviles-Rivero",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04527"
  },
  {
    "id": "arXiv:2106.04530",
    "title": "Learning from Multiple Noisy Partial Labelers",
    "abstract": "Programmatic weak supervision creates models without hand-labeled training\ndata by combining the outputs of noisy, user-written rules and other heuristic\nlabelers. Existing frameworks make the restrictive assumption that labelers\noutput a single class label. Enabling users to create partial labelers that\noutput subsets of possible class labels would greatly expand the expressivity\nof programmatic weak supervision. We introduce this capability by defining a\nprobabilistic generative model that can estimate the underlying accuracies of\nmultiple noisy partial labelers without ground truth labels. We prove that this\nclass of models is generically identifiable up to label swapping under mild\nconditions. We also show how to scale up learning to 100k examples in one\nminute, a 300X speed up compared to a naive implementation. We evaluate our\nframework on three text classification and six object classification tasks. On\ntext tasks, adding partial labels increases average accuracy by 9.6 percentage\npoints. On image tasks, we show that partial labels allow us to approach some\nzero-shot object classification problems with programmatic weak supervision by\nusing class attributes as partial labelers. Our framework is able to achieve\naccuracy comparable to recent embedding-based zero-shot learning methods using\nonly pre-trained attribute detectors",
    "descriptor": "",
    "authors": [
      "Peilin Yu",
      "Tiffany Ding",
      "Stephen H. Bach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04530"
  },
  {
    "id": "arXiv:2106.04531",
    "title": "RobustNav: Towards Benchmarking Robustness in Embodied Navigation",
    "abstract": "As an attempt towards assessing the robustness of embodied navigation agents,\nwe propose RobustNav, a framework to quantify the performance of embodied\nnavigation agents when exposed to a wide variety of visual - affecting RGB\ninputs - and dynamics - affecting transition dynamics - corruptions. Most\nrecent efforts in visual navigation have typically focused on generalizing to\nnovel target environments with similar appearance and dynamics characteristics.\nWith RobustNav, we find that some standard embodied navigation agents\nsignificantly underperform (or fail) in the presence of visual or dynamics\ncorruptions. We systematically analyze the kind of idiosyncrasies that emerge\nin the behavior of such agents when operating under corruptions. Finally, for\nvisual corruptions in RobustNav, we show that while standard techniques to\nimprove robustness such as data-augmentation and self-supervised adaptation\noffer some zero-shot resistance and improvements in navigation performance,\nthere is still a long way to go in terms of recovering lost performance\nrelative to clean \"non-corrupt\" settings, warranting more research in this\ndirection. Our code is available at https://github.com/allenai/robustnav",
    "descriptor": "\nComments: 18 pages, 8 figures, Code: this https URL\n",
    "authors": [
      "Prithvijit Chattopadhyay",
      "Judy Hoffman",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04531"
  },
  {
    "id": "arXiv:2106.04533",
    "title": "Chasing Sparsity in Vision Transformers:An End-to-End Exploration",
    "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but\ntheir enormous model sizes and training costs remain daunting. Conventional\npost-training pruning often incurs higher training budgets. In contrast, this\npaper aims to trim down both the training memory overhead and the inference\ncomplexity, without scarifying the achievable accuracy. We launch and report\nthe first-of-its-kind comprehensive exploration, on taking a unified approach\nof integrating sparsity in ViTs \"from end to end\". Specifically, instead of\ntraining full ViTs, we dynamically extract and train sparse subnetworks, while\nsticking to a fixed small parameter budget. Our approach jointly optimizes\nmodel parameters and explores connectivity throughout training, ending up with\none sparse network as the final output. The approach is seamlessly extended\nfrom unstructured to structured sparsity, the latter by considering to guide\nthe prune-and-grow of self-attention heads inside ViTs. For additional\nefficiency gains, we further co-explore data and architecture sparsity, by\nplugging in a novel learnable token selector to adaptively determine the\ncurrently most vital patches. Extensive results validate the effectiveness of\nour proposals on ImageNet with diverse ViT backbones. For instance, at 40%\nstructured sparsity, our sparsified DeiT-Base can achieve 0.42% accuracy gain,\nat 33.13% and 24.70% running time} savings, compared to its dense counterpart.\nPerhaps most surprisingly, we find that the proposed sparse (co-)training can\neven improve the ViT accuracy rather than compromising it, making sparsity a\ntantalizing \"free lunch\". For example, our sparsified DeiT-Small at 5%, 50%\nsparsity for (data, architecture), improves 0.28% top-1 accuracy and meanwhile\nenjoys 49.32% FLOPs and 4.40% running time savings.",
    "descriptor": "",
    "authors": [
      "Tianlong Chen",
      "Yu Cheng",
      "Zhe Gan",
      "Lu Yuan",
      "Lei Zhang",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04533"
  },
  {
    "id": "arXiv:2106.04534",
    "title": "High moment and pathwise error estimates for fully discrete mixed finite  element approximations of the Stochastic Stokes Equations with Multiplicative  Noises",
    "abstract": "This paper is concerned with high moment and pathwise error estimates for\nboth velocity and pressure approximations of the Euler-Maruyama scheme for time\ndiscretization and its two fully discrete mixed finite element discretizations.\nThe main idea for deriving the high moment error estimates for the velocity\napproximation is to use a bootstrap technique starting from the second moment\nerror estimate. The pathwise error estimate, which is sub-optimal in the energy\nnorm, is obtained by using Kolmogorov's theorem based on the high moment error\nestimates. Unlike for the velocity error estimate, the higher moment and\npathwise error estimates for the pressure approximation are derived in a\ntime-averaged norm. In addition, the impact of noise types on the rates of\nconvergence for both velocity and pressure approximations is also addressed.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Liet Vo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.04534"
  },
  {
    "id": "arXiv:2106.04537",
    "title": "Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with  Recurrent Networks",
    "abstract": "Deep neural networks are powerful machines for visual pattern recognition,\nbut reasoning tasks that are easy for humans may still be difficult for neural\nmodels. Humans possess the ability to extrapolate reasoning strategies learned\non simple problems to solve harder examples, often by thinking for longer. For\nexample, a person who has learned to solve small mazes can easily extend the\nvery same search techniques to solve much larger mazes by spending more time.\nIn computers, this behavior is often achieved through the use of algorithms,\nwhich scale to arbitrarily hard problem instances at the cost of more\ncomputation. In contrast, the sequential computing budget of feed-forward\nneural networks is limited by their depth, and networks trained on simple\nproblems have no way of extending their reasoning to accommodate harder\nproblems. In this work, we show that recurrent networks trained to solve simple\nproblems with few recurrent steps can indeed solve much more complex problems\nsimply by performing additional recurrences during inference. We demonstrate\nthis algorithmic behavior of recurrent networks on prefix sum computation,\nmazes, and chess. In all three domains, networks trained on simple problem\ninstances are able to extend their reasoning abilities at test time simply by\n\"thinking for longer.\"",
    "descriptor": "",
    "authors": [
      "Avi Schwarzschild",
      "Eitan Borgnia",
      "Arjun Gupta",
      "Furong Huang",
      "Uzi Vishkin",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04537"
  },
  {
    "id": "arXiv:2106.04538",
    "title": "What Makes Multimodal Learning Better than Single (Provably)",
    "abstract": "The world provides us with data of multiple modalities. Intuitively, models\nfusingdata from different modalities outperform unimodal models, since more\ninformationis aggregated. Recently, joining the success of deep learning, there\nis an influentialline of work on deep multimodal learning, which has remarkable\nempirical resultson various applications. However, theoretical justifications\nin this field are notablylacking.Can multimodal provably perform better than\nunimodal? In this paper, we answer this question under a most popular\nmultimodal learningframework, which firstly encodes features from different\nmodalities into a commonlatent space and seamlessly maps the latent\nrepresentations into the task space. Weprove that learning with multiple\nmodalities achieves a smaller population risk thanonly using its subset of\nmodalities. The main intuition is that the former has moreaccurate estimate of\nthe latent space representation. To the best of our knowledge,this is the first\ntheoretical treatment to capture important qualitative phenomenaobserved in\nreal multimodal applications. Combining with experiment results, weshow that\nmultimodal learning does possess an appealing formal guarantee.",
    "descriptor": "\nComments: 15 pages, 2 figures\n",
    "authors": [
      "Yu Huang",
      "Chenzhuang Du",
      "Zihui Xue",
      "Xuanyao Chen",
      "Hang Zhao",
      "Longbo Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04538"
  },
  {
    "id": "arXiv:2106.04546",
    "title": "LEADS: Learning Dynamical Systems that Generalize Across Environments",
    "abstract": "When modeling dynamical systems from real-world data samples, the\ndistribution of data often changes according to the environment in which they\nare captured, and the dynamics of the system itself vary from one environment\nto another. Generalizing across environments thus challenges the conventional\nframeworks. The classical settings suggest either considering data as i.i.d.\nand learning a single model to cover all situations or learning\nenvironment-specific models. Both are sub-optimal: the former disregards the\ndiscrepancies between environments leading to biased solutions, while the\nlatter does not exploit their potential commonalities and is prone to scarcity\nproblems. We propose LEADS, a novel framework that leverages the commonalities\nand discrepancies among known environments to improve model generalization.\nThis is achieved with a tailored training formulation aiming at capturing\ncommon dynamics within a shared model while additional terms capture\nenvironment-specific dynamics. We ground our approach in theory, exhibiting a\ndecrease in sample complexity with our approach and corroborate these results\nempirically, instantiating it for linear dynamics. Moreover, we concretize this\nframework for neural networks and evaluate it experimentally on representative\nfamilies of nonlinear dynamics. We show that this new setting can exploit\nknowledge extracted from environment-dependent data and improves generalization\nfor both known and novel environments.",
    "descriptor": "",
    "authors": [
      "Yuan Yin",
      "Ibrahim Ayed",
      "Emmanuel de B\u00e9zenac",
      "Nicolas Baskiotis",
      "Patrick Gallinari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04546"
  },
  {
    "id": "arXiv:2106.04547",
    "title": "Automatic Generation of Machine Learning Synthetic Data Using ROS",
    "abstract": "Data labeling is a time intensive process. As such, many data scientists use\nvarious tools to aid in the data generation and labeling process. While these\ntools help automate labeling, many still require user interaction throughout\nthe process. Additionally, most target only a few network frameworks. Any\nresearchers exploring multiple frameworks must find additional tools orwrite\nconversion scripts. This paper presents an automated tool for generating\nsynthetic data in arbitrary network formats. It uses Robot Operating System\n(ROS) and Gazebo, which are common tools in the robotics community. Through ROS\nparadigms, it allows extensive user customization of the simulation environment\nand data generation process. Additionally, a plugin-like framework allows the\ndevelopment of arbitrary data format writers without the need to change the\nmain body of code. Using this tool, the authors were able to generate an\narbitrarily large image dataset for three unique training formats using\napproximately 15 min of user setup time and a variable amount of hands-off run\ntime, depending on the dataset size. The source code for this data generation\ntool is available at https://github.com/Navy-RISE-Lab/nn_data_collection",
    "descriptor": "\nComments: DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited. NAWCAD-LKE Release Number 2021-72. Published in HCI International 2021 by Springer. The final authenticated version is available online at this https URL\n",
    "authors": [
      "Kyle M. Hart",
      "Ari B. Goodman",
      "Ryan P. O'Shea"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04547"
  },
  {
    "id": "arXiv:2106.04549",
    "title": "KIGLIS: Smart Networks for Smart Cities",
    "abstract": "Smart cities will be characterized by a variety of intelligent and networked\nservices, each with specific requirements for the underlying network\ninfrastructure. While smart city architectures and services have been studied\nextensively, little attention has been paid to the network technology. The\nKIGLIS research project, consisting of a consortium of companies, universities\nand research institutions, focuses on artificial intelligence for optimizing\nfiber-optic networks of a smart city, with a special focus on future mobility\napplications, such as automated driving. In this paper, we present early\nresults on our process of collecting smart city requirements for communication\nnetworks, which will lead towards reference infrastructure and architecture\nsolutions. Finally, we suggest directions in which artificial intelligence will\nimprove smart city networks.",
    "descriptor": "",
    "authors": [
      "Daniel Bogdoll",
      "Patrick Matalla",
      "Christoph F\u00fcllner",
      "Christian Raack",
      "Shi Li",
      "Tobias K\u00e4fer",
      "Stefan Orf",
      "Marc Ren\u00e9 Zofka",
      "Finn Sartoris",
      "Christoph Schweikert",
      "Thomas Pfeiffer",
      "Andr\u00e9 Richter",
      "Sebastian Randel",
      "Rene Bonk"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04549"
  },
  {
    "id": "arXiv:2106.04550",
    "title": "DETReg: Unsupervised Pretraining with Region Priors for Object Detection",
    "abstract": "Unsupervised pretraining has recently proven beneficial for computer vision\ntasks, including object detection. However, previous self-supervised approaches\nare not designed to handle a key aspect of detection: localizing objects. Here,\nwe present DETReg, an unsupervised pretraining approach for object DEtection\nwith TRansformers using Region priors. Motivated by the two tasks underlying\nobject detection: localization and categorization, we combine two complementary\nsignals for self-supervision. For an object localization signal, we use pseudo\nground truth object bounding boxes from an off-the-shelf unsupervised region\nproposal method, Selective Search, which does not require training data and can\ndetect objects at a high recall rate and very low precision. The categorization\nsignal comes from an object embedding loss that encourages invariant object\nrepresentations, from which the object category can be inferred. We show how to\ncombine these two signals to train the Deformable DETR detection architecture\nfrom large amounts of unlabeled data. DETReg improves the performance over\ncompetitive baselines and previous self-supervised methods on standard\nbenchmarks like MS COCO and PASCAL VOC. DETReg also outperforms previous\nsupervised and unsupervised baseline approaches on low-data regime when trained\nwith only 1%, 2%, 5%, and 10% of the labeled data on MS COCO. For code and\npretrained models, visit the project page at https://amirbar.net/detreg",
    "descriptor": "\nComments: preprint, under review\n",
    "authors": [
      "Amir Bar",
      "Xin Wang",
      "Vadim Kantorov",
      "Colorado J Reed",
      "Roei Herzig",
      "Gal Chechik",
      "Anna Rohrbach",
      "Trevor Darrell",
      "Amir Globerson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04550"
  },
  {
    "id": "arXiv:2106.04552",
    "title": "Neural Speaker Embeddings for Ultrasound-based Silent Speech Interfaces",
    "abstract": "Articulatory-to-acoustic mapping seeks to reconstruct speech from a recording\nof the articulatory movements, for example, an ultrasound video. Just like\nspeech signals, these recordings represent not only the linguistic content, but\nare also highly specific to the actual speaker. Hence, due to the lack of\nmulti-speaker data sets, researchers have so far concentrated on\nspeaker-dependent modeling. Here, we present multi-speaker experiments using\nthe recently published TaL80 corpus. To model speaker characteristics, we\nadjusted the x-vector framework popular in speech processing to operate with\nultrasound tongue videos. Next, we performed speaker recognition experiments\nusing 50 speakers from the corpus. Then, we created speaker embedding vectors\nand evaluated them on the remaining speakers. Finally, we examined how the\nembedding vector influences the accuracy of our ultrasound-to-speech conversion\nnetwork in a multi-speaker scenario. In the experiments we attained speaker\nrecognition error rates below 3\\%, and we also found that the embedding vectors\ngeneralize nicely to unseen speakers. Our first attempt to apply them in a\nmulti-speaker silent speech framework brought about a marginal reduction in the\nerror rate of the spectral estimation step.",
    "descriptor": "\nComments: 5 pages, 3 figures, 3 tables\n",
    "authors": [
      "Amin Honarmandi Shandiz",
      "L\u00e1szl\u00f3 T\u00f3th",
      "G\u00e1bor Gosztolya",
      "Alexandra Mark\u00f3",
      "Tam\u00e1s G\u00e1bor Csap\u00f3"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04552"
  },
  {
    "id": "arXiv:2106.04554",
    "title": "A Survey of Transformers",
    "abstract": "Transformers have achieved great success in many artificial intelligence\nfields, such as natural language processing, computer vision, and audio\nprocessing. Therefore, it is natural to attract lots of interest from academic\nand industry researchers. Up to the present, a great variety of Transformer\nvariants (a.k.a. X-formers) have been proposed, however, a systematic and\ncomprehensive literature review on these Transformer variants is still missing.\nIn this survey, we provide a comprehensive review of various X-formers. We\nfirst briefly introduce the vanilla Transformer and then propose a new taxonomy\nof X-formers. Next, we introduce the various X-formers from three perspectives:\narchitectural modification, pre-training, and applications. Finally, we outline\nsome potential directions for future research.",
    "descriptor": "",
    "authors": [
      "Tianyang Lin",
      "Yuxin Wang",
      "Xiangyang Liu",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04554"
  },
  {
    "id": "arXiv:2106.04555",
    "title": "Hierarchical Lov\u00e1sz Embeddings for Proposal-free Panoptic Segmentation",
    "abstract": "Panoptic segmentation brings together two separate tasks: instance and\nsemantic segmentation. Although they are related, unifying them faces an\napparent paradox: how to learn simultaneously instance-specific and\ncategory-specific (i.e. instance-agnostic) representations jointly. Hence,\nstate-of-the-art panoptic segmentation methods use complex models with a\ndistinct stream for each task. In contrast, we propose Hierarchical Lov\\'asz\nEmbeddings, per pixel feature vectors that simultaneously encode instance- and\ncategory-level discriminative information. We use a hierarchical Lov\\'asz hinge\nloss to learn a low-dimensional embedding space structured into a unified\nsemantic and instance hierarchy without requiring separate network branches or\nobject proposals. Besides modeling instances precisely in a proposal-free\nmanner, our Hierarchical Lov\\'asz Embeddings generalize to categories by using\na simple Nearest-Class-Mean classifier, including for non-instance \"stuff\"\nclasses where instance segmentation methods are not applicable. Our simple\nmodel achieves state-of-the-art results compared to existing proposal-free\npanoptic segmentation methods on Cityscapes, COCO, and Mapillary Vistas.\nFurthermore, our model demonstrates temporal stability between video frames.",
    "descriptor": "\nComments: 13 pages, 9 figures, including supplementary material. To be published in CVPR 2021\n",
    "authors": [
      "Tommi Kerola",
      "Jie Li",
      "Atsushi Kanehira",
      "Yasunori Kudo",
      "Alexis Vallet",
      "Adrien Gaidon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04555"
  },
  {
    "id": "arXiv:2106.04559",
    "title": "Turing: an Accurate and Interpretable Multi-Hypothesis Cross-Domain  Natural Language Database Interface",
    "abstract": "A natural language database interface (NLDB) can democratize data-driven\ninsights for non-technical users. However, existing Text-to-SQL semantic\nparsers cannot achieve high enough accuracy in the cross-database setting to\nallow good usability in practice. This work presents Turing, a NLDB system\ntoward bridging this gap. The cross-domain semantic parser of Turing with our\nnovel value prediction method achieves $75.1\\%$ execution accuracy, and\n$78.3\\%$ top-5 beam execution accuracy on the Spider validation set. To benefit\nfrom the higher beam accuracy, we design an interactive system where the SQL\nhypotheses in the beam are explained step-by-step in natural language, with\ntheir differences highlighted. The user can then compare and judge the\nhypotheses to select which one reflects their intention if any. The English\nexplanations of SQL queries in Turing are produced by our high-precision\nnatural language generation system based on synchronous grammars.",
    "descriptor": "\nComments: ACL 2021 demonstration track\n",
    "authors": [
      "Peng Xu",
      "Wenjie Zi",
      "Hamidreza Shahidi",
      "\u00c1kos K\u00e1d\u00e1r",
      "Keyi Tang",
      "Wei Yang",
      "Jawad Ateeq",
      "Harsh Barot",
      "Meidan Alon",
      "Yanshuai Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04559"
  },
  {
    "id": "arXiv:2106.04560",
    "title": "Scaling Vision Transformers",
    "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have\nrecently attained state-of-the-art results on many computer vision benchmarks.\nScale is a primary ingredient in attaining excellent results, therefore,\nunderstanding a model's scaling properties is a key to designing future\ngenerations effectively. While the laws for scaling Transformer language models\nhave been studied, it is unknown how Vision Transformers scale. To address\nthis, we scale ViT models and data, both up and down, and characterize the\nrelationships between error rate, data, and compute. Along the way, we refine\nthe architecture and training of ViT, reducing memory consumption and\nincreasing accuracy the resulting models. As a result, we successfully train a\nViT model with two billion parameters, which attains a new state-of-the-art on\nImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot\nlearning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10\nexamples per class.",
    "descriptor": "\nComments: Xiaohua, Alex, and Lucas contributed equally\n",
    "authors": [
      "Xiaohua Zhai",
      "Alexander Kolesnikov",
      "Neil Houlsby",
      "Lucas Beyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04560"
  },
  {
    "id": "arXiv:2106.04561",
    "title": "Safe Deep Q-Network for Autonomous Vehicles at Unsignalized Intersection",
    "abstract": "We propose a safe DRL approach for autonomous vehicle (AV) navigation through\ncrowds of pedestrians while making a left turn at an unsignalized intersection.\nOur method uses two long-short term memory (LSTM) models that are trained to\ngenerate the perceived state of the environment and the future trajectories of\npedestrians given noisy observations of their movement. A future collision\nprediction algorithm based on the future trajectories of the ego vehicle and\npedestrians is used to mask unsafe actions if the system predicts a collision.\nThe performance of our approach is evaluated in two experiments using the\nhigh-fidelity CARLA simulation environment. The first experiment tests the\nperformance of our method at intersections that are similar to the training\nintersection and the second experiment tests our method at intersections with a\ndifferent topology. For both experiments, our methods do not result in a\ncollision with a pedestrian while still navigating the intersection at a\nreasonable speed.",
    "descriptor": "\nComments: 11 pages, 6 figures, 5 Tables. arXiv admin note: text overlap with arXiv:2105.00153\n",
    "authors": [
      "Kasra Mokhtari",
      "Alan R. Wagner"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04561"
  },
  {
    "id": "arXiv:2106.04563",
    "title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation",
    "abstract": "While deep and large pre-trained models are the state-of-the-art for various\nnatural language processing tasks, their huge size poses significant challenges\nfor practical uses in resource constrained settings. Recent works in knowledge\ndistillation propose task-agnostic as well as task-specific methods to compress\nthese models, with task-specific ones often yielding higher compression rate.\nIn this work, we develop a new task-agnostic distillation framework\nXtremeDistilTransformers that leverages the advantage of task-specific methods\nfor learning a small universal model that can be applied to arbitrary tasks and\nlanguages. To this end, we study the transferability of several source tasks,\naugmentation resources and model architecture for distillation. We evaluate our\nmodel performance on multiple tasks, including the General Language\nUnderstanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and\na massive multi-lingual NER dataset with 41 languages.",
    "descriptor": "\nComments: Code and checkpoints released (links in draft)\n",
    "authors": [
      "Subhabrata Mukherjee",
      "Ahmed Hassan Awadallah",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04563"
  },
  {
    "id": "arXiv:2106.04564",
    "title": "Are Pretrained Transformers Robust in Intent Classification? A Missing  Ingredient in Evaluation of Out-of-Scope Intent Detection",
    "abstract": "Pretrained Transformer-based models were reported to be robust in intent\nclassification. In this work, we first point out the importance of in-domain\nout-of-scope detection in few-shot intent recognition tasks and then illustrate\nthe vulnerability of pretrained Transformer-based models against samples that\nare in-domain but out-of-scope (ID-OOS). We empirically show that pretrained\nmodels do not perform well on both ID-OOS examples and general out-of-scope\nexamples, especially on fine-grained few-shot intent detection tasks. To figure\nout how the models mistakenly classify ID-OOS intents as in-scope intents, we\nfurther conduct analysis on confidence scores and the overlapping keywords and\nprovide several prospective directions for future work. We release the relevant\nresources to facilitate future research.",
    "descriptor": "",
    "authors": [
      "Jian-Guo Zhang",
      "Kazuma Hashimoto",
      "Yao Wan",
      "Ye Liu",
      "Caiming Xiong",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04564"
  },
  {
    "id": "arXiv:2106.04565",
    "title": "Translate, then Parse! A strong baseline for Cross-Lingual AMR Parsing",
    "abstract": "In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers\ndevelop models that project sentences from various languages onto their AMRs to\ncapture their essential semantic structures: given a sentence in any language,\nwe aim to capture its core semantic content through concepts connected by\nmanifold types of semantic relations. Methods typically leverage large silver\ntraining data to learn a single model that is able to project non-English\nsentences to AMRs. However, we find that a simple baseline tends to be\nover-looked: translating the sentences to English and projecting their AMR with\na monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this\nsimple two-step base-line, and enhance it with a strong NMT system and a strong\nAMR parser. Our experiments show that T+P outperforms a recent state-of-the-art\nsystem across all tested languages: German, Italian, Spanish and Mandarin with\n+14.6, +12.6, +14.3 and +16.0 Smatch points.",
    "descriptor": "\nComments: IWPT 2021\n",
    "authors": [
      "Sarah Uhrig",
      "Yoalli Rezepka Garcia",
      "Juri Opitz",
      "Anette Frank"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04565"
  },
  {
    "id": "arXiv:2106.04566",
    "title": "Data-Efficient Instance Generation from Instance Discrimination",
    "abstract": "Generative Adversarial Networks (GANs) have significantly advanced image\nsynthesis, however, the synthesis quality drops significantly given a limited\namount of training data. To improve the data efficiency of GAN training, prior\nwork typically employs data augmentation to mitigate the overfitting of the\ndiscriminator yet still learn the discriminator with a bi-classification (i.e.,\nreal vs. fake) task. In this work, we propose a data-efficient Instance\nGeneration (InsGen) method based on instance discrimination. Concretely,\nbesides differentiating the real domain from the fake domain, the discriminator\nis required to distinguish every individual image, no matter it comes from the\ntraining set or from the generator. In this way, the discriminator can benefit\nfrom the infinite synthesized samples for training, alleviating the overfitting\nproblem caused by insufficient training data. A noise perturbation strategy is\nfurther introduced to improve its discriminative power. Meanwhile, the learned\ninstance discrimination capability from the discriminator is in turn exploited\nto encourage the generator for diverse generation. Extensive experiments\ndemonstrate the effectiveness of our method on a variety of datasets and\ntraining settings. Noticeably, on the setting of 2K training images from the\nFFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID\nimprovement.",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Ceyuan Yang",
      "Yujun Shen",
      "Yinghao Xu",
      "Bolei Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04566"
  },
  {
    "id": "arXiv:2106.04569",
    "title": "Simulated Adversarial Testing of Face Recognition Models",
    "abstract": "Most machine learning models are validated and tested on fixed datasets. This\ncan give an incomplete picture of the capabilities and weaknesses of the model.\nSuch weaknesses can be revealed at test time in the real world. The risks\ninvolved in such failures can be loss of profits, loss of time or even loss of\nlife in certain critical applications. In order to alleviate this issue,\nsimulators can be controlled in a fine-grained manner using interpretable\nparameters to explore the semantic image manifold. In this work, we propose a\nframework for learning how to test machine learning algorithms using simulators\nin an adversarial manner in order to find weaknesses in the model before\ndeploying it in critical scenarios. We apply this model in a face recognition\nscenario. We are the first to show that weaknesses of models trained on real\ndata can be discovered using simulated samples. Using our proposed method, we\ncan find adversarial synthetic faces that fool contemporary face recognition\nmodels. This demonstrates the fact that these models have weaknesses that are\nnot measured by commonly used validation datasets. We hypothesize that this\ntype of adversarial examples are not isolated, but usually lie in connected\ncomponents in the latent space of the simulator. We present a method to find\nthese adversarial regions as opposed to the typical adversarial points found in\nthe adversarial example literature.",
    "descriptor": "",
    "authors": [
      "Nataniel Ruiz",
      "Adam Kortylewski",
      "Weichao Qiu",
      "Cihang Xie",
      "Sarah Adel Bargal",
      "Alan Yuille",
      "Stan Sclaroff"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04569"
  },
  {
    "id": "arXiv:2106.04570",
    "title": "Meta Learning for Knowledge Distillation",
    "abstract": "We present Meta Learning for Knowledge Distillation (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models. The code is available at\nhttps://github.com/JetRunner/MetaDistil",
    "descriptor": "",
    "authors": [
      "Wangchunshu Zhou",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04570"
  },
  {
    "id": "arXiv:2106.04571",
    "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog",
    "abstract": "Everyday conversations require understanding everyday events, which in turn,\nrequires understanding temporal commonsense concepts interwoven with those\nevents. Despite recent progress with massive pre-trained language models (LMs)\nsuch as T5 and GPT-3, their capability of temporal reasoning in dialogs remains\nlargely under-explored. In this paper, we present the first study to\ninvestigate pre-trained LMs for their temporal reasoning capabilities in\ndialogs by introducing a new task and a crowd-sourced English challenge set,\nTIMEDIAL. We formulate TIME-DIAL as a multiple-choice cloze task with over 1.1K\ncarefully curated dialogs. Empirical results demonstrate that even the best\nperforming models struggle on this task compared to humans, with 23 absolute\npoints of gap in accuracy. Furthermore, our analysis reveals that the models\nfail to reason about dialog context correctly; instead, they rely on shallow\ncues based on existing temporal patterns in context, motivating future research\nfor modeling temporal concepts in text and robust contextual reasoning about\nthem. The dataset is publicly available at:\nhttps://github.com/google-research-datasets/timedial.",
    "descriptor": "",
    "authors": [
      "Lianhui Qin",
      "Aditya Gupta",
      "Shyam Upadhyay",
      "Luheng He",
      "Yejin Choi",
      "Manaal Faruqui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04571"
  },
  {
    "id": "arXiv:2106.03880",
    "title": "Encoding-dependent generalization bounds for parametrized quantum  circuits",
    "abstract": "A large body of recent work has begun to explore the potential of\nparametrized quantum circuits (PQCs) as machine learning models, within the\nframework of hybrid quantum-classical optimization. In particular, theoretical\nguarantees on the out-of-sample performance of such models, in terms of\ngeneralization bounds, have emerged. However, none of these generalization\nbounds depend explicitly on how the classical input data is encoded into the\nPQC. We derive generalization bounds for PQC-based models that depend\nexplicitly on the strategy used for data-encoding. These imply bounds on the\nperformance of trained PQC-based models on unseen data. Moreover, our results\nfacilitate the selection of optimal data-encoding strategies via structural\nrisk minimization, a mathematically rigorous framework for model selection. We\nobtain our generalization bounds by bounding the complexity of PQC-based models\nas measured by the Rademacher complexity and the metric entropy, two complexity\nmeasures from statistical learning theory. To achieve this, we rely on a\nrepresentation of PQC-based models via trigonometric functions. Our\ngeneralization bounds emphasize the importance of well-considered data-encoding\nstrategies for PQC-based models.",
    "descriptor": "\nComments: 27 pages, 3 figures\n",
    "authors": [
      "Matthias C. Caro",
      "Elies Gil-Fuster",
      "Johannes Jakob Meyer",
      "Jens Eisert",
      "Ryan Sweke"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03880"
  },
  {
    "id": "arXiv:2106.03887",
    "title": "A Catalog of Formulations for the Network Pricing Problem",
    "abstract": "We study the network pricing problem where the leader maximizes their revenue\nby determining the optimal amounts of tolls to charge on a set of arcs, under\nthe assumption that the followers will react rationally and choose the shortest\npaths to travel. Many distinct single-level reformulations to this bilevel\noptimization program have been proposed, however, their relationship has not\nbeen established. In this paper, we aim to build a connection between those\nreformulations and explore the combination of the path representation with\nvarious modeling options, allowing us to generate 12 different reformulations\nof the problem. Moreover, we propose a new path enumeration scheme, path-based\npreprocessing, and hybrid framework to further improve performance and\nrobustness when solving the final model. We provide numerical results,\ncomparing all the derived reformulations and confirming the efficiency of the\nnovel dimensionality reduction procedures.",
    "descriptor": "\nComments: 35 pages, 7 figures\n",
    "authors": [
      "Quang Minh Bui",
      "Bernard Gendron",
      "Margarida Carvalho"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03887"
  },
  {
    "id": "arXiv:2106.03891",
    "title": "NISQ Algorithm for Semidefinite Programming",
    "abstract": "Semidefinite Programming (SDP) is a class of convex optimization programs\nwith vast applications in control theory, quantum information, combinatorial\noptimization and operational research. Noisy intermediate-scale quantum (NISQ)\nalgorithms aim to make an efficient use of the current generation of quantum\nhardware. However, optimizing variational quantum algorithms is a challenge as\nit is an NP-hard problem that in general requires an exponential time to solve\nand can contain many far from optimal local minima. Here, we present a current\nterm NISQ algorithm for SDP. The classical optimization program of our NISQ\nsolver is another SDP over a smaller dimensional ansatz space. We harness the\nSDP based formulation of the Hamiltonian ground state problem to design a NISQ\neigensolver. Unlike variational quantum eigensolvers, the classical\noptimization program of our eigensolver is convex, can be solved in polynomial\ntime with the number of ansatz parameters and every local minimum is a global\nminimum. Further, we demonstrate the potential of our NISQ SDP solver by\nfinding the largest eigenvalue of up to $2^{1000}$ dimensional matrices and\nsolving graph problems related to quantum contextuality. We also discuss NISQ\nalgorithms for rank-constrained SDPs. Our work extends the application of NISQ\ncomputers onto one of the most successful algorithmic frameworks of the past\nfew decades.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Kishor Bharti",
      "Tobias Haug",
      "Vlatko Vedral",
      "Leong-Chuan Kwek"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03891"
  },
  {
    "id": "arXiv:2106.03898",
    "title": "SPANet: Generalized Permutationless Set Assignment for Particle Physics  using Symmetry Preserving Attention",
    "abstract": "The creation of unstable heavy particles at the Large Hadron Collider is the\nmost direct way to address some of the deepest open questions in physics.\nCollisions typically produce variable-size sets of observed particles which\nhave inherent ambiguities complicating the assignment of observed particles to\nthe decay products of the heavy particles. Current strategies for tackling\nthese challenges in the physics community ignore the physical symmetries of the\ndecay products and consider all possible assignment permutations and do not\nscale to complex configurations. Attention based deep learning methods for\nsequence modelling have achieved state-of-the-art performance in natural\nlanguage processing, but they lack built-in mechanisms to deal with the unique\nsymmetries found in physical set-assignment problems. We introduce a novel\nmethod for constructing symmetry-preserving attention networks which reflect\nthe problem's natural invariances to efficiently find assignments without\nevaluating all permutations. This general approach is applicable to arbitrarily\ncomplex configurations and significantly outperforms current methods, improving\nreconstruction efficiency between 19\\% - 35\\% on typical benchmark problems\nwhile decreasing inference time by two to five orders of magnitude on the most\ncomplex events, making many important and previously intractable cases\ntractable.\nA full code repository containing a general library, the specific\nconfiguration used, and a complete dataset release, are avaiable at\nhttps://github.com/Alexanders101/SPANet",
    "descriptor": "\nComments: submitted to NeurIPS 2021\n",
    "authors": [
      "Alexander Shmakov",
      "Michael James Fenton",
      "Ta-Wei Ho",
      "Shih-Chieh Hsu",
      "Daniel Whiteson",
      "Pierre Baldi"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03898"
  },
  {
    "id": "arXiv:2106.03905",
    "title": "AutoPtosis",
    "abstract": "Blepharoptosis, or ptosis as it is more commonly referred to, is a condition\nof the eyelid where the upper eyelid droops. The current diagnosis for ptosis\ninvolves cumbersome manual measurements that are time-consuming and prone to\nhuman error. In this paper, we present AutoPtosis, an artificial intelligence\nbased system with interpretable results for rapid diagnosis of ptosis. We\nutilize a diverse dataset collected at the University of Illinois Hospital and\nHealth to successfully develop a robust deep learning model for prediction and\nalso develop a clinically inspired model that calculates the marginal reflex\ndistance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician\nverified data that had an equal class balance. The proposed algorithm can help\nin the rapid and timely diagnosis of ptosis, significantly reduce the burden on\nthe healthcare system, and save the patients and clinics valuable resources.",
    "descriptor": "",
    "authors": [
      "Abdullah Aleem",
      "Manoj Prabhakar Nallabothula",
      "Pete Setabutr",
      "Joelle A. Hallak",
      "Darvin Yi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03905"
  },
  {
    "id": "arXiv:2106.03915",
    "title": "A Graphical Representation of Membrane Filtration",
    "abstract": "We analyze the performance of membrane filters represented by pore networks\nusing two criteria: 1) total volumetric throughput of filtrate over the filter\nlifetime and 2) accumulated foulant concentration in the filtrate. We first\nformulate the governing equations of fluid flow on a general network, and we\nmodel transport and adsorption of particles (foulants) within the network by\nimposing an advection equation with a sink term on each pore (edge) as well as\nconservation of fluid and foulant volumetric flow rates at each pore junction\n(network vertex). Such a setup yields a system of partial differential\nequations on the network. We study the influence of three geometric network\nparameters on filter performance: 1) average number of neighbors of each\nvertex; 2) initial total void volume of the pore network; and 3) tortuosity of\nthe network. We find that total volumetric throughput depends more strongly on\nthe initial void volume than on average number of neighbors. Tortuosity,\nhowever, turns out to be a universal parameter, leading to almost perfect\ncollapse of all results for a variety of different network architectures. In\nparticular, the accumulated foulant concentration in the filtrate shows an\nexponential decay as tortuosity increases.",
    "descriptor": "",
    "authors": [
      "Binan Gu",
      "Lou Kondic",
      "Linda J. Cummings"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03915"
  },
  {
    "id": "arXiv:2106.03936",
    "title": "Partial Optimal Transport for a Constant-Volume Lagrangian Mesh with  Free Boundaries",
    "abstract": "This article introduces a representation of dynamic meshes, adapted to some\nnumerical simulations that require controlling the volume of objects with free\nboundaries, such as incompressible fluid simulation, some astrophysical\nsimulations at cosmological scale, and shape/topology optimization. The\nalgorithm decomposes the simulated object into a set of convex cells called a\nLaguerre diagram, parameterized by the position of $N$ points in 3D and $N$\nadditional parameters that control the volumes of the cells. These parameters\nare found as the (unique) solution of a convex optimization problem --\nsemi-discrete Monge-Amp\\`ere equation -- stemming from optimal transport\ntheory. In this article, this setting is extended to objects with free\nboundaries and arbitrary topology, evolving in a domain of arbitrary shape, by\nsolving a partial optimal transport problem. The resulting Lagrangian scheme\nmakes it possible to accurately control the volume of the object, while\nprecisely tracking interfaces, interactions, collisions, and topology changes.",
    "descriptor": "\nComments: 37 pages, 17 figures\n",
    "authors": [
      "Bruno L\u00e9vy"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.03936"
  },
  {
    "id": "arXiv:2106.03948",
    "title": "Nonequilibrium Thermodynamics in Measuring Carbon Footprints:  Disentangling Structure and Artifact in Input-Output Accounting",
    "abstract": "Multiregional input-output (MRIO) tables, in conjunction with Leontief\nanalysis, are widely-used to assess the geographical distribution of carbon\nemissions and the economic activities that cause them. We examine Leontief\nanalysis as a model, demonstrating commonalities with modern approaches in\ninformation theory and nonequilibrium statistical mechanics. Paralleling the\nphysical concept of thermo-majorization, we define the concept of\neco-majorization and show it is a sufficient condition to determine the\ndirectionality of embodied impact flows. Surprisingly, relatively small trade\ndeficits and geographically heterogeneous impacts greatly increase the\nappearance of eco-majorization, regardless of any further content in the MRIO\ntables used. Our results are bolstered by a statistical analysis of null models\nof MRIO tables developed by the Global Trade Aggregation Project.",
    "descriptor": "\nComments: 14 pages, 5 figures; 2 appendices; this http URL\n",
    "authors": [
      "Samuel P. Loomis",
      "Mark Cooper",
      "James P. Crutchfield"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Information Theory (cs.IT)",
      "Physics and Society (physics.soc-ph)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03948"
  },
  {
    "id": "arXiv:2106.03970",
    "title": "Batch Normalization Orthogonalizes Representations in Deep Random  Networks",
    "abstract": "This paper underlines a subtle property of batch-normalization (BN):\nSuccessive batch normalizations with random linear transformations make hidden\nrepresentations increasingly orthogonal across layers of a deep neural network.\nWe establish a non-asymptotic characterization of the interplay between depth,\nwidth, and the orthogonality of deep representations. More precisely, under a\nmild assumption, we prove that the deviation of the representations from\northogonality rapidly decays with depth up to a term inversely proportional to\nthe network width. This result has two main implications: 1) Theoretically, as\nthe depth grows, the distribution of the representation -- after the linear\nlayers -- contracts to a Wasserstein-2 ball around an isotropic Gaussian\ndistribution. Furthermore, the radius of this Wasserstein ball shrinks with the\nwidth of the network. 2) In practice, the orthogonality of the representations\ndirectly influences the performance of stochastic gradient descent (SGD). When\nrepresentations are initially aligned, we observe SGD wastes many iterations to\northogonalize representations before the classification. Nevertheless, we\nexperimentally show that starting optimization from orthogonal representations\nis sufficient to accelerate SGD, with no need for BN.",
    "descriptor": "",
    "authors": [
      "Hadi Daneshmand",
      "Amir Joudaki",
      "Francis Bach"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03970"
  },
  {
    "id": "arXiv:2106.03975",
    "title": "Equilibria in Repeated Games with Countably Many Players and  Tail-Measurable Payoffs",
    "abstract": "We prove that every repeated game with countably many players, finite action\nsets, and tail-measurable payoffs admits an $\\epsilon$-equilibrium, for every\n$\\epsilon > 0$.",
    "descriptor": "",
    "authors": [
      "Galit Ashkenazi-Golan",
      "Janos Flesch",
      "Arkadi Predtetchinski",
      "Eilon Solan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03975"
  },
  {
    "id": "arXiv:2106.03991",
    "title": "Quantum Computers Can Find Quadratic Nonresidues in Deterministic  Polynomial Time",
    "abstract": "An integer $a$ is a quadratic nonresidue for a prime $p$ if $x^2 \\equiv a\n\\bmod p$ has no solution. Quadratic nonresidues may be found by probabilistic\nmethods in polynomial time. However, without assuming the Generalized Riemann\nHypothesis, no deterministic polynomial-time algorithm is known. We present a\nquantum algorithm which generates a random quadratic nonresidue in\ndeterministic polynomial time.",
    "descriptor": "\nComments: 7 pages, 6 figures\n",
    "authors": [
      "Thomas G. Draper"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.03991"
  },
  {
    "id": "arXiv:2106.04001",
    "title": "Optimized Data Rate Allocation for Dynamic Sensor Fusion over Resource  Constrained Communication Networks",
    "abstract": "This paper presents a new method to solve a dynamic sensor fusion problem. We\nconsider a large number of remote sensors which measure a common Gauss-Markov\nprocess and encoders that transmit the measurements to a data fusion center\nthrough the resource restricted communication network. The proposed approach\nheuristically minimizes a weighted sum of communication costs subject to a\nconstraint on the state estimation error at the fusion center. The\ncommunication costs are quantified as the expected bitrates from the sensors to\nthe fusion center. We show that the problem as formulated is a\ndifference-of-convex program and apply the convex-concave procedure (CCP) to\nobtain a heuristic solution. We consider a 1D heat transfer model and 2D target\ntracking by a drone swarm model for numerical studies. Through these\nsimulations, we observe that our proposed approach has a tendency to assign\nzero data rate to unnecessary sensors indicating that our approach is sparsity\npromoting, and an effective sensor selection heuristic.",
    "descriptor": "",
    "authors": [
      "Hyunho Jung",
      "Ali Reza Pedram",
      "Travis Craig Cuvelier",
      "Takashi Tanaka"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.04001"
  },
  {
    "id": "arXiv:2106.04013",
    "title": "The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width  Limit at Initialization",
    "abstract": "Theoretical results show that neural networks can be approximated by Gaussian\nprocesses in the infinite-width limit. However, for fully connected networks,\nit has been previously shown that for any fixed network width, $n$, the\nGaussian approximation gets worse as the network depth, $d$, increases. Given\nthat modern networks are deep, this raises the question of how well modern\narchitectures, like ResNets, are captured by the infinite-width limit. To\nprovide a better approximation, we study ReLU ResNets in the\ninfinite-depth-and-width limit, where both depth and width tend to infinity as\ntheir ratio, $d/n$, remains constant. In contrast to the Gaussian\ninfinite-width limit, we show theoretically that the network exhibits\nlog-Gaussian behaviour at initialization in the infinite-depth-and-width limit,\nwith parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we\ndemonstrate that even basic properties of standard ResNet architectures are\npoorly captured by the Gaussian limit, but remarkably well captured by our\nlog-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at\ninitialization are hypoactivated: fewer than half of the ReLUs are activated.\nAdditionally, we calculate the interlayer correlations, which have the effect\nof exponentially increasing the variance of the network output. Based on our\nanalysis, we introduce Balanced ResNets, a simple architecture modification,\nwhich eliminates hypoactivation and interlayer correlations and is more\namenable to theoretical analysis.",
    "descriptor": "",
    "authors": [
      "Mufan Bill Li",
      "Mihai Nica",
      "Daniel M. Roy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04013"
  },
  {
    "id": "arXiv:2106.04018",
    "title": "Intrinsic Dimension Estimation",
    "abstract": "It has long been thought that high-dimensional data encountered in many\npractical machine learning tasks have low-dimensional structure, i.e., the\nmanifold hypothesis holds. A natural question, thus, is to estimate the\nintrinsic dimension of a given population distribution from a finite sample. We\nintroduce a new estimator of the intrinsic dimension and provide finite sample,\nnon-asymptotic guarantees. We then apply our techniques to get new sample\ncomplexity bounds for Generative Adversarial Networks (GANs) depending only on\nthe intrinsic dimension of the data.",
    "descriptor": "",
    "authors": [
      "Adam Block",
      "Zeyu Jia",
      "Yury Polyanskiy",
      "Alexander Rakhlin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04018"
  },
  {
    "id": "arXiv:2106.04078",
    "title": "End-to-End Speaker Diarization Conditioned on Speech Activity and  Overlap Detection",
    "abstract": "In this paper, we present a conditional multitask learning method for\nend-to-end neural speaker diarization (EEND). The EEND system has shown\npromising performance compared with traditional clustering-based methods,\nespecially in the case of overlapping speech. In this paper, to further improve\nthe performance of the EEND system, we propose a novel multitask learning\nframework that solves speaker diarization and a desired subtask while\nexplicitly considering the task dependency. We optimize speaker diarization\nconditioned on speech activity and overlap detection that are subtasks of\nspeaker diarization, based on the probabilistic chain rule. Experimental\nresults show that our proposed method can leverage a subtask to effectively\nmodel speaker diarization, and outperforms conventional EEND systems in terms\nof diarization error rate.",
    "descriptor": "\nComments: Accepted for SLT 2021\n",
    "authors": [
      "Yuki Takashima",
      "Yusuke Fujita",
      "Shinji Watanabe",
      "Shota Horiguchi",
      "Paola Garc\u00eda",
      "Kenji Nagamatsu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04078"
  },
  {
    "id": "arXiv:2106.04083",
    "title": "On the Average (Edge-)Connectivity of Minimally $k$-(Edge-)Connected  Graphs",
    "abstract": "Let $G$ be a graph of order $n$ and let $u,v$ be vertices of $G$. Let\n$\\kappa_G(u,v)$ denote the maximum number of internally disjoint $u$--$v$ paths\nin $G$. Then the average connectivity $\\overline{\\kappa}(G)$ of $G$, is defined\nas $ \\overline{\\kappa}(G)=\\sum_{\\{u,v\\}\\subseteq V(G)}\n\\kappa_G(u,v)/\\tbinom{n}{2}. $ If $k \\ge 1$ is an integer, then $G$ is\nminimally $k$-connected if $\\kappa(G)=k$ and $\\kappa(G-e) < k$ for every edge\n$e$ of $G$. We say that $G$ is an optimal minimally $k$-connected graph if $G$\nhas maximum average connectivity among all minimally $k$-connected graphs of\norder $n$. Casablanca, Mol and Oellermann showed that every optimal minimally\n2-connected graph $G$ is bipartite, with the set of vertices of degree 2 and\nthe set of vertices of degree exceeding 2 forming the partite sets. They also\nproved that $\\overline{\\kappa}(G) < 9/4$ for all minimally $2$-connected graphs\n$G$ and that this bound is asymptotically sharp. We conjecture that for every\ninteger $k \\ge 3$, if $G$ is an optimal minimally $k$-connected graph of order\n$n\\geq 2k+1$, then $G$ is bipartite, with the set of vertices of degree $k$ and\nthe set of vertices of degree exceeding $k$ as its partite sets. We show that\nif this conjecture is true, then $\\overline{\\kappa}(G)< 9k/8$ for every\nminimally $k$-connected graph $G$. For every $k \\ge 3$, we describe an infinite\nfamily of minimally $k$-connected graphs whose average connectivity is\nasymptotically $9k/8$. Analogous results are established for the average\nedge-connectivity of minimally $k$-edge-connected graphs.",
    "descriptor": "\nComments: 16 pages, 3 figures\n",
    "authors": [
      "Lucas Mol",
      "Ortrud R. Oellermann",
      "Vibhav Oswal"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.04083"
  },
  {
    "id": "arXiv:2106.04089",
    "title": "Credit Assignment Through Broadcasting a Global Error Vector",
    "abstract": "Backpropagation (BP) uses detailed, unit-specific feedback to train deep\nneural networks (DNNs) with remarkable success. That biological neural circuits\nappear to perform credit assignment, but cannot implement BP, implies the\nexistence of other powerful learning algorithms. Here, we explore the extent to\nwhich a globally broadcast learning signal, coupled with local weight updates,\nenables training of DNNs. We present both a learning rule, called global\nerror-vector broadcasting (GEVB), and a class of DNNs, called vectorized\nnonnegative networks (VNNs), in which this learning rule operates. VNNs have\nvector-valued units and nonnegative weights past the first layer. The GEVB\nlearning rule generalizes three-factor Hebbian learning, updating each weight\nby an amount proportional to the inner product of the presynaptic activation\nand a globally broadcast error vector when the postsynaptic unit is active. We\nprove that these weight updates are matched in sign to the gradient, enabling\naccurate credit assignment. Moreover, at initialization, these updates are\nexactly proportional to the gradient in the limit of infinite network width.\nGEVB matches the performance of BP in VNNs, and in some cases outperforms\ndirect feedback alignment (DFA) applied in conventional networks. Unlike DFA,\nGEVB successfully trains convolutional layers. Altogether, our theoretical and\nempirical results point to a surprisingly powerful role for a global learning\nsignal in training DNNs.",
    "descriptor": "\nComments: 18 pages, 6 figures\n",
    "authors": [
      "David G. Clark",
      "L. F. Abbott",
      "SueYeon Chung"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04089"
  },
  {
    "id": "arXiv:2106.04130",
    "title": "EnMcGAN: Adversarial Ensemble Learning for 3D Complete Renal Structures  Segmentation",
    "abstract": "3D complete renal structures(CRS) segmentation targets on segmenting the\nkidneys, tumors, renal arteries and veins in one inference. Once successful, it\nwill provide preoperative plans and intraoperative guidance for laparoscopic\npartial nephrectomy(LPN), playing a key role in the renal cancer treatment.\nHowever, no success has been reported in 3D CRS segmentation due to the complex\nshapes of renal structures, low contrast and large anatomical variation. In\nthis study, we utilize the adversarial ensemble learning and propose Ensemble\nMulti-condition GAN(EnMcGAN) for 3D CRS segmentation for the first time. Its\ncontribution is three-fold. 1)Inspired by windowing, we propose the\nmulti-windowing committee which divides CTA image into multiple narrow windows\nwith different window centers and widths enhancing the contrast for salient\nboundaries and soft tissues. And then, it builds an ensemble segmentation model\non these narrow windows to fuse the segmentation superiorities and improve\nwhole segmentation quality. 2)We propose the multi-condition GAN which equips\nthe segmentation model with multiple discriminators to encourage the segmented\nstructures meeting their real shape conditions, thus improving the shape\nfeature extraction ability. 3)We propose the adversarial weighted ensemble\nmodule which uses the trained discriminators to evaluate the quality of\nsegmented structures, and normalizes these evaluation scores for the ensemble\nweights directed at the input image, thus enhancing the ensemble results. 122\npatients are enrolled in this study and the mean Dice coefficient of the renal\nstructures achieves 84.6%. Extensive experiments with promising results on\nrenal structures reveal powerful segmentation accuracy and great clinical\nsignificance in renal cancer treatment.",
    "descriptor": "",
    "authors": [
      "Yuting He",
      "Rongjun Ge",
      "Xiaoming Qi",
      "Guanyu Yang",
      "Yang Chen",
      "Youyong Kong",
      "Huazhong Shu",
      "Jean-Louis Coatrieux",
      "Shuo Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04130"
  },
  {
    "id": "arXiv:2106.04145",
    "title": "Unbalanced Optimal Transport through Non-negative Penalized Linear  Regression",
    "abstract": "This paper addresses the problem of Unbalanced Optimal Transport (UOT) in\nwhich the marginal conditions are relaxed (using weighted penalties in lieu of\nequality) and no additional regularization is enforced on the OT plan. In this\ncontext, we show that the corresponding optimization problem can be\nreformulated as a non-negative penalized linear regression problem. This\nreformulation allows us to propose novel algorithms inspired from inverse\nproblems and nonnegative matrix factorization. In particular, we consider\nmajorization-minimization which leads in our setting to efficient\nmultiplicative updates for a variety of penalties. Furthermore, we derive for\nthe first time an efficient algorithm to compute the regularization path of UOT\nwith quadratic penalties. The proposed algorithm provides a continuity of\npiece-wise linear OT plans converging to the solution of balanced OT\n(corresponding to infinite penalty weights). We perform several numerical\nexperiments on simulated and real data illustrating the new algorithms, and\nprovide a detailed discussion about more sophisticated optimization tools that\ncan further be used to solve OT problems thanks to our reformulation.",
    "descriptor": "\nComments: Laetitia Chapel and R\\'emi Flamary have equal contribution\n",
    "authors": [
      "Laetitia Chapel",
      "R\u00e9mi Flamary",
      "Haoran Wu",
      "C\u00e9dric F\u00e9votte",
      "Gilles Gasso"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04145"
  },
  {
    "id": "arXiv:2106.04170",
    "title": "Conditional Deep Inverse Rosenblatt Transports",
    "abstract": "We present a novel offline-online method to mitigate the computational burden\nof the characterization of conditional beliefs in statistical learning. In the\noffline phase, the proposed method learns the joint law of the belief random\nvariables and the observational random variables in the tensor-train (TT)\nformat. In the online phase, it utilizes the resulting order-preserving\nconditional transport map to issue real-time characterization of the\nconditional beliefs given new observed information. Compared with the\nstate-of-the-art normalizing flows techniques, the proposed method relies on\nfunction approximation and is equipped with thorough performance analysis. This\nalso allows us to further extend the capability of transport maps in\nchallenging problems with high-dimensional observations and high-dimensional\nbelief variables. On the one hand, we present novel heuristics to reorder\nand/or reparametrize the variables to enhance the approximation power of TT. On\nthe other, we integrate the TT-based transport maps and the parameter\nreordering/reparametrization into layered compositions to further improve the\nperformance of the resulting transport maps. We demonstrate the efficiency of\nthe proposed method on various statistical learning tasks in ordinary\ndifferential equations (ODEs) and partial differential equations (PDEs).",
    "descriptor": "",
    "authors": [
      "Tiangang Cui",
      "Sergey Dolgov",
      "Olivier Zahm"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.04170"
  },
  {
    "id": "arXiv:2106.04176",
    "title": "Efficient solution method based on inverse dynamics for optimal control  problems of rigid body systems",
    "abstract": "We propose an efficient way of solving optimal control problems for\nrigid-body systems on the basis of inverse dynamics and the multiple-shooting\nmethod. We treat all variables, including the state, acceleration, and control\ninput torques, as optimization variables and treat the inverse dynamics as an\nequality constraint. We eliminate the update of the control input torques from\nthe linear equation of Newton's method by applying condensing for inverse\ndynamics. The size of the resultant linear equation is the same as that of the\nmultiple-shooting method based on forward dynamics except for the variables\nrelated to the passive joints and contacts. Compared with the conventional\nmethods based on forward dynamics, the proposed method reduces the\ncomputational cost of the dynamics and their sensitivities by utilizing the\nrecursive Newton-Euler algorithm (RNEA) and its partial derivatives. In\naddition, it increases the sparsity of the Hessian of the Karush-Kuhn-Tucker\nconditions, which reduces the computational cost, e.g., of Riccati recursion.\nNumerical experiments show that the proposed method outperforms\nstate-of-the-art implementations of differential dynamic programming based on\nforward dynamics in terms of computational time and numerical robustness.",
    "descriptor": "\nComments: 7 pages, 3 figures. This paper has been accepted to be presented 2021 IEEE International Conference on Robotics and Automation (ICRA2021)\n",
    "authors": [
      "Sotaro Katayama",
      "Toshiyuki Ohtsuka"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04176"
  },
  {
    "id": "arXiv:2106.04193",
    "title": "Targeted Active Learning for Bayesian Decision-Making",
    "abstract": "Active learning is usually applied to acquire labels of informative data\npoints in supervised learning, to maximize accuracy in a sample-efficient way.\nHowever, maximizing the accuracy is not the end goal when the results are used\nfor decision-making, for example in personalized medicine or economics. We\nargue that when acquiring samples sequentially, separating learning and\ndecision-making is sub-optimal, and we introduce a novel active learning\nstrategy which takes the down-the-line decision problem into account.\nSpecifically, we introduce a novel active learning criterion which maximizes\nthe expected information gain on the posterior distribution of the optimal\ndecision. We compare our decision-making-aware active learning strategy to\nexisting alternatives on both simulated and real data, and show improved\nperformance in decision-making accuracy.",
    "descriptor": "",
    "authors": [
      "Louis Filstroff",
      "Iiris Sundin",
      "Petrus Mikkola",
      "Aleksei Tiulpin",
      "Juuso Kylm\u00e4oja",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04193"
  },
  {
    "id": "arXiv:2106.04197",
    "title": "Seismic Inverse Modeling Method based on Generative Adversarial Network",
    "abstract": "Seismic inverse modeling is a common method in reservoir prediction and it\nplays a vital role in the exploration and development of oil and gas.\nConventional seismic inversion method is difficult to combine with complicated\nand abstract knowledge on geological mode and its uncertainty is difficult to\nbe assessed. The paper proposes an inversion modeling method based on GAN\nconsistent with geology, well logs, seismic data. GAN is a the most promising\ngeneration model algorithm that extracts spatial structure and abstract\nfeatures of training images. The trained GAN can reproduce the models with\nspecific mode. In our test, 1000 models were generated in 1 second. Based on\nthe trained GAN after assessment, the optimal result of models can be\ncalculated through Bayesian inversion frame. Results show that inversion models\nconform to observation data and have a low uncertainty under the premise of\nfast generation. This seismic inverse modeling method increases the efficiency\nand quality of inversion iteration. It is worthy of studying and applying in\nfusion of seismic data and geological knowledge.",
    "descriptor": "\nComments: 22 pages,13 figures\n",
    "authors": [
      "Pengfei Xie",
      "YanShu Yin",
      "JiaGen Hou",
      "Mei Chen",
      "Lixin Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.04197"
  },
  {
    "id": "arXiv:2106.04228",
    "title": "Decentralized Learning in Online Queuing Systems",
    "abstract": "Motivated by packet routing in computer networks, online queuing systems are\ncomposed of queues receiving packets at different rates. Repeatedly, they send\npackets to servers, each of them treating only at most one packet at a time. In\nthe centralized case, the number of accumulated packets remains bounded (i.e.,\nthe system is \\textit{stable}) as long as the ratio between service rates and\narrival rates is larger than $1$. In the decentralized case, individual\nno-regret strategies ensures stability when this ratio is larger than $2$. Yet,\nmyopically minimizing regret disregards the long term effects due to the\ncarryover of packets to further rounds. On the other hand, minimizing long term\ncosts leads to stable Nash equilibria as soon as the ratio exceeds\n$\\frac{e}{e-1}$. Stability with decentralized learning strategies with a ratio\nbelow $2$ was a major remaining question. We first argue that for ratios up to\n$2$, cooperation is required for stability of learning strategies, as selfish\nminimization of policy regret, a \\textit{patient} notion of regret, might\nindeed still be unstable in this case. We therefore consider cooperative queues\nand propose the first learning decentralized algorithm guaranteeing stability\nof the system as long as the ratio of rates is larger than $1$, thus reaching\nperformances comparable to centralized strategies.",
    "descriptor": "",
    "authors": [
      "Flore Sentenac",
      "Etienne Boursier",
      "Vianney Perchet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.04228"
  },
  {
    "id": "arXiv:2106.04229",
    "title": "BIGDML: Towards Exact Machine Learning Force Fields for Materials",
    "abstract": "Machine-learning force fields (MLFF) should be accurate, computationally and\ndata efficient, and applicable to molecules, materials, and interfaces thereof.\nCurrently, MLFFs often introduce tradeoffs that restrict their practical\napplicability to small subsets of chemical space or require exhaustive datasets\nfor training. Here, we introduce the Bravais-Inspired Gradient-Domain Machine\nLearning (BIGDML) approach and demonstrate its ability to construct reliable\nforce fields using a training set with just 10-200 geometries for materials\nincluding pristine and defect-containing 2D and 3D semiconductors and metals,\nas well as chemisorbed and physisorbed atomic and molecular adsorbates on\nsurfaces. The BIGDML model employs the full relevant symmetry group for a given\nmaterial, does not assume artificial atom types or localization of atomic\ninteractions and exhibits high data efficiency and state-of-the-art energy\naccuracies (errors substantially below 1 meV per atom) for an extended set of\nmaterials. Extensive path-integral molecular dynamics carried out with BIGDML\nmodels demonstrate the counterintuitive localization of benzene--graphene\ndynamics induced by nuclear quantum effects and allow to rationalize the\nArrhenius behavior of hydrogen diffusion coefficient in a Pd crystal for a wide\nrange of temperatures.",
    "descriptor": "\nComments: 15 pages, 8 figures, development of methodology and applications\n",
    "authors": [
      "Huziel E. Sauceda",
      "Luis E. G\u00e1lvez-Gonz\u00e1lez",
      "Stefan Chmiela",
      "Lauro Oliver Paz-Borb\u00f3n",
      "Klaus-Robert M\u00fcller",
      "Alexandre Tkatchenko"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.04229"
  },
  {
    "id": "arXiv:2106.04271",
    "title": "Inference for Network Regression Models with Community Structure",
    "abstract": "Network regression models, where the outcome comprises the valued edge in a\nnetwork and the predictors are actor or dyad-level covariates, are used\nextensively in the social and biological sciences. Valid inference relies on\naccurately modeling the residual dependencies among the relations. Frequently\nhomogeneity assumptions are placed on the errors which are commonly incorrect\nand ignore critical, natural clustering of the actors. In this work, we present\na novel regression modeling framework that models the errors as resulting from\na community-based dependence structure and exploits the subsequent\nexchangeability properties of the error distribution to obtain parsimonious\nstandard errors for regression parameters.",
    "descriptor": "",
    "authors": [
      "Mengjie Pan",
      "Tyler H. McCormick",
      "Bailey K. Fosdick"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04271"
  },
  {
    "id": "arXiv:2106.04281",
    "title": "Generative adversarial network with object detector discriminator for  enhanced defect detection on ultrasonic B-scans",
    "abstract": "Non-destructive testing is a set of techniques for defect detection in\nmaterials. While the set of imaging techniques are manifold, ultrasonic imaging\nis the one used the most. The analysis is mainly performed by human inspectors\nmanually analyzing recorded images. The low number of defects in real\nultrasonic inspections and legal issues considering data from such inspections\nmake it difficult to obtain proper results from automatic ultrasonic image\n(B-scan) analysis. In this paper, we present a novel deep learning Generative\nAdversarial Network model for generating ultrasonic B-scans with defects in\ndistinct locations. Furthermore, we show that generated B-scans can be used for\nsynthetic data augmentation, and can improve the performance of deep\nconvolutional neural object detection networks. Our novel method is\ndemonstrated on a dataset of almost 4000 B-scans with more than 6000 annotated\ndefects. Defect detection performance when training on real data yielded\naverage precision of 71%. By training only on generated data the results\nincreased to 72.1%, and by mixing generated and real data we achieve 75.7%\naverage precision. We believe that synthetic data generation can generalize to\nother challenges with limited datasets and could be used for training human\npersonnel.",
    "descriptor": "",
    "authors": [
      "Luka Posilovi\u0107",
      "Duje Medak",
      "Marko Subasic",
      "Marko Budimir",
      "Sven Loncaric"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04281"
  },
  {
    "id": "arXiv:2106.04299",
    "title": "A direct product theorem for quantum communication complexity with  applications to device-independent QKD",
    "abstract": "We give a direct product theorem for the entanglement-assisted interactive\nquantum communication complexity of an $l$-player predicate $\\mathsf{V}$. In\nparticular we show that for a distribution $p$ that is product across the input\nsets of the $l$ players, the success probability of any entanglement-assisted\nquantum communication protocol for computing $n$ copies of $\\mathsf{V}$, whose\ncommunication is $o(\\log(\\mathrm{eff}^*(\\mathsf{V},p))\\cdot n)$, goes down\nexponentially in $n$. Here $\\mathrm{eff}^*(\\mathsf{V}, p)$ is a distributional\nversion of the quantum efficiency or partition bound introduced by Laplante,\nLerays and Roland (2014), which is a lower bound on the distributional quantum\ncommunication complexity of computing a single copy of $\\mathsf{V}$ with\nrespect to $p$.\nAs an application of our result, we show that it is possible to do\ndevice-independent quantum key distribution (DIQKD) without the assumption that\ndevices do not leak any information after inputs are provided to them. We\nanalyze the DIQKD protocol given by Jain, Miller and Shi (2017), and show that\nwhen the protocol is carried out with devices that are compatible with $n$\ncopies of the Magic Square game, it is possible to extract $\\Omega(n)$ bits of\nkey from it, even in the presence of $O(n)$ bits of leakage. Our security proof\nis parallel, i.e., the honest parties can enter all their inputs into their\ndevices at once, and works for a leakage model that is arbitrarily interactive,\ni.e., the devices of the honest parties Alice and Bob can exchange information\nwith each other and with the eavesdropper Eve in any number of rounds, as long\nas the total number of bits or qubits communicated is bounded.",
    "descriptor": "",
    "authors": [
      "Rahul Jain",
      "Srijita Kundu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.04299"
  },
  {
    "id": "arXiv:2106.04312",
    "title": "Speech BERT Embedding For Improving Prosody in Neural TTS",
    "abstract": "This paper presents a speech BERT model to extract embedded prosody\ninformation in speech segments for improving the prosody of synthesized speech\nin neural text-to-speech (TTS). As a pre-trained model, it can learn prosody\nattributes from a large amount of speech data, which can utilize more data than\nthe original training data used by the target TTS. The embedding is extracted\nfrom the previous segment of a fixed length in the proposed BERT. The extracted\nembedding is then used together with the mel-spectrogram to predict the\nfollowing segment in the TTS decoder. Experimental results obtained by the\nTransformer TTS show that the proposed BERT can extract fine-grained,\nsegment-level prosody, which is complementary to utterance-level prosody to\nimprove the final prosody of the TTS speech. The objective distortions measured\non a single speaker TTS are reduced between the generated speech and original\nrecordings. Subjective listening tests also show that the proposed approach is\nfavorably preferred over the TTS without the BERT prosody embedding module, for\nboth in-domain and out-of-domain applications. For Microsoft professional,\nsingle/multiple speakers and the LJ Speaker in the public database, subjective\npreference is similarly confirmed with the new BERT prosody embedding. TTS demo\naudio samples are in https://judy44chen.github.io/TTSSpeechBERT/.",
    "descriptor": "",
    "authors": [
      "Liping Chen",
      "Yan Deng",
      "Xi Wang",
      "Frank K. Soong",
      "Lei He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04312"
  },
  {
    "id": "arXiv:2106.04330",
    "title": "Weighted Sparse Subspace Representation: A Unified Framework for  Subspace Clustering, Constrained Clustering, and Active Learning",
    "abstract": "Spectral-based subspace clustering methods have proved successful in many\nchallenging applications such as gene sequencing, image recognition, and motion\nsegmentation. In this work, we first propose a novel spectral-based subspace\nclustering algorithm that seeks to represent each point as a sparse convex\ncombination of a few nearby points. We then extend the algorithm to constrained\nclustering and active learning settings. Our motivation for developing such a\nframework stems from the fact that typically either a small amount of labelled\ndata is available in advance; or it is possible to label some points at a cost.\nThe latter scenario is typically encountered in the process of validating a\ncluster assignment. Extensive experiments on simulated and real data sets show\nthat the proposed approach is effective and competitive with state-of-the-art\nmethods.",
    "descriptor": "",
    "authors": [
      "Hankui Peng",
      "Nicos G. Pavlidis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04330"
  },
  {
    "id": "arXiv:2106.04362",
    "title": "DIPS-Plus: The Enhanced Database of Interacting Protein Structures for  Interface Prediction",
    "abstract": "How and where proteins interface with one another can ultimately impact the\nproteins' functions along with a range of other biological processes. As such,\nprecise computational methods for protein interface prediction (PIP) come\nhighly sought after as they could yield significant advances in drug discovery\nand design as well as protein function analysis. However, the traditional\nbenchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a\npaltry 230 complexes for training, validating, and testing different machine\nlearning algorithms. In this work, we expand on a dataset recently introduced\nfor this task, the Database of Interacting Protein Structures (DIPS), to\npresent DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for\ngeometric deep learning of protein interfaces. The previous version of DIPS\ncontains only the Cartesian coordinates and types of the atoms comprising a\ngiven protein complex, whereas DIPS-Plus now includes a plethora of new\nresidue-level features including protrusion indices, half-sphere amino acid\ncompositions, and new profile hidden Markov model (HMM)-based sequence features\nfor each amino acid, giving researchers a large, well-curated feature bank for\ntraining protein interface prediction methods.",
    "descriptor": "\nComments: 16 pages, 1 figure, and 3 tables. Under review\n",
    "authors": [
      "Alex Morehead",
      "Chen Chen",
      "Ada Sedova",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2106.04362"
  },
  {
    "id": "arXiv:2106.04377",
    "title": "Virtual Screening of Pharmaceutical Compounds with hERG Inhibitory  Activity (Cardiotoxicity) using Ensemble Learning",
    "abstract": "In silico prediction of cardiotoxicity with high sensitivity and specificity\nfor potential drug molecules can be of immense value. Hence, building machine\nlearning classification models, based on some features extracted from the\nmolecular structure of drugs, which are capable of efficiently predicting\ncardiotoxicity is critical. In this paper, we consider the application of\nvarious machine learning approaches, and then propose an ensemble classifier\nfor the prediction of molecular activity on a Drug Discovery Hackathon (DDH)\n(1st reference) dataset. We have used only 2-D descriptors of SMILE notations\nfor our prediction. Our ensemble classification uses 5 classifiers (2 Random\nForest Classifiers, 2 Support Vector Machines and a Dense Neural Network) and\nuses Max-Voting technique and Weighted-Average technique for final decision.",
    "descriptor": "\nComments: 23 pages, 2 figures, 5 tables\n",
    "authors": [
      "Aditya Sarkar",
      "Arnav Bhavsar"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04377"
  },
  {
    "id": "arXiv:2106.04381",
    "title": "Computer-Assisted Analysis of Biomedical Images",
    "abstract": "Nowadays, the amount of heterogeneous biomedical data is increasing more and\nmore thanks to novel sensing techniques and high-throughput technologies. In\nreference to biomedical image analysis, the advances in image acquisition\nmodalities and high-throughput imaging experiments are creating new challenges.\nThis huge information ensemble could overwhelm the analytic capabilities needed\nby physicians in their daily decision-making tasks as well as by biologists\ninvestigating complex biochemical systems. In particular, quantitative imaging\nmethods convey scientifically and clinically relevant information in\nprediction, prognosis or treatment response assessment, by also considering\nradiomics approaches. Therefore, the computational analysis of medical and\nbiological images plays a key role in radiology and laboratory applications. In\nthis regard, frameworks based on advanced Machine Learning and Computational\nIntelligence can significantly improve traditional Image Processing and Pattern\nRecognition approaches. However, conventional Artificial Intelligence\ntechniques must be tailored to address the unique challenges concerning\nbiomedical imaging data. This thesis aims at proposing novel and advanced\ncomputer-assisted methods for biomedical image analysis, also as an instrument\nin the development of Clinical Decision Support Systems, by always keeping in\nmind the clinical feasibility of the developed solutions. In conclusion, the\nultimate goal of these research studies is to gain clinically and biologically\nuseful insights that can guide differential diagnosis and therapies, leading\ntowards biomedical data integration for personalized medicine. As a matter of\nfact, the proposed computer-assisted bioimage analysis methods can be\nbeneficial for the definition of imaging biomarkers, as well as for\nquantitative medicine and biology.",
    "descriptor": "\nComments: PhD Thesis in Computer Science, University of Milano-Bicocca, Milan, Italy\n",
    "authors": [
      "Leonardo Rundo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04381"
  },
  {
    "id": "arXiv:2106.04383",
    "title": "Using a New Nonlinear Gradient Method for Solving Large Scale Convex  Optimization Problems with an Application on Arabic Medical Text",
    "abstract": "Gradient methods have applications in multiple fields, including signal\nprocessing, image processing, and dynamic systems. In this paper, we present a\nnonlinear gradient method for solving convex supra-quadratic functions by\ndeveloping the search direction, that done by hybridizing between the two\nconjugate coefficients HRM [2] and NHS [1]. The numerical results proved the\neffectiveness of the presented method by applying it to solve standard problems\nand reaching the exact solution if the objective function is quadratic convex.\nAlso presented in this article, an application to the problem of named entities\nin the Arabic medical language, as it proved the stability of the proposed\nmethod and its efficiency in terms of execution time.",
    "descriptor": "",
    "authors": [
      "Jaafar Hammoud",
      "Ali Eisab",
      "Natalia Dobrenkoa",
      "Natalia Gusarovaa"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04383"
  },
  {
    "id": "arXiv:2106.04390",
    "title": "The Effect of Pore Structure in Flapping Wings on Flight Performance",
    "abstract": "This study investigates the effects of porosity on flying creatures such as\ndragonflies, moths, hummingbirds, etc. wing and shows that pores can affect\nwing performance. These studies were performed by 3D porous flapping wing flow\nanalyses on Comsol Multiphysics. In this study, we analyzed different numbers\nof the porous wing at different angles of inclination in order to see the\neffect of pores on lift and drag forces. To compare the results 9 different\nanalyses were performed. In these analyses, airflow velocity was taken as 5\nm/s, angle of attack as 5 degrees, frequency as 25 Hz, and flapping angle as 30\ndegrees. By keeping these values constant, the number of pores was changed to\n36, 48, and 60, and the pore angles of inclination to 60, 70, and 80 degrees.\nAnalyses were carried out by giving laminar flow to this wing designed in the\nComsol Multiphysics program. The importance of pores was investigated by\ncomparing the results of these analyses.",
    "descriptor": "",
    "authors": [
      "Abdurrahim Yilmaz",
      "Asli Tekeci",
      "Meryem Ece Ozyetkin",
      "Ali Anil Demircali",
      "Kagan Unsal",
      "Huseyin Uvet"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04390"
  },
  {
    "id": "arXiv:2106.04410",
    "title": "Error Mitigation for Deep Quantum Optimization Circuits by Leveraging  Problem Symmetries",
    "abstract": "High error rates and limited fidelity of quantum gates in near-term quantum\ndevices are the central obstacles to successful execution of the Quantum\nApproximate Optimization Algorithm (QAOA). In this paper we introduce an\napplication-specific approach for mitigating the errors in QAOA evolution by\nleveraging the symmetries present in the classical objective function to be\noptimized. Specifically, the QAOA state is projected into the\nsymmetry-restricted subspace, with projection being performed either at the end\nof the circuit or throughout the evolution. Our approach improves the fidelity\nof the QAOA state, thereby increasing both the accuracy of the sample estimate\nof the QAOA objective and the probability of sampling the binary string\ncorresponding to that objective value. We demonstrate the efficacy of the\nproposed methods on QAOA applied to the MaxCut problem, although our methods\nare general and apply to any objective function with symmetries, as well as to\nthe generalization of QAOA with alternative mixers. We experimentally verify\nthe proposed methods on an IBM Quantum processor, utilizing up to 5 qubits.\nWhen leveraging a global bit-flip symmetry, our approach leads to a 23% average\nimprovement in quantum state fidelity.",
    "descriptor": "",
    "authors": [
      "Ruslan Shaydulin",
      "Alexey Galda"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.04410"
  },
  {
    "id": "arXiv:2106.04416",
    "title": "Context-Specific Causal Discovery for Categorical Data Using Staged  Trees",
    "abstract": "Causal discovery algorithms aims at untangling complex causal relationships\nusing observational data only. Here, we introduce new causal discovery\nalgorithms based on staged tree models, which can represent complex and\nnon-symmetric causal effects. To demonstrate the efficacy of our algorithms, we\nintroduce a new distance, inspired by the widely used structural interventional\ndistance, to quantify the closeness between two staged trees in terms of their\ncorresponding causal inference statements. A simulation study highlights the\nefficacy of staged trees in uncovering complex, asymmetric causal relationship\nfrom data and a real-world data application illustrates their use in a\npractical causal analysis.",
    "descriptor": "",
    "authors": [
      "Manuele Leonelli",
      "Gherardo Varando"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04416"
  },
  {
    "id": "arXiv:2106.04436",
    "title": "Cooperation and competition between pair and multi-player social games  in spatial populations",
    "abstract": "The conflict between individual and collective interests is in the heart of\nevery social dilemmas established by evolutionary game theory. We cannot avoid\nthese conflicts but sometimes we may choose which interaction framework to use\nas a battlefield. For instance some people like to be part of a larger group\nwhile other persons prefer to interact in a more personalized, individual way.\nBoth attitudes can be formulated via appropriately chosen traditional games. In\nparticular, the prisoner's dilemma game is based on pair interaction while the\npublic goods game represents multi-point interactions of group members. To\nreveal the possible advantage of a certain attitude we extend these models by\nallowing players not simply to change their strategies but also let them to\nvary their attitudes for a higher individual income. We show that both\nattitudes could be the winner at a specific parameter value. Interestingly,\nhowever, the subtle interplay between different states may result in a\ncounterintuitive evolutionary outcome where the increase of the multiplication\nfactor of public goods game drives the population to a fully defector state. We\npoint out that the accompanying pattern formation can only be understood via\nthe multipoint or multi-player interactions of different microscopic states\nwhere the vicinity of a particular state may influence the relation of two\nother competitors.",
    "descriptor": "\nComments: 18 pages, preprint format, 6 figures, accepted for publication in Sci. Rep\n",
    "authors": [
      "Attila Szolnoki",
      "Xiaojie Chen"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computer Science and Game Theory (cs.GT)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2106.04436"
  },
  {
    "id": "arXiv:2106.04452",
    "title": "3KG: Contrastive Learning of 12-Lead Electrocardiograms using  Physiologically-Inspired Augmentations",
    "abstract": "Self-supervised contrastive learning approaches leverage modality-specific\ncontext or invariances to pretrain models using unlabeled data. While\ncontrastive learning has demonstrated promising on results in the image domain,\nthere has been limited work on determining how to exploit modality-specific\ninvariances in biosignals such as the electrocardiogram. In this work, we\npropose 3KG, a method to generate positive pairs for contrastive learning using\nphysiologically-inspired 3D augmentations of the 12-lead electrocardiogram. We\nevaluate representation quality by fine-tuning a linear layer for the\ndownstream task of 24-class diagnosis on the PhysioNet 2020 challenge training\ndata, and find that models trained with physiologically-inspired augmentations\nboth outperform and complement standard time-series augmentations. Our best\nperforming strategy, which incorporates spatial rotation, spatial scaling, and\ntime masking, achieves a performance increase of 0.16, .086, and .046 in mean\nAUROC over a randomly initialized baseline at 1%, 10%, and 100% label fractions\nrespectively. Additionally, we show that the strength of spatial augmentations\ndoes not significantly affect the quality of the learned representations.\nFinally, we investigate the clinical relevance of how physiologically-inspired\naugmentations affect the performance of our classifier on different disease\nsubgroupings. As expert annotations are often expensive and scarce for medical\ncontexts, our approach highlights the potential of machine learning to tackle\nmedical problems with large quantities of unlabeled biosignal data by\nexploiting their unique biological properties.",
    "descriptor": "",
    "authors": [
      "Bryan Gopal",
      "Ryan W. Han",
      "Gautham Raghupathi",
      "Andrew Y. Ng",
      "Geoffrey H. Tison",
      "Pranav Rajpurkar"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.04452"
  },
  {
    "id": "arXiv:2106.04455",
    "title": "Adaptive transfer learning",
    "abstract": "In transfer learning, we wish to make inference about a target population\nwhen we have access to data both from the distribution itself, and from a\ndifferent but related source distribution. We introduce a flexible framework\nfor transfer learning in the context of binary classification, allowing for\ncovariate-dependent relationships between the source and target distributions\nthat are not required to preserve the Bayes decision boundary. Our main\ncontributions are to derive the minimax optimal rates of convergence (up to\npoly-logarithmic factors) in this problem, and show that the optimal rate can\nbe achieved by an algorithm that adapts to key aspects of the unknown transfer\nrelationship, as well as the smoothness and tail parameters of our\ndistributional classes. This optimal rate turns out to have several regimes,\ndepending on the interplay between the relative sample sizes and the strength\nof the transfer relationship, and our algorithm achieves optimality by careful,\ndecision tree-based calibration of local nearest-neighbour procedures.",
    "descriptor": "",
    "authors": [
      "Henry W. J. Reeve",
      "Timothy I. Cannings",
      "Richard J. Samworth"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04455"
  },
  {
    "id": "arXiv:2106.04463",
    "title": "PolypGen: A multi-center polyp detection and segmentation dataset for  generalisability assessment",
    "abstract": "Polyps in the colon are widely known as cancer precursors identified by\ncolonoscopy either related to diagnostic work-up for symptoms, colorectal\ncancer screening or systematic surveillance of certain diseases. Whilst most\npolyps are benign, the number, size and the surface structure of the polyp are\ntightly linked to the risk of colon cancer. There exists a high missed\ndetection rate and incomplete removal of colon polyps due to the variable\nnature, difficulties to delineate the abnormality, high recurrence rates and\nthe anatomical topography of the colon. In the past, several methods have been\nbuilt to automate polyp detection and segmentation. However, the key issue of\nmost methods is that they have not been tested rigorously on a large\nmulti-center purpose-built dataset. Thus, these methods may not generalise to\ndifferent population datasets as they overfit to a specific population and\nendoscopic surveillance. To this extent, we have curated a dataset from 6\ndifferent centers incorporating more than 300 patients. The dataset includes\nboth single frame and sequence data with 3446 annotated polyp labels with\nprecise delineation of polyp boundaries verified by six senior\ngastroenterologists. To our knowledge, this is the most comprehensive detection\nand pixel-level segmentation dataset curated by a team of computational\nscientists and expert gastroenterologists. This dataset has been originated as\nthe part of the Endocv2021 challenge aimed at addressing generalisability in\npolyp detection and segmentation. In this paper, we provide comprehensive\ninsight into data construction and annotation strategies, annotation quality\nassurance and technical validation for our extended EndoCV2021 dataset which we\nrefer to as PolypGen.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Sharib Ali",
      "Debesh Jha",
      "Noha Ghatwary",
      "Stefano Realdon",
      "Renato Cannizzaro",
      "Osama E. Salem",
      "Dominique Lamarque",
      "Christian Daul",
      "Kim V. Anonsen",
      "Michael A. Riegler",
      "P\u00e5l Halvorsen",
      "Jens Rittscher",
      "Thomas de Lange",
      "James E. East"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04463"
  },
  {
    "id": "arXiv:2106.04464",
    "title": "Augmenting Molecular Deep Generative Models with Topological Data  Analysis Representations",
    "abstract": "Deep generative models have emerged as a powerful tool for learning\ninformative molecular representations and designing novel molecules with\ndesired properties, with applications in drug discovery and material design.\nDeep generative auto-encoders defined over molecular SMILES strings have been a\npopular choice for that purpose. However, capturing salient molecular\nproperties like quantum-chemical energies remains challenging and requires\nsophisticated neural net models of molecular graphs or geometry-based\ninformation. As a simpler and more efficient alternative, we present a SMILES\nVariational Auto-Encoder (VAE) augmented with topological data analysis (TDA)\nrepresentations of molecules, known as persistence images. Our experiments show\nthat this TDA augmentation enables a SMILES VAE to capture the complex relation\nbetween 3D geometry and electronic properties, and allows generation of novel,\ndiverse, and valid molecules with geometric features consistent with the\ntraining data, which exhibit a varying range of global electronic structural\nproperties, such as a small HOMO-LUMO gap - a critical property for designing\norganic solar cells. We demonstrate that our TDA augmentation yields better\nsuccess in downstream tasks compared to models trained without these\nrepresentations and can assist in targeted molecule discovery.",
    "descriptor": "",
    "authors": [
      "Yair Schiff",
      "Vijil Chenthamarakshan",
      "Samuel Hoffman",
      "Karthikeyan Natesan Ramamurthy",
      "Payel Das"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2106.04464"
  },
  {
    "id": "arXiv:2106.04469",
    "title": "Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex  Decentralized Optimization Over Time-Varying Networks",
    "abstract": "We consider the task of minimizing the sum of smooth and strongly convex\nfunctions stored in a decentralized manner across the nodes of a communication\nnetwork whose links are allowed to change in time. We solve two fundamental\nproblems for this task. First, we establish the first lower bounds on the\nnumber of decentralized communication rounds and the number of local\ncomputations required to find an $\\epsilon$-accurate solution. Second, we\ndesign two optimal algorithms that attain these lower bounds: (i) a variant of\nthe recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a\nmulti-consensus subroutine, which is optimal in the case when access to the\ndual gradients is assumed, and (ii) a novel algorithm, called ADOM+, which is\noptimal in the case when access to the primal gradients is assumed. We\ncorroborate the theoretical efficiency of these algorithms by performing an\nexperimental comparison with existing state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Dmitry Kovalev",
      "Elnur Gasanov",
      "Peter Richt\u00e1rik",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04469"
  },
  {
    "id": "arXiv:2106.04492",
    "title": "Description and Discussion on DCASE 2021 Challenge Task 2: Unsupervised  Anomalous Sound Detection for Machine Condition Monitoring under Domain  Shifted Conditions",
    "abstract": "We present the task description and discussion on the results of the DCASE\n2021 Challenge Task 2. Last year, we organized unsupervised anomalous sound\ndetection (ASD) task; identifying whether the given sound is normal or\nanomalous without anomalous training data. In this year, we organize an\nadvanced unsupervised ASD task under domain-shift conditions which focuses on\nthe inevitable problem for the practical use of ASD systems. The main challenge\nof this task is to detect unknown anomalous sounds where the acoustic\ncharacteristics of the training and testing samples are different, i.e.\ndomain-shifted. This problem is frequently occurs due to changes in seasons,\nmanufactured products, and/or environmental noise. After the challenge\nsubmission deadline, we will add challenge results and analysis of the\nsubmissions.",
    "descriptor": "\nComments: Submitted to DCASE 2021 Workshop. arXiv admin note: text overlap with arXiv:2006.05822\n",
    "authors": [
      "Yohei Kawaguchi",
      "Keisuke Imoto",
      "Yuma Koizumi",
      "Noboru Harada",
      "Daisuke Niizumi",
      "Kota Dohi",
      "Ryo Tanabe",
      "Harsh Purohit",
      "Takashi Endo"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04492"
  },
  {
    "id": "arXiv:2106.04508",
    "title": "Energy-Efficient Adaptive System Reconfiguration for Dynamic Deadlines  in Autonomous Driving",
    "abstract": "The increasing computing demands of autonomous driving applications make\nenergy optimizations critical for reducing battery capacity and vehicle weight.\nCurrent energy optimization methods typically target traditional real-time\nsystems with static deadlines, resulting in conservative energy savings that\nare unable to exploit additional energy optimizations due to dynamic deadlines\narising from the vehicle's change in velocity and driving context. We present\nan adaptive system optimization and reconfiguration approach that dynamically\nadapts the scheduling parameters and processor speeds to satisfy dynamic\ndeadlines while consuming as little energy as possible. Our experimental\nresults with an autonomous driving task set from Bosch and real-world driving\ndata show energy reductions up to 46.4% on average in typical dynamic driving\nscenarios compared with traditional static energy optimization methods,\ndemonstrating great potential for dynamic energy optimization gains by\nexploiting dynamic deadlines.",
    "descriptor": "\nComments: IEEE ISORC 2021\n",
    "authors": [
      "Saehanseul Yi",
      "Tae-Wook Kim",
      "Jong-Chan Kim",
      "Nikil Dutt"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04508"
  },
  {
    "id": "arXiv:2106.04509",
    "title": "MoCL: Contrastive Learning on Molecular Graphs with Multi-level Domain  Knowledge",
    "abstract": "Recent years have seen a rapid growth of utilizing graph neural networks\n(GNNs) in the biomedical domain for tackling drug-related problems. However,\nlike any other deep architectures, GNNs are data hungry. While requiring labels\nin real world is often expensive, pretraining GNNs in an unsupervised manner\nhas been actively explored. Among them, graph contrastive learning, by\nmaximizing the mutual information between paired graph augmentations, has been\nshown to be effective on various downstream tasks. However, the current graph\ncontrastive learning framework has two limitations. First, the augmentations\nare designed for general graphs and thus may not be suitable or powerful enough\nfor certain domains. Second, the contrastive scheme only learns representations\nthat are invariant to local perturbations and thus does not consider the global\nstructure of the dataset, which may also be useful for downstream tasks.\nTherefore, in this paper, we study graph contrastive learning in the context of\nbiomedical domain, where molecular graphs are present. We propose a novel\nframework called MoCL, which utilizes domain knowledge at both local- and\nglobal-level to assist representation learning. The local-level domain\nknowledge guides the augmentation process such that variation is introduced\nwithout changing graph semantics. The global-level knowledge encodes the\nsimilarity information between graphs in the entire dataset and helps to learn\nrepresentations with richer semantics. The entire model is learned through a\ndouble contrast objective. We evaluate MoCL on various molecular datasets under\nboth linear and semi-supervised settings and results show that MoCL achieves\nstate-of-the-art performance.",
    "descriptor": "\nComments: KDD 2021\n",
    "authors": [
      "Mengying Sun",
      "Jing Xing",
      "Huijun Wang",
      "Bin Chen",
      "Jiayu Zhou"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04509"
  },
  {
    "id": "arXiv:2106.04510",
    "title": "Random Forest classifier for EEG-based seizure prediction",
    "abstract": "Epileptic seizure prediction has gained considerable interest in the\ncomputational Epilepsy research community. This paper presents a Machine\nLearning based method for epileptic seizure prediction which outperforms\nstate-of-the art methods. We compute a probability for a given epoch, of being\npre-ictal against interictal using the Random Forest classifier and introduce\nnew concepts to enhance the robustness of the algorithm to false alarms. We\nassessed our method on 20 patients of the benchmark scalp EEG CHB-MIT dataset\nfor a seizure prediction horizon (SPH) of 5 minutes and a seizure occurrence\nperiod (SOP) of 30 minutes. Our approach achieves a sensitivity of 82.07 % and\na low false positive rate (FPR) of 0.0799 /h. We also tested our approach on\nintracranial EEG recordings.",
    "descriptor": "\nComments: all the python code used for this work can be found in this repository: this https URL\n",
    "authors": [
      "Remy Ben Messaoud",
      "Mario Chavez"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.04510"
  },
  {
    "id": "arXiv:2106.04540",
    "title": "Object Based Attention Through Internal Gating",
    "abstract": "Object-based attention is a key component of the visual system, relevant for\nperception, learning, and memory. Neurons tuned to features of attended objects\ntend to be more active than those associated with non-attended objects. There\nis a rich set of models of this phenomenon in computational neuroscience.\nHowever, there is currently a divide between models that successfully match\nphysiological data but can only deal with extremely simple problems and models\nof attention used in computer vision. For example, attention in the brain is\nknown to depend on top-down processing, whereas self-attention in deep learning\ndoes not. Here, we propose an artificial neural network model of object-based\nattention that captures the way in which attention is both top-down and\nrecurrent. Our attention model works well both on simple test stimuli, such as\nthose using images of handwritten digits, and on more complex stimuli, such as\nnatural images drawn from the COCO dataset. We find that our model replicates a\nrange of findings from neuroscience, including attention-invariant tuning,\ninhibition of return, and attention-mediated scaling of activity. Understanding\nobject based attention is both computationally interesting and a key problem\nfor computational neuroscience.",
    "descriptor": "",
    "authors": [
      "Jordan Lei",
      "Ari S. Benjamin",
      "Konrad P. Kording"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.04540"
  },
  {
    "id": "arXiv:1606.07886",
    "title": "Timed Multiset Rewriting and the Verification of Time-Sensitive  Distributed Systems",
    "abstract": "Comments: Updated version with corrected proofs",
    "descriptor": "\nComments: Updated version with corrected proofs\n",
    "authors": [
      "Max Kanovich",
      "Tajana Ban Kirigin",
      "Vivek Nigam",
      "Andre Scedrov",
      "Carolyn Talcott"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/1606.07886"
  },
  {
    "id": "arXiv:1703.01610",
    "title": "Improving Regret Bounds for Combinatorial Semi-Bandits with  Probabilistically Triggered Arms and Its Applications",
    "abstract": "Comments: This is the full version of the paper accepted at NIPS'2017",
    "descriptor": "\nComments: This is the full version of the paper accepted at NIPS'2017\n",
    "authors": [
      "Qinshi Wang",
      "Wei Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1703.01610"
  },
  {
    "id": "arXiv:1708.00568",
    "title": "On $w$-mixtures: Finite convex combinations of prescribed component  distributions",
    "abstract": "Comments: 34 pages, extend a preliminary paper (ICASSP 2018)",
    "descriptor": "\nComments: 34 pages, extend a preliminary paper (ICASSP 2018)\n",
    "authors": [
      "Frank Nielsen",
      "Richard Nock"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1708.00568"
  },
  {
    "id": "arXiv:1805.10174",
    "title": "f-CNN$^{\\text{x}}$: A Toolflow for Mapping Multi-CNN Applications on  FPGAs",
    "abstract": "Comments: Accepted at the 28th International Conference on Field Programmable Logic & Applications (FPL) 2018",
    "descriptor": "\nComments: Accepted at the 28th International Conference on Field Programmable Logic & Applications (FPL) 2018\n",
    "authors": [
      "Stylianos I. Venieris",
      "Christos-Savvas Bouganis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/1805.10174"
  },
  {
    "id": "arXiv:1908.06656",
    "title": "On the edge-biclique graph and the iterated edge-biclique operator",
    "abstract": "On the edge-biclique graph and the iterated edge-biclique operator",
    "descriptor": "",
    "authors": [
      "Leandro Montero",
      "Sylvain Legay"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/1908.06656"
  },
  {
    "id": "arXiv:1912.00670",
    "title": "An improved approximation algorithm for ATSP",
    "abstract": "An improved approximation algorithm for ATSP",
    "descriptor": "",
    "authors": [
      "Vera Traub",
      "Jens Vygen"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1912.00670"
  },
  {
    "id": "arXiv:1912.02631",
    "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning",
    "abstract": "Comments: This work appeared at the 26th Annual Network and Distributed System Security Symposium (NDSS) 2020. Update: An improved version of this framework is available at arXiv:2106.02850",
    "descriptor": "\nComments: This work appeared at the 26th Annual Network and Distributed System Security Symposium (NDSS) 2020. Update: An improved version of this framework is available at arXiv:2106.02850\n",
    "authors": [
      "Harsh Chaudhari",
      "Rahul Rachuri",
      "Ajith Suresh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1912.02631"
  },
  {
    "id": "arXiv:2001.04283",
    "title": "Electricity prices and tariffs to keep everyone happy: a framework for  fixed and nodal prices coexistence in distribution grids with optimal tariffs  for investment cost recovery",
    "abstract": "Electricity prices and tariffs to keep everyone happy: a framework for  fixed and nodal prices coexistence in distribution grids with optimal tariffs  for investment cost recovery",
    "descriptor": "",
    "authors": [
      "Iacopo Savelli",
      "Thomas Morstyn"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2001.04283"
  },
  {
    "id": "arXiv:2002.03968",
    "title": "E-APR: Mapping the Effectiveness of Automated Program Repair",
    "abstract": "E-APR: Mapping the Effectiveness of Automated Program Repair",
    "descriptor": "",
    "authors": [
      "Aldeida Aleti",
      "Matias Martinez"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2002.03968"
  },
  {
    "id": "arXiv:2002.07676",
    "title": "A Possibility in Algorithmic Fairness: Can Calibration and Equal Error  Rates Be Reconciled?",
    "abstract": "Comments: 2nd Symposium on Foundations of Responsible Computing (FORC 2021) this https URL",
    "descriptor": "\nComments: 2nd Symposium on Foundations of Responsible Computing (FORC 2021) this https URL\n",
    "authors": [
      "Claire Lazar Reich",
      "Suhas Vijaykumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.07676"
  },
  {
    "id": "arXiv:2002.09721",
    "title": "General theory of interpolation error estimates on anisotropic meshes",
    "abstract": "Comments: 29 pages, 2 figures. In \"General theory of interpolation error estimates on anisotropic meshes\" (Japan Journal of Industrial and Applied Mathematics, 38 (2021) 163-191), Theorem 2 has been found to be incorrect and misleading. Corrections to an error is shown in \"General theory of interpolation error estimates on anisotropic meshes, part II\" (arXiv:2106.03339)",
    "descriptor": "\nComments: 29 pages, 2 figures. In \"General theory of interpolation error estimates on anisotropic meshes\" (Japan Journal of Industrial and Applied Mathematics, 38 (2021) 163-191), Theorem 2 has been found to be incorrect and misleading. Corrections to an error is shown in \"General theory of interpolation error estimates on anisotropic meshes, part II\" (arXiv:2106.03339)\n",
    "authors": [
      "Hiroki Ishizaka",
      "Kenta Kobayashi",
      "Takuya Tsuchiya"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2002.09721"
  },
  {
    "id": "arXiv:2002.12690",
    "title": "Criteria for the numerical constant recognition",
    "abstract": "Comments: 20 pages + Supplemental Material",
    "descriptor": "\nComments: 20 pages + Supplemental Material\n",
    "authors": [
      "Andrzej Odrzywolek"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Symbolic Computation (cs.SC)",
      "Other Statistics (stat.OT)"
    ],
    "url": "https://arxiv.org/abs/2002.12690"
  },
  {
    "id": "arXiv:2003.06566",
    "title": "On the benefits of defining vicinal distributions in latent space",
    "abstract": "Comments: $\\textbf{Best Paper Award}$ at CVPR 2021 Workshop on $\\textbf{Adversarial Machine Learning in Real-World Computer Vision (AML-CV)}$. Also accepted at ICLR 2021 Workshops on $\\textbf{Robust-Reliable Machine Learning}$ (Oral) and $\\textbf{Generalization beyond the training distribution}$ (Abstract)",
    "descriptor": "\nComments: $\\textbf{Best Paper Award}$ at CVPR 2021 Workshop on $\\textbf{Adversarial Machine Learning in Real-World Computer Vision (AML-CV)}$. Also accepted at ICLR 2021 Workshops on $\\textbf{Robust-Reliable Machine Learning}$ (Oral) and $\\textbf{Generalization beyond the training distribution}$ (Abstract)\n",
    "authors": [
      "Puneet Mangla",
      "Vedant Singh",
      "Shreyas Jayant Havaldar",
      "Vineeth N Balasubramanian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.06566"
  },
  {
    "id": "arXiv:2003.14204",
    "title": "Verification of Nonblockingness in Bounded Petri Nets With Minimax Basis  Reachability Graphs",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Chao Gu",
      "Ziyue Ma",
      "Zhiwu Li",
      "Alessandro Giua"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2003.14204"
  },
  {
    "id": "arXiv:2004.02251",
    "title": "Semantics of the Unwritten: The Effect of End of Paragraph and Sequence  Tokens on Text Generation with GPT2",
    "abstract": "Comments: Accepted by ACL-IJCNLP SRW 2021",
    "descriptor": "\nComments: Accepted by ACL-IJCNLP SRW 2021\n",
    "authors": [
      "He Bai",
      "Peng Shi",
      "Jimmy Lin",
      "Luchen Tan",
      "Kun Xiong",
      "Wen Gao",
      "Jie Liu",
      "Ming Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2004.02251"
  },
  {
    "id": "arXiv:2004.03959",
    "title": "The Loss Surfaces of Neural Networks with General Activation Functions",
    "abstract": "Comments: 50 pages, 11 figures; references added for Kac-Rice reduction to RMT method; updates following JSTAT review and publication",
    "descriptor": "\nComments: 50 pages, 11 figures; references added for Kac-Rice reduction to RMT method; updates following JSTAT review and publication\n",
    "authors": [
      "Nicholas P. Baskerville",
      "Jonathan P. Keating",
      "Francesco Mezzadri",
      "Joseph Najnudel"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2004.03959"
  },
  {
    "id": "arXiv:2004.04460",
    "title": "PANDORA Talks: Personality and Demographics on Reddit",
    "abstract": "Comments: Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, NAACL 2021, this https URL",
    "descriptor": "\nComments: Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, NAACL 2021, this https URL\n",
    "authors": [
      "Matej Gjurkovi\u0107",
      "Mladen Karan",
      "Iva Vukojevi\u0107",
      "Mihaela Bo\u0161njak",
      "Jan \u0160najder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2004.04460"
  },
  {
    "id": "arXiv:2004.07601",
    "title": "Suicidal Ideation and Mental Disorder Detection with Attentive Relation  Networks",
    "abstract": "Suicidal Ideation and Mental Disorder Detection with Attentive Relation  Networks",
    "descriptor": "",
    "authors": [
      "Shaoxiong Ji",
      "Xue Li",
      "Zi Huang",
      "Erik Cambria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2004.07601"
  },
  {
    "id": "arXiv:2004.15008",
    "title": "Lexical Semantic Recognition",
    "abstract": "Comments: 11 pages, 3 figures; to appear at MWE 2021",
    "descriptor": "\nComments: 11 pages, 3 figures; to appear at MWE 2021\n",
    "authors": [
      "Nelson F. Liu",
      "Daniel Hershcovich",
      "Michael Kranzlein",
      "Nathan Schneider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2004.15008"
  },
  {
    "id": "arXiv:2005.00792",
    "title": "ForecastQA: A Question Answering Challenge for Event Forecasting with  Temporal Text Data",
    "abstract": "Comments: Accepted to ACL 2021. Project page: this https URL",
    "descriptor": "\nComments: Accepted to ACL 2021. Project page: this https URL\n",
    "authors": [
      "Woojeong Jin",
      "Rahul Khanna",
      "Suji Kim",
      "Dong-Ho Lee",
      "Fred Morstatter",
      "Aram Galstyan",
      "Xiang Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.00792"
  },
  {
    "id": "arXiv:2005.00976",
    "title": "A Concise yet Effective model for Non-Aligned Incomplete Multi-view and  Missing Multi-label Learning",
    "abstract": "Comments: 15 pages, 7 figures",
    "descriptor": "\nComments: 15 pages, 7 figures\n",
    "authors": [
      "Xiang Li",
      "Songcan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2005.00976"
  },
  {
    "id": "arXiv:2005.07519",
    "title": "Evaluating and Improving Adversarial Robustness of Machine  Learning-Based Network Intrusion Detectors",
    "abstract": "Comments: This article has been accepted for publication by IEEE JSAC",
    "descriptor": "\nComments: This article has been accepted for publication by IEEE JSAC\n",
    "authors": [
      "Dongqi Han",
      "Zhiliang Wang",
      "Ying Zhong",
      "Wenqi Chen",
      "Jiahai Yang",
      "Shuqiang Lu",
      "Xingang Shi",
      "Xia Yin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2005.07519"
  },
  {
    "id": "arXiv:2005.09959",
    "title": "Psychometrics in Behavioral Software Engineering: A Methodological  Introduction with Guidelines",
    "abstract": "Comments: 56 pages (pp. 1-36 for the main paper, pp. 37-56 working example in the appendix), 8 figures in the main paper. Accepted for publication at ACM TOSEM",
    "descriptor": "\nComments: 56 pages (pp. 1-36 for the main paper, pp. 37-56 working example in the appendix), 8 figures in the main paper. Accepted for publication at ACM TOSEM\n",
    "authors": [
      "Daniel Graziotin",
      "Per Lenberg",
      "Robert Feldt",
      "Stefan Wagner"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2005.09959"
  },
  {
    "id": "arXiv:2005.12979",
    "title": "Seamlessly Unifying Attributes and Items: Conversational Recommendation  for Cold-Start Users",
    "abstract": "Comments: TOIS 2021",
    "descriptor": "\nComments: TOIS 2021\n",
    "authors": [
      "Shijun Li",
      "Wenqiang Lei",
      "Qingyun Wu",
      "Xiangnan He",
      "Peng Jiang",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.12979"
  },
  {
    "id": "arXiv:2006.04492",
    "title": "Speedy Performance Estimation for Neural Architecture Search",
    "abstract": "Comments: 23 pages, 14 figures",
    "descriptor": "\nComments: 23 pages, 14 figures\n",
    "authors": [
      "Binxin Ru",
      "Clare Lyle",
      "Lisa Schut",
      "Miroslav Fil",
      "Mark van der Wilk",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.04492"
  },
  {
    "id": "arXiv:2006.04740",
    "title": "The Heavy-Tail Phenomenon in SGD",
    "abstract": "The Heavy-Tail Phenomenon in SGD",
    "descriptor": "",
    "authors": [
      "Mert Gurbuzbalaban",
      "Umut Simsekli",
      "Lingjiong Zhu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2006.04740"
  },
  {
    "id": "arXiv:2006.04873",
    "title": "A Stochastic Subgradient Method for Distributionally Robust Non-Convex  Learning",
    "abstract": "A Stochastic Subgradient Method for Distributionally Robust Non-Convex  Learning",
    "descriptor": "",
    "authors": [
      "Mert G\u00fcrb\u00fczbalaban",
      "Andrzej Ruszczy\u0144ski",
      "Landi Zhu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2006.04873"
  },
  {
    "id": "arXiv:2006.05170",
    "title": "A pseudo-spectral Strang splitting method for linear dispersive problems  with transparent boundary conditions",
    "abstract": "A pseudo-spectral Strang splitting method for linear dispersive problems  with transparent boundary conditions",
    "descriptor": "",
    "authors": [
      "Lukas Einkemmer",
      "Alexander Ostermann",
      "Mirko Residori"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2006.05170"
  },
  {
    "id": "arXiv:2006.05356",
    "title": "Scalable Thompson Sampling using Sparse Gaussian Process Models",
    "abstract": "Scalable Thompson Sampling using Sparse Gaussian Process Models",
    "descriptor": "",
    "authors": [
      "Sattar Vakili",
      "Henry Moss",
      "Artem Artemev",
      "Vincent Dutordoir",
      "Victor Picheny"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.05356"
  },
  {
    "id": "arXiv:2006.05681",
    "title": "Off-Policy Risk-Sensitive Reinforcement Learning Based Constrained  Robust Optimal Control",
    "abstract": "Off-Policy Risk-Sensitive Reinforcement Learning Based Constrained  Robust Optimal Control",
    "descriptor": "",
    "authors": [
      "Cong Li",
      "Fangzhou Liu",
      "Zhehua Zhou",
      "Martin Buss"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2006.05681"
  },
  {
    "id": "arXiv:2006.08831",
    "title": "Physics-aware Spatiotemporal Modules with Auxiliary Tasks for  Meta-Learning",
    "abstract": "Comments: To be published in the 30th International Joint Conference on Artificial Intelligence (IJCAI-21)",
    "descriptor": "\nComments: To be published in the 30th International Joint Conference on Artificial Intelligence (IJCAI-21)\n",
    "authors": [
      "Sungyong Seo",
      "Chuizheng Meng",
      "Sirisha Rambhatla",
      "Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.08831"
  },
  {
    "id": "arXiv:2006.11223",
    "title": "Unified Representation Learning for Efficient Medical Image Analysis",
    "abstract": "Unified Representation Learning for Efficient Medical Image Analysis",
    "descriptor": "",
    "authors": [
      "Ghada Zamzmi",
      "Sivaramakrishnan Rajaraman",
      "Sameer Antani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.11223"
  },
  {
    "id": "arXiv:2006.11741",
    "title": "Isometric Gaussian Process Latent Variable Model for Dissimilarity Data",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Martin J\u00f8rgensen",
      "S\u00f8ren Hauberg"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.11741"
  },
  {
    "id": "arXiv:2006.15615",
    "title": "A Real-Time Dispatching Strategy for Shared Automated Electric Vehicles  with Performance Guarantees",
    "abstract": "A Real-Time Dispatching Strategy for Shared Automated Electric Vehicles  with Performance Guarantees",
    "descriptor": "",
    "authors": [
      "Li Li",
      "Theodoros Pantelidis",
      "Joseph Y.J. Chow",
      "Saif Eddin Jabari"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2006.15615"
  },
  {
    "id": "arXiv:2007.00992",
    "title": "Rethinking Channel Dimensions for Efficient Model Design",
    "abstract": "Comments: 13 pages, 8 figures, CVPR 2021",
    "descriptor": "\nComments: 13 pages, 8 figures, CVPR 2021\n",
    "authors": [
      "Dongyoon Han",
      "Sangdoo Yun",
      "Byeongho Heo",
      "YoungJoon Yoo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.00992"
  },
  {
    "id": "arXiv:2007.08199",
    "title": "Learning from Noisy Labels with Deep Neural Networks: A Survey",
    "abstract": "Comments: If your paper is missing, contact me: ghkswns91@gmail.com",
    "descriptor": "\nComments: If your paper is missing, contact me: ghkswns91@gmail.com\n",
    "authors": [
      "Hwanjun Song",
      "Minseok Kim",
      "Dongmin Park",
      "Yooju Shin",
      "Jae-Gil Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.08199"
  },
  {
    "id": "arXiv:2007.08319",
    "title": "Less is More: A privacy-respecting Android malware classifier using  Federated Learning",
    "abstract": "Comments: 20 pages, 8 figures",
    "descriptor": "\nComments: 20 pages, 8 figures\n",
    "authors": [
      "Rafa G\u00e1lvez",
      "Veelasha Moonsamy",
      "Claudia Diaz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.08319"
  },
  {
    "id": "arXiv:2007.12556",
    "title": "Dynamic proofs of retrievability with low server storage",
    "abstract": "Dynamic proofs of retrievability with low server storage",
    "descriptor": "",
    "authors": [
      "Gaspard Anthoine",
      "Jean-Guillaume Dumas",
      "Michael Hanling",
      "M\u00e9lanie de Jonghe",
      "Aude Maignan",
      "Cl\u00e9ment Pernet",
      "Daniel Roche"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2007.12556"
  },
  {
    "id": "arXiv:2008.03606",
    "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning",
    "abstract": "Comments: Version 2 provides stronger theoretical results and more thorough experiments",
    "descriptor": "\nComments: Version 2 provides stronger theoretical results and more thorough experiments\n",
    "authors": [
      "Sai Praneeth Karimireddy",
      "Martin Jaggi",
      "Satyen Kale",
      "Mehryar Mohri",
      "Sashank J. Reddi",
      "Sebastian U. Stich",
      "Ananda Theertha Suresh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.03606"
  },
  {
    "id": "arXiv:2009.00438",
    "title": "String Stability of Connected Vehicle Platoons under Lossy V2V  Communication",
    "abstract": "Comments: Final version accepted to IEEE Transactions on Intelligent Transportation Systems",
    "descriptor": "\nComments: Final version accepted to IEEE Transactions on Intelligent Transportation Systems\n",
    "authors": [
      "Vamsi Vegamoor",
      "Sivakumar Rathinam",
      "Swaroop Darbha"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2009.00438"
  },
  {
    "id": "arXiv:2009.04441",
    "title": "Addressing Fairness in Classification with a Model-Agnostic  Multi-Objective Algorithm",
    "abstract": "Comments: Accepted at UAI 2021. 14 pages, 5 figures, 4 tables",
    "descriptor": "\nComments: Accepted at UAI 2021. 14 pages, 5 figures, 4 tables\n",
    "authors": [
      "Kirtan Padh",
      "Diego Antognini",
      "Emma Lejal Glaude",
      "Boi Faltings",
      "Claudiu Musat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.04441"
  },
  {
    "id": "arXiv:2009.04949",
    "title": "Sum-Rank BCH Codes and Cyclic-Skew-Cyclic Codes",
    "abstract": "Comments: Lemma 26 has been rewritten with respect to the previous version",
    "descriptor": "\nComments: Lemma 26 has been rewritten with respect to the previous version\n",
    "authors": [
      "Umberto Mart\u00ednez-Pe\u00f1as"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2009.04949"
  },
  {
    "id": "arXiv:2009.05331",
    "title": "The PREVENTION Challenge: How Good Are Humans Predicting Lane Changes?",
    "abstract": "Comments: This work was accepted and presented at IEEE Intelligent Vehicles Symposium 2020",
    "descriptor": "\nComments: This work was accepted and presented at IEEE Intelligent Vehicles Symposium 2020\n",
    "authors": [
      "A. Quintanar",
      "R. Izquierdo",
      "I. Parra",
      "D. Fern\u00e1ndez-Llorca",
      "M. A. Sotelo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.05331"
  },
  {
    "id": "arXiv:2009.07526",
    "title": "CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation",
    "abstract": "Comments: Accepted by IJCAI 2021. SOLE copyright holder is IJCAI (International Joint Conferences on Artificial Intelligence)",
    "descriptor": "\nComments: Accepted by IJCAI 2021. SOLE copyright holder is IJCAI (International Joint Conferences on Artificial Intelligence)\n",
    "authors": [
      "Jing Yu",
      "Yuan Chai",
      "Yujing Wang",
      "Yue Hu",
      "Qi Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2009.07526"
  },
  {
    "id": "arXiv:2009.07707",
    "title": "DeepC2: AI-powered Covert Botnet Command and Control on OSNs",
    "abstract": "Comments: 13 pages, 15 figures, 7 tables. Discussion on possible countermeasures updated",
    "descriptor": "\nComments: 13 pages, 15 figures, 7 tables. Discussion on possible countermeasures updated\n",
    "authors": [
      "Zhi Wang",
      "Chaoge Liu",
      "Xiang Cui",
      "Di Wu",
      "Jie Yin",
      "Jiaxi Liu",
      "Jialong Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2009.07707"
  },
  {
    "id": "arXiv:2010.03449",
    "title": "Super-Human Performance in Online Low-latency Recognition of  Conversational Speech",
    "abstract": "Comments: To appear in Interspeech 2021",
    "descriptor": "\nComments: To appear in Interspeech 2021\n",
    "authors": [
      "Thai-Son Nguyen",
      "Sebastian Stueker",
      "Alex Waibel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.03449"
  },
  {
    "id": "arXiv:2010.04227",
    "title": "A low discrepancy sequence on graphs",
    "abstract": "Comments: Accepted for publication in Journal of Fourier Analysis and Applications",
    "descriptor": "\nComments: Accepted for publication in Journal of Fourier Analysis and Applications\n",
    "authors": [
      "A. Cloninger",
      "H. N. Mhaskar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2010.04227"
  },
  {
    "id": "arXiv:2010.04806",
    "title": "AutoQA: From Databases To QA Semantic Parsers With Only Synthetic  Training Data",
    "abstract": "Comments: To appear in EMNLP 2020",
    "descriptor": "\nComments: To appear in EMNLP 2020\n",
    "authors": [
      "Silei Xu",
      "Sina J. Semnani",
      "Giovanni Campagna",
      "Monica S. Lam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2010.04806"
  },
  {
    "id": "arXiv:2010.05620",
    "title": "$\\ell_0$-based Sparse Canonical Correlation Analysis",
    "abstract": "$\\ell_0$-based Sparse Canonical Correlation Analysis",
    "descriptor": "",
    "authors": [
      "Ofir Lindenbaum",
      "Moshe Salhov",
      "Amir Averbuch",
      "Yuval Kluger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.05620"
  },
  {
    "id": "arXiv:2010.06529",
    "title": "On the Fairness of Causal Algorithmic Recourse",
    "abstract": "Comments: v3 with additional experiments, new case study, new introduction, and revised structure",
    "descriptor": "\nComments: v3 with additional experiments, new case study, new introduction, and revised structure\n",
    "authors": [
      "Julius von K\u00fcgelgen",
      "Amir-Hossein Karimi",
      "Umang Bhatt",
      "Isabel Valera",
      "Adrian Weller",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.06529"
  },
  {
    "id": "arXiv:2010.09055",
    "title": "Large-Scale Maintenance and Unit Commitment: A Decentralized Subgradient  Approach",
    "abstract": "Large-Scale Maintenance and Unit Commitment: A Decentralized Subgradient  Approach",
    "descriptor": "",
    "authors": [
      "Paritosh Ramanan",
      "Murat Yildirim",
      "Nagi Gebraeel",
      "Edmond Chow"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2010.09055"
  },
  {
    "id": "arXiv:2010.10459",
    "title": "An Umbrella Converse for Data Exchange: Applied to Caching, Computing,  and Shuffling",
    "abstract": "Comments: 32 pages, refined some sections over previous version (shorter version appeared in ITW 2020)",
    "descriptor": "\nComments: 32 pages, refined some sections over previous version (shorter version appeared in ITW 2020)\n",
    "authors": [
      "Prasad Krishnan",
      "Lakshmi Natarajan",
      "V. Lalitha"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2010.10459"
  },
  {
    "id": "arXiv:2010.12163",
    "title": "Improved Worst-Case Regret Bounds for Randomized Least-Squares Value  Iteration",
    "abstract": "Comments: Updated version, bug fixed",
    "descriptor": "\nComments: Updated version, bug fixed\n",
    "authors": [
      "Priyank Agrawal",
      "Jinglin Chen",
      "Nan Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.12163"
  },
  {
    "id": "arXiv:2010.12684",
    "title": "Dynamic Contextualized Word Embeddings",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Valentin Hofmann",
      "Janet B. Pierrehumbert",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2010.12684"
  },
  {
    "id": "arXiv:2010.14771",
    "title": "Batch Reinforcement Learning with a Nonparametric Off-Policy Policy  Gradient",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2001.02435",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2001.02435\n",
    "authors": [
      "Samuele Tosatto",
      "Jo\u00e3o Carvalho",
      "Jan Peters"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2010.14771"
  },
  {
    "id": "arXiv:2010.15366",
    "title": "Stabilizing Label Assignment for Speech Separation by Self-supervised  Pre-training",
    "abstract": "Comments: Interspeech 2021",
    "descriptor": "\nComments: Interspeech 2021\n",
    "authors": [
      "Sung-Feng Huang",
      "Shun-Po Chuang",
      "Da-Rong Liu",
      "Yi-Chen Chen",
      "Gene-Ping Yang",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2010.15366"
  },
  {
    "id": "arXiv:2011.03700",
    "title": "On the Complexity of CSP-based Ideal Membership Problems",
    "abstract": "On the Complexity of CSP-based Ideal Membership Problems",
    "descriptor": "",
    "authors": [
      "Andrei A. Bulatov",
      "Akbar Rafiey"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Logic in Computer Science (cs.LO)",
      "Commutative Algebra (math.AC)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2011.03700"
  },
  {
    "id": "arXiv:2011.04408",
    "title": "SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and  Benchmark under Multiple Environments",
    "abstract": "Comments: 20 pages, 9 figures, 8 tables",
    "descriptor": "\nComments: 20 pages, 9 figures, 8 tables\n",
    "authors": [
      "Hanjiang Hu",
      "Baoquan Yang",
      "Zhijian Qiao",
      "Ding Zhao",
      "Hesheng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.04408"
  },
  {
    "id": "arXiv:2011.06331",
    "title": "Memetic Search for Vehicle Routing with Simultaneous Pickup-Delivery and  Time Windows",
    "abstract": "Memetic Search for Vehicle Routing with Simultaneous Pickup-Delivery and  Time Windows",
    "descriptor": "",
    "authors": [
      "Shengcai Liu",
      "Ke Tang",
      "Xin Yao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2011.06331"
  },
  {
    "id": "arXiv:2011.06806",
    "title": "On the stability properties of Gated Recurrent Units neural networks",
    "abstract": "Comments: Preprint submitted to the Elsevier Systems & Control Letters. Copyright may be transferred without notice",
    "descriptor": "\nComments: Preprint submitted to the Elsevier Systems & Control Letters. Copyright may be transferred without notice\n",
    "authors": [
      "Fabio Bonassi",
      "Marcello Farina",
      "Riccardo Scattolini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.06806"
  },
  {
    "id": "arXiv:2011.08225",
    "title": "Automatic selection of clustering algorithms using supervised graph  embedding",
    "abstract": "Automatic selection of clustering algorithms using supervised graph  embedding",
    "descriptor": "",
    "authors": [
      "Noy Cohen-Shapira",
      "Lior Rokach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.08225"
  },
  {
    "id": "arXiv:2011.08826",
    "title": "Deep Active Surface Models",
    "abstract": "Comments: 11 pages, 7 figures, 6 tables",
    "descriptor": "\nComments: 11 pages, 7 figures, 6 tables\n",
    "authors": [
      "Udaranga Wickramasinghe",
      "Graham Knott",
      "Pascal Fua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.08826"
  },
  {
    "id": "arXiv:2011.10450",
    "title": "Graph Tikhonov Regularization and Interpolation via Random Spanning  Forests",
    "abstract": "Graph Tikhonov Regularization and Interpolation via Random Spanning  Forests",
    "descriptor": "",
    "authors": [
      "Yusuf Pilavci",
      "Pierre-Olivier Amblard",
      "Simon Barthelme",
      "Nicolas Tremblay"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2011.10450"
  },
  {
    "id": "arXiv:2011.13034",
    "title": "Accommodating Picky Customers: Regret Bound and Exploration Complexity  for Multi-Objective Reinforcement Learning",
    "abstract": "Accommodating Picky Customers: Regret Bound and Exploration Complexity  for Multi-Objective Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Jingfeng Wu",
      "Vladimir Braverman",
      "Lin F. Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.13034"
  },
  {
    "id": "arXiv:2011.14791",
    "title": "NeuralFusion: Online Depth Fusion in Latent Space",
    "abstract": "NeuralFusion: Online Depth Fusion in Latent Space",
    "descriptor": "",
    "authors": [
      "Silvan Weder",
      "Johannes L. Sch\u00f6nberger",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.14791"
  },
  {
    "id": "arXiv:2011.15003",
    "title": "Convolutive Transfer Function Invariant SDR training criteria for  Multi-Channel Reverberant Speech Separation",
    "abstract": "Comments: Accepted by ICASSP 2021",
    "descriptor": "\nComments: Accepted by ICASSP 2021\n",
    "authors": [
      "Christoph Boeddeker",
      "Wangyou Zhang",
      "Tomohiro Nakatani",
      "Keisuke Kinoshita",
      "Tsubasa Ochiai",
      "Marc Delcroix",
      "Naoyuki Kamo",
      "Yanmin Qian",
      "Reinhold Haeb-Umbach"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2011.15003"
  },
  {
    "id": "arXiv:2012.00856",
    "title": "Using Formal Methods for Autonomous Systems: Five Recipes for Formal  Verification",
    "abstract": "Comments: Revision1, resubmitted to Journal of Risk and Reliability",
    "descriptor": "\nComments: Revision1, resubmitted to Journal of Risk and Reliability\n",
    "authors": [
      "Matt Luckcuck"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2012.00856"
  },
  {
    "id": "arXiv:2012.03085",
    "title": "Graph Mixture Density Networks",
    "abstract": "Graph Mixture Density Networks",
    "descriptor": "",
    "authors": [
      "Federico Errica",
      "Davide Bacciu",
      "Alessio Micheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.03085"
  },
  {
    "id": "arXiv:2012.03826",
    "title": "An Empirical Study of Assumptions in Bayesian Optimisation",
    "abstract": "An Empirical Study of Assumptions in Bayesian Optimisation",
    "descriptor": "",
    "authors": [
      "Alexander I. Cowen-Rivers",
      "Wenlong Lyu",
      "Rasul Tutunov",
      "Zhi Wang",
      "Antoine Grosnit",
      "Ryan Rhys Griffiths",
      "Hao Jianye",
      "Jun Wang",
      "Jan Peters",
      "Haitham Bou Ammar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2012.03826"
  },
  {
    "id": "arXiv:2012.04567",
    "title": "Bayesian Image Reconstruction using Deep Generative Models",
    "abstract": "Comments: 25 pages, 18 figures, 3 tables",
    "descriptor": "\nComments: 25 pages, 18 figures, 3 tables\n",
    "authors": [
      "Razvan V Marinescu",
      "Daniel Moyer",
      "Polina Golland"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.04567"
  },
  {
    "id": "arXiv:2012.05156",
    "title": "Implicit Regularization in ReLU Networks with the Square Loss",
    "abstract": "Comments: Small changes due to reviews",
    "descriptor": "\nComments: Small changes due to reviews\n",
    "authors": [
      "Gal Vardi",
      "Ohad Shamir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.05156"
  },
  {
    "id": "arXiv:2012.06260",
    "title": "Comparison of Anomaly Detectors: Context Matters",
    "abstract": "Comparison of Anomaly Detectors: Context Matters",
    "descriptor": "",
    "authors": [
      "V\u00edt \u0160kv\u00e1ra",
      "Jan Franc\u016f",
      "Mat\u011bj Zorek",
      "Tom\u00e1\u0161 Pevn\u00fd",
      "V\u00e1clav \u0160m\u00eddl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.06260"
  },
  {
    "id": "arXiv:2012.07498",
    "title": "Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape  Modeling and Reconstruction from Raw Point Clouds",
    "abstract": "Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape  Modeling and Reconstruction from Raw Point Clouds",
    "descriptor": "",
    "authors": [
      "Wenbin Zhao",
      "Jiabao Lei",
      "Yuxin Wen",
      "Jianguo Zhang",
      "Kui Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.07498"
  },
  {
    "id": "arXiv:2012.07654",
    "title": "Session-Aware Query Auto-completion using Extreme Multi-label Ranking",
    "abstract": "Comments: Accepted in KDD 2021",
    "descriptor": "\nComments: Accepted in KDD 2021\n",
    "authors": [
      "Nishant Yadav",
      "Rajat Sen",
      "Daniel N. Hill",
      "Arya Mazumdar",
      "Inderjit S. Dhillon"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.07654"
  },
  {
    "id": "arXiv:2012.08379",
    "title": "Efficient PTAS for the Maximum Traveling Salesman Problem in a Metric  Space of Fixed Doubling Dimension",
    "abstract": "Efficient PTAS for the Maximum Traveling Salesman Problem in a Metric  Space of Fixed Doubling Dimension",
    "descriptor": "",
    "authors": [
      "Vladimir Shenmaier"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2012.08379"
  },
  {
    "id": "arXiv:2012.09385",
    "title": "Balancing Geometry and Density: Path Distances on High-Dimensional Data",
    "abstract": "Balancing Geometry and Density: Path Distances on High-Dimensional Data",
    "descriptor": "",
    "authors": [
      "Anna Little",
      "Daniel McKenzie",
      "James Murphy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.09385"
  },
  {
    "id": "arXiv:2012.12218",
    "title": "BKT-LSTM: Efficient Student Modeling for knowledge tracing and student  performance prediction",
    "abstract": "BKT-LSTM: Efficient Student Modeling for knowledge tracing and student  performance prediction",
    "descriptor": "",
    "authors": [
      "Sein Minn"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.12218"
  },
  {
    "id": "arXiv:2012.13638",
    "title": "A Standard Grammar for Temporal Logics on Finite Traces",
    "abstract": "Comments: 17 pages",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Marco Favorito"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2012.13638"
  },
  {
    "id": "arXiv:2012.14350",
    "title": "DeepBeam: Deep Waveform Learning for Coordination-Free Beam Management  in mmWave Networks",
    "abstract": "Comments: 10 pages, 15 figures. Please cite it as M. Polese, F. Restuccia, and T. Melodia, \"DeepBeam: Deep Waveform Learning for Coordination-Free Beam Management in mmWave Networks\", Proc. of ACM Intl. Symp. on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing (ACM MobiHoc), October 2021",
    "descriptor": "\nComments: 10 pages, 15 figures. Please cite it as M. Polese, F. Restuccia, and T. Melodia, \"DeepBeam: Deep Waveform Learning for Coordination-Free Beam Management in mmWave Networks\", Proc. of ACM Intl. Symp. on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing (ACM MobiHoc), October 2021\n",
    "authors": [
      "Michele Polese",
      "Francesco Restuccia",
      "Tommaso Melodia"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.14350"
  },
  {
    "id": "arXiv:2012.14906",
    "title": "Decentralized Control with Graph Neural Networks",
    "abstract": "Decentralized Control with Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Fernando Gama",
      "Qingbiao Li",
      "Ekaterina Tolstaya",
      "Amanda Prorok",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2012.14906"
  },
  {
    "id": "arXiv:2012.15503",
    "title": "Patch-wise++ Perturbation for Adversarial Targeted Attacks",
    "abstract": "Patch-wise++ Perturbation for Adversarial Targeted Attacks",
    "descriptor": "",
    "authors": [
      "Lianli Gao",
      "Qilong Zhang",
      "Jingkuan Song",
      "Heng Tao Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.15503"
  },
  {
    "id": "arXiv:2012.15814",
    "title": "Language-Mediated, Object-Centric Representation Learning",
    "abstract": "Comments: ACL 2021 Findings. First two authors contributed equally; last two authors contributed equally. Project page: this https URL",
    "descriptor": "\nComments: ACL 2021 Findings. First two authors contributed equally; last two authors contributed equally. Project page: this https URL\n",
    "authors": [
      "Ruocheng Wang",
      "Jiayuan Mao",
      "Samuel J. Gershman",
      "Jiajun Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.15814"
  },
  {
    "id": "arXiv:2012.15859",
    "title": "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    "abstract": "Comments: In Proceedings of ACL 2021, 9 pages",
    "descriptor": "\nComments: In Proceedings of ACL 2021, 9 pages\n",
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Rebecca Marchant",
      "Ricardo Mu\u00f1oz Sanchez",
      "Mugdha Pandya",
      "Adam Lopez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.15859"
  },
  {
    "id": "arXiv:2101.00063",
    "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets",
    "abstract": "Comments: Accepted at ACL-IJCNLP 2021",
    "descriptor": "\nComments: Accepted at ACL-IJCNLP 2021\n",
    "authors": [
      "Xiaohan Chen",
      "Yu Cheng",
      "Shuohang Wang",
      "Zhe Gan",
      "Zhangyang Wang",
      "Jingjing Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.00063"
  },
  {
    "id": "arXiv:2101.01321",
    "title": "I-BERT: Integer-only BERT Quantization",
    "abstract": "I-BERT: Integer-only BERT Quantization",
    "descriptor": "",
    "authors": [
      "Sehoon Kim",
      "Amir Gholami",
      "Zhewei Yao",
      "Michael W. Mahoney",
      "Kurt Keutzer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.01321"
  },
  {
    "id": "arXiv:2101.01666",
    "title": "Robust R-Peak Detection in Low-Quality Holter ECGs using 1D  Convolutional Neural Network",
    "abstract": "Robust R-Peak Detection in Low-Quality Holter ECGs using 1D  Convolutional Neural Network",
    "descriptor": "",
    "authors": [
      "Muhammad Uzair Zahid",
      "Serkan Kiranyaz",
      "Turker Ince",
      "Ozer Can Devecioglu",
      "Muhammad E. H. Chowdhury",
      "Amith Khandakar",
      "Anas Tahir",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.01666"
  },
  {
    "id": "arXiv:2101.01785",
    "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic",
    "abstract": "Comments: All authors contributed equally. The order is alphabetical",
    "descriptor": "\nComments: All authors contributed equally. The order is alphabetical\n",
    "authors": [
      "Muhammad Abdul-Mageed",
      "AbdelRahim Elmadany",
      "El Moatez Billah Nagoudi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.01785"
  },
  {
    "id": "arXiv:2101.02322",
    "title": "Mesh Total Generalized Variation for Denoising",
    "abstract": "Mesh Total Generalized Variation for Denoising",
    "descriptor": "",
    "authors": [
      "Zheng Liu",
      "YanLei Li",
      "Weina Wang",
      "Ligang Liu",
      "Renjie Chen"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2101.02322"
  },
  {
    "id": "arXiv:2101.02555",
    "title": "Explainable AI and Adoption of Financial Algorithmic Advisors: an  Experimental Study",
    "abstract": "Comments: accepted: AIES '21",
    "descriptor": "\nComments: accepted: AIES '21\n",
    "authors": [
      "Daniel Ben David",
      "Yehezkel S. Resheff",
      "Talia Tron"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.02555"
  },
  {
    "id": "arXiv:2101.03735",
    "title": "Optimizing Biomanufacturing Harvesting Decisions under Limited  Historical Data",
    "abstract": "Comments: 36 pages, 6 figures",
    "descriptor": "\nComments: 36 pages, 6 figures\n",
    "authors": [
      "Bo Wang",
      "Wei Xie",
      "Tugce Martagan",
      "Alp Akcay",
      "Bram van Ravenstein"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.03735"
  },
  {
    "id": "arXiv:2101.06197",
    "title": "Deciding What to Learn: A Rate-Distortion Approach",
    "abstract": "Deciding What to Learn: A Rate-Distortion Approach",
    "descriptor": "",
    "authors": [
      "Dilip Arumugam",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2101.06197"
  },
  {
    "id": "arXiv:2101.06475",
    "title": "Slot Machines: Discovering Winning Combinations of Random Weights in  Neural Networks",
    "abstract": "Slot Machines: Discovering Winning Combinations of Random Weights in  Neural Networks",
    "descriptor": "",
    "authors": [
      "Maxwell Mbabilla Aladago",
      "Lorenzo Torresani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.06475"
  },
  {
    "id": "arXiv:2101.07196",
    "title": "Impact of COVID-19 on IoT Adoption in Healthcare, Smart Homes, Smart  Buildings, Smart Cities, Transportation and Industrial IoT",
    "abstract": "Comments: This is the version accepted at Sensors 2021",
    "descriptor": "\nComments: This is the version accepted at Sensors 2021\n",
    "authors": [
      "Muhammad Umair",
      "Muhammad Aamir Cheema",
      "Omer Cheema",
      "Huan Li",
      "Hua Lu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2101.07196"
  },
  {
    "id": "arXiv:2101.07365",
    "title": "Fast Privacy-Preserving Text Classification based on Secure Multiparty  Computation",
    "abstract": "Fast Privacy-Preserving Text Classification based on Secure Multiparty  Computation",
    "descriptor": "",
    "authors": [
      "Amanda Resende",
      "Davis Railsback",
      "Rafael Dowsley",
      "Anderson C. A. Nascimento",
      "Diego F. Aranha"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.07365"
  },
  {
    "id": "arXiv:2101.07936",
    "title": "Joint Inter-path and Intra-path Multiplexing for Terahertz Widely-spaced  Multi-subarray Hybrid Beamforming Systems",
    "abstract": "Comments: 30 pages",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Longfei Yan",
      "Yuhang Chen",
      "Chong Han",
      "Jinhong Yuan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2101.07936"
  },
  {
    "id": "arXiv:2101.08303",
    "title": "From Local Pseudorandom Generators to Hardness of Learning",
    "abstract": "From Local Pseudorandom Generators to Hardness of Learning",
    "descriptor": "",
    "authors": [
      "Amit Daniely",
      "Gal Vardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.08303"
  },
  {
    "id": "arXiv:2101.09337",
    "title": "Approximate Byzantine Fault-Tolerance in Distributed Optimization",
    "abstract": "Comments: 40 pages, 5 figures, and 1 table. The report is an important extension to prior work this https URL, and arXiv:2003.09675; Added an extra experiment section in machine learning",
    "descriptor": "\nComments: 40 pages, 5 figures, and 1 table. The report is an important extension to prior work this https URL, and arXiv:2003.09675; Added an extra experiment section in machine learning\n",
    "authors": [
      "Shuo Liu",
      "Nirupam Gupta",
      "Nitin H. Vaidya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2101.09337"
  },
  {
    "id": "arXiv:2101.11654",
    "title": "Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for  White Blood Cells Nucleus",
    "abstract": "Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for  White Blood Cells Nucleus",
    "descriptor": "",
    "authors": [
      "Zahra Mousavi Kouzehkanan",
      "Sajad Tavakoli",
      "Arezoo Alipanah"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.11654"
  },
  {
    "id": "arXiv:2102.00527",
    "title": "A Runtime-Based Computational Performance Predictor for Deep Neural  Network Training",
    "abstract": "Comments: 19 pages, 7 figures. Appears in the Proceedings of the 2021 USENIX Annual Technical Conference (USENIX ATC '21). Code available at this https URL",
    "descriptor": "\nComments: 19 pages, 7 figures. Appears in the Proceedings of the 2021 USENIX Annual Technical Conference (USENIX ATC '21). Code available at this https URL\n",
    "authors": [
      "Geoffrey X. Yu",
      "Yubo Gao",
      "Pavel Golikov",
      "Gennady Pekhimenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2102.00527"
  },
  {
    "id": "arXiv:2102.00760",
    "title": "Fast rates in structured prediction",
    "abstract": "Comments: 14 main pages, 3 main figures, 43 pages, 4 figures (with appendix)",
    "descriptor": "\nComments: 14 main pages, 3 main figures, 43 pages, 4 figures (with appendix)\n",
    "authors": [
      "Vivien Cabannes",
      "Alessandro Rudi",
      "Francis Bach"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2102.00760"
  },
  {
    "id": "arXiv:2102.02708",
    "title": "Fractionally Log-Concave and Sector-Stable Polynomials: Counting Planar  Matchings and More",
    "abstract": "Fractionally Log-Concave and Sector-Stable Polynomials: Counting Planar  Matchings and More",
    "descriptor": "",
    "authors": [
      "Yeganeh Alimohammadi",
      "Nima Anari",
      "Kirankumar Shiragur",
      "Thuy-Duong Vuong"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2102.02708"
  },
  {
    "id": "arXiv:2102.03311",
    "title": "Online Bin Packing with Predictions",
    "abstract": "Comments: 28 pages, 4 figures",
    "descriptor": "\nComments: 28 pages, 4 figures\n",
    "authors": [
      "Spyros Angelopoulos",
      "Shahin Kamali",
      "Kimia Shadkami"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.03311"
  },
  {
    "id": "arXiv:2102.03799",
    "title": "Online Limited Memory Neural-Linear Bandits with Likelihood Matching",
    "abstract": "Comments: ICML 2021. arXiv admin note: text overlap with arXiv:1901.08612",
    "descriptor": "\nComments: ICML 2021. arXiv admin note: text overlap with arXiv:1901.08612\n",
    "authors": [
      "Ofir Nabati",
      "Tom Zahavy",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.03799"
  },
  {
    "id": "arXiv:2102.04270",
    "title": "Enabling Binary Neural Network Training on the Edge",
    "abstract": "Enabling Binary Neural Network Training on the Edge",
    "descriptor": "",
    "authors": [
      "Erwei Wang",
      "James J. Davis",
      "Daniele Moro",
      "Piotr Zielinski",
      "Claudionor Coelho",
      "Satrajit Chatterjee",
      "Peter Y. K. Cheung",
      "George A. Constantinides"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2102.04270"
  },
  {
    "id": "arXiv:2102.05313",
    "title": "Conditional and Adversarial Euler-based Generators For Time Series",
    "abstract": "Comments: 14 page, 9 Figures",
    "descriptor": "\nComments: 14 page, 9 Figures\n",
    "authors": [
      "Carl Remlinger",
      "Joseph Mikael",
      "Romuald Elie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2102.05313"
  },
  {
    "id": "arXiv:2102.05800",
    "title": "Robust Policy Gradient against Strong Data Corruption",
    "abstract": "Robust Policy Gradient against Strong Data Corruption",
    "descriptor": "",
    "authors": [
      "Xuezhou Zhang",
      "Yiding Chen",
      "Xiaojin Zhu",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05800"
  },
  {
    "id": "arXiv:2102.05996",
    "title": "Fairness Through Regularization for Learning to Rank",
    "abstract": "Comments: 34 pages",
    "descriptor": "\nComments: 34 pages\n",
    "authors": [
      "Nikola Konstantinov",
      "Christoph H. Lampert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.05996"
  },
  {
    "id": "arXiv:2102.06406",
    "title": "A Too-Good-to-be-True Prior to Reduce Shortcut Reliance",
    "abstract": "Comments: 14 pages, 9 figures",
    "descriptor": "\nComments: 14 pages, 9 figures\n",
    "authors": [
      "Nikolay Dagaev",
      "Brett D. Roads",
      "Xiaoliang Luo",
      "Daniel N. Barry",
      "Kaustubh R. Patil",
      "Bradley C. Love"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06406"
  },
  {
    "id": "arXiv:2102.08208",
    "title": "Conditional Distributional Treatment Effect with Kernel Conditional Mean  Embeddings and U-Statistic Regression",
    "abstract": "Conditional Distributional Treatment Effect with Kernel Conditional Mean  Embeddings and U-Statistic Regression",
    "descriptor": "",
    "authors": [
      "Junhyung Park",
      "Uri Shalit",
      "Bernhard Sch\u00f6lkopf",
      "Krikamol Muandet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.08208"
  },
  {
    "id": "arXiv:2102.08248",
    "title": "Hierarchical VAEs Know What They Don't Know",
    "abstract": "Comments: To appear in Proceedings of the 38th International Conference on Machine Learning (ICML 2021). 18 pages, source code available at this https URL, this https URL and this https URL",
    "descriptor": "\nComments: To appear in Proceedings of the 38th International Conference on Machine Learning (ICML 2021). 18 pages, source code available at this https URL, this https URL and this https URL\n",
    "authors": [
      "Jakob D. Havtorn",
      "Jes Frellsen",
      "S\u00f8ren Hauberg",
      "Lars Maal\u00f8e"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.08248"
  },
  {
    "id": "arXiv:2102.08604",
    "title": "SWAD: Domain Generalization by Seeking Flat Minima",
    "abstract": "SWAD: Domain Generalization by Seeking Flat Minima",
    "descriptor": "",
    "authors": [
      "Junbum Cha",
      "Sanghyuk Chun",
      "Kyungjae Lee",
      "Han-Cheol Cho",
      "Seunghyun Park",
      "Yunsung Lee",
      "Sungrae Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.08604"
  },
  {
    "id": "arXiv:2102.09788",
    "title": "Sequential- and Parallel- Constrained Max-value Entropy Search via  Information Lower Bound",
    "abstract": "Comments: 33pages, 7 figures",
    "descriptor": "\nComments: 33pages, 7 figures\n",
    "authors": [
      "Shion Takeno",
      "Tomoyuki Tamura",
      "Kazuki Shitara",
      "Masayuki Karasuyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09788"
  },
  {
    "id": "arXiv:2102.10032",
    "title": "Approximation and Learning with Deep Convolutional Models: a Kernel  Perspective",
    "abstract": "Approximation and Learning with Deep Convolutional Models: a Kernel  Perspective",
    "descriptor": "",
    "authors": [
      "Alberto Bietti"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.10032"
  },
  {
    "id": "arXiv:2102.10438",
    "title": "Unsupervised Medical Image Alignment with Curriculum Learning",
    "abstract": "Comments: Accepted at ICIP 2021",
    "descriptor": "\nComments: Accepted at ICIP 2021\n",
    "authors": [
      "Mihail Burduja",
      "Radu Tudor Ionescu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.10438"
  },
  {
    "id": "arXiv:2102.10898",
    "title": "Adaptive Video Configuration and Bitrate Allocation for Vehicles",
    "abstract": "Comments: Accepted at workshop for Road Vehicle Teleoperation (WS09) at the 2021 IEEE Intelligent Vehicles Symposium (IV21)",
    "descriptor": "\nComments: Accepted at workshop for Road Vehicle Teleoperation (WS09) at the 2021 IEEE Intelligent Vehicles Symposium (IV21)\n",
    "authors": [
      "Andreas Schimpe",
      "Simon Hoffmann",
      "Frank Diermeyer"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2102.10898"
  },
  {
    "id": "arXiv:2102.11011",
    "title": "The Uncanny Similarity of Recurrence and Depth",
    "abstract": "The Uncanny Similarity of Recurrence and Depth",
    "descriptor": "",
    "authors": [
      "Avi Schwarzschild",
      "Arjun Gupta",
      "Amin Ghiasi",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.11011"
  },
  {
    "id": "arXiv:2102.11270",
    "title": "Softmax Policy Gradient Methods Can Take Exponential Time to Converge",
    "abstract": "Softmax Policy Gradient Methods Can Take Exponential Time to Converge",
    "descriptor": "",
    "authors": [
      "Gen Li",
      "Yuting Wei",
      "Yuejie Chi",
      "Yuantao Gu",
      "Yuxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11270"
  },
  {
    "id": "arXiv:2102.11494",
    "title": "Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games",
    "abstract": "Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games",
    "descriptor": "",
    "authors": [
      "Yu Bai",
      "Chi Jin",
      "Huan Wang",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11494"
  },
  {
    "id": "arXiv:2102.11582",
    "title": "Deterministic Neural Networks with Inductive Biases Capture Epistemic  and Aleatoric Uncertainty",
    "abstract": "Deterministic Neural Networks with Inductive Biases Capture Epistemic  and Aleatoric Uncertainty",
    "descriptor": "",
    "authors": [
      "Jishnu Mukhoti",
      "Andreas Kirsch",
      "Joost van Amersfoort",
      "Philip H.S. Torr",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11582"
  },
  {
    "id": "arXiv:2102.11717",
    "title": "A Novel Greedy-Step Bellman Optimality Equation for Efficient Value  Propagation",
    "abstract": "A Novel Greedy-Step Bellman Optimality Equation for Efficient Value  Propagation",
    "descriptor": "",
    "authors": [
      "Yuhui Wang",
      "Qingyuan Wu",
      "Pengcheng He",
      "Xiaoyang Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2102.11717"
  },
  {
    "id": "arXiv:2102.12594",
    "title": "Directional Bias Amplification",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Angelina Wang",
      "Olga Russakovsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.12594"
  },
  {
    "id": "arXiv:2102.12855",
    "title": "Modular Deep Reinforcement Learning for Continuous Motion Planning with  Temporal Logic",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2010.06797",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2010.06797\n",
    "authors": [
      "Mingyu Cai",
      "Mohammadhosein Hasanbeig",
      "Shaoping Xiao",
      "Alessandro Abate",
      "Zhen Kan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2102.12855"
  },
  {
    "id": "arXiv:2102.12894",
    "title": "Constrained Optimization to Train Neural Networks on Critical and  Under-Represented Classes",
    "abstract": "Constrained Optimization to Train Neural Networks on Critical and  Under-Represented Classes",
    "descriptor": "",
    "authors": [
      "Sara Sangalli",
      "Ertunc Erdil",
      "Andreas Hoetker",
      "Olivio Donati",
      "Ender Konukoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.12894"
  },
  {
    "id": "arXiv:2102.13156",
    "title": "Physics-Integrated Variational Autoencoders for Robust and Interpretable  Generative Modeling",
    "abstract": "Physics-Integrated Variational Autoencoders for Robust and Interpretable  Generative Modeling",
    "descriptor": "",
    "authors": [
      "Naoya Takeishi",
      "Alexandros Kalousis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13156"
  },
  {
    "id": "arXiv:2102.13268",
    "title": "DRIBO: Robust Deep Reinforcement Learning via Multi-View Information  Bottleneck",
    "abstract": "Comments: 27 pages",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Jiameng Fan",
      "Wenchao Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.13268"
  },
  {
    "id": "arXiv:2102.13359",
    "title": "Energy Efficiency Maximization in the Uplink Delta-OMA Networks",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Ramin Hashemi",
      "Hamzeh Beyranvand",
      "Mohammad Robat Mili",
      "Ata Khalili",
      "Hina Tabassum",
      "Derrick Wing Kwan Ng"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2102.13359"
  },
  {
    "id": "arXiv:2102.13515",
    "title": "Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning",
    "abstract": "Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "V\u00edctor Campos",
      "Pablo Sprechmann",
      "Steven Hansen",
      "Andre Barreto",
      "Steven Kapturowski",
      "Alex Vitvitskyi",
      "Adri\u00e0 Puigdom\u00e8nech Badia",
      "Charles Blundell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13515"
  },
  {
    "id": "arXiv:2103.01391",
    "title": "Sample Complexity and Overparameterization Bounds for Temporal  Difference Learning with Neural Network Approximation",
    "abstract": "Sample Complexity and Overparameterization Bounds for Temporal  Difference Learning with Neural Network Approximation",
    "descriptor": "",
    "authors": [
      "Semih Cayci",
      "Siddhartha Satpathi",
      "Niao He",
      "R. Srikant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.01391"
  },
  {
    "id": "arXiv:2103.02383",
    "title": "Nonlinear MPC for Offset-Free Tracking of systems learned by GRU Neural  Networks",
    "abstract": "Comments: This work is the extended version of the article accepted at the Third IFAC Conference on Modelling, Identification and Control of Nonlinear Systems (MICNON 2021) for publication under a Creative Commons Licence CC-BY-NC-ND",
    "descriptor": "\nComments: This work is the extended version of the article accepted at the Third IFAC Conference on Modelling, Identification and Control of Nonlinear Systems (MICNON 2021) for publication under a Creative Commons Licence CC-BY-NC-ND\n",
    "authors": [
      "Fabio Bonassi",
      "Caio Fabio Oliveira da Silva",
      "Riccardo Scattolini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.02383"
  },
  {
    "id": "arXiv:2103.03817",
    "title": "Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled  Zero-Touch 6G Networks: Model-Free DRL Approach",
    "abstract": "Comments: Under Revision By IEEE TNSM",
    "descriptor": "\nComments: Under Revision By IEEE TNSM\n",
    "authors": [
      "Amirhossein Shaghaghi",
      "Abolfazl Zakeri",
      "Nader Mokari",
      "Mohammad Reza Javan",
      "Mohammad Behdadfar",
      "Eduard A Jorswieck"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.03817"
  },
  {
    "id": "arXiv:2103.04244",
    "title": "Counterfactuals and Causability in Explainable Artificial Intelligence:  Theory, Algorithms, and Applications",
    "abstract": "Counterfactuals and Causability in Explainable Artificial Intelligence:  Theory, Algorithms, and Applications",
    "descriptor": "",
    "authors": [
      "Yu-Liang Chou",
      "Catarina Moreira",
      "Peter Bruza",
      "Chun Ouyang",
      "Joaquim Jorge"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.04244"
  },
  {
    "id": "arXiv:2103.08140",
    "title": "Post-Quantum Succinct Arguments: Breaking the Quantum Rewinding Barrier",
    "abstract": "Comments: 50 pages, 2 figures",
    "descriptor": "\nComments: 50 pages, 2 figures\n",
    "authors": [
      "Alessandro Chiesa",
      "Fermi Ma",
      "Nicholas Spooner",
      "Mark Zhandry"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2103.08140"
  },
  {
    "id": "arXiv:2103.08834",
    "title": "GSVNet: Guided Spatially-Varying Convolution for Fast Semantic  Segmentation on Video",
    "abstract": "GSVNet: Guided Spatially-Varying Convolution for Fast Semantic  Segmentation on Video",
    "descriptor": "",
    "authors": [
      "Shih-Po Lee",
      "Si-Cun Chen",
      "Wen-Hsiao Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.08834"
  },
  {
    "id": "arXiv:2103.09943",
    "title": "Fast and High-Quality Blind Multi-Spectral Image Pansharpening",
    "abstract": "Comments: 17 pages, 47 figures, journal, accepted by IEEE Transactions on Geoscience and Remote Sensing",
    "descriptor": "\nComments: 17 pages, 47 figures, journal, accepted by IEEE Transactions on Geoscience and Remote Sensing\n",
    "authors": [
      "Lantao Yu",
      "Dehong Liu",
      "Hassan Mansour",
      "Petros T. Boufounos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.09943"
  },
  {
    "id": "arXiv:2103.10884",
    "title": "Localized Reduced Basis Additive Schwarz Methods",
    "abstract": "Localized Reduced Basis Additive Schwarz Methods",
    "descriptor": "",
    "authors": [
      "Martin J. Gander",
      "Stephan Rave"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.10884"
  },
  {
    "id": "arXiv:2103.11144",
    "title": "Unsupervised Feature Learning for Manipulation with Contrastive Domain  Randomization",
    "abstract": "Comments: Accepted to ICRA 2021, code can be found at this https URL",
    "descriptor": "\nComments: Accepted to ICRA 2021, code can be found at this https URL\n",
    "authors": [
      "Carmel Rabinovitz",
      "Niko Grupen",
      "Aviv Tamar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.11144"
  },
  {
    "id": "arXiv:2103.11988",
    "title": "Self-paced ensemble learning for speech and audio classification",
    "abstract": "Comments: Accepted at INTERSPEECH 2021",
    "descriptor": "\nComments: Accepted at INTERSPEECH 2021\n",
    "authors": [
      "Nicolae-Catalin Ristea",
      "Radu Tudor Ionescu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.11988"
  },
  {
    "id": "arXiv:2103.12048",
    "title": "Extracting the Unknown from Long Math Problems",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Ndapa Nakashole"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "History and Overview (math.HO)"
    ],
    "url": "https://arxiv.org/abs/2103.12048"
  },
  {
    "id": "arXiv:2103.16547",
    "title": "The Elastic Lottery Ticket Hypothesis",
    "abstract": "The Elastic Lottery Ticket Hypothesis",
    "descriptor": "",
    "authors": [
      "Xiaohan Chen",
      "Yu Cheng",
      "Shuohang Wang",
      "Zhe Gan",
      "Jingjing Liu",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.16547"
  },
  {
    "id": "arXiv:2104.01112",
    "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language",
    "abstract": "NaturalProofs: Mathematical Theorem Proving in Natural Language",
    "descriptor": "",
    "authors": [
      "Sean Welleck",
      "Jiacheng Liu",
      "Ronan Le Bras",
      "Hannaneh Hajishirzi",
      "Yejin Choi",
      "Kyunghyun Cho"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.01112"
  },
  {
    "id": "arXiv:2104.01616",
    "title": "Towards Lifelong Learning of End-to-end ASR",
    "abstract": "Comments: Accepted to INTERSPEECH 2021. We acknowledge the support of Salesforce Research Deep Learning Grant",
    "descriptor": "\nComments: Accepted to INTERSPEECH 2021. We acknowledge the support of Salesforce Research Deep Learning Grant\n",
    "authors": [
      "Heng-Jui Chang",
      "Hung-yi Lee",
      "Lin-shan Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.01616"
  },
  {
    "id": "arXiv:2104.02704",
    "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with  Common Sense and World Knowledge",
    "abstract": "Comments: NAACL 2021",
    "descriptor": "\nComments: NAACL 2021\n",
    "authors": [
      "Canwen Xu",
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Julian McAuley",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.02704"
  },
  {
    "id": "arXiv:2104.04848",
    "title": "Autoequivariant Network Search via Group Decomposition",
    "abstract": "Autoequivariant Network Search via Group Decomposition",
    "descriptor": "",
    "authors": [
      "Sourya Basu",
      "Akshayaa Magesh",
      "Harshit Yadav",
      "Lav R. Varshney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.04848"
  },
  {
    "id": "arXiv:2104.05778",
    "title": "Efficient Space-time Video Super Resolution using Low-Resolution Flow  and Mask Upsampling",
    "abstract": "Comments: Accepted at NTIRE Workshop, CVPR 2021. Code and models: this https URL",
    "descriptor": "\nComments: Accepted at NTIRE Workshop, CVPR 2021. Code and models: this https URL\n",
    "authors": [
      "Saikat Dutta",
      "Nisarg A. Shah",
      "Anurag Mittal"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.05778"
  },
  {
    "id": "arXiv:2104.07096",
    "title": "A Large-Scale Analysis of Mixed Initiative in Information-Seeking  Dialogues for Conversational Search",
    "abstract": "Comments: 32 pages; To appear in ACM Transactions on Information Systems (TOIS), Special Issue on Conversational Search and Recommendation. 2021",
    "descriptor": "\nComments: 32 pages; To appear in ACM Transactions on Information Systems (TOIS), Special Issue on Conversational Search and Recommendation. 2021\n",
    "authors": [
      "Svitlana Vakulenko",
      "Evangelos Kanoulas",
      "Maarten de Rijke"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2104.07096"
  },
  {
    "id": "arXiv:2104.08669",
    "title": "Fifty Three Matrix Factorizations: A systematic approach",
    "abstract": "Comments: 73 pages",
    "descriptor": "\nComments: 73 pages\n",
    "authors": [
      "Alan Edelman",
      "Sungwoo Jeong"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2104.08669"
  },
  {
    "id": "arXiv:2104.08955",
    "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation  Training",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Shaked Dovrat",
      "Eliya Nachmani",
      "Lior Wolf"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.08955"
  },
  {
    "id": "arXiv:2104.10586",
    "title": "Mixture of Robust Experts (MoRE): A Flexible Defense Against Multiple  Perturbations",
    "abstract": "Mixture of Robust Experts (MoRE): A Flexible Defense Against Multiple  Perturbations",
    "descriptor": "",
    "authors": [
      "Kaidi Xu",
      "Chenan Wang",
      "Xue Lin",
      "Bhavya Kailkhura",
      "Ryan Goldhahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.10586"
  },
  {
    "id": "arXiv:2104.13100",
    "title": "Shellcode_IA32: A Dataset for Automatic Shellcode Generation",
    "abstract": "Comments: Paper accepted to NLP4Prog Workshop 2021 co-located with ACL-IJCNLP 2021",
    "descriptor": "\nComments: Paper accepted to NLP4Prog Workshop 2021 co-located with ACL-IJCNLP 2021\n",
    "authors": [
      "Pietro Liguori",
      "Erfan Al-Hossami",
      "Domenico Cotroneo",
      "Roberto Natella",
      "Bojan Cukic",
      "Samira Shaikh"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.13100"
  },
  {
    "id": "arXiv:2104.13790",
    "title": "FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive  Optimizers by Exploiting Strong Convexity",
    "abstract": "FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive  Optimizers by Exploiting Strong Convexity",
    "descriptor": "",
    "authors": [
      "Yangfan Zhou",
      "Kaizhu Huang",
      "Cheng Cheng",
      "Xuguang Wang",
      "Amir Hussain",
      "Xin Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.13790"
  },
  {
    "id": "arXiv:2104.14007",
    "title": "Interaction-GCN: A Graph Convolutional Network based framework for  social interaction recognition in egocentric videos",
    "abstract": "Comments: Accepted to ICIP 2021",
    "descriptor": "\nComments: Accepted to ICIP 2021\n",
    "authors": [
      "Simone Felicioni",
      "Mariella Dimiccoli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.14007"
  },
  {
    "id": "arXiv:2104.14336",
    "title": "Document Collection Visual Question Answering",
    "abstract": "Document Collection Visual Question Answering",
    "descriptor": "",
    "authors": [
      "Rub\u00e8n Tito",
      "Dimosthenis Karatzas",
      "Ernest Valveny"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2104.14336"
  },
  {
    "id": "arXiv:2104.14528",
    "title": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for  Gastric Histopathology Image Classification",
    "abstract": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for  Gastric Histopathology Image Classification",
    "descriptor": "",
    "authors": [
      "Haoyuan Chen",
      "Chen Li",
      "Xiaoyan Li",
      "Ge Wang",
      "Weiming Hu",
      "Yixin Li",
      "Wanli Liu",
      "Changhao Sun",
      "Yudong Yao",
      "Yueyang Teng",
      "Marcin Grzegorzek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.14528"
  },
  {
    "id": "arXiv:2104.14556",
    "title": "Discover the Unknown Biased Attribute of an Image Classifier",
    "abstract": "Discover the Unknown Biased Attribute of an Image Classifier",
    "descriptor": "",
    "authors": [
      "Zhiheng Li",
      "Chenliang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.14556"
  },
  {
    "id": "arXiv:2105.00351",
    "title": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus  Spike Proteins",
    "abstract": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus  Spike Proteins",
    "descriptor": "",
    "authors": [
      "Moo K. Chung",
      "Hernando Ombao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2105.00351"
  },
  {
    "id": "arXiv:2105.01161",
    "title": "Approximability of all finite CSPs in the dynamic streaming setting",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2102.12351",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2102.12351\n",
    "authors": [
      "Chi-Ning Chou",
      "Alexander Golovnev",
      "Madhu Sudan",
      "Santhoshini Velusamy"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2105.01161"
  },
  {
    "id": "arXiv:2105.03767",
    "title": "Aerospace Sliding Mode Control Toolbox: Relative Degree Approach with  Resource Prospector Lander and Launch Vehicle Case Studies",
    "abstract": "Aerospace Sliding Mode Control Toolbox: Relative Degree Approach with  Resource Prospector Lander and Launch Vehicle Case Studies",
    "descriptor": "",
    "authors": [
      "S. Kode",
      "Y. Shtessel",
      "A. Levant",
      "J. Rakoczy",
      "M. Hannan",
      "J. Orr"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2105.03767"
  },
  {
    "id": "arXiv:2105.03791",
    "title": "Enhancing Transformers with Gradient Boosted Decision Trees for NLI  Fine-Tuning",
    "abstract": "Comments: Findings of ACL 2021",
    "descriptor": "\nComments: Findings of ACL 2021\n",
    "authors": [
      "Benjamin Minixhofer",
      "Milan Gritta",
      "Ignacio Iacobacci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.03791"
  },
  {
    "id": "arXiv:2105.03902",
    "title": "Learning Gradient Fields for Molecular Conformation Generation",
    "abstract": "Comments: ICML 2021, Long talk",
    "descriptor": "\nComments: ICML 2021, Long talk\n",
    "authors": [
      "Chence Shi",
      "Shitong Luo",
      "Minkai Xu",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2105.03902"
  },
  {
    "id": "arXiv:2105.04646",
    "title": "Deeply-Debiased Off-Policy Interval Estimation",
    "abstract": "Deeply-Debiased Off-Policy Interval Estimation",
    "descriptor": "",
    "authors": [
      "Chengchun Shi",
      "Runzhe Wan",
      "Victor Chernozhukov",
      "Rui Song"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.04646"
  },
  {
    "id": "arXiv:2105.05227",
    "title": "Doing Natural Language Processing in A Natural Way: An NLP toolkit based  on object-oriented knowledge base and multi-level grammar base",
    "abstract": "Doing Natural Language Processing in A Natural Way: An NLP toolkit based  on object-oriented knowledge base and multi-level grammar base",
    "descriptor": "",
    "authors": [
      "Yu Guo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.05227"
  },
  {
    "id": "arXiv:2105.05320",
    "title": "Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph  Clustering",
    "abstract": "Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph  Clustering",
    "descriptor": "",
    "authors": [
      "Yiming Wang",
      "Dongxia Chang",
      "Zhiqian Fu",
      "Yao Zhao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.05320"
  },
  {
    "id": "arXiv:2105.05599",
    "title": "StutterNet: Stuttering Detection Using Time Delay Neural Network",
    "abstract": "Comments: Accepted in EUSIPCO 2021: European Signal Processing Conference",
    "descriptor": "\nComments: Accepted in EUSIPCO 2021: European Signal Processing Conference\n",
    "authors": [
      "Shakeel A. Sheikh",
      "Md Sahidullah",
      "Fabrice Hirsch",
      "Slim Ouni"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2105.05599"
  },
  {
    "id": "arXiv:2105.08285",
    "title": "Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing",
    "abstract": "Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing",
    "descriptor": "",
    "authors": [
      "Anshumali Shrivastava",
      "Zhao Song",
      "Zhaozhuo Xu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.08285"
  },
  {
    "id": "arXiv:2105.09016",
    "title": "E(n) Equivariant Normalizing Flows",
    "abstract": "E(n) Equivariant Normalizing Flows",
    "descriptor": "",
    "authors": [
      "Victor Garcia Satorras",
      "Emiel Hoogeboom",
      "Fabian B. Fuchs",
      "Ingmar Posner",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.09016"
  },
  {
    "id": "arXiv:2105.10433",
    "title": "When is Assortment Optimization Optimal?",
    "abstract": "When is Assortment Optimization Optimal?",
    "descriptor": "",
    "authors": [
      "Will Ma"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2105.10433"
  },
  {
    "id": "arXiv:2105.10497",
    "title": "Intriguing Properties of Vision Transformers",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Muzammal Naseer",
      "Kanchana Ranasinghe",
      "Salman Khan",
      "Munawar Hayat",
      "Fahad Shahbaz Khan",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.10497"
  },
  {
    "id": "arXiv:2105.11755",
    "title": "Planning Mm-Wave Access Networks With Reconfigurable Intelligent  Surfaces",
    "abstract": "Planning Mm-Wave Access Networks With Reconfigurable Intelligent  Surfaces",
    "descriptor": "",
    "authors": [
      "Eugenio Moro",
      "Ilario Filippini",
      "Antonio Capone",
      "Danilo De Donno"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2105.11755"
  },
  {
    "id": "arXiv:2105.12002",
    "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to  Improving Generalization",
    "abstract": "Comments: The 59th annual meeting of the Association for Computational Linguistics (ACL 2021)",
    "descriptor": "\nComments: The 59th annual meeting of the Association for Computational Linguistics (ACL 2021)\n",
    "authors": [
      "Chen Liang",
      "Simiao Zuo",
      "Minshuo Chen",
      "Haoming Jiang",
      "Xiaodong Liu",
      "Pengcheng He",
      "Tuo Zhao",
      "Weizhu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.12002"
  },
  {
    "id": "arXiv:2105.12806",
    "title": "A Universal Law of Robustness via Isoperimetry",
    "abstract": "A Universal Law of Robustness via Isoperimetry",
    "descriptor": "",
    "authors": [
      "S\u00e9bastien Bubeck",
      "Mark Sellke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.12806"
  },
  {
    "id": "arXiv:2105.12980",
    "title": "Investigating label suggestions for opinion mining in German Covid-19  social media",
    "abstract": "Comments: To Appear at ACL 2021",
    "descriptor": "\nComments: To Appear at ACL 2021\n",
    "authors": [
      "Tilman Beck",
      "Ji-Ung Lee",
      "Christina Viehmann",
      "Marcus Maurer",
      "Oliver Quiring",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.12980"
  },
  {
    "id": "arXiv:2105.13171",
    "title": "Dewetting dynamics of anisotropic particles -- a level set numerical  approach",
    "abstract": "Dewetting dynamics of anisotropic particles -- a level set numerical  approach",
    "descriptor": "",
    "authors": [
      "Siddharth Gavhale",
      "Karel Svadlenka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2105.13171"
  },
  {
    "id": "arXiv:2105.14082",
    "title": "Bh\u0101$\\unicode{x1E63}$\u0101citra: Visualising the dialect geography of  South Asia",
    "abstract": "Comments: 5 pages, 4 figures. To appear at LChange'21 workshop located at ACL 2021",
    "descriptor": "\nComments: 5 pages, 4 figures. To appear at LChange'21 workshop located at ACL 2021\n",
    "authors": [
      "Aryaman Arora",
      "Adam Farris",
      "Gopalakrishnan R",
      "Samopriya Basu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.14082"
  },
  {
    "id": "arXiv:2105.14188",
    "title": "We Know What You Want: An Advertising Strategy Recommender System for  Online Advertising",
    "abstract": "Comments: Accepted by KDD 2021",
    "descriptor": "\nComments: Accepted by KDD 2021\n",
    "authors": [
      "Liyi Guo",
      "Junqi Jin",
      "Haoqi Zhang",
      "Zhenzhe Zheng",
      "Zhiye Yang",
      "Zhizhuang Xing",
      "Fei Pan",
      "Lvyin Niu",
      "Fan Wu",
      "Haiyang Xu",
      "Chuan Yu",
      "Yuning Jiang",
      "Xiaoqiang Zhu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.14188"
  },
  {
    "id": "arXiv:2105.14517",
    "title": "GeoQA: A Geometric Question Answering Benchmark Towards Multimodal  Numerical Reasoning",
    "abstract": "Comments: Accepted to Findings of ACL 2021",
    "descriptor": "\nComments: Accepted to Findings of ACL 2021\n",
    "authors": [
      "Jiaqi Chen",
      "Jianheng Tang",
      "Jinghui Qin",
      "Xiaodan Liang",
      "Lingbo Liu",
      "Eric P. Xing",
      "Liang Lin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.14517"
  },
  {
    "id": "arXiv:2105.14576",
    "title": "StyTr^2: Unbiased Image Style Transfer with Transformers",
    "abstract": "StyTr^2: Unbiased Image Style Transfer with Transformers",
    "descriptor": "",
    "authors": [
      "Yingying Deng",
      "Fan Tang",
      "Xingjia Pan",
      "Weiming Dong",
      "Chongyang Ma",
      "Changsheng Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2105.14576"
  },
  {
    "id": "arXiv:2105.14710",
    "title": "Robustifying $\\ell_\\infty$ Adversarial Training to the Union of  Perturbation Models",
    "abstract": "Robustifying $\\ell_\\infty$ Adversarial Training to the Union of  Perturbation Models",
    "descriptor": "",
    "authors": [
      "Ameya D. Patil",
      "Michael Tuttle",
      "Alexander G. Schwing",
      "Naresh R. Shanbhag"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.14710"
  },
  {
    "id": "arXiv:2105.14720",
    "title": "Energy-preserving fully-discrete schemes for nonlinear stochastic wave  equations with multiplicative noise",
    "abstract": "Energy-preserving fully-discrete schemes for nonlinear stochastic wave  equations with multiplicative noise",
    "descriptor": "",
    "authors": [
      "Jialin Hong",
      "Baohui Hou",
      "Liying Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.14720"
  },
  {
    "id": "arXiv:2105.14875",
    "title": "Bangla Natural Language Processing: A Comprehensive Review of Classical,  Machine Learning, and Deep Learning Based Methods",
    "abstract": "Comments: This preprint will be submitted to IEEE Access Journal and it contains total of 43 pages",
    "descriptor": "\nComments: This preprint will be submitted to IEEE Access Journal and it contains total of 43 pages\n",
    "authors": [
      "Ovishake Sen",
      "Mohtasim Fuad",
      "MD. Nazrul Islam",
      "Jakaria Rabbi",
      "MD. Kamrul Hasan",
      "Mohammed Baz",
      "Mehedi Masud",
      "Md. Abdul Awal",
      "Awal Ahmed Fime",
      "Md. Tahmid Hasan Fuad",
      "Delowar Sikder",
      "MD. Akil Raihan Iftee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.14875"
  },
  {
    "id": "arXiv:2106.00250",
    "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags",
    "abstract": "Comments: 7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021",
    "descriptor": "\nComments: 7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021\n",
    "authors": [
      "Kshitij Gupta",
      "Devansh Gautam",
      "Radhika Mamidi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.00250"
  },
  {
    "id": "arXiv:2106.00314",
    "title": "Dual Graph enhanced Embedding Neural Network for CTR Prediction",
    "abstract": "Comments: KDD 2021",
    "descriptor": "\nComments: KDD 2021\n",
    "authors": [
      "Wei Guo",
      "Rong Su",
      "Renhao Tan",
      "Huifeng Guo",
      "Yingxue Zhang",
      "Zhirong Liu",
      "Ruiming Tang",
      "Xiuqiang He"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.00314"
  },
  {
    "id": "arXiv:2106.00559",
    "title": "Predicting Vehicles Trajectories in Urban Scenarios with Transformer  Networks and Augmented Information",
    "abstract": "Comments: This work has been accepted for publication at IEEE Intelligent Vehicles Symposium 2021",
    "descriptor": "\nComments: This work has been accepted for publication at IEEE Intelligent Vehicles Symposium 2021\n",
    "authors": [
      "A. Quintanar",
      "D. Fern\u00e1ndez-Llorca",
      "I. Parra",
      "R. Izquierdo",
      "M. A. Sotelo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.00559"
  },
  {
    "id": "arXiv:2106.00715",
    "title": "A Theory for Locus Ellipticity of Poncelet 3-Periodic Centers",
    "abstract": "Comments: 14 pages, 5 figures, 3 tables, and 5 video links",
    "descriptor": "\nComments: 14 pages, 5 figures, 3 tables, and 5 video links\n",
    "authors": [
      "Mark Helman",
      "Dominique Laurain",
      "Dan Reznik",
      "Ronaldo Garcia"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)",
      "Robotics (cs.RO)",
      "Algebraic Geometry (math.AG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.00715"
  },
  {
    "id": "arXiv:2106.00772",
    "title": "Information Theoretic Measures for Fairness-aware Feature Selection",
    "abstract": "Comments: 15 pages, 6 figures",
    "descriptor": "\nComments: 15 pages, 6 figures\n",
    "authors": [
      "Sajad Khodadadian",
      "Mohamed Nafea",
      "AmirEmad Ghassami",
      "Negar Kiyavash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.00772"
  },
  {
    "id": "arXiv:2106.00958",
    "title": "A Generalizable Approach to Learning Optimizers",
    "abstract": "A Generalizable Approach to Learning Optimizers",
    "descriptor": "",
    "authors": [
      "Diogo Almeida",
      "Clemens Winter",
      "Jie Tang",
      "Wojciech Zaremba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00958"
  },
  {
    "id": "arXiv:2106.01087",
    "title": "Is Sparse Attention more Interpretable?",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Clara Meister",
      "Stefan Lazov",
      "Isabelle Augenstein",
      "Ryan Cotterell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.01087"
  },
  {
    "id": "arXiv:2106.02122",
    "title": "Experimental Comparison of Visual and Single-Receiver GPS Odometry",
    "abstract": "Comments: 7 pages, 9 figures",
    "descriptor": "\nComments: 7 pages, 9 figures\n",
    "authors": [
      "Benjamin Congram",
      "Timothy D. Barfoot"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02122"
  },
  {
    "id": "arXiv:2106.02190",
    "title": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug  Discovery",
    "abstract": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug  Discovery",
    "descriptor": "",
    "authors": [
      "Yulun Wu",
      "Nicholas Choma",
      "Andrew Chen",
      "Mikaela Cashman",
      "\u00c9rica T. Prates",
      "Manesh Shah",
      "Ver\u00f3nica G. Melesse Vergara",
      "Austin Clyde",
      "Thomas S. Brettin",
      "Wibe A. de Jong",
      "Neeraj Kumar",
      "Martha S. Head",
      "Rick L. Stevens",
      "Peter Nugent",
      "Daniel A. Jacobson",
      "James B. Brown"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2106.02190"
  },
  {
    "id": "arXiv:2106.02391",
    "title": "Data-Driven Control Design with LMIs and Dynamic Programming",
    "abstract": "Data-Driven Control Design with LMIs and Dynamic Programming",
    "descriptor": "",
    "authors": [
      "Donghwan Lee",
      "Do Wan Kim"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02391"
  },
  {
    "id": "arXiv:2106.02487",
    "title": "Debiasing a First-order Heuristic for Approximate Bi-level Optimization",
    "abstract": "Comments: Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021. arXiv admin note: text overlap with arXiv:2006.03631",
    "descriptor": "\nComments: Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021. arXiv admin note: text overlap with arXiv:2006.03631\n",
    "authors": [
      "Valerii Likhosherstov",
      "Xingyou Song",
      "Krzysztof Choromanski",
      "Jared Davis",
      "Adrian Weller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02487"
  },
  {
    "id": "arXiv:2106.02513",
    "title": "Generative Text Modeling through Short Run Inference",
    "abstract": "Comments: 10 pages",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Bo Pang",
      "Erik Nijkamp",
      "Tian Han",
      "Ying Nian Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02513"
  },
  {
    "id": "arXiv:2106.02594",
    "title": "Self-Supervised Learning of Domain Invariant Features for Depth  Estimation",
    "abstract": "Comments: 16 pages",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Hiroyasu Akada",
      "Shariq Farooq Bhat",
      "Ibraheem Alhashim",
      "Peter Wonka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02594"
  },
  {
    "id": "arXiv:2106.02613",
    "title": "Beyond Target Networks: Improving Deep $Q$-learning with Functional  Regularization",
    "abstract": "Beyond Target Networks: Improving Deep $Q$-learning with Functional  Regularization",
    "descriptor": "",
    "authors": [
      "Alexandre Pich\u00e9",
      "Joseph Marino",
      "Gian Maria Marconi",
      "Christopher Pal",
      "Mohammad Emtiyaz Khan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02613"
  },
  {
    "id": "arXiv:2106.02796",
    "title": "Principal Bit Analysis: Autoencoding with Schur-Concave Loss",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Sourbh Bhadane",
      "Aaron B. Wagner",
      "Jayadev Acharya"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02796"
  },
  {
    "id": "arXiv:2106.02818",
    "title": "Variational Leakage: The Role of Information Complexity in Privacy  Leakage",
    "abstract": "Variational Leakage: The Role of Information Complexity in Privacy  Leakage",
    "descriptor": "",
    "authors": [
      "Amir Ahooye Atashin",
      "Behrooz Razeghi",
      "Deniz G\u00fcnd\u00fcz",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02818"
  },
  {
    "id": "arXiv:2106.02850",
    "title": "Tetrad: Actively Secure 4PC for Secure Training and Inference",
    "abstract": "Tetrad: Actively Secure 4PC for Secure Training and Inference",
    "descriptor": "",
    "authors": [
      "Nishat Koti",
      "Arpita Patra",
      "Rahul Rachuri",
      "Ajith Suresh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02850"
  },
  {
    "id": "arXiv:2106.02885",
    "title": "Category Contrast for Unsupervised Domain Adaptation in Visual Tasks",
    "abstract": "Category Contrast for Unsupervised Domain Adaptation in Visual Tasks",
    "descriptor": "",
    "authors": [
      "Jiaxing Huang",
      "Dayan Guan",
      "Aoran Xiao",
      "Shijian Lu",
      "Ling Shao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02885"
  },
  {
    "id": "arXiv:2106.02900",
    "title": "Differentially Private Multi-Armed Bandits in the Shuffle Model",
    "abstract": "Differentially Private Multi-Armed Bandits in the Shuffle Model",
    "descriptor": "",
    "authors": [
      "Jay Tenenbaum",
      "Haim Kaplan",
      "Yishay Mansour",
      "Uri Stemmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02900"
  },
  {
    "id": "arXiv:2106.02989",
    "title": "Exploring the Disproportion Between Scientific Productivity and  Knowledge Amount",
    "abstract": "Comments: Luoyi Fu and Huquan Kang contribute equally to the work. Xinbing Wang and Chenghu Zhou both are corresponding authors",
    "descriptor": "\nComments: Luoyi Fu and Huquan Kang contribute equally to the work. Xinbing Wang and Chenghu Zhou both are corresponding authors\n",
    "authors": [
      "Luoyi Fu",
      "Huquan Kang",
      "Jianghao Wang",
      "Ling Yao",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.02989"
  },
  {
    "id": "arXiv:2106.03029",
    "title": "Planning Multimodal Exploratory Actions for Online Robot Attribute  Learning",
    "abstract": "Comments: To be published in Robotics: Science and Systems (RSS), July 12-16, 2021",
    "descriptor": "\nComments: To be published in Robotics: Science and Systems (RSS), July 12-16, 2021\n",
    "authors": [
      "Xiaohan Zhang",
      "Jivko Sinapov",
      "Shiqi Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03029"
  },
  {
    "id": "arXiv:2106.03160",
    "title": "Multi-agent Modeling of Hazard-Household-Infrastructure Nexus for  Equitable Resilience Assessment",
    "abstract": "Multi-agent Modeling of Hazard-Household-Infrastructure Nexus for  Equitable Resilience Assessment",
    "descriptor": "",
    "authors": [
      "Amir Esmalian",
      "Wanqiu Wang",
      "Ali Mostafavi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.03160"
  },
  {
    "id": "arXiv:2106.03221",
    "title": "PAC Best Arm Identification Under a Deadline",
    "abstract": "Comments: In submission",
    "descriptor": "\nComments: In submission\n",
    "authors": [
      "Brijen Thananjeyan",
      "Kirthevasan Kandasamy",
      "Ion Stoica",
      "Michael I. Jordan",
      "Ken Goldberg",
      "Joseph E. Gonzalez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03221"
  },
  {
    "id": "arXiv:2106.03257",
    "title": "Structured Reordering for Modeling Latent Alignments in Sequence  Transduction",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Bailin Wang",
      "Mirella Lapata",
      "Ivan Titov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03257"
  },
  {
    "id": "arXiv:2106.03262",
    "title": "Low-complexity Voronoi shaping for the Gaussian channel",
    "abstract": "Comments: 10 pages, 6 figures",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "S. Li",
      "A. Mirani",
      "M. Karlsson",
      "E. Agrell"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03262"
  },
  {
    "id": "arXiv:2106.03269",
    "title": "Itihasa: A large-scale corpus for Sanskrit to English translation",
    "abstract": "Comments: WAT 2021",
    "descriptor": "\nComments: WAT 2021\n",
    "authors": [
      "Rahul Aralikatte",
      "Miryam de Lhoneux",
      "Anoop Kunchukuttan",
      "Anders S\u00f8gaard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03269"
  },
  {
    "id": "arXiv:2106.03441",
    "title": "Attention Temperature Matters in Abstractive Summarization Distillation",
    "abstract": "Attention Temperature Matters in Abstractive Summarization Distillation",
    "descriptor": "",
    "authors": [
      "Shengqiang Zhang",
      "Xingxing Zhang",
      "Hangbo Bao",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03441"
  },
  {
    "id": "arXiv:2106.03502",
    "title": "Efficient training for future video generation based on hierarchical  disentangled representation of latent variables",
    "abstract": "Efficient training for future video generation based on hierarchical  disentangled representation of latent variables",
    "descriptor": "",
    "authors": [
      "Naoya Fushishita",
      "Antonio Tejero-de-Pablos",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03502"
  },
  {
    "id": "arXiv:2106.03518",
    "title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion  Cause Extraction",
    "abstract": "Comments: ACL2021 Main Conference Long paper",
    "descriptor": "\nComments: ACL2021 Main Conference Long paper\n",
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Gabriele Pergola",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03518"
  },
  {
    "id": "arXiv:2106.03530",
    "title": "CAiRE in DialDoc21: Data Augmentation for Information-Seeking Dialogue  System",
    "abstract": "Comments: Accepted in DialDoc21 Workshop in ACL 2021. Etsuko Ishii and Yan Xu contributed equally to this work",
    "descriptor": "\nComments: Accepted in DialDoc21 Workshop in ACL 2021. Etsuko Ishii and Yan Xu contributed equally to this work\n",
    "authors": [
      "Etsuko Ishii",
      "Yan Xu",
      "Genta Indra Winata",
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Zihan Liu",
      "Peng Xu",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03530"
  },
  {
    "id": "arXiv:2106.03617",
    "title": "PAIO: A Software-Defined Storage Data Plane Framework",
    "abstract": "Comments: 15 pages, 8 figures. Submitted to IEEE Transactions on Parallel and Distributed Systems",
    "descriptor": "\nComments: 15 pages, 8 figures. Submitted to IEEE Transactions on Parallel and Distributed Systems\n",
    "authors": [
      "Ricardo Macedo",
      "Yusuke Tanimura",
      "Jason Haga",
      "Vijay Chidambaram",
      "Jos\u00e9 Pereira",
      "Jo\u00e3o Paulo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2106.03617"
  },
  {
    "id": "arXiv:2106.03694",
    "title": "Detection of marine floating plastic using Sentinel-2 imagery and  machine learning models",
    "abstract": "Comments: 30 pages",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Srikanta Sannigrahi",
      "Bidroha Basu",
      "Arunima Sarkar Basu",
      "Francesco Pilla"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03694"
  },
  {
    "id": "arXiv:2106.03720",
    "title": "Person Re-Identification with a Locally Aware Transformer",
    "abstract": "Comments: 10 pages, 2 figure, submitted to NeurIPS 2021",
    "descriptor": "\nComments: 10 pages, 2 figure, submitted to NeurIPS 2021\n",
    "authors": [
      "Charu Sharma",
      "Siddhant R. Kapil",
      "David Chapman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03720"
  },
  {
    "id": "arXiv:2106.03722",
    "title": "Error Loss Networks",
    "abstract": "Error Loss Networks",
    "descriptor": "",
    "authors": [
      "Badong Chen",
      "Yunfei Zheng",
      "Pengju Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03722"
  },
  {
    "id": "arXiv:2106.03764",
    "title": "On the Expressive Power of Self-Attention Matrices",
    "abstract": "On the Expressive Power of Self-Attention Matrices",
    "descriptor": "",
    "authors": [
      "Valerii Likhosherstov",
      "Krzysztof Choromanski",
      "Adrian Weller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03764"
  },
  {
    "id": "arXiv:2106.03798",
    "title": "DoubleField: Bridging the Neural Surface and Radiance Fields for  High-fidelity Human Rendering",
    "abstract": "DoubleField: Bridging the Neural Surface and Radiance Fields for  High-fidelity Human Rendering",
    "descriptor": "",
    "authors": [
      "Ruizhi Shao",
      "Hongwen Zhang",
      "He Zhang",
      "Yanpei Cao",
      "Tao Yu",
      "Yebin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03798"
  }
]