[
  {
    "id": "arXiv:2106.02639",
    "title": "Singular Dynamic Mode Decompositions",
    "abstract": "This manuscript is aimed at addressing several long standing limitations of\ndynamic mode decompositions in the application of Koopman analysis. Principle\namong these limitations are the convergence of associated Dynamic Mode\nDecomposition algorithms and the existence of Koopman modes. To address these\nlimitations, two major modifications are made, where Koopman operators are\nremoved from the analysis in light of Liouville operators (known as Koopman\ngenerators in special cases), and these operators are shown to be compact for\ncertain pairs of Hilbert spaces selected separately as the domain and range of\nthe operator. While eigenfunctions are discarded in this analysis, a viable\nreconstruction algorithm is still demonstrated, and the sacrifice of\neigenfunctions realizes the theoretical goals of DMD analysis that have yet to\nbe achieved in other contexts. The manuscript concludes with the description of\na Dynamic Mode Decomposition algorithm that converges when a dense collection\nof occupation kernels, arising from the data, are leveraged in the analysis.",
    "descriptor": "\nComments: 11 pages. YouTube playlist supporting this manuscript can be found here: this https URL\n",
    "authors": [
      "Joel A. Rosenfeld",
      "Rushikesh Kamalapurkar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Functional Analysis (math.FA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.02639"
  },
  {
    "id": "arXiv:2106.02654",
    "title": "Churn Reduction via Distillation",
    "abstract": "In real-world systems, models are frequently updated as more data becomes\navailable, and in addition to achieving high accuracy, the goal is to also\nmaintain a low difference in predictions compared to the base model (i.e.\npredictive ``churn''). If model retraining results in vastly different\nbehavior, then it could cause negative effects in downstream systems,\nespecially if this churn can be avoided with limited impact on model accuracy.\nIn this paper, we show an equivalence between training with distillation using\nthe base model as the teacher and training with an explicit constraint on the\npredictive churn. We then show that distillation performs strongly for low\nchurn training against a number of recent baselines on a wide range of datasets\nand model architectures, including fully-connected networks, convolutional\nnetworks, and transformers.",
    "descriptor": "",
    "authors": [
      "Heinrich Jiang",
      "Harikrishna Narasimhan",
      "Dara Bahri",
      "Andrew Cotter",
      "Afshin Rostamizadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02654"
  },
  {
    "id": "arXiv:2106.02656",
    "title": "Approximating Nash Social Welfare under Binary XOS and Binary  Subadditive Valuations",
    "abstract": "We study the problem of allocating indivisible goods among agents in a fair\nand economically efficient manner. In this context, the Nash social\nwelfare--defined as the geometric mean of agents' valuations for their assigned\nbundles--stands as a fundamental measure that quantifies the extent of fairness\nof an allocation. Focusing on instances in which the agents' valuations have\nbinary marginals, we develop essentially tight results for (approximately)\nmaximizing Nash social welfare under two of the most general classes of\ncomplement-free valuations, i.e., under binary XOS and binary subadditive\nvaluations.\nFor binary XOS valuations, we develop a polynomial-time algorithm that finds\na constant-factor (specifically 288) approximation for the optimal Nash social\nwelfare, in the standard value-oracle model. The allocations computed by our\nalgorithm also achieve constant-factor approximation for social welfare and the\ngroupwise maximin share guarantee. These results imply that--in the case of\nbinary XOS valuations--there necessarily exists an allocation that\nsimultaneously satisfies multiple (approximate) fairness and efficiency\ncriteria. We complement the algorithmic result by proving that Nash social\nwelfare maximization is APX-hard under binary XOS valuations.\nFurthermore, this work establishes an interesting separation between the\nbinary XOS and binary subadditive settings. In particular, we prove that an\nexponential number of value queries are necessarily required to obtain even a\nsub-linear approximation for Nash social welfare under binary subadditive\nvaluations.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.02656"
  },
  {
    "id": "arXiv:2106.02658",
    "title": "W-RST: Towards a Weighted RST-style Discourse Framework",
    "abstract": "Aiming for a better integration of data-driven and linguistically-inspired\napproaches, we explore whether RST Nuclearity, assigning a binary assessment of\nimportance between text segments, can be replaced by automatically generated,\nreal-valued scores, in what we call a Weighted-RST framework. In particular, we\nfind that weighted discourse trees from auxiliary tasks can benefit key NLP\ndownstream applications, compared to nuclearity-centered approaches. We further\nshow that real-valued importance distributions partially and interestingly\nalign with the assessment and uncertainty of human annotators.",
    "descriptor": "\nComments: 9 pages, Accepted at ACL 2021\n",
    "authors": [
      "Patrick Huber",
      "Wen Xiao",
      "Giuseppe Carenini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02658"
  },
  {
    "id": "arXiv:2106.02666",
    "title": "Counterfactual Explanations Can Be Manipulated",
    "abstract": "Counterfactual explanations are emerging as an attractive option for\nproviding recourse to individuals adversely impacted by algorithmic decisions.\nAs they are deployed in critical applications (e.g. law enforcement, financial\nlending), it becomes important to ensure that we clearly understand the\nvulnerabilities of these methods and find ways to address them. However, there\nis little understanding of the vulnerabilities and shortcomings of\ncounterfactual explanations. In this work, we introduce the first framework\nthat describes the vulnerabilities of counterfactual explanations and shows how\nthey can be manipulated. More specifically, we show counterfactual explanations\nmay converge to drastically different counterfactuals under a small\nperturbation indicating they are not robust. Leveraging this insight, we\nintroduce a novel objective to train seemingly fair models where counterfactual\nexplanations find much lower cost recourse under a slight perturbation. We\ndescribe how these models can unfairly provide low-cost recourse for specific\nsubgroups in the data while appearing fair to auditors. We perform experiments\non loan and violent crime prediction data sets where certain subgroups achieve\nup to 20x lower cost recourse under the perturbation. These results raise\nconcerns regarding the dependability of current counterfactual explanation\ntechniques, which we hope will inspire investigations in robust counterfactual\nexplanations.",
    "descriptor": "",
    "authors": [
      "Dylan Slack",
      "Sophie Hilgard",
      "Himabindu Lakkaraju",
      "Sameer Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02666"
  },
  {
    "id": "arXiv:2106.02668",
    "title": "Emergent Communication of Generalizations",
    "abstract": "To build agents that can collaborate effectively with others, recent research\nhas trained artificial agents to communicate with each other in Lewis-style\nreferential games. However, this often leads to successful but uninterpretable\ncommunication. We argue that this is due to the game objective: communicating\nabout a single object in a shared visual context is prone to overfitting and\ndoes not encourage language useful beyond concrete reference. In contrast,\nhuman language conveys a rich variety of abstract ideas. To promote such\nskills, we propose games that require communicating generalizations over sets\nof objects representing abstract visual concepts, optionally with separate\ncontexts for each agent. We find that these games greatly improve systematicity\nand interpretability of the learned languages, according to several metrics in\nthe literature. Finally, we propose a method for identifying logical operations\nembedded in the emergent languages by learning an approximate compositional\nreconstruction of the language.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Jesse Mu",
      "Noah Goodman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02668"
  },
  {
    "id": "arXiv:2106.02670",
    "title": "Robust Resource Allocation for Multi-Antenna URLLC-OFDMA Systems in a  Smart Factory",
    "abstract": "In this paper, we investigate the worst-case robust beamforming design and\nresource block (RB) assignment problem for total transmit power minimization of\nthe central controller while guaranteeing each robot's transmission with target\nnumber of data bits and within required ultra-low latency and extremely high\nreliability. By using the property of the independence of each robot's\nbeamformer design, we can obtain the equivalent power control design form of\nthe original beamforming design. The binary RB mapping indicators are\ntransformed into continuous ones with additional $\\ell_0$-norm constraints to\npromote sparsity on each RB. A novel non-convex penalty (NCP) approach is\napplied to solve such $\\ell_0$-norm constraints. Numerical results demonstrate\nthe superiority of the NCP approach to the well-known reweighted $\\ell_1$\nmethod in terms of the optimized power consumption, convergence rate and\nrobustness to channel realizations. Also, the impacts of latency, reliability,\nnumber of transmit antennas and channel uncertainty on the system performance\nare revealed.",
    "descriptor": "",
    "authors": [
      "Jing Cheng",
      "Chao Shen",
      "Shuqiang Xia"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02670"
  },
  {
    "id": "arXiv:2106.02674",
    "title": "Differentially Private Deep Learning under the Fairness Lens",
    "abstract": "Differential Privacy (DP) is an important privacy-enhancing technology for\nprivate machine learning systems. It allows to measure and bound the risk\nassociated with an individual participation in a computation. However, it was\nrecently observed that DP learning systems may exacerbate bias and unfairness\nfor different groups of individuals. This paper builds on these important\nobservations and sheds light on the causes of the disparate impacts arising in\nthe problem of differentially private empirical risk minimization. It focuses\non the accuracy disparity arising among groups of individuals in two\nwell-studied DP learning methods: output perturbation and differentially\nprivate stochastic gradient descent. The paper analyzes which data and model\nproperties are responsible for the disproportionate impacts, why these aspects\nare affecting different groups disproportionately and proposes guidelines to\nmitigate these effects. The proposed approach is evaluated on several datasets\nand settings.",
    "descriptor": "",
    "authors": [
      "Cuong Tran",
      "My H. Dinh",
      "Ferdinando Fioretto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02674"
  },
  {
    "id": "arXiv:2106.02676",
    "title": "A novel multi-scale loss function for classification problems in machine  learning",
    "abstract": "We introduce two-scale loss functions for use in various gradient descent\nalgorithms applied to classification problems via deep neural networks. This\nnew method is generic in the sense that it can be applied to a wide range of\nmachine learning architectures, from deep neural networks to support vector\nmachines for example. These two-scale loss functions allow to focus the\ntraining onto objects in the training set which are not well classified. This\nleads to an increase in several measures of performance for\nappropriately-defined two-scale loss functions with respect to the more\nclassical cross-entropy when tested on traditional deep neural networks on the\nMNIST, CIFAR10, and CIFAR100 data-sets.",
    "descriptor": "\nComments: 22 pages; 11 figure\n",
    "authors": [
      "Leonid Berlyand",
      "Robert Creese",
      "Pierre-Emmanuel Jabin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02676"
  },
  {
    "id": "arXiv:2106.02677",
    "title": "Relay Selection and Resource Allocation for Ultra-Reliable Uplink  Transmission in Smart Factory Scenarios",
    "abstract": "In this paper, a relay-aided two-phase transmission protocol for the smart\nfactory scenario is proposed. This protocol aims at enabling all robots'\nultra-reliable target number of uplink critical data transmission within a\nlatency constraint by jointly optimizing the relay selection, resource block\n(RB) assignment, and transmit power allocation. Such protocol design is\nformulated as a mixed-integer and strictly non-convex problem where\noptimization variables are mutual coupling, which is definitely challenging.\nInstead of conventional methods designed for solving the problem, we leverage\nthe properties of the relative entropy function to equivalently transform the\nproblem without introducing extra constraints. As the packet error probability\nrequirements of each robot under two possible transmission modes are coupled in\none overall reliability constraint, the big-M technique is applied to decouple\nit into two corresponding reliability constraints. One is for direct\ntransmission mode, and the other is for cooperative transmission mode.\nMoreover, both non-convex penalty (NCP) and quadratic penalty (QP) approaches\nare utilized to deal with the binary indicator constraints. Based on such\npenalty methods, a sequence of penalized approximated convex problems can be\niteratively solved for sub-optimal solutions. Numerical results demonstrate the\nefficiency of such two penalty methods from the perspectives of sub-optimal\nvalues of total transmit power and convergence rate. Further, the impacts of\nreliability, the number and location of relays, the number of robots, the\ntarget number of data bits on the total power consumption are analyzed.",
    "descriptor": "",
    "authors": [
      "Jing Cheng",
      "Chao Shen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.02677"
  },
  {
    "id": "arXiv:2106.02679",
    "title": "Layered gradient accumulation and modular pipeline parallelism: fast and  efficient training of large language models",
    "abstract": "The advent of the transformer has sparked a quick growth in the size of\nlanguage models, far outpacing hardware improvements. (Dense) transformers are\nexpected to reach the trillion-parameter scale in the near future, for which\ntraining requires thousands or even tens of thousands of GPUs. We investigate\nthe challenges of training at this scale and beyond on commercially available\nhardware. In particular, we analyse the shortest possible training time for\ndifferent configurations of distributed training, leveraging empirical scaling\nlaws for language models to estimate the optimal (critical) batch size.\nContrary to popular belief, we find no evidence for a memory wall, and instead\nargue that the real limitation -- other than the cost -- lies in the training\nduration.\nIn addition to this analysis, we introduce two new methods, \\textit{layered\ngradient accumulation} and \\textit{modular pipeline parallelism}, which\ntogether cut the shortest training time by half. The methods also reduce data\nmovement, lowering the network requirement to a point where a fast InfiniBand\nconnection is not necessary. This increased network efficiency also improve on\nthe methods introduced with the ZeRO optimizer, reducing the memory usage to a\ntiny fraction of the available GPU memory.",
    "descriptor": "\nComments: 22 pages, 8 figures\n",
    "authors": [
      "Joel Lamy-Poirier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.02679"
  },
  {
    "id": "arXiv:2106.02680",
    "title": "How to Decompose a Tensor with Group Structure",
    "abstract": "In this work we study the orbit recovery problem, which is a natural\nabstraction for the problem of recovering a planted signal from noisy\nmeasurements under unknown group actions. Many important inverse problems in\nstatistics, engineering and the sciences fit into this framework. Prior work\nhas studied cases when the group is discrete and/or abelian. However\nfundamentally new techniques are needed in order to handle more complex group\nactions.\nOur main result is a quasi-polynomial time algorithm to solve orbit recovery\nover $SO(3)$ - i.e. the cryo-electron tomography problem which asks to recover\nthe three-dimensional structure of a molecule from noisy measurements of\nrandomly rotated copies of it. We analyze a variant of the frequency marching\nheuristic in the framework of smoothed analysis. Our approach exploits the\nlayered structure of the invariant polynomials, and simultaneously yields a new\nclass of tensor decomposition algorithms that work in settings when the tensor\nis not low-rank but rather where the factors are algebraically related to each\nother by a group action.",
    "descriptor": "",
    "authors": [
      "Allen Liu",
      "Ankur Moitra"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02680"
  },
  {
    "id": "arXiv:2106.02681",
    "title": "SpikePropamine: Differentiable Plasticity in Spiking Neural Networks",
    "abstract": "The adaptive changes in synaptic efficacy that occur between spiking neurons\nhave been demonstrated to play a critical role in learning for biological\nneural networks. Despite this source of inspiration, many learning focused\napplications using Spiking Neural Networks (SNNs) retain static synaptic\nconnections, preventing additional learning after the initial training period.\nHere, we introduce a framework for simultaneously learning the underlying\nfixed-weights and the rules governing the dynamics of synaptic plasticity and\nneuromodulated synaptic plasticity in SNNs through gradient descent. We further\ndemonstrate the capabilities of this framework on a series of challenging\nbenchmarks, learning the parameters of several plasticity rules including BCM,\nOja's, and their respective set of neuromodulatory variants. The experimental\nresults display that SNNs augmented with differentiable plasticity are\nsufficient for solving a set of challenging temporal learning tasks that a\ntraditional SNN fails to solve, even in the presence of significant noise.\nThese networks are also shown to be capable of producing locomotion on a\nhigh-dimensional robotic learning task, where near-minimal degradation in\nperformance is observed in the presence of novel conditions not seen during the\ninitial training period.",
    "descriptor": "",
    "authors": [
      "Samuel Schmidgall",
      "Julia Ashkanazy",
      "Wallace Lawson",
      "Joe Hays"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02681"
  },
  {
    "id": "arXiv:2106.02684",
    "title": "Learning Policies with Zero or Bounded Constraint Violation for  Constrained MDPs",
    "abstract": "We address the issue of safety in reinforcement learning. We pose the problem\nin an episodic framework of a constrained Markov decision process. Existing\nresults have shown that it is possible to achieve a reward regret of\n$\\tilde{\\mathcal{O}}(\\sqrt{K})$ while allowing an\n$\\tilde{\\mathcal{O}}(\\sqrt{K})$ constraint violation in $K$ episodes. A\ncritical question that arises is whether it is possible to keep the constraint\nviolation even smaller. We show that when a strictly safe policy is known, then\none can confine the system to zero constraint violation with arbitrarily high\nprobability while keeping the reward regret of order\n$\\tilde{\\mathcal{O}}(\\sqrt{K})$. The algorithm which does so employs the\nprinciple of optimistic pessimism in the face of uncertainty to achieve safe\nexploration. When no strictly safe policy is known, though one is known to\nexist, then it is possible to restrict the system to bounded constraint\nviolation with arbitrarily high probability. This is shown to be realized by a\nprimal-dual algorithm with an optimistic primal estimate and a pessimistic dual\nupdate.",
    "descriptor": "",
    "authors": [
      "Tao Liu",
      "Ruida Zhou",
      "Dileep Kalathil",
      "P. R. Kumar",
      "Chao Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02684"
  },
  {
    "id": "arXiv:2106.02685",
    "title": "Massively Parallel and Dynamic Algorithms for Minimum Size Clustering",
    "abstract": "In this paper, we study the $r$-gather problem, a natural formulation of\nminimum-size clustering in metric spaces. The goal of $r$-gather is to\npartition $n$ points into clusters such that each cluster has size at least\n$r$, and the maximum radius of the clusters is minimized. This additional\nconstraint completely changes the algorithmic nature of the problem, and many\nclustering techniques fail. Also previous dynamic and parallel algorithms do\nnot achieve desirable complexity. We propose algorithms both in the Massively\nParallel Computation (MPC) model and in the dynamic setting. Our MPC algorithm\nhandles input points from the Euclidean space $\\mathbb{R}^d$. It computes an\n$O(1)$-approximate solution of $r$-gather in $O(\\log^{\\varepsilon} n)$ rounds\nusing total space $O(n^{1+\\gamma}\\cdot d)$ for arbitrarily small constants\n$\\varepsilon,\\gamma > 0$. In addition our algorithm is fully scalable, i.e.,\nthere is no lower bound on the memory per machine. Our dynamic algorithm\nmaintains an $O(1)$-approximate $r$-gather solution under insertions/deletions\nof points in a metric space with doubling dimension $d$. The update time is $r\n\\cdot 2^{O(d)}\\cdot \\log^{O(1)}\\Delta$ and the query time is $2^{O(d)}\\cdot\n\\log^{O(1)}\\Delta$, where $\\Delta$ is the ratio between the largest and the\nsmallest distance.",
    "descriptor": "",
    "authors": [
      "Alessandro Epasto",
      "Mohammad Mahdian",
      "Vahab Mirrokni",
      "Peilin Zhong"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02685"
  },
  {
    "id": "arXiv:2106.02687",
    "title": "Towards real time assessment of earthfill dams via Model Order Reduction",
    "abstract": "The use of Internet of Things (IoT) technologies is becoming a preferred\nsolution for the assessment of tailings dams' safety. Real-time sensor\nmonitoring proves to be a key tool for reducing the risk related to these\never-evolving earth-fill structures, that exhibit a high rate of sudden and\nhazardous failures. In order to optimally exploit real-time embankment\nmonitoring, one major hindrance has to be overcome: the creation of a\nsupporting numerical model for stability analysis, with rapid-enough response\nto perform data assimilation in real time. A model should be built, such that\nits response can be obtained faster than the physical evolution of the analyzed\nphenomenon. In this work, Reduced Order Modelling (ROM) is used to boost\ncomputational efficiency in solving the coupled hydro-mechanical system of\nequations governing the problem. The Reduced Basis method is applied to the\ncoupled hydro-mechanical equations that govern the groundwater flow, that are\nmade non-linear as a result of considering an unsaturated soil. The resulting\nmodel's performance is assessed by solving a 2D and a 3D problem relevant to\ntailings dams' safety. The ROM technique achieves a speedup of 3 to 15 times\nwith respect to the full-order model (FOM) while maintaining high levels of\naccuracy.",
    "descriptor": "",
    "authors": [
      "Christina Nasikaa",
      "Pedro Diez",
      "Pierre Gerard",
      "Thierry J. Massart",
      "Sergio Zlotnik"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.02687"
  },
  {
    "id": "arXiv:2106.02688",
    "title": "Egalitarian Resource Sharing Over Multiple Rounds",
    "abstract": "It is often beneficial for agents to pool their resources in order to better\naccommodate fluctuations in individual demand. Many multi-round resource\nallocation mechanisms operate in an online manner: in each round, the agents\nspecify their demands for that round, and the mechanism determines a\ncorresponding allocation. In this paper, we focus instead on the offline\nsetting in which the agents specify their demand for each round at the outset.\nWe formulate a specific resource allocation problem in this setting, and design\nand analyze an associated mechanism based on the solution concept of\nlexicographic maximin fairness. We present an efficient implementation of our\nmechanism, and prove that it is Pareto-efficient, envy-free, non-wasteful,\nresource monotonic, population monotonic, and group strategyproof. We also\nprove that our mechanism guarantees each agent at least half of the utility\nthat they can obtain by not sharing their resources. We complement these\npositive results by proving that no maximin fair mechanism can improve on the\naforementioned factor of one-half.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Fu Li",
      "C. Gregory Plaxton",
      "Vaibhav B. Sinha"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.02688"
  },
  {
    "id": "arXiv:2106.02689",
    "title": "RegionViT: Regional-to-Local Attention for Vision Transformers",
    "abstract": "Vision transformer (ViT) has recently showed its strong capability in\nachieving comparable results to convolutional neural networks (CNNs) on image\nclassification. However, vanilla ViT simply inherits the same architecture from\nthe natural language processing directly, which is often not optimized for\nvision applications. Motivated by this, in this paper, we propose a new\narchitecture that adopts the pyramid structure and employ a novel\nregional-to-local attention rather than global self-attention in vision\ntransformers. More specifically, our model first generates regional tokens and\nlocal tokens from an image with different patch sizes, where each regional\ntoken is associated with a set of local tokens based on the spatial location.\nThe regional-to-local attention includes two steps: first, the regional\nself-attention extract global information among all regional tokens and then\nthe local self-attention exchanges the information among one regional token and\nthe associated local tokens via self-attention. Therefore, even though local\nself-attention confines the scope in a local region but it can still receive\nglobal information. Extensive experiments on three vision tasks, including\nimage classification, object detection and action recognition, show that our\napproach outperforms or is on par with state-of-the-art ViT variants including\nmany concurrent works. Our source codes and models will be publicly available.",
    "descriptor": "",
    "authors": [
      "Chun-Fu Chen",
      "Rameswar Panda",
      "Quanfu Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02689"
  },
  {
    "id": "arXiv:2106.02692",
    "title": "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting  User Questions About Human or Non-Human Identity",
    "abstract": "Humans are increasingly interacting with machines through language, sometimes\nin contexts where the user may not know they are talking to a machine (like\nover the phone or a text chatbot). We aim to understand how system designers\nand researchers might allow their systems to confirm its non-human identity. We\ncollect over 2,500 phrasings related to the intent of ``Are you a robot?\". This\nis paired with over 2,500 adversarially selected utterances where only\nconfirming the system is non-human would be insufficient or disfluent. We\ncompare classifiers to recognize the intent and discuss the precision/recall\nand model complexity tradeoffs. Such classifiers could be integrated into\ndialog systems to avoid undesired deception. We then explore how both a\ngenerative research model (Blender) as well as two deployed systems (Amazon\nAlexa, Google Assistant) handle this intent, finding that systems often fail to\nconfirm their non-human identity. Finally, we try to understand what a good\nresponse to the intent would be, and conduct a user study to compare the\nimportant aspects when responding to this intent.",
    "descriptor": "",
    "authors": [
      "David Gros",
      "Yu Li",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.02692"
  },
  {
    "id": "arXiv:2106.02694",
    "title": "Efficient Classification of Very Large Images with Tiny Objects",
    "abstract": "An increasing number of applications in the computer vision domain,\nspecially, in medical imaging and remote sensing, are challenging when the goal\nis to classify very large images with tiny objects. More specifically, these\ntype of classification tasks face two key challenges: $i$) the size of the\ninput image in the target dataset is usually in the order of megapixels,\nhowever, existing deep architectures do not easily operate on such big images\ndue to memory constraints, consequently, we seek a memory-efficient method to\nprocess these images; and $ii$) only a small fraction of the input images are\ninformative of the label of interest, resulting in low region of interest (ROI)\nto image ratio. However, most of the current convolutional neural networks\n(CNNs) are designed for image classification datasets that have relatively\nlarge ROIs and small image size (sub-megapixel). Existing approaches have\naddressed these two challenges in isolation. We present an end-to-end CNN model\ntermed Zoom-In network that leverages hierarchical attention sampling for\nclassification of large images with tiny objects using a single GPU. We\nevaluate our method on two large-image datasets and one gigapixel dataset.\nExperimental results show that our model achieves higher accuracy than existing\nmethods while requiring less computing resources.",
    "descriptor": "",
    "authors": [
      "Fanjie Kong",
      "Ricardo Henao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02694"
  },
  {
    "id": "arXiv:2106.02695",
    "title": "Meta-Learning with Fewer Tasks through Task Interpolation",
    "abstract": "Meta-learning enables algorithms to quickly learn a newly encountered task\nwith just a few labeled examples by transferring previously learned knowledge.\nHowever, the bottleneck of current meta-learning algorithms is the requirement\nof a large number of meta-training tasks, which may not be accessible in\nreal-world scenarios. To address the challenge that available tasks may not\ndensely sample the space of tasks, we propose to augment the task set through\ninterpolation. By meta-learning with task interpolation (MLTI), our approach\neffectively generates additional tasks by randomly sampling a pair of tasks and\ninterpolating the corresponding features and labels. Under both gradient-based\nand metric-based meta-learning settings, our theoretical analysis shows MLTI\ncorresponds to a data-adaptive meta-regularization and further improves the\ngeneralization. Empirically, in our experiments on eight datasets from diverse\ndomains including image recognition, pose prediction, molecule property\nprediction, and medical image classification, we find that the proposed general\nMLTI framework is compatible with representative meta-learning algorithms and\nconsistently outperforms other state-of-the-art strategies.",
    "descriptor": "",
    "authors": [
      "Huaxiu Yao",
      "Linjun Zhang",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02695"
  },
  {
    "id": "arXiv:2106.02697",
    "title": "Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees",
    "abstract": "Tree-based models underpin many modern semantic search engines and\nrecommender systems due to their sub-linear inference times. In industrial\napplications, these models operate at extreme scales, where every bit of\nperformance is critical. Memory constraints at extreme scales also require that\nmodels be sparse, hence tree-based models are often back-ended by sparse matrix\nalgebra routines. However, there are currently no sparse matrix techniques\nspecifically designed for the sparsity structure one encounters in tree-based\nmodels for extreme multi-label ranking/classification (XMR/XMC) problems. To\naddress this issue, we present the masked sparse chunk multiplication (MSCM)\ntechnique, a sparse matrix technique specifically tailored to XMR trees. MSCM\nis easy to implement, embarrassingly parallelizable, and offers a significant\nperformance boost to any existing tree inference pipeline at no cost. We\nperform a comprehensive study of MSCM applied to several different sparse\ninference schemes and benchmark our methods on a general purpose extreme\nmulti-label ranking framework. We observe that MSCM gives consistently dramatic\nspeedups across both the online and batch inference settings, single- and\nmulti-threaded settings, and on many different tree models and datasets. To\ndemonstrate its utility in industrial applications, we apply MSCM to an\nenterprise-scale semantic product search problem with 100 million products and\nachieve sub-millisecond latency of 0.88 ms per query on a single thread -- an\n8x reduction in latency over vanilla inference techniques. The MSCM technique\nrequires absolutely no sacrifices to model accuracy as it gives exactly the\nsame results as standard sparse matrix techniques. Therefore, we believe that\nMSCM will enable users of XMR trees to save a substantial amount of compute\nresources in their inference pipelines at very little cost.",
    "descriptor": "",
    "authors": [
      "Philip A. Etter",
      "Kai Zhong",
      "Hsiang-Fu Yu",
      "Lexing Ying",
      "Inderjit Dhillon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02697"
  },
  {
    "id": "arXiv:2106.02699",
    "title": "Contact Tracing Information Improves the Performance of Group Testing  Algorithms",
    "abstract": "Group testing can help maintain a widespread testing program using fewer\nresources amid a pandemic. In group testing, we are given $n$ samples, one per\nindividual. These samples are arranged into $m < n$ pooled samples, where each\npool is obtained by mixing a subset of the $n$ individual samples. Infected\nindividuals are then identified using a group testing algorithm. In this paper,\nwe use side information (SI) collected from contact tracing (CT) within\nnonadaptive/single-stage group testing algorithms. We generate CT SI data by\nincorporating characteristics of disease spread between individuals. These data\nare fed into two signal and measurement models for group testing, and numerical\nresults show that our algorithms provide improved sensitivity and specificity.\nWe also show how to incorporate CT SI into the design of the pooling matrix.\nThat said, our numerical results suggest that the utilization of SI in the\npooling matrix design does not yield significant performance gains beyond the\nincorporation of SI in the group testing algorithm.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2011.14186\n",
    "authors": [
      "Ritesh Goenka",
      "Shu-Jie Cao",
      "Chau-Wai Wong",
      "Ajit Rajwade",
      "Dror Baron"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.02699"
  },
  {
    "id": "arXiv:2106.02701",
    "title": "Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction",
    "abstract": "Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of projection neuron morphology, but\nmanual neuron reconstruction remains a bottleneck. Here we present a method\ninspired by hidden Markov modeling and appearance modeling of fluorescent\nneuron images that can automatically trace neuronal processes. Our method\nleverages dynamic programming to scale to terabyte sized image data and can be\napplied to images with one or more neurons. We applied our algorithm to the\noutput of image segmentation models where false negatives severed neuronal\nprocesses, and showed that it can follow axons in the presence of noise or\nnearby neurons. Our method has the potential to be integrated into a semi or\nfully automated reconstruction pipeline. Additionally, it creates a framework\nthrough which users can intervene with hard constraints to, for example, rule\nout certain reconstructions, or assign axons to particular cell bodies.",
    "descriptor": "\nComments: First draft presented at 2021 Brain Initiative Cell Census Network PI meeting\n",
    "authors": [
      "Thomas L. Athey",
      "Daniel Tward",
      "Ulrich Mueller",
      "Michael I. Miller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02701"
  },
  {
    "id": "arXiv:2106.02702",
    "title": "Subgroup Fairness in Two-Sided Markets",
    "abstract": "It is well known that two-sided markets are unfair in a number of ways. For\ninstance, female workers at Uber earn less than their male colleagues per mile\ndriven. Similar observations have been made for other minority subgroups in\nother two-sided markets. Here, we suggest a novel market-clearing mechanism for\ntwo-sided markets, which promotes equalisation of the pay per hour worked\nacross multiple subgroups, as well as within each subgroup. In the process, we\nintroduce a novel notion of subgroup fairness (which we call Inter-fairness),\nwhich can be combined with other notions of fairness within each subgroup\n(called Intra-fairness), and the utility for the customers (Customer-Care) in\nthe objective of the market-clearing problem. While the novel non-linear terms\nin the objective complicate market clearing by making the problem non-convex,\nwe show that a certain non-convex augmented Lagrangian relaxation can be\napproximated to any precision in time polynomial in the number of market\nparticipants using semi-definite programming. This makes it possible to\nimplement the market-clearing mechanism efficiently. On the example of\ndriver-ride assignment in an Uber-like system, we demonstrate the efficacy and\nscalability of the approach, and trade-offs between Inter- and Intra-fairness.",
    "descriptor": "",
    "authors": [
      "Quan Zhou",
      "Jakub Marecek",
      "Robert N. Shorten"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02702"
  },
  {
    "id": "arXiv:2106.02705",
    "title": "Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task  Learning",
    "abstract": "As multi-task models gain popularity in a wider range of machine learning\napplications, it is becoming increasingly important for practitioners to\nunderstand the fairness implications associated with those models. Most\nexisting fairness literature focuses on learning a single task more fairly,\nwhile how ML fairness interacts with multiple tasks in the joint learning\nsetting is largely under-explored. In this paper, we are concerned with how\ngroup fairness (e.g., equal opportunity, equalized odds) as an ML fairness\nconcept plays out in the multi-task scenario. In multi-task learning, several\ntasks are learned jointly to exploit task correlations for a more efficient\ninductive transfer. This presents a multi-dimensional Pareto frontier on (1)\nthe trade-off between group fairness and accuracy with respect to each task, as\nwell as (2) the trade-offs across multiple tasks. We aim to provide a deeper\nunderstanding on how group fairness interacts with accuracy in multi-task\nlearning, and we show that traditional approaches that mainly focus on\noptimizing the Pareto frontier of multi-task accuracy might not perform well on\nfairness goals. We propose a new set of metrics to better capture the\nmulti-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely\npresented in a multi-task learning setting. We further propose a\nMulti-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task\nlearning. Experiments on several real-world datasets demonstrate the\neffectiveness of our proposed approach.",
    "descriptor": "",
    "authors": [
      "Yuyan Wang",
      "Xuezhi Wang",
      "Alex Beutel",
      "Flavien Prost",
      "Jilin Chen",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02705"
  },
  {
    "id": "arXiv:2106.02707",
    "title": "The spreading potential problem",
    "abstract": "Viral marketing campaigns target primarily those individuals who are central\nin social networks and hence have social influence. Marketing events, however,\nmay attract diverse audience. Despite the importance of event marketing, the\ninfluence of heterogeneous target groups is not well understood yet. In this\npaper, we define the Spreading Potential (SP) problem of target group selection\ngiven the mixture of influential and ordinary agents in groups. The SP problem\nis different from the well-known Influence Maximization (IM) problem that aims\nto find the most influential individual agents but does not consider mixture of\ninfluential and ordinary agents in target groups. We provide a systemic test\nfor ranking influence measures in the SP problem based on node sampling and on\na novel statistical method, the Sum of Ranking Differences. Using a Linear\nThreshold diffusion model on an online social network, we evaluate seven\nnetwork measures of social influence. We demonstrate that the statistical\nassessment of these influence measures is remarkably different in the SP\nproblem, when low-ranked individuals are present, from the IM problem, when we\nfocus on the algorithm's top choices exclusively.",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs R. Sziklai",
      "Bal\u00e1zs Lengyel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.02707"
  },
  {
    "id": "arXiv:2106.02708",
    "title": "On the Design of Strategic Task Recommendations for Sustainable  Crowdsourcing-Based Content Moderation",
    "abstract": "Crowdsourcing-based content moderation is a platform that hosts content\nmoderation tasks for crowd workers to review user submissions (e.g. text,\nimages and videos) and make decisions regarding the admissibility of the posted\ncontent, along with a gamut of other tasks such as image labeling and\nspeech-to-text conversion. In an attempt to reduce cognitive overload at the\nworkers and improve system efficiency, these platforms offer personalized task\nrecommendations according to the worker's preferences. However, the current\nstate-of-the-art recommendation systems disregard the effects on worker's\nmental health, especially when they are repeatedly exposed to content\nmoderation tasks with extreme content (e.g. violent images, hate-speech). In\nthis paper, we propose a novel, strategic recommendation system for the\ncrowdsourcing platform that recommends jobs based on worker's mental status.\nSpecifically, this paper models interaction between the crowdsourcing\nplatform's recommendation system (leader) and the worker (follower) as a\nBayesian Stackelberg game where the type of the follower corresponds to the\nworker's cognitive atrophy rate and task preferences. We discuss how rewards\nand costs should be designed to steer the game towards desired outcomes in\nterms of maximizing the platform's productivity, while simultaneously improving\nthe working conditions of crowd workers.",
    "descriptor": "\nComments: Presented at International Workshop on Autonomous Agents for Social Good (AASG), May 2021\n",
    "authors": [
      "Sainath Sanga",
      "Venkata Sriram Siddhardh Nadendla"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.02708"
  },
  {
    "id": "arXiv:2106.02709",
    "title": "Domain Range Semigroups and Finite Representations",
    "abstract": "Relational semigroups with domain and range are a useful tool for modelling\nnondeterministic programs. We prove that the representation class $R(D,R,*)$ is\nnot finitely axiomatisable, answering arXiv:1811.01712 Question 5.1. We show\nthat any signature containing $D, R, \\smile, ;$ but not $-$ or $\\cdot$ has the\nfinite representation property, an extension of the result for ordered domain\nalgebras $\\{0,1,D,R, \\leq, 1', \\smile,;\\}$. We survey the results in the area\nof the finite representation property and raise a number of open questions.",
    "descriptor": "",
    "authors": [
      "Ja\u0161 \u0160emrl"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.02709"
  },
  {
    "id": "arXiv:2106.02711",
    "title": "SketchGen: Generating Constrained CAD Sketches",
    "abstract": "Computer-aided design (CAD) is the most widely used modeling approach for\ntechnical design. The typical starting point in these designs is 2D sketches\nwhich can later be extruded and combined to obtain complex three-dimensional\nassemblies. Such sketches are typically composed of parametric primitives, such\nas points, lines, and circular arcs, augmented with geometric constraints\nlinking the primitives, such as coincidence, parallelism, or orthogonality.\nSketches can be represented as graphs, with the primitives as nodes and the\nconstraints as edges. Training a model to automatically generate CAD sketches\ncan enable several novel workflows, but is challenging due to the complexity of\nthe graphs and the heterogeneity of the primitives and constraints. In\nparticular, each type of primitive and constraint may require a record of\ndifferent size and parameter types. We propose SketchGen as a generative model\nbased on a transformer architecture to address the heterogeneity problem by\ncarefully designing a sequential language for the primitives and constraints\nthat allows distinguishing between different primitive or constraint types and\ntheir parameters, while encouraging our model to re-use information across\nrelated parameters, encoding shared structure. A particular highlight of our\nwork is the ability to produce primitives linked via constraints that enables\nthe final output to be further regularized via a constraint solver. We evaluate\nour model by demonstrating constraint prediction for given sets of primitives\nand full sketch generation from scratch, showing that our approach\nsignificantly out performs the state-of-the-art in CAD sketch generation.",
    "descriptor": "\nComments: 21 pages, 12 figures, 8 tables\n",
    "authors": [
      "Wamiq Reyaz Para",
      "Shariq Farooq Bhat",
      "Paul Guerrero",
      "Tom Kelly",
      "Niloy Mitra",
      "Leonidas Guibas",
      "Peter Wonka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.02711"
  },
  {
    "id": "arXiv:2106.02714",
    "title": "Point Cloud Failure Criterion for Composites using k-Nearest Neighbor  Classification",
    "abstract": "Numerous theories of failure have been postulated and implemented in various\ncommercial programs for composite materials. Even the best theories have had\nlimited success in predicting damage and failure in validation exercises. In\nview of this background, many researchers have started exploring the use of\nmultiscale modeling to improve the fidelity of the modeling and simulation of\nvarious structural and materials systems. In this paper, a multi-scale modeling\nscheme is used to illustrate how a combination of virtual and laboratory\ntesting programs can be used to generate a point cloud of failure surface data\nthat can then be queried during finite element analysis at the continuum scale\nto ascertain if the onset of failure has occurred. The k-nearest neighbor\n(k-NN) classification concept is used to obtain the answer to the query. A\nlinear, elastic, static finite element example using a unidirectional composite\nshows that the framework can be generated and used effectively and efficiently\nwith the possibility to extend the approach for all types of composite\narchitectures and behaviors.",
    "descriptor": "\nComments: 19 pages, 11 figures\n",
    "authors": [
      "Subramaniam Rajan",
      "Bilal Khaled",
      "Loukham Shyamsunder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02714"
  },
  {
    "id": "arXiv:2106.02715",
    "title": "Auditing Source Diversity Bias in Video Search Results Using Virtual  Agents",
    "abstract": "We audit the presence of domain-level source diversity bias in video search\nresults. Using a virtual agent-based approach, we compare outputs of four\nWestern and one non-Western search engines for English and Russian queries. Our\nfindings highlight that source diversity varies substantially depending on the\nlanguage with English queries returning more diverse outputs. We also find\ndisproportionately high presence of a single platform, YouTube, in top search\noutputs for all Western search engines except Google. At the same time, we\nobserve that Youtube's major competitors such as Vimeo or Dailymotion do not\nappear in the sampled Google's video search results. This finding suggests that\nGoogle might be downgrading the results from the main competitors of\nGoogle-owned Youtube and highlights the necessity for further studies focusing\non the presence of own-content bias in Google's search results.",
    "descriptor": "",
    "authors": [
      "Aleksandra Urman",
      "Mykola Makhortykh",
      "Roberto Ulloa"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.02715"
  },
  {
    "id": "arXiv:2106.02716",
    "title": "VEER: Disagreement-Free Multi-objective Configuration",
    "abstract": "Software comes with many configuration options, satisfying varying needs from\nusers. Exploring those options for non-functional requirements can be tedious,\ntime-consuming, and even error-prone (if done manually). Worse, many software\nsystems can be tuned to multiple objectives (e.g., faster response time, fewer\nmemory requirements, decreased network traffic, decreased energy consumption,\netc.). Learning how to adjust the system among these multiple objectives is\ncomplicated due to the trade-off among objectives; i.e., things that seem\nuseful to achieve one objective could be detrimental to another objective.\nConsequentially, the optimizer built for one objective may have different (or\neven opposite) insights on how to locate good solutions from the optimizer\nbuilt from another objective. In this paper, we define this scenario as the\nmodel disagreement problem.\nOne possible solution to this problem is to find a one-dimensional\napproximation to the N-objective space. In this way, the case is converted to a\nsingle-objective optimization, which is naturally confusion-free. This paper\ndemonstrates VEER, a tool that builds such an approximation by combining our\ndimensionality-reduction heuristic on top of one of the state-of-the-art\noptimizers, FLASH. VEER can explore very large configuration spaces by\nevaluating just a small fraction of the total number of configurations (e.g., a\nspace of 81,000 configurations can be explored by 70 samples).\nThe experimental result in this paper demonstrates the feasibility of our\napproach in terms of the on-par quality of the solution set generated by the\noptimizer and the resolved model disagreement within the optimizer. Moreover,\nwe demonstrate that VEER has an improved computational complexity compared to\nthe original optimizer (up to 1,000 times faster while maintaining on-par\nperformance).",
    "descriptor": "\nComments: 11 pages, 6 figures, 4 tables, submitted to ASE'21\n",
    "authors": [
      "Kewen Peng",
      "Christian Kaltenecker",
      "Norbert Siegmund",
      "Sven Apel",
      "Tim Menzies"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.02716"
  },
  {
    "id": "arXiv:2106.02719",
    "title": "Hierarchical Video Generation for Complex Data",
    "abstract": "Videos can often be created by first outlining a global description of the\nscene and then adding local details. Inspired by this we propose a hierarchical\nmodel for video generation which follows a coarse to fine approach. First our\nmodel generates a low resolution video, establishing the global scene\nstructure, that is then refined by subsequent levels in the hierarchy. We train\neach level in our hierarchy sequentially on partial views of the videos. This\nreduces the computational complexity of our generative model, which scales to\nhigh-resolution videos beyond a few frames. We validate our approach on\nKinetics-600 and BDD100K, for which we train a three level model capable of\ngenerating 256x256 videos with 48 frames.",
    "descriptor": "",
    "authors": [
      "Lluis Castrejon",
      "Nicolas Ballas",
      "Aaron Courville"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02719"
  },
  {
    "id": "arXiv:2106.02720",
    "title": "An Even More Optimal Stochastic Optimization Algorithm: Minibatching and  Interpolation Learning",
    "abstract": "We present and analyze an algorithm for optimizing smooth and convex or\nstrongly convex objectives using minibatch stochastic gradient estimates. The\nalgorithm is optimal with respect to its dependence on both the minibatch size\nand minimum expected loss simultaneously. This improves over the optimal method\nof Lan (2012), which is insensitive to the minimum expected loss; over the\noptimistic acceleration of Cotter et al. (2011), which has suboptimal\ndependence on the minibatch size; and over the algorithm of Liu and Belkin\n(2018), which is limited to least squares problems and is also similarly\nsuboptimal with respect to the minibatch size. Applied to interpolation\nlearning, the improvement over Cotter et al. and Liu and Belkin translates to a\nlinear, rather than square-root, parallelization speedup.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Blake Woodworth",
      "Nathan Srebro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02720"
  },
  {
    "id": "arXiv:2106.02725",
    "title": "MultiOpEd: A Corpus of Multi-Perspective News Editorials",
    "abstract": "We propose MultiOpEd, an open-domain news editorial corpus that supports\nvarious tasks pertaining to the argumentation structure in news editorials,\nfocusing on automatic perspective discovery. News editorial is a genre of\npersuasive text, where the argumentation structure is usually implicit.\nHowever, the arguments presented in an editorial typically center around a\nconcise, focused thesis, which we refer to as their perspective. MultiOpEd aims\nat supporting the study of multiple tasks relevant to automatic perspective\ndiscovery, where a system is expected to produce a single-sentence thesis\nstatement summarizing the arguments presented. We argue that identifying and\nabstracting such natural language perspectives from editorials is a crucial\nstep toward studying the implicit argumentation structure in news editorials.\nWe first discuss the challenges and define a few conceptual tasks towards our\ngoal. To demonstrate the utility of MultiOpEd and the induced tasks, we study\nthe problem of perspective summarization in a multi-task learning setting, as a\ncase study. We show that, with the induced tasks as auxiliary tasks, we can\nimprove the quality of the perspective summary generated. We hope that\nMultiOpEd will be a useful resource for future studies on argumentation in the\nnews editorial domain.",
    "descriptor": "",
    "authors": [
      "Siyi Liu",
      "Sihao Chen",
      "Xander Uyttendaele",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02725"
  },
  {
    "id": "arXiv:2106.02726",
    "title": "Popularity is linked to neural coordination: Neural evidence for an Anna  Karenina principle in social networks",
    "abstract": "People differ in how they attend to, interpret, and respond to their\nsurroundings. Convergent processing of the world may be one factor that\ncontributes to social connections between individuals. We used neuroimaging and\nnetwork analysis to investigate whether the most central individuals in their\ncommunities (as measured by in-degree centrality, a notion of popularity)\nprocess the world in a particularly normative way. More central individuals had\nexceptionally similar neural responses to their peers and especially to each\nother in brain regions associated with high-level interpretations and social\ncognition (e.g., in the default-mode network), whereas less-central individuals\nexhibited more idiosyncratic responses. Self-reported enjoyment of and interest\nin stimuli followed a similar pattern, but accounting for these data did not\nchange our main results. These findings suggest an \"Anna Karenina principle\" in\nsocial networks: Highly-central individuals process the world in exceptionally\nsimilar ways, whereas less-central individuals process the world in\nidiosyncratic ways.",
    "descriptor": "",
    "authors": [
      "Elisa C. Baek",
      "Ryan Hyon",
      "Karina L\u00f3pez",
      "Emily S. Finn",
      "Mason A. Porter",
      "Carolyn Parkinson"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.02726"
  },
  {
    "id": "arXiv:2106.02731",
    "title": "Technical Report: Man-in-the-Middle Attack Resistant Secret Key  Generation via Channel Randomization",
    "abstract": "Physical-layer based key generation schemes exploit the channel reciprocity\nfor secret key extraction, which can achieve information-theoretic secrecy\nagainst eavesdroppers. Such methods, although practical, have been shown to be\nvulnerable against man-in-the-middle (MitM) attacks, where an active adversary,\nMallory, can influence and infer part of the secret key generated between Alice\nand Bob by injecting her own packet upon observing highly correlated\nchannel/RSS measurements from Alice and Bob. As all the channels remain stable\nwithin the channel coherence time, Mallory's injected packets cause Alice and\nBob to measure similar RSS, which allows Mallory to successfully predict the\nderived key bits. To defend against such a MitM attack, we propose to utilize a\nreconfigurable antenna at one of the legitimate transceivers to proactively\nrandomize the channel state across different channel probing rounds. The\nrandomization of the antenna mode at every probing round breaks the temporal\ncorrelation of the channels from the adversary to the legitimate devices, while\npreserving the reciprocity of the channel between the latter. This prevents key\ninjection from the adversary without affecting Alice and Bob's ability to\nmeasure common randomness. We theoretically analyze the security of the\nprotocol and conduct extensive simulations and real-world experiments to\nevaluate its performance. Our results show that our approach eliminates the\nadvantage of an active MitM attack by driving down the probability of\nsuccessfully guessing bits of the secret key to a random guess.",
    "descriptor": "\nComments: 13 pages, 8 figures, 4 tables\n",
    "authors": [
      "Yanjun Pan",
      "Ziqi Xu",
      "Ming Li",
      "Loukas Lazos"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02731"
  },
  {
    "id": "arXiv:2106.02732",
    "title": "BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian  Optimization",
    "abstract": "Decision-based attacks (DBA), wherein attackers perturb inputs to spoof\nlearning algorithms by observing solely the output labels, are a type of severe\nadversarial attacks against Deep Neural Networks (DNNs) requiring minimal\nknowledge of attackers. State-of-the-art DBA attacks relying on zeroth-order\ngradient estimation require an excessive number of queries. Recently, Bayesian\noptimization (BO) has shown promising in reducing the number of queries in\nscore-based attacks (SBA), in which attackers need to observe real-valued\nprobability scores as outputs. However, extending BO to the setting of DBA is\nnontrivial because in DBA only output labels instead of real-valued scores, as\nneeded by BO, are available to attackers. In this paper, we close this gap by\nproposing an efficient DBA attack, namely BO-DBA. Different from existing\napproaches, BO-DBA generates adversarial examples by searching so-called\n\\emph{directions of perturbations}. It then formulates the problem as a BO\nproblem that minimizes the real-valued distortion of perturbations. With the\noptimized perturbation generation process, BO-DBA converges much faster than\nthe state-of-the-art DBA techniques. Experimental results on pre-trained\nImageNet classifiers show that BO-DBA converges within 200 queries while the\nstate-of-the-art DBA techniques need over 15,000 queries to achieve the same\nlevel of perturbation distortion. BO-DBA also shows similar attack success\nrates even as compared to BO-based SBA attacks but with less distortion.",
    "descriptor": "",
    "authors": [
      "Zhuosheng Zhang",
      "Shucheng Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02732"
  },
  {
    "id": "arXiv:2106.02733",
    "title": "DISCO: accurate Discrete Scale Convolutions",
    "abstract": "Scale is often seen as a given, disturbing factor in many vision tasks. When\ndoing so it is one of the factors why we need more data during learning. In\nrecent work scale equivariance was added to convolutional neural networks. It\nwas shown to be effective for a range of tasks. We aim for accurate\nscale-equivariant convolutional neural networks (SE-CNNs) applicable for\nproblems where high granularity of scale and small filter sizes are required.\nCurrent SE-CNNs rely on weight sharing and filter rescaling, the latter of\nwhich is accurate for integer scales only. To reach accurate scale\nequivariance, we derive general constraints under which scale-convolution\nremains equivariant to discrete rescaling. We find the exact solution for all\ncases where it exists, and compute the approximation for the rest. The discrete\nscale-convolution pays off, as demonstrated in a new state-of-the-art\nclassification on MNIST-scale and improving the results on STL-10. With the\nsame SE scheme, we also improve the computational effort of a scale-equivariant\nSiamese tracker on OTB-13.",
    "descriptor": "",
    "authors": [
      "Ivan Sosnovik",
      "Artem Moskalev",
      "Arnold Smeulders"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02733"
  },
  {
    "id": "arXiv:2106.02734",
    "title": "Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial  Robustness",
    "abstract": "We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck\nas a regularizer for learning an adversarially robust deep neural network\nclassifier. We show that the HSIC bottleneck enhances robustness to adversarial\nattacks both theoretically and experimentally. Our experiments on multiple\nbenchmark datasets and architectures demonstrate that incorporating an HSIC\nbottleneck regularizer attains competitive natural accuracy and improves\nadversarial robustness, both with and without adversarial examples during\ntraining.",
    "descriptor": "",
    "authors": [
      "Zifeng Wang",
      "Tong Jian",
      "Aria Masoomi",
      "Stratis Ioannidis",
      "Jennifer Dy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02734"
  },
  {
    "id": "arXiv:2106.02736",
    "title": "Exposing the Implicit Energy Networks behind Masked Language Models via  Metropolis--Hastings",
    "abstract": "While recent work has shown that scores from models trained by the ubiquitous\nmasked language modeling (MLM) objective effectively discriminate probable and\nimprobable sequences, it is still an open question if these MLMs specify a\nprincipled probability distribution over the space of possible sequences. In\nthis paper, we interpret MLMs as energy-based sequence models and propose two\nenergy parametrizations derivable from the trained MLMs. In order to draw\nsamples correctly from these models, we develop a tractable \\emph{sampling}\nscheme based on the Metropolis--Hastings Monte Carlo algorithm. In our\napproach, samples are proposed from the same masked conditionals used for\ntraining the masked language models, and they are accepted or rejected based on\ntheir energy values according to the target distribution. We validate the\neffectiveness of the proposed parametrizations by exploring the quality of\nsamples drawn from these energy-based models on the conditional generation task\nof machine translation. We theoretically and empirically justify our sampling\nalgorithm by showing that the masked conditionals on their own do not yield a\nMarkov chain whose stationary distribution is that of our target distribution,\nand our approach generates higher quality samples than other recently proposed\nundirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,\n2019).",
    "descriptor": "",
    "authors": [
      "Kartik Goyal",
      "Chris Dyer",
      "Taylor Berg-Kirkpatrick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02736"
  },
  {
    "id": "arXiv:2106.02737",
    "title": "Negotiation-Aware Reachability-Based Safety Verification for  AutonomousDriving in Interactive Scenarios",
    "abstract": "Safety assurance is a critical yet challenging aspect when developing\nself-driving technologies. Hamilton-Jacobi backward-reachability analysis is a\nformal verification tool for verifying the safety of dynamic systems in the\npresence of disturbances. However, the standard approach is too conservative to\nbe applied to self-driving applications due to its worst-case assumption on\nhumans' behaviors (i.e., guard against worst-case outcomes). In this work, we\nintegrate a learning-based prediction algorithm and a game-theoretic human\nbehavioral model to online update the conservativeness of backward-reachability\nanalysis. We evaluate our approach using real driving data. The results show\nthat, with reasonable assumptions on human behaviors, our approach can\neffectively reduce the conservativeness of the standard approach without\nsacrificing its safety verification ability.",
    "descriptor": "\nComments: This work is presented at the ICRA 2021 Workshop on Safe Robot Control with Learned Motion and Environment Models\n",
    "authors": [
      "Ran Tian",
      "Anjian Li",
      "Masayoshi Tomizuka",
      "Liting Sun"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02737"
  },
  {
    "id": "arXiv:2106.02738",
    "title": "Encoder-Decoder Neural Architecture Optimization for Keyword Spotting",
    "abstract": "Keyword spotting aims to identify specific keyword audio utterances. In\nrecent years, deep convolutional neural networks have been widely utilized in\nkeyword spotting systems. However, their model architectures are mainly based\non off-the shelfbackbones such as VGG-Net or ResNet, instead of specially\ndesigned for the task. In this paper, we utilize neural architecture search to\ndesign convolutional neural network models that can boost the performance of\nkeyword spotting while maintaining an acceptable memory footprint.\nSpecifically, we search the model operators and their connections in a specific\nsearch space with Encoder-Decoder neural architecture optimization. Extensive\nevaluations on Google's Speech Commands Dataset show that the model\narchitecture searched by our approach achieves a state-of-the-art accuracy of\nover 97%.",
    "descriptor": "\nComments: Accepted for Interspeech2021\n",
    "authors": [
      "Tong Mo",
      "Bang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.02738"
  },
  {
    "id": "arXiv:2106.02740",
    "title": "ZeroWaste Dataset: Towards Automated Waste Recycling",
    "abstract": "Less than 35% of recyclable waste is being actually recycled in the US, which\nleads to increased soil and sea pollution and is one of the major concerns of\nenvironmental researchers as well as the common public. At the heart of the\nproblem is the inefficiencies of the waste sorting process (separating paper,\nplastic, metal, glass, etc.) due to the extremely complex and cluttered nature\nof the waste stream. Automated waste detection strategies have a great\npotential to enable more efficient, reliable and safer waste sorting practices,\nbut the literature lacks comprehensive datasets and methodology for the\nindustrial waste sorting solutions. In this paper, we take a step towards\ncomputer-aided waste detection and present the first in-the-wild\nindustrial-grade waste detection and segmentation dataset, ZeroWaste. This\ndataset contains over1800fully segmented video frames collected from a real\nwaste sorting plant along with waste material labels for training and\nevaluation of the segmentation methods, as well as over6000unlabeled frames\nthat can be further used for semi-supervised and self-supervised learning\ntechniques. ZeroWaste also provides frames of the conveyor belt before and\nafter the sorting process, comprising a novel setup that can be used for\nweakly-supervised segmentation. We present baselines for fully-, semi- and\nweakly-supervised segmentation methods. Our experimental results demonstrate\nthat state-of-the-art segmentation methods struggle to correctly detect and\nclassify target objects which suggests the challenging nature of our proposed\nin-the-wild dataset. We believe that ZeroWastewill catalyze research in object\ndetection and semantic segmentation in extreme clutter as well as applications\nin the recycling domain. Our project page can be found\natthis http URL",
    "descriptor": "",
    "authors": [
      "Dina Bashkirova",
      "Ziliang Zhu",
      "James Akl",
      "Fadi Alladkani",
      "Ping Hu",
      "Vitaly Ablavsky",
      "Berk Calli",
      "Sarah Adel Bargal",
      "Kate Saenko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02740"
  },
  {
    "id": "arXiv:2106.02742",
    "title": "Latent Time-Adaptive Drift-Diffusion Model",
    "abstract": "Animals can quickly learn the timing of events with fixed intervals and their\nrate of acquisition does not depend on the length of the interval. In contrast,\nrecurrent neural networks that use gradient based learning have difficulty\npredicting the timing of events that depend on stimulus that occurred long ago.\nWe present the latent time-adaptive drift-diffusion model (LTDDM), an extension\nto the time-adaptive drift-diffusion model (TDDM), a model for animal learning\nof timing that exhibits behavioural properties consistent with experimental\ndata from animals. The performance of LTDDM is compared to that of a state of\nthe art long short-term memory (LSTM) recurrent neural network across three\ntiming tasks. Differences in the relative performance of these two models is\ndiscussed and it is shown how LTDDM can learn these events time series orders\nof magnitude faster than recurrent neural networks.",
    "descriptor": "",
    "authors": [
      "Gabriele Cimolino",
      "Francois Rivest"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02742"
  },
  {
    "id": "arXiv:2106.02743",
    "title": "SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural  Networks",
    "abstract": "Graph Neural Networks (GNNs) are the first choice methods for graph machine\nlearning problems thanks to their ability to learn state-of-the-art level\nrepresentations from graph-structured data. However, centralizing a massive\namount of real-world graph data for GNN training is prohibitive due to\nuser-side privacy concerns, regulation restrictions, and commercial\ncompetition. Federated Learning is the de-facto standard for collaborative\ntraining of machine learning models over many distributed edge devices without\nthe need for centralization. Nevertheless, training graph neural networks in a\nfederated setting is vaguely defined and brings statistical and systems\nchallenges. This work proposes SpreadGNN, a novel multi-task federated training\nframework capable of operating in the presence of partial labels and absence of\na central server for the first time in the literature. SpreadGNN extends\nfederated multi-task learning to realistic serverless settings for GNNs, and\nutilizes a novel optimization algorithm with a convergence guarantee,\nDecentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized\nmulti-task learning problems. We empirically demonstrate the efficacy of our\nframework on a variety of non-I.I.D. distributed graph-level molecular property\nprediction datasets with partial labels. Our results show that SpreadGNN\noutperforms GNN models trained over a central server-dependent federated\nlearning system, even in constrained topologies. The source code is publicly\navailable at https://github.com/FedML-AI/SpreadGNN",
    "descriptor": "\nComments: Three co-1st authors have equal contribution (alphabetical order)\n",
    "authors": [
      "Chaoyang He",
      "Emir Ceyani",
      "Keshav Balasubramanian",
      "Murali Annavaram",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02743"
  },
  {
    "id": "arXiv:2106.02745",
    "title": "Discovering Multi-Agent Auto-Curricula in Two-Player Zero-Sum Games",
    "abstract": "When solving two-player zero-sum games, multi-agent reinforcement learning\n(MARL) algorithms often create populations of agents where, at each iteration,\na new agent is discovered as the best response to a mixture over the opponent\npopulation. Within such a process, the update rules of \"who to compete with\"\n(i.e., the opponent mixture) and \"how to beat them\" (i.e., finding best\nresponses) are underpinned by manually developed game theoretical principles\nsuch as fictitious play and Double Oracle. In this paper we introduce a\nframework, LMAC, based on meta-gradient descent that automates the discovery of\nthe update rule without explicit human design. Specifically, we parameterise\nthe opponent selection module by neural networks and the best-response module\nby optimisation subroutines, and update their parameters solely via interaction\nwith the game engine, where both players aim to minimise their exploitability.\nSurprisingly, even without human design, the discovered MARL algorithms achieve\ncompetitive or even better performance with the state-of-the-art\npopulation-based game solvers (e.g., PSRO) on Games of Skill, differentiable\nLotto, non-transitive Mixture Games, Iterated Matching Pennies, and Kuhn Poker.\nAdditionally, we show that LMAC is able to generalise from small games to large\ngames, for example training on Kuhn Poker and outperforming PSRO on Leduc\nPoker. Our work inspires a promising future direction to discover general MARL\nalgorithms solely from data.",
    "descriptor": "\nComments: corresponding to &lt;yaodong.yang@outlook.com&gt;\n",
    "authors": [
      "Xidong Feng",
      "Oliver Slumbers",
      "Yaodong Yang",
      "Ziyu Wan",
      "Bo Liu",
      "Stephen McAleer",
      "Ying Wen",
      "Jun Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.02745"
  },
  {
    "id": "arXiv:2106.02747",
    "title": "Quantum Reduction of Finding Short Code Vectors to the Decoding Problem",
    "abstract": "We give a quantum reduction from finding short codewords in a random linear\ncode to decoding for the Hamming metric. This is the first time such a\nreduction (classical or quantum) has been obtained. Our reduction adapts to\nlinear codes Stehl\\'{e}-Steinfield-Tanaka-Xagawa' re-interpretation of Regev's\nquantum reduction from finding short lattice vectors to solving the Closest\nVector Problem. The Hamming metric is a much coarser metric than the Euclidean\nmetric and this adaptation has needed several new ingredients to make it work.\nFor instance, in order to have a meaningful reduction it is necessary in the\nHamming metric to choose a very large decoding radius and this needs in many\ncases to go beyond the radius where decoding is unique. Another crucial step\nfor the analysis of the reduction is the choice of the errors that are being\nfed to the decoding algorithm. For lattices, errors are usually sampled\naccording to a Gaussian distribution. However, it turns out that the Bernoulli\ndistribution (the analogue for codes of the Gaussian) is too much spread out\nand can not be used for the reduction with codes. Instead we choose here the\nuniform distribution over errors of a fixed weight and bring in orthogonal\npolynomials tools to perform the analysis and an additional amplitude\namplification step to obtain the aforementioned result.",
    "descriptor": "",
    "authors": [
      "Thomas Debris-Alazard",
      "Maxime Remaud",
      "Jean-Pierre Tillich"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.02747"
  },
  {
    "id": "arXiv:2106.02748",
    "title": "Decentralized Q-Learning in Zero-sum Markov Games",
    "abstract": "We study multi-agent reinforcement learning (MARL) in infinite-horizon\ndiscounted zero-sum Markov games. We focus on the practical but challenging\nsetting of decentralized MARL, where agents make decisions without coordination\nby a centralized controller, but only based on their own payoffs and local\nactions executed. The agents need not observe the opponent's actions or\npayoffs, possibly being even oblivious to the presence of the opponent, nor be\naware of the zero-sum structure of the underlying game, a setting also referred\nto as radically uncoupled in the literature of learning in games. In this\npaper, we develop for the first time a radically uncoupled Q-learning dynamics\nthat is both rational and convergent: the learning dynamics converges to the\nbest response to the opponent's strategy when the opponent follows an\nasymptotically stationary strategy; the value function estimates converge to\nthe payoffs at a Nash equilibrium when both agents adopt the dynamics. The key\nchallenge in this decentralized setting is the non-stationarity of the learning\nenvironment from an agent's perspective, since both her own payoffs and the\nsystem evolution depend on the actions of other agents, and each agent adapts\ntheir policies simultaneously and independently. To address this issue, we\ndevelop a two-timescale learning dynamics where each agent updates her local\nQ-function and value function estimates concurrently, with the latter happening\nat a slower timescale.",
    "descriptor": "",
    "authors": [
      "Muhammed O. Sayin",
      "Kaiqing Zhang",
      "David S. Leslie",
      "Tamer Basar",
      "Asuman Ozdaglar"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02748"
  },
  {
    "id": "arXiv:2106.02749",
    "title": "Predify: Augmenting deep neural networks with brain-inspired predictive  coding dynamics",
    "abstract": "Deep neural networks excel at image classification, but their performance is\nfar less robust to input perturbations than human perception. In this work we\nexplore whether this shortcoming may be partly addressed by incorporating\nbrain-inspired recurrent dynamics in deep convolutional networks. We take\ninspiration from a popular framework in neuroscience: 'predictive coding'. At\neach layer of the hierarchical model, generative feedback 'predicts' (i.e.,\nreconstructs) the pattern of activity in the previous layer. The reconstruction\nerrors are used to iteratively update the network's representations across\ntimesteps, and to optimize the network's feedback weights over the natural\nimage dataset-a form of unsupervised training. We show that implementing this\nstrategy into two popular networks, VGG16 and EfficientNetB0, improves their\nrobustness against various corruptions. We hypothesize that other feedforward\nnetworks could similarly benefit from the proposed framework. To promote\nresearch in this direction, we provide an open-sourced PyTorch-based package\ncalled Predify, which can be used to implement and investigate the impacts of\nthe predictive coding dynamics in any convolutional neural network.",
    "descriptor": "\nComments: Preprint under review\n",
    "authors": [
      "Bhavin Choksi",
      "Milad Mozafari",
      "Callum Biggs O'May",
      "Benjamin Ador",
      "Andrea Alamia",
      "Rufin VanRullen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.02749"
  },
  {
    "id": "arXiv:2106.02754",
    "title": "Unconditionally energy stable and first-order accurate numerical schemes  for the heat equation with uncertain temperature-dependent conductivity",
    "abstract": "In this paper, we present first-order accurate numerical methods for solution\nof the heat equation with uncertain temperature-dependent thermal conductivity.\nEach algorithm yields a shared coefficient matrix for the ensemble set\nimproving computational efficiency. Both mixed and Robin-type boundary\nconditions are treated. In contrast with alternative, related methodologies,\nstability and convergence are unconditional. In particular, we prove\nunconditional, energy stability and optimal-order error estimates. A battery of\nnumerical tests are presented to illustrate both the theory and application of\nthese algorithms.",
    "descriptor": "",
    "authors": [
      "J. A. Fiordilino",
      "M. Winger"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.02754"
  },
  {
    "id": "arXiv:2106.02755",
    "title": "Kernel approximation on algebraic varieties",
    "abstract": "Low-rank approximation of kernels is a fundamental mathematical problem with\nwidespread algorithmic applications. Often the kernel is restricted to an\nalgebraic variety, e.g., in problems involving sparse or low-rank data. We show\nthat significantly better approximations are obtainable in this setting: the\nrank required to achieve a given error depends on the variety's dimension\nrather than the ambient dimension, which is typically much larger. This is true\nin both high-precision and high-dimensional regimes. Our results are presented\nfor smooth isotropic kernels, the predominant class of kernels used in\napplications. Our main technical insight is to approximate smooth kernels by\npolynomial kernels, and leverage two key properties of polynomial kernels that\nhold when they are restricted to a variety. First, their ranks decrease\nexponentially in the variety's co-dimension. Second, their maximum values are\ngoverned by their values over a small set of points. Together, our results\nprovide a general approach for exploiting (approximate) \"algebraic structure\"\nin datasets in order to efficiently solve large-scale data science problems.",
    "descriptor": "",
    "authors": [
      "Jason M. Altschuler",
      "Pablo A. Parrilo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2106.02755"
  },
  {
    "id": "arXiv:2106.02757",
    "title": "Heuristic-Guided Reinforcement Learning",
    "abstract": "We provide a framework for accelerating reinforcement learning (RL)\nalgorithms by heuristics constructed from domain knowledge or offline data.\nTabula rasa RL algorithms require environment interactions or computation that\nscales with the horizon of the sequential decision-making task. Using our\nframework, we show how heuristic-guided RL induces a much shorter-horizon\nsubproblem that provably solves the original task. Our framework can be viewed\nas a horizon-based regularization for controlling bias and variance in RL under\na finite interaction budget. On the theoretical side, we characterize\nproperties of a good heuristic and its impact on RL acceleration. In\nparticular, we introduce the novel concept of an \"improvable heuristic\" -- a\nheuristic that allows an RL agent to extrapolate beyond its prior knowledge. On\nthe empirical side, we instantiate our framework to accelerate several\nstate-of-the-art algorithms in simulated robotic control tasks and procedurally\ngenerated games. Our framework complements the rich literature on warm-starting\nRL with expert demonstrations or exploratory datasets, and introduces a\nprincipled method for injecting prior knowledge into RL.",
    "descriptor": "",
    "authors": [
      "Ching-An Cheng",
      "Andrey Kolobov",
      "Adith Swaminathan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02757"
  },
  {
    "id": "arXiv:2106.02762",
    "title": "Faster and Generalized Temporal Triangle Counting, via Degeneracy  Ordering",
    "abstract": "Triangle counting is a fundamental technique in network analysis, that has\nreceived much attention in various input models. The vast majority of triangle\ncounting algorithms are targeted to static graphs. Yet, many real-world graphs\nare directed and temporal, where edges come with timestamps. Temporal triangles\nyield much more information, since they account for both the graph topology and\nthe timestamps.\nTemporal triangle counting has seen a few recent results, but there are\nvarying definitions of temporal triangles. In all cases, temporal triangle\npatterns enforce constraints on the time interval between edges (in the\ntriangle). We define a general notion $(\\delta_{1,3}, \\delta_{1,2},\n\\delta_{2,3})$-temporal triangles that allows for separate time constraints for\nall pairs of edges.\nOur main result is a new algorithm, DOTTT (Degeneracy Oriented Temporal\nTriangle Totaler), that exactly counts all directed variants of $(\\delta_{1,3},\n\\delta_{1,2}, \\delta_{2,3})$-temporal triangles. Using the classic idea of\ndegeneracy ordering with careful combinatorial arguments, we can prove that\nDOTTT runs in $O(m\\kappa\\log m)$ time, where $m$ is the number of (temporal)\nedges and $\\kappa$ is the graph degeneracy (max core number). Up to log\nfactors, this matches the running time of the best static triangle counters.\nMoreover, this running time is better than existing.\nDOTTT has excellent practical behavior and runs twice as fast as existing\nstate-of-the-art temporal triangle counters (and is also more general). For\nexample, DOTTT computes all types of temporal queries in Bitcoin temporal\nnetwork with half a billion edges in less than an hour on a commodity machine.",
    "descriptor": "\nComments: To be published in KDD 2021\n",
    "authors": [
      "Noujan Pashanasangi",
      "C. Seshadhri"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02762"
  },
  {
    "id": "arXiv:2106.02766",
    "title": "Quantum Measurement Adversary",
    "abstract": "Multi-source-extractors are functions that extract uniform randomness from\nmultiple (weak) sources of randomness. Quantum multi-source-extractors were\nconsidered by Kasher and Kempe (for the quantum-independent-adversary and the\nquantum-bounded-storage-adversary), Chung, Li and Wu (for the\ngeneral-entangled-adversary) and Arnon-Friedman, Portmann and Scholz (for the\nquantum-Markov-adversary). One of the main objectives of this work is to unify\nall the existing quantum multi-source adversary models. We propose two new\nmodels of adversaries: 1) the quantum-measurement-adversary (qm-adv), which\ngenerates side-information using entanglement and on post-measurement and 2)\nthe quantum-communication-adversary (qc-adv), which generates side-information\nusing entanglement and communication between multiple sources. We show that, 1.\nqm-adv is the strongest adversary among all the known adversaries, in the sense\nthat the side-information of all other adversaries can be generated by qm-adv.\n2. The (generalized) inner-product function (in fact a general class of\ntwo-wise independent functions) continues to work as a good extractor against\nqm-adv with matching parameters as that of Chor and Goldreich. 3. A\nnon-malleable-extractor proposed by Li (against classical-adversaries)\ncontinues to be secure against quantum side-information. This result implies a\nnon-malleable-extractor result of Aggarwal, Chung, Lin and Vidick with uniform\nseed. We strengthen their result via a completely different proof to make the\nnon-malleable-extractor of Li secure against quantum side-information even when\nthe seed is not uniform. 4. A modification (working with weak sources instead\nof uniform sources) of the Dodis and Wichs protocol for privacy-amplification\nis secure against active quantum adversaries. This strengthens on a recent\nresult due to Aggarwal, Chung, Lin and Vidick which uses uniform sources.",
    "descriptor": "",
    "authors": [
      "Divesh Aggarwal",
      "Naresh Goud Boddu",
      "Rahul Jain",
      "Maciej Obremski"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.02766"
  },
  {
    "id": "arXiv:2106.02768",
    "title": "Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate  Prediction",
    "abstract": "Cross domain recommender system constitutes a powerful method to tackle the\ncold-start and sparsity problem by aggregating and transferring user\npreferences across multiple category domains. Therefore, it has great potential\nto improve click-through-rate prediction performance in online commerce\nplatforms having many domains of products. While several cross domain\nsequential recommendation models have been proposed to leverage information\nfrom a source domain to improve CTR predictions in a target domain, they did\nnot take into account bidirectional latent relations of user preferences across\nsource-target domain pairs. As such, they cannot provide enhanced cross-domain\nCTR predictions for both domains simultaneously. In this paper, we propose a\nnovel approach to cross-domain sequential recommendations based on the dual\nlearning mechanism that simultaneously transfers information between two\nrelated domains in an iterative manner until the learning process stabilizes.\nIn particular, the proposed Dual Attentive Sequential Learning (DASL) model\nconsists of two novel components Dual Embedding and Dual Attention, which\njointly establish the two-stage learning process: we first construct dual\nlatent embeddings that extract user preferences in both domains simultaneously,\nand subsequently provide cross-domain recommendations by matching the extracted\nlatent embeddings with candidate items through dual-attention learning\nmechanism. We conduct extensive offline experiments on three real-world\ndatasets to demonstrate the superiority of our proposed model, which\nsignificantly and consistently outperforms several state-of-the-art baselines\nacross all experimental settings. We also conduct an online A/B test at a major\nvideo streaming platform Alibaba-Youku, where our proposed model significantly\nimproves business performance over the latest production system in the company.",
    "descriptor": "\nComments: Accepted to KDD21\n",
    "authors": [
      "Pan Li",
      "Zhichao Jiang",
      "Maofei Que",
      "Yao Hu",
      "Alexander Tuzhilin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02768"
  },
  {
    "id": "arXiv:2106.02769",
    "title": "Privacy-Preserving Training of Tree Ensembles over Continuous Data",
    "abstract": "Most existing Secure Multi-Party Computation (MPC) protocols for\nprivacy-preserving training of decision trees over distributed data assume that\nthe features are categorical. In real-life applications, features are often\nnumerical. The standard ``in the clear'' algorithm to grow decision trees on\ndata with continuous values requires sorting of training examples for each\nfeature in the quest for an optimal cut-point in the range of feature values in\neach node. Sorting is an expensive operation in MPC, hence finding secure\nprotocols that avoid such an expensive step is a relevant problem in\nprivacy-preserving machine learning. In this paper we propose three more\nefficient alternatives for secure training of decision tree based models on\ndata with continuous features, namely: (1) secure discretization of the data,\nfollowed by secure training of a decision tree over the discretized data; (2)\nsecure discretization of the data, followed by secure training of a random\nforest over the discretized data; and (3) secure training of extremely\nrandomized trees (``extra-trees'') on the original data. Approaches (2) and (3)\nboth involve randomizing feature choices. In addition, in approach (3)\ncut-points are chosen randomly as well, thereby alleviating the need to sort or\nto discretize the data up front. We implemented all proposed solutions in the\nsemi-honest setting with additive secret sharing based MPC. In addition to\nmathematically proving that all proposed approaches are correct and secure, we\nexperimentally evaluated and compared them in terms of classification accuracy\nand runtime. We privately train tree ensembles over data sets with 1000s of\ninstances or features in a few minutes, with accuracies that are at par with\nthose obtained in the clear. This makes our solution orders of magnitude more\nefficient than the existing approaches, which are based on oblivious sorting.",
    "descriptor": "",
    "authors": [
      "Samuel Adams",
      "Chaitali Choudhary",
      "Martine De Cock",
      "Rafael Dowsley",
      "David Melanson",
      "Anderson C. A. Nascimento",
      "Davis Railsback",
      "Jianwei Shen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02769"
  },
  {
    "id": "arXiv:2106.02770",
    "title": "Accelerating Stochastic Simulation with Interactive Neural Processes",
    "abstract": "Stochastic simulations such as large-scale, spatiotemporal, age-structured\nepidemic models are computationally expensive at fine-grained resolution. We\npropose Interactive Neural Process (INP), an interactive framework to\ncontinuously learn a deep learning surrogate model and accelerate simulation.\nOur framework is based on the novel integration of Bayesian active learning,\nstochastic simulation and deep sequence modeling. In particular, we develop a\nnovel spatiotemporal neural process model to mimic the underlying process\ndynamics. Our model automatically infers the latent process which describes the\nintrinsic uncertainty of the simulator. This also gives rise to a new\nacquisition function that can quantify the uncertainty of deep learning\npredictions. We design Bayesian active learning algorithms to iteratively query\nthe simulator, gather more data, and continuously improve the model. We perform\ntheoretical analysis and demonstrate that our approach reduces sample\ncomplexity compared with random sampling in high dimension. Empirically, we\ndemonstrate our framework can faithfully imitate the behavior of a complex\ninfectious disease simulator with a small number of examples, enabling rapid\nsimulation and scenario exploration.",
    "descriptor": "",
    "authors": [
      "Dongxia Wu",
      "Matteo Chinazzi",
      "Alessandro Vespignani",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02770"
  },
  {
    "id": "arXiv:2106.02771",
    "title": "PURS: Personalized Unexpected Recommender System for Improving User  Satisfaction",
    "abstract": "Classical recommender system methods typically face the filter bubble problem\nwhen users only receive recommendations of their familiar items, making them\nbored and dissatisfied. To address the filter bubble problem, unexpected\nrecommendations have been proposed to recommend items significantly deviating\nfrom user's prior expectations and thus surprising them by presenting \"fresh\"\nand previously unexplored items to the users. In this paper, we describe a\nnovel Personalized Unexpected Recommender System (PURS) model that incorporates\nunexpectedness into the recommendation process by providing multi-cluster\nmodeling of user interests in the latent space and personalized unexpectedness\nvia the self-attention mechanism and via selection of an appropriate unexpected\nactivation function. Extensive offline experiments on three real-world datasets\nillustrate that the proposed PURS model significantly outperforms the\nstate-of-the-art baseline approaches in terms of both accuracy and\nunexpectedness measures. In addition, we conduct an online A/B test at a major\nvideo platform Alibaba-Youku, where our model achieves over 3\\% increase in the\naverage video view per user metric. The proposed model is in the process of\nbeing deployed by the company.",
    "descriptor": "\nComments: Accepted to RecSys20\n",
    "authors": [
      "Pan Li",
      "Maofei Que",
      "Zhichao Jiang",
      "Yao Hu",
      "Alexander Tuzhilin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02771"
  },
  {
    "id": "arXiv:2106.02773",
    "title": "GLSD: The Global Large-Scale Ship Database and Baseline Evaluations",
    "abstract": "In this paper, we introduce a challenging global large-scale ship database\n(called GLSD), designed specifically for ship detection tasks. The designed\nGLSD database includes a total of 140,616 annotated instances from 100,729\nimages. Based on the collected images, we propose 13 categories that widely\nexists in international routes. These categories include sailing boat, fishing\nboat, passenger ship, war ship, general cargo ship, container ship, bulk cargo\ncarrier, barge, ore carrier, speed boat, canoe, oil carrier, and tug. The\nmotivations of developing GLSD include the following: 1) providing a refined\nship detection database; 2) providing the worldwide researchers of ship\ndetection and exhaustive label information (bounding box and ship class label)\nin one uniform global database; and 3) providing a large-scale ship database\nwith geographic information (port and country information) that benefits\nmulti-modal analysis. In addition, we discuss the evaluation protocols given\nimage characteristics in GLSD and analyze the performance of selected\nstate-of-the-art object detection algorithms on GSLD, providing baselines for\nfuture studies. More information regarding the designed GLSD can be found at\nhttps://github.com/jiaming-wang/GLSD.",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Zhenfeng Shao",
      "Jiaming Wang",
      "Lianbing Deng",
      "Xiao Huang",
      "Tao Lu",
      "Ruiqian Zhang",
      "Xianwei Lv",
      "Qing Ding",
      "Zhiqiang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02773"
  },
  {
    "id": "arXiv:2106.02774",
    "title": "Sparsification for Sums of Exponentials and its Algorithmic Applications",
    "abstract": "Many works in signal processing and learning theory operate under the\nassumption that the underlying model is simple, e.g. that a signal is\napproximately $k$-Fourier-sparse or that a distribution can be approximated by\na mixture model that has at most $k$ components. However the problem of fitting\nthe parameters of such a model becomes more challenging when the\nfrequencies/components are too close together.\nIn this work we introduce new methods for sparsifying sums of exponentials\nand give various algorithmic applications. First we study Fourier-sparse\ninterpolation without a frequency gap, where Chen et al. gave an algorithm for\nfinding an $\\epsilon$-approximate solution which uses $k' = \\mbox{poly}(k, \\log\n1/\\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in\none dimension without a separation condition. Kernel density estimators give an\n$\\epsilon$-approximation that uses $k' = O(k/\\epsilon^2)$ components. These\nmethods both output models that are much more complex than what we started out\nwith. We show how to post-process to reduce the number of\nfrequencies/components down to $k' = \\widetilde{O}(k)$, which is optimal up to\nlogarithmic factors. Moreover we give applications to model selection. In\nparticular, we give the first algorithms for approximately (and robustly)\ndetermining the number of components in a Gaussian mixture model that work\nwithout a separation condition.",
    "descriptor": "",
    "authors": [
      "Jerry Li",
      "Allen Liu",
      "Ankur Moitra"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02774"
  },
  {
    "id": "arXiv:2106.02775",
    "title": "Visual communication of object concepts at different levels of  abstraction",
    "abstract": "People can produce drawings of specific entities (e.g., Garfield), as well as\ngeneral categories (e.g., \"cat\"). What explains this ability to produce such\nvaried drawings of even highly familiar object concepts? We hypothesized that\ndrawing objects at different levels of abstraction depends on both sensory\ninformation and representational goals, such that drawings intended to portray\na recently seen object preserve more detail than those intended to represent a\ncategory. Participants drew objects cued either with a photo or a category\nlabel. For each cue type, half the participants aimed to draw a specific\nexemplar; the other half aimed to draw the category. We found that label-cued\ncategory drawings were the most recognizable at the basic level, whereas\nphoto-cued exemplar drawings were the least recognizable. Together, these\nfindings highlight the importance of task context for explaining how people use\ndrawings to communicate visual concepts in different ways.",
    "descriptor": "\nComments: To appear in Proceedings of the 43rd Annual Meeting of the Cognitive Science Society. 7 pages, 5 figures\n",
    "authors": [
      "Justin Yang",
      "Judith E. Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02775"
  },
  {
    "id": "arXiv:2106.02776",
    "title": "Study of Multi-Branch Tomlinson-Harashima Precoding with  Multiple-Antenna Systems and Rate Splitting",
    "abstract": "Rate splitting (RS) has emerged as a valuable technology for wireless\ncommunications systems due to its capability to deal with uncertainties in the\nchannel state information at the transmitter (CSIT). RS with linear and\nnon-linear precoders, such as the Tomlinson-Harashima (THP) precoder, have been\nexplored in the downlink (DL) of multiuser multi antenna systems. In this work,\nwe propose a multi-branch (MB) scheme for a RS-based multiple-antenna system,\nwhich creates patterns to order the transmitted symbols and enhances the\noverall sum rate performance compared to existing approaches. Closed-form\nexpressions are derived for the sum rate through statistical analysis.\nSimulation results show that the proposed MB-THP for RS outperforms\nconventional THP and MB-THP schemes.",
    "descriptor": "\nComments: 2 figures, 7 pages\n",
    "authors": [
      "A. Flores",
      "R. C. de Lamare",
      "B. Clerckx"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.02776"
  },
  {
    "id": "arXiv:2106.02777",
    "title": "Immediate Proximity Detection Using Wi-Fi-Enabled Smartphones",
    "abstract": "Smartphone apps for exposure notification and contact tracing have been shown\nto be effective in controlling the COVID-19 pandemic. However, Bluetooth Low\nEnergy tokens similar to those broadcast by existing apps can still be picked\nup far away from the transmitting device. In this paper, we present a new class\nof methods for detecting whether or not two Wi-Fi-enabled devices are in\nimmediate physical proximity, i.e. 2 or fewer meters apart, as established by\nthe U.S. Centers for Disease Control and Prevention (CDC). Our goal is to\nenhance the accuracy of smartphone-based exposure notification and contact\ntracing systems. We present a set of binary machine learning classifiers that\ntake as input pairs of Wi-Fi RSSI fingerprints. We empirically verify that a\nsingle classifier cannot generalize well to a range of different environments\nwith vastly different numbers of detectable Wi-Fi Access Points (APs). However,\nspecialized classifiers, tailored to situations where the number of detectable\nAPs falls within a certain range, are able to detect immediate physical\nproximity significantly more accurately. As such, we design three classifiers\nfor situations with low, medium, and high numbers of detectable APs. These\nclassifiers distinguish between pairs of RSSI fingerprints recorded 2 or fewer\nmeters apart and pairs recorded further apart but still in Bluetooth range. We\ncharacterize their balanced accuracy for this task to be between 66.8% and\n77.8%.",
    "descriptor": "\nComments: 12 pages, 1 figure\n",
    "authors": [
      "Zach Van Hyfte",
      "Avideh Zakhor"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02777"
  },
  {
    "id": "arXiv:2106.02778",
    "title": "Radar-Camera Pixel Depth Association for Depth Completion",
    "abstract": "While radar and video data can be readily fused at the detection level,\nfusing them at the pixel level is potentially more beneficial. This is also\nmore challenging in part due to the sparsity of radar, but also because\nautomotive radar beams are much wider than a typical pixel combined with a\nlarge baseline between camera and radar, which results in poor association\nbetween radar pixels and color pixel. A consequence is that depth completion\nmethods designed for LiDAR and video fare poorly for radar and video. Here we\npropose a radar-to-pixel association stage which learns a mapping from radar\nreturns to pixels. This mapping also serves to densify radar returns. Using\nthis as a first stage, followed by a more traditional depth completion method,\nwe are able to achieve image-guided depth completion with radar and video. We\ndemonstrate performance superior to camera and radar alone on the nuScenes\ndataset. Our source code is available at https://github.com/longyunf/rc-pda.",
    "descriptor": "\nComments: IEEE Conference on Computer Vision and Pattern Recognition, 2021\n",
    "authors": [
      "Yunfei Long",
      "Daniel Morris",
      "Xiaoming Liu",
      "Marcos Castro",
      "Punarjay Chakravarty",
      "Praveen Narayanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02778"
  },
  {
    "id": "arXiv:2106.02779",
    "title": "PEEL: A Provable Removal Attack on Deep Hiding",
    "abstract": "Deep hiding, embedding images into another using deep neural networks, has\nshown its great power in increasing the message capacity and robustness. In\nthis paper, we conduct an in-depth study of state-of-the-art deep hiding\nschemes and analyze their hidden vulnerabilities. Then, according to our\nobservations and analysis, we propose a novel ProvablE rEmovaL attack (PEEL)\nusing image inpainting to remove secret images from containers without any\nprior knowledge about the deep hiding scheme. We also propose a systemic\nmethodology to improve the efficiency and image quality of PEEL by carefully\ndesigning a removal strategy and fully utilizing the visual information of\ncontainers. Extensive evaluations show our attacks can completely remove secret\nimages and has negligible impact on the quality of containers.",
    "descriptor": "",
    "authors": [
      "Tao Xiang",
      "Hangcheng Liu",
      "Shangwei Guo",
      "Tianwei Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02779"
  },
  {
    "id": "arXiv:2106.02781",
    "title": "IPS300+: a Challenging Multimodal Dataset for Intersection Perception  System",
    "abstract": "Due to the high complexity and occlusion, insufficient perception in the\ncrowded urban intersection can be a serious safety risk for both human drivers\nand autonomous algorithms, whereas CVIS (Cooperative Vehicle Infrastructure\nSystem) is a proposed solution for full-participants perception in this\nscenario. However, the research on roadside multimodal perception is still in\nits infancy, and there is no open-source dataset for such scenario.\nAccordingly, this paper fills the gap. Through an IPS (Intersection Perception\nSystem) installed at the diagonal of the intersection, this paper proposes a\nhigh-quality multimodal dataset for the intersection perception task. The\ncenter of the experimental intersection covers an area of 3000m2, and the\nextended distance reaches 300m, which is typical for CVIS. The first batch of\nopen-source data includes 14198 frames, and each frame has an average of 319.84\nlabels, which is 9.6 times larger than the most crowded dataset (H3D dataset in\n2019) by now. In order to facilitate further study, this dataset tries to keep\nthe label documents consistent with the KITTI dataset, and a standardized\nbenchmark is created for algorithm evaluation. Our dataset is available at:\nthis http URL",
    "descriptor": "",
    "authors": [
      "Huanan Wang",
      "Xinyu Zhang",
      "Jun Li",
      "Zhiwei Li",
      "Lei Yang",
      "Shuyue Pan",
      "Yongqiang Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02781"
  },
  {
    "id": "arXiv:2106.02782",
    "title": "On Perceptual Lossy Compression: The Cost of Perceptual Reconstruction  and An Optimal Training Framework",
    "abstract": "Lossy compression algorithms are typically designed to achieve the lowest\npossible distortion at a given bit rate. However, recent studies show that\npursuing high perceptual quality would lead to increase of the lowest\nachievable distortion (e.g., MSE). This paper provides nontrivial results\ntheoretically revealing that, \\textit{1}) the cost of achieving perfect\nperception quality is exactly a doubling of the lowest achievable MSE\ndistortion, \\textit{2}) an optimal encoder for the \"classic\" rate-distortion\nproblem is also optimal for the perceptual compression problem, \\textit{3})\ndistortion loss is unnecessary for training a perceptual decoder. Further, we\npropose a novel training framework to achieve the lowest MSE distortion under\nperfect perception constraint at a given bit rate. This framework uses a GAN\nwith discriminator conditioned on an MSE-optimized encoder, which is superior\nover the traditional framework using distortion plus adversarial loss.\nExperiments are provided to verify the theoretical finding and demonstrate the\nsuperiority of the proposed training framework.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Zeyu Yan",
      "Fei Wen",
      "Rendong Ying",
      "Chao Ma",
      "Peilin Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02782"
  },
  {
    "id": "arXiv:2106.02787",
    "title": "BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue  Modeling",
    "abstract": "Task-oriented dialogue (ToD) benchmarks provide an important avenue to\nmeasure progress and develop better conversational agents. However, existing\ndatasets for end-to-end ToD modeling are limited to a single language,\nhindering the development of robust end-to-end ToD systems for multilingual\ncountries and regions. Here we introduce BiToD, the first bilingual\nmulti-domain dataset for end-to-end task-oriented dialogue modeling. BiToD\ncontains over 7k multi-domain dialogues (144k utterances) with a large and\nrealistic bilingual knowledge base. It serves as an effective benchmark for\nevaluating bilingual ToD systems and cross-lingual transfer learning\napproaches. We provide state-of-the-art baselines under three evaluation\nsettings (monolingual, bilingual, and cross-lingual). The analysis of our\nbaselines in different settings highlights 1) the effectiveness of training a\nbilingual ToD system compared to two independent monolingual ToD systems, and\n2) the potential of leveraging a bilingual knowledge base and cross-lingual\ntransfer learning to improve the system performance under low resource\ncondition.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Genta Indra Winata",
      "Peng Xu",
      "Feijun Jiang",
      "Yuxiang Hu",
      "Chen Shi",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02787"
  },
  {
    "id": "arXiv:2106.02791",
    "title": "Motion Planning Transformers: One Model to Plan Them All",
    "abstract": "Transformers have become the powerhouse of natural language processing and\nrecently found use in computer vision tasks. Their effective use of attention\ncan be used in other contexts as well, and in this paper, we propose a\ntransformer-based approach for efficiently solving the complex motion planning\nproblems. Traditional neural network-based motion planning uses convolutional\nnetworks to encode the planning space, but these methods are limited to fixed\nmap sizes, which is often not realistic in the real-world. Our approach first\nidentifies regions on the map using transformers to provide attention to map\nareas likely to include the best path, and then applies local planners to\ngenerate the final collision-free path. We validate our method on a variety of\nrandomly generated environments with different map sizes, demonstrating\nreduction in planning complexity and achieving comparable accuracy to\ntraditional planners.",
    "descriptor": "",
    "authors": [
      "Jacob J. Johnson",
      "Linjun Li",
      "Ahmed H. Qureshi",
      "Michael C. Yip"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02791"
  },
  {
    "id": "arXiv:2106.02792",
    "title": "Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related  Domains",
    "abstract": "Social media has become a valuable resource for the study of suicidal\nideation and the assessment of suicide risk. Among social media platforms,\nReddit has emerged as the most promising one due to its anonymity and its focus\non topic-based communities (subreddits) that can be indicative of someone's\nstate of mind or interest regarding mental health disorders such as\nr/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on\nsuicide risk assessment has been the small amount of labeled data. We propose\nan empirical investigation into several classes of weakly-supervised\napproaches, and show that using pseudo-labeling based on related issues around\nmental health (e.g., anxiety, depression) helps improve model performance for\nsuicide risk assessment.",
    "descriptor": "\nComments: ACL 2021 short paper. Code is available at this https URL (under construction)\n",
    "authors": [
      "Chenghao Yang",
      "Yudong Zhang",
      "Smaranda Muresan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02792"
  },
  {
    "id": "arXiv:2106.02793",
    "title": "Solving hybrid machine learning tasks by traversing weight space  geodesics",
    "abstract": "Machine learning problems have an intrinsic geometric structure as central\nobjects including a neural network's weight space and the loss function\nassociated with a particular task can be viewed as encoding the intrinsic\ngeometry of a given machine learning problem. Therefore, geometric concepts can\nbe applied to analyze and understand theoretical properties of machine learning\nstrategies as well as to develop new algorithms. In this paper, we address\nthree seemingly unrelated open questions in machine learning by viewing them\nthrough a unified framework grounded in differential geometry. Specifically, we\nview the weight space of a neural network as a manifold endowed with a\nRiemannian metric that encodes performance on specific tasks. By defining a\nmetric, we can construct geodesic, minimum length, paths in weight space that\nrepresent sets of networks of equivalent or near equivalent functional\nperformance on a specific task. We, then, traverse geodesic paths while\nidentifying networks that satisfy a second objective. Inspired by the geometric\ninsight, we apply our geodesic framework to 3 major applications: (i) Network\nsparsification (ii) Mitigating catastrophic forgetting by constructing networks\nwith high performance on a series of objectives and (iii) Finding high-accuracy\npaths connecting distinct local optima of deep networks in the non-convex loss\nlandscape. Our results are obtained on a wide range of network architectures\n(MLP, VGG11/16) trained on MNIST, CIFAR-10/100. Broadly, we introduce a\ngeometric framework that unifies a range of machine learning objectives and\nthat can be applied to multiple classes of neural network architectures.",
    "descriptor": "\nComments: 11 pages, 7 figures\n",
    "authors": [
      "Guruprasad Raghavan",
      "Matt Thomson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02793"
  },
  {
    "id": "arXiv:2106.02794",
    "title": "MPC-based Realtime Power System Control with DNN-based  Prediction/Sensitivity-Estimation",
    "abstract": "This paper presents a model predictive control (MPC)-based online real-time\nadaptive control scheme for emergency voltage control in power systems. Despite\ntremendous success in various applications, real-time implementation of MPC for\ncontrol in power systems has not been successful due to its online\ncomputational burden for large-sized systems that takes more time than\navailable between the two control decisions. This long-standing problem is\naddressed here by developing a novel MPC-based adaptive control framework which\n(i) adapts the nominal offline computed control, by successive control\ncorrections, at each control decision point using the latest measurements, (ii)\nutilizes data-driven approach for prediction of voltage trajectory and its\nsensitivity with respect to control using trained deep neural networks (DNNs).\nIn addition, a realistic coordination scheme among control inputs of static var\ncompensators (SVC), load-shedding (LS), and load tap-changers (LTC) is\npresented with a goal of maintaining bus voltages within a predefined\npermissible range, where the delayed effect of LTC action is also incorporated\nin a novel way. The performance of the proposed scheme is validated for IEEE\n9-bus as well as 39-bus systems, with $\\pm 20\\%$ variations in nominal loading\nconditions. We also show that the proposed new scheme speeds up the online\ncomputation by a factor of 20 bringing it down to under one-tenth the control\ninterval, making the MPC-based power system control practically feasible.",
    "descriptor": "",
    "authors": [
      "Ramij Raja Hossain",
      "Ratnesh Kumar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02794"
  },
  {
    "id": "arXiv:2106.02795",
    "title": "Learnable Fourier Features for Multi-DimensionalSpatial Positional  Encoding",
    "abstract": "Attentional mechanisms are order-invariant. Positional encoding is a crucial\ncomponent to allow attention-based deep model architectures such as Transformer\nto address sequences or images where the position of information matters. In\nthis paper, we propose a novel positional encoding method based on learnable\nFourier features. Instead of hard-coding each position as a token or a vector,\nwe represent each position, which can be multi-dimensional, as a trainable\nencoding based on learnable Fourier feature mapping, modulated with a\nmulti-layer perceptron. The representation is particularly advantageous for a\nspatial multi-dimensional position, e.g., pixel positions on an image, where\n$L_2$ distances or more complex positional relationships need to be captured.\nOur experiments based on several public benchmark tasks show that our learnable\nFourier feature representation for multi-dimensional positional encoding\noutperforms existing methods by both improving the accuracy and allowing faster\nconvergence.",
    "descriptor": "",
    "authors": [
      "Yang Li",
      "Si Si",
      "Gang Li",
      "Cho-Jui Hsieh",
      "Samy Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02795"
  },
  {
    "id": "arXiv:2106.02796",
    "title": "Principle Bit Analysis: Autoencoding with Schur-Concave Loss",
    "abstract": "We consider a linear autoencoder in which the latent variables are quantized,\nor corrupted by noise, and the constraint is Schur-concave in the set of latent\nvariances. Although finding the optimal encoder/decoder pair for this setup is\na nonconvex optimization problem, we show that decomposing the source into its\nprincipal components is optimal. If the constraint is strictly Schur-concave\nand the empirical covariance matrix has only simple eigenvalues, then any\noptimal encoder/decoder must decompose the source in this way. As one\napplication, we consider a strictly Schur-concave constraint that estimates the\nnumber of bits needed to represent the latent variables under fixed-rate\nencoding, a setup that we call \\emph{Principal Bit Analysis (PBA)}. This yields\na practical, general-purpose, fixed-rate compressor that outperforms existing\nalgorithms. As a second application, we show that a prototypical\nautoencoder-based variable-rate compressor is guaranteed to decompose the\nsource into its principal components.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Sourbh Bhadane",
      "Aaron B. Wagner",
      "Jayadev Acharya"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02796"
  },
  {
    "id": "arXiv:2106.02797",
    "title": "Neural Distributed Source Coding",
    "abstract": "Distributed source coding is the task of encoding an input in the absence of\ncorrelated side information that is only available to the decoder. Remarkably,\nSlepian and Wolf showed in 1973 that an encoder that has no access to the\ncorrelated side information can asymptotically achieve the same compression\nrate as when the side information is available at both the encoder and the\ndecoder. While there is significant prior work on this topic in information\ntheory, practical distributed source coding has been limited to synthetic\ndatasets and specific correlation structures. Here we present a general\nframework for lossy distributed source coding that is agnostic to the\ncorrelation structure and can scale to high dimensions. Rather than relying on\nhand-crafted source-modeling, our method utilizes a powerful conditional deep\ngenerative model to learn the distributed encoder and decoder. We evaluate our\nmethod on realistic high-dimensional datasets and show substantial improvements\nin distributed compression performance.",
    "descriptor": "",
    "authors": [
      "Jay Whang",
      "Anish Acharya",
      "Hyeji Kim",
      "Alexandros G. Dimakis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02797"
  },
  {
    "id": "arXiv:2106.02801",
    "title": "Trajectory Optimization of Chance-Constrained Nonlinear Stochastic  Systems for Motion Planning and Control",
    "abstract": "We present gPC-SCP: Generalized Polynomial Chaos-based Sequential Convex\nProgramming method to compute a sub-optimal solution for a continuous-time\nchance-constrained stochastic nonlinear optimal control problem (SNOC) problem.\nThe approach enables motion planning and control of robotic systems under\nuncertainty. The proposed method involves two steps. The first step is to\nderive a deterministic nonlinear optimal control problem (DNOC) with convex\nconstraints that are surrogate to the SNOC by using gPC expansion and the\ndistributionally-robust convex subset of the chance constraints. The second\nstep is to solve the DNOC problem using sequential convex programming (SCP) for\ntrajectory generation and control. We prove that in the unconstrained case, the\noptimal value of the DNOC converges to that of SNOC asymptotically and that any\nfeasible solution of the constrained DNOC is a feasible solution of the\nchance-constrained SNOC. We derive a stable stochastic model predictive\ncontroller using the gPC-SCP for tracking a trajectory in the presence of\nuncertainty. We empirically demonstrate the efficacy of the gPC-SCP method for\nthe following three test cases: 1) collision checking under uncertainty in\nactuation, 2) collision checking with stochastic obstacle model, and 3) safe\ntrajectory tracking under uncertainty in the dynamics and obstacle location by\nusing a receding horizon control approach. We validate the effectiveness of the\ngPC-SCP method on the robotic spacecraft testbed.",
    "descriptor": "\nComments: submitted to IEEE Transactions on Robotics\n",
    "authors": [
      "Yashwanth Kumar Nakka",
      "Soon-Jo Chung"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02801"
  },
  {
    "id": "arXiv:2106.02802",
    "title": "Experimental and computational comparison of freeze-thaw induced  pressure generation in red and sugar maple",
    "abstract": "Sap exudation is the process whereby trees such as sugar (Acer saccharum) and\nred maple (A. rubrum) generate high positive stem pressure in response to\nrepeated freeze-thaw cycles. This elevated xylem pressure permits sap to be\nharvested over a period of several weeks and hence is a major factor in the\nviability of the maple syrup industry. The extensive literature on sap\nexudation documents various competing hypotheses regarding the physical and\nbiological mechanisms driving positive pressure generation in maple, but to\ndate relatively little effort has been expended on devising detailed\nmathematical models for the exudation process. In this paper, we utilize an\nexisting model of Graf et al. [J. Roy. Soc. Interface 12:20150665, 2015] that\ndescribes heat and mass transport within the multiphase gas-liquid-ice mixture\nwithin the porous xylem tissue. The model captures the inherent multiscale\nnature of xylem transport by including phase change and osmotic transport\nwithin wood cells on the microscale, which is coupled to heat transport through\nthe tree stem on the macroscale. We extend this model by incorporating a root\nreflection coefficient that introduces an asymmetry in root water flux and\nhence permits a more realistic accumulation of stem pressure. A parametric\nstudy based on simulations with synthetic temperature data singles out the\nessential model parameters that have greatest impact on stem pressure build-up.\nMeasured daily temperature fluctuations are then used as model inputs and the\nresulting simulated pressures are compared directly with experimental\nmeasurements taken from mature red and sugar maple stems during the sap harvest\nseason. The results demonstrate that our multiscale freeze-thaw model\nreproduces realistic exudation behavior, thereby providing novel insights into\nthe specific physical mechanisms that dominate positive pressure generation in\nmaple trees.",
    "descriptor": "\nComments: 7 figures\n",
    "authors": [
      "Maryam Zarrinderakht",
      "Isabell Konrad",
      "Timothy R. Wilmot",
      "Timothy D. Perkins",
      "Abby van den Berg",
      "John M. Stockie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2106.02802"
  },
  {
    "id": "arXiv:2106.02804",
    "title": "Points2Polygons: Context-Based Segmentation from Weak Labels Using  Adversarial Networks",
    "abstract": "In applied image segmentation tasks, the ability to provide numerous and\nprecise labels for training is paramount to the accuracy of the model at\ninference time. However, this overhead is often neglected, and recently\nproposed segmentation architectures rely heavily on the availability and\nfidelity of ground truth labels to achieve state-of-the-art accuracies. Failure\nto acknowledge the difficulty in creating adequate ground truths can lead to an\nover-reliance on pre-trained models or a lack of adoption in real-world\napplications. We introduce Points2Polygons (P2P), a model which makes use of\ncontextual metric learning techniques that directly addresses this problem.\nPoints2Polygons performs well against existing fully-supervised segmentation\nbaselines with limited training data, despite using lightweight segmentation\nmodels (U-Net with a ResNet18 backbone) and having access to only weak labels\nin the form of object centroids and no pre-training. We demonstrate this on\nseveral different small but non-trivial datasets. We show that metric learning\nusing contextual data provides key insights for self-supervised tasks in\ngeneral, and allow segmentation models to easily generalize across\ntraditionally label-intensive domains in computer vision.",
    "descriptor": "\nComments: Submitted to NeurIPS 2021\n",
    "authors": [
      "Kuai Yu",
      "Hakeem Frank",
      "Daniel Wilson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02804"
  },
  {
    "id": "arXiv:2106.02807",
    "title": "The Four Levels of Fixed-Points in Mean-Field Models",
    "abstract": "The fixed-point analysis refers to the study of fixed-points that arise in\nthe context of complex systems with many interacting entities. In this\nexpository paper, we describe four levels of fixed-points in mean-field\ninteracting particle systems. These four levels are (i) the macroscopic\nobservables of the system, (ii) the probability distribution over states of a\nparticle at equilibrium, (iii) the time evolution of the probability\ndistribution over states of a particle, and (iv) the probability distribution\nover trajectories. We then discuss relationships among the fixed-points at\nthese four levels. Finally, we describe some issues that arise in the\nfixed-point analysis when the system possesses multiple fixed-points at the\nlevel of distribution over states, and how one goes beyond the fixed-point\nanalysis to tackle such issues.",
    "descriptor": "\nComments: Accepted for publication at the Twenty Seventh National Conference on Communications (NCC 2021)\n",
    "authors": [
      "Sarath Yasodharan",
      "Rajesh Sundaresan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.02807"
  },
  {
    "id": "arXiv:2106.02808",
    "title": "A Variational Perspective on Diffusion-Based Generative Models and Score  Matching",
    "abstract": "Discrete-time diffusion-based generative models and score matching methods\nhave shown promising results in modeling high-dimensional image data. Recently,\nSong et al. (2021) show that diffusion processes that transform data into noise\ncan be reversed via learning the score function, i.e. the gradient of the\nlog-density of the perturbed data. They propose to plug the learned score\nfunction into an inverse formula to define a generative diffusion process.\nDespite the empirical success, a theoretical underpinning of this procedure is\nstill lacking. In this work, we approach the (continuous-time) generative\ndiffusion directly and derive a variational framework for likelihood\nestimation, which includes continuous-time normalizing flows as a special case,\nand can be seen as an infinitely deep variational autoencoder. Under this\nframework, we show that minimizing the score-matching loss is equivalent to\nmaximizing a lower bound of the likelihood of the plug-in reverse SDE proposed\nby Song et al. (2021), bridging the theoretical gap.",
    "descriptor": "",
    "authors": [
      "Chin-Wei Huang",
      "Jae Hyun Lim",
      "Aaron Courville"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02808"
  },
  {
    "id": "arXiv:2106.02809",
    "title": "T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing",
    "abstract": "Hazy images reduce the visibility of the image content, and haze will lead to\nfailure in handling subsequent computer vision tasks. In this paper, we address\nthe problem of image dehazing by proposing a dehazing network named T-Net,\nwhich consists of a backbone network based on the U-Net architecture and a dual\nattention module. And it can achieve multi-scale feature fusion by using skip\nconnections with a new fusion strategy. Furthermore, by repeatedly unfolding\nthe plain T-Net, Stack T-Net is proposed to take advantage of the dependence of\ndeep features across stages via a recursive strategy. In order to reduce\nnetwork parameters, the intra-stage recursive computation of ResNet is adopted\nin our Stack T-Net. And we take both the stage-wise result and the original\nhazy image as input to each T-Net and finally output the prediction of clean\nimage. Experimental results on both synthetic and real-world images demonstrate\nthat our plain T-Net and the advanced Stack T-Net perform favorably against the\nstate-of-the-art dehazing algorithms, and show that our Stack T-Net could\nfurther improve the dehazing effect, demonstrating the effectiveness of the\nrecursive strategy.",
    "descriptor": "",
    "authors": [
      "Lirong Zheng",
      "Yanshan Li",
      "Kaihao Zhang",
      "Wenhan Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02809"
  },
  {
    "id": "arXiv:2106.02811",
    "title": "Full-Dimensional Rate Enhancement for UAV-Enabled Communications via  Intelligent Omni-Surface",
    "abstract": "This paper investigates the achievable rate maximization problem of a\ndownlink unmanned aerial vehicle (UAV)-enabled communication system aided by an\nintelligent omni-surface (IOS). Different from the state-of-the-art\nreconfigurable intelligent surface (RIS) that only reflects incident signals,\nthe IOS can simultaneously reflect and transmit the signals, thereby providing\nfull-dimensional rate enhancement. To tackle such a problem, we formulate it by\njointly optimizing the IOS's phase shift and the UAV trajectory. Although it is\ndifficult to solve it optimally due to its non-convexity, we propose an\nefficient iterative algorithm to obtain a high-quality suboptimal solution.\nSimulation results show that the IOS-assisted UAV communications can achieve\nmore significant improvement in achievable rates than other benchmark schemes.",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Bin Duo",
      "Yifan Liu",
      "Qingqing Wu",
      "Xiaojun Yuan",
      "Yonghui Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02811"
  },
  {
    "id": "arXiv:2106.02813",
    "title": "Web based disease prediction and recommender system",
    "abstract": "Worldwide, several cases go undiagnosed due to poor healthcare support in\nremote areas. In this context, a centralized system is needed for effective\nmonitoring and analysis of the medical records. A web-based patient diagnostic\nsystem is a central platform to store the medical history and predict the\npossible disease based on the current symptoms experienced by a patient to\nensure faster and accurate diagnosis. Early disease prediction can help the\nusers determine the severity of the disease and take quick action. The proposed\nweb-based disease prediction system utilizes machine learning based\nclassification techniques on a data set acquired from the National Centre of\nDisease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive\nbayes classification approaches are utilized and an ensemble voting algorithm\nis also proposed where each classifier is assigned weights dynamically based on\nthe prediction confidence. The proposed system is also equipped with a\nrecommendation scheme to recommend the type of tests based on the existing\nsymptoms of the patient, so that necessary precautions can be taken. A\ncentralized database ensures that the medical data is preserved and there is\ntransparency in the system. The tampering into the system is prevented by\ngiving the no \"updation\" rights once the diagnosis is created.",
    "descriptor": "",
    "authors": [
      "Harish Rajora",
      "Narinder Singh Punn",
      "Sanjay Kumar Sonbhadra",
      "Sonali Agarwal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02813"
  },
  {
    "id": "arXiv:2106.02817",
    "title": "ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph  Networks",
    "abstract": "Imbalanced classification on graphs is ubiquitous yet challenging in many\nreal-world applications, such as fraudulent node detection. Recently, graph\nneural networks (GNNs) have shown promising performance on many network\nanalysis tasks. However, most existing GNNs have almost exclusively focused on\nthe balanced networks, and would get unappealing performance on the imbalanced\nnetworks. To bridge this gap, in this paper, we present a generative\nadversarial graph network model, called ImGAGN to address the imbalanced\nclassification problem on graphs. It introduces a novel generator for graph\nstructure data, named GraphGenerator, which can simulate both the minority\nclass nodes' attribute distribution and network topological structure\ndistribution by generating a set of synthetic minority nodes such that the\nnumber of nodes in different classes can be balanced. Then a graph\nconvolutional network (GCN) discriminator is trained to discriminate between\nreal nodes and fake (i.e., generated) nodes, and also between minority nodes\nand majority nodes on the synthetic balanced network. To validate the\neffectiveness of the proposed method, extensive experiments are conducted on\nfour real-world imbalanced network datasets. Experimental results demonstrate\nthat the proposed method ImGAGN outperforms state-of-the-art algorithms for\nsemi-supervised imbalanced node classification task.",
    "descriptor": "\nComments: to be published in KDD'2021\n",
    "authors": [
      "Liang Qu",
      "Huaisheng Zhu",
      "Ruiqi Zheng",
      "Yuhui Shi",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02817"
  },
  {
    "id": "arXiv:2106.02818",
    "title": "Variational Leakage: The Role of Information Complexity in Privacy  Leakage",
    "abstract": "We study the role of information complexity in privacy leakage about an\nattribute of an adversary's interest, which is not known a priori to the system\ndesigner. Considering the supervised representation learning setup and using\nneural networks to parameterize the variational bounds of information\nquantities, we study the impact of the following factors on the amount of\ninformation leakage: information complexity regularizer weight, latent space\ndimension, the cardinalities of the known utility and unknown sensitive\nattribute sets, the correlation between utility and sensitive attributes, and a\npotential bias in a sensitive attribute of adversary's interest. We conduct\nextensive experiments on Colored-MNIST and CelebA datasets to evaluate the\neffect of information complexity on the amount of intrinsic leakage.",
    "descriptor": "",
    "authors": [
      "Amir Ahooye Atashin",
      "Behrooz Razeghi",
      "Deniz G\u00fcnd\u00fcz",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02818"
  },
  {
    "id": "arXiv:2106.02820",
    "title": "GraphMI: Extracting Private Graph Data from Graph Neural Networks",
    "abstract": "As machine learning becomes more widely used for critical applications, the\nneed to study its implications in privacy turns to be urgent. Given access to\nthe target model and auxiliary information, the model inversion attack aims to\ninfer sensitive features of the training dataset, which leads to great privacy\nconcerns. Despite its success in grid-like domains, directly applying model\ninversion techniques on non-grid domains such as graph achieves poor attack\nperformance due to the difficulty to fully exploit the intrinsic properties of\ngraphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge\nthis gap, we present \\textbf{Graph} \\textbf{M}odel \\textbf{I}nversion attack\n(GraphMI), which aims to extract private graph data of the training graph by\ninverting GNN, one of the state-of-the-art graph analysis tools. Specifically,\nwe firstly propose a projected gradient module to tackle the discreteness of\ngraph edges while preserving the sparsity and smoothness of graph features.\nThen we design a graph auto-encoder module to efficiently exploit graph\ntopology, node attributes, and target model parameters for edge inference. With\nthe proposed methods, we study the connection between model inversion risk and\nedge influence and show that edges with greater influence are more likely to be\nrecovered. Extensive experiments over several public datasets demonstrate the\neffectiveness of our method. We also show that differential privacy in its\ncanonical form can hardly defend our attack while preserving decent utility.",
    "descriptor": "\nComments: 7 pages, 6 figures, accepted by IJCAI'21\n",
    "authors": [
      "Zaixi Zhang",
      "Qi Liu",
      "Zhenya Huang",
      "Hao Wang",
      "Chengqiang Lu",
      "Chuanren Liu",
      "Enhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02820"
  },
  {
    "id": "arXiv:2106.02821",
    "title": "Lifelong Learning of Hate Speech Classification on Social Media",
    "abstract": "Existing work on automated hate speech classification assumes that the\ndataset is fixed and the classes are pre-defined. However, the amount of data\nin social media increases every day, and the hot topics changes rapidly,\nrequiring the classifiers to be able to continuously adapt to new data without\nforgetting the previously learned knowledge. This ability, referred to as\nlifelong learning, is crucial for the real-word application of hate speech\nclassifiers in social media. In this work, we propose lifelong learning of hate\nspeech classification on social media. To alleviate catastrophic forgetting, we\npropose to use Variational Representation Learning (VRL) along with a memory\nmodule based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural\nNetwork). Experimentally, we show that combining variational representation\nlearning and the LB-SOINN memory module achieves better performance than the\ncommonly-used lifelong learning techniques.",
    "descriptor": "\nComments: NAACL 2021\n",
    "authors": [
      "Jing Qian",
      "Hong Wang",
      "Mai ElSherief",
      "Xifeng Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02821"
  },
  {
    "id": "arXiv:2106.02822",
    "title": "$\\mathcal{H}_2/\\mathcal{H}_{-}$ Distributed Fault Detection and  Isolation for Heterogeneous Multi-Agent Systems",
    "abstract": "The paper deals with the problem of distributed fault detection and isolation\n(FDI) for a group of heterogeneous multi-agent systems. The developed formation\nfor the FDI is taken into account as a distributed observer design methodology,\nwhere the interaction between the agent and its neighbors is described as a\nvector of distributed relative output measurements. Based on two performance\nindexes $\\mathcal{H}_2$ and $\\mathcal{H}_{-}$, sufficient conditions are given\nto ensure the residual signals robust to the disturbances and sensitive with\nrespect to the fault signals. In addition, we show that by using our proposed\napproach, each agent is able to estimate both its own states and states of its\nnearest neighbors in the presence of disturbances and faults. Finally,\nnumerical simulations are provided to demonstrate the effectiveness of the\ntheoretically analyzed results.",
    "descriptor": "",
    "authors": [
      "Thiem V. Pham",
      "Quynh T. T. Nguyen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02822"
  },
  {
    "id": "arXiv:2106.02824",
    "title": "Making CNNs Interpretable by Building Dynamic Sequential Decision  Forests with Top-down Hierarchy Learning",
    "abstract": "In this paper, we propose a generic model transfer scheme to make\nConvlutional Neural Networks (CNNs) interpretable, while maintaining their high\nclassification accuracy. We achieve this by building a differentiable decision\nforest on top of CNNs, which enjoys two characteristics: 1) During training,\nthe tree hierarchies of the forest are learned in a top-down manner under the\nguidance from the category semantics embedded in the pre-trained CNN weights;\n2) During inference, a single decision tree is dynamically selected from the\nforest for each input sample, enabling the transferred model to make sequential\ndecisions corresponding to the attributes shared by semantically-similar\ncategories, rather than directly performing flat classification. We name the\ntransferred model deep Dynamic Sequential Decision Forest (dDSDF). Experimental\nresults show that dDSDF not only achieves higher classification accuracy than\nits conuterpart, i.e., the original CNN, but has much better interpretability,\nas qualitatively it has plausible hierarchies and quantitatively it leads to\nmore precise saliency maps.",
    "descriptor": "",
    "authors": [
      "Yilin Wang",
      "Shaozuo Yu",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02824"
  },
  {
    "id": "arXiv:2106.02826",
    "title": "Dynamic Resource Configuration for Low-Power IoT Networks: A  Multi-Objective Reinforcement Learning Method",
    "abstract": "Considering grant-free transmissions in low-power IoT networks with unknown\ntime-frequency distribution of interference, we address the problem of Dynamic\nResource Configuration (DRC), which amounts to a Markov decision process.\nUnfortunately, off-the-shelf methods based on single-objective reinforcement\nlearning cannot guarantee energy-efficient transmission, especially when all\nfrequency-domain channels in a time interval are interfered. Therefore, we\npropose a novel DRC scheme where configuration policies are optimized with a\nMulti-Objective Reinforcement Learning (MORL) framework. Numerical results show\nthat the average decision error rate achieved by the MORL-based DRC can be even\nless than 12% of that yielded by the conventional R-learning-based approach.",
    "descriptor": "\nComments: Accepted to IEEE Communications Letters\n",
    "authors": [
      "Yang Huang",
      "Caiyong Hao",
      "Yijie Mao",
      "Fuhui Zhou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.02826"
  },
  {
    "id": "arXiv:2106.02829",
    "title": "A Split-face Study of Novel Robotic Prototype vs Human Operator in Skin  Rejuvenation Using Q-switched Nd:Yag Laser: Accuracy, Efficacy and Safety",
    "abstract": "Background: Robotic technologies involved in skin laser are emerging.\nObjective: To compare the accuracy, efficacy and safety of novel robotic\nprototype with human operator in laser operation performance for skin\nphoto-rejuvenation. Methods: Seventeen subjects were enrolled in a prospective,\ncomparative split-face trial. Q-switch 1064nm laser conducted by the robotic\nprototype was provided on the right side of the face and that by the\nprofessional practitioner on the left. Each subject underwent a single time,\none-pass, non-overlapped treatment on an equal size area of the forehead and\ncheek. Objective assessments included: treatment duration, laser irradiation\nshots, laser coverage percentage, VISIA parameters, skin temperature and the\nVAS pain scale. Results: Average time taken by robotic manipulator was longer\nthan human operator; the average number of irradiation shots of both sides had\nno significant differences. Laser coverage rate of robotic manipulator (60.2\n+-15.1%) was greater than that of human operator (43.6 +-12.9%). The VISIA\nparameters showed no significant differences between robotic manipulator and\nhuman operator. No short or long-term side effects were observed with maximum\nVAS score of 1 point. Limitations: Only one section of laser treatment was\nperformed. Conclusion: Laser operation by novel robotic prototype is more\nreliable, stable and accurate than human operation.",
    "descriptor": "",
    "authors": [
      "Si Un Chan",
      "Cheong Cheong Ip",
      "Chengxiang Lian",
      "Muhammad Muddassir",
      "Domingo Gomez Dominguez",
      "Wai Kit Ming",
      "Jianhao Du",
      "Yue Zheng",
      "David Navarro-Alarcon",
      "Lie Hua Deng"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02829"
  },
  {
    "id": "arXiv:2106.02831",
    "title": "A novel method for recommendation systems using invasive weed  optimization",
    "abstract": "One of the popular approaches in recommendation systems is Collaborative\nFiltering (CF). The most significant step in CF is choosing the appropriate set\nof users. For this purpose, similarity measures are usually used for computing\nthe similarity between a specific user and the other users. This paper proposes\na new invasive weed optimization (IWO) based CF approach that uses users'\ncontext to identify important and effective users set. By using a newly defined\nsimilarity measure based on both rating values and a measure values called\nconfidence, the proposed approach calculates the similarity between users and\nthus identifies and filters the most similar users to a specific user. It then\nuses IWO to calculate the importance degree of users and finally, by using the\nidentified important users and their importance degrees it predicts unknown\nratings. To evaluate the proposed method, several experiments have been\nperformed on two known real world datasets and the results show that the\nproposed method improves the state of the art results up to 15% in terms of\nRoot Mean Square Error (RMSE) and Mean Absolute Error (MAE).",
    "descriptor": "",
    "authors": [
      "Fahimeh Soltaninejad",
      "Amir Jalaly Bidgoly"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.02831"
  },
  {
    "id": "arXiv:2106.02833",
    "title": "Improving Automated Evaluation of Open Domain Dialog via Diverse  Reference Augmentation",
    "abstract": "Multiple different responses are often plausible for a given open domain\ndialog context. Prior work has shown the importance of having multiple valid\nreference responses for meaningful and robust automated evaluations. In such\ncases, common practice has been to collect more human written references.\nHowever, such collection can be expensive, time consuming, and not easily\nscalable. Instead, we propose a novel technique for automatically expanding a\nhuman generated reference to a set of candidate references. We fetch plausible\nreferences from knowledge sources, and adapt them so that they are more fluent\nin context of the dialog instance in question. More specifically, we use (1) a\ncommonsense knowledge base to elicit a large number of plausible reactions\ngiven the dialog history (2) relevant instances retrieved from dialog corpus,\nusing similar past as well as future contexts. We demonstrate that our\nautomatically expanded reference sets lead to large improvements in\ncorrelations of automated metrics with human ratings of system outputs for\nDailyDialog dataset.",
    "descriptor": "\nComments: Findings of ACL 2021\n",
    "authors": [
      "Varun Gangal",
      "Harsh Jhamtani",
      "Eduard Hovy",
      "Taylor Berg-Kirkpatrick"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02833"
  },
  {
    "id": "arXiv:2106.02834",
    "title": "MergeDistill: Merging Pre-trained Language Models using Distillation",
    "abstract": "Pre-trained multilingual language models (LMs) have achieved state-of-the-art\nresults in cross-lingual transfer, but they often lead to an inequitable\nrepresentation of languages due to limited capacity, skewed pre-training data,\nand sub-optimal vocabularies. This has prompted the creation of an ever-growing\npre-trained model universe, where each model is trained on large amounts of\nlanguage or domain specific data with a carefully curated, linguistically\ninformed vocabulary. However, doing so brings us back full circle and prevents\none from leveraging the benefits of multilinguality. To address the gaps at\nboth ends of the spectrum, we propose MergeDistill, a framework to merge\npre-trained LMs in a way that can best leverage their assets with minimal\ndependencies, using task-agnostic knowledge distillation. We demonstrate the\napplicability of our framework in a practical setting by leveraging\npre-existing teacher LMs and training student LMs that perform competitively\nwith or even outperform teacher LMs trained on several orders of magnitude more\ndata and with a fixed model capacity. We also highlight the importance of\nteacher selection and its impact on student model performance.",
    "descriptor": "\nComments: ACL 2021 Findings\n",
    "authors": [
      "Simran Khanuja",
      "Melvin Johnson",
      "Partha Talukdar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02834"
  },
  {
    "id": "arXiv:2106.02835",
    "title": "On the Role of Entropy-based Loss for Learning Causal Structures with  Continuous Optimization",
    "abstract": "Causal discovery from observational data is an important but challenging task\nin many scientific fields. Recently, NOTEARS [Zheng et al., 2018] formulates\nthe causal structure learning problem as a continuous optimization problem\nusing least-square loss with an acyclicity constraint. Though the least-square\nloss function is well justified under the standard Gaussian noise assumption,\nit is limited if the assumption does not hold. In this work, we theoretically\nshow that the violation of the Gaussian noise assumption will hinder the causal\ndirection identification, making the causal orientation fully determined by the\ncausal strength as well as the variances of noises in the linear case and the\nnoises of strong non-Gaussianity in the nonlinear case. Consequently, we\npropose a more general entropy-based loss that is theoretically consistent with\nthe likelihood score under any noise distribution. We run extensive empirical\nevaluations on both synthetic data and real-world data to validate the\neffectiveness of the proposed method and show that our method achieves the best\nin Structure Hamming Distance, False Discovery Rate, and True Positive Rate\nmatrices.",
    "descriptor": "",
    "authors": [
      "Ruichu Cai",
      "Weilin Chen",
      "Jie Qiao",
      "Zhifeng Hao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02835"
  },
  {
    "id": "arXiv:2106.02836",
    "title": "Constrained Generalized Additive 2 Model with Consideration of  High-Order Interactions",
    "abstract": "In recent years, machine learning and AI have been introduced in many\nindustrial fields. In fields such as finance, medicine, and autonomous driving,\nwhere the inference results of a model may have serious consequences, high\ninterpretability as well as prediction accuracy is required. In this study, we\npropose CGA2M+, which is based on the Generalized Additive 2 Model (GA2M) and\ndiffers from it in two major ways. The first is the introduction of\nmonotonicity. Imposing monotonicity on some functions based on an analyst's\nknowledge is expected to improve not only interpretability but also\ngeneralization performance. The second is the introduction of a higher-order\nterm: given that GA2M considers only second-order interactions, we aim to\nbalance interpretability and prediction accuracy by introducing a higher-order\nterm that can capture higher-order interactions. In this way, we can improve\nprediction performance without compromising interpretability by applying\nlearning innovation. Numerical experiments showed that the proposed model has\nhigh predictive performance and interpretability. Furthermore, we confirmed\nthat generalization performance is improved by introducing monotonicity.",
    "descriptor": "",
    "authors": [
      "Akihisa Watanabe",
      "Michiya Kuramata",
      "Kaito Majima",
      "Haruka Kiyohara",
      "Kensho Kondo",
      "Kazuhide Nakata"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02836"
  },
  {
    "id": "arXiv:2106.02839",
    "title": "Upward planar drawings with two slopes",
    "abstract": "In an upward planar 2-slope drawing of a digraph, edges are drawn as\nstraight-line segments in the upward direction without crossings using only two\ndifferent slopes. We investigate whether a given upward planar digraph admits\nsuch a drawing and, if so, how to construct it. For the fixed embedding\nscenario, we give a simple characterisation and a linear-time construction by\nadopting algorithms from orthogonal drawings. For the variable embedding\nscenario, we describe a linear-time algorithm for single-source digraphs, a\nquartic-time algorithm for series-parallel digraphs, and a fixed-parameter\ntractable algorithm for general digraphs. For the latter two classes, we make\nuse of SPQR-trees and the notion of upward spirality. As an application of this\ndrawing style, we show how to draw an upward planar phylogenetic network with\ntwo slopes such that all leaves lie on a horizontal line.",
    "descriptor": "",
    "authors": [
      "Jonathan Klawitter",
      "Tamara Mchedlidze"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.02839"
  },
  {
    "id": "arXiv:2106.02842",
    "title": "Multi-Camera Vehicle Counting Using Edge-AI",
    "abstract": "This paper presents a novel solution to automatically count vehicles in a\nparking lot using images captured by smart cameras. Unlike most of the\nliterature on this task, which focuses on the analysis of single images, this\npaper proposes the use of multiple visual sources to monitor a wider parking\narea from different perspectives. The proposed multi-camera system is capable\nof automatically estimate the number of cars present in the entire parking lot\ndirectly on board the edge devices. It comprises an on-device deep\nlearning-based detector that locates and counts the vehicles from the captured\nimages and a decentralized geometric-based approach that can analyze the\ninter-camera shared areas and merge the data acquired by all the devices. We\nconduct the experimental evaluation on an extended version of the CNRPark-EXT\ndataset, a collection of images taken from the parking lot on the campus of the\nNational Research Council (CNR) in Pisa, Italy. We show that our system is\nrobust and takes advantage of the redundant information deriving from the\ndifferent cameras, improving the overall performance without requiring any\nextra geometrical information of the monitored scene.",
    "descriptor": "\nComments: Submitted to Expert Systems With Applications\n",
    "authors": [
      "Luca Ciampi",
      "Claudio Gennaro",
      "Fabio Carrara",
      "Fabrizio Falchi",
      "Claudio Vairo",
      "Giuseppe Amato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02842"
  },
  {
    "id": "arXiv:2106.02845",
    "title": "Semi-Supervised Domain Adaptation via Adaptive and Progressive Feature  Alignment",
    "abstract": "Contemporary domain adaptive semantic segmentation aims to address data\nannotation challenges by assuming that target domains are completely\nunannotated. However, annotating a few target samples is usually very\nmanageable and worthwhile especially if it improves the adaptation performance\nsubstantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive\nimage Segmentation network that employs a few labeled target samples as anchors\nfor adaptive and progressive feature alignment between labeled source samples\nand unlabeled target samples. We position the few labeled target samples as\nreferences that gauge the similarity between source and target features and\nguide adaptive inter-domain alignment for learning more similar source\nfeatures. In addition, we replace the dissimilar source features by\nhigh-confidence target features continuously during the iterative training\nprocess, which achieves progressive intra-domain alignment between confident\nand unconfident target features. Extensive experiments show the proposed SSDAS\ngreatly outperforms a number of baselines, i.e., UDA-based semantic\nsegmentation and SSDA-based image classification. In addition, SSDAS is\ncomplementary and can be easily incorporated into UDA-based methods with\nconsistent improvements in domain adaptive semantic segmentation.",
    "descriptor": "",
    "authors": [
      "Jiaxing Huang",
      "Dayan Guan",
      "Aoran Xiao",
      "Shijian Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02845"
  },
  {
    "id": "arXiv:2106.02848",
    "title": "Numerical Composition of Differential Privacy",
    "abstract": "We give a fast algorithm to optimally compose privacy guarantees of\ndifferentially private (DP) algorithms to arbitrary accuracy. Our method is\nbased on the notion of privacy loss random variables to quantify the privacy\nloss of DP algorithms. The running time and memory needed for our algorithm to\napproximate the privacy curve of a DP algorithm composed with itself $k$ times\nis $\\tilde{O}(\\sqrt{k})$. This improves over the best prior method by Koskela\net al. (2020) which requires $\\tilde{\\Omega}(k^{1.5})$ running time. We\ndemonstrate the utility of our algorithm by accurately computing the privacy\nloss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm\nspeeds up the privacy computations by a few orders of magnitude compared to\nprior work, while maintaining similar accuracy.",
    "descriptor": "",
    "authors": [
      "Sivakanth Gopi",
      "Yin Tat Lee",
      "Lukas Wutschitz"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02848"
  },
  {
    "id": "arXiv:2106.02850",
    "title": "Tetrad: Actively Secure 4PC for Secure Training and Inference",
    "abstract": "In this work, we design an efficient mixed-protocol framework, Tetrad, with\napplications to privacy-preserving machine learning. It is designed for the\nfour-party setting with at most one active corruption and supports rings.\nOur fair multiplication protocol requires communicating only 5 ring elements\nimproving over the state-of-the-art protocol of Trident (Chaudhari et al.\nNDSS'20). The technical highlights of Tetrad include efficient (a) truncation\nwithout any overhead, (b) multi-input multiplication protocols for arithmetic\nand boolean worlds, (c) garbled-world, tailor-made for the mixed-protocol\nframework, and (d) conversion mechanisms to switch between the computation\nstyles. The fair framework is also extended to provide robustness without\ninflating the costs.\nThe competence of Tetrad is tested with benchmarks for deep neural networks\nsuch as LeNet and VGG16 and support vector machines. One variant of our\nframework aims at minimizing the execution time, while the other focuses on the\nmonetary cost. We observe improvements up to 6x over Trident across these\nparameters.",
    "descriptor": "",
    "authors": [
      "Nishat Koti",
      "Arpita Patra",
      "Rahul Rachuri",
      "Ajith Suresh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02850"
  },
  {
    "id": "arXiv:2106.02851",
    "title": "SURPRISE! and When to Schedule It",
    "abstract": "Information flow measures, over the duration of a game, the audience's belief\nof who will win, and thus can reflect the amount of surprise in a game. To\nquantify the relationship between information flow and audiences' perceived\nquality, we conduct a case study where subjects watch one of the world's\nbiggest esports events, LOL S10. In addition to eliciting information flow, we\nalso ask subjects to report their rating for each game. We find that the amount\nof surprise in the end of the game plays a dominant role in predicting the\nrating. This suggests the importance of incorporating when the surprise occurs,\nin addition to the amount of surprise, in perceived quality models. For content\nproviders, it implies that everything else being equal, it is better for twists\nto be more likely to happen toward the end of a show rather than uniformly\nthroughout.",
    "descriptor": "",
    "authors": [
      "Zhihuan Huang",
      "Shengwei Xu",
      "You Shan",
      "Yuxuan Lu",
      "Yuqing Kong",
      "Tracy Xiao Liu",
      "Grant Schoenebeck"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.02851"
  },
  {
    "id": "arXiv:2106.02852",
    "title": "Patch Slimming for Efficient Vision Transformers",
    "abstract": "This paper studies the efficiency problem for visual transformers by\nexcavating redundant calculation in given networks. The recent transformer\narchitecture has demonstrated its effectiveness for achieving excellent\nperformance on a series of computer vision tasks. However, similar to that of\nconvolutional neural networks, the huge computational cost of vision\ntransformers is still a severe issue. Considering that the attention mechanism\naggregates different patches layer-by-layer, we present a novel patch slimming\napproach that discards useless patches in a top-down paradigm. We first\nidentify the effective patches in the last layer and then use them to guide the\npatch selection process of previous layers. For each layer, the impact of a\npatch on the final output feature is approximated and patches with less impact\nwill be removed. Experimental results on benchmark datasets demonstrate that\nthe proposed method can significantly reduce the computational costs of vision\ntransformers without affecting their performances. For example, over 45% FLOPs\nof the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the\nImageNet dataset.",
    "descriptor": "",
    "authors": [
      "Yehui Tang",
      "Kai Han",
      "Yunhe Wang",
      "Chang Xu",
      "Jianyuan Guo",
      "Chao Xu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02852"
  },
  {
    "id": "arXiv:2106.02853",
    "title": "Region-aware Adaptive Instance Normalization for Image Harmonization",
    "abstract": "Image composition plays a common but important role in photo editing. To\nacquire photo-realistic composite images, one must adjust the appearance and\nvisual style of the foreground to be compatible with the background. Existing\ndeep learning methods for harmonizing composite images directly learn an image\nmapping network from the composite to the real one, without explicit\nexploration on visual style consistency between the background and the\nforeground images. To ensure the visual style consistency between the\nforeground and the background, in this paper, we treat image harmonization as a\nstyle transfer problem. In particular, we propose a simple yet effective\nRegion-aware Adaptive Instance Normalization (RAIN) module, which explicitly\nformulates the visual style from the background and adaptively applies them to\nthe foreground. With our settings, our RAIN module can be used as a drop-in\nmodule for existing image harmonization networks and is able to bring\nsignificant improvements. Extensive experiments on the existing image\nharmonization benchmark datasets show the superior capability of the proposed\nmethod. Code is available at {https://github.com/junleen/RainNet}.",
    "descriptor": "\nComments: Accepted to IEEE CVPR 2021\n",
    "authors": [
      "Jun Ling",
      "Han Xue",
      "Li Song",
      "Rong Xie",
      "Xiao Gu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02853"
  },
  {
    "id": "arXiv:2106.02855",
    "title": "Multi-armed Bandit Algorithms on System-on-Chip: Go Frequentist or  Bayesian?",
    "abstract": "Multi-armed Bandit (MAB) algorithms identify the best arm among multiple arms\nvia exploration-exploitation trade-off without prior knowledge of arm\nstatistics. Their usefulness in wireless radio, IoT, and robotics demand\ndeployment on edge devices, and hence, a mapping on system-on-chip (SoC) is\ndesired. Theoretically, the Bayesian approach-based Thompson Sampling (TS)\nalgorithm offers better performance than the frequentist approach-based Upper\nConfidence Bound (UCB) algorithm. However, TS is not synthesizable due to Beta\nfunction. We address this problem by approximating it via a pseudo-random\nnumber generator-based approach and efficiently realize the TS algorithm on\nZynq SoC. In practice, the type of arms distribution (e.g., Bernoulli,\nGaussian, etc.) is unknown and hence, a single algorithm may not be optimal. We\npropose a reconfigurable and intelligent MAB (RI-MAB) framework. Here,\nintelligence enables the identification of appropriate MAB algorithms for a\ngiven environment, and reconfigurability allows on-the-fly switching between\nalgorithms on the SoC. This eliminates the need for parallel implementation of\nalgorithms resulting in huge savings in resources and power consumption. We\nanalyze the functional correctness, area, power, and execution time of the\nproposed and existing architectures for various arm distributions, word-length,\nand hardware-software co-design approaches. We demonstrate the superiority of\nthe RI-MAB over TS and UCB only architectures.",
    "descriptor": "",
    "authors": [
      "S. V. Sai Santosh",
      "Sumit J. Darak"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02855"
  },
  {
    "id": "arXiv:2106.02856",
    "title": "Reinforcement Learning for Assignment Problem with Time Constraints",
    "abstract": "We present an end-to-end framework for the Assignment Problem with multiple\ntasks mapped to a group of workers, using reinforcement learning while\npreserving many constraints. Tasks and workers have time constraints and there\nis a cost associated with assigning a worker to a task. Each worker can perform\nmultiple tasks until it exhausts its allowed time units (capacity). We train a\nreinforcement learning agent to find near optimal solutions to the problem by\nminimizing total cost associated with the assignments while maintaining hard\nconstraints. We use proximal policy optimization to optimize model parameters.\nThe model generates a sequence of actions in real-time which correspond to task\nassignment to workers, without having to retrain for changes in the dynamic\nstate of the environment. In our problem setting reward is computed as negative\nof the assignment cost. We also demonstrate our results on bin packing and\ncapacitated vehicle routing problem, using the same framework. Our results\noutperform Google OR-Tools using MIP and CP-SAT solvers with large problem\ninstances, in terms of solution quality and computation time.",
    "descriptor": "",
    "authors": [
      "Sharmin Pathan",
      "Vyom Shrivastava"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02856"
  },
  {
    "id": "arXiv:2106.02858",
    "title": "A controllability method for Maxwell's equations",
    "abstract": "We propose a controllability method for the numerical solution of\ntime-harmonic Maxwell's equations in their first-order formulation. By\nminimizing a quadratic cost functional, which measures the deviation from\nperiodicity, the controllability method determines iteratively a periodic\nsolution in the time domain. At each conjugate gradient iteration, the gradient\nof the cost functional is simply computed by running any time-dependent\nsimulation code forward and backward for one period, thus leading to a\nnon-intrusive implementation easily integrated into existing software.\nMoreover, the proposed algorithm automatically inherits the parallelism,\nscalability, and low memory footprint of the underlying time-domain solver.\nSince the time-periodic solution obtained by minimization is not necessarily\nunique, we apply a cheap post-processing filtering procedure which recovers the\ntime-harmonic solution from any minimizer. Finally, we present a series of\nnumerical examples which show that our algorithm greatly speeds up the\nconvergence towards the desired time-harmonic solution when compared to simply\nrunning the time-marching code until the time-harmonic regime is eventually\nreached.",
    "descriptor": "",
    "authors": [
      "T. Chaumont-Frelet",
      "M.J. Grote",
      "S. Lanteri",
      "J.H. Tang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.02858"
  },
  {
    "id": "arXiv:2106.02859",
    "title": "Convolutional Neural Networks with Gated Recurrent Connections",
    "abstract": "The convolutional neural network (CNN) has become a basic model for solving\nmany computer vision problems. In recent years, a new class of CNNs, recurrent\nconvolution neural network (RCNN), inspired by abundant recurrent connections\nin the visual systems of animals, was proposed. The critical element of RCNN is\nthe recurrent convolutional layer (RCL), which incorporates recurrent\nconnections between neurons in the standard convolutional layer. With\nincreasing number of recurrent computations, the receptive fields (RFs) of\nneurons in RCL expand unboundedly, which is inconsistent with biological facts.\nWe propose to modulate the RFs of neurons by introducing gates to the recurrent\nconnections. The gates control the amount of context information inputting to\nthe neurons and the neurons' RFs therefore become adaptive. The resulting layer\nis called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a\ndeep model called gated RCNN (GRCNN). The GRCNN was evaluated on several\ncomputer vision tasks including object recognition, scene text recognition and\nobject detection, and obtained much better results than the RCNN. In addition,\nwhen combined with other adaptive RF techniques, the GRCNN demonstrated\ncompetitive performance to the state-of-the-art models on benchmark datasets\nfor these tasks. The codes are released at\n\\href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}.",
    "descriptor": "\nComments: Accepted by TPAMI. An extension of our previous NeurIPS 2017 paper \"Gated recurrent convolution neural network for OCR\". We demonstrate the good performance of GRCNN on image classification and object detection. Codes are available at: this https URL\n",
    "authors": [
      "Jianfeng Wang",
      "Xiaolin Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02859"
  },
  {
    "id": "arXiv:2106.02862",
    "title": "Antenna Array Diagnosis for Millimeter-Wave MIMO Systems",
    "abstract": "The densely packed antennas of millimeter-Wave (mmWave) MIMO systems are\noften blocked by the rain, snow, dust and even by fingers, which will change\nthe channel's characteristics and degrades the system's performance. In order\nto solve this problem, we propose a cross-entropy inspired antenna array\ndiagnosis detection (CE-AAD) technique by exploiting the correlations of\nadjacent antennas, when blockages occur at the transmitter. Then, we extend the\nproposed CE-AAD algorithm to the case, where blockages occur at transmitter and\nreceiver simultaneously. Our simulation results show that the proposed CE-AAD\nalgorithm outperforms its traditional counterparts.",
    "descriptor": "",
    "authors": [
      "Siqi Ma",
      "Wenqian Shen",
      "Jianping An",
      "Lajos Hanzo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02862"
  },
  {
    "id": "arXiv:2106.02864",
    "title": "An End-to-End Breast Tumour Classification Model Using Context-Based  Patch Modelling- A BiLSTM Approach for Image Classification",
    "abstract": "Researchers working on computational analysis of Whole Slide Images (WSIs) in\nhistopathology have primarily resorted to patch-based modelling due to large\nresolution of each WSI. The large resolution makes WSIs infeasible to be fed\ndirectly into the machine learning models due to computational constraints.\nHowever, due to patch-based analysis, most of the current methods fail to\nexploit the underlying spatial relationship among the patches. In our work, we\nhave tried to integrate this relationship along with feature-based correlation\namong the extracted patches from the particular tumorous region. For the given\ntask of classification, we have used BiLSTMs to model both forward and backward\ncontextual relationship. RNN based models eliminate the limitation of sequence\nsize by allowing the modelling of variable size images within a deep learning\nmodel. We have also incorporated the effect of spatial continuity by exploring\ndifferent scanning techniques used to sample patches. To establish the\nefficiency of our approach, we trained and tested our model on two datasets,\nmicroscopy images and WSI tumour regions. After comparing with contemporary\nliterature we achieved the better performance with accuracy of 90% for\nmicroscopy image dataset. For WSI tumour region dataset, we compared the\nclassification results with deep learning networks such as ResNet, DenseNet,\nand InceptionV3 using maximum voting technique. We achieved the highest\nperformance accuracy of 84%. We found out that BiLSTMs with CNN features have\nperformed much better in modelling patches into an end-to-end Image\nclassification network. Additionally, the variable dimensions of WSI tumour\nregions were used for classification without the need for resizing. This\nsuggests that our method is independent of tumour image size and can process\nlarge dimensional images without losing the resolution details.",
    "descriptor": "\nComments: 36 pages, 5 figures, 9 tables. Published in Computerized Medical Imaging and Graphics\n",
    "authors": [
      "Suvidha Tripathi",
      "Satish Kumar Singh",
      "Hwee Kuan Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.02864"
  },
  {
    "id": "arXiv:2106.02865",
    "title": "Consensus Analysis over Clustered Networks of Multi-Agent Systems under  External Disturbances",
    "abstract": "This paper studies a consensus problem of multi-agent systems subjected to\nexternal disturbances over the clustered network. It considers that the agents\nare divided into several clusters. They are almost all the time isolated one\nfrom another, which has a directed spanning tree. The goal of agents achieves a\ncommon value. To support interaction between clusters with a minimum exchange\nof information, we consider that each cluster has an agent, who can exchange\ninformation to any agents outside of its cluster at some discrete instants of\ntime. Our main contribution proposes a consensus protocol, which takes into\naccount the continuous-time communications among agents inside the clusters and\ndiscrete-time communication information across clusters. Accordingly, the\nconsensus and the robust $\\mathcal{H}_{\\infty}$ consensus over the clustered\nnetwork are respectively analyzed. Thanks to results from matrix theory and\nalgebraic graph theory, we show that the proposed control protocols can solve\nthe problems mentioned above. Finally, a numerical example is given to show the\neffectiveness of the proposed theoretical results.",
    "descriptor": "",
    "authors": [
      "Thiem V. Pham",
      "Quynh T. T. Nguyen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02865"
  },
  {
    "id": "arXiv:2106.02866",
    "title": "Conditional Contrastive Learning: Removing Undesirable Information in  Self-Supervised Representations",
    "abstract": "Self-supervised learning is a form of unsupervised learning that leverages\nrich information in data to learn representations. However, data sometimes\ncontains certain information that may be undesirable for downstream tasks. For\ninstance, gender information may lead to biased decisions on many\ngender-irrelevant tasks. In this paper, we develop conditional contrastive\nlearning to remove undesirable information in self-supervised representations.\nTo remove the effect of the undesirable variable, our proposed approach\nconditions on the undesirable variable (i.e., by fixing the variations of it)\nduring the contrastive learning process. In particular, inspired by the\ncontrastive objective InfoNCE, we introduce Conditional InfoNCE (C-InfoNCE),\nand its computationally efficient variant, Weak-Conditional InfoNCE\n(WeaC-InfoNCE), for conditional contrastive learning. We demonstrate\nempirically that our methods can successfully learn self-supervised\nrepresentations for downstream tasks while removing a great level of\ninformation related to the undesirable variables. We study three scenarios,\neach with a different type of undesirable variables: task-irrelevant\nmeta-information for self-supervised speech representation learning, sensitive\nattributes for fair representation learning, and domain specification for\nmulti-domain visual representation learning.",
    "descriptor": "",
    "authors": [
      "Yao-Hung Hubert Tsai",
      "Martin Q. Ma",
      "Han Zhao",
      "Kun Zhang",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02866"
  },
  {
    "id": "arXiv:2106.02867",
    "title": "Ensemble Defense with Data Diversity: Weak Correlation Implies Strong  Robustness",
    "abstract": "In this paper, we propose a framework of filter-based ensemble of deep\nneuralnetworks (DNNs) to defend against adversarial attacks. The framework\nbuilds an ensemble of sub-models -- DNNs with differentiated preprocessing\nfilters. From the theoretical perspective of DNN robustness, we argue that\nunder the assumption of high quality of the filters, the weaker the\ncorrelations of the sensitivity of the filters are, the more robust the\nensemble model tends to be, and this is corroborated by the experiments of\ntransfer-based attacks. Correspondingly, we propose a principle that chooses\nthe specific filters with smaller Pearson correlation coefficients, which\nensures the diversity of the inputs received by DNNs, as well as the\neffectiveness of the entire framework against attacks. Our ensemble models are\nmore robust than those constructed by previous defense methods like adversarial\ntraining, and even competitive with the classical ensemble of adversarial\ntrained DNNs under adversarial attacks when the attacking radius is large.",
    "descriptor": "",
    "authors": [
      "Renjue Li",
      "Hanwei Zhang",
      "Pengfei Yang",
      "Cheng-Chao Huang",
      "Aimin Zhou",
      "Bai Xue",
      "Lijun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02867"
  },
  {
    "id": "arXiv:2106.02869",
    "title": "Integrating Auxiliary Information in Self-supervised Learning",
    "abstract": "This paper presents to integrate the auxiliary information (e.g., additional\nattributes for data such as the hashtags for Instagram images) in the\nself-supervised learning process. We first observe that the auxiliary\ninformation may bring us useful information about data structures: for\ninstance, the Instagram images with the same hashtags can be semantically\nsimilar. Hence, to leverage the structural information from the auxiliary\ninformation, we present to construct data clusters according to the auxiliary\ninformation. Then, we introduce the Clustering InfoNCE (Cl-InfoNCE) objective\nthat learns similar representations for augmented variants of data from the\nsame cluster and dissimilar representations for data from different clusters.\nOur approach contributes as follows: 1) Comparing to conventional\nself-supervised representations, the auxiliary-information-infused\nself-supervised representations bring the performance closer to the supervised\nrepresentations; 2) The presented Cl-InfoNCE can also work with unsupervised\nconstructed clusters (e.g., k-means clusters) and outperform strong\nclustering-based self-supervised learning approaches, such as the Prototypical\nContrastive Learning (PCL) method; 3) We show that Cl-InfoNCE may be a better\napproach to leverage the data clustering information, by comparing it to the\nbaseline approach - learning to predict the clustering assignments with\ncross-entropy loss. For analysis, we connect the goodness of the learned\nrepresentations with the statistical relationships: i) the mutual information\nbetween the labels and the clusters and ii) the conditional entropy of the\nclusters given the labels.",
    "descriptor": "",
    "authors": [
      "Yao-Hung Hubert Tsai",
      "Tianqin Li",
      "Weixin Liu",
      "Peiyuan Liao",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02869"
  },
  {
    "id": "arXiv:2106.02870",
    "title": "Bidirectional Distillation for Top-K Recommender System",
    "abstract": "Recommender systems (RS) have started to employ knowledge distillation, which\nis a model compression technique training a compact model (student) with the\nknowledge transferred from a cumbersome model (teacher). The state-of-the-art\nmethods rely on unidirectional distillation transferring the knowledge only\nfrom the teacher to the student, with an underlying assumption that the teacher\nis always superior to the student. However, we demonstrate that the student\nperforms better than the teacher on a significant proportion of the test set,\nespecially for RS. Based on this observation, we propose Bidirectional\nDistillation (BD) framework whereby both the teacher and the student\ncollaboratively improve with each other. Specifically, each model is trained\nwith the distillation loss that makes to follow the other's prediction along\nwith its original loss function. For effective bidirectional distillation, we\npropose rank discrepancy-aware sampling scheme to distill only the informative\nknowledge that can fully enhance each other. The proposed scheme is designed to\neffectively cope with a large performance gap between the teacher and the\nstudent. Trained in the bidirectional way, it turns out that both the teacher\nand the student are significantly improved compared to when being trained\nseparately. Our extensive experiments on real-world datasets show that our\nproposed framework consistently outperforms the state-of-the-art competitors.\nWe also provide analyses for an in-depth understanding of BD and ablation\nstudies to verify the effectiveness of each proposed component.",
    "descriptor": "\nComments: WWW 2021\n",
    "authors": [
      "Wonbin Kweon",
      "SeongKu Kang",
      "Hwanjo Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.02870"
  },
  {
    "id": "arXiv:2106.02871",
    "title": "Discrete Frechet distance for closed curves",
    "abstract": "The paper presents a discrete variation of the Frechet distance between\nclosed curves, which can be seen as an approximation of the continuous measure.\nA rather straightforward approach to compute the discrete Frechet distance\nbetween two closed sequences of m and n points using binary search takes O(mn\nlog mn) time. We present an algorithm that takes O(mn log* mn) time, where log*\nis the iterated logarithm.",
    "descriptor": "\nComments: 14 pages, 4 figures, 3 algorithms\n",
    "authors": [
      "Evgeniy Vodolazskiy"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.02871"
  },
  {
    "id": "arXiv:2106.02874",
    "title": "RDA: Robust Domain Adaptation via Fourier Adversarial Attacking",
    "abstract": "Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled\nsource domain and an unsupervised loss in an unlabeled target domain, which\noften faces more severe overfitting (than classical supervised learning) as the\nsupervised source loss has clear domain gap and the unsupervised target loss is\noften noisy due to the lack of annotations. This paper presents RDA, a robust\ndomain adaptation technique that introduces adversarial attacking to mitigate\noverfitting in UDA. We achieve robust domain adaptation by a novel Fourier\nadversarial attacking (FAA) method that allows large magnitude of perturbation\nnoises but has minimal modification of image semantics, the former is critical\nto the effectiveness of its generated adversarial samples due to the existence\nof 'domain gaps'. Specifically, FAA decomposes images into multiple frequency\ncomponents (FCs) and generates adversarial samples by just perturbating certain\nFCs that capture little semantic information. With FAA-generated samples, the\ntraining can continue the 'random walk' and drift into an area with a flat loss\nlandscape, leading to more robust domain adaptation. Extensive experiments over\nmultiple domain adaptation tasks show that RDA can work with different computer\nvision tasks with superior performance.",
    "descriptor": "",
    "authors": [
      "Jiaxing Huang",
      "Dayan Guan",
      "Aoran Xiao",
      "Shijian Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02874"
  },
  {
    "id": "arXiv:2106.02875",
    "title": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease  Progression",
    "abstract": "Modeling a system's temporal behaviour in reaction to external stimuli is a\nfundamental problem in many areas. Pure Machine Learning (ML) approaches often\nfail in the small sample regime and cannot provide actionable insights beyond\npredictions. A promising modification has been to incorporate expert domain\nknowledge into ML models. The application we consider is predicting the\nprogression of disease under medications, where a plethora of domain knowledge\nis available from pharmacology. Pharmacological models describe the dynamics of\ncarefully-chosen medically meaningful variables in terms of systems of Ordinary\nDifferential Equations (ODEs). However, these models only describe a limited\ncollection of variables, and these variables are often not observable in\nclinical environments. To close this gap, we propose the latent hybridisation\nmodel (LHM) that integrates a system of expert-designed ODEs with\nmachine-learned Neural ODEs to fully describe the dynamics of the system and to\nlink the expert and latent variables to observable quantities. We evaluated LHM\non synthetic data as well as real-world intensive care data of COVID-19\npatients. LHM consistently outperforms previous works, especially when few\ntraining samples are available such as at the beginning of the pandemic.",
    "descriptor": "",
    "authors": [
      "Zhaozhi Qian",
      "William R. Zame",
      "Mihaela van der Schaar",
      "Lucas M. Fleuren",
      "Paul Elbers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02875"
  },
  {
    "id": "arXiv:2106.02878",
    "title": "A Generative Node-attribute Network Model for Detecting Generalized  Structure",
    "abstract": "Exploring meaningful structural regularities embedded in networks is a key to\nunderstanding and analyzing the structure and function of a network. The\nnode-attribute information can help improve such understanding and analysis.\nHowever, most of the existing methods focus on detecting traditional\ncommunities, i.e., groupings of nodes with dense internal connections and\nsparse external ones. In this paper, based on the connectivity behavior of\nnodes and homogeneity of attributes, we propose a principle model (named GNAN),\nwhich can generate both topology information and attribute information. The new\nmodel can detect not only community structure, but also a range of other types\nof structure in networks, such as bipartite structure, core-periphery\nstructure, and their mixture structure, which are collectively referred to as\ngeneralized structure. The proposed model that combines topological information\nand node-attribute information can detect communities more accurately than the\nmodel that only uses topology information. The dependency between attributes\nand communities can be automatically learned by our model and thus we can\nignore the attributes that do not contain useful information. The model\nparameters are inferred by using the expectation-maximization algorithm. And a\ncase study is provided to show the ability of our model in the semantic\ninterpretability of communities. Experiments on both synthetic and real-world\nnetworks show that the new model is competitive with other state-of-the-art\nmodels.",
    "descriptor": "",
    "authors": [
      "Wei Liu",
      "Zhenhai Chang",
      "Caiyan Jia",
      "Yimei Zheng"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02878"
  },
  {
    "id": "arXiv:2106.02881",
    "title": "Graph Infomax Adversarial Learning for Treatment Effect Estimation with  Networked Observational Data",
    "abstract": "Treatment effect estimation from observational data is a critical research\ntopic across many domains. The foremost challenge in treatment effect\nestimation is how to capture hidden confounders. Recently, the growing\navailability of networked observational data offers a new opportunity to deal\nwith the issue of hidden confounders. Unlike networked data in traditional\ngraph learning tasks, such as node classification and link detection, the\nnetworked data under the causal inference problem has its particularity, i.e.,\nimbalanced network structure. In this paper, we propose a Graph Infomax\nAdversarial Learning (GIAL) model for treatment effect estimation, which makes\nfull use of the network structure to capture more information by recognizing\nthe imbalance in network structure. We evaluate the performance of our GIAL\nmodel on two benchmark datasets, and the results demonstrate superiority over\nthe state-of-the-art methods.",
    "descriptor": "\nComments: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21), August 14--18, 2021, Virtual Event, Singapore\n",
    "authors": [
      "Zhixuan Chu",
      "Stephen L. Rathbun",
      "Sheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02881"
  },
  {
    "id": "arXiv:2106.02885",
    "title": "Category Contrast for Unsupervised Domain Adaptation in Visual Tasks",
    "abstract": "Instance contrast for unsupervised representation learning has achieved great\nsuccess in recent years. In this work, we explore the idea of instance\ncontrastive learning in unsupervised domain adaptation (UDA) and propose a\nnovel Category Contrast technique (CaCo) that introduces semantic priors on top\nof instance discrimination for visual UDA tasks. By considering instance\ncontrastive learning as a dictionary look-up operation, we construct a\nsemantics-aware dictionary with samples from both source and target domains\nwhere each target sample is assigned a (pseudo) category label based on the\ncategory priors of source samples. This allows category contrastive learning\n(between target queries and the category-level dictionary) for\ncategory-discriminative yet domain-invariant feature representations: samples\nof the same category (from either source or target domain) are pulled closer\nwhile those of different categories are pushed apart simultaneously. Extensive\nUDA experiments in multiple visual tasks ($e.g.$, segmentation, classification\nand detection) show that the simple implementation of CaCo achieves superior\nperformance as compared with the highly-optimized state-of-the-art methods.\nAnalytically and empirically, the experiments also demonstrate that CaCo is\ncomplementary to existing UDA methods and generalizable to other learning\nsetups such as semi-supervised learning, unsupervised model adaptation, etc.",
    "descriptor": "",
    "authors": [
      "Jiaxing Huang",
      "Dayan Guan",
      "Aoran Xiao",
      "Shijian Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02885"
  },
  {
    "id": "arXiv:2106.02886",
    "title": "Context-Aware Sparse Deep Coordination Graphs",
    "abstract": "Learning sparse coordination graphs adaptive to the coordination dynamics\namong agents is a long-standing problem in cooperative multi-agent learning.\nThis paper studies this problem by proposing several value-based and\nobservation-based schemes for learning dynamic topologies and evaluating them\non a new Multi-Agent COordination (MACO) benchmark. The benchmark collects\nclassic coordination problems in the literature, increases their difficulty,\nand classifies them into different types. By analyzing the individual\nadvantages of each learning scheme on each type of problem and their overall\nperformance, we propose a novel method using the variance of utility difference\nfunctions to learn context-aware sparse coordination topologies. Moreover, our\nmethod learns action representations that effectively reduce the influence of\nutility functions' estimation errors on graph construction. Experiments show\nthat our method significantly outperforms dense and static topologies across\nthe MACO and StarCraft II micromanagement benchmark.",
    "descriptor": "",
    "authors": [
      "Tonghan Wang",
      "Liang Zeng",
      "Weijun Dong",
      "Qianlan Yang",
      "Yang Yu",
      "Chongjie Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02886"
  },
  {
    "id": "arXiv:2106.02887",
    "title": "An Empirical Study on Tensor Shape Faults in Deep Learning Systems",
    "abstract": "Software developers frequently adopt deep learning (DL) libraries to\nincorporate learning solutions into software systems. However, misuses of these\nlibraries can cause various DL faults. Among them, tensor shape faults are most\nprevalent. Tensor shape faults occur when restriction conditions of operations\nare not met, leading to many system crashes. To support efficient detection and\nfixing of these faults, we conduct an empirical study to obtain a deep insight.\nWe construct SFData, a set of 146 buggy programs with crashing tensor shape\nfaults (i.e., those causing programs to crash). By analyzing the faults in\nSFData, we categorize them into four types and get some valuable observations.",
    "descriptor": "",
    "authors": [
      "Dangwei Wu",
      "Beijun Shen",
      "Yuting Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.02887"
  },
  {
    "id": "arXiv:2106.02888",
    "title": "Bandwidth-based Step-Sizes for Non-Convex Stochastic Optimization",
    "abstract": "Many popular learning-rate schedules for deep neural networks combine a\ndecaying trend with local perturbations that attempt to escape saddle points\nand bad local minima. We derive convergence guarantees for bandwidth-based\nstep-sizes, a general class of learning-rates that are allowed to vary in a\nbanded region. This framework includes cyclic and non-monotonic step-sizes for\nwhich no theoretical guarantees were previously known. We provide worst-case\nguarantees for SGD on smooth non-convex problems under several bandwidth-based\nstep sizes, including stagewise $1/\\sqrt{t}$ and the popular step-decay\n(constant and then drop by a constant), which is also shown to be optimal.\nMoreover, we show that its momentum variant (SGDM) converges as fast as SGD\nwith the bandwidth-based step-decay step-size. Finally, we propose some novel\nstep-size schemes in the bandwidth-based family and verify their efficiency on\nseveral deep neural network training tasks.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Wang",
      "Mikael Johansson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02888"
  },
  {
    "id": "arXiv:2106.02890",
    "title": "Can Subnetwork Structure be the Key to Out-of-Distribution  Generalization?",
    "abstract": "Can models with particular structure avoid being biased towards spurious\ncorrelation in out-of-distribution (OOD) generalization? Peters et al. (2016)\nprovides a positive answer for linear cases. In this paper, we use a functional\nmodular probing method to analyze deep model structures under OOD setting. We\ndemonstrate that even in biased models (which focus on spurious correlation)\nthere still exist unbiased functional subnetworks. Furthermore, we articulate\nand demonstrate the functional lottery ticket hypothesis: full network contains\na subnetwork that can achieve better OOD performance. We then propose Modular\nRisk Minimization to solve the subnetwork selection problem. Our algorithm\nlearns the subnetwork structure from a given dataset, and can be combined with\nany other OOD regularization methods. Experiments on various OOD generalization\ntasks corroborate the effectiveness of our method.",
    "descriptor": "\nComments: Accepted to ICML2021 as long talk\n",
    "authors": [
      "Dinghuai Zhang",
      "Kartik Ahuja",
      "Yilun Xu",
      "Yisen Wang",
      "Aaron Courville"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02890"
  },
  {
    "id": "arXiv:2106.02892",
    "title": "Training Robust Graph Neural Networks with Topology Adaptive Edge  Dropping",
    "abstract": "Graph neural networks (GNNs) are processing architectures that exploit graph\nstructural information to model representations from network data. Despite\ntheir success, GNNs suffer from sub-optimal generalization performance given\nlimited training data, referred to as over-fitting. This paper proposes\nTopology Adaptive Edge Dropping (TADropEdge) method as an adaptive data\naugmentation technique to improve generalization performance and learn robust\nGNN models. We start by explicitly analyzing how random edge dropping increases\nthe data diversity during training, while indicating i.i.d. edge dropping does\nnot account for graph structural information and could result in noisy\naugmented data degrading performance. To overcome this issue, we consider graph\nconnectivity as the key property that captures graph topology. TADropEdge\nincorporates this factor into random edge dropping such that the edge-dropped\nsubgraphs maintain similar topology as the underlying graph, yielding more\nsatisfactory data augmentation. In particular, TADropEdge first leverages the\ngraph spectrum to assign proper weights to graph edges, which represent their\ncriticality for establishing the graph connectivity. It then normalizes the\nedge weights and drops graph edges adaptively based on their normalized\nweights. Besides improving generalization performance, TADropEdge reduces\nvariance for efficient training and can be applied as a generic method modular\nto different GNN models. Intensive experiments on real-life and synthetic\ndatasets corroborate theory and verify the effectiveness of the proposed\nmethod.",
    "descriptor": "",
    "authors": [
      "Zhan Gao",
      "Subhrajit Bhattacharya",
      "Leiming Zhang",
      "Rick S. Blum",
      "Alejandro Ribeiro",
      "Brian M. Sadler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02892"
  },
  {
    "id": "arXiv:2106.02894",
    "title": "MoleHD: Automated Drug Discovery using Brain-Inspired Hyperdimensional  Computing",
    "abstract": "Modern drug discovery is often time-consuming, complex and cost-ineffective\ndue to the large volume of molecular data and complicated molecular properties.\nRecently, machine learning algorithms have shown promising results in virtual\nscreening of automated drug discovery by predicting molecular properties. While\nemerging learning methods such as graph neural networks and recurrent neural\nnetworks exhibit high accuracy, they are also notoriously computation-intensive\nand memory-intensive with operations such as feature embeddings or deep\nconvolutions. In this paper, we propose a viable alternative to neural network\nclassifiers. We present MoleHD, a method based on brain-inspired\nhyperdimensional computing (HDC) for molecular property prediction. We first\ntransform the SMILES presentation of molecules into feature vectors by SMILE-PE\ntokenizers pretrained on the ChEMBL database. Then, we develop HDC encoders to\nproject such features into high-dimensional vectors that are used for training\nand inference. We perform an extensive evaluation using 30 classification tasks\nfrom 3 widely-used molecule datasets and compare MoleHD with 10 baseline\nmethods including 6 SOTA neural network classifiers. Results show that MoleHD\nis able to outperform all the baseline methods on average across 30\nclassification tasks with significantly reduced computing cost. To the best of\nour knowledge, we develop the first HDC-based method for drug discovery. The\npromising results presented in this paper can potentially lead to a novel path\nin drug discovery research.",
    "descriptor": "\nComments: Under review for NeurIPS 2021\n",
    "authors": [
      "Dongning Ma",
      "Xun Jiao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.02894"
  },
  {
    "id": "arXiv:2106.02898",
    "title": "Dynamic Resolution Network",
    "abstract": "Deep convolutional neural networks (CNNs) are often of sophisticated design\nwith numerous convolutional layers and learnable parameters for the accuracy\nreason. To alleviate the expensive costs of deploying them on mobile devices,\nrecent works have made huge efforts for excavating redundancy in pre-defined\narchitectures. Nevertheless, the redundancy on the input resolution of modern\nCNNs has not been fully investigated, i.e., the resolution of input image is\nfixed. In this paper, we observe that the smallest resolution for accurately\npredicting the given image is different using the same neural network. To this\nend, we propose a novel dynamic-resolution network (DRNet) in which the\nresolution is determined dynamically based on each input sample. Thus, a\nresolution predictor with negligible computational costs is explored and\noptimized jointly with the desired network. In practice, the predictor learns\nthe smallest resolution that can retain and even exceed the original\nrecognition accuracy for each image. During the inference, each input image\nwill be resized to its predicted resolution for minimizing the overall\ncomputation burden. We then conduct extensive experiments on several benchmark\nnetworks and datasets. The results show that our DRNet can be embedded in any\noff-the-shelf network architecture to obtain a considerable reduction in\ncomputational complexity. For instance, DRNet achieves similar performance with\nan about 34% computation reduction, while gains 1.4% accuracy increase with 10%\ncomputation reduction compared to the original ResNet-50 on ImageNet.",
    "descriptor": "",
    "authors": [
      "Mingjian Zhu",
      "Kai Han",
      "Enhua Wu",
      "Qiulin Zhang",
      "Ying Nie",
      "Zhenzhong Lan",
      "Yunhe Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02898"
  },
  {
    "id": "arXiv:2106.02900",
    "title": "Differentially Private Multi-Armed Bandits in the Shuffle Model",
    "abstract": "We give an $(\\varepsilon,\\delta)$-differentially private algorithm for the\nmulti-armed bandit (MAB) problem in the shuffle model with a\ndistribution-dependent regret of $O\\left(\\left(\\sum_{a\\in\n[k]:\\Delta_a>0}\\frac{\\log\nT}{\\Delta_a}\\right)+\\frac{k\\sqrt{\\log\\frac{1}{\\delta}}\\log\nT}{\\varepsilon}\\right)$, and a distribution-independent regret of\n$O\\left(\\sqrt{kT\\log T}+\\frac{k\\sqrt{\\log\\frac{1}{\\delta}}\\log\nT}{\\varepsilon}\\right)$, where $T$ is the number of rounds, $\\Delta_a$ is the\nsuboptimality gap of the arm $a$, and $k$ is the total number of arms. Our\nupper bound almost matches the regret of the best known algorithms for the\ncentralized model, and significantly outperforms the best known algorithm in\nthe local model.",
    "descriptor": "",
    "authors": [
      "Jay Tenenbaum",
      "Haim Kaplan",
      "Yishay Mansour",
      "Uri Stemmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02900"
  },
  {
    "id": "arXiv:2106.02902",
    "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT",
    "abstract": "Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\narticle, we probe BERT specifically to understand and measure the relational\nknowledge it captures in its parametric memory. While probing for linguistic\nunderstanding is commonly applied to all layers of BERT as well as fine-tuned\nmodels, this has not been done for factual knowledge. We utilize existing\nknowledge base completion tasks (LAMA) to probe every layer of pre-trained as\nwell as fine-tuned BERT models(ranking, question answering, NER). Our findings\nshow that knowledge is not just contained in BERT's final layers. Intermediate\nlayers contribute a significant amount (17-60%) to the total knowledge found.\nProbing intermediate layers also reveals how different types of knowledge\nemerge at varying rates. When BERT is fine-tuned, relational knowledge is\nforgotten. The extent of forgetting is impacted by the fine-tuning objective\nand the training data. We found that ranking models forget the least and retain\nmore knowledge in their final layer compared to masked language modeling and\nquestion-answering. However, masked language modeling performed the best at\nacquiring new knowledge from the training data. When it comes to learning\nfacts, we found that capacity and fact density are key factors. We hope this\ninitial work will spur further research into understanding the parametric\nmemory of language models and the effect of training objectives on factual\nknowledge. The code to repeat the experiments is publicly available on GitHub.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2010.09313\n",
    "authors": [
      "Jonas Wallat",
      "Jaspreet Singh",
      "Avishek Anand"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02902"
  },
  {
    "id": "arXiv:2106.02914",
    "title": "Feature Flow Regularization: Improving Structured Sparsity in Deep  Neural Networks",
    "abstract": "Pruning is a model compression method that removes redundant parameters in\ndeep neural networks (DNNs) while maintaining accuracy. Most available filter\npruning methods require complex treatments such as iterative pruning, features\nstatistics/ranking, or additional optimization designs in the training process.\nIn this paper, we propose a simple and effective regularization strategy from a\nnew perspective of evolution of features, which we call feature flow\nregularization (FFR), for improving structured sparsity and filter pruning in\nDNNs. Specifically, FFR imposes controls on the gradient and curvature of\nfeature flow along the neural network, which implicitly increases the sparsity\nof the parameters. The principle behind FFR is that coherent and smooth\nevolution of features will lead to an efficient network that avoids redundant\nparameters. The high structured sparsity obtained from FFR enables us to prune\nfilters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and\nTiny ImageNet datasets demonstrate that FFR can significantly improve both\nunstructured and structured sparsity. Our pruning results in terms of reduction\nof parameters and FLOPs are comparable to or even better than those of\nstate-of-the-art pruning methods.",
    "descriptor": "",
    "authors": [
      "Yue Wu",
      "Yuan Lan",
      "Luchan Zhang",
      "Yang Xiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02914"
  },
  {
    "id": "arXiv:2106.02921",
    "title": "Empirically Evaluating Creative Arc Negotiation for Improvisational  Decision-making",
    "abstract": "Action selection from many options with few constraints is crucial for\nimprovisation and co-creativity. Our previous work proposed creative arc\nnegotiation to solve this problem, i.e., selecting actions to follow an\nauthor-defined `creative arc' or trajectory over estimates of novelty,\nunexpectedness, and quality for potential actions. The CARNIVAL agent\narchitecture demonstrated this approach for playing the Props game from improv\ntheatre in the Robot Improv Circus installation. This article evaluates the\ncreative arc negotiation experience with CARNIVAL through two crowdsourced\nobserver studies and one improviser laboratory study. The studies focus on\nsubjects' ability to identify creative arcs in performance and their preference\nfor creative arc negotiation compared to a random selection baseline. Our\nresults show empirically that observers successfully identified creative arcs\nin performances. Both groups also preferred creative arc negotiation in agent\ncreativity and logical coherence, while observers enjoyed it more too.",
    "descriptor": "\nComments: 10 pages, 5 figures, 8 tables, accepted to ACM Creativity & Cognition 2021, Best Paper Award\n",
    "authors": [
      "Mikhail Jacob",
      "Brian Magerko"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.02921"
  },
  {
    "id": "arXiv:2106.02922",
    "title": "A finite element method for two-phase flow with material viscous  interface",
    "abstract": "This paper studies a model of two-phase flow with an immersed material\nviscous interface and a finite element method for numerical solution of the\nresulting system of PDEs. The interaction between the bulk and surface media is\ncharacterized by no-penetration and slip with friction interface conditions.\nThe system is shown to be dissipative and a model stationary problem is proved\nto be well-posed. The finite element method applied in this paper belongs to a\nfamily of unfitted discretizations. The performance of the method when model\nand discretization parameters vary is assessed. Moreover, an iterative\nprocedure based on the splitting of the system into bulk and surface problems\nis introduced and studied numerically.",
    "descriptor": "",
    "authors": [
      "Maxim Olshanskii",
      "Annalisa Quaini",
      "Qi Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.02922"
  },
  {
    "id": "arXiv:2106.02923",
    "title": "Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$  Regularization",
    "abstract": "There have been many recent advances in representation learning; however,\nunsupervised representation learning can still struggle with model\nidentification issues. Variational Auto-Encoders (VAEs) and their extensions\nsuch as $\\beta$-VAEs have been shown to locally align latent variables with PCA\ndirections, which can help to improve model disentanglement under some\nconditions. Borrowing inspiration from Independent Component Analysis (ICA) and\nsparse coding, we propose applying an $L_1$ loss to the VAE's generative\nJacobian during training to encourage local latent variable alignment with\nindependent factors of variation in the data. We demonstrate our results on a\nvariety of datasets, giving qualitative and quantitative results using\ninformation theoretic and modularity measures that show our added $L_1$ cost\nencourages local axis alignment of the latent representation with individual\nfactors of variation.",
    "descriptor": "\nComments: 16 pages, 9 figures\n",
    "authors": [
      "Travers Rhodes",
      "Daniel D. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02923"
  },
  {
    "id": "arXiv:2106.02925",
    "title": "Tensor Normal Training for Deep Learning Models",
    "abstract": "Despite the predominant use of first-order methods for training deep learning\nmodels, second-order methods, and in particular, natural gradient methods,\nremain of interest because of their potential for accelerating training through\nthe use of curvature information. Several methods with non-diagonal\npreconditioning matrices, including KFAC and Shampoo, have been proposed and\nshown to be effective. Based on the so-called tensor normal (TN) distribution,\nwe propose and analyze a brand new approximate natural gradient method, Tensor\nNormal Training (TNT), which like Shampoo, only requires knowledge on the shape\nof the training parameters. By approximating the probabilistically based Fisher\nmatrix, as opposed to the empirical Fisher matrix, our method uses the\nlayer-wise covariance of the sampling based gradient as the pre-conditioning\nmatrix. Moreover, the assumption that the sampling-based (tensor) gradient\nfollows a TN distribution, ensures that its covariance has a Kronecker\nseparable structure, which leads to a tractable approximation to the Fisher\nmatrix. Consequently, TNT's memory requirements and per-iteration computational\ncosts are only slightly higher than those for first-order methods. In our\nexperiments, TNT exhibited superior optimization performance to KFAC and\nShampoo, and to state-of-the-art first-order methods. Moreover, TNT\ndemonstrated its ability to generalize as well as these first-order methods,\nusing fewer epochs.",
    "descriptor": "",
    "authors": [
      "Yi Ren",
      "Donald Goldfarb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02925"
  },
  {
    "id": "arXiv:2106.02926",
    "title": "IM-META: Influence Maximization Using Node Metadata in Networks With  Unknown Topology",
    "abstract": "In real-world applications of influence maximization (IM), the network\nstructure is often unknown. In this case, we may identify the most influential\nseed nodes by exploring only a part of the underlying network given a small\nbudget for node queries. Motivated by the fact that collecting node metadata is\nmore cost-effective than investigating the relationship between nodes via\nqueried nodes, we develop IM-META, an end-to-end solution to IM in networks\nwith unknown topology by retrieving information from both queries and node\nmetadata. However, using such metadata to aid the IM process is not without\nrisk due to the noisy nature of metadata and uncertainties in connectivity\ninference. To tackle these challenges, we formulate an IM problem that aims to\nfind two sets, i.e., seed nodes and queried nodes. We propose an effective\nmethod that iteratively performs three steps: 1) we learn the relationship\nbetween collected metadata and edges via a Siamese neural network model, 2) we\nselect a number of inferred influential edges to construct a reinforced graph\nused for discovering an optimal seed set, and 3) we identify the next node to\nquery by maximizing the inferred influence spread using a topology-aware\nranking strategy. By querying only 5% of nodes, IM-META reaches 93% of the\nupper bound performance.",
    "descriptor": "\nComments: 14 pages, 12 figures, 2 tables\n",
    "authors": [
      "Cong Tran",
      "Won-Yong Shin",
      "Andreas Spitz"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.02926"
  },
  {
    "id": "arXiv:2106.02927",
    "title": "A Framework for Dynamic Optimal Next-Hop Selection and RF Interface  Setting in IoT with the Same Source Requests",
    "abstract": "Various applications of machines in Internet of Things (IoT) require\ndifferent bandwidths. Each machine may choose its RF interface, according to\nrequired bandwidth for sending its data. We propose an optimal next-hop\nselection framework with dynamic RF interface settings for sources with same\nrequested bandwidth. This framework enables machines to optimally select\nnetwork devices with different RF equipment. In this way, the efficiency and\ncorrect use of RF network resources can be improved. The simulations show that,\nthe average data rate of sources improved between 11.1% to 117% and the average\nunmatched source improved between 1.9% and 5.3%.",
    "descriptor": "",
    "authors": [
      "Monireh Allah Gholi Ghasri",
      "Ali Mohammad Afshin Hemmatyar"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02927"
  },
  {
    "id": "arXiv:2106.02930",
    "title": "Spectral Temporal Graph Neural Network for Trajectory Prediction",
    "abstract": "An effective understanding of the contextual environment and accurate motion\nforecasting of surrounding agents is crucial for the development of autonomous\nvehicles and social mobile robots. This task is challenging since the behavior\nof an autonomous agent is not only affected by its own intention, but also by\nthe static environment and surrounding dynamically interacting agents. Previous\nworks focused on utilizing the spatial and temporal information in time domain\nwhile not sufficiently taking advantage of the cues in frequency domain. To\nthis end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which\ncan capture inter-agent correlations and temporal dependency simultaneously in\nfrequency domain in addition to time domain. SpecTGNN operates on both an agent\ngraph with dynamic state information and an environment graph with the features\nextracted from context images in two streams. The model integrates graph\nFourier transform, spectral graph convolution and temporal gated convolution to\nencode history information and forecast future trajectories. Moreover, we\nincorporate a multi-head spatio-temporal attention mechanism to mitigate the\neffect of error propagation in a long time horizon. We demonstrate the\nperformance of SpecTGNN on two public trajectory prediction benchmark datasets,\nwhich achieves state-of-the-art performance in terms of prediction accuracy.",
    "descriptor": "\nComments: ICRA 2021\n",
    "authors": [
      "Defu Cao",
      "Jiachen Li",
      "Hengbo Ma",
      "Masayoshi Tomizuka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02930"
  },
  {
    "id": "arXiv:2106.02933",
    "title": "k-Mixup Regularization for Deep Learning via Optimal Transport",
    "abstract": "Mixup is a popular regularization technique for training deep neural networks\nthat can improve generalization and increase adversarial robustness. It\nperturbs input training data in the direction of other randomly-chosen\ninstances in the training set. To better leverage the structure of the data, we\nextend mixup to \\emph{$k$-mixup} by perturbing $k$-batches of training points\nin the direction of other $k$-batches using displacement interpolation,\ninterpolation under the Wasserstein metric. We demonstrate theoretically and in\nsimulations that $k$-mixup preserves cluster and manifold structures, and we\nextend theory studying efficacy of standard mixup. Our empirical results show\nthat training with $k$-mixup further improves generalization and robustness on\nbenchmark datasets.",
    "descriptor": "",
    "authors": [
      "Kristjan Greenewald",
      "Anming Gu",
      "Mikhail Yurochkin",
      "Justin Solomon",
      "Edward Chien"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02933"
  },
  {
    "id": "arXiv:2106.02934",
    "title": "Lightweight Dual-channel Target Speaker Separation for Mobile Voice  Communication",
    "abstract": "Nowadays, there is a strong need to deploy the target speaker separation\n(TSS) model on mobile devices with a limitation of the model size and\ncomputational complexity. To better perform TSS for mobile voice communication,\nwe first make a dual-channel dataset based on a specific scenario, LibriPhone.\nSpecifically, to better mimic the real-case scenario, instead of simulating\nfrom the single-channel dataset, LibriPhone is made by simultaneously replaying\npairs of utterances from LibriSpeech by two professional artificial heads and\nrecording by two built-in microphones of the mobile. Then, we propose a\nlightweight time-frequency domain separation model, LSTM-Former, which is based\non the LSTM framework with source-to-noise ratio (SI-SNR) loss. For the\nexperiments on Libri-Phone, we explore the dual-channel LSTMFormer model and a\nsingle-channel version by a random single channel of Libri-Phone. Experimental\nresult shows that the dual-channel LSTM-Former outperforms the single-channel\nLSTMFormer with relative 25% improvement. This work provides a feasible\nsolution for the TSS task on mobile devices, playing back and recording\nmultiple data sources in real application scenarios for getting dual-channel\nreal data can assist the lightweight model to achieve higher performance.",
    "descriptor": "",
    "authors": [
      "Yuanyuan Bao",
      "Yanze Xu",
      "Na Xu",
      "Wenjing Yang",
      "Hongfeng Li",
      "Shicong Li",
      "Yongtao Jia",
      "Fei Xiang",
      "Jincheng He",
      "Ming Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.02934"
  },
  {
    "id": "arXiv:2106.02937",
    "title": "Generalized Linear One-Way Jumping Finite Automata",
    "abstract": "A new discontinuous model of computation called one-way jumping finite\nautomata was defined by H. Chigahara et. al. This model was a restricted\nversion of the model jumping finite automata. These automata read an input\nsymbol-by-symbol and jump only in one direction. A generalized linear one-way\njumping finite automaton makes jumps after deleting a substring of an input\nstring and then changes its state. These automata can make sequence of jumps in\nonly one direction on an input string either from left to right or from right\nto left. We show that newly defined model is powerful than its original\ncounterpart. We define and compare the variants, generalized right linear\none-way jumping finite automata and generalized left linear one-way jumping\nfinite automata. We also compare the newly defined models with Chomsky\nhierarchy. Finally, we explore closure properties of the model.",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Ujjwal Kumar Mishra",
      "Kalpana Mahalingam",
      "Rama Raghavan"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.02937"
  },
  {
    "id": "arXiv:2106.02938",
    "title": "Energy-Based Learning for Cooperative Games, with Applications to  Feature/Data/Model Valuations",
    "abstract": "Valuation problems, such as attribution-based feature interpretation, data\nvaluation and model valuation for ensembles, become increasingly more important\nin many machine learning applications. Such problems are commonly solved by\nwell-known game-theoretic criteria, such as Shapley value or Banzhaf index. In\nthis work, we present a novel energy-based treatment for cooperative games,\nwith a theoretical justification by the maximum entropy framework.\nSurprisingly, by conducting variational inference of the energy-based model, we\nrecover various game-theoretic valuation criteria, such as Shapley value and\nBanzhaf index, through conducting one-step gradient ascent for maximizing the\nmean-field ELBO objective. This observation also verifies the rationality of\nexisting criteria, as they are all trying to decouple the correlations among\nthe players through the mean-field approach. By running gradient ascent for\nmultiple steps, we achieve a trajectory of the valuations, among which we\ndefine the valuation with the best conceivable decoupling error as the\nVariational Index. We experimentally demonstrate that the proposed Variational\nIndex enjoys intriguing properties on certain synthetic and real-world\nvaluation problems.",
    "descriptor": "",
    "authors": [
      "Yatao Bian",
      "Yu Rong",
      "Tingyang Xu",
      "Jiaxiang Wu",
      "Andreas Krause",
      "Junzhou Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02938"
  },
  {
    "id": "arXiv:2106.02940",
    "title": "Same State, Different Task: Continual Reinforcement Learning without  Interference",
    "abstract": "Continual Learning (CL) considers the problem of training an agent\nsequentially on a set of tasks while seeking to retain performance on all\nprevious tasks. A key challenge in CL is catastrophic forgetting, which arises\nwhen performance on a previously mastered task is reduced when learning a new\ntask. While a variety of methods exist to combat forgetting, in some cases\ntasks are fundamentally incompatible with each other and thus cannot be learnt\nby a single policy. This can occur, in reinforcement learning (RL) when an\nagent may be rewarded for achieving different goals from the same observation.\nIn this paper we formalize this ``interference'' as distinct from the problem\nof forgetting. We show that existing CL methods based on single neural network\npredictors with shared replay buffers fail in the presence of interference.\nInstead, we propose a simple method, OWL, to address this challenge. OWL learns\na factorized policy, using shared feature extraction layers, but separate\nheads, each specializing on a new task. The separate heads in OWL are used to\nprevent interference. At test time, we formulate policy selection as a\nmulti-armed bandit problem, and show it is possible to select the best policy\nfor an unknown task using feedback from the environment. The use of bandit\nalgorithms allows the OWL agent to constructively re-use different continually\nlearnt policies at different times during an episode. We show in multiple RL\nenvironments that existing replay based CL methods fail, while OWL is able to\nachieve close to optimal performance when training sequentially.",
    "descriptor": "\nComments: 20 pages, 12 figures\n",
    "authors": [
      "Samuel Kessler",
      "Jack Parker-Holder",
      "Philip Ball",
      "Stefan Zohren",
      "Stephen J. Roberts"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02940"
  },
  {
    "id": "arXiv:2106.02942",
    "title": "Time-Optimal Sublinear Algorithms for Matching and Vertex Cover",
    "abstract": "We present a near-tight analysis of the average \"query complexity\" -- \\`a la\nNguyen and Onak [FOCS'08] -- of the randomized greedy maximal matching\nalgorithm, improving over the bound of Yoshida, Yamamoto and Ito [STOC'09]. For\nany $n$-vertex graph of average degree $\\bar{d}$, this leads to the following\nsublinear-time algorithms for estimating the size of maximum matching and\nminimum vertex cover, all of which are provably time-optimal up to logarithmic\nfactors:\n$\\bullet$ A multiplicative $(2+\\epsilon)$-approximation in\n$\\widetilde{O}(n/\\epsilon^2)$ time using adjacency list queries. This (nearly)\nmatches an $\\Omega(n)$ time lower bound for any multiplicative approximation\nand is, notably, the first $O(1)$-approximation that runs in $o(n^{1.5})$ time.\n$\\bullet$ A $(2, \\epsilon n)$-approximation in $\\widetilde{O}((\\bar{d} +\n1)/\\epsilon^2)$ time using adjacency list queries. This (nearly) matches an\n$\\Omega(\\bar{d}+1)$ lower bound of Parnas and Ron [TCS'07] which holds for any\n$(O(1), \\epsilon n)$-approximation, and improves over the bounds of [Yoshida et\nal. STOC'09; Onak et al. SODA'12] and [Kapralov et al. SODA'20]: The former two\ntake at least quadratic time in the degree which can be as large as\n$\\Omega(n^2)$ and the latter obtains a much larger approximation.\n$\\bullet$ A $(2, \\epsilon n)$-approximation in $\\widetilde{O}(n/\\epsilon^3)$\ntime using adjacency matrix queries. This (nearly) matches an $\\Omega(n)$ time\nlower bound in this model and improves over the $\\widetilde{O}(n\\sqrt{n})$-time\n$(2, \\epsilon n)$-approximate algorithm of [Chen, Kannan, and Khanna ICALP'20].\nIt also turns out that any non-trivial multiplicative approximation in the\nadjacency matrix model requires $\\Omega(n^2)$ time, so the additive $\\epsilon\nn$ error is necessary too.\nAs immediate corollaries, we get improved sublinear time estimators for\n(variants of) TSP and an improved AMPC algorithm for maximal matching.",
    "descriptor": "",
    "authors": [
      "Soheil Behnezhad"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.02942"
  },
  {
    "id": "arXiv:2106.02943",
    "title": "Learning Routines for Effective Off-Policy Reinforcement Learning",
    "abstract": "The performance of reinforcement learning depends upon designing an\nappropriate action space, where the effect of each action is measurable, yet,\ngranular enough to permit flexible behavior. So far, this process involved\nnon-trivial user choices in terms of the available actions and their execution\nfrequency. We propose a novel framework for reinforcement learning that\neffectively lifts such constraints. Within our framework, agents learn\neffective behavior over a routine space: a new, higher-level action space,\nwhere each routine represents a set of 'equivalent' sequences of granular\nactions with arbitrary length. Our routine space is learned end-to-end to\nfacilitate the accomplishment of underlying off-policy reinforcement learning\nobjectives. We apply our framework to two state-of-the-art off-policy\nalgorithms and show that the resulting agents obtain relevant performance\nimprovements while requiring fewer interactions with the environment per\nepisode, improving computational efficiency.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Edoardo Cetin",
      "Oya Celiktutan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02943"
  },
  {
    "id": "arXiv:2106.02947",
    "title": "Complexity of Modular Circuits",
    "abstract": "We study how the complexity of modular circuits computing AND depends on the\ndepth of the circuits and the prime factorization of the modulus they use. In\nparticular our construction of subexponential circuits of depth 2 for AND helps\nus to classify (modulo Exponential Time Hypothesis) modular circuits with\nrespect to the complexity of their satisfiability. We also study a precise\ncorrelation between this complexity and the sizes of modular circuits realizing\nAND. On the other hand showing that AND can be computed by a polynomial size\nprobabilistic modular circuit of depth 2 (with O(log n) random bits) providing\na probabilistic computational model that can not be derandomized.\nWe apply our methods to determine (modulo ETH) the complexity of solving\nequations over groups of symmetries of regular polygons with an odd number of\nsides. These groups form a paradigm for some of the remaining cases in\ncharacterizing finite groups with respect to the complexity of their equation\nsolving.",
    "descriptor": "",
    "authors": [
      "Pawe\u0142 M. Idziak",
      "Piotr Kawa\u0142ek",
      "Jacek Krzaczkowski"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.02947"
  },
  {
    "id": "arXiv:2106.02950",
    "title": "Second-order finite difference approximations of the upper-convected  time derivative",
    "abstract": "In this work, new finite difference schemes are presented for dealing with\nthe upper-convected time derivative in the context of the generalized Lie\nderivative. The upper-convected time derivative, which is usually encountered\nin the constitutive equation of the popular viscoelastic models, is\nreformulated in order to obtain approximations of second-order in time for\nsolving a simplified constitutive equation in one and two dimensions. The\ntheoretical analysis of the truncation errors of the methods takes into account\nthe linear and quadratic interpolation operators based on a Lagrangian\nframework. Numerical experiments illustrating the theoretical results for the\nmodel equation defined in one and two dimensions are included. Finally, the\nfinite difference approximations of second-order in time are also applied for\nsolving a two-dimensional Oldroyd-B constitutive equation subjected to a\nprescribed velocity field at different Weissenberg numbers.",
    "descriptor": "",
    "authors": [
      "Debora O. Medeiros",
      "Hirofumi Notsu",
      "Cassio M. Oishi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.02950"
  },
  {
    "id": "arXiv:2106.02951",
    "title": "Controller Synthesis for Omega-Regular and Steady-State Specifications",
    "abstract": "Given a Markov decision process (MDP) and a linear-time ($\\omega$-regular or\nLTL) specification, the controller synthesis problem aims to compute the\noptimal policy that satisfies the specification. More recently, problems that\nreason over the asymptotic behavior of systems have been proposed through the\nlens of steady-state planning. This entails finding a control policy for an MDP\nsuch that the Markov chain induced by the solution policy satisfies a given set\nof constraints on its steady-state distribution. This paper studies a\ngeneralization of the controller synthesis problem for a linear-time\nspecification under steady-state constraints on the asymptotic behavior. We\npresent an algorithm to find a deterministic policy satisfying $\\omega$-regular\nand steady-state constraints by characterizing the solutions as an integer\nlinear program, and experimentally evaluate our approach.",
    "descriptor": "",
    "authors": [
      "Alvaro Velasquez",
      "Ashutosh Trivedi",
      "Ismail Alkhouri",
      "Andre Beckus",
      "George Atia"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02951"
  },
  {
    "id": "arXiv:2106.02952",
    "title": "Brno Urban Dataset: Winter Extention",
    "abstract": "Research on autonomous driving is advancing dramatically and requires new\ndata and techniques to progress even further. To reflect this pressure, we\npresent an extension of our recent work - the Brno Urban Dataset (BUD). The new\ndata focus on winter conditions in various snow-covered environments and\nfeature additional LiDAR and radar sensors for object detection in front of the\nvehicle. The improvement affects the old data as well. We provide YOLO\ndetection annotations for all old RGB images in the dataset. The detections are\nfurther also transferred by our original algorithm into the infra-red (IR)\nimages, captured by the thermal camera. To our best knowledge, it makes this\ndataset the largest source of machine-annotated thermal images currently\navailable. The dataset is published under MIT license on\nhttps://github.com/Robotics-BUT/Brno-Urban-Dataset.",
    "descriptor": "",
    "authors": [
      "Adam Ligocki",
      "Ales Jelinek",
      "Ludek Zalud"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02952"
  },
  {
    "id": "arXiv:2106.02953",
    "title": "Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent  Biases",
    "abstract": "Visual search is a ubiquitous and often challenging daily task, exemplified\nby looking for the car keys at home or a friend in a crowd. An intriguing\nproperty of some classical search tasks is an asymmetry such that finding a\ntarget A among distractors B can be easier than finding B among A. To elucidate\nthe mechanisms responsible for asymmetry in visual search, we propose a\ncomputational model that takes a target and a search image as inputs and\nproduces a sequence of eye movements until the target is found. The model\nintegrates eccentricity-dependent visual recognition with target-dependent\ntop-down cues. We compared the model against human behavior in six paradigmatic\nsearch tasks that show asymmetry in humans. Without prior exposure to the\nstimuli or task-specific training, the model provides a plausible mechanism for\nsearch asymmetry. We hypothesized that the polarity of search asymmetry arises\nfrom experience with the natural environment. We tested this hypothesis by\ntraining the model on an augmented version of ImageNet where the biases of\nnatural images were either removed or reversed. The polarity of search\nasymmetry disappeared or was altered depending on the training protocol. This\nstudy highlights how classical perceptual properties can emerge in neural\nnetwork models, without the need for task-specific training, but rather as a\nconsequence of the statistical properties of the developmental diet fed to the\nmodel. All source code and stimuli are publicly available\nhttps://github.com/kreimanlab/VisualSearchAsymmetry",
    "descriptor": "",
    "authors": [
      "Shashi Kant Gupta",
      "Mengmi Zhang",
      "Chia-Chien Wu",
      "Jeremy M. Wolfe",
      "Gabriel Kreiman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.02953"
  },
  {
    "id": "arXiv:2106.02954",
    "title": "Denoising Word Embeddings by Averaging in a Shared Space",
    "abstract": "We introduce a new approach for smoothing and improving the quality of word\nembeddings. We consider a method of fusing word embeddings that were trained on\nthe same corpus but with different initializations. We project all the models\nto a shared vector space using an efficient implementation of the Generalized\nProcrustes Analysis (GPA) procedure, previously used in multilingual word\ntranslation. Our word representation demonstrates consistent improvements over\nthe raw models as well as their simplistic average, on a range of tasks. As the\nnew representations are more stable and reliable, there is a noticeable\nimprovement in rare word evaluations.",
    "descriptor": "\nComments: Accepted to *SEM 2021\n",
    "authors": [
      "Avi Caciularu",
      "Ido Dagan",
      "Jacob Goldberger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02954"
  },
  {
    "id": "arXiv:2106.02956",
    "title": "KupenStack: Kubernetes based Cloud Native OpenStack",
    "abstract": "OpenStack is an open-source private cloud used to run VMs and its related\ncloud services. OpenStack deployment, management, and upgradation require lots\nof efforts and manual troubleshooting. Also, workloads and services offered by\nOpenStack cannot self-heal itself on failures. We present KupenStack, a\nCloud-Native OpenStack as Code model built on top of Kubernetes stack as Custom\nResources. KupenStack is a controller that interacts between Kubernetes and\nOpenStack and automates complex operations like scaling, LCM, zero-downtime,\nself-healing, version upgrades, configuration management, and offers OpenStack\nas a service through code. KupenStack builds cloud-native values like immutable\ninfrastructure, declarative APIs for OpenStack without changing any OpenStack\ncode. If a VM workload goes down for some reason, then KupenStack handles it\nand automatically spins up a new instance. KupenStack uses OpenStack on\nKubernetes deployment for lifecycle management of OpenStack.",
    "descriptor": "\nComments: 11 Pages, 6 Figures\n",
    "authors": [
      "Parth Yadav",
      "Vipin Kumar Rathi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.02956"
  },
  {
    "id": "arXiv:2106.02960",
    "title": "Meta-Learning with Variational Semantic Memory for Word Sense  Disambiguation",
    "abstract": "A critical challenge faced by supervised word sense disambiguation (WSD) is\nthe lack of large annotated datasets with sufficient coverage of words in their\ndiversity of senses. This inspired recent research on few-shot WSD using\nmeta-learning. While such work has successfully applied meta-learning to learn\nnew word senses from very few examples, its performance still lags behind its\nfully supervised counterpart. Aiming to further close this gap, we propose a\nmodel of semantic memory for WSD in a meta-learning setting. Semantic memory\nencapsulates prior experiences seen throughout the lifetime of the model, which\naids better generalization in limited data settings. Our model is based on\nhierarchical variational inference and incorporates an adaptive memory update\nrule via a hypernetwork. We show our model advances the state of the art in\nfew-shot WSD, supports effective learning in extremely data scarce (e.g.\none-shot) scenarios and produces meaning prototypes that capture similar senses\nof distinct words.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Yingjun Du",
      "Nithin Holla",
      "Xiantong Zhen",
      "Cees G.M. Snoek",
      "Ekaterina Shutova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02960"
  },
  {
    "id": "arXiv:2106.02961",
    "title": "Meta-research on COVID-19: An overview of the early trends",
    "abstract": "COVID-19 is having a dramatic impact on research and researchers. The\npandemic has underlined the severity of known challenges in research and\nsurfaced new ones, but also accelerated the adoption of innovations and\nmanifested new opportunities. This review considers early trends emerging from\nmeta-research on COVID-19. In particular, it focuses on the following topics:\ni) mapping COVID-19 research; ii) data and machine learning; iii) research\npractices including open access and open data, reviewing, publishing and\nfunding; iv) communicating research to the public; v) the impact of COVID-19 on\nresearchers, in particular with respect to gender and career trajectories. This\noverview finds that most early meta-research on COVID-19 has been reactive and\nfocused on short-term questions, while more recently a shift to consider the\nlong-term consequences of COVID-19 is taking place. Based on these findings,\nthe author speculates that some aspects of doing research during COVID-19 are\nmore likely to persist than others. These include: the shift to virtual for\nacademic events such as conferences; the use of openly accessible pre-prints;\nthe `datafication' of scholarly literature and consequent broader adoption of\nmachine learning in science communication; the public visibility of research\nand researchers on social and online media.",
    "descriptor": "",
    "authors": [
      "Giovanni Colavizza"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.02961"
  },
  {
    "id": "arXiv:2106.02965",
    "title": "Extracting Weighted Automata for Approximate Minimization in Language  Modelling",
    "abstract": "In this paper we study the approximate minimization problem for language\nmodelling. We assume we are given some language model as a black box. The\nobjective is to obtain a weighted finite automaton (WFA) that fits within a\ngiven size constraint and which mimics the behaviour of the original model\nwhile minimizing some notion of distance between the black box and the\nextracted WFA. We provide an algorithm for the approximate minimization of\nblack boxes trained for language modelling of sequential data over a one-letter\nalphabet. By reformulating the problem in terms of Hankel matrices, we leverage\nclassical results on the approximation of Hankel operators, namely the\ncelebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral\nnorm to measure the distance between the black box and the WFA. We provide\ntheoretical guarantees to study the potentially infinite-rank Hankel matrix of\nthe black box, without accessing the training data, and we prove that our\nmethod returns an asymptotically-optimal approximation.",
    "descriptor": "\nComments: The names of the authors appear in alphabetical order\n",
    "authors": [
      "Clara Lacroce",
      "Prakash Panangaden",
      "Guillaume Rabusseau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.02965"
  },
  {
    "id": "arXiv:2106.02968",
    "title": "Low Budget Active Learning via Wasserstein Distance: An Integer  Programming Approach",
    "abstract": "Given restrictions on the availability of data, active learning is the\nprocess of training a model with limited labeled data by selecting a core\nsubset of an unlabeled data pool to label. Although selecting the most useful\npoints for training is an optimization problem, the scale of deep learning data\nsets forces most selection strategies to employ efficient heuristics. Instead,\nwe propose a new integer optimization problem for selecting a core set that\nminimizes the discrete Wasserstein distance from the unlabeled pool. We\ndemonstrate that this problem can be tractably solved with a Generalized\nBenders Decomposition algorithm. Our strategy requires high-quality latent\nfeatures which we obtain by unsupervised learning on the unlabeled pool.\nNumerical results on several data sets show that our optimization approach is\ncompetitive with baselines and particularly outperforms them in the low budget\nregime where less than one percent of the data set is labeled.",
    "descriptor": "",
    "authors": [
      "Rafid Mahmood",
      "Sanja Fidler",
      "Marc T. Law"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02968"
  },
  {
    "id": "arXiv:2106.02969",
    "title": "FedNL: Making Newton-Type Methods Applicable to Federated Learning",
    "abstract": "Inspired by recent work of Islamov et al (2021), we propose a family of\nFederated Newton Learn (FedNL) methods, which we believe is a marked step in\nthe direction of making second-order methods applicable to FL. In contrast to\nthe aforementioned work, FedNL employs a different Hessian learning technique\nwhich i) enhances privacy as it does not rely on the training data to be\nrevealed to the coordinating server, ii) makes it applicable beyond generalized\nlinear models, and iii) provably works with general contractive compression\noperators for compressing the local Hessians, such as Top-$K$ or Rank-$R$,\nwhich are vastly superior in practice. Notably, we do not need to rely on error\nfeedback for our methods to work with contractive compressors. Moreover, we\ndevelop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that\nsupport partial participation, and globalization via cubic regularization and\nline search, respectively, and FedNL-BC, which is a variant that can further\nbenefit from bidirectional compression of gradients and models, i.e., smart\nuplink gradient and smart downlink model compression. We prove local\nconvergence rates that are independent of the condition number, the number of\ntraining data points, and compression variance. Our communication efficient\nHessian learning technique provably learns the Hessian at the optimum. Finally,\nwe perform a variety of numerical experiments that show that our FedNL methods\nhave state-of-the-art communication complexity when compared to key baselines.",
    "descriptor": "\nComments: 63 pages, 7 algorithms, 14 figures\n",
    "authors": [
      "Mher Safaryan",
      "Rustem Islamov",
      "Xun Qian",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02969"
  },
  {
    "id": "arXiv:2106.02970",
    "title": "Modeling Coordinated vs. P2P Mining: An Analysis of Inefficiency and  Inequality in Proof-of-Work Blockchains",
    "abstract": "We study efficiency in a proof-of-work blockchain with non-zero latencies,\nfocusing in particular on the (inequality in) individual miners' efficiencies.\nPrior work attributed differences in miners' efficiencies mostly to attacks,\nbut we pursue a different question: Can inequality in miners' efficiencies be\nexplained by delays, even when all miners are honest? Traditionally, such\nefficiency-related questions were tackled only at the level of the overall\nsystem, and in a peer-to-peer (P2P) setting where miners directly connect to\none another. Despite it being common today for miners to pool compute\ncapacities in a mining pool managed by a centralized coordinator, efficiency in\nsuch a coordinated setting has barely been studied.\nIn this paper, we propose a simple model of a proof-of-work blockchain with\nlatencies for both the P2P and the coordinated settings. We derive a\nclosed-form expression for the efficiency in the coordinated setting with an\narbitrary number of miners and arbitrary latencies, both for the overall system\nand for each individual miner. We leverage this result to show that\ninequalities arise from variability in the delays, but that if all miners are\nequidistant from the coordinator, they have equal efficiency irrespective of\ntheir compute capacities. We then prove that, under a natural consistency\ncondition, the overall system efficiency in the P2P setting is higher than that\nin the coordinated setting. Finally, we perform a simulation-based study to\ndemonstrate that even in the P2P setting delays between miners introduce\ninequalities, and that there is a more complex interplay between delays and\ncompute capacities.",
    "descriptor": "\nComments: 12 pages, 11 figures\n",
    "authors": [
      "Mohamed Alzayat",
      "Johnnatan Messias",
      "Balakrishnan Chandrasekaran",
      "Krishna P. Gummadi",
      "Patrick Loiseau"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02970"
  },
  {
    "id": "arXiv:2106.02972",
    "title": "Zero-shot Task Adaptation using Natural Language",
    "abstract": "Imitation learning and instruction-following are two common approaches to\ncommunicate a user's intent to a learning agent. However, as the complexity of\ntasks grows, it could be beneficial to use both demonstrations and language to\ncommunicate with an agent. In this work, we propose a novel setting where an\nagent is given both a demonstration and a description, and must combine\ninformation from both the modalities. Specifically, given a demonstration for a\ntask (the source task), and a natural language description of the differences\nbetween the demonstrated task and a related but different task (the target\ntask), our goal is to train an agent to complete the target task in a zero-shot\nsetting, that is, without any demonstrations for the target task. To this end,\nwe introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a\nsource demonstration and a linguistic description of how the target task\ndiffers, learns to output a reward / value function that accurately describes\nthe target task. Our experiments show that on a diverse set of adaptations, our\napproach is able to complete more than 95% of target tasks when using\ntemplate-based descriptions, and more than 70% when using free-form natural\nlanguage.",
    "descriptor": "",
    "authors": [
      "Prasoon Goyal",
      "Raymond J. Mooney",
      "Scott Niekum"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02972"
  },
  {
    "id": "arXiv:2106.02973",
    "title": "Forced Variational Integrator Networks for Prediction and Control of  Mechanical Systems",
    "abstract": "As deep learning becomes more prevalent for prediction and control of real\nphysical systems, it is important that these overparameterized models are\nconsistent with physically plausible dynamics. This elicits a problem with how\nmuch inductive bias to impose on the model through known physical parameters\nand principles to reduce complexity of the learning problem to give us more\nreliable predictions. Recent work employs discrete variational integrators\nparameterized as a neural network architecture to learn conservative Lagrangian\nsystems. The learned model captures and enforces global energy preserving\nproperties of the system from very few trajectories. However, most real systems\nare inherently non-conservative and, in practice, we would also like to apply\nactuation. In this paper we extend this paradigm to account for general forcing\n(e.g. control input and damping) via discrete d'Alembert's principle which may\nultimately be used for control applications. We show that this forced\nvariational integrator networks (FVIN) architecture allows us to accurately\naccount for energy dissipation and external forcing while still capturing the\ntrue underlying energy-based passive dynamics. We show that in application this\ncan result in highly-data efficient model-based control and can predict on real\nnon-conservative systems.",
    "descriptor": "\nComments: To appear in L4DC 2021\n",
    "authors": [
      "Aaron Havens",
      "Girish Chowdhary"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02973"
  },
  {
    "id": "arXiv:2106.02974",
    "title": "Enhancing Taxonomy Completion with Concept Generation via Fusing  Relational Representations",
    "abstract": "Automatic construction of a taxonomy supports many applications in\ne-commerce, web search, and question answering. Existing taxonomy expansion or\ncompletion methods assume that new concepts have been accurately extracted and\ntheir embedding vectors learned from the text corpus. However, one critical and\nfundamental challenge in fixing the incompleteness of taxonomies is the\nincompleteness of the extracted concepts, especially for those whose names have\nmultiple words and consequently low frequency in the corpus. To resolve the\nlimitations of extraction-based methods, we propose GenTaxo to enhance taxonomy\ncompletion by identifying positions in existing taxonomies that need new\nconcepts and then generating appropriate concept names. Instead of relying on\nthe corpus for concept embeddings, GenTaxo learns the contextual embeddings\nfrom their surrounding graph-based and language-based relational information,\nand leverages the corpus for pre-training a concept name generator.\nExperimental results demonstrate that GenTaxo improves the completeness of\ntaxonomies over existing methods.",
    "descriptor": "",
    "authors": [
      "Qingkai Zeng",
      "Jinfeng Lin",
      "Wenhao Yu",
      "Jane Cleland-Huang",
      "Meng Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02974"
  },
  {
    "id": "arXiv:2106.02976",
    "title": "Fortifying Vehicular Security Through Low Overhead Physically Unclonable  Functions",
    "abstract": "Within vehicles, the Controller Area Network (CAN) allows efficient\ncommunication between the electronic control units (ECUs) responsible for\ncontrolling the various subsystems. The CAN protocol was not designed to\ninclude much support for secure communication. The fact that so many critical\nsystems can be accessed through an insecure communication network presents a\nmajor security concern. Adding security features to CAN is difficult due to the\nlimited resources available to the individual ECUs and the costs that would be\nassociated with adding the necessary hardware to support any additional\nsecurity operations without overly degrading the performance of standard\ncommunication. Replacing the protocol is another option, but it is subject to\nmany of the same problems. The lack of security becomes even more concerning as\nvehicles continue to adopt smart features. Smart vehicles have a multitude of\ncommunication interfaces would an attacker could exploit to gain access to the\nnetworks. In this work we propose a security framework that is based on\nphysically unclonable functions (PUFs) and lightweight cryptography (LWC). The\nframework does not require any modification to the standard CAN protocol while\nalso minimizing the amount of additional message overhead required for its\noperation. The improvements in our proposed framework results in major\nreduction in the number of CAN frames that must be sent during operation. For a\nsystem with 20 ECUs for example, our proposed framework only requires 6.5% of\nthe number of CAN frames that is required by the existing approach to\nsuccessfully authenticate every ECU.",
    "descriptor": "\nComments: 19 Pages\n",
    "authors": [
      "Carson Labrado",
      "Himanshu Thapliyal",
      "Saraju P. Mohanty"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2106.02976"
  },
  {
    "id": "arXiv:2106.02981",
    "title": "APMF < APSP? Gomory-Hu Tree for Unweighted Graphs in Almost-Quadratic  Time",
    "abstract": "We design an $n^{2+o(1)}$-time algorithm that constructs a cut-equivalent\n(Gomory-Hu) tree of a simple graph on $n$ nodes. This bound is almost-optimal\nin terms of $n$, and it improves on the recent $\\tilde{O}(n^{2.5})$ bound by\nthe authors (STOC 2021), which was the first to break the cubic barrier.\nConsequently, the All-Pairs Maximum-Flow (APMF) problem has time complexity\n$n^{2+o(1)}$, and for the first time in history, this problem can be solved\nfaster than All-Pairs Shortest Paths (APSP). We further observe that an\nalmost-linear time algorithm (in terms of the number of edges $m$) is not\npossible without first obtaining a subcubic algorithm for multigraphs.\nFinally, we derandomize our algorithm, obtaining the first subcubic\ndeterministic algorithm for Gomory-Hu Tree in simple graphs, showing that\nrandomness is not necessary for beating the $n-1$ times max-flow bound from\n1961. The upper bound is $\\tilde{O}(n^{2\\frac{2}{3}})$ and it would improve to\n$n^{2+o(1)}$ if there is a deterministic single-pair maximum-flow algorithm\nthat is almost-linear. The key novelty is in using a ``dynamic pivot''\ntechnique instead of the randomized pivot selection that was central in recent\nworks.",
    "descriptor": "",
    "authors": [
      "Amir Abboud",
      "Robert Krauthgamer",
      "Ohad Trabelsi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02981"
  },
  {
    "id": "arXiv:2106.02982",
    "title": "Sensor Fusion-based GNSS Spoofing Attack Detection Framework for  Autonomous Vehicles",
    "abstract": "In this study, a sensor fusion based GNSS spoofing attack detection framework\nis presented that consists of three concurrent strategies for an autonomous\nvehicle (AV): (i) prediction of location shift, (ii) detection of turns (left\nor right), and (iii) recognition of motion state (including standstill state).\nData from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering\nangle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural\nnetwork model, which is a long short-term memory (LSTM) network for predicting\nthe location shift, i.e., the distance that an AV travels between two\nconsecutive timestamps. We have then combined k-Nearest Neighbors (k-NN) and\nDynamic Time Warping (DTW) algorithms to detect turns using data from the\nsteering angle sensor. In addition, data from an AV's speed sensor is used to\nrecognize the AV's motion state including the standstill state. To prove the\nefficacy of the sensor fusion-based attack detection framework, attack datasets\nare created for three unique and sophisticated spoofing attacks turn by turn,\novershoot, and stop using the publicly available real-world Honda Research\nInstitute Driving Dataset (HDD). Our analysis reveals that the sensor\nfusion-based detection framework successfully detects all three types of\nspoofing attacks within the required computational latency threshold.",
    "descriptor": "",
    "authors": [
      "Sagar Dasgupta",
      "Mizanur Rahman",
      "Mhafuzul Islam",
      "Mashrur Chowdhury"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02982"
  },
  {
    "id": "arXiv:2106.02984",
    "title": "Design of hazard based model and collision avoidance system",
    "abstract": "The primary goal of this paper is to examine the motorcyclists' activities\nduring the overtaking period, as well as to develop a model of total overtaking\ntime. For the experimental study, instrumented motorcycles were used to collect\ndata and design an overall overtaking period model. The possibility of death\nduring an attempt to overtake is maximum for people walking on rural roads,\nmainly identified by a scarcity of pathways and higher speeds of vehicles. It\nis important to recognize and prototype driver actions during the overtaking\nmoves to set up collision avoidance strategies in order to prevent these\ncollisions, such that device adjustments are acceptable as they occur beyond\nthe comfort zone of the drivers. The purpose of this research is to address\nboth vehicle overtaking movements along urban roads and to develop a collision\navoidance system. This research, based on tests performed and instrumented\ndriving information, may lead to the discovery of advanced driver assistance\nsystems, analyzing driver behavior during overtaking. A total of 500 overtaking\nmovements were registered with 50 motorcycles set up with a high-resolution\ncamera and GPS system implemented by 50 professional bikers in India in an\nundivided one-way road. A technique was developed to collect data explaining\nthe actions of the motorcyclists, based on video and GPS analyses. The overall\novertaking period was designed using a risk-based model, which indicates the\nspan of the overtaking based on several coefficients. The proposed model is\nuseful to analyze the behavior during overtaking moves, as well as to develop\nroad and vehicle safety systems to reduce the chances of accidents.",
    "descriptor": "",
    "authors": [
      "Md Faysal Kabir",
      "Sahadev Roy"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02984"
  },
  {
    "id": "arXiv:2106.02985",
    "title": "Escaping Saddle Points Faster with Stochastic Momentum",
    "abstract": "Stochastic gradient descent (SGD) with stochastic momentum is popular in\nnonconvex stochastic optimization and particularly for the training of deep\nneural networks. In standard SGD, parameters are updated by improving along the\npath of the gradient at the current iterate on a batch of examples, where the\naddition of a ``momentum'' term biases the update in the direction of the\nprevious change in parameters. In non-stochastic convex optimization one can\nshow that a momentum adjustment provably reduces convergence time in many\nsettings, yet such results have been elusive in the stochastic and non-convex\nsettings. At the same time, a widely-observed empirical phenomenon is that in\ntraining deep networks stochastic momentum appears to significantly improve\nconvergence time, variants of it have flourished in the development of other\npopular update methods, e.g. ADAM [KB15], AMSGrad [RKK18], etc. Yet theoretical\njustification for the use of stochastic momentum has remained a significant\nopen question. In this paper we propose an answer: stochastic momentum improves\ndeep network training because it modifies SGD to escape saddle points faster\nand, consequently, to more quickly find a second order stationary point. Our\ntheoretical results also shed light on the related question of how to choose\nthe ideal momentum parameter--our analysis suggests that $\\beta \\in [0,1)$\nshould be large (close to 1), which comports with empirical findings. We also\nprovide experimental findings that further validate these conclusions.",
    "descriptor": "\nComments: Accepted at ICLR 2020\n",
    "authors": [
      "Jun-Kun Wang",
      "Chi-Heng Lin",
      "Jacob Abernethy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02985"
  },
  {
    "id": "arXiv:2106.02987",
    "title": "Analysis of a semi-augmented mixed finite element method for  double-diffusive natural convection in porous media",
    "abstract": "In this paper we study a stationary double-diffusive natural convection\nproblem in porous media given by a Navier-Stokes/Darcy type system, for\ndescribing the velocity and the pressure, coupled to a vector\nadvection-diffusion equation describing the heat and substance concentration,\nof a viscous fluid in a porous media with physical boundary conditions. The\nmodel problem is rewritten in terms of a first-order system, without the\npressure, based on the introduction of the strain tensor and a nonlinear\npseudo-stress tensor in the fluid equations. After a variational approach, the\nresulting weak model is then augmented using appropriate redundant penalization\nterms for the fluid equations along with a standard primal formulation for the\nheat and substance concentration. Then, it is rewritten as an equivalent\nfixed-point problem. Well-posedness and uniqueness results for both the\ncontinuous and the discrete schemes are stated, as well as the respective\nconvergence result under certain regularity assumptions combined with the\nLax-Milgram theorem, and the Banach and Brouwer fixed-point theorems. In\nparticular, Raviart-Thomas elements of order $k$ are used for approximating the\npseudo-stress tensor, piecewise polynomials of degree $ \\leq k$ and $\\leq k+1$\nare utilized for approximating the strain tensor and the velocity,\nrespectively, and the heat and substance concentration are approximated by\nmeans of Lagrange finite elements of order $\\leq k+1$. Optimal a priori error\nestimates are derived and confirmed through some numerical examples that\nillustrate the performance of the proposed semi-augmented mixed-primal scheme.",
    "descriptor": "",
    "authors": [
      "Mario Alvarez",
      "Eligio Colmenares",
      "Fil\u00e1nder A. Sequeira"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.02987"
  },
  {
    "id": "arXiv:2106.02989",
    "title": "Exploring the Disproportion Between Scientific Productivity and  Knowledge Amount",
    "abstract": "The pursuit of knowledge is the permanent goal of human beings. Scientific\nliterature, as the major medium that carries knowledge between scientists,\nexhibits explosive growth during the last century. Despite the frequent use of\nmany tangible measures, such as citation, impact factor and g-index, to\nquantify the influence of papers from different perspectives based on\nscientific productivity, it has not yet been well understood how the\nrelationship between scientific productivity and knowledge amount turns out to\nbe, i.e., how the knowledge value of papers and knowledge amount vary with\ndevelopment of the discipline. This raises the question of whether high\nscientific productivity equals large knowledge amount. Here, building on rich\nliterature on academic conferences and journals, we collect 185 million\narticles covering 19 disciplines published during 1970 to 2020, and establish\ncitation network research area to represent the knowledge flow from the authors\nof the article being cited to the authors of the articles that cite it under\neach specific area. As a result, the structure formed during the evolution of\neach scientific area can implicitly tells how the knowledge flows between nodes\nand how it behaves as the number of literature (productivity) increases. By\nleveraging Structural entropy in structured high-dimensional space and Shannon\nentropy in unstructured probability space, we propose the Quantitative Index of\nKnowledge (KQI), which is taken as the subtraction between the two types of\nentropy, to reflect the extent of disorder difference (knowledge amount) caused\nby structure (order). With the aid of KQI, we find that, although the published\nliterature shows an explosive growth, the amount of knowledge (KQI) contained\nin it obviously slows down, and there is a threshold after which the growth of\nknowledge accelerates...",
    "descriptor": "\nComments: Luoyi Fu and Huquan Kang contribute equally to the work. Xinbing Wang and Chenghu Zhou both are corresponding authors\n",
    "authors": [
      "Luoyi Fu",
      "Huquan Kang",
      "Jianghao Wang",
      "Ling Yao",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.02989"
  },
  {
    "id": "arXiv:2106.02990",
    "title": "Self-Damaging Contrastive Learning",
    "abstract": "The recent breakthrough achieved by contrastive learning accelerates the pace\nfor deploying unsupervised training on real-world data applications. However,\nunlabeled data in reality is commonly imbalanced and shows a long-tail\ndistribution, and it is unclear how robustly the latest contrastive learning\nmethods could perform in the practical scenario. This paper proposes to\nexplicitly tackle this challenge, via a principled framework called\nSelf-Damaging Contrastive Learning (SDCLR), to automatically balance the\nrepresentation learning without knowing the classes. Our main inspiration is\ndrawn from the recent finding that deep models have difficult-to-memorize\nsamples, and those may be exposed through network pruning. It is further\nnatural to hypothesize that long-tail samples are also tougher for the model to\nlearn well due to insufficient examples. Hence, the key innovation in SDCLR is\nto create a dynamic self-competitor model to contrast with the target model,\nwhich is a pruned version of the latter. During training, contrasting the two\nmodels will lead to adaptive online mining of the most easily forgotten samples\nfor the current target model, and implicitly emphasize them more in the\ncontrastive loss. Extensive experiments across multiple datasets and imbalance\nsettings show that SDCLR significantly improves not only overall accuracies but\nalso balancedness, in terms of linear evaluation on the full-shot and few-shot\nsettings. Our code is available at: https://github.com/VITA-Group/SDCLR.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Ziyu Jiang",
      "Tianlong Chen",
      "Bobak Mortazavi",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02990"
  },
  {
    "id": "arXiv:2106.02992",
    "title": "Distributed Task Allocation in Homogeneous Swarms Using Language Measure  Theory",
    "abstract": "In this paper, we present algorithms for synthesizing controllers to\ndistribute a group (possibly swarms) of homogeneous robots (agents) over\nheterogeneous tasks which are operated in parallel. We present algorithms as\nwell as analysis for global and local-feedback-based controller for the swarms.\nUsing ergodicity property of irreducible Markov chains, we design a controller\nfor global swarm control. Furthermore, to provide some degree of autonomy to\nthe agents, we augment this global controller by a local feedback-based\ncontroller using Language measure theory. We provide analysis of the proposed\nalgorithms to show their correctness. Numerical experiments are shown to\nillustrate the performance of the proposed algorithms.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Devesh K. Jha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02992"
  },
  {
    "id": "arXiv:2106.02993",
    "title": "PID-GAN: A GAN Framework based on a Physics-informed Discriminator for  Uncertainty Quantification with Physics",
    "abstract": "As applications of deep learning (DL) continue to seep into critical\nscientific use-cases, the importance of performing uncertainty quantification\n(UQ) with DL has become more pressing than ever before. In scientific\napplications, it is also important to inform the learning of DL models with\nknowledge of physics of the problem to produce physically consistent and\ngeneralized solutions. This is referred to as the emerging field of\nphysics-informed deep learning (PIDL). We consider the problem of developing\nPIDL formulations that can also perform UQ. To this end, we propose a novel\nphysics-informed GAN architecture, termed PID-GAN, where the knowledge of\nphysics is used to inform the learning of both the generator and discriminator\nmodels, making ample use of unlabeled data instances. We show that our proposed\nPID-GAN framework does not suffer from imbalance of generator gradients from\nmultiple loss terms as compared to state-of-the-art. We also empirically\ndemonstrate the efficacy of our proposed framework on a variety of case studies\ninvolving benchmark physics-based PDEs as well as imperfect physics. All the\ncode and datasets used in this study have been made available on this link :\nhttps://github.com/arkadaw9/PID-GAN.",
    "descriptor": "\nComments: 11 pages, 11 figures, 2 tables, Published at KDD 2021\n",
    "authors": [
      "Arka Daw",
      "M. Maruf",
      "Anuj Karpatne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02993"
  },
  {
    "id": "arXiv:2106.02994",
    "title": "Learning Topology from Synthetic Data for Unsupervised Depth Completion",
    "abstract": "We present a method for inferring dense depth maps from images and sparse\ndepth measurements by leveraging synthetic data to learn the association of\nsparse point clouds with dense natural shapes, and using the image as evidence\nto validate the predicted depth map. Our learned prior for natural shapes uses\nonly sparse depth as input, not images, so the method is not affected by the\ncovariate shift when attempting to transfer learned models from synthetic data\nto real ones. This allows us to use abundant synthetic data with ground truth\nto learn the most difficult component of the reconstruction process, which is\ntopology estimation, and use the image to refine the prediction based on\nphotometric evidence. Our approach uses fewer parameters than previous methods,\nyet, achieves the state of the art on both indoor and outdoor benchmark\ndatasets. Code available at:\nhttps://github.com/alexklwong/learning-topology-synthetic-data.",
    "descriptor": "",
    "authors": [
      "Alex Wong",
      "Safa Cicek",
      "Stefano Soatto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.02994"
  },
  {
    "id": "arXiv:2106.02996",
    "title": "About Digital Communication Methods for Visible Light Communication",
    "abstract": "The visible light communication (VLC) by LED is one of the important\ncommunication methods because LED can work as high speed and VLC sends the\ninformation by high flushing LED. We use the pulse wave modulation for the VLC\nwith LED because LED can be controlled easily by the microcontroller, which has\nthe digital output pins. At the pulse wave modulation, deciding the high and\nlow voltage by the middle voltage when the receiving signal level is amplified\nis equal to deciding it by the threshold voltage without amplification. In this\npaper, we proposed two methods that adjust the threshold value using counting\nthe slot number and measuring the signal level. The number of signal slots is\nconstant per one symbol when we use Pulse Position Modulation (PPM). If the\nnumber of received signal slots per one symbol time is less than the\ntheoretical value, that means the threshold value is higher than the optimal\nvalue. If it is more than the theoretical value, that means the threshold value\nis lower. So, we can adjust the threshold value using the number of received\nsignal slots. At the second proposed method, the average received signal level\nis not equal to the signal level because there is a ratio between the number of\nhigh slots and low slots. So, we can calculate the threshold value from the\naverage received signal level and the slot ratio. We show these performances as\nreal experiments.",
    "descriptor": "",
    "authors": [
      "Wataru Uemura",
      "Yasuhiro Fukumori",
      "Takato Hayama"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.02996"
  },
  {
    "id": "arXiv:2106.02997",
    "title": "Causal Abstractions of Neural Networks",
    "abstract": "Structural analysis methods (e.g., probing and feature attribution) are\nincreasingly important tools for neural network analysis. We propose a new\nstructural analysis method grounded in a formal theory of \\textit{causal\nabstraction} that provides rich characterizations of model-internal\nrepresentations and their roles in input/output behavior. In this method,\nneural representations are aligned with variables in interpretable causal\nmodels, and then \\textit{interchange interventions} are used to experimentally\nverify that the neural representations have the causal properties of their\naligned variables. We apply this method in a case study to analyze neural\nmodels trained on Multiply Quantified Natural Language Inference (MQNLI)\ncorpus, a highly complex NLI dataset that was constructed with a\ntree-structured natural logic causal model. We discover that a BERT-based model\nwith state-of-the-art performance successfully realizes the approximate causal\nstructure of the natural logic causal model, whereas a simpler baseline model\nfails to show any such structure, demonstrating that neural representations\nencode the compositional structure of MQNLI examples.",
    "descriptor": "",
    "authors": [
      "Atticus Geiger",
      "Hanson Lu",
      "Thomas Icard",
      "Christopher Potts"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02997"
  },
  {
    "id": "arXiv:2106.03000",
    "title": "Individually Rational Land and Neighbor Allocation: Impossibility  Results",
    "abstract": "We consider a setting in which agents are allocated land plots and they have\nadditive preferences over which plot they get and who their neighbor is.\nStrategyproofness, Pareto optimality, and individual rationality are three\nfundamental properties in economic design. We present two impossibility results\nshowing that the three properties are incompatible in this context.",
    "descriptor": "",
    "authors": [
      "Haris Aziz"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03000"
  },
  {
    "id": "arXiv:2106.03001",
    "title": "Joint Design for Simultaneously Transmitting And Reflecting (STAR) RIS  Assisted NOMA Systems",
    "abstract": "Different from traditional reflection-only reconfigurable intelligent\nsurfaces (RISs), simultaneously transmitting and reflecting RISs (STAR-RISs)\nrepresent a novel technology, which extends the \\textit{half-space} coverage to\n\\textit{full-space} coverage by simultaneously transmitting and reflecting\nincident signals. STAR-RISs provide new degrees-of-freedom (DoF) for\nmanipulating signal propagation. Motivated by the above, a novel STAR-RIS\nassisted non-orthogonal multiple access (NOMA) (STAR-RIS-NOMA) system is\nproposed in this paper. Our objective is to maximize the achievable sum rate by\njointly optimizing the decoding order, power allocation coefficients, active\nbeamforming, and transmission and reflection beamforming. However, the\nformulated problem is non-convex with intricately coupled variables. To tackle\nthis challenge, a suboptimal two-layer iterative algorithm is proposed.\nSpecifically, in the inner-layer iteration, for a given decoding order, the\npower allocation coefficients, active beamforming, transmission and reflection\nbeamforming are optimized alternatingly. For the outer-layer iteration, the\ndecoding order of NOMA users in each cluster is updated with the solutions\nobtained from the inner-layer iteration. Moreover, an efficient decoding order\ndetermination scheme is proposed based on the equivalent-combined channel\ngains. Simulation results are provided to demonstrate that the proposed\nSTAR-RSI-NOMA system, aided by our proposed algorithm, outperforms conventional\nRIS-NOMA and RIS assisted orthogonal multiple access (RIS-OMA) systems.",
    "descriptor": "",
    "authors": [
      "Jiakuo Zuo",
      "Yuanwei Liu",
      "Zhiguo Ding",
      "Lingyang Song",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03001"
  },
  {
    "id": "arXiv:2106.03002",
    "title": "Multilayer Representation and Multiscale Analysis on Data Networks",
    "abstract": "The constant increase in the complexity of data networks motivates the search\nfor strategies that make it possible to reduce current monitoring times. This\npaper shows the way in which multilayer network representation and the\napplication of multiscale analysis techniques, as applied to software-defined\nnetworks, allows for the visualization of anomalies from \"coarse views of the\nnetwork topology\". This implies the analysis of fewer data, and consequently\nthe reduction of the time that a process takes to monitor the network. The fact\nthat software-defined networks allow for the obtention of a global view of\nnetwork behavior facilitates detail recovery from affected zones detected in\nmonitoring processes. The method is evaluated by calculating the reduction\nfactor of nodes, checked during anomaly detection, with respect to the total\nnumber of nodes in the network.",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Luz Angela Aristiz\u00e1bal Q",
      "Nicol\u00e1s Toro G"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03002"
  },
  {
    "id": "arXiv:2106.03004",
    "title": "Exploring the Limits of Out-of-Distribution Detection",
    "abstract": "Near out-of-distribution detection (OOD) is a major challenge for deep neural\nnetworks. We demonstrate that large-scale pre-trained transformers can\nsignificantly improve the state-of-the-art (SOTA) on a range of near OOD tasks\nacross different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD\ndetection, we improve the AUROC from 85% (current SOTA) to more than 96% using\nVision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD\ndetection benchmark, we improve the AUROC from 66% to 77% using transformers\nand unsupervised pre-training. To further improve performance, we explore the\nfew-shot outlier exposure setting where a few examples from outlier classes may\nbe available; we show that pre-trained transformers are particularly\nwell-suited for outlier exposure, and that the AUROC of OOD detection on\nCIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,\nand 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained\ntransformers such as CLIP, we explore a new way of using just the names of\noutlier classes as a sole source of information without any accompanying\nimages, and show that this outperforms previous SOTA on standard vision OOD\nbenchmark tasks.",
    "descriptor": "\nComments: S.F. and J.R. contributed equally\n",
    "authors": [
      "Stanislav Fort",
      "Jie Ren",
      "Balaji Lakshminarayanan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03004"
  },
  {
    "id": "arXiv:2106.03010",
    "title": "An Adaptive Framework for Learning Unsupervised Depth Completion",
    "abstract": "We present a method to infer a dense depth map from a color image and\nassociated sparse depth measurements. Our main contribution lies in the design\nof an annealing process for determining co-visibility (occlusions,\ndisocclusions) and the degree of regularization to impose on the model. We show\nthat regularization and co-visibility are related via the fitness (residual) of\nmodel to data and both can be unified into a single framework to improve the\nlearning process. Our method is an adaptive weighting scheme that guides\noptimization by measuring the residual at each pixel location over each\ntraining step for (i) estimating a soft visibility mask and (ii) determining\nthe amount of regularization. We demonstrate the effectiveness our method by\napplying it to several recent unsupervised depth completion methods and\nimproving their performance on public benchmark datasets, without incurring\nadditional trainable parameters or increase in inference time. Code available\nat: https://github.com/alexklwong/adaframe-depth-completion.",
    "descriptor": "",
    "authors": [
      "Alex Wong",
      "Xiaohan Fei",
      "Byung-Woo Hong",
      "Stefano Soatto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03010"
  },
  {
    "id": "arXiv:2106.03015",
    "title": "Learning proofs for the classification of nilpotent semigroups",
    "abstract": "Machine learning is applied to find proofs, with smaller or smallest numbers\nof nodes, for the classification of 4-nilpotent semigroups.",
    "descriptor": "",
    "authors": [
      "Carlos Simpson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic (math.LO)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2106.03015"
  },
  {
    "id": "arXiv:2106.03016",
    "title": "Topological Measurement of Deep Neural Networks Using Persistent  Homology",
    "abstract": "The inner representation of deep neural networks (DNNs) is indecipherable,\nwhich makes it difficult to tune DNN models, control their training process,\nand interpret their outputs. In this paper, we propose a novel approach to\ninvestigate the inner representation of DNNs through topological data analysis\n(TDA). Persistent homology (PH), one of the outstanding methods in TDA, was\nemployed for investigating the complexities of trained DNNs. We constructed\nclique complexes on trained DNNs and calculated the one-dimensional PH of DNNs.\nThe PH reveals the combinational effects of multiple neurons in DNNs at\ndifferent resolutions, which is difficult to be captured without using PH.\nEvaluations were conducted using fully connected networks (FCNs) and networks\ncombining FCNs and convolutional neural networks (CNNs) trained on the MNIST\nand CIFAR-10 data sets. Evaluation results demonstrate that the PH of DNNs\nreflects both the excess of neurons and problem difficulty, making PH one of\nthe prominent methods for investigating the inner representation of DNNs.",
    "descriptor": "\nComments: 17 pages, 7 figures\n",
    "authors": [
      "Satoru Watanabe",
      "Hayato Yamana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03016"
  },
  {
    "id": "arXiv:2106.03018",
    "title": "Towards Logging Noisiness Theory: quality aspects to characterize  unwanted log entries",
    "abstract": "Context: Logging tasks track the system's functioning by keeping records of\nevidence that have been analyzed by monitoring and observability activities.\nFor these activities to be effective, it is necessary to consider the quality\nof the consumed information. Problem: However, the presence of noise - unwanted\ninformation - compromises the log files' quality. The noisiness of a log file\ncan be affected among other things by: (i) the wrong severity log choices, (ii)\nthe production of duplicate entries, (iii) the incompleteness of the\ninformation, (iv) the inappropriate format of the entries, (v) the amount of\ninformation generated. Objective: This work aims to broadly define the concept\nof noise in the context of logging, proposing the initial steps of Logging\nNoisiness, a theory on quality aspects to characterize unwanted log entries.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Eduardo Mendes",
      "Fabio Petrillo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03018"
  },
  {
    "id": "arXiv:2106.03020",
    "title": "Embracing Ambiguity: Shifting the Training Target of NLI Models",
    "abstract": "Natural Language Inference (NLI) datasets contain examples with highly\nambiguous labels. While many research works do not pay much attention to this\nfact, several recent efforts have been made to acknowledge and embrace the\nexistence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore\nthe option of training directly on the estimated label distribution of the\nannotators in the NLI task, using a learning loss based on this ambiguity\ndistribution instead of the gold-labels. We prepare AmbiNLI, a trial dataset\nobtained from readily available sources, and show it is possible to reduce\nChaosNLI divergence scores when finetuning on this data, a promising first step\ntowards learning how to capture linguistic ambiguity. Additionally, we show\nthat training on the same amount of data but targeting the ambiguity\ndistribution instead of gold-labels can result in models that achieve higher\nperformance and learn better representations for downstream tasks.",
    "descriptor": "\nComments: Accepted to ACL 2021\n",
    "authors": [
      "Johannes Mario Meissner",
      "Napat Thumwanit",
      "Saku Sugawara",
      "Akiko Aizawa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03020"
  },
  {
    "id": "arXiv:2106.03021",
    "title": "SADRNet: Self-Aligned Dual Face Regression Networks for Robust 3D Dense  Face Alignment and Reconstruction",
    "abstract": "Three-dimensional face dense alignment and reconstruction in the wild is a\nchallenging problem as partial facial information is commonly missing in\noccluded and large pose face images. Large head pose variations also increase\nthe solution space and make the modeling more difficult. Our key idea is to\nmodel occlusion and pose to decompose this challenging task into several\nrelatively more manageable subtasks. To this end, we propose an end-to-end\nframework, termed as Self-aligned Dual face Regression Network (SADRNet), which\npredicts a pose-dependent face, a pose-independent face. They are combined by\nan occlusion-aware self-alignment to generate the final 3D face. Extensive\nexperiments on two popular benchmarks, AFLW2000-3D and Florence, demonstrate\nthat the proposed method achieves significant superior performance over\nexisting state-of-the-art methods.",
    "descriptor": "\nComments: To appear in IEEE Transactions on Image Processing. Code and model is available at this https URL\n",
    "authors": [
      "Zeyu Ruan",
      "Changqing Zou",
      "Longhai Wu",
      "Gangshan Wu",
      "Limin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03021"
  },
  {
    "id": "arXiv:2106.03027",
    "title": "Boosting a Model Zoo for Multi-Task and Continual Learning",
    "abstract": "Leveraging data from multiple tasks, either all at once, or incrementally, to\nlearn one model is an idea that lies at the heart of multi-task and continual\nlearning methods. Ideally, such a model predicts each task more accurately than\nif the task were trained in isolation. We show using tools in statistical\nlearning theory (i) how tasks can compete for capacity, i.e., including a\nparticular task can deteriorate the accuracy on a given task, and (ii) that the\nideal set of tasks that one should train together in order to perform well on a\ngiven task is different for different tasks. We develop methods to discover\nsuch competition in typical benchmark datasets which suggests that the\nprevalent practice of training with all tasks leaves performance on the table.\nThis motivates our \"Model Zoo\", which is a boosting-based algorithm that builds\nan ensemble of models, each of which is very small, and it is trained on a\nsmaller set of tasks. Model Zoo achieves large gains in prediction accuracy\ncompared to state-of-the-art methods across a variety of existing benchmarks in\nmulti-task and continual learning, as well as more challenging ones of our\ncreation. We also show that even a model trained independently on all tasks\noutperforms all existing multi-task and continual learning methods.",
    "descriptor": "",
    "authors": [
      "Rahul Ramesh",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03027"
  },
  {
    "id": "arXiv:2106.03028",
    "title": "Collaborative Causal Discovery with Atomic Interventions",
    "abstract": "We introduce a new Collaborative Causal Discovery problem, through which we\nmodel a common scenario in which we have multiple independent entities each\nwith their own causal graph, and the goal is to simultaneously learn all these\ncausal graphs. We study this problem without the causal sufficiency assumption,\nusing Maximal Ancestral Graphs (MAG) to model the causal graphs, and assuming\nthat we have the ability to actively perform independent single vertex (or\natomic) interventions on the entities. If the $M$ underlying (unknown) causal\ngraphs of the entities satisfy a natural notion of clustering, we give\nalgorithms that leverage this property and recovers all the causal graphs using\nroughly logarithmic in $M$ number of atomic interventions per entity. These are\nsignificantly fewer than $n$ atomic interventions per entity required to learn\neach causal graph separately, where $n$ is the number of observable nodes in\nthe causal graph. We complement our results with a lower bound and discuss\nvarious extensions of our collaborative setting.",
    "descriptor": "",
    "authors": [
      "Raghavendra Addanki",
      "Shiva Prasad Kasiviswanathan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03028"
  },
  {
    "id": "arXiv:2106.03029",
    "title": "Planning Multimodal Exploratory Actions for Online Robot Attribute  Learning",
    "abstract": "Robots frequently need to perceive object attributes, such as \"red,\" \"heavy,\"\nand \"empty,\" using multimodal exploratory actions, such as \"look,\" \"lift,\" and\n\"shake.\" Robot attribute learning algorithms aim to learn an observation model\nfor each perceivable attribute given an exploratory action. Once the attribute\nmodels are learned, they can be used to identify attributes of new objects,\nanswering questions, such as \"Is this object red and empty?\" Attribute learning\nand identification are being treated as two separate problems in the\nliterature. In this paper, we first define a new problem called online robot\nattribute learning (On-RAL), where the robot works on attribute learning and\nattribute identification simultaneously. Then we develop an algorithm called\ninformation-theoretic reward shaping (ITRS) that actively addresses the\ntrade-off between exploration and exploitation in On-RAL problems. ITRS was\ncompared with competitive robot attribute learning baselines, and experimental\nresults demonstrate ITRS' superiority in learning efficiency and identification\naccuracy.",
    "descriptor": "\nComments: To be published in Robotics: Science and Systems (RSS), July 12-16, 2021\n",
    "authors": [
      "Xiaohan Zhang",
      "Jivko Sinapov",
      "Shiqi Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03029"
  },
  {
    "id": "arXiv:2106.03031",
    "title": "Do Grammatical Error Correction Models Realize Grammatical  Generalization?",
    "abstract": "There has been an increased interest in data generation approaches to\ngrammatical error correction (GEC) using pseudo data. However, these approaches\nsuffer from several issues that make them inconvenient for real-world\ndeployment including a demand for large amounts of training data. On the other\nhand, some errors based on grammatical rules may not necessarily require a\nlarge amount of data if GEC models can realize grammatical generalization. This\nstudy explores to what extent GEC models generalize grammatical knowledge\nrequired for correcting errors. We introduce an analysis method using synthetic\nand real GEC datasets with controlled vocabularies to evaluate whether models\ncan generalize to unseen errors. We found that a current standard\nTransformer-based GEC model fails to realize grammatical generalization even in\nsimple settings with limited vocabulary and syntax, suggesting that it lacks\nthe generalization ability required to correct errors from provided training\nexamples.",
    "descriptor": "\nComments: ACL 2021 (Findings)\n",
    "authors": [
      "Masato Mita",
      "Hitomi Yanaka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03031"
  },
  {
    "id": "arXiv:2106.03033",
    "title": "Graph Belief Propagation Networks",
    "abstract": "With the wide-spread availability of complex relational data, semi-supervised\nnode classification in graphs has become a central machine learning problem.\nGraph neural networks are a recent class of easy-to-train and accurate methods\nfor this problem that map the features in the neighborhood of a node to its\nlabel, but they ignore label correlation during inference and their predictions\nare difficult to interpret. On the other hand, collective classification is a\ntraditional approach based on interpretable graphical models that explicitly\nmodel label correlations. Here, we introduce a model that combines the\nadvantages of these two approaches, where we compute the marginal probabilities\nin a conditional random field, similar to collective classification, and the\npotentials in the random field are learned through end-to-end training, akin to\ngraph neural networks. In our model, potentials on each node only depend on\nthat node's features, and edge potentials are learned via a coupling matrix.\nThis structure enables simple training with interpretable parameters, scales to\nlarge networks, naturally incorporates training labels at inference, and is\noften more accurate than related approaches. Our approach can be viewed as\neither an interpretable message-passing graph neural network or a collective\nclassification method with higher capacity and modernized training.",
    "descriptor": "",
    "authors": [
      "Junteng Jia",
      "Cenk Baykal",
      "Vamsi K. Potluru",
      "Austin R. Benson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03033"
  },
  {
    "id": "arXiv:2106.03036",
    "title": "Real-Time Cognitive Evaluation of Online Learners through Automatically  Generated Questions",
    "abstract": "With the increased adoption of E-learning platforms, keeping online learners\nengaged throughout a lesson is challenging. One approach to tackle this\nchallenge is to probe learn-ers periodically by asking questions. The paper\npresents an approach to generate questions from a given video lecture\nautomatically. The generated questions are aimed to evaluate learners'\nlower-level cognitive abilities. The approach automatically extracts text from\nvideo lectures to generates wh-kinds of questions. When learners respond with\nan answer, the proposed approach further evaluates the response and provides\nfeedback. Besides enhancing learner's engagement, this approach's main benefits\nare that it frees instructors from design-ing questions to check the\ncomprehension of a topic. Thus, instructors can spend this time productively on\nother activities.",
    "descriptor": "",
    "authors": [
      "Ritu Gala",
      "Revathi Vijayaraghavan",
      "Valmik Nikam",
      "Arvind Kiwelekar"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03036"
  },
  {
    "id": "arXiv:2106.03039",
    "title": "Multi-facet Contextual Bandits: A Neural Network Perspective",
    "abstract": "Contextual multi-armed bandit has shown to be an effective tool in\nrecommender systems. In this paper, we study a novel problem of multi-facet\nbandits involving a group of bandits, each characterizing the users' needs from\none unique aspect. In each round, for the given user, we need to select one arm\nfrom each bandit, such that the combination of all arms maximizes the final\nreward. This problem can find immediate applications in E-commerce, healthcare,\netc. To address this problem, we propose a novel algorithm, named MuFasa, which\nutilizes an assembled neural network to jointly learn the underlying reward\nfunctions of multiple bandits. It estimates an Upper Confidence Bound (UCB)\nlinked with the expected reward to balance between exploitation and\nexploration. Under mild assumptions, we provide the regret analysis of MuFasa.\nIt can achieve the near-optimal $\\widetilde{ \\mathcal{O}}((K+1)\\sqrt{T})$\nregret bound where $K$ is the number of bandits and $T$ is the number of played\nrounds. Furthermore, we conduct extensive experiments to show that MuFasa\noutperforms strong baselines on real-world data sets.",
    "descriptor": "\nComments: KDD'21, 11 pages\n",
    "authors": [
      "Yikun Ban",
      "Jingrui He",
      "Curtiss B. Cook"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03039"
  },
  {
    "id": "arXiv:2106.03040",
    "title": "Discovery of Layered Software Architecture from Source Code Using Ego  Networks",
    "abstract": "Software architecture refers to the high-level abstraction of a system\nincluding the configuration of the involved elements and the interactions and\nrelationships that exist between them. Source codes can be easily built by\nreferring to the software architectures. However, the reverse process i.e.\nderivation of the software architecture from the source code is a challenging\ntask. Further, such an architecture consists of multiple layers, and\ndistributing the existing elements into these layers should be done accurately\nand efficiently. In this paper, a novel approach is presented for the recovery\nof layered architectures from Java-based software systems using the concept of\nego networks. Ego networks have traditionally been used for social network\nanalysis, but in this paper, they are modified in a particular way and tuned to\nsuit the mentioned task. Specifically, a dependency network is extracted from\nthe source code to create an ego network. The ego network is processed to\ncreate and optimize ego layers in a particular structure. These ego layers when\nintegrated and optimized together give the final layered architecture. The\nproposed approach is evaluated in two ways: on static versions of three\nopen-source software, and a continuously evolving software system. The\ndistribution of nodes amongst the proposed layers and the committed violations\nare observed on both class level and package level. The proposed method is seen\nto outperform the existing standard approaches over multiple performance\nmetrics. We also carry out the analysis of variation in the results concerning\nthe change in the node selection strategy and the frequency. The empirical\nobservations show promising signs for recovering software architecture layers\nfrom source codes using this technique and also extending it further to other\nlanguages and software.",
    "descriptor": "\nComments: Reviewed in ICSE 2021\n",
    "authors": [
      "Sanjay Thakare",
      "Arvind W Kiwelekar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03040"
  },
  {
    "id": "arXiv:2106.03041",
    "title": "DAMSL: Domain Agnostic Meta Score-based Learning",
    "abstract": "In this paper, we propose Domain Agnostic Meta Score-based Learning (DAMSL),\na novel, versatile and highly effective solution that delivers significant\nout-performance over state-of-the-art methods for cross-domain few-shot\nlearning. We identify key problems in previous meta-learning methods\nover-fitting to the source domain, and previous transfer-learning methods\nunder-utilizing the structure of the support set. The core idea behind our\nmethod is that instead of directly using the scores from a fine-tuned feature\nencoder, we use these scores to create input coordinates for a domain agnostic\nmetric space. A graph neural network is applied to learn an embedding and\nrelation function over these coordinates to process all information contained\nin the score distribution of the support set. We test our model on both\nestablished CD-FSL benchmarks and new domains and show that our method\novercomes the limitations of previous meta-learning and transfer-learning\nmethods to deliver substantial improvements in accuracy across both smaller and\nlarger domain shifts.",
    "descriptor": "\nComments: Accepted to CVPR 2021 L2ID Workshop\n",
    "authors": [
      "John Cai",
      "Bill Cai",
      "Shengmei Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03041"
  },
  {
    "id": "arXiv:2106.03042",
    "title": "Clone-Seeker: Effective Code Clone Search Using Annotations",
    "abstract": "Source code search plays an important role in software development, e.g. for\nexploratory development or opportunistic reuse of existing code from a code\nbase. Often, exploration of different implementations with the same\nfunctionality is needed for tasks like automated software transplantation,\nsoftware diversification, and software repair. Code clones, which are\nsyntactically or semantically similar code fragments, are perfect candidates\nfor such tasks. Searching for code clones involves a given search query to\nretrieve the relevant code fragments. We propose a novel approach called\nClone-Seeker that focuses on utilizing clone class features in retrieving code\nclones. For this purpose, we generate metadata for each code clone in the form\nof a natural language document. The metadata includes a pre-processed list of\nidentifiers from the code clones augmented with a list of keywords indicating\nthe semantics of the code clone. This keyword list can be extracted from a\nmanually annotated general description of the clone class, or automatically\ngenerated from the source code of the entire clone class. This approach helps\ndevelopers to perform code clone search based on a search query written either\nas source code terms, or as natural language. In our quantitative evaluation,\nwe show that (1) Clone-Seeker has a higher recall when searching for semantic\ncode clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; and (2)\nClone-Seeker can accurately search for relevant code clones by applying natural\nlanguage queries.",
    "descriptor": "",
    "authors": [
      "Muhammad Hammad",
      "\u00d6nder Babur",
      "Hamid Abdul Basit",
      "Mark van den Brand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03042"
  },
  {
    "id": "arXiv:2106.03043",
    "title": "Occlusion-aware Unsupervised Learning of Depth from 4-D Light Fields",
    "abstract": "Depth estimation is a fundamental issue in 4-D light field processing and\nanalysis. Although recent supervised learning-based light field depth\nestimation methods have significantly improved the accuracy and efficiency of\ntraditional optimization-based ones, these methods rely on the training over\nlight field data with ground-truth depth maps which are challenging to obtain\nor even unavailable for real-world light field data. Besides, due to the\ninevitable gap (or domain difference) between real-world and synthetic data,\nthey may suffer from serious performance degradation when generalizing the\nmodels trained with synthetic data to real-world data. By contrast, we propose\nan unsupervised learning-based method, which does not require ground-truth\ndepth as supervision during training. Specifically, based on the basic\nknowledge of the unique geometry structure of light field data, we present an\nocclusion-aware strategy to improve the accuracy on occlusion areas, in which\nwe explore the angular coherence among subsets of the light field views to\nestimate initial depth maps, and utilize a constrained unsupervised loss to\nlearn their corresponding reliability for final depth prediction. Additionally,\nwe adopt a multi-scale network with a weighted smoothness loss to handle the\ntextureless areas. Experimental results on synthetic data show that our method\ncan significantly shrink the performance gap between the previous unsupervised\nmethod and supervised ones, and produce depth maps with comparable accuracy to\ntraditional methods with obviously reduced computational cost. Moreover,\nexperiments on real-world datasets show that our method can avoid the domain\nshift problem presented in supervised methods, demonstrating the great\npotential of our method.",
    "descriptor": "\nComments: 10 pages, 9 figures\n",
    "authors": [
      "Jing Jin",
      "Junhui Hou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03043"
  },
  {
    "id": "arXiv:2106.03044",
    "title": "Emotion-aware Chat Machine: Automatic Emotional Response Generation for  Human-like Emotional Interaction",
    "abstract": "The consistency of a response to a given post at semantic-level and\nemotional-level is essential for a dialogue system to deliver human-like\ninteractions. However, this challenge is not well addressed in the literature,\nsince most of the approaches neglect the emotional information conveyed by a\npost while generating responses. This article addresses this problem by\nproposing a unifed end-to-end neural architecture, which is capable of\nsimultaneously encoding the semantics and the emotions in a post for generating\nmore intelligent responses with appropriately expressed emotions. Extensive\nexperiments on real-world data demonstrate that the proposed method outperforms\nthe state-of-the-art methods in terms of both content coherence and emotion\nappropriateness.",
    "descriptor": "\nComments: Accepted at CIKM 2019. arXiv admin note: substantial text overlap with arXiv:2011.07432\n",
    "authors": [
      "Wei Wei",
      "Jiayi Liu",
      "Xianling Mao",
      "Guibing Guo",
      "Feida Zhu",
      "Pan Zhou",
      "Yuchong Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03044"
  },
  {
    "id": "arXiv:2106.03046",
    "title": "Empowering Language Understanding with Counterfactual Reasoning",
    "abstract": "Present language understanding methods have demonstrated extraordinary\nability of recognizing patterns in texts via machine learning. However,\nexisting methods indiscriminately use the recognized patterns in the testing\nphase that is inherently different from us humans who have counterfactual\nthinking, e.g., to scrutinize for the hard testing samples. Inspired by this,\nwe propose a Counterfactual Reasoning Model, which mimics the counterfactual\nthinking by learning from few counterfactual samples. In particular, we devise\na generation module to generate representative counterfactual samples for each\nfactual sample, and a retrospective module to retrospect the model prediction\nby comparing the counterfactual and factual samples. Extensive experiments on\nsentiment analysis (SA) and natural language inference (NLI) validate the\neffectiveness of our method.",
    "descriptor": "\nComments: Accepted by Findings of ACL'21\n",
    "authors": [
      "Fuli Feng",
      "Jizhi Zhang",
      "Xiangnan He",
      "Hanwang Zhang",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03046"
  },
  {
    "id": "arXiv:2106.03048",
    "title": "How Did This Get Funded?! Automatically Identifying Quirky Scientific  Achievements",
    "abstract": "Humor is an important social phenomenon, serving complex social and\npsychological functions. However, despite being studied for millennia humor is\ncomputationally not well understood, often considered an AI-complete problem.\nIn this work, we introduce a novel setting in humor mining: automatically\ndetecting funny and unusual scientific papers. We are inspired by the Ig Nobel\nprize, a satirical prize awarded annually to celebrate funny scientific\nachievements (example past winner: \"Are cows more likely to lie down the longer\nthey stand?\"). This challenging task has unique characteristics that make it\nparticularly suitable for automatic learning. We construct a dataset containing\nthousands of funny papers and use it to learn classifiers, combining findings\nfrom psychology and linguistics with recent advances in NLP. We use our models\nto identify potentially funny papers in a large dataset of over 630,000\narticles. The results demonstrate the potential of our methods, and more\nbroadly the utility of integrating state-of-the-art NLP methods with insights\nfrom more traditional disciplines.",
    "descriptor": "\nComments: To be published in the main conference of ACL-IJCNLP2021. Code and dataset can be found here: this https URL\n",
    "authors": [
      "Chen Shani",
      "Nadav Borenstein",
      "Dafna Shahaf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03048"
  },
  {
    "id": "arXiv:2106.03049",
    "title": "Embedded vs. External Controllers in Software-Defined IoT Networks",
    "abstract": "The flexible and programmable architectural model offered by Software-Defined\nNetworking (SDN) has re-imagined modern networks. Supported by powerful\nhardware and high-speed communications between devices and the controller, SDN\nprovides a means to virtualize control functionality and enable rapid network\nreconfiguration in response to dynamic application requirements. However,\nrecent efforts to apply SDN's centralized control model to the Internet of\nThings (IoT) have identified significant challenges due to the constraints\nfaced by embedded low-power devices and networks that reside at the IoT edge.\nIn particular, reliance on external SDN controllers on the backbone network\nintroduces a performance bottleneck (e.g., latency). To this end, we advocate a\ncase for supporting Software-Defined IoT networks through the introduction of\nlightweight SDN controllers directly on the embedded hardware. We firstly\nexplore the performance of two popular SDN implementations for IoT mesh\nnetworks, $\\mu$SDN and SDN-WISE, showing the former demonstrates considerable\ngains over the latter. We consequently employ $\\mu$SDN to conduct a study of\nembedded vs. external SDN controller performance. We highlight how the\nadvantage of an embedded controller is reduced as the network scales, and\nquantify a point at which an external controller should be used for larger\nnetworks.",
    "descriptor": "",
    "authors": [
      "Miheer Kulkarni",
      "Michael Baddeley",
      "Israat Haque"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03049"
  },
  {
    "id": "arXiv:2106.03050",
    "title": "Efficient Continuous Control with Double Actors and Regularized Critics",
    "abstract": "How to obtain good value estimation is one of the key problems in\nReinforcement Learning (RL). Current value estimation methods, such as DDPG and\nTD3, suffer from unnecessary over- or underestimation bias. In this paper, we\nexplore the potential of double actors, which has been neglected for a long\ntime, for better value function estimation in continuous setting. First, we\nuncover and demonstrate the bias alleviation property of double actors by\nbuilding double actors upon single critic and double critics to handle\noverestimation bias in DDPG and underestimation bias in TD3 respectively. Next,\nwe interestingly find that double actors help improve the exploration ability\nof the agent. Finally, to mitigate the uncertainty of value estimate from\ndouble critics, we further propose to regularize the critic networks under\ndouble actors architecture, which gives rise to Double Actors Regularized\nCritics (DARC) algorithm. Extensive experimental results on challenging\ncontinuous control tasks show that DARC significantly outperforms\nstate-of-the-art methods with higher sample efficiency.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Jiafei Lyu",
      "Xiaoteng Ma",
      "Jiangpeng Yan",
      "Xiu Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03050"
  },
  {
    "id": "arXiv:2106.03051",
    "title": "ScheduleNet: Learn to solve multi-agent scheduling problems with  reinforcement learning",
    "abstract": "We propose ScheduleNet, a RL-based real-time scheduler, that can solve\nvarious types of multi-agent scheduling problems. We formulate these problems\nas a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a\ndecentralized decision-making policy that can effectively coordinate multiple\nagents to complete tasks. The decision making procedure of ScheduleNet\nincludes: (1) representing the state of a scheduling problem with the\nagent-task graph, (2) extracting node embeddings for agent and tasks nodes, the\nimportant relational information among agents and tasks, by employing the\ntype-aware graph attention (TGA), and (3) computing the assignment probability\nwith the computed node embeddings. We validate the effectiveness of ScheduleNet\nas a general learning-based scheduler for solving various types of multi-agent\nscheduling tasks, including multiple salesman traveling problem (mTSP) and job\nshop scheduling problem (JSP).",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Junyoung Park",
      "Sanjar Bakhtiyar",
      "Jinkyoo Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03051"
  },
  {
    "id": "arXiv:2106.03058",
    "title": "Approximate Graph Propagation",
    "abstract": "Efficient computation of node proximity queries such as transition\nprobabilities, Personalized PageRank, and Katz are of fundamental importance in\nvarious graph mining and learning tasks. In particular, several recent works\nleverage fast node proximity computation to improve the scalability of Graph\nNeural Networks (GNN). However, prior studies on proximity computation and GNN\nfeature propagation are on a case-by-case basis, with each paper focusing on a\nparticular proximity measure.\nIn this paper, we propose Approximate Graph Propagation (AGP), a unified\nrandomized algorithm that computes various proximity queries and GNN feature\npropagation, including transition probabilities, Personalized PageRank, heat\nkernel PageRank, Katz, SGC, GDC, and APPNP. Our algorithm provides a\ntheoretical bounded error guarantee and runs in almost optimal time complexity.\nWe conduct an extensive experimental study to demonstrate AGP's effectiveness\nin two concrete applications: local clustering with heat kernel PageRank and\nnode classification with GNNs. Most notably, we present an empirical study on a\nbillion-edge graph Papers100M, the largest publicly available GNN dataset so\nfar. The results show that AGP can significantly improve various existing GNN\nmodels' scalability without sacrificing prediction accuracy.",
    "descriptor": "\nComments: ACM SIGKDD 2021\n",
    "authors": [
      "Hanzhi Wang",
      "Mingguo He",
      "Zhewei Wei",
      "Sibo Wang",
      "Ye Yuan",
      "Xiaoyong Du",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03058"
  },
  {
    "id": "arXiv:2106.03060",
    "title": "Big-Five, MPTI, Eysenck or HEXACO: The Ideal Personality Model for  Personality-aware Recommendation Systems",
    "abstract": "Personality-aware recommendation systems have been proven to achieve high\naccuracy compared to conventional recommendation systems. In addition to that,\npersonality-aware recommendation systems could help alleviate cold start and\ndata sparsity problems. Most of the existing works use Big-Five personality\nmodel to represent the user's personality, this is due to the popularity of\nBig-Five model in the literature of psychology. However, from personality\ncomputing perspective, the choice of the most suitable personality model that\nsatisfy the requirements of the recommendation application and the recommended\ncontent type still needs further investigation. In this paper, we study and\ncompare four personality-aware recommendation systems based on different\npersonality models, namely Big-Five, Eysenck and HEXACO from the personality\ntraits theory, and Myers-Briggs Type Indicator (MPTI) from the personality\ntypes theory. Following that, we propose a hybrid personality model for\nrecommendation that takes advantage of the personality traits models, as well\nas the personality types models. Through extensive experiments on\nrecommendation dataset, we prove the efficiency of the proposed model,\nespecially in cold start settings.",
    "descriptor": "",
    "authors": [
      "Sahraoui Dhelim",
      "Liming Luke Chen",
      "Nyothiri Aung",
      "Wenyin Zhang",
      "Huansheng Ning"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03060"
  },
  {
    "id": "arXiv:2106.03062",
    "title": "On Training Sample Memorization: Lessons from Benchmarking Generative  Modeling with a Large-scale Competition",
    "abstract": "Many recent developments on generative models for natural images have relied\non heuristically-motivated metrics that can be easily gamed by memorizing a\nsmall sample from the true distribution or training a model directly to improve\nthe metric. In this work, we critically evaluate the gameability of these\nmetrics by designing and deploying a generative modeling competition. Our\ncompetition received over 11000 submitted models. The competitiveness between\nparticipants allowed us to investigate both intentional and unintentional\nmemorization in generative modeling. To detect intentional memorization, we\npropose the ``Memorization-Informed Fr\\'echet Inception Distance'' (MiFID) as a\nnew memorization-aware metric and design benchmark procedures to ensure that\nwinning submissions made genuine improvements in perceptual quality.\nFurthermore, we manually inspect the code for the 1000 top-performing models to\nunderstand and label different forms of memorization. Our analysis reveals that\nunintentional memorization is a serious and common issue in popular generative\nmodels. The generated images and our memorization labels of those models as\nwell as code to compute MiFID are released to facilitate future studies on\nbenchmarking generative models.",
    "descriptor": "\nComments: In Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), August 2021\n",
    "authors": [
      "Ching-Yuan Bai",
      "Hsuan-Tien Lin",
      "Colin Raffel",
      "Wendy Chih-wen Kan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03062"
  },
  {
    "id": "arXiv:2106.03064",
    "title": "Using GANs to Augment Data for Cloud Image Segmentation Task",
    "abstract": "While cloud/sky image segmentation has extensive real-world applications, a\nlarge amount of labelled data is needed to train a highly accurate models to\nperform the task. Scarcity of such volumes of cloud/sky images with\ncorresponding ground-truth binary maps makes it highly difficult to train such\ncomplex image segmentation models. In this paper, we demonstrate the\neffectiveness of using Generative Adversarial Networks (GANs) to generate data\nto augment the training set in order to increase the prediction accuracy of\nimage segmentation model. We further present a way to estimate ground-truth\nbinary maps for the GAN-generated images to facilitate their effective use as\naugmented images. Finally, we validate our work with different statistical\ntechniques.",
    "descriptor": "\nComments: Published in IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2021\n",
    "authors": [
      "Mayank Jain",
      "Conor Meegan",
      "Soumyabrata Dev"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.03064"
  },
  {
    "id": "arXiv:2106.03065",
    "title": "Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues",
    "abstract": "In this paper, we propose to combine pretrained language models with the\nmodular dialogue paradigm for open-domain dialogue modeling. Our method,\nsemantic-enhanced finetuning, instantiates conversation understanding,\nplanning, and response generation as a language model finetuning task. At\ninference, we disentangle semantic and token variations by specifying sampling\nmethods and constraints for each module separately. For training and\nevaluation, we present X-Weibo, a Chinese multi-turn open-domain dialogue\ndataset with automatic annotation for emotions, DAs, and topical words.\nExperiments show that semantic-enhanced finetuning outperforms strong baselines\non non-semantic and semantic metrics, improves the human-evaluated relevance,\ncoherence, and informativeness, and exhibits considerable controllability over\nsemantic variables.",
    "descriptor": "",
    "authors": [
      "Chen Henry Wu",
      "Yinhe Zheng",
      "Yida Wang",
      "Zhenyu Yang",
      "Minlie Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03065"
  },
  {
    "id": "arXiv:2106.03066",
    "title": "Efficient Secure State Estimation against Sparse Integrity Attack for  System with Non-derogatory Dynamics",
    "abstract": "We consider the problem of estimating the state of a time-invariant linear\nGaussian system in the presence of integrity attacks. The attacker can\ncompromise $p$ out of $m$ sensors, the set of which is fixed over time and\nunknown to the system operator, and manipulate the measurements arbitrarily.\nUnder the assumption that all the unstable eigenvalues of system matrix $A$\nhave geometric multiplicity 1 (unstable part of $A$ is non-derogatory), we\npropose a secure estimation scheme that is resilient to integrity attack as\nlong as the system is $2p$-sparse detectable, which is proved to be the\nfundamental limit of secure dynamic estimation. In the absence of attack, the\nproposed estimation coincides with Kalman estimation with a certain probability\nthat can be adjusted to trade-off between performance with and without attack.\nFurthermore, the detectability condition checking in the designing phase and\nthe estimation computing in the online operating phase are both computationally\nefficient. A numerical example is provided to corroborate the results and\nillustrate the performance of the proposed estimator.",
    "descriptor": "\nComments: 21 pages, 5 figures. arXiv admin note: text overlap with arXiv:2105.06064\n",
    "authors": [
      "Zishuo Li",
      "Yilin Mo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03066"
  },
  {
    "id": "arXiv:2106.03069",
    "title": "Multi-Level Graph Encoding with Structural-Collaborative Relation  Learning for Skeleton-Based Person Re-Identification",
    "abstract": "Skeleton-based person re-identification (Re-ID) is an emerging open topic\nproviding great value for safety-critical applications. Existing methods\ntypically extract hand-crafted features or model skeleton dynamics from the\ntrajectory of body joints, while they rarely explore valuable relation\ninformation contained in body structure or motion. To fully explore body\nrelations, we construct graphs to model human skeletons from different levels,\nand for the first time propose a Multi-level Graph encoding approach with\nStructural-Collaborative Relation learning (MG-SCR) to encode discriminative\ngraph features for person Re-ID. Specifically, considering that\nstructurally-connected body components are highly correlated in a skeleton, we\nfirst propose a multi-head structural relation layer to learn different\nrelations of neighbor body-component nodes in graphs, which helps aggregate key\ncorrelative features for effective node representations. Second, inspired by\nthe fact that body-component collaboration in walking usually carries\nrecognizable patterns, we propose a cross-level collaborative relation layer to\ninfer collaboration between different level components, so as to capture more\ndiscriminative skeleton graph features. Finally, to enhance graph dynamics\nencoding, we propose a novel self-supervised sparse sequential prediction task\nfor model pre-training, which facilitates encoding high-level graph semantics\nfor person Re-ID. MG-SCR outperforms state-of-the-art skeleton-based methods,\nand it achieves superior performance to many multi-modal methods that utilize\nextra RGB or depth features. Our codes are available at\nhttps://github.com/Kali-Hac/MG-SCR.",
    "descriptor": "\nComments: Accepted at IJCAI 2021 Main Track. Sole copyright holder is IJCAI. Codes are available at this https URL\n",
    "authors": [
      "Haocong Rao",
      "Shihao Xu",
      "Xiping Hu",
      "Jun Cheng",
      "Bin Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03069"
  },
  {
    "id": "arXiv:2106.03075",
    "title": "DL-DDA -- Deep Learning based Dynamic Difficulty Adjustment with UX and  Gameplay constraints",
    "abstract": "Dynamic difficulty adjustment ($DDA$) is a process of automatically changing\na game difficulty for the optimization of user experience. It is a vital part\nof almost any modern game. Most existing DDA approaches concentrate on the\nexperience of a player without looking at the rest of the players. We propose a\nmethod that automatically optimizes user experience while taking into\nconsideration other players and macro constraints imposed by the game. The\nmethod is based on deep neural network architecture that involves a count loss\nconstraint that has zero gradients in most of its support. We suggest a method\nto optimize this loss function and provide theoretical analysis for its\nperformance. Finally, we provide empirical results of an internal experiment\nthat was done on $200,000$ players and was found to outperform the\ncorresponding manual heuristics crafted by game design experts.",
    "descriptor": "\nComments: accepted to IEEE Conference on Games (CoG), 2021\n",
    "authors": [
      "Dvir Ben Or",
      "Michael Kolomenkin",
      "Gil Shabat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.03075"
  },
  {
    "id": "arXiv:2106.03076",
    "title": "Complexity Analysis of Stein Variational Gradient Descent Under  Talagrand's Inequality T1",
    "abstract": "We study the complexity of Stein Variational Gradient Descent (SVGD), which\nis an algorithm to sample from $\\pi(x) \\propto \\exp(-F(x))$ where $F$ smooth\nand nonconvex. We provide a clean complexity bound for SVGD in the population\nlimit in terms of the Stein Fisher Information (or squared Kernelized Stein\nDiscrepancy), as a function of the dimension of the problem $d$ and the desired\naccuracy $\\varepsilon$. Unlike existing work, we do not make any assumption on\nthe trajectory of the algorithm. Instead, our key assumption is that the target\ndistribution satisfies Talagrand's inequality T1.",
    "descriptor": "\nComments: 15 pages, 2 Lemmas, 2 Propositions, 1 Theorem, 3 Corollaries\n",
    "authors": [
      "Adil Salim",
      "Lukang Sun",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03076"
  },
  {
    "id": "arXiv:2106.03078",
    "title": "A novel Deep Neural Network architecture for non-linear system  identification",
    "abstract": "We present a novel Deep Neural Network (DNN) architecture for non-linear\nsystem identification. We foster generalization by constraining DNN\nrepresentational power. To do so, inspired by fading memory systems, we\nintroduce inductive bias (on the architecture) and regularization (on the loss\nfunction). This architecture allows for automatic complexity selection based\nsolely on available data, in this way the number of hyper-parameters that must\nbe chosen by the user is reduced. Exploiting the highly parallelizable DNN\nframework (based on Stochastic optimization methods) we successfully apply our\nmethod to large scale datasets.",
    "descriptor": "",
    "authors": [
      "Luca Zancato",
      "Alessandro Chiuso"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03078"
  },
  {
    "id": "arXiv:2106.03079",
    "title": "Redefining measures of Layered Architecture",
    "abstract": "Layered architecture represents the software structure in the form of layers.\nEvery element in the software is assigned to one of the layers such that the\nrelationship amongst the elements is maintained. A set of design principles\nrules the process of construction of the layered architecture. Various\nstatistical measures have been defined to check whether the layered\narchitecture of a given software is following these design principles or not.\nIn this paper, we redefine the measures of layered architecture based on the\nrelationship between the software components. The measures check for the\nviolations committed regarding the back calls, skip calls, and cyclic\nstructures. Further, we also introduce a new measure to verify the logical\nseparation amongst the layers. The system's current architecture is extracted\nfrom the source code and represented using a three-tier layered structure,\nwhich is the defacto standard architecture of Java applications. The redefined\nmeasures are applied to determine the conformance of layering principles in the\nsystem. We evaluate five different software systems for their architecture\nconsistency. The results obtained on our redefined measures are compared to\nthose obtained by applying the standard set of measures. A quantitative\nanalysis of the proposed measures is performed, and we conclude that they can\ndetermine the consideration of layering principles followed during the\ndevelopment of a software system.",
    "descriptor": "\nComments: Reviewed for SCAM 2020\n",
    "authors": [
      "Sanjay Thakare",
      "Arvind W Kiwelekar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03079"
  },
  {
    "id": "arXiv:2106.03084",
    "title": "Combining Static Word Embeddings and Contextual Representations for  Bilingual Lexicon Induction",
    "abstract": "Bilingual Lexicon Induction (BLI) aims to map words in one language to their\ntranslations in another, and are typically through learning linear projections\nto align monolingual word representation spaces. Two classes of word\nrepresentations have been explored for BLI: static word embeddings and\ncontextual representations, but there is no studies to combine both. In this\npaper, we propose a simple yet effective mechanism to combine the static word\nembeddings and the contextual representations to utilize the advantages of both\nparadigms. We test the combination mechanism on various language pairs under\nthe supervised and unsupervised BLI benchmark settings. Experiments show that\nour mechanism consistently improves performances over robust BLI baselines on\nall language pairs by averagely improving 3.2 points in the supervised setting,\nand 3.1 points in the unsupervised setting.",
    "descriptor": "\nComments: Accepted to Findings of ACL2021\n",
    "authors": [
      "Jinpeng Zhang",
      "Baijun Ji",
      "Nini Xiao",
      "Xiangyu Duan",
      "Min Zhang",
      "Yangbin Shi",
      "Weihua Luo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03084"
  },
  {
    "id": "arXiv:2106.03085",
    "title": "An Ontology Model for Climatic Data Analysis",
    "abstract": "Recently ontologies have been exploited in a wide range of research areas for\ndata modeling and data management. They greatly assists in defining the\nsemantic model of the underlying data combined with domain knowledge. In this\npaper, we propose the Climate Analysis (CA) Ontology to model climate datasets\nused by remote sensing analysts. We use the data published by National Oceanic\nand Atmospheric Administration (NOAA) to further explore how ontology modeling\ncan be used to facilitate the field of climatic data processing. The idea of\nthis work is to convert relational climate data to the Resource Description\nFramework (RDF) data model, so that it can be stored in a graph database and\neasily accessed through the Web as Linked Data. Typically, this provides\nclimate researchers, who are interested in datasets such as NOAA, with the\npotential of enriching and interlinking with other databases. As a result, our\napproach facilitates data integration and analysis of diverse climatic data\nsources and allows researchers to interrogate these sources directly on the Web\nusing the standard SPARQL query language.",
    "descriptor": "\nComments: Published in IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2021\n",
    "authors": [
      "Jiantao Wu",
      "Fabrizio Orlandi",
      "Declan O'Sullivan",
      "Soumyabrata Dev"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.03085"
  },
  {
    "id": "arXiv:2106.03087",
    "title": "Neural Implicit 3D Shapes from Single Images with Spatial Patterns",
    "abstract": "3D shape reconstruction from a single image has been a long-standing problem\nin computer vision. The problem is ill-posed and highly challenging due to the\ninformation loss and occlusion that occurred during the imagery capture. In\ncontrast to previous methods that learn holistic shape priors, we propose a\nmethod to learn spatial pattern priors for inferring the invisible regions of\nthe underlying shape, wherein each 3D sample in the implicit shape\nrepresentation is associated with a set of points generated by hand-crafted 3D\nmappings, along with their local image features. The proposed spatial pattern\nis significantly more informative and has distinctive descriptions on both\nvisible and occluded locations. Most importantly, the key to our work is the\nubiquitousness of the spatial patterns across shapes, which enables reasoning\ninvisible parts of the underlying objects and thus greatly mitigates the\nocclusion issue. We devise a neural network that integrates spatial pattern\nrepresentations and demonstrate the superiority of the proposed method on\nwidely used metrics.",
    "descriptor": "",
    "authors": [
      "Yixin Zhuang",
      "Yunzhe Liu",
      "Baoquan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03087"
  },
  {
    "id": "arXiv:2106.03088",
    "title": "Reducing the feature divergence of RGB and near-infrared images using  Switchable Normalization",
    "abstract": "Visual pattern recognition over agricultural areas is an important\napplication of aerial image processing. In this paper, we consider the\nmulti-modality nature of agricultural aerial images and show that naively\ncombining different modalities together without taking the feature divergence\ninto account can lead to sub-optimal results. Thus, we apply a Switchable\nNormalization block to our DeepLabV3 segmentation model to alleviate the\nfeature divergence. Using the popular symmetric Kullback Leibler divergence\nmeasure, we show that our model can greatly reduce the divergence between RGB\nand near-infrared channels. Together with a hybrid loss function, our model\nachieves nearly 10\\% improvements in mean IoU over previously published\nbaseline.",
    "descriptor": "\nComments: CVPR2020 AgriVision workshop\n",
    "authors": [
      "Siwei Yang",
      "Shaozuo Yu",
      "Bingchen Zhao",
      "Yin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03088"
  },
  {
    "id": "arXiv:2106.03089",
    "title": "Referring Transformer: A One-step Approach to Multi-task Visual  Grounding",
    "abstract": "As an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension/segmentation) has been widely\nexplored Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-specific one-stage\narchitectures. In this paper, we propose a simple one-stage multi-task\nframework for visual grounding tasks. Specifically, we leverage a transformer\narchitecture, where two modalities are fused in a visual-lingual encoder. In\nthe decoder, the model learns to generate contextualized lingual queries which\nare then decoded and used to directly regress the bounding box and produce a\nsegmentation mask for the corresponding referred regions. With this simple but\nhighly contextualized model, we outperform state-of-the-arts methods by a large\nmargin on both REC and RES tasks. We also show that a simple pre-training\nschedule (on an external dataset) further improves the performance. Extensive\nexperiments and ablations illustrate that our model benefits greatly from\ncontextualized information and multi-task training.",
    "descriptor": "",
    "authors": [
      "Muchen Li",
      "Leonid Sigal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03089"
  },
  {
    "id": "arXiv:2106.03090",
    "title": "Deep Matching Prior: Test-Time Optimization for Dense Correspondence",
    "abstract": "Conventional techniques to establish dense correspondences across visually or\nsemantically similar images focused on designing a task-specific matching\nprior, which is difficult to model. To overcome this, recent learning-based\nmethods have attempted to learn a good matching prior within a model itself on\nlarge training data. The performance improvement was apparent, but the need for\nsufficient training data and intensive learning hinders their applicability.\nMoreover, using the fixed model at test time does not account for the fact that\na pair of images may require their own prior, thus providing limited\nperformance and poor generalization to unseen images. In this paper, we show\nthat an image pair-specific prior can be captured by solely optimizing the\nuntrained matching networks on an input pair of images. Tailored for such\ntest-time optimization for dense correspondence, we present a residual matching\nnetwork and a confidence-aware contrastive loss to guarantee a meaningful\nconvergence. Experiments demonstrate that our framework, dubbed Deep Matching\nPrior (DMP), is competitive, or even outperforms, against the latest\nlearning-based methods on several benchmarks for geometric matching and\nsemantic matching, even though it requires neither large training data nor\nintensive learning. With the networks pre-trained, DMP attains state-of-the-art\nperformance on all benchmarks.",
    "descriptor": "\nComments: 19 pages, 19 figures. The code will be made available at this https URL\n",
    "authors": [
      "Sunghwan Hong",
      "Seungryong Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03090"
  },
  {
    "id": "arXiv:2106.03096",
    "title": "TabularNet: A Neural Network Architecture for Understanding Semantic  Structures of Tabular Data",
    "abstract": "Tabular data are ubiquitous for the widespread applications of tables and\nhence have attracted the attention of researchers to extract underlying\ninformation. One of the critical problems in mining tabular data is how to\nunderstand their inherent semantic structures automatically. Existing studies\ntypically adopt Convolutional Neural Network (CNN) to model the spatial\ninformation of tabular structures yet ignore more diverse relational\ninformation between cells, such as the hierarchical and paratactic\nrelationships. To simultaneously extract spatial and relational information\nfrom tables, we propose a novel neural network architecture, TabularNet. The\nspatial encoder of TabularNet utilizes the row/column-level Pooling and the\nBidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information\nand local positional correlation, respectively. For relational information, we\ndesign a new graph construction method based on the WordNet tree and adopt a\nGraph Convolutional Network (GCN) based encoder that focuses on the\nhierarchical and paratactic relationships between cells. Our neural network\narchitecture can be a unified neural backbone for different understanding tasks\nand utilized in a multitask scenario. We conduct extensive experiments on three\nclassification tasks with two real-world spreadsheet data sets, and the results\ndemonstrate the effectiveness of our proposed TabularNet over state-of-the-art\nbaselines.",
    "descriptor": "\nComments: 10 pages, 7 figures, to be published in the proceedings of KDD 2021\n",
    "authors": [
      "Lun Du",
      "Fei Gao",
      "Xu Chen",
      "Ran Jia",
      "Junshan Wang",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03096"
  },
  {
    "id": "arXiv:2106.03097",
    "title": "Preservation of the Global Knowledge by Not-True Self Knowledge  Distillation in Federated Learning",
    "abstract": "In Federated Learning (FL), a strong global model is collaboratively learned\nby aggregating the clients' locally trained models. Although this allows no\nneed to access clients' data directly, the global model's convergence often\nsuffers from data heterogeneity. This paper suggests that forgetting could be\nthe bottleneck of global convergence. We observe that fitting on biased local\ndistribution shifts the feature on global distribution and results in\nforgetting of global knowledge. We consider this phenomenon as an analogy to\nContinual Learning, which also faces catastrophic forgetting when fitted on the\nnew task distribution. Based on our findings, we hypothesize that tackling down\nthe forgetting in local training relives the data heterogeneity problem. To\nthis end, we propose a simple yet effective framework Federated Local\nSelf-Distillation (FedLSD), which utilizes the global knowledge on locally\navailable data. By following the global perspective on local data, FedLSD\nencourages the learned features to preserve global knowledge and have\nconsistent views across local models, thus improving convergence without\ncompromising data privacy. Under our framework, we further extend FedLSD to\nFedLS-NTD, which only considers the not-true class signals to compensate noisy\nprediction of the global model. We validate that both FedLSD and FedLS-NTD\nsignificantly improve the performance in standard FL benchmarks in various\nsetups, especially in the extreme data heterogeneity cases.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Gihun Lee",
      "Yongjin Shin",
      "Minchan Jeong",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03097"
  },
  {
    "id": "arXiv:2106.03099",
    "title": "A Primer on Multi-Neuron Relaxation-based Adversarial Robustness  Certification",
    "abstract": "The existence of adversarial examples poses a real danger when deep neural\nnetworks are deployed in the real world. The go-to strategy to quantify this\nvulnerability is to evaluate the model against specific attack algorithms. This\napproach is however inherently limited, as it says little about the robustness\nof the model against more powerful attacks not included in the evaluation. We\ndevelop a unified mathematical framework to describe relaxation-based\nrobustness certification methods, which go beyond adversary-specific robustness\nevaluation and instead provide provable robustness guarantees against attacks\nby any adversary. We discuss the fundamental limitations posed by single-neuron\nrelaxations and show how the recent ``k-ReLU'' multi-neuron relaxation\nframework of Singh et al. (2019) obtains tighter correlation-aware activation\nbounds by leveraging additional relational constraints among groups of neurons.\nSpecifically, we show how additional pre-activation bounds can be mapped to\ncorresponding post-activation bounds and how they can in turn be used to obtain\ntighter robustness certificates. We also present an intuitive way to visualize\ndifferent relaxation-based certification methods. By approximating multiple\nnon-linearities jointly instead of separately, the k-ReLU method is able to\nbypass the convex barrier imposed by single neuron relaxations.",
    "descriptor": "",
    "authors": [
      "Kevin Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03099"
  },
  {
    "id": "arXiv:2106.03100",
    "title": "Optimal error estimation of a time-spectral method for fractional  diffusion problems with low regularity data",
    "abstract": "This paper is devoted to the error analysis of a time-spectral algorithm for\nfractional diffusion problems of order $\\alpha$ ($0 < \\alpha < 1$). The\nsolution regularity in the Sobolev space is revisited, and new regularity\nresults in the Besov space are established. A time-spectral algorithm is\ndeveloped which adopts a standard spectral method and a conforming linear\nfinite element method for temporal and spatial discretizations, respectively.\nOptimal error estimates are derived with nonsmooth data. Particularly, a sharp\ntemporal convergence rate $1+2\\alpha$ is shown theoretically and numerically.",
    "descriptor": "",
    "authors": [
      "Hao Luo",
      "Xiaoping Xie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03100"
  },
  {
    "id": "arXiv:2106.03102",
    "title": "On the Dual of Generalized Bent Functions",
    "abstract": "In this paper, we study the dual of generalized bent functions $f:\nV_{n}\\rightarrow \\mathbb{Z}_{p^k}$ where $V_{n}$ is an $n$-dimensional vector\nspace over $\\mathbb{F}_{p}$ and $p$ is an odd prime, $k$ is a positive integer.\nIt is known that weakly regular generalized bent functions always appear in\npairs since the dual of a weakly regular generalized bent function is also a\nweakly regular generalized bent function. The dual of non-weakly regular\ngeneralized bent functions can be generalized bent or not generalized bent. By\ngeneralizing the construction of \\cite{Cesmelioglu5}, we obtain an explicit\nconstruction of generalized bent functions for which the dual can be\ngeneralized bent or not generalized bent. We show that the generalized indirect\nsum construction method given in \\cite{Wang} can provide a secondary\nconstruction of generalized bent functions for which the dual can be\ngeneralized bent or not generalized bent. By using the knowledge on ideal\ndecomposition in cyclotomic field, we prove that $f^{**}(x)=f(-x)$ if $f$ is a\ngeneralized bent function and its dual $f^{*}$ is also a generalized bent\nfunction. For any non-weakly regular generalized bent function $f$ which\nsatisfies that $f(x)=f(-x)$ and its dual $f^{*}$ is generalized bent, we give a\nproperty and as a consequence, we prove that there is no self-dual generalized\nbent function $f: V_{n}\\rightarrow \\mathbb{Z}_{p^k}$ if $p\\equiv 3 \\ (mod \\ 4)$\nand $n$ is odd. For $p \\equiv 1 \\ (mod \\ 4)$ or $p\\equiv 3 \\ (mod \\ 4)$ and $n$\nis even, we give a secondary construction of self-dual generalized bent\nfunctions. In the end, we characterize the relations between the generalized\nbentness of the dual of generalized bent functions and the bentness of the dual\nof bent functions, as well as the self-duality relations between generalized\nbent functions and bent functions by the decomposition of generalized bent\nfunctions.",
    "descriptor": "",
    "authors": [
      "Jiaxin Wang",
      "Fang-Wei Fu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03102"
  },
  {
    "id": "arXiv:2106.03103",
    "title": "Enhancing Label Correlation Feedback in Multi-Label Text Classification  via Multi-Task Learning",
    "abstract": "In multi-label text classification (MLTC), each given document is associated\nwith a set of correlated labels. To capture label correlations, previous\nclassifier-chain and sequence-to-sequence models transform MLTC to a sequence\nprediction task. However, they tend to suffer from label order dependency,\nlabel combination over-fitting and error propagation problems. To address these\nproblems, we introduce a novel approach with multi-task learning to enhance\nlabel correlation feedback. We first utilize a joint embedding (JE) mechanism\nto obtain the text and label representation simultaneously. In MLTC task, a\ndocument-label cross attention (CA) mechanism is adopted to generate a more\ndiscriminative document representation. Furthermore, we propose two auxiliary\nlabel co-occurrence prediction tasks to enhance label correlation learning: 1)\nPairwise Label Co-occurrence Prediction (PLCP), and 2) Conditional Label\nCo-occurrence Prediction (CLCP). Experimental results on AAPD and RCV1-V2\ndatasets show that our method outperforms competitive baselines by a large\nmargin. We analyze low-frequency label performance, label dependency, label\ncombination diversity and coverage speed to show the effectiveness of our\nproposed method on label correlation learning.",
    "descriptor": "\nComments: Accepted by ACL 2021 (Finding)\n",
    "authors": [
      "Ximing Zhang",
      "Qian-Wen Zhang",
      "Zhao Yan",
      "Ruifang Liu",
      "Yunbo Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03103"
  },
  {
    "id": "arXiv:2106.03106",
    "title": "Uformer: A General U-Shaped Transformer for Image Restoration",
    "abstract": "In this paper, we present Uformer, an effective and efficient\nTransformer-based architecture, in which we build a hierarchical\nencoder-decoder network using the Transformer block for image restoration.\nUformer has two core designs to make it suitable for this task. The first key\nelement is a local-enhanced window Transformer block, where we use\nnon-overlapping window-based self-attention to reduce the computational\nrequirement and employ the depth-wise convolution in the feed-forward network\nto further improve its potential for capturing local context. The second key\nelement is that we explore three skip-connection schemes to effectively deliver\ninformation from the encoder to the decoder. Powered by these two designs,\nUformer enjoys a high capability for capturing useful dependencies for image\nrestoration. Extensive experiments on several image restoration tasks\ndemonstrate the superiority of Uformer, including image denoising, deraining,\ndeblurring and demoireing. We expect that our work will encourage further\nresearch to explore Transformer-based architectures for low-level vision tasks.\nThe code and models will be available at\nhttps://github.com/ZhendongWang6/Uformer.",
    "descriptor": "",
    "authors": [
      "Zhendong Wang",
      "Xiaodong Cun",
      "Jianmin Bao",
      "Jianzhuang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03106"
  },
  {
    "id": "arXiv:2106.03110",
    "title": "Asymmetric Loss Functions for Learning with Noisy Labels",
    "abstract": "Robust loss functions are essential for training deep neural networks with\nbetter generalization power in the presence of noisy labels. Symmetric loss\nfunctions are confirmed to be robust to label noise. However, the symmetric\ncondition is overly restrictive. In this work, we propose a new class of loss\nfunctions, namely \\textit{asymmetric loss functions}, which are robust to\nlearning with noisy labels for various types of noise. We investigate general\ntheoretical properties of asymmetric loss functions, including classification\ncalibration, excess risk bound, and noise tolerance. Meanwhile, we introduce\nthe asymmetry ratio to measure the asymmetry of a loss function. The empirical\nresults show that a higher ratio would provide better noise tolerance.\nMoreover, we modify several commonly-used loss functions and establish the\nnecessary and sufficient conditions for them to be asymmetric. Experimental\nresults on benchmark datasets demonstrate that asymmetric loss functions can\noutperform state-of-the-art methods. The code is available at\n\\href{https://github.com/hitcszx/ALFs}{https://github.com/hitcszx/ALFs}",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Xiong Zhou",
      "Xianming Liu",
      "Junjun Jiang",
      "Xin Gao",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03110"
  },
  {
    "id": "arXiv:2106.03111",
    "title": "Lexical Semantic Change Discovery",
    "abstract": "While there is a large amount of research in the field of Lexical Semantic\nChange Detection, only few approaches go beyond a standard benchmark evaluation\nof existing models. In this paper, we propose a shift of focus from change\ndetection to change discovery, i.e., discovering novel word senses over time\nfrom the full corpus vocabulary. By heavily fine-tuning a type-based and a\ntoken-based approach on recently published German data, we demonstrate that\nboth models can successfully be applied to discover new words undergoing\nmeaning change. Furthermore, we provide an almost fully automated framework for\nboth evaluation and discovery.",
    "descriptor": "\nComments: ACL 2021, 9 pages\n",
    "authors": [
      "Sinan Kurtyigit",
      "Maike Park",
      "Dominik Schlechtweg",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03111"
  },
  {
    "id": "arXiv:2106.03112",
    "title": "Rethinking Training from Scratch for Object Detection",
    "abstract": "The ImageNet pre-training initialization is the de-facto standard for object\ndetection. He et al. found it is possible to train detector from scratch(random\ninitialization) while needing a longer training schedule with proper\nnormalization technique. In this paper, we explore to directly pre-training on\ntarget dataset for object detection. Under this situation, we discover that the\nwidely adopted large resizing strategy e.g. resize image to (1333, 800) is\nimportant for fine-tuning but it's not necessary for pre-training.\nSpecifically, we propose a new training pipeline for object detection that\nfollows `pre-training and fine-tuning', utilizing low resolution images within\ntarget dataset to pre-training detector then load it to fine-tuning with high\nresolution images. With this strategy, we can use batch normalization(BN) with\nlarge bath size during pre-training, it's also memory efficient that we can\napply it on machine with very limited GPU memory(11G). We call it direct\ndetection pre-training, and also use direct pre-training for short. Experiment\nresults show that direct pre-training accelerates the pre-training phase by\nmore than 11x on COCO dataset while with even +1.8mAP compared to ImageNet\npre-training. Besides, we found direct pre-training is also applicable to\ntransformer based backbones e.g. Swin Transformer. Code will be available.",
    "descriptor": "\nComments: tech reports\n",
    "authors": [
      "Yang Li",
      "Hong Zhang",
      "Yu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03112"
  },
  {
    "id": "arXiv:2106.03114",
    "title": "Leveraging spectral analysis to elucidate membrane locking and unlocking  in isogeometric finite element formulations of the curved Euler-Bernoulli  beam",
    "abstract": "In this paper, we initiate the use of spectral analysis for assessing locking\nphenomena in finite element formulations. We propose to ``measure'' locking by\ncomparing the difference between eigenvalue and mode error curves computed on\ncoarse meshes with ``asymptotic'' error curves computed on ``overkill'' meshes,\nboth plotted with respect to the normalized mode number. To demonstrate the\nintimate relation between membrane locking and spectral accuracy, we focus on\nthe example of a circular ring discretized with isogeometric curved\nEuler-Bernoulli beam elements. We show that the\ntransverse-displacement-dominating modes are locking-prone, while the\ncircumferential-displacement-dominating modes are naturally locking-free. We\nuse eigenvalue and mode errors to assess five isogeometric finite element\nformulations in terms of their locking-related efficiency: the\ndisplacement-based formulation with full and reduced integration and three\nlocking-free formulations based on the B-bar, discrete strain gap and\nHellinger-Reissner methods. Our study shows that spectral analysis uncovers\nlocking-related effects across the spectrum of eigenvalues and eigenmodes,\nrigorously characterizing membrane locking in the displacement-based\nformulation and unlocking in the locking-free formulations.",
    "descriptor": "",
    "authors": [
      "Thi-Hoa Nguyen",
      "Ren\u00e9 R. Hiemstra",
      "Dominik Schillinger"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.03114"
  },
  {
    "id": "arXiv:2106.03121",
    "title": "End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning  Tasks",
    "abstract": "Neural models and symbolic algorithms have recently been combined for tasks\nrequiring both perception and reasoning. Neural models ground perceptual input\ninto a conceptual vocabulary, on which a classical reasoning algorithm is\napplied to generate output. A key limitation is that such neural-to-symbolic\nmodels can only be trained end-to-end for tasks where the output space is\nsymbolic. In this paper, we study neural-symbolic-neural models for reasoning\ntasks that require a conversion from an image input (e.g., a partially filled\nsudoku) to an image output (e.g., the image of the completed sudoku). While\ndesigning such a three-step hybrid architecture may be straightforward, the key\ntechnical challenge is end-to-end training -- how to backpropagate without\nintermediate supervision through the symbolic component. We propose NSNnet, an\narchitecture that combines an image reconstruction loss with a novel output\nencoder to generate a supervisory signal, develops update algorithms that\nleverage policy gradient methods for supervision, and optimizes loss using a\nnovel subsampling heuristic. We experiment on problem settings where symbolic\nalgorithms are easily specified: a visual maze solving task and a visual Sudoku\nsolver where the supervision is in image form. Experiments show high accuracy\nwith significantly less data compared to purely neural approaches.",
    "descriptor": "",
    "authors": [
      "Ananye Agarwal",
      "Pradeep Shenoy",
      "Mausam"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03121"
  },
  {
    "id": "arXiv:2106.03122",
    "title": "ModelCI-e: Enabling Continual Learning in Deep Learning Serving Systems",
    "abstract": "MLOps is about taking experimental ML models to production, i.e., serving the\nmodels to actual users. Unfortunately, existing ML serving systems do not\nadequately handle the dynamic environments in which online data diverges from\noffline training data, resulting in tedious model updating and deployment\nworks. This paper implements a lightweight MLOps plugin, termed ModelCI-e\n(continuous integration and evolution), to address the issue. Specifically, it\nembraces continual learning (CL) and ML deployment techniques, providing\nend-to-end supports for model updating and validation without serving engine\ncustomization. ModelCI-e includes 1) a model factory that allows CL researchers\nto prototype and benchmark CL models with ease, 2) a CL backend to automate and\norchestrate the model updating efficiently, and 3) a web interface for an ML\nteam to manage CL service collaboratively. Our preliminary results demonstrate\nthe usability of ModelCI-e, and indicate that eliminating the interference\nbetween model updating and inference workloads is crucial for higher system\nefficiency.",
    "descriptor": "",
    "authors": [
      "Yizheng Huang",
      "Huaizheng Zhang",
      "Yonggang Wen",
      "Peng Sun",
      "Nguyen Binh Duong TA"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03122"
  },
  {
    "id": "arXiv:2106.03128",
    "title": "MOC-GAN: Mixing Objects and Captions to Generate Realistic Images",
    "abstract": "Generating images with conditional descriptions gains increasing interests in\nrecent years. However, existing conditional inputs are suffering from either\nunstructured forms (captions) or limited information and expensive labeling\n(scene graphs). For a targeted scene, the core items, objects, are usually\ndefinite while their interactions are flexible and hard to clearly define.\nThus, we introduce a more rational setting, generating a realistic image from\nthe objects and captions. Under this setting, objects explicitly define the\ncritical roles in the targeted images and captions implicitly describe their\nrich attributes and connections. Correspondingly, a MOC-GAN is proposed to mix\nthe inputs of two modalities to generate realistic images. It firstly infers\nthe implicit relations between object pairs from the captions to build a\nhidden-state scene graph. So a multi-layer representation containing objects,\nrelations and captions is constructed, where the scene graph provides the\nstructures of the scene and the caption provides the image-level guidance. Then\na cascaded attentive generative network is designed to coarse-to-fine generate\nphrase patch by paying attention to the most relevant words in the caption. In\naddition, a phrase-wise DAMSM is proposed to better supervise the fine-grained\nphrase-patch consistency. On COCO dataset, our method outperforms the\nstate-of-the-art methods on both Inception Score and FID while maintaining high\nvisual quality. Extensive experiments demonstrate the unique features of our\nproposed method.",
    "descriptor": "\nComments: 9 pages, 3 figures, submitted to NeurIPS 2021\n",
    "authors": [
      "Tao Ma",
      "Yikang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03128"
  },
  {
    "id": "arXiv:2106.03131",
    "title": "The Fine-Grained Hardness of Sparse Linear Regression",
    "abstract": "Sparse linear regression is the well-studied inference problem where one is\ngiven a design matrix $\\mathbf{A} \\in \\mathbb{R}^{M\\times N}$ and a response\nvector $\\mathbf{b} \\in \\mathbb{R}^M$, and the goal is to find a solution\n$\\mathbf{x} \\in \\mathbb{R}^{N}$ which is $k$-sparse (that is, it has at most\n$k$ non-zero coordinates) and minimizes the prediction error $||\\mathbf{A}\n\\mathbf{x} - \\mathbf{b}||_2$. On the one hand, the problem is known to be\n$\\mathcal{NP}$-hard which tells us that no polynomial-time algorithm exists\nunless $\\mathcal{P} = \\mathcal{NP}$. On the other hand, the best known\nalgorithms for the problem do a brute-force search among $N^k$ possibilities.\nIn this work, we show that there are no better-than-brute-force algorithms,\nassuming any one of a variety of popular conjectures including the weighted\n$k$-clique conjecture from the area of fine-grained complexity, or the hardness\nof the closest vector problem from the geometry of numbers. We also show the\nimpossibility of better-than-brute-force algorithms when the prediction error\nis measured in other $\\ell_p$ norms, assuming the strong exponential-time\nhypothesis.",
    "descriptor": "",
    "authors": [
      "Aparna Gupte",
      "Vinod Vaikuntanathan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03131"
  },
  {
    "id": "arXiv:2106.03132",
    "title": "Collective transport via sequential caging",
    "abstract": "We propose a decentralized algorithm to collaboratively transport arbitrarily\nshaped objects using a swarm of robots. Our approach starts with a task\nallocation phase that sequentially distributes locations around the object to\nbe transported starting from a seed robot that makes first contact with the\nobject. Our approach does not require previous knowledge of the shape of the\nobject to ensure caging. To push the object to a goal location, we estimate the\nrobots required to apply force on the object based on the angular difference\nbetween the target and the object. During transport, the robots follow a\nsequence of intermediate goal locations specifying the required pose of the\nobject at that location. We evaluate our approach in a physics-based simulator\nwith up to 100 robots, using three generic paths. Experiments using a group of\nKheperaIV robots demonstrate the effectiveness of our approach in a real\nsetting.\nKeywords: Collaborative transport, Task Allocation, Caging, Robot Swarms",
    "descriptor": "\nComments: Number of Pages - 14 Number of figures - 9 Accepted by Distributed Autonomous Robotic Systems ' 2021\n",
    "authors": [
      "Vivek Shankar Vardharajan",
      "Karthik Soma",
      "Giovanni Beltrame"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.03132"
  },
  {
    "id": "arXiv:2106.03134",
    "title": "Semi-Riemannian Graph Convolutional Networks",
    "abstract": "Graph Convolutional Networks (GCNs) are typically studied through the lens of\nEuclidean geometry. Non-Euclidean Riemannian manifolds provide specific\ninductive biases for embedding hierarchical or spherical data, but cannot align\nwell with data of mixed topologies. We consider a larger class of\nsemi-Riemannian manifolds with indefinite metric that generalize hyperboloid\nand sphere as well as their submanifolds. We develop new geodesic tools that\nallow for extending neural network operations into geodesically disconnected\nsemi-Riemannian manifolds. As a consequence, we derive a principled\nSemi-Riemannian GCN that first models data in semi-Riemannian manifolds of\nconstant nonzero curvature in the context of graph neural networks. Our method\nprovides a geometric inductive bias that is sufficiently flexible to model\nmixed heterogeneous topologies like hierarchical graphs with cycles. Empirical\nresults demonstrate that our method outperforms Riemannian counterparts when\nembedding graphs of complex topologies.",
    "descriptor": "\nComments: 23 pages, 5 figures, submitted to NeurIPS 2021\n",
    "authors": [
      "Bo Xiong",
      "Shichao Zhu",
      "Nico Potyka",
      "Shirui Pan",
      "Chuan Zhou",
      "Steffen Staab"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03134"
  },
  {
    "id": "arXiv:2106.03135",
    "title": "Go with the Flows: Mixtures of Normalizing Flows for Point Cloud  Generation and Reconstruction",
    "abstract": "Recently normalizing flows (NFs) have demonstrated state-of-the-art\nperformance on modeling 3D point clouds while allowing sampling with arbitrary\nresolution at inference time. However, these flow-based models still require\nlong training times and large models for representing complicated geometries.\nThis work enhances their representational power by applying mixtures of NFs to\npoint clouds. We show that in this more general framework each component learns\nto specialize in a particular subregion of an object in a completely\nunsupervised fashion. By instantiating each mixture component with a\ncomparatively small NF we generate point clouds with improved details compared\nto single-flow-based models while using fewer parameters and considerably\nreducing the inference runtime. We further demonstrate that by adding data\naugmentation, individual mixture components can learn to specialize in a\nsemantically meaningful manner. We evaluate mixtures of NFs on generation,\nautoencoding and single-view reconstruction based on the ShapeNet dataset.",
    "descriptor": "",
    "authors": [
      "Janis Postels",
      "Mengya Liu",
      "Riccardo Spezialetti",
      "Luc Van Gool",
      "Federico Tombari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03135"
  },
  {
    "id": "arXiv:2106.03136",
    "title": "3D Convolution Neural Network based Person Identification using Gait  cycles",
    "abstract": "Human identification plays a prominent role in terms of security. In modern\ntimes security is becoming the key term for an individual or a country,\nespecially for countries which are facing internal or external threats. Gait\nanalysis is interpreted as the systematic study of the locomotive in humans. It\ncan be used to extract the exact walking features of individuals. Walking\nfeatures depends on biological as well as the physical feature of the object;\nhence, it is unique to every individual. In this work, gait features are used\nto identify an individual. The steps involve object detection, background\nsubtraction, silhouettes extraction, skeletonization, and training 3D\nConvolution Neural Network on these gait features. The model is trained and\nevaluated on the dataset acquired by CASIA B Gait, which consists of 15000\nvideos of 124 subjects walking pattern captured from 11 different angles\ncarrying objects such as bag and coat. The proposed method focuses more on the\nlower body part to extract features such as the angle between knee and thighs,\nhip angle, angle of contact, and many other features. The experimental results\nare compared with amongst accuracies of silhouettes as datasets for training\nand skeletonized image as training data. The results show that extracting the\ninformation from skeletonized data yields improved accuracy.",
    "descriptor": "",
    "authors": [
      "Ravi Shekhar Tiwari",
      "Supraja P",
      "Rijo Jackson Tom"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03136"
  },
  {
    "id": "arXiv:2106.03138",
    "title": "Deviation Maximization for Rank-Revealing QR Factorizations",
    "abstract": "In this paper we introduce a new column selection strategy, named here\n``Deviation Maximization\", and apply it to compute rank-revealing QR\nfactorizations as an alternative to the well known block version of the QR\nfactorization with the column pivoting method, called QP3 and currently\nimplemented in LAPACK's xgeqp3 routine. We show that the resulting algorithm,\nnamed QRDM, has similar rank-revealing properties of QP3 and better execution\ntimes. We present numerical test results on a wide data set of numerically\nsingular matrices, which has become a reference in the recent literature.",
    "descriptor": "\nComments: 31 pages, 7 figures, submitted to Numerical Algorithms\n",
    "authors": [
      "Monica Dessole",
      "Fabio Marcuzzi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03138"
  },
  {
    "id": "arXiv:2106.03142",
    "title": "A Physics-Informed Deep Learning Paradigm for Traffic State Estimation  and Fundamental Diagram Discovery",
    "abstract": "Traffic state estimation (TSE) bifurcates into two main categories,\nmodel-driven and data-driven (e.g., machine learning, ML) approaches, while\neach suffers from either deficient physics or small data. To mitigate these\nlimitations, recent studies introduced hybrid methods, such as physics-informed\ndeep learning (PIDL), which contains both model-driven and data-driven\ncomponents. This paper contributes an improved paradigm, called\nphysics-informed deep learning with a fundamental diagram learner (PIDL+FDL),\nwhich integrates ML terms into the model-driven component to learn a functional\nform of a fundamental diagram (FD), i.e., a mapping from traffic density to\nflow or velocity. The proposed PIDL+FDL has the advantages of performing the\nTSE learning, model parameter discovery, and FD discovery simultaneously. This\npaper focuses on highway TSE with observed data from loop detectors, using\ntraffic density or velocity as traffic variables. We demonstrate the use of\nPIDL+FDL to solve popular first-order and second-order traffic flow models and\nreconstruct the FD relation as well as model parameters that are outside the FD\nterm. We then evaluate the PIDL+FDL-based TSE using the Next Generation\nSIMulation (NGSIM) dataset. The experimental results show the superiority of\nthe PIDL+FDL in terms of improved estimation accuracy and data efficiency over\nadvanced baseline TSE methods, and additionally, the capacity to properly learn\nthe unknown underlying FD relation.",
    "descriptor": "",
    "authors": [
      "Rongye Shi",
      "Zhaobin Mo",
      "Kuang Huang",
      "Xuan Di",
      "Qiang Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03142"
  },
  {
    "id": "arXiv:2106.03143",
    "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional  Embeddings",
    "abstract": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
    "descriptor": "",
    "authors": [
      "Tatiana Likhomanenko",
      "Qiantong Xu",
      "Ronan Collobert",
      "Gabriel Synnaeve",
      "Alex Rogozhnikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03143"
  },
  {
    "id": "arXiv:2106.03146",
    "title": "Oriented Object Detection with Transformer",
    "abstract": "Object detection with Transformers (DETR) has achieved a competitive\nperformance over traditional detectors, such as Faster R-CNN. However, the\npotential of DETR remains largely unexplored for the more challenging task of\narbitrary-oriented object detection problem. We provide the first attempt and\nimplement Oriented Object DEtection with TRansformer ($\\bf O^2DETR$) based on\nan end-to-end network. The contributions of $\\rm O^2DETR$ include: 1) we\nprovide a new insight into oriented object detection, by applying Transformer\nto directly and efficiently localize objects without a tedious process of\nrotated anchors as in conventional detectors; 2) we design a simple but highly\nefficient encoder for Transformer by replacing the attention mechanism with\ndepthwise separable convolution, which can significantly reduce the memory and\ncomputational cost of using multi-scale features in the original Transformer;\n3) our $\\rm O^2DETR$ can be another new benchmark in the field of oriented\nobject detection, which achieves up to 3.85 mAP improvement over Faster R-CNN\nand RetinaNet. We simply fine-tune the head mounted on $\\rm O^2DETR$ in a\ncascaded architecture and achieve a competitive performance over SOTA in the\nDOTA dataset.",
    "descriptor": "",
    "authors": [
      "Teli Ma",
      "Mingyuan Mao",
      "Honghui Zheng",
      "Peng Gao",
      "Xiaodi Wang",
      "Shumin Han",
      "Errui Ding",
      "Baochang Zhang",
      "David Doermann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03146"
  },
  {
    "id": "arXiv:2106.03148",
    "title": "Assessing Attendance by Peer Information",
    "abstract": "Attendance rate is an important indicator of students' study motivation,\nbehavior and Psychological status; However, the heterogeneous nature of student\nattendance rates due to the course registration difference or the\nonline/offline difference in a blended learning environment makes it\nchallenging to compare attendance rates. In this paper, we propose a novel\nmethod called Relative Attendance Index (RAI) to measure attendance rates,\nwhich reflects students' efforts on attending courses. While traditional\nattendance focuses on the record of a single person or course, relative\nattendance emphasizes peer attendance information of relevant individuals or\ncourses, making the comparisons of attendance more justified. Experimental\nresults on real-life data show that RAI can indeed better reflect student\nengagement.",
    "descriptor": "",
    "authors": [
      "Pan Deng",
      "Jianjun Zhou",
      "Jing Lyu",
      "Zitong Zhao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03148"
  },
  {
    "id": "arXiv:2106.03149",
    "title": "Large-scale Unsupervised Semantic Segmentation",
    "abstract": "Powered by the ImageNet dataset, unsupervised learning on large-scale data\nhas made significant advances for classification tasks. There are two major\nchallenges to allow such an attractive learning modality for segmentation\ntasks: i) a large-scale benchmark for assessing algorithms is missing; ii)\nunsupervised shape representation learning is difficult. We propose a new\nproblem of large-scale unsupervised semantic segmentation (LUSS) with a newly\ncreated benchmark dataset to track the research progress. Based on the ImageNet\ndataset, we propose the ImageNet-S dataset with 1.2 million training images and\n40k high-quality semantic segmentation annotations for evaluation. Our\nbenchmark has a high data diversity and a clear task objective. We also present\na simple yet effective baseline method that works surprisingly well for LUSS.\nIn addition, we benchmark related un/weakly supervised methods accordingly,\nidentifying the challenges and possible directions of LUSS.",
    "descriptor": "",
    "authors": [
      "Shang-Hua Gao",
      "Zhong-Yu Li",
      "Ming-Hsuan Yang",
      "Ming-Ming Cheng",
      "Junwei Han",
      "Philip Torr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03149"
  },
  {
    "id": "arXiv:2106.03151",
    "title": "Attend and Select: A Segment Attention based Selection Mechanism for  Microblog Hashtag Generation",
    "abstract": "Automatic microblog hashtag generation can help us better and faster\nunderstand or process the critical content of microblog posts.\nConventional sequence-to-sequence generation methods can produce phrase-level\nhashtags and have achieved remarkable performance on this task. However, they\nare incapable of filtering out secondary information and not good at capturing\nthe discontinuous semantics among crucial tokens.\nA hashtag is formed by tokens or phrases that may originate from various\nfragmentary segments of the original text.\nIn this work, we propose an end-to-end Transformer-based generation model\nwhich consists of three phases: encoding, segments-selection, and decoding. The\nmodel transforms discontinuous semantic segments from the source text into a\nsequence of hashtags.\nSpecifically, we introduce a novel Segments Selection Mechanism (SSM) for\nTransformer to obtain segmental representations tailored to phrase-level\nhashtag generation.\nBesides, we introduce two large-scale hashtag generation datasets, which are\nnewly collected from Chinese Weibo and English Twitter.\nExtensive evaluations on the two datasets reveal our approach's superiority\nwith significant improvements to extraction and generation baselines. The code\nand datasets are available at \\url{https://github.com/OpenSUM/HashtagGen}.",
    "descriptor": "",
    "authors": [
      "Qianren Mao",
      "Xi Li",
      "Hao Peng",
      "Bang Liu",
      "Shu Guo",
      "Jianxin Li",
      "Lihong Wang",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03151"
  },
  {
    "id": "arXiv:2106.03152",
    "title": "Technical Report: Temporal Aggregate Representations",
    "abstract": "This technical report extends our work presented in [9] with more\nexperiments. In [9], we tackle long-term video understanding, which requires\nreasoning from current and past or future observations and raises several\nfundamental questions. How should temporal or sequential relationships be\nmodelled? What temporal extent of information and context needs to be\nprocessed? At what temporal scale should they be derived? [9] addresses these\nquestions with a flexible multi-granular temporal aggregation framework. In\nthis report, we conduct further experiments with this framework on different\ntasks and a new dataset, EPIC-KITCHENS-100.",
    "descriptor": "",
    "authors": [
      "Fadime Sener",
      "Dibyadip Chatterjee",
      "Angela Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03152"
  },
  {
    "id": "arXiv:2106.03155",
    "title": "SoftDICE for Imitation Learning: Rethinking Off-policy Distribution  Matching",
    "abstract": "We present SoftDICE, which achieves state-of-the-art performance for\nimitation learning. SoftDICE fixes several key problems in ValueDICE, an\noff-policy distribution matching approach for sample-efficient imitation\nlearning. Specifically, the objective of ValueDICE contains logarithms and\nexponentials of expectations, for which the mini-batch gradient estimate is\nalways biased. Second, ValueDICE regularizes the objective with replay buffer\nsamples when expert demonstrations are limited in number, which however changes\nthe original distribution matching problem. Third, the re-parametrization trick\nused to derive the off-policy objective relies on an implicit assumption that\nrarely holds in training. We leverage a novel formulation of distribution\nmatching and consider an entropy-regularized off-policy objective, which yields\na completely offline algorithm called SoftDICE. Our empirical results show that\nSoftDICE recovers the expert policy with only one demonstration trajectory and\nno further on-policy/off-policy samples. SoftDICE also stably outperforms\nValueDICE and other baselines in terms of sample efficiency on Mujoco benchmark\ntasks.",
    "descriptor": "",
    "authors": [
      "Mingfei Sun",
      "Anuj Mahajan",
      "Katja Hofmann",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03155"
  },
  {
    "id": "arXiv:2106.03157",
    "title": "Self-supervised Rubik's Cube Solver",
    "abstract": "This work demonstrates that deep neural networks (DNNs) can solve a\ncombinatorial problem merely through self-supervised learning. While\nresearchers have employed explicit logic, heuristics, and reinforcement\nlearning to tackle combinatorial problems, such methods are often complex and\ncostly to implement, requiring lots of knowledge, coding, and adjustments.\nHence, in the present study, I propose a robust and straightforward method of\nself-supervised learning to solve a combinatorial problem. Specifically, taking\nRubik's Cube as an example, this work shows that a DNN can implicitly learn\nconvoluted probability distributions of optimal choices from randomly generated\ncombinations. Tested on $1,000$ Rubik's Cube instances, a DNN successfully\nsolved all of them near-optimally. Although the proposed method is validated\nonly on Rubik's Cube, it is potentially useful for other problems and\nreal-world applications with its simplicity, stability, and robustness.",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Kyo Takano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03157"
  },
  {
    "id": "arXiv:2106.03158",
    "title": "Learning Video Models from Text: Zero-Shot Anticipation for Procedural  Actions",
    "abstract": "Can we teach a robot to recognize and make predictions for activities that it\nhas never seen before? We tackle this problem by learning models for video from\ntext. This paper presents a hierarchical model that generalizes instructional\nknowledge from large-scale text-corpora and transfers the knowledge to video.\nGiven a portion of an instructional video, our model recognizes and predicts\ncoherent and plausible actions multiple steps into the future, all in rich\nnatural language. To demonstrate the capabilities of our model, we introduce\nthe \\emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot\nlearning, recognition and anticipation. Extensive experiments with various\nevaluation metrics demonstrate the potential of our method for generalization,\ngiven limited video data for training models.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1812.02501\n",
    "authors": [
      "Fadime Sener",
      "Rishabh Saraf",
      "Angela Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03158"
  },
  {
    "id": "arXiv:2106.03160",
    "title": "Multi-agent Modeling of Hazard-Household-Infrastructure Nexus for  Equitable Resilience Assessment",
    "abstract": "To enable integrating social equity considerations in infrastructure\nresilience assessments, this study created a new computational multi-agent\nsimulation model which enables integrated assessment of hazard, infrastructure\nsystem, and household elements and their interactions. With a focus on\nhurricane-induced power outages, the model consists of three elements: 1) the\nhazard component simulates exposure of the community to a hurricane with\nvarying intensity levels; 2) the physical infrastructure component simulates\nthe power network and its probabilistic failures and restoration under\ndifferent hazard scenarios; and 3) the households component captures the\ndynamic processes related to preparation, information seeking, and response\nactions of households facing hurricane-induced power outages. We used empirical\ndata from household surveys in conjunction with theoretical decision-making\nmodels to abstract and simulate the underlying mechanisms affecting experienced\nhardship of households. The multi-agent simulation model was then tested in the\ncontext of Harris County, Texas, and verified and validated using empirical\nresults from Hurricane Harvey in 2017. Then, the model was used to examine\neffects of different factors such as forewarning durations, social network\ntypes, and restoration and resource allocation strategies on reducing the\nsocietal impacts of service disruptions in an equitable manner. The results\nshow that improving the restoration prioritization strategy to focus on\nvulnerable populations is an effective approach, especially during\nhigh-intensity events. The results show the capability of the proposed\ncomputational model for capturing the dynamic and complex interactions in the\nnexus of humans, hazards, and infrastructure systems to better integrate\nhuman-centric aspects in resilience planning and into assessment of\ninfrastructure systems in disasters.",
    "descriptor": "",
    "authors": [
      "Amir Esmalian",
      "Wanqiu Wang",
      "Ali Mostafavi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.03160"
  },
  {
    "id": "arXiv:2106.03161",
    "title": "Identifying Populist Paragraphs in Text: A machine-learning approach",
    "abstract": "Abstract: In this paper we present an approach to develop a\ntext-classification model which would be able to identify populist content in\ntext. The developed BERT-based model is largely successful in identifying\npopulist content in text and produces only a negligible amount of False\nNegatives, which makes it well-suited as a content analysis automation tool,\nwhich shortlists potentially relevant content for human validation.",
    "descriptor": "\nComments: 18 pages, 2 Figures, 3 Tables in main text, 2 tables in Annexes\n",
    "authors": [
      "Jogil\u0117 Ulinkskait\u0117",
      "Lukas Pukelis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03161"
  },
  {
    "id": "arXiv:2106.03162",
    "title": "Transformed ROIs for Capturing Visual Transformations in Videos",
    "abstract": "Modeling the visual changes that an action brings to a scene is critical for\nvideo understanding. Currently, CNNs process one local neighbourhood at a time,\nso contextual relationships over longer ranges, while still learnable, are\nindirect. We present TROI, a plug-and-play module for CNNs to reason between\nmid-level feature representations that are otherwise separated in space and\ntime. The module relates localized visual entities such as hands and\ninteracting objects and transforms their corresponding regions of interest\ndirectly in the feature maps of convolutional layers. With TROI, we achieve\nstate-of-the-art action recognition results on the large-scale datasets\nSomething-Something-V2 and Epic-Kitchens-100.",
    "descriptor": "",
    "authors": [
      "Abhinav Rai",
      "Fadime Sener",
      "Angela Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03162"
  },
  {
    "id": "arXiv:2106.03164",
    "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language  Model Adaptation",
    "abstract": "Adapter-based tuning has recently arisen as an alternative to fine-tuning. It\nworks by adding light-weight adapter modules to a pretrained language model\n(PrLM) and only updating the parameters of adapter modules when learning on a\ndownstream task. As such, it adds only a few trainable parameters per new task,\nallowing a high degree of parameter sharing. Prior studies have shown that\nadapter-based tuning often achieves comparable results to fine-tuning. However,\nexisting work only focuses on the parameter-efficient aspect of adapter-based\ntuning while lacking further investigation on its effectiveness. In this paper,\nwe study the latter. We first show that adapter-based tuning better mitigates\nforgetting issues than fine-tuning since it yields representations with less\ndeviation from those generated by the initial PrLM. We then empirically compare\nthe two tuning methods on several downstream NLP tasks and settings. We\ndemonstrate that 1) adapter-based tuning outperforms fine-tuning on\nlow-resource and cross-lingual tasks; 2) it is more robust to overfitting and\nless sensitive to changes in learning rates.",
    "descriptor": "\nComments: Accepted by ACL 2021 (long paper)\n",
    "authors": [
      "Ruidan He",
      "Linlin Liu",
      "Hai Ye",
      "Qingyu Tan",
      "Bosheng Ding",
      "Liying Cheng",
      "Jia-Wei Low",
      "Lidong Bing",
      "Luo Si"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03164"
  },
  {
    "id": "arXiv:2106.03170",
    "title": "FlexParser -- the adaptive log file parser for continuous results in a  changing world",
    "abstract": "Any modern system writes events into files, called log files. Those contain\ncrucial information which are subject to various analyses. Examples range from\ncybersecurity, intrusion detection over usage analyses to trouble shooting.\nBefore data analysis is possible, desired information needs to be extracted\nfirst out of the semi-structured log messages. State of the art event parsing\noften assumes static log events. However, any modern system is updated\nconsistently and with updates also log file structures can change. We call\nthose changes 'mutations' and study parsing performance for different mutation\ncases. Latest research discovers mutations using anomaly detection post mortem,\nhowever, does not cover actual continuous parsing. Thus, we propose a novel,\nflexible parser, called FlexParser which can extract desired values despite\ngradual changes in the log messages. It implies basic text preprocessing\nfollowed by a supervised Deep Learning method. We train a stateful LSTM on\nparsing one event per data set. Statefulness enforces the model to learn log\nmessage structures across several messages. Our model was tested on seven\ndifferent, publicly available log file data sets and various kinds of\nmutations. Exhibiting an average F1-Score of 0.98, it outperforms other Deep\nLearning methods as well as state-of-the-art unsupervised parsers.",
    "descriptor": "\nComments: 18 pages, 8 figures, 3 tables\n",
    "authors": [
      "Nadine Ruecker",
      "Andreas Maier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03170"
  },
  {
    "id": "arXiv:2106.03171",
    "title": "Feature-based Style Randomization for Domain Generalization",
    "abstract": "As a recent noticeable topic, domain generalization (DG) aims to first learn\na generic model on multiple source domains and then directly generalize to an\narbitrary unseen target domain without any additional adaption. In previous DG\nmodels, by generating virtual data to supplement observed source domains, the\ndata augmentation based methods have shown its effectiveness. To simulate the\npossible unseen domains, most of them enrich the diversity of original data via\nimage-level style transformation. However, we argue that the potential styles\nare hard to be exhaustively illustrated and fully augmented due to the limited\nreferred styles, leading the diversity could not be always guaranteed. Unlike\nimage-level augmentation, we in this paper develop a simple yet effective\nfeature-based style randomization module to achieve feature-level augmentation,\nwhich can produce random styles via integrating random noise into the original\nstyle. Compared with existing image-level augmentation, our feature-level\naugmentation favors a more goal-oriented and sample-diverse way. Furthermore,\nto sufficiently explore the efficacy of the proposed module, we design a novel\nprogressive training strategy to enable all parameters of the network to be\nfully trained. Extensive experiments on three standard benchmark datasets,\ni.e., PACS, VLCS and Office-Home, highlight the superiority of our method\ncompared to the state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Yue Wang",
      "Lei Qi",
      "Yinghuan Shi",
      "Yang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03171"
  },
  {
    "id": "arXiv:2106.03176",
    "title": "The Limits of Multi-task Peer Prediction",
    "abstract": "Recent advances in multi-task peer prediction have greatly expanded our\nknowledge about the power of multi-task peer prediction mechanisms. Various\nmechanisms have been proposed in different settings to elicit different types\nof information. But we still lack understanding about when desirable mechanisms\nwill exist for a multi-task peer prediction problem. In this work, we study the\nelicitability of multi-task peer prediction problems. We consider a designer\nwho has certain knowledge about the underlying information structure and wants\nto elicit certain information from a group of participants. Our goal is to\ninfer the possibility of having a desirable mechanism based on the primitives\nof the problem.\nOur contribution is twofold. First, we provide a characterization of the\nelicitable multi-task peer prediction problems, assuming that the designer only\nuses scoring mechanisms. Scoring mechanisms are the mechanisms that reward\nparticipants' reports for different tasks separately. The characterization uses\na geometric approach based on the power diagram characterization in the\nsingle-task setting ([Lambert and Shoham, 2009, Frongillo and Witkowski,\n2017]). For general mechanisms, we also give a necessary condition for a\nmulti-task problem to be elicitable.\nSecond, we consider the case when the designer aims to elicit some properties\nthat are linear in the participant's posterior about the state of the world. We\nfirst show that in some cases, the designer basically can only elicit the\nposterior itself. We then look into the case when the designer aims to elicit\nthe participants' posteriors. We give a necessary condition for the posterior\nto be elicitable. This condition implies that the mechanisms proposed by Kong\nand Schoenebeck are already the best we can hope for in their setting, in the\nsense that their mechanisms can solve any problem instance that can possibly be\nelicitable.",
    "descriptor": "",
    "authors": [
      "Shuran Zheng",
      "Fang-Yi Yu",
      "Yiling Chen"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2106.03176"
  },
  {
    "id": "arXiv:2106.03178",
    "title": "Path-specific Effects Based on Information Accounts of Causality",
    "abstract": "Path-specific effects in mediation analysis provide a useful tool for\nfairness analysis, which is mostly based on nested counterfactuals. However,\nthe dictum ``no causation without manipulation'' implies that path-specific\neffects might be induced by certain interventions. This paper proposes a new\npath intervention inspired by information accounts of causality, and develops\nthe corresponding intervention diagrams and $\\pi$-formula. Compared with the\ninterventionist approach of Robins et al.(2020) based on nested\ncounterfactuals, our proposed path intervention method explicitly describes the\nmanipulation in structural causal model with a simple information transferring\ninterpretation, and does not require the non-existence of recanting witness to\nidentify path-specific effects. Hence, it could serve useful communications and\ntheoretical focus for mediation analysis.",
    "descriptor": "",
    "authors": [
      "Heyang Gong",
      "Ke Zhu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Other Statistics (stat.OT)"
    ],
    "url": "https://arxiv.org/abs/2106.03178"
  },
  {
    "id": "arXiv:2106.03180",
    "title": "Transformer in Convolutional Neural Networks",
    "abstract": "We tackle the low-efficiency flaw of vision transformer caused by the high\ncomputational/space complexity in Multi-Head Self-Attention (MHSA). To this\nend, we propose the Hierarchical MHSA (H-MHSA), whose representation is\ncomputed in a hierarchical manner. Specifically, our H-MHSA first learns\nfeature relationships within small grids by viewing image patches as tokens.\nThen, small grids are merged into larger ones, within which feature\nrelationship is learned by viewing each small grid at the preceding step as a\ntoken. This process is iterated to gradually reduce the number of tokens. The\nH-MHSA module is readily pluggable into any CNN architectures and amenable to\ntraining via backpropagation. We call this new backbone TransCNN, and it\nessentially inherits the advantages of both transformer and CNN. Experiments\ndemonstrate that TransCNN achieves state-of-the-art accuracy for image\nrecognition. Code and pretrained models are available at\nhttps://github.com/yun-liu/TransCNN. This technical report will keep updating\nby adding more experiments.",
    "descriptor": "",
    "authors": [
      "Yun Liu",
      "Guolei Sun",
      "Yu Qiu",
      "Le Zhang",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03180"
  },
  {
    "id": "arXiv:2106.03181",
    "title": "Transient Chaos in BERT",
    "abstract": "Language is an outcome of our complex and dynamic human-interactions and the\ntechnique of natural language processing (NLP) is hence built on human\nlinguistic activities. Bidirectional Encoder Representations from Transformers\n(BERT) has recently gained its popularity by establishing the state-of-the-art\nscores in several NLP benchmarks. A Lite BERT (ALBERT) is literally\ncharacterized as a lightweight version of BERT, in which the number of BERT\nparameters is reduced by repeatedly applying the same neural network called\nTransformer's encoder layer. By pre-training the parameters with a massive\namount of natural language data, ALBERT can convert input sentences into\nversatile high-dimensional vectors potentially capable of solving multiple NLP\ntasks. In that sense, ALBERT can be regarded as a well-designed\nhigh-dimensional dynamical system whose operator is the Transformer's encoder,\nand essential structures of human language are thus expected to be encapsulated\nin its dynamics. In this study, we investigated the embedded properties of\nALBERT to reveal how NLP tasks are effectively solved by exploiting its\ndynamics. We thereby aimed to explore the nature of human language from the\ndynamical expressions of the NLP model. Our short-term analysis clarified that\nthe pre-trained model stably yields trajectories with higher dimensionality,\nwhich would enhance the expressive capacity required for NLP tasks. Also, our\nlong-term analysis revealed that ALBERT intrinsically shows transient chaos, a\ntypical nonlinear phenomenon showing chaotic dynamics only in its transient,\nand the pre-trained ALBERT model tends to produce the chaotic trajectory for a\nsignificantly longer time period compared to a randomly-initialized one. Our\nresults imply that local chaoticity would contribute to improving NLP\nperformance, uncovering a novel aspect in the role of chaotic dynamics in human\nlanguage behaviors.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Katsuma Inoue",
      "Soh Ohara",
      "Yasuo Kuniyoshi",
      "Kohei Nakajima"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2106.03181"
  },
  {
    "id": "arXiv:2106.03185",
    "title": "Tight Lower Bounds for the RMR Complexity of Recoverable Mutual  Exclusion",
    "abstract": "We present a tight RMR complexity lower bound for the recoverable mutual\nexclusion (RME) problem, defined by Golab and Ramaraju \\cite{GR2019a}. In\nparticular, we show that any $n$-process RME algorithm using only atomic read,\nwrite, fetch-and-store, fetch-and-increment, and compare-and-swap operations,\nhas an RMR complexity of $\\Omega(\\log n/\\log\\log n)$ on the CC and DSM model.\nThis lower bound covers all realistic synchronization primitives that have been\nused in RME algorithms and matches the best upper bounds of algorithms\nemploying swap objects (e.g., [5,6,10]).\nAlgorithms with better RMR complexity than that have only been obtained by\neither (i) assuming that all failures are system-wide [7], (ii) employing\nfetch-and-add objects of size $(\\log n)^{\\omega(1)}$ [12], or (iii) using\nartificially defined synchronization primitives that are not available in\nactual systems [6,9].",
    "descriptor": "\nComments: 36 pages, 0 figures\n",
    "authors": [
      "David Yu Cheng Chan",
      "Philipp Woelfel"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03185"
  },
  {
    "id": "arXiv:2106.03186",
    "title": "On the Power of Shallow Learning",
    "abstract": "A deluge of recent work has explored equivalences between wide neural\nnetworks and kernel methods. A central theme is that one can analytically find\nthe kernel corresponding to a given wide network architecture, but despite\nmajor implications for architecture design, no work to date has asked the\nconverse question: given a kernel, can one find a network that realizes it? We\naffirmatively answer this question for fully-connected architectures,\ncompletely characterizing the space of achievable kernels. Furthermore, we give\na surprising constructive proof that any kernel of any wide, deep,\nfully-connected net can also be achieved with a network with just one hidden\nlayer and a specially-designed pointwise activation function. We experimentally\nverify our construction and demonstrate that, by just choosing the activation\nfunction, we can design a wide shallow network that mimics the generalization\nperformance of any wide, deep, fully-connected network.",
    "descriptor": "\nComments: 13 pages, 5 figures\n",
    "authors": [
      "James B. Simon",
      "Sajant Anand",
      "Michael R. DeWeese"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03186"
  },
  {
    "id": "arXiv:2106.03188",
    "title": "Combinatorial Optimization for Panoptic Segmentation: An End-to-End  Trainable Approach",
    "abstract": "We propose an end-to-end trainable architecture for simultaneous semantic and\ninstance segmentation (a.k.a. panoptic segmentation) consisting of a\nconvolutional neural network and an asymmetric multiway cut problem solver. The\nlatter solves a combinatorial optimization problem that elegantly incorporates\nsemantic and boundary predictions to produce a panoptic labeling. Our\nformulation allows to directly maximize a smooth surrogate of the panoptic\nquality metric by backpropagating the gradient through the optimization\nproblem. Experimental evaluation shows improvement of end-to-end learning\nw.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our\napproach shows the utility of using combinatorial optimization in tandem with\ndeep learning in a challenging large scale real-world problem and showcases\nbenefits and insights into training such an architecture end-to-end.",
    "descriptor": "",
    "authors": [
      "Ahmed Abbas",
      "Paul Swoboda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03188"
  },
  {
    "id": "arXiv:2106.03191",
    "title": "An $L^p$-weak Galerkin method for second order elliptic equations in  non-divergence form",
    "abstract": "This article presents a new primal-dual weak Galerkin method for second order\nelliptic equations in non-divergence form. The new method is devised as a\nconstrained $L^p$-optimization problem with constraints that mimic the second\norder elliptic equation by using the discrete weak Hessian locally on each\nelement. An equivalent min-max characterization is derived to show the\nexistence and uniqueness of the numerical solution. Optimal order error\nestimates are established for the numerical solution under the discrete\n$W^{2,p}$ norm, as well as the standard $W^{1,p}$ and $L^p$ norms. An\nequivalent characterization of the optimization problem in term of a system of\nfixed-point equations via the proximity operator is presented. An iterative\nalgorithm is designed based on the fixed-point equations to solve the\noptimization problems. Implementation of the iterative algorithm is studied and\nconvergence of the iterative algorithm is established. Numerical experiments\nfor both smooth and non-smooth coefficients problems are presented to verify\nthe theoretical findings.",
    "descriptor": "\nComments: 36 pages\n",
    "authors": [
      "Waixiang Cao",
      "Junping Wang",
      "Yuesheng Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03191"
  },
  {
    "id": "arXiv:2106.03192",
    "title": "Let's be explicit about that: Distant supervision for implicit discourse  relation classification via connective prediction",
    "abstract": "In implicit discourse relation classification, we want to predict the\nrelation between adjacent sentences in the absence of any overt discourse\nconnectives. This is challenging even for humans, leading to shortage of\nannotated data, a fact that makes the task even more difficult for supervised\nmachine learning approaches. In the current study, we perform implicit\ndiscourse relation classification without relying on any labeled implicit\nrelation. We sidestep the lack of data through explicitation of implicit\nrelations to reduce the task to two sub-problems: language modeling and\nexplicit discourse relation classification, a much easier problem. Our\nexperimental results show that this method can even marginally outperform the\nstate-of-the-art, in spite of being much simpler than alternative models of\ncomparable performance. Moreover, we show that the achieved performance is\nrobust across domains as suggested by the zero-shot experiments on a completely\ndifferent domain. This indicates that recent advances in language modeling have\nmade language models sufficiently good at capturing inter-sentence relations\nwithout the help of explicit discourse markers.",
    "descriptor": "\nComments: To be presented at Unimplicit 2021\n",
    "authors": [
      "Murathan Kurfal\u0131",
      "Robert \u00d6stling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03192"
  },
  {
    "id": "arXiv:2106.03193",
    "title": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual  Machine Translation",
    "abstract": "One of the biggest challenges hindering progress in low-resource and\nmultilingual machine translation is the lack of good evaluation benchmarks.\nCurrent evaluation benchmarks either lack good coverage of low-resource\nlanguages, consider only restricted domains, or are low quality because they\nare constructed using semi-automatic procedures. In this work, we introduce the\nFLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from\nEnglish Wikipedia and covering a variety of different topics and domains. These\nsentences have been translated in 101 languages by professional translators\nthrough a carefully controlled process. The resulting dataset enables better\nassessment of model quality on the long tail of low-resource languages,\nincluding the evaluation of many-to-many multilingual translation systems, as\nall translations are multilingually aligned. By publicly releasing such a\nhigh-quality and high-coverage dataset, we hope to foster progress in the\nmachine translation community and beyond.",
    "descriptor": "",
    "authors": [
      "Naman Goyal",
      "Cynthia Gao",
      "Vishrav Chaudhary",
      "Peng-Jen Chen",
      "Guillaume Wenzek",
      "Da Ju",
      "Sanjana Krishnan",
      "Marc'Aurelio Ranzato",
      "Francisco Guzman",
      "Angela Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03193"
  },
  {
    "id": "arXiv:2106.03194",
    "title": "Robust Implicit Networks via Non-Euclidean Contractions",
    "abstract": "Implicit neural networks, a.k.a., deep equilibrium networks, are a class of\nimplicit-depth learning models where function evaluation is performed by\nsolving a fixed point equation. They generalize classic feedforward models and\nare equivalent to infinite-depth weight-tied feedforward networks. While\nimplicit models show improved accuracy and significant reduction in memory\nconsumption, they can suffer from ill-posedness and convergence instability.\nThis paper provides a new framework to design well-posed and robust implicit\nneural networks based upon contraction theory for the non-Euclidean norm\n$\\ell_\\infty$. Our framework includes (i) a novel condition for well-posedness\nbased on one-sided Lipschitz constants, (ii) an average iteration for computing\nfixed-points, and (iii) explicit estimates on input-output Lipschitz constants.\nAdditionally, we design a training problem with the well-posedness condition\nand the average iteration as constraints and, to achieve robust models, with\nthe input-output Lipschitz constant as a regularizer. Our $\\ell_\\infty$\nwell-posedness condition leads to a larger polytopic training search space than\nexisting conditions and our average iteration enjoys accelerated convergence.\nFinally, we perform several numerical experiments for function estimation and\ndigit classification through the MNIST data set. Our numerical results\ndemonstrate improved accuracy and robustness of the implicit models with\nsmaller input-output Lipschitz bounds.",
    "descriptor": "",
    "authors": [
      "Saber Jafarpour",
      "Alexander Davydov",
      "Anton V. Proskurnikov",
      "Francesco Bullo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03194"
  },
  {
    "id": "arXiv:2106.03195",
    "title": "Meta-Learning Reliable Priors in the Function Space",
    "abstract": "Meta-Learning promises to enable more data-efficient inference by harnessing\nprevious experience from related learning tasks. While existing meta-learning\nmethods help us to improve the accuracy of our predictions in face of data\nscarcity, they fail to supply reliable uncertainty estimates, often being\ngrossly overconfident in their predictions. Addressing these shortcomings, we\nintroduce a novel meta-learning framework, called F-PACOH, that treats\nmeta-learned priors as stochastic processes and performs meta-level\nregularization directly in the function space. This allows us to directly steer\nthe probabilistic predictions of the meta-learner towards high epistemic\nuncertainty in regions of insufficient meta-training data and, thus, obtain\nwell-calibrated uncertainty estimates. Finally, we showcase how our approach\ncan be integrated with sequential decision making, where reliable uncertainty\nquantification is imperative. In our benchmark study on meta-learning for\nBayesian Optimization (BO), F-PACOH significantly outperforms all other\nmeta-learners and standard baselines. Even in a challenging lifelong BO\nsetting, where optimization tasks arrive one at a time and the meta-learner\nneeds to build up informative prior knowledge incrementally, our proposed\nmethod demonstrates strong positive transfer.",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Jonas Rothfuss",
      "Dominique Heyn",
      "Jinfan Chen",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03195"
  },
  {
    "id": "arXiv:2106.03207",
    "title": "Mitigating Covariate Shift in Imitation Learning via Offline Data  Without Great Coverage",
    "abstract": "This paper studies offline Imitation Learning (IL) where an agent learns to\nimitate an expert demonstrator without additional online environment\ninteractions. Instead, the learner is presented with a static offline dataset\nof state-action-next state transition triples from a potentially less\nproficient behavior policy. We introduce Model-based IL from Offline data\n(MILO): an algorithmic framework that utilizes the static dataset to solve the\noffline IL problem efficiently both in theory and in practice. In theory, even\nif the behavior policy is highly sub-optimal compared to the expert, we show\nthat as long as the data from the behavior policy provides sufficient coverage\non the expert state-action traces (and with no necessity for a global coverage\nover the entire state-action space), MILO can provably combat the covariate\nshift issue in IL. Complementing our theory results, we also demonstrate that a\npractical implementation of our approach mitigates covariate shift on benchmark\nMuJoCo continuous control tasks. We demonstrate that with behavior policies\nwhose performances are less than half of that of the expert, MILO still\nsuccessfully imitates with an extremely low number of expert state-action pairs\nwhile traditional offline IL method such as behavior cloning (BC) fails\ncompletely. Source code is provided at https://github.com/jdchang1/milo.",
    "descriptor": "\nComments: 42 pages, 5 figures, 7 tables\n",
    "authors": [
      "Jonathan D. Chang",
      "Masatoshi Uehara",
      "Dhruv Sreenivas",
      "Rahul Kidambi",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03207"
  },
  {
    "id": "arXiv:2106.03209",
    "title": "Adversarial Classification of the Attacks on Smart Grids Using Game  Theory and Deep Learning",
    "abstract": "Smart grids are vulnerable to cyber-attacks. This paper proposes a\ngame-theoretic approach to evaluate the variations caused by an attacker on the\npower measurements. Adversaries can gain financial benefits through the\nmanipulation of the meters of smart grids. On the other hand, there is a\ndefender that tries to maintain the accuracy of the meters. A zero-sum game is\nused to model the interactions between the attacker and defender. In this\npaper, two different defenders are used and the effectiveness of each defender\nin different scenarios is evaluated. Multi-layer perceptrons (MLPs) and\ntraditional state estimators are the two defenders that are studied in this\npaper. The utility of the defender is also investigated in adversary-aware and\nadversary-unaware situations. Our simulations suggest that the utility which is\ngained by the adversary drops significantly when the MLP is used as the\ndefender. It will be shown that the utility of the defender is variant in\ndifferent scenarios, based on the defender that is being used. In the end, we\nwill show that this zero-sum game does not yield a pure strategy, and the mixed\nstrategy of the game is calculated.",
    "descriptor": "\nComments: Accepted to ACM Workshop on Wireless Security and Machine Learning (WiseML) 2021\n",
    "authors": [
      "Kian Hamedani",
      "Lingjia Liu",
      "Jithin Jagannath",
      "Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03209"
  },
  {
    "id": "arXiv:2106.03210",
    "title": "Alpha Matte Generation from Single Input for Portrait Matting",
    "abstract": "Portrait matting is an important research problem with a wide range of\napplications, such as video conference app, image/video editing, and\npost-production. The goal is to predict an alpha matte that identifies the\neffect of each pixel on the foreground subject. Traditional approaches and most\nof the existing works utilized an additional input, e.g., trimap, background\nimage, to predict alpha matte. However, providing additional input is not\nalways practical. Besides, models are too sensitive to these additional inputs.\nIn this paper, we introduce an additional input-free approach to perform\nportrait matting using Generative Adversarial Nets (GANs). We divide the main\ntask into two subtasks. For this, we propose a segmentation network for the\nperson segmentation and the alpha generation network for alpha matte\nprediction. While the segmentation network takes an input image and produces a\ncoarse segmentation map, the alpha generation network utilizes the same input\nimage as well as a coarse segmentation map that is produced by the segmentation\nnetwork to predict the alpha matte. Besides, we present a segmentation encoding\nblock to downsample the coarse segmentation map and provide feature\nrepresentation to the residual block. Furthermore, we propose border loss to\npenalize only the borders of the subject separately which is more likely to be\nchallenging and we also adapt perceptual loss for portrait matting. To train\nthe proposed system, we combine two different popular training datasets to\nimprove the amount of data as well as diversity to address domain shift\nproblems in the inference time. We tested our model on three different\nbenchmark datasets, namely Adobe Image Matting dataset, Portrait Matting\ndataset, and Distinctions dataset. The proposed method outperformed the MODNet\nmethod that also takes a single input.",
    "descriptor": "",
    "authors": [
      "Dogucan Yaman",
      "Haz\u0131m Kemal Ekenel",
      "Alexander Waibel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03210"
  },
  {
    "id": "arXiv:2106.03211",
    "title": "Distributed Learning and its Application for Time-Series Prediction",
    "abstract": "Extreme events are occurrences whose magnitude and potential cause extensive\ndamage on people, infrastructure, and the environment. Motivated by the extreme\nnature of the current global health landscape, which is plagued by the\ncoronavirus pandemic, we seek to better understand and model extreme events.\nModeling extreme events is common in practice and plays an important role in\ntime-series prediction applications. Our goal is to (i) compare and investigate\nthe effect of some common extreme events modeling methods to explore which\nmethod can be practical in reality and (ii) accelerate the deep learning\ntraining process, which commonly uses deep recurrent neural network (RNN), by\nimplementing the asynchronous local Stochastic Gradient Descent (SGD) framework\namong multiple compute nodes. In order to verify our distributed extreme events\nmodeling, we evaluate our proposed framework on a stock data set S\\&P500, with\na standard recurrent neural network. Our intuition is to explore the (best)\nextreme events modeling method which could work well under the distributed deep\nlearning setting. Moreover, by using asynchronous distributed learning, we aim\nto significantly reduce the communication cost among the compute nodes and\ncentral server, which is the main bottleneck of almost all distributed learning\nframeworks.\nWe implement our proposed work and evaluate its performance on representative\ndata sets, such as S\\&P500 stock in $5$-year period. The experimental results\nvalidate the correctness of the design principle and show a significant\ntraining duration reduction upto $8$x, compared to the baseline single compute\nnode. Our results also show that our proposed work can achieve the same level\nof test accuracy, compared to the baseline setting.",
    "descriptor": "\nComments: 8 pages, 10 figures, and 2 tables\n",
    "authors": [
      "Nhuong V. Nguyen",
      "Sybille Legitime"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03211"
  },
  {
    "id": "arXiv:2106.03213",
    "title": "On Local Aggregation in Heterophilic Graphs",
    "abstract": "Many recent works have studied the performance of Graph Neural Networks\n(GNNs) in the context of graph homophily - a label-dependent measure of\nconnectivity. Traditional GNNs generate node embeddings by aggregating\ninformation from a node's neighbors in the graph. Recent results in node\nclassification tasks show that this local aggregation approach performs poorly\nin graphs with low homophily (heterophilic graphs). Several mechanisms have\nbeen proposed to improve the accuracy of GNNs on such graphs by increasing the\naggregation range of a GNN layer, either through multi-hop aggregation, or\nthrough long-range aggregation from distant nodes. In this paper, we show that\nproperly tuned classical GNNs and multi-layer perceptrons match or exceed the\naccuracy of recent long-range aggregation methods on heterophilic graphs. Thus,\nour results highlight the need for alternative datasets to benchmark long-range\nGNN aggregation mechanisms. We also show that homophily is a poor measure of\nthe information in a node's local neighborhood and propose the Neighborhood\nInformation Content(NIC) metric, which is a novel information-theoretic graph\nmetric. We argue that NIC is more relevant for local aggregation methods as\nused by GNNs. We show that, empirically, it correlates better with GNN accuracy\nin node classification tasks than homophily.",
    "descriptor": "",
    "authors": [
      "Hesham Mostafa",
      "Marcel Nassar",
      "Somdeb Majumdar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03213"
  },
  {
    "id": "arXiv:2106.03215",
    "title": "PreferenceNet: Encoding Human Preferences in Auction Design with Deep  Learning",
    "abstract": "The design of optimal auctions is a problem of interest in economics, game\ntheory and computer science. Despite decades of effort, strategyproof,\nrevenue-maximizing auction designs are still not known outside of restricted\nsettings. However, recent methods using deep learning have shown some success\nin approximating optimal auctions, recovering several known solutions and\noutperforming strong baselines when optimal auctions are not known. In addition\nto maximizing revenue, auction mechanisms may also seek to encourage socially\ndesirable constraints such as allocation fairness or diversity. However, these\nphilosophical notions neither have standardization nor do they have widely\naccepted formal definitions. In this paper, we propose PreferenceNet, an\nextension of existing neural-network-based auction mechanisms to encode\nconstraints using (potentially human-provided) exemplars of desirable\nallocations. In addition, we introduce a new metric to evaluate an auction\nallocations' adherence to such socially desirable constraints and demonstrate\nthat our proposed method is competitive with current state-of-the-art\nneural-network based auction designs. We validate our approach through human\nsubject research and show that we are able to effectively capture real human\npreferences. Our code is available at\nhttps://github.com/neeharperi/PreferenceNet",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Neehar Peri",
      "Michael J. Curry",
      "Samuel Dooley",
      "John P. Dickerson"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.03215"
  },
  {
    "id": "arXiv:2106.03216",
    "title": "On Memorization in Probabilistic Deep Generative Models",
    "abstract": "Recent advances in deep generative models have led to impressive results in a\nvariety of application domains. Motivated by the possibility that deep learning\nmodels might memorize part of the input data, there have been increased efforts\nto understand how memorization can occur. In this work, we extend a recently\nproposed measure of memorization for supervised learning (Feldman, 2019) to the\nunsupervised density estimation problem and simplify the accompanying\nestimator. Next, we present an exploratory study that demonstrates how\nmemorization can arise in probabilistic deep generative models, such as\nvariational autoencoders. This reveals that the form of memorization to which\nthese models are susceptible differs fundamentally from mode collapse and\noverfitting. Finally, we discuss several strategies that can be used to limit\nmemorization in practice.",
    "descriptor": "",
    "authors": [
      "Gerrit J. J. van den Burg",
      "Christopher K. I. Williams"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03216"
  },
  {
    "id": "arXiv:2106.03219",
    "title": "Experience Report: Writing A Portable GPU Runtime with OpenMP 5.1",
    "abstract": "GPU runtimes are historically implemented in CUDA or other vendor specific\nlanguages dedicated to GPU programming. In this work we show that OpenMP 5.1,\nwith minor compiler extensions, is capable of replacing existing solutions\nwithout a performance penalty. The result is a performant and portable GPU\nruntime that can be compiled with LLVM/Clang to Nvidia and AMD GPUs without the\nneed for CUDA or HIP during its development and compilation.\nWhile we tried to be OpenMP compliant, we identified the need for compiler\nextensions to achieve the CUDA performance with our OpenMP runtime. We hope\nthat future versions of OpenMP adopt our extensions to make device programming\nin OpenMP also portable across compilers, not only across execution platforms.\nThe library we ported to OpenMP is the OpenMP device runtime that provides\nOpenMP functionality on the GPU. This work opens the door for shipping OpenMP\noffloading with a Linux distribution's LLVM package as the package manager\nwould not need a vendor SDK to build the compiler and runtimes. Furthermore,\nour OpenMP device runtime can support a new GPU target through the use of a few\ncompiler intrinsics rather than requiring a reimplementation of the entire\nruntime.",
    "descriptor": "",
    "authors": [
      "Shilei Tian",
      "Jon Chesterfield",
      "Johannes Doerfert",
      "Barbara Chapman"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03219"
  },
  {
    "id": "arXiv:2106.03220",
    "title": "PYROBOCOP : Python-based Robotic Control & Optimization Package for  Manipulation and Collision Avoidance",
    "abstract": "PYROBOCOP is a lightweight Python-based package for control and optimization\nof robotic systems described by nonlinear Differential Algebraic Equations\n(DAEs). In particular, the package can handle systems with contacts that are\ndescribed by complementarity constraints and provides a general framework for\nspecifying obstacle avoidance constraints. The package performs direct\ntranscription of the DAEs into a set of nonlinear equations by performing\northogonal collocation on finite elements. The resulting optimization problem\nbelongs to the class of Mathematical Programs with Complementarity Constraints\n(MPCCs). MPCCs fail to satisfy commonly assumed constraint qualifications and\nrequire special handling of the complementarity constraints in order for\nNonLinear Program (NLP) solvers to solve them effectively. PYROBOCOP provides\nautomatic reformulation of the complementarity constraints that enables NLP\nsolvers to perform optimization of robotic systems. The package is interfaced\nwith ADOLC for obtaining sparse derivatives by automatic differentiation and\nIPOPT for performing optimization. We demonstrate the effectiveness of our\napproach in terms of speed and flexibility. We provide several numerical\nexamples for several robotic systems with collision avoidance as well as\ncontact constraints represented using complementarity constraints. We provide\ncomparisons with other open source optimization packages like CasADi and Pyomo .",
    "descriptor": "\nComments: Under review at IJRR\n",
    "authors": [
      "Arvind U. Raghunathan",
      "Devesh K. Jha",
      "Diego Romeres"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03220"
  },
  {
    "id": "arXiv:2106.03221",
    "title": "PAC Best Arm Identification Under a Deadline",
    "abstract": "We study $(\\epsilon, \\delta)$-PAC best arm identification, where a\ndecision-maker must identify an $\\epsilon$-optimal arm with probability at\nleast $1 - \\delta$, while minimizing the number of arm pulls (samples). Most of\nthe work on this topic is in the sequential setting, where there is no\nconstraint on the \\emph{time} taken to identify such an arm; this allows the\ndecision-maker to pull one arm at a time. In this work, the decision-maker is\ngiven a deadline of $T$ rounds, where, on each round, it can adaptively choose\nwhich arms to pull and how many times to pull them; this distinguishes the\nnumber of decisions made (i.e., time or number of rounds) from the number of\nsamples acquired (cost). Such situations occur in clinical trials, where one\nmay need to identify a promising treatment under a deadline while minimizing\nthe number of test subjects, or in simulation-based studies run on the cloud,\nwhere we can elastically scale up or down the number of virtual machines to\nconduct as many experiments as we wish, but need to pay for the resource-time\nused. As the decision-maker can only make $T$ decisions, she may need to pull\nsome arms excessively relative to a sequential algorithm in order to perform\nwell on all possible problems. We formalize this added difficulty with two\nhardness results that indicate that unlike sequential settings, the ability to\nadapt to the problem difficulty is constrained by the finite deadline. We\npropose Elastic Batch Racing (EBR), a novel algorithm for this setting and\nbound its sample complexity, showing that EBR is optimal with respect to both\nhardness results. We present simulations evaluating EBR in this setting, where\nit outperforms baselines by several orders of magnitude.",
    "descriptor": "\nComments: In submission\n",
    "authors": [
      "Brijen Thananjeyan",
      "Kirthevasan Kandasamy",
      "Ion Stoica",
      "Michael I. Jordan",
      "Ken Goldberg",
      "Joseph E. Gonzalez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03221"
  },
  {
    "id": "arXiv:2106.03223",
    "title": "Few-shot segmentation of medical images based on meta-learning with  implicit gradients",
    "abstract": "Classical supervised methods commonly used often suffer from the requirement\nof an abudant number of training samples and are unable to generalize on unseen\ndatasets. As a result, the broader application of any trained model is very\nlimited in clinical settings. However, few-shot approaches can minimize the\nneed for enormous reliable ground truth labels that are both labor intensive\nand expensive. To this end, we propose to exploit an optimization-based\nimplicit model agnostic meta-learning {iMAML} algorithm in a few-shot setting\nfor medical image segmentation. Our approach can leverage the learned weights\nfrom a diverse set of training samples and can be deployed on a new unseen\ndataset. We show that unlike classical few-shot learning approaches, our method\nhas improved generalization capability. To our knowledge, this is the first\nwork that exploits iMAML for medical image segmentation. Our quantitative\nresults on publicly available skin and polyp datasets show that the proposed\nmethod outperforms the naive supervised baseline model and two recent few-shot\nsegmentation approaches by large margins.",
    "descriptor": "",
    "authors": [
      "Rabindra Khadga",
      "Debesh Jha",
      "Sharib Ali",
      "Steven Hicks",
      "Vajira Thambawita",
      "Michael A. Riegler",
      "P\u00e5l Halvorsen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03223"
  },
  {
    "id": "arXiv:2106.03225",
    "title": "Efficient Lottery Ticket Finding: Less Data is More",
    "abstract": "The lottery ticket hypothesis (LTH) reveals the existence of winning tickets\n(sparse but critical subnetworks) for dense networks, that can be trained in\nisolation from random initialization to match the latter's accuracies. However,\nfinding winning tickets requires burdensome computations in the\ntrain-prune-retrain process, especially on large-scale datasets (e.g.,\nImageNet), restricting their practical benefits. This paper explores a new\nperspective on finding lottery tickets more efficiently, by doing so only with\na specially selected subset of data, called Pruning-Aware Critical set (PrAC\nset), rather than using the full training set. The concept of PrAC set was\ninspired by the recent observation, that deep networks have samples that are\neither hard to memorize during training, or easy to forget during pruning. A\nPrAC set is thus hypothesized to capture those most challenging and informative\nexamples for the dense model. We observe that a high-quality winning ticket can\nbe found with training and pruning the dense network on the very compact PrAC\nset, which can substantially save training iterations for the ticket finding\nprocess. Extensive experiments validate our proposal across diverse datasets\nand network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny\nImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set\nsizes. On top of them, we can obtain the same competitive winning tickets for\nthe corresponding dense networks, yet saving up to 82.85%~92.77%,\n63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially,\nwe show that a PrAC set found is reusable across different network\narchitectures, which can amortize the extra cost of finding PrAC sets, yielding\na practical regime for efficient lottery ticket finding.",
    "descriptor": "",
    "authors": [
      "Zhenyu Zhang",
      "Xuxi Chen",
      "Tianlong Chen",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03225"
  },
  {
    "id": "arXiv:2106.03228",
    "title": "Distributional Reinforcement Learning with Unconstrained Monotonic  Neural Networks",
    "abstract": "The distributional reinforcement learning (RL) approach advocates for\nrepresenting the complete probability distribution of the random return instead\nof only modelling its expectation. A distributional RL algorithm may be\ncharacterised by two main components, namely the representation and\nparameterisation of the distribution and the probability metric defining the\nloss. This research considers the unconstrained monotonic neural network (UMNN)\narchitecture, a universal approximator of continuous monotonic functions which\nis particularly well suited for modelling different representations of a\ndistribution (PDF, CDF, quantile function). This property enables the\ndecoupling of the effect of the function approximator class from that of the\nprobability metric. The paper firstly introduces a methodology for learning\ndifferent representations of the random return distribution. Secondly, a novel\ndistributional RL algorithm named unconstrained monotonic deep Q-network\n(UMDQN) is presented. Lastly, in light of this new algorithm, an empirical\ncomparison is performed between three probability quasimetrics, namely the\nKullback-Leibler divergence, Cramer distance and Wasserstein distance. The\nresults call for a reconsideration of all probability metrics in distributional\nRL, which contrasts with the dominance of the Wasserstein distance in recent\npublications.",
    "descriptor": "",
    "authors": [
      "Thibaut Th\u00e9ate",
      "Antoine Wehenkel",
      "Adrien Bolland",
      "Gilles Louppe",
      "Damien Ernst"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03228"
  },
  {
    "id": "arXiv:2106.03232",
    "title": "A Targeted Assessment of Incremental Processing in Neural LanguageModels  and Humans",
    "abstract": "We present a targeted, scaled-up comparison of incremental processing in\nhumans and neural language models by collecting by-word reaction time data for\nsixteen different syntactic test suites across a range of structural phenomena.\nHuman reaction time data comes from a novel online experimental paradigm called\nthe Interpolated Maze task. We compare human reaction times to by-word\nprobabilities for four contemporary language models, with different\narchitectures and trained on a range of data set sizes. We find that across\nmany phenomena, both humans and language models show increased processing\ndifficulty in ungrammatical sentence regions with human and model `accuracy'\nscores (a la Marvin and Linzen(2018)) about equal. However, although language\nmodel outputs match humans in direction, we show that models systematically\nunder-predict the difference in magnitude of incremental processing difficulty\nbetween grammatical and ungrammatical sentences. Specifically, when models\nencounter syntactic violations they fail to accurately predict the longer\nreaction times observed in the human data. These results call into question\nwhether contemporary language models are approaching human-like performance for\nsensitivity to syntactic violations.",
    "descriptor": "\nComments: To appear at ACL 2021\n",
    "authors": [
      "Ethan Gotlieb Wilcox",
      "Pranali Vani",
      "Roger P. Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03232"
  },
  {
    "id": "arXiv:2106.03233",
    "title": "A Pre-training Oracle for Predicting Distances in Social Networks",
    "abstract": "In this paper, we propose a novel method to make distance predictions in\nreal-world social networks. As predicting missing distances is a difficult\nproblem, we take a two-stage approach. Structural parameters for families of\nsynthetic networks are first estimated from a small set of measurements of a\nreal-world network and these synthetic networks are then used to pre-train the\npredictive neural networks. Since our model first searches for the most\nsuitable synthetic graph parameters which can be used as an \"oracle\" to create\narbitrarily large training data sets, we call our approach \"Oracle Search\nPre-training\" (OSP). For example, many real-world networks exhibit a Power law\nstructure in their node degree distribution, so a Power law model can provide a\nfoundation for the desired oracle to generate synthetic pre-training networks,\nif the appropriate Power law graph parameters can be estimated. Accordingly, we\nconduct experiments on real-world Facebook, Email, and Train Bombing networks\nand show that OSP outperforms models without pre-training, models pre-trained\nwith inaccurate parameters, and other distance prediction schemes such as\nLow-rank Matrix Completion. In particular, we achieve a prediction error of\nless than one hop with only 1% of sampled distances from the social network.\nOSP can be easily extended to other domains such as random networks by choosing\nan appropriate model to generate synthetic training data, and therefore\npromises to impact many different network learning problems.",
    "descriptor": "\nComments: KDD conference 2021\n",
    "authors": [
      "Gunjan Mahindre",
      "Randy Paffenroth",
      "Anura Jayasumana",
      "Rasika Karkare"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03233"
  },
  {
    "id": "arXiv:2106.03234",
    "title": "A call for better unit testing for invariant risk minimisation",
    "abstract": "In this paper we present a controlled study on the linearized IRM framework\n(IRMv1) introduced in Arjovsky et al. (2020). We show that IRMv1 (and its\nvariants) framework can be potentially unstable under small changes to the\noptimal regressor. This can, notably, lead to worse generalisation to new\nenvironments, even compared with ERM which converges simply to the global\nminimum for all training environments mixed up all together. We also highlight\nthe isseus of scaling in the the IRMv1 setup. These observations highlight the\nimportance of rigorous evaluation and importance of unit-testing for measuring\nprogress towards IRM.",
    "descriptor": "\nComments: Manuscript v1.0\n",
    "authors": [
      "Chunyang Xiao",
      "Pranava Madhyastha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03234"
  },
  {
    "id": "arXiv:2106.03236",
    "title": "Graph2Graph Learning with Conditional Autoregressive Models",
    "abstract": "We present a graph neural network model for solving graph-to-graph learning\nproblems. Most deep learning on graphs considers ``simple'' problems such as\ngraph classification or regressing real-valued graph properties. For such\ntasks, the main requirement for intermediate representations of the data is to\nmaintain the structure needed for output, i.e., keeping classes separated or\nmaintaining the order indicated by the regressor. However, a number of learning\ntasks, such as regressing graph-valued output, generative models, or graph\nautoencoders, aim to predict a graph-structured output. In order to\nsuccessfully do this, the learned representations need to preserve far more\nstructure. We present a conditional auto-regressive model for graph-to-graph\nlearning and illustrate its representational capabilities via experiments on\nchallenging subgraph predictions from graph algorithmics; as a graph\nautoencoder for reconstruction and visualization; and on pretraining\nrepresentations that allow graph classification with limited labeled data.",
    "descriptor": "",
    "authors": [
      "Guan Wang",
      "Francois Bernard Lauze",
      "Aasa Feragen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03236"
  },
  {
    "id": "arXiv:2106.03242",
    "title": "Highlighting the Importance of Reducing Research Bias and Carbon  Emissions in CNNs",
    "abstract": "Convolutional neural networks (CNNs) have become commonplace in addressing\nmajor challenges in computer vision. Researchers are not only coming up with\nnew CNN architectures but are also researching different techniques to improve\nthe performance of existing architectures. However, there is a tendency to\nover-emphasize performance improvement while neglecting certain important\nvariables such as simplicity, versatility, the fairness of comparisons, and\nenergy efficiency. Overlooking these variables in architectural design and\nevaluation has led to research bias and a significantly negative environmental\nimpact. Furthermore, this can undermine the positive impact of research in\nusing deep learning models to tackle climate change. Here, we perform an\nextensive and fair empirical study of a number of proposed techniques to gauge\nthe utility of each technique for segmentation and classification. Our findings\nrestate the importance of favoring simplicity over complexity in model design\n(Occam's Razor). Furthermore, our results indicate that simple standardized\npractices can lead to a significant reduction in environmental impact with\nlittle drop in performance. We highlight that there is a need to rethink the\ndesign and evaluation of CNNs to alleviate the issue of research bias and\ncarbon emissions.",
    "descriptor": "",
    "authors": [
      "Ahmed Badar",
      "Arnav Varma",
      "Adrian Staniec",
      "Mahmoud Gamal",
      "Omar Magdy",
      "Haris Iqbal",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03242"
  },
  {
    "id": "arXiv:2106.03243",
    "title": "Neural Active Learning with Performance Guarantees",
    "abstract": "We investigate the problem of active learning in the streaming setting in\nnon-parametric regimes, where the labels are stochastically generated from a\nclass of functions on which we make no assumptions whatsoever. We rely on\nrecently proposed Neural Tangent Kernel (NTK) approximation tools to construct\na suitable neural embedding that determines the feature space the algorithm\noperates on and the learned model computed atop. Since the shape of the label\nrequesting threshold is tightly related to the complexity of the function to be\nlearned, which is a-priori unknown, we also derive a version of the algorithm\nwhich is agnostic to any prior knowledge. This algorithm relies on a regret\nbalancing scheme to solve the resulting online model selection problem, and is\ncomputationally efficient. We prove joint guarantees on the cumulative regret\nand number of requested labels which depend on the complexity of the labeling\nfunction at hand. In the linear case, these guarantees recover known minimax\nresults of the generalization error as a function of the label complexity in a\nstandard statistical learning setting.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Pranjal Awasthi",
      "Christoph Dann",
      "Claudio Gentile",
      "Ayush Sekhari",
      "Zhilei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03243"
  },
  {
    "id": "arXiv:2106.03245",
    "title": "Verification in the Loop: Correct-by-Construction Control Learning with  Reach-avoid Guarantees",
    "abstract": "In the current control design of safety-critical autonomous systems, formal\nverification techniques are typically applied after the controller is designed\nto evaluate whether the required properties (e.g., safety) are satisfied.\nHowever, due to the increasing system complexity and the fundamental hardness\nof designing a controller with formal guarantees, such an open-loop process of\ndesign-then-verify often results in many iterations and fails to provide the\nnecessary guarantees. In this paper, we propose a correct-by-construction\ncontrol learning framework that integrates the verification into the control\ndesign process in a closed-loop manner, i.e., design-while-verify.\nSpecifically, we leverage the verification results (computed reachable set of\nthe system state) to construct feedback metrics for control learning, which\nmeasure how likely the current design of control parameters can meet the\nrequired reach-avoid property for safety and goal-reaching. We formulate an\noptimization problem based on such metrics for tuning the controller\nparameters, and develop an approximated gradient descent algorithm with a\ndifference method to solve the optimization problem and learn the controller.\nThe learned controller is formally guaranteed to meet the required reach-avoid\nproperty. By treating verifiability as a first-class objective and effectively\nleveraging the verification results during the control learning process, our\napproach can significantly improve the chance of finding a control design with\nformal property guarantees. This is demonstrated via a set of experiments on\nboth linear and non-linear systems that use model-based or neural network based\ncontrollers.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Yixuan Wang",
      "Chao Huang",
      "Zhaoran Wang",
      "Zhilu Wang",
      "Qi Zhu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.03245"
  },
  {
    "id": "arXiv:2106.03246",
    "title": "Extractive Research Slide Generation Using Windowed Labeling Ranking",
    "abstract": "Presentation slides describing the content of scientific and technical papers\nare an efficient and effective way to present that work. However, manually\ngenerating presentation slides is labor intensive. We propose a method to\nautomatically generate slides for scientific papers based on a corpus of 5000\npaper-slide pairs compiled from conference proceedings websites. The sentence\nlabeling module of our method is based on SummaRuNNer, a neural sequence model\nfor extractive summarization. Instead of ranking sentences based on semantic\nsimilarities in the whole document, our algorithm measures importance and\nnovelty of sentences by combining semantic and lexical features within a\nsentence window. Our method outperforms several baseline methods including\nSummaRuNNer by a significant margin in terms of ROUGE score.",
    "descriptor": "",
    "authors": [
      "Athar Sefid",
      "Jian Wu",
      "Prasenjit Mitra",
      "Lee Giles"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03246"
  },
  {
    "id": "arXiv:2106.03251",
    "title": "DyDiff-VAE: A Dynamic Variational Framework for Information Diffusion  Prediction",
    "abstract": "This paper describes a novel diffusion model, DyDiff-VAE, for information\ndiffusion prediction on social media. Given the initial content and a sequence\nof forwarding users, DyDiff-VAE aims to estimate the propagation likelihood for\nother potential users and predict the corresponding user rankings. Inferring\nuser interests from diffusion data lies the foundation of diffusion prediction,\nbecause users often forward the information in which they are interested or the\ninformation from those who share similar interests. Their interests also evolve\nover time as the result of the dynamic social influence from neighbors and the\ntime-sensitive information gained inside/outside the social media. Existing\nworks fail to model users' intrinsic interests from the diffusion data and\nassume user interests remain static along the time. DyDiff-VAE advances the\nstate of the art in two directions: (i) We propose a dynamic encoder to infer\nthe evolution of user interests from observed diffusion data. (ii) We propose a\ndual attentive decoder to estimate the propagation likelihood by integrating\ninformation from both the initial cascade content and the forwarding user\nsequence. Extensive experiments on four real-world datasets from Twitter and\nYoutube demonstrate the advantages of the proposed model; we show that it\nachieves 43.3% relative gains over the best baseline on average. Moreover, it\nhas the lowest run-time compared with recurrent neural network based models.",
    "descriptor": "\nComments: In SIGIR'21\n",
    "authors": [
      "Ruijie Wang",
      "Zijie Huang",
      "Shengzhong Liu",
      "Huajie Shao",
      "Dongxin Liu",
      "Jinyang Li",
      "Tianshi Wang",
      "Dachun Sun",
      "Shuochao Yao",
      "Tarek Abdelzaher"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03251"
  },
  {
    "id": "arXiv:2106.03253",
    "title": "Tabular Data: Deep Learning is Not All You Need",
    "abstract": "A key element of AutoML systems is setting the types of models that will be\nused for each type of task. For classification and regression problems with\ntabular data, the use of tree ensemble models (like XGBoost) is usually\nrecommended. However, several deep learning models for tabular data have\nrecently been proposed, claiming to outperform XGBoost for some use-cases. In\nthis paper, we explore whether these deep models should be a recommended option\nfor tabular data, by rigorously comparing the new deep models to XGBoost on a\nvariety of datasets. In addition to systematically comparing their accuracy, we\nconsider the tuning and computation they require. Our study shows that XGBoost\noutperforms these deep models across the datasets, including datasets used in\nthe papers that proposed the deep models. We also demonstrate that XGBoost\nrequires much less tuning. On the positive side, we show that an ensemble of\nthe deep models and XGBoost performs better on these datasets than XGBoost\nalone.",
    "descriptor": "",
    "authors": [
      "Ravid Shwartz-Ziv",
      "Amitai Armon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03253"
  },
  {
    "id": "arXiv:2106.03254",
    "title": "Power System Transient Modeling and Simulation using Integrated Circuit",
    "abstract": "Transient stability analysis (TSA) plays an important role in power system\nanalysis to investigate the stability of power system. Traditionally, transient\nstability analysis methods have been developed using time domain simulation by\nmeans of numerical integration method. In this paper, a new approach is\nproposed to model power systems as an integrated circuit and simulate the power\nsystem dynamic behavior by integrated circuit simulator. The proposed method\nmodeled power grid, generator, governor, and exciter with high fidelity. The\npower system dynamic simulation accuracy and efficiency of the proposed\napproach are verified and demonstrated by case study on an IEEE standard\nsystem.",
    "descriptor": "\nComments: Has been accepted by 2021 PES General Meeting\n",
    "authors": [
      "Xiang Zhang",
      "Renchang Dai",
      "Peng Wei",
      "Yijing Liu",
      "Guangyi Liu",
      "Zhiwei Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03254"
  },
  {
    "id": "arXiv:2106.03257",
    "title": "Structured Reordering for Modeling Latent Alignments in Sequence  Transduction",
    "abstract": "Despite success in many domains, neural models struggle in settings where\ntrain and test examples are drawn from different distributions. In particular,\nin contrast to humans, conventional sequence-to-sequence (seq2seq) models fail\nto generalize systematically, i.e., interpret sentences representing novel\ncombinations of concepts (e.g., text segments) seen in training. Traditional\ngrammar formalisms excel in such settings by implicitly encoding alignments\nbetween input and output segments, but are hard to scale and maintain. Instead\nof engineering a grammar, we directly model segment-to-segment alignments as\ndiscrete structured latent variables within a neural seq2seq model. To\nefficiently explore the large space of alignments, we introduce a reorder-first\nalign-later framework whose central component is a neural reordering module\nproducing {\\it separable} permutations. We present an efficient dynamic\nprogramming algorithm performing exact marginal inference of separable\npermutations, and, thus, enabling end-to-end differentiable training of our\nmodel. The resulting seq2seq model exhibits better systematic generalization\nthan standard models on synthetic problems and NLP tasks (i.e., semantic\nparsing and machine translation).",
    "descriptor": "",
    "authors": [
      "Bailin Wang",
      "Mirella Lapata",
      "Ivan Titov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03257"
  },
  {
    "id": "arXiv:2106.03259",
    "title": "Understand and Improve Contrastive Learning Methods for Visual  Representation: A Review",
    "abstract": "Traditional supervised learning methods are hitting a bottleneck because of\ntheir dependency on expensive manually labeled data and their weaknesses such\nas limited generalization ability and vulnerability to adversarial attacks. A\npromising alternative, self-supervised learning, as a type of unsupervised\nlearning, has gained popularity because of its potential to learn effective\ndata representations without manual labeling. Among self-supervised learning\nalgorithms, contrastive learning has achieved state-of-the-art performance in\nseveral fields of research. This literature review aims to provide an\nup-to-date analysis of the efforts of researchers to understand the key\ncomponents and the limitations of self-supervised learning.",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Ran Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03259"
  },
  {
    "id": "arXiv:2106.03260",
    "title": "Error estimate of a decoupled numerical scheme for the  Cahn-Hilliard-Stokes-Darcy system",
    "abstract": "We analyze a fully discrete finite element numerical scheme for the\nCahn-Hilliard-Stokes-Darcy system that models two-phase flows in coupled free\nflow and porous media. To avoid a well-known difficulty associated with the\ncoupling between the Cahn-Hilliard equation and the fluid motion, we make use\nof the operator-splitting in the numerical scheme, so that these two solvers\nare decoupled, which in turn would greatly improve the computational\nefficiency. The unique solvability and the energy stability have been proved\nin~\\cite{CHW2017}. In this work, we carry out a detailed convergence analysis\nand error estimate for the fully discrete finite element scheme, so that the\noptimal rate convergence order is established in the energy norm, i.e.,, in the\n$\\ell^\\infty (0, T; H^1) \\cap \\ell^2 (0, T; H^2)$ norm for the phase variables,\nas well as in the $\\ell^\\infty (0, T; H^1) \\cap \\ell^2 (0, T; H^2)$ norm for\nthe velocity variable. Such an energy norm error estimate leads to a\ncancellation of a nonlinear error term associated with the convection part,\nwhich turns out to be a key step to pass through the analysis. In addition, a\ndiscrete $\\ell^2 (0;T; H^3)$ bound of the numerical solution for the phase\nvariables plays an important role in the error estimate, which is accomplished\nvia a discrete version of Gagliardo-Nirenberg inequality in the finite element\nsetting.",
    "descriptor": "",
    "authors": [
      "Wenbin Chen",
      "Daozhi Han",
      "Cheng Wang",
      "Shufen Wang",
      "Xiaoming Wang",
      "Yichao Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03260"
  },
  {
    "id": "arXiv:2106.03262",
    "title": "Low-complexity Voronoi shaping for the Gaussian channel",
    "abstract": "Voronoi constellations (VCs) are finite sets of vectors of a coding lattice\nenclosed by the translated Voronoi region of a shaping lattice, which is a\nsublattice of the coding lattice. In conventional VCs, the shaping lattice is a\nscaled-up version of the coding lattice. In this paper, we design\nlow-complexity VCs with a cubic coding lattice of up to 32 dimensions, in which\npseudo-Gray labeling is applied to minimize the bit error rate. The designed\nVCs have considerable shaping gains of up to 1.03 dB and finer choices of\nspectral efficiencies in practice. A mutual information estimation method and a\nlog-likelihood approximation method based on importance sampling for very large\nconstellations are proposed and applied to the designed VCs. With error-control\ncoding, the proposed VCs can have higher achievable information rates than the\nconventional scaled VCs because of their inherently good pseudo-Gray labeling\nfeature, with a lower decoding complexity.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "S. Li",
      "A. Mirani",
      "M. Karlsson",
      "E. Agrell"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03262"
  },
  {
    "id": "arXiv:2106.03269",
    "title": "Itihasa: A large-scale corpus for Sanskrit to English translation",
    "abstract": "This work introduces Itihasa, a large-scale translation dataset containing\n93,000 pairs of Sanskrit shlokas and their English translations. The shlokas\nare extracted from two Indian epics viz., The Ramayana and The Mahabharata. We\nfirst describe the motivation behind the curation of such a dataset and follow\nup with empirical analysis to bring out its nuances. We then benchmark the\nperformance of standard translation models on this corpus and show that even\nstate-of-the-art transformer architectures perform poorly, emphasizing the\ncomplexity of the dataset.",
    "descriptor": "\nComments: WAT 2021\n",
    "authors": [
      "Rahul Aralikatte",
      "Miryam de Lhoneux",
      "Anoop Kunchukuttan",
      "Anders S\u00f8gaard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03269"
  },
  {
    "id": "arXiv:2106.03270",
    "title": "Meta-learning for downstream aware and agnostic pretraining",
    "abstract": "Neural network pretraining is gaining attention due to its outstanding\nperformance in natural language processing applications. However, pretraining\nusually leverages predefined task sequences to learn general linguistic clues.\nThe lack of mechanisms in choosing proper tasks during pretraining makes the\nlearning and knowledge encoding inefficient. We thus propose using\nmeta-learning to select tasks that provide the most informative learning\nsignals in each episode of pretraining. With the proposed method, we aim to\nachieve better efficiency in computation and memory usage for the pretraining\nprocess and resulting networks while maintaining the performance. In this\npreliminary work, we discuss the algorithm of the method and its two variants,\ndownstream-aware and downstream-agnostic pretraining. Our experiment plan is\nalso summarized, while empirical results will be shared in our future works.",
    "descriptor": "\nComments: Extended abstract\n",
    "authors": [
      "Hongyin Luo",
      "Shuyan Dong",
      "Yung-Sung Chuang",
      "Shang-Wen Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03270"
  },
  {
    "id": "arXiv:2106.03271",
    "title": "Fixing Vulnerabilities Potentially Hinders Maintainability",
    "abstract": "Security is a requirement of utmost importance to produce high-quality\nsoftware. However, there is still a considerable amount of vulnerabilities\nbeing discovered and fixed almost weekly. We hypothesize that developers affect\nthe maintainability of their codebases when patching vulnerabilities. This\npaper evaluates the impact of patches to improve security on the\nmaintainability of open-source software. Maintainability is measured based on\nthe Better Code Hub's model of 10 guidelines on a dataset, including 1300\nsecurity-related commits. Results show evidence of a trade-off between security\nand maintainability for 41.90% of the cases, i.e., developers may hinder\nsoftware maintainability. Our analysis shows that 38.29% of patches increased\nsoftware complexity and 37.87% of patches increased the percentage of LOCs per\nunit. The implications of our study are that changes to codebases while\npatching vulnerabilities need to be performed with extra care; tools for patch\nrisk assessment should be integrated into the CI/CD pipeline; computer science\ncurricula needs to be updated; and, more secure programming languages are\nnecessary.",
    "descriptor": "\nComments: Accepted at the Empirical Software Engineering Journal\n",
    "authors": [
      "Sofia Reis",
      "Rui Abreu",
      "Luis Cruz"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03271"
  },
  {
    "id": "arXiv:2106.03273",
    "title": "Control-Oriented Model-Based Reinforcement Learning with Implicit  Differentiation",
    "abstract": "The shortcomings of maximum likelihood estimation in the context of\nmodel-based reinforcement learning have been highlighted by an increasing\nnumber of papers. When the model class is misspecified or has a limited\nrepresentational capacity, model parameters with high likelihood might not\nnecessarily result in high performance of the agent on a downstream control\ntask. To alleviate this problem, we propose an end-to-end approach for model\nlearning which directly optimizes the expected returns using implicit\ndifferentiation. We treat a value function that satisfies the Bellman\noptimality operator induced by the model as an implicit function of model\nparameters and show how to differentiate the function. We provide theoretical\nand empirical evidence highlighting the benefits of our approach in the model\nmisspecification regime compared to likelihood-based methods.",
    "descriptor": "\nComments: Code at this https URL\n",
    "authors": [
      "Evgenii Nikishin",
      "Romina Abachi",
      "Rishabh Agarwal",
      "Pierre-Luc Bacon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03273"
  },
  {
    "id": "arXiv:2106.03275",
    "title": "What if we Increase the Number of Objectives? Theoretical and Empirical  Implications for Many-objective Optimization",
    "abstract": "The difficulty of solving a multi-objective optimization problem is impacted\nby the number of objectives to be optimized. The presence of many objectives\ntypically introduces a number of challenges that affect the choice/design of\noptimization algorithms. This paper investigates the drivers of these\nchallenges from two angles: (i) the influence of the number of objectives on\nproblem characteristics and (ii) the practical behavior of commonly used\nprocedures and algorithms for coping with many objectives. In addition to\nreviewing various drivers, the paper makes theoretical contributions by\nquantifying some drivers and/or verifying these drivers empirically by carrying\nout experiments on multi-objective NK landscapes and other typical benchmarks.\nWe then make use of our theoretical and empirical findings to derive practical\nrecommendations to support algorithm design. Finally, we discuss remaining\ntheoretical gaps and opportunities for future research in the area of multi-\nand many-objective optimization.",
    "descriptor": "",
    "authors": [
      "Richard Allmendinger",
      "Andrzej Jaszkiewicz",
      "Arnaud Liefooghe",
      "Christiane Tammer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.03275"
  },
  {
    "id": "arXiv:2106.03278",
    "title": "Coordinating Followers to Reach Better Equilibria: End-to-End Gradient  Descent for Stackelberg Games",
    "abstract": "A growing body of work in game theory extends the traditional Stackelberg\ngame to settings with one leader and multiple followers who play a Nash\nequilibrium. Standard approaches for computing equilibria in these games\nreformulate the followers' best response as constraints in the leader's\noptimization problem. These reformulation approaches can sometimes be\neffective, but often get trapped in low-quality solutions when followers'\nobjectives are non-linear or non-quadratic. Moreover, these approaches assume a\nunique equilibrium or a specific equilibrium concept, e.g., optimistic or\npessimistic, which is a limiting assumption in many situations. To overcome\nthese limitations, we propose a stochastic gradient descent--based approach,\nwhere the leader's strategy is updated by differentiating through the\nfollowers' best responses. We frame the leader's optimization as a learning\nproblem against followers' equilibrium, which allows us to decouple the\nfollowers' equilibrium constraints from the leader's problem. This approach\nalso addresses cases with multiple equilibria and arbitrary equilibrium\nselection procedures by back-propagating through a sampled Nash equilibrium. To\nthis end, this paper introduces a novel concept called equilibrium flow to\nformally characterize the set of equilibrium selection processes where the\ngradient with respect to a sampled equilibrium is an unbiased estimate of the\ntrue gradient. We evaluate our approach experimentally against existing\nbaselines in three Stackelberg problems with multiple followers and find that\nin each case, our approach is able to achieve higher utility for the leader.",
    "descriptor": "",
    "authors": [
      "Kai Wang",
      "Lily Xu",
      "Andrew Perrault",
      "Michael K. Reiter",
      "Milind Tambe"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03278"
  },
  {
    "id": "arXiv:2106.03279",
    "title": "Learning MDPs from Features: Predict-Then-Optimize for Sequential  Decision Problems by Reinforcement Learning",
    "abstract": "In the predict-then-optimize framework, the objective is to train a\npredictive model, mapping from environment features to parameters of an\noptimization problem, which maximizes decision quality when the optimization is\nsubsequently solved. Recent work on decision-focused learning shows that\nembedding the optimization problem in the training pipeline can improve\ndecision quality and help generalize better to unseen tasks compared to relying\non an intermediate loss function for evaluating prediction quality. We study\nthe predict-then-optimize framework in the context of sequential decision\nproblems (formulated as MDPs) that are solved via reinforcement learning. In\nparticular, we are given environment features and a set of trajectories from\ntraining MDPs, which we use to train a predictive model that generalizes to\nunseen test MDPs without trajectories. Two significant computational challenges\narise in applying decision-focused learning to MDPs: (i) large state and action\nspaces make it infeasible for existing techniques to differentiate through MDP\nproblems, and (ii) the high-dimensional policy space, as parameterized by a\nneural network, makes differentiating through a policy expensive. We resolve\nthe first challenge by sampling provably unbiased derivatives to approximate\nand differentiate through optimality conditions, and the second challenge by\nusing a low-rank approximation to the high-dimensional sample-based\nderivatives. We implement both Bellman--based and policy gradient--based\ndecision-focused learning on three different MDP problems with missing\nparameters, and show that decision-focused learning performs better in\ngeneralization to unseen tasks.",
    "descriptor": "",
    "authors": [
      "Kai Wang",
      "Sanket Shat",
      "Haipeng Chen",
      "Andrew Perrault",
      "Finale Doshi-Velez",
      "Milind Tambe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03279"
  },
  {
    "id": "arXiv:2106.03283",
    "title": "Video Imprint",
    "abstract": "A new unified video analytics framework (ER3) is proposed for complex event\nretrieval, recognition and recounting, based on the proposed video imprint\nrepresentation, which exploits temporal correlations among image features\nacross video frames. With the video imprint representation, it is convenient to\nreverse map back to both temporal and spatial locations in video frames,\nallowing for both key frame identification and key areas localization within\neach frame. In the proposed framework, a dedicated feature alignment module is\nincorporated for redundancy removal across frames to produce the tensor\nrepresentation, i.e., the video imprint. Subsequently, the video imprint is\nindividually fed into both a reasoning network and a feature aggregation\nmodule, for event recognition/recounting and event retrieval tasks,\nrespectively. Thanks to its attention mechanism inspired by the memory networks\nused in language modeling, the proposed reasoning network is capable of\nsimultaneous event category recognition and localization of the key pieces of\nevidence for event recounting. In addition, the latent structure in our\nreasoning network highlights the areas of the video imprint, which can be\ndirectly used for event recounting. With the event retrieval task, the compact\nvideo representation aggregated from the video imprint contributes to better\nretrieval results than existing state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Zhanning Gao",
      "Le Wang",
      "Nebojsa Jojic",
      "Zhenxing Niu",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03283"
  },
  {
    "id": "arXiv:2106.03287",
    "title": "Stein ICP for Uncertainty Estimation in Point Cloud Matching",
    "abstract": "Quantification of uncertainty in point cloud matching is critical in many\ntasks such as pose estimation, sensor fusion, and grasping. Iterative closest\npoint (ICP) is a commonly used pose estimation algorithm which provides a point\nestimate of the transformation between two point clouds. There are many sources\nof uncertainty in this process that may arise due to sensor noise, ambiguous\nenvironment, and occlusion. However, for safety critical problems such as\nautonomous driving, a point estimate of the pose transformation is not\nsufficient as it does not provide information about the multiple solutions.\nCurrent probabilistic ICP methods usually do not capture all sources of\nuncertainty and may provide unreliable transformation estimates which can have\na detrimental effect in state estimation or decision making tasks that use this\ninformation. In this work we propose a new algorithm to align two point clouds\nthat can precisely estimate the uncertainty of ICP's transformation parameters.\nWe develop a Stein variational inference framework with gradient based\noptimization of ICP's cost function. The method provides a non-parametric\nestimate of the transformation, can model complex multi-modal distributions,\nand can be effectively parallelized on a GPU. Experiments using 3D kinect data\nas well as sparse indoor/outdoor LiDAR data show that our method is capable of\nefficiently producing accurate pose uncertainty estimates.",
    "descriptor": "\nComments: 10 pages, 8 figures, Letters\n",
    "authors": [
      "Fahira Afzal Maken",
      "Fabio Ramos",
      "Lionel Ott"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03287"
  },
  {
    "id": "arXiv:2106.03290",
    "title": "FACT: A Full-body Ad-hoc Collaboration Testbed for Modeling Complex  Teamwork",
    "abstract": "Robots are envisioned to work alongside humans in applications ranging from\nin-home assistance to collaborative manufacturing. Research on human-robot\ncollaboration (HRC) has helped develop various aspects of social intelligence\nnecessary for robots to participate in effective, fluid collaborations with\nhumans. However, HRC research has focused on dyadic, structured, and minimal\ncollaborations between humans and robots that may not fully represent the large\nscale and emergent nature of more complex, unstructured collaborative\nactivities. Thus, there remains a need for shared testbeds, datasets, and\nevaluation metrics that researchers can use to better model natural, ad-hoc\nhuman collaborative behaviors and develop robot capabilities intended for large\nscale emergent collaborations. We present one such shared resource - FACT\n(Full-body Ad-hoc Collaboration Testbed), an openly accessible testbed for\nresearchers to obtain an expansive view of the individual and team-based\nbehaviors involved in complex, co-located teamwork. We detail observations from\na preliminary exploration with teams of various sizes and discuss potential\nresearch questions that may be investigated using the testbed. Our goal is for\nFACT to be an initial resource that supports a more holistic investigation of\nhuman-robot collaboration.",
    "descriptor": "\nComments: 5 pages, 3 figures, 2021 ICRA Workshop on Social Intelligence in Humans and Robots\n",
    "authors": [
      "Gopika Ajaykumar",
      "Annie Mao",
      "Jeremy Brown",
      "Chien-Ming Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03290"
  },
  {
    "id": "arXiv:2106.03297",
    "title": "On the Language Coverage Bias for Neural Machine Translation",
    "abstract": "Language coverage bias, which indicates the content-dependent differences\nbetween sentence pairs originating from the source and target languages, is\nimportant for neural machine translation (NMT) because the target-original\ntraining data is not well exploited in current practice. By carefully designing\nexperiments, we provide comprehensive analyses of the language coverage bias in\nthe training data, and find that using only the source-original data achieves\ncomparable performance with using full training data. Based on these\nobservations, we further propose two simple and effective approaches to\nalleviate the language coverage bias problem through explicitly distinguishing\nbetween the source- and target-original training data, which consistently\nimprove the performance over strong baselines on six WMT20 translation tasks.\nComplementary to the translationese effect, language coverage bias provides\nanother explanation for the performance drop caused by back-translation. We\nalso apply our approach to both back- and forward-translation and find that\nmitigating the language coverage bias can improve the performance of both the\ntwo representative data augmentation methods and their tagged variants.",
    "descriptor": "\nComments: ACL 2021, Long Findings\n",
    "authors": [
      "Shuo Wang",
      "Zhaopeng Tu",
      "Zhixing Tan",
      "Shuming Shi",
      "Maosong Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03297"
  },
  {
    "id": "arXiv:2106.03299",
    "title": "Video Instance Segmentation using Inter-Frame Communication Transformers",
    "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS)\nbased on transformers. Recently, the per-clip pipeline shows superior\nperformance over per-frame methods leveraging richer information from multiple\nframes. However, previous per-clip models require heavy computation and memory\nusage to achieve frame-to-frame communications, limiting practicality. In this\nwork, we propose Inter-frame Communication Transformers (IFC), which\nsignificantly reduces the overhead for information-passing between frames by\nefficiently encoding the context within the input clip. Specifically, we\npropose to utilize concise memory tokens as a mean of conveying information as\nwell as summarizing each frame scene. The features of each frame are enriched\nand correlated with other frames through exchange of information between the\nprecisely encoded memory tokens. We validate our method on the latest benchmark\nsets and achieved the state-of-the-art performance (AP 44.6 on YouTube-VIS 2019\nval set using the offline inference) while having a considerably fast runtime\n(89.4 FPS). Our method can also be applied to near-online inference for\nprocessing a video in real-time with only a small delay. The code will be made\navailable.",
    "descriptor": "",
    "authors": [
      "Sukjun Hwang",
      "Miran Heo",
      "Seoung Wug Oh",
      "Seon Joo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03299"
  },
  {
    "id": "arXiv:2106.03300",
    "title": "Sum of Ranked Range Loss for Supervised Learning",
    "abstract": "In forming learning objectives, one oftentimes needs to aggregate a set of\nindividual values to a single output. Such cases occur in the aggregate loss,\nwhich combines individual losses of a learning model over each training sample,\nand in the individual loss for multi-label learning, which combines prediction\nscores over all class labels. In this work, we introduce the sum of ranked\nrange (SoRR) as a general approach to form learning objectives. A ranked range\nis a consecutive sequence of sorted values of a set of real numbers. The\nminimization of SoRR is solved with the difference of convex algorithm (DCA).\nWe explore two applications in machine learning of the minimization of the SoRR\nframework, namely the AoRR aggregate loss for binary/multi-class classification\nat the sample level and the TKML individual loss for multi-label/multi-class\nclassification at the label level. A combination loss of AoRR and TKML is\nproposed as a new learning objective for improving the robustness of\nmulti-label learning in the face of outliers in sample and labels alike. Our\nempirical results highlight the effectiveness of the proposed optimization\nframeworks and demonstrate the applicability of proposed losses using synthetic\nand real data sets.",
    "descriptor": "\nComments: 40 pages. arXiv admin note: substantial text overlap with arXiv:2010.01741\n",
    "authors": [
      "Shu Hu",
      "Yiming Ying",
      "Xin Wang",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03300"
  },
  {
    "id": "arXiv:2106.03302",
    "title": "Rack-Aware Regenerating Codes with Multiple Erasure Tolerance",
    "abstract": "In a modern distributed storage system, storage nodes are organized in racks,\nand the cross-rack communication dominates the system bandwidth. In this paper,\nwe focus on the rack-aware storage system. The initial setting was immediately\nrepairing every single node failure. However, multiple node failures are\nfrequent, and some systems may even wait for multiple nodes failures to occur\nbefore repairing them in order to keep costs down. For the purpose of still\nbeing able to repair them properly when multiple failures occur, we relax the\nrepair model of the rack-aware storage system. In the repair process, the\ncross-rack connections (i.e., the number of helper racks connected for repair\nwhich is called repair degree) and the intra-rack connections (i.e., the number\nof helper nodes in the rack contains the failed node) are all reduced. We focus\non minimizing the cross-rack bandwidth in the rack-aware storage system with\nmultiple erasure tolerances. First, the fundamental tradeoff between the repair\nbandwidth and the storage size for functional repair is established. Then, the\ntwo extreme points corresponding to the minimum storage and minimum cross-rack\nrepair bandwidth are obtained. Second, the explicitly construct corresponding\nto the two points are given. Both of them have minimum sub-packetization level\n(i.e., the number of symbols stored in each node) and small repair degree.\nBesides, the size of underlying finite field is approximately the block length\nof the code. Finally, for the convenience of practical use, we also establish a\ntransformation to convert our codes into systematic codes.",
    "descriptor": "\nComments: 12 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2101.08738\n",
    "authors": [
      "Liyang Zhou",
      "Zhifang Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03302"
  },
  {
    "id": "arXiv:2106.03304",
    "title": "Packaged Software Implementation Requirements Engineering by Small  Software Enterprises",
    "abstract": "Small to medium sized business enterprises (SMEs) generally thrive because\nthey have successfully done something unique within a niche market. For this\nreason, SMEs may seek to protect their competitive advantage by avoiding any\nstandardization encouraged by the use of packaged software (PS). Packaged\nsoftware implementation at SMEs therefore presents challenges relating to how\nbest to respond to misfits between the functionality offered by the packaged\nsoftware and each SME's business needs. An important question relates to which\nprocesses small software enterprises - or Small to Medium-Sized Software\nDevelopment Companies (SMSSDCs) - apply in order to identify and then deal with\nthese misfits. To explore the processes of packaged software (PS)\nimplementation, an ethnographic study was conducted to gain in-depth insights\ninto the roles played by analysts in two SMSSDCs. The purpose of the study was\nto understand PS implementation in terms of requirements engineering (or\n'PSIRE'). Data collected during the ethnographic study were analyzed using an\ninductive approach. Based on our analysis of the cases we constructed a\ntheoretical model explaining the requirements engineering process for PS\nimplementation, and named it the PSIRE Parallel Star Model. The Parallel Star\nModel shows that during PSIRE, more than one RE process can be carried out at\nthe same time. The Parallel Star Model has few constraints, because not only\ncan processes be carried out in parallel, but they do not always have to be\nfollowed in a particular order. This paper therefore offers a novel\ninvestigation and explanation of RE practices for packaged software\nimplementation, approaching the phenomenon from the viewpoint of the analysts,\nand offers the first extensive study of packaged software implementation RE\n(PSIRE) in SMSSDCs.",
    "descriptor": "\nComments: Conference proceeding, 9 pages, 2 figures, 1 tables\n",
    "authors": [
      "Issam Jebreen",
      "Robert Wellington",
      "Stephen G. MacDonell"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03304"
  },
  {
    "id": "arXiv:2106.03305",
    "title": "Faster Cut-Equivalent Trees in Simple Graphs",
    "abstract": "Let $G = (V, E)$ be an undirected connected simple graph on $n$ vertices. A\ncut-equivalent tree of $G$ is an edge-weighted tree on the same vertex set $V$,\nsuch that for any pair of vertices $s, t\\in V$, the minimum $(s, t)$-cut in the\ntree is also a minimum $(s, t)$-cut in $G$, and these two cuts have the same\ncut value. In a recent paper [Abboud, Krauthgamer and Trabelsi, 2021], the\nauthors propose the first subcubic time algorithm for constructing a\ncut-equivalent tree. More specifically, their algorithm has\n$\\widetilde{O}(n^{2.5})$ running time.\nIn this paper, we improve the running time to $\\widehat{O}(n^2)$ if\nalmost-linear time max-flow algorithms exist. Also, using the currently fastest\nmax-flow algorithm by [van den Brand \\etal, 2021], our algorithm runs in time\n$\\widetilde{O}(n^{17/8})$.",
    "descriptor": "",
    "authors": [
      "Tianyi Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03305"
  },
  {
    "id": "arXiv:2106.03306",
    "title": "HoroPCA: Hyperbolic Dimensionality Reduction via Horospherical  Projections",
    "abstract": "This paper studies Principal Component Analysis (PCA) for data lying in\nhyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of\nsubspaces spanned by these directions, (2) a method of projection onto\nsubspaces that preserves information in these directions, and (3) an objective\nto optimize, namely the variance explained by projections. We generalize each\nof these concepts to the hyperbolic space and propose HoroPCA, a method for\nhyperbolic dimensionality reduction. By focusing on the core problem of\nextracting principal directions, HoroPCA theoretically better preserves\ninformation in the original data such as distances, compared to previous\ngeneralizations of PCA. Empirically, we validate that HoroPCA outperforms\nexisting dimensionality reduction methods, significantly reducing error in\ndistance preservation. As a data whitening method, it improves downstream\nclassification by up to 3.9% compared to methods that don't use whitening.\nFinally, we show that HoroPCA can be used to visualize hyperbolic data in two\ndimensions.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Ines Chami",
      "Albert Gu",
      "Dat Nguyen",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03306"
  },
  {
    "id": "arXiv:2106.03307",
    "title": "Terrain Adaptive Gait Transitioning for a Quadruped Robot using Model  Predictive Control",
    "abstract": "Legged robots can traverse challenging terrain, use perception to plan their\nsafe foothold positions, and navigate the environment. Such unique mobility\ncapabilities make these platforms a perfect candidate for scenarios such as\nsearch and rescue, inspection, and exploration tasks. While traversing through\nsuch terrains, the robot's instability is a significant concern. Many times the\nrobot needs to switch gaits depending on its environment. Due to the complex\ndynamics of quadruped robots, classical PID control fails to provide high\nstability. Thus, there is a need for advanced control methods like the Model\nPredictive Control (MPC) which uses the system model and the nature of the\nterrain in order to predict the stable body pose of the robot. The controller\nalso provides correction to any external disturbances that result in a change\nin the desired behavior of the robot. The MPC controller is designed in MATLAB,\nfor full body torque control. The controller performance was verified on Boston\nDynamics Spot in Webots simulator. The robot is able to provide correction for\nexternal perturbations up to 150 N and also resist falls till 80 cm.",
    "descriptor": "\nComments: To be published in the proceedings of the 26th IEEE International Conference on Automation and Computing (ICAC'21)\n",
    "authors": [
      "Prathamesh Saraf",
      "Abhishek Sarkar",
      "Arshad Javed"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03307"
  },
  {
    "id": "arXiv:2106.03309",
    "title": "Adopting Softer Approaches in the Study of Repository Data: A  Comparative Analysis",
    "abstract": "Context: Given the acknowledged need to understand the people processes\nenacted during software development, software repositories and mailing lists\nhave become a focus for many studies. However, researchers have tended to use\nmostly mathematical and frequency-based techniques to examine the software\nartifacts contained within them. Objective: There is growing recognition that\nthese approaches uncover only a partial picture of what happens during software\nprojects, and deeper contextual approaches may provide further understanding of\nthe intricate nature of software teams' dynamics. We demonstrate the relevance\nand utility of such approaches in this study. Method: We use psycholinguistics\nand directed content analysis (CA) to study the way project tasks drive teams'\nattitudes and knowledge sharing. We compare the outcomes of these two\napproaches and offer methodological advice for researchers using similar forms\nof repository data. Results: Our analysis reveals significant differences in\nthe way teams work given their portfolio of tasks and the distribution of\nroles. Conclusion: We overcome the limitations associated with employing purely\nquantitative approaches, while avoiding the time-intensive and potentially\ninvasive nature of field work required in full case studies.",
    "descriptor": "\nComments: Conference paper, 8 pages, 7 tables. arXiv admin note: text overlap with arXiv:2102.06317\n",
    "authors": [
      "Sherlock A. Licorish",
      "Stephen G. MacDonell"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03309"
  },
  {
    "id": "arXiv:2106.03310",
    "title": "Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model",
    "abstract": "Knowledge distillation (KD) is a successful approach for deep neural network\nacceleration, with which a compact network (student) is trained by mimicking\nthe softmax output of a pre-trained high-capacity network (teacher). In\ntradition, KD usually relies on access to the training samples and the\nparameters of the white-box teacher to acquire the transferred knowledge.\nHowever, these prerequisites are not always realistic due to storage costs or\nprivacy issues in real-world applications. Here we propose the concept of\ndecision-based black-box (DB3) knowledge distillation, with which the student\nis trained by distilling the knowledge from a black-box teacher (parameters are\nnot accessible) that only returns classes rather than softmax outputs. We start\nwith the scenario when the training set is accessible. We represent a sample's\nrobustness against other classes by computing its distances to the teacher's\ndecision boundaries and use it to construct the soft label for each training\nsample. After that, the student can be trained via standard KD. We then extend\nthis approach to a more challenging scenario in which even accessing the\ntraining data is not feasible. We propose to generate pseudo samples\ndistinguished by the teacher's decision boundaries to the largest extent and\nconstruct soft labels for them, which are used as the transfer set. We evaluate\nour approaches on various benchmark networks and datasets and experiment\nresults demonstrate their effectiveness. Codes are available at:\nhttps://github.com/zwang84/zsdb3kd.",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Zi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03310"
  },
  {
    "id": "arXiv:2106.03314",
    "title": "Measuring Generalization with Optimal Transport",
    "abstract": "Understanding the generalization of deep neural networks is one of the most\nimportant tasks in deep learning. Although much progress has been made,\ntheoretical error bounds still often behave disparately from empirical\nobservations. In this work, we develop margin-based generalization bounds,\nwhere the margins are normalized with optimal transport costs between\nindependent random subsets sampled from the training distribution. In\nparticular, the optimal transport cost can be interpreted as a generalization\nof variance which captures the structural properties of the learned feature\nspace. Our bounds robustly predict the generalization error, given training\ndata and network parameters, on large scale datasets. Theoretically, we\ndemonstrate that the concentration and separation of features play crucial\nroles in generalization, supporting empirical results in the literature. The\ncode is available at \\url{https://github.com/chingyaoc/kV-Margin}.",
    "descriptor": "",
    "authors": [
      "Ching-Yao Chuang",
      "Youssef Mroueh",
      "Kristjan Greenewald",
      "Antonio Torralba",
      "Stefanie Jegelka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03314"
  },
  {
    "id": "arXiv:2106.03315",
    "title": "Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from\nsentences, where each triplet includes an entity, its associated sentiment, and\nthe opinion span explaining the reason for the sentiment. Most existing\nresearch addresses this problem in a multi-stage pipeline manner, which\nneglects the mutual information between such three elements and has the problem\nof error propagation. In this paper, we propose a Semantic and Syntactic\nEnhanced aspect Sentiment triplet Extraction model (S3E2) to fully exploit the\nsyntactic and semantic relationships between the triplet elements and jointly\nextract them. Specifically, we design a Graph-Sequence duel representation and\nmodeling paradigm for the task of ASTE: we represent the semantic and syntactic\nrelationships between word pairs in a sentence by graph and encode it by Graph\nNeural Networks (GNNs), as well as modeling the original sentence by LSTM to\npreserve the sequential information. Under this setting, we further apply a\nmore efficient inference strategy for the extraction of triplets. Extensive\nevaluations on four benchmark datasets show that S3E2 significantly outperforms\nexisting approaches, which proves our S3E2's superiority and flexibility in an\nend-to-end fashion.",
    "descriptor": "",
    "authors": [
      "Zhexue Chen",
      "Hong Huang",
      "Bang Liu",
      "Xuanhua Shi",
      "Hai Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03315"
  },
  {
    "id": "arXiv:2106.03316",
    "title": "Exploring to establish an appropriate model for mage aesthetic  assessment via CNN-based RSRL: An empirical study",
    "abstract": "To establish an appropriate model for photo aesthetic assessment, in this\npaper, a D-measure which reflects the disentanglement degree of the final layer\nFC nodes of CNN is introduced. By combining F-measure with D-measure to obtain\na FD measure, an algorithm of determining the optimal model from the multiple\nphoto score prediction models generated by CNN-based repetitively self-revised\nlearning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP)\nand the assessment interest region(AIR) of the models are defined and\ncalculated. The experimental results show that the FD measure is effective for\nestablishing the appropriate model from the multiple score prediction models\nwith different CNN structures. Moreover, the FD-determined optimal models with\nthe comparatively high FD always have the FFP an AIR which are close to the\nhuman's aesthetic perception when enjoying photos.",
    "descriptor": "",
    "authors": [
      "Ying Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03316"
  },
  {
    "id": "arXiv:2106.03317",
    "title": "Smooth Implicit Hybrid Upwinding for Compositional Multiphase Flow in  Porous Media",
    "abstract": "In subsurface multiphase flow simulations, poor nonlinear solver performance\nis a significant runtime sink. The system of fully implicit mass balance\nequations is highly nonlinear and often difficult to solve for the nonlinear\nsolver, generally Newton(-Raphson). Strong nonlinearities can cause Newton\niterations to converge very slowly. This frequently results in time step cuts,\nleading to computationally expensive simulations. Much literature has looked\ninto how to improve the nonlinear solver through enhancements or safeguarding\nupdates. In this work, we take a different approach; we aim to improve\nconvergence with a smoother finite volume discretization scheme which is more\nsuitable for the Newton solver.\nBuilding on recent work, we propose a novel total velocity hybrid upwinding\nscheme with weighted average flow mobilities (WA-HU TV) that is unconditionally\nmonotone and extends to compositional multiphase simulations. Analyzing the\nsolution space of a one-cell problem, we demonstrate the improved properties of\nthe scheme and explain how it leverages the advantages of both phase potential\nupwinding and arithmetic averaging. This results in a flow subproblem that is\nsmooth with respect to changes in the sign of phase fluxes, and is well-behaved\nwhen phase velocities are large or when co-current viscous forces dominate.\nAdditionally, we propose a WA-HU scheme with a total mass (WA-HU TM)\nformulation that includes phase densities in the weighted averaging.\nThe proposed WA-HU TV consistently outperforms existing schemes, yielding\nbenefits from 5\\% to over 50\\% reduction in nonlinear iterations. The WA-HU TM\nscheme also shows promising results; in some cases leading to even more\nefficiency. However, WA-HU TM can occasionally also lead to convergence issues.\nOverall, based on the current results, we recommend the adoption of the WA-HU\nTV scheme as it is highly efficient and robust.",
    "descriptor": "",
    "authors": [
      "Sebastian B.M. Bosma",
      "Francois P. Hamon",
      "Brad T. Mallison",
      "Hamdi A. Tchelepi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03317"
  },
  {
    "id": "arXiv:2106.03323",
    "title": "A Comprehensive Survey on Image Dehazing Based on Deep Learning",
    "abstract": "The presence of haze significantly reduces the quality of images. Researchers\nhave designed a variety of algorithms for image dehazing (ID) to restore the\nquality of hazy images. However, there are few studies that summarize the deep\nlearning (DL) based dehazing technologies. In this paper, we conduct a\ncomprehensive survey on the recent proposed dehazing methods. Firstly, we\nsummarize the commonly used datasets, loss functions and evaluation metrics.\nSecondly, we group the existing researches of ID into two major categories:\nsupervised ID and unsupervised ID. The core ideas of various influential\ndehazing models are introduced. Finally, the open issues for future research on\nID are pointed out.",
    "descriptor": "\nComments: Paper accepted at IJCAI 2021 (Survey Track)\n",
    "authors": [
      "Jie Gui",
      "Xiaofeng Cong",
      "Yuan Cao",
      "Wenqi Ren",
      "Jun Zhang",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03323"
  },
  {
    "id": "arXiv:2106.03324",
    "title": "Uncertain Process Data with Probabilistic Knowledge: Problem  Characterization and Challenges",
    "abstract": "Motivated by the abundance of uncertain event data from multiple sources\nincluding physical devices and sensors, this paper presents the task of\nrelating a stochastic process observation to a process model that can be\nrendered from a dataset. In contrast to previous research that suggested to\ntransform a stochastically known event log into a less informative uncertain\nlog with upper and lower bounds on activity frequencies, we consider the\nchallenge of accommodating the probabilistic knowledge into conformance\nchecking techniques. Based on a taxonomy that captures the spectrum of\nconformance checking cases under stochastic process observations, we present\nthree types of challenging cases. The first includes conformance checking of a\nstochastically known log with respect to a given process model. The second case\nextends the first to classify a stochastically known log into one of several\nprocess models. The third case extends the two previous ones into settings in\nwhich process models are only stochastically known. The suggested problem\ncaptures the increasingly growing number of applications in which sensors\nprovide probabilistic process information.",
    "descriptor": "",
    "authors": [
      "Izack Cohen",
      "Avigdor Gal"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03324"
  },
  {
    "id": "arXiv:2106.03328",
    "title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in  Federated Learning",
    "abstract": "Secure aggregation is a critical component in federated learning, which\nenables the server to learn the aggregate model of the users without observing\ntheir local models. Conventionally, secure aggregation algorithms focus only on\nensuring the privacy of individual users in a single training round. We contend\nthat such designs can lead to significant privacy leakages over multiple\ntraining rounds, due to partial user selection/participation at each round of\nfederated learning. In fact, we empirically show that the conventional random\nuser selection strategies for federated learning lead to leaking users'\nindividual models within number of rounds linear in the number of users. To\naddress this challenge, we introduce a secure aggregation framework with\nmulti-round privacy guarantees. In particular, we introduce a new metric to\nquantify the privacy guarantees of federated learning over multiple training\nrounds, and develop a structured user selection strategy that guarantees the\nlong-term privacy of each user (over any number of training rounds). Our\nframework also carefully accounts for the fairness and the average number of\nparticipating users at each round. We perform several experiments on MNIST and\nCIFAR-10 datasets in the IID and the non-IID settings to demonstrate the\nperformance improvement over the baseline algorithms, both in terms of privacy\nprotection and test accuracy.",
    "descriptor": "",
    "authors": [
      "Jinhyun So",
      "Ramy E. Ali",
      "Basak Guler",
      "Jiantao Jiao",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03328"
  },
  {
    "id": "arXiv:2106.03329",
    "title": "Improved Method for Dealing with Discontinuities in Power System  Transient Simulation Based on Frequency Response Optimized Integrators  Considering Second Order Derivative",
    "abstract": "Potential disagreement in the result induced by discontinuities is revealed\nin this paper between a novel power system transient simulation scheme using\nnumerical integrators considering second order derivative and conventional ones\nusing numerical integrators considering first order derivative. The\ndisagreement is due to the formula of the different numerical integrators. An\nimproved method for dealing with discontinuities in the novel transient\nsimulation scheme is proposed to resolve the disagreement. The effectiveness of\nthe improved method is demonstrated and verified via numerical case studies.\nAlthough the disagreement is studied on and the improved method is proposed for\na particular transient simulation scheme, similar conclusions also apply to\nother ones using numerical integrators considering high order derivative.",
    "descriptor": "\nComments: Accepted by the 2021 IEEE Midwest Symposium on Circuits and Systems\n",
    "authors": [
      "Sheng Lei",
      "Alexander Flueck"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03329"
  },
  {
    "id": "arXiv:2106.03330",
    "title": "Contextual Guided Segmentation Framework for Semi-supervised Video  Instance Segmentation",
    "abstract": "In this paper, we propose Contextual Guided Segmentation (CGS) framework for\nvideo instance segmentation in three passes. In the first pass, i.e., preview\nsegmentation, we propose Instance Re-Identification Flow to estimate main\nproperties of each instance (i.e., human/non-human, rigid/deformable,\nknown/unknown category) by propagating its preview mask to other frames. In the\nsecond pass, i.e., contextual segmentation, we introduce multiple contextual\nsegmentation schemes. For human instance, we develop skeleton-guided\nsegmentation in a frame along with object flow to correct and refine the result\nacross frames. For non-human instance, if the instance has a wide variation in\nappearance and belongs to known categories (which can be inferred from the\ninitial mask), we adopt instance segmentation. If the non-human instance is\nnearly rigid, we train FCNs on synthesized images from the first frame of a\nvideo sequence. In the final pass, i.e., guided segmentation, we develop a\nnovel fined-grained segmentation method on non-rectangular regions of interest\n(ROIs). The natural-shaped ROI is generated by applying guided attention from\nthe neighbor frames of the current one to reduce the ambiguity in the\nsegmentation of different overlapping instances. Forward mask propagation is\nfollowed by backward mask propagation to further restore missing instance\nfragments due to re-appeared instances, fast motion, occlusion, or heavy\ndeformation. Finally, instances in each frame are merged based on their depth\nvalues, together with human and non-human object interaction and rare instance\npriority. Experiments conducted on the DAVIS Test-Challenge dataset demonstrate\nthe effectiveness of our proposed framework. We achieved the 3rd consistently\nin the DAVIS Challenges 2017-2019 with 75.4%, 72.4%, and 78.4% in terms of\nglobal score, region similarity, and contour accuracy, respectively.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Trung-Nghia Le",
      "Tam V. Nguyen",
      "Minh-Triet Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03330"
  },
  {
    "id": "arXiv:2106.03331",
    "title": "SelfDoc: Self-Supervised Document Representation Learning",
    "abstract": "We propose SelfDoc, a task-agnostic pre-training framework for document image\nunderstanding. Because documents are multimodal and are intended for sequential\nreading, our framework exploits the positional, textual, and visual information\nof every semantically meaningful component in a document, and it models the\ncontextualization between each block of content. Unlike existing document\npre-training models, our model is coarse-grained instead of treating individual\nwords as input, therefore avoiding an overly fine-grained with excessive\ncontextualization. Beyond that, we introduce cross-modal learning in the model\npre-training phase to fully leverage multimodal information from unlabeled\ndocuments. For downstream usage, we propose a novel modality-adaptive attention\nmechanism for multimodal feature fusion by adaptively emphasizing language and\nvision signals. Our framework benefits from self-supervised pre-training on\ndocuments without requiring annotations by a feature masking training strategy.\nIt achieves superior performance on multiple downstream tasks with\nsignificantly fewer document images used in the pre-training stage compared to\nprevious works.",
    "descriptor": "\nComments: To appear in CVPR'2021\n",
    "authors": [
      "Peizhao Li",
      "Jiuxiang Gu",
      "Jason Kuen",
      "Vlad I. Morariu",
      "Handong Zhao",
      "Rajiv Jain",
      "Varun Manjunatha",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03331"
  },
  {
    "id": "arXiv:2106.03336",
    "title": "Wide-Baseline Relative Camera Pose Estimation with Directional Learning",
    "abstract": "Modern deep learning techniques that regress the relative camera pose between\ntwo images have difficulty dealing with challenging scenarios, such as large\ncamera motions resulting in occlusions and significant changes in perspective\nthat leave little overlap between images. These models continue to struggle\neven with the benefit of large supervised training datasets. To address the\nlimitations of these models, we take inspiration from techniques that show\nregressing keypoint locations in 2D and 3D can be improved by estimating a\ndiscrete distribution over keypoint locations. Analogously, in this paper we\nexplore improving camera pose regression by instead predicting a discrete\ndistribution over camera poses. To realize this idea, we introduce\nDirectionNet, which estimates discrete distributions over the 5D relative pose\nspace using a novel parameterization to make the estimation problem tractable.\nSpecifically, DirectionNet factorizes relative camera pose, specified by a 3D\nrotation and a translation direction, into a set of 3D direction vectors. Since\n3D directions can be identified with points on the sphere, DirectionNet\nestimates discrete distributions on the sphere as its output. We evaluate our\nmodel on challenging synthetic and real pose estimation datasets constructed\nfrom Matterport3D and InteriorNet. Promising results show a near 50% reduction\nin error over direct regression methods.",
    "descriptor": "",
    "authors": [
      "Kefan Chen",
      "Noah Snavely",
      "Ameesh Makadia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03336"
  },
  {
    "id": "arXiv:2106.03337",
    "title": "Summary Grounded Conversation Generation",
    "abstract": "Many conversation datasets have been constructed in the recent years using\ncrowdsourcing. However, the data collection process can be time consuming and\npresents many challenges to ensure data quality. Since language generation has\nimproved immensely in recent years with the advancement of pre-trained language\nmodels, we investigate how such models can be utilized to generate entire\nconversations, given only a summary of a conversation as the input. We explore\nthree approaches to generate summary grounded conversations, and evaluate the\ngenerated conversations using automatic measures and human judgements. We also\nshow that the accuracy of conversation summarization can be improved by\naugmenting a conversation summarization dataset with generated conversations.",
    "descriptor": "\nComments: Findings of ACL - 2021, 9 pages\n",
    "authors": [
      "Chulaka Gunasekara",
      "Guy Feigenblat",
      "Benjamin Sznajder",
      "Sachindra Joshi",
      "David Konopnicki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03337"
  },
  {
    "id": "arXiv:2106.03339",
    "title": "General theory of interpolation error estimates on anisotropic meshes,  part II",
    "abstract": "We present a general theory of interpolation error estimates for smooth\nfunctions and inverse inequalities on anisotropic meshes. In our theory, the\nerror of interpolation is bound in terms of the diameter of a simplex and a\ngeometric parameter. In the two-dimensional case, our geometric parameter is\nequivalent to the circumradius of a triangle. In the three-dimensional case,\nour geometric parameter also represents the flatness of a tetrahedron. This\npaper also includes corrections to an error in \"General theory of interpolation\nerror estimates on anisotropic meshes\" (Japan Journal of Industrial and Applied\nMathematics, 38 (2021) 163-191), in which Theorem 2 was incorrect.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Hiroki Ishizaka",
      "Kenta Kobayashi",
      "Takuya Tsuchiya"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03339"
  },
  {
    "id": "arXiv:2106.03340",
    "title": "Instrument Space Selection for Kernel Maximum Moment Restriction",
    "abstract": "Kernel maximum moment restriction (KMMR) recently emerges as a popular\nframework for instrumental variable (IV) based conditional moment restriction\n(CMR) models with important applications in conditional moment (CM) testing and\nparameter estimation for IV regression and proximal causal learning. The\neffectiveness of this framework, however, depends critically on the choice of a\nreproducing kernel Hilbert space (RKHS) chosen as a space of instruments. In\nthis work, we presents a systematic way to select the instrument space for\nparameter estimation based on a principle of the least identifiable instrument\nspace (LIIS) that identifies model parameters with the least space complexity.\nOur selection criterion combines two distinct objectives to determine such an\noptimal space: (i) a test criterion to check identifiability; (ii) an\ninformation criterion based on the effective dimension of RKHSs as a complexity\nmeasure. We analyze the consistency of our method in determining the LIIS, and\ndemonstrate its effectiveness for parameter estimation via simulations.",
    "descriptor": "",
    "authors": [
      "Rui Zhang",
      "Krikamol Muandet",
      "Bernhard Sch\u00f6lkopf",
      "Masaaki Imaizumi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03340"
  },
  {
    "id": "arXiv:2106.03343",
    "title": "Energy Aligning for Biased Models",
    "abstract": "Training on class-imbalanced data usually results in biased models that tend\nto predict samples into the majority classes, which is a common and notorious\nproblem. From the perspective of energy-based model, we demonstrate that the\nfree energies of categories are aligned with the label distribution\ntheoretically, thus the energies of different classes are expected to be close\nto each other when aiming for ``balanced'' performance. However, we discover a\nsevere energy-bias phenomenon in the models trained on class-imbalanced\ndataset. To eliminate the bias, we propose a simple and effective method named\nEnergy Aligning by merely adding the calculated shift scalars onto the output\nlogits during inference, which does not require to (i) modify the network\narchitectures, (ii) intervene the standard learning paradigm, (iii) perform\ntwo-stage training. The proposed algorithm is evaluated on two class\nimbalance-related tasks under various settings: class incremental learning and\nlong-tailed recognition. Experimental results show that energy aligning can\neffectively alleviate class imbalance issue and outperform state-of-the-art\nmethods on several benchmarks.",
    "descriptor": "",
    "authors": [
      "Bowen Zhao",
      "Chen Chen",
      "Qi Ju",
      "ShuTao Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03343"
  },
  {
    "id": "arXiv:2106.03345",
    "title": "A Joint Model for Dropped Pronoun Recovery and Conversational Discourse  Parsing in Chinese Conversational Speech",
    "abstract": "In this paper, we present a neural model for joint dropped pronoun recovery\n(DPR) and conversational discourse parsing (CDP) in Chinese conversational\nspeech. We show that DPR and CDP are closely related, and a joint model\nbenefits both tasks. We refer to our model as DiscProReco, and it first encodes\nthe tokens in each utterance in a conversation with a directed Graph\nConvolutional Network (GCN). The token states for an utterance are then\naggregated to produce a single state for each utterance. The utterance states\nare then fed into a biaffine classifier to construct a conversational discourse\ngraph. A second (multi-relational) GCN is then applied to the utterance states\nto produce a discourse relation-augmented representation for the utterances,\nwhich are then fused together with token states in each utterance as input to a\ndropped pronoun recovery layer. The joint model is trained and evaluated on a\nnew Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) dataset that we\nannotated with both two types of information. Experimental results on the SPDPR\ndataset and other benchmarks show that DiscProReco significantly outperforms\nthe state-of-the-art baselines of both tasks.",
    "descriptor": "\nComments: Accepted by ACL2021\n",
    "authors": [
      "Jingxuan Yang",
      "Kerui Xu",
      "Jun Xu",
      "Si Li",
      "Sheng Gao",
      "Jun Guo",
      "Nianwen Xue",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03345"
  },
  {
    "id": "arXiv:2106.03346",
    "title": "QFuzz: Quantitative Fuzzing for Side Channels",
    "abstract": "Side channels pose a significant threat to the confidentiality of software\nsystems. Such vulnerabilities are challenging to detect and evaluate because\nthey arise from non-functional properties of software such as execution times\nand require reasoning on multiple execution traces. Recently, noninterference\nnotions have been adapted in static analysis, symbolic execution, and greybox\nfuzzing techniques. However, noninterference is a strict notion and may reject\nsecurity even if the strength of information leaks are weak. A quantitative\nnotion of security allows for the relaxation of noninterference and tolerates\nsmall (unavoidable) leaks. Despite progress in recent years, the existing\nquantitative approaches have scalability limitations in practice. In this work,\nwe present QFuzz, a greybox fuzzing technique to quantitatively evaluate the\nstrength of side channels with a focus on min entropy. Min entropy is a measure\nbased on the number of distinguishable observations (partitions) to assess the\nresulting threat from an attacker who tries to compromise secrets in one try.\nWe develop a novel greybox fuzzing equipped with two partitioning algorithms\nthat try to maximize the number of distinguishable observations and the cost\ndifferences between them. We evaluate QFuzz on a large set of benchmarks from\nexisting work and real-world libraries (with a total of 70 subjects). QFuzz\ncompares favorably to three state-of-the-art detection techniques. QFuzz\nprovides quantitative information about leaks beyond the capabilities of all\nthree techniques. Crucially, we compare QFuzz to a state-of-the-art\nquantification tool and find that QFuzz significantly outperforms the tool in\nscalability while maintaining similar precision. Overall, we find that our\napproach scales well for real-world applications and provides useful\ninformation to evaluate resulting threats. Additionally, QFuzz identifies a\nzero-d...",
    "descriptor": "",
    "authors": [
      "Yannic Noller",
      "Saeid Tizpaz-Niari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03346"
  },
  {
    "id": "arXiv:2106.03348",
    "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
    "abstract": "Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, \\ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Yufei Xu",
      "Qiming Zhang",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03348"
  },
  {
    "id": "arXiv:2106.03349",
    "title": "A Stochastic Model for Block Segmentation of Images Based on the  Quadtree and the Bayes Code for It",
    "abstract": "In information theory, lossless compression of general data is based on an\nexplicit assumption of a stochastic generative model on target data. However,\nin lossless image compression, the researchers have mainly focused on the\ncoding procedure that outputs the coded sequence from the input image, and the\nassumption of the stochastic generative model is implicit. In these studies,\nthere is a difficulty in confirming the information-theoretical optimality of\nthe coding procedure to the stochastic generative model. Hence, in this paper,\nwe propose a novel stochastic generative model of images by redefining the\nimplicit stochastic generative model in a previous coding procedure. That is\nbased on the quadtree so that our model effectively represents the variable\nblock size segmentation of images. Then, we construct the Bayes code optimal\nfor the proposed stochastic generative model. In general, the computational\ncost to calculate the posterior distribution required in the Bayes code\nincreases exponentially for the image size. However, we introduce an efficient\nalgorithm to calculate it in the polynomial order of the image size without\nloss of the optimality. Some experiments are performed to confirm the\nflexibility of the proposed stochastic model and the efficiency of the\nintroduced algorithm.",
    "descriptor": "",
    "authors": [
      "Yuta Nakahara",
      "Toshiyasu Matsushima"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03349"
  },
  {
    "id": "arXiv:2106.03351",
    "title": "Continual Active Learning for Efficient Adaptation of Machine Learning  Models to Changing Image Acquisition",
    "abstract": "Imaging in clinical routine is subject to changing scanner protocols,\nhardware, or policies in a typically heterogeneous set of acquisition hardware.\nAccuracy and reliability of deep learning models suffer from those changes as\ndata and targets become inconsistent with their initial static training set.\nContinual learning can adapt to a continuous data stream of a changing imaging\nenvironment. Here, we propose a method for continual active learning on a data\nstream of medical images. It recognizes shifts or additions of new imaging\nsources - domains -, adapts training accordingly, and selects optimal examples\nfor labelling. Model training has to cope with a limited labelling budget,\nresembling typical real world scenarios. We demonstrate our method on\nT1-weighted magnetic resonance images from three different scanners with the\ntask of brain age estimation. Results demonstrate that the proposed method\noutperforms naive active learning while requiring less manual labelling.",
    "descriptor": "\nComments: Accepted for publication at the 27th international conference on Information Processing in Medical Imaging (IPMI) 2021\n",
    "authors": [
      "Matthias Perkonigg",
      "Johannes Hofmanninger",
      "Georg Langs"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03351"
  },
  {
    "id": "arXiv:2106.03352",
    "title": "The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces",
    "abstract": "Modern reinforcement learning (RL) commonly engages practical problems with\nlarge state spaces, where function approximation must be deployed to\napproximate either the value function or the policy. While recent progresses in\nRL theory address a rich set of RL problems with general function\napproximation, such successes are mostly restricted to the single-agent\nsetting. It remains elusive how to extend these results to multi-agent RL,\nespecially due to the new challenges arising from its game-theoretical nature.\nThis paper considers two-player zero-sum Markov Games (MGs). We propose a new\nalgorithm that can provably find the Nash equilibrium policy using a polynomial\nnumber of samples, for any MG with low multi-agent Bellman-Eluder dimension --\na new complexity measure adapted from its single-agent version (Jin et al.,\n2021). A key component of our new algorithm is the exploiter, which facilitates\nthe learning of the main player by deliberately exploiting her weakness. Our\ntheoretical framework is generic, which applies to a wide range of models\nincluding but not limited to tabular MGs, MGs with linear or kernel function\napproximation, and MGs with rich observations.",
    "descriptor": "",
    "authors": [
      "Chi Jin",
      "Qinghua Liu",
      "Tiancheng Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03352"
  },
  {
    "id": "arXiv:2106.03353",
    "title": "Understanding Neural Code Intelligence Through Program Simplification",
    "abstract": "A wide range of code intelligence (CI) tools, powered by deep neural\nnetworks, have been developed recently to improve programming productivity and\nperform program analysis. To reliably use such tools, developers often need to\nreason about the behavior of the underlying models and the factors that affect\nthem. This is especially challenging for tools backed by deep neural networks.\nVarious methods have tried to reduce this opacity in the vein of\n\"transparent/interpretable-AI\". However, these approaches are often specific to\na particular set of network architectures, even requiring access to the\nnetwork's parameters. This makes them difficult to use for the average\nprogrammer, which hinders the reliable adoption of neural CI systems. In this\npaper, we propose a simple, model-agnostic approach to identify critical input\nfeatures for models in CI systems, by drawing on software debugging research,\nspecifically delta debugging. Our approach, SIVAND, uses simplification\ntechniques that reduce the size of input programs of a CI model while\npreserving the predictions of the model. We show that this approach yields\nremarkably small outputs and is broadly applicable across many model\narchitectures and problem domains. We find that the models in our experiments\noften rely heavily on just a few syntactic features in input programs. We\nbelieve that SIVAND's extracted features may help understand neural CI systems'\npredictions and learned behavior.",
    "descriptor": "\nComments: The 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE'21)\n",
    "authors": [
      "Md Rafiqul Islam Rabin",
      "Vincent J. Hellendoorn",
      "Mohammad Amin Alipour"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.03353"
  },
  {
    "id": "arXiv:2106.03354",
    "title": "Parameter-free Statistically Consistent Interpolation:  Dimension-independent Convergence Rates for Hilbert kernel regression",
    "abstract": "Previously, statistical textbook wisdom has held that interpolating noisy\ndata will generalize poorly, but recent work has shown that data interpolation\nschemes can generalize well. This could explain why overparameterized deep nets\ndo not necessarily overfit. Optimal data interpolation schemes have been\nexhibited that achieve theoretical lower bounds for excess risk in any\ndimension for large data (Statistically Consistent Interpolation). These are\nnon-parametric Nadaraya-Watson estimators with singular kernels. The recently\nproposed weighted interpolating nearest neighbors method (wiNN) is in this\nclass, as is the previously studied Hilbert kernel interpolation scheme, in\nwhich the estimator has the form $\\hat{f}(x)=\\sum_i y_i w_i(x)$, where $w_i(x)=\n\\|x-x_i\\|^{-d}/\\sum_j \\|x-x_j\\|^{-d}$. This estimator is unique in being\ncompletely parameter-free. While statistical consistency was previously proven,\nconvergence rates were not established. Here, we comprehensively study the\nfinite sample properties of Hilbert kernel regression. We prove that the excess\nrisk is asymptotically equivalent pointwise to $\\sigma^2(x)/\\ln(n)$ where\n$\\sigma^2(x)$ is the noise variance. We show that the excess risk of the plugin\nclassifier is less than $2|f(x)-1/2|^{1-\\alpha}\\,(1+\\varepsilon)^\\alpha\n\\sigma^\\alpha(x)(\\ln(n))^{-\\frac{\\alpha}{2}}$, for any $0<\\alpha<1$, where $f$\nis the regression function $x\\mapsto\\mathbb{E}[y|x]$. We derive asymptotic\nequivalents of the moments of the weight functions $w_i(x)$ for large $n$, for\ninstance for $\\beta>1$, $\\mathbb{E}[w_i^{\\beta}(x)]\\sim_{n\\rightarrow\n\\infty}((\\beta-1)n\\ln(n))^{-1}$. We derive an asymptotic equivalent for the\nLagrange function and exhibit the nontrivial extrapolation properties of this\nestimator. We present heuristic arguments for a universal $w^{-2}$ power-law\nbehavior of the probability density of the weights in the large $n$ limit.",
    "descriptor": "\nComments: 30 Pages, 3 Figures\n",
    "authors": [
      "Partha P Mitra",
      "Cl\u00e9ment Sire"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Functional Analysis (math.FA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03354"
  },
  {
    "id": "arXiv:2106.03355",
    "title": "Sub-trajectory Similarity Join with Obfuscation",
    "abstract": "User trajectory data is becoming increasingly accessible due to the\nprevalence of GPS-equipped devices such as smartphones. Many existing studies\nfocus on querying trajectories that are similar to each other in their\nentirety. We observe that trajectories partially similar to each other contain\nuseful information about users' travel patterns which should not be ignored.\nSuch partially similar trajectories are critical in applications such as\nepidemic contact tracing. We thus propose to query trajectories that are within\na given distance range from each other for a given period of time. We formulate\nthis problem as a sub-trajectory similarity join query named as the STS-Join.\nWe further propose a distributed index structure and a query algorithm for\nSTS-Join, where users retain their raw location data and only send obfuscated\ntrajectories to a server for query processing. This helps preserve user\nlocation privacy which is vital when dealing with such data. Theoretical\nanalysis and experiments on real data confirm the effectiveness and the\nefficiency of our proposed index structure and query algorithm.",
    "descriptor": "",
    "authors": [
      "Yanchuan Chang",
      "Jianzhong Qi",
      "Egemen Tanin",
      "Xingjun Ma",
      "Hanan Samet"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.03355"
  },
  {
    "id": "arXiv:2106.03356",
    "title": "DMBGN: Deep Multi-Behavior Graph Networks for Voucher Redemption Rate  Prediction",
    "abstract": "In E-commerce, vouchers are important marketing tools to enhance users'\nengagement and boost sales and revenue. The likelihood that a user redeems a\nvoucher is a key factor in voucher distribution decision. User-item\nClick-Through-Rate (CTR) models are often applied to predict the user-voucher\nredemption rate. However, the voucher scenario involves more complicated\nrelations among users, items and vouchers. The users' historical behavior in a\nvoucher collection activity reflects users' voucher usage patterns, which is\nnevertheless overlooked by the CTR-based solutions. In this paper, we propose a\nDeep Multi-behavior Graph Networks (DMBGN) to shed light on this field for the\nvoucher redemption rate prediction. The complex structural user-voucher-item\nrelationships are captured by a User-Behavior Voucher Graph (UVG). User\nbehavior happening both before and after voucher collection is taken into\nconsideration, and a high-level representation is extracted by Higher-order\nGraph Neural Networks. On top of a sequence of UVGs, an attention network is\nbuilt which can help to learn users' long-term voucher redemption preference.\nExtensive experiments on three large-scale production datasets demonstrate the\nproposed DMBGN model is effective, with 10% to 16% relative AUC improvement\nover Deep Neural Networks (DNN), and 2% to 4% AUC improvement over Deep\nInterest Network (DIN). Source code and a sample dataset are made publicly\navailable to facilitate future research.",
    "descriptor": "\nComments: 9 pages, 5 figures, accepted full paper SIGKDD'21 applied data science track\n",
    "authors": [
      "Fengtong Xiao",
      "Lin Li",
      "Weinan Xu",
      "Jingyu Zhao",
      "Xiaofeng Yang",
      "Jun Lang",
      "Hao Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03356"
  },
  {
    "id": "arXiv:2106.03360",
    "title": "The Computational and Latency Advantage of Quantum Communication  Networks",
    "abstract": "This article summarises the current status of classical communication\nnetworks and identifies some critical open research challenges that can only be\nsolved by leveraging quantum technologies. By now, the main goal of quantum\ncommunication networks has been security. However, quantum networks can do more\nthan just exchange secure keys or serve the needs of quantum computers. In\nfact, the scientific community is still investigating on the possible use\ncases/benefits that quantum communication networks can bring. Thus, this\narticle aims at pointing out and clearly describing how quantum communication\nnetworks can enhance in-network distributed computing and reduce the overall\nend-to-end latency, beyond the intrinsic limits of classical technologies.\nFurthermore, we also explain how entanglement can reduce the communication\ncomplexity (overhead) that future classical virtualised networks will\nexperience.",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Roberto Ferrara",
      "Riccardo Bassoli",
      "Christian Deppe",
      "Frank H.P. Fitzek",
      "Holger Boche"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03360"
  },
  {
    "id": "arXiv:2106.03364",
    "title": "Effect of Adaptive and Fixed Shared Steering Control on Distracted  Driver Behavior",
    "abstract": "Driver distraction is a well-known cause for traffic collisions worldwide.\nStudies have indicated that shared steering control, which actively provides\nhaptic guidance torque on the steering wheel, effectively improves the\nperformance of distracted drivers. Recently, adaptive shared steering control\nbased on the physiological status of the driver has been developed, although\nits effect on distracted driver behavior remains unclear. To this end, a\nhigh-fidelity driving simulator experiment was conducted involving 18\nparticipants performing double lane changes. The experimental conditions\ncomprised two driver states: attentive and distracted. Under each condition,\nevaluations were performed on three types of haptic guidance: none (manual),\nfixed authority, and adaptive authority based on feedback from the forearm\nsurface electromyography of the driver. Evaluation results indicated that, for\nboth attentive and distracted drivers, haptic guidance with adaptive authority\nyielded lower driver workload and reduced lane departure risk than manual\ndriving and fixed authority. Moreover, there was a tendency for distracted\ndrivers to reduce grip strength on the steering wheel to follow the haptic\nguidance with fixed authority, resulting in a relatively shorter double lane\nchange duration.",
    "descriptor": "",
    "authors": [
      "Zheng Wang",
      "Satoshi Suga",
      "Edric John Cruz Nacpil",
      "Bo Yang",
      "Kimihiko Nakano"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03364"
  },
  {
    "id": "arXiv:2106.03366",
    "title": "Spectral Independence via Stability and Applications to Holant-Type  Problems",
    "abstract": "This paper formalizes connections between stability of polynomials and\nconvergence rates of Markov Chain Monte Carlo (MCMC) algorithms. We prove that\nif a (multivariate) partition function is nonzero in a region around a real\npoint $\\lambda$ then spectral independence holds at $\\lambda$. As a\nconsequence, for Holant-type problems (e.g., spin systems) on bounded-degree\ngraphs, we obtain optimal $O(n\\log n)$ mixing time bounds for the single-site\nupdate Markov chain known as the Glauber dynamics. Our result significantly\nimproves the running time guarantees obtained via the polynomial interpolation\nmethod of Barvinok (2017), refined by Patel and Regts (2017).\nThere are a variety of applications of our results. In this paper, we focus\non Holant-type (i.e., edge-coloring) problems, including weighted edge covers\nand weighted even subgraphs. For the weighted edge cover problem (and several\nnatural generalizations) we obtain an $O(n\\log{n})$ sampling algorithm on\nbounded-degree graphs. The even subgraphs problem corresponds to the\nhigh-temperature expansion of the ferromagnetic Ising model. We obtain an\n$O(n\\log{n})$ sampling algorithm for the ferromagnetic Ising model with a\nnonzero external field on bounded-degree graphs, which improves upon the\nclassical result of Jerrum and Sinclair (1993) for this class of graphs. We\nobtain further applications to antiferromagnetic two-spin models on line\ngraphs, weighted graph homomorphisms, tensor networks, and more.",
    "descriptor": "",
    "authors": [
      "Zongchen Chen",
      "Kuikui Liu",
      "Eric Vigoda"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Mathematical Physics (math-ph)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.03366"
  },
  {
    "id": "arXiv:2106.03368",
    "title": "Verification of Component Fault Trees Using Error Effect Simulations",
    "abstract": "The growing complexity of safety-relevant systems causes an increasing effort\nfor safety assurance. The reduction of development costs and time-to-market,\nwhile guaranteeing safe operation, is therefore a major challenge. In order to\nenable efficient safety assessment of complex architectures, we present an\napproach, which combines deductive safety analyses, in form of Component Fault\nTrees (CFTs), with an Error Effect Simulation (EES) for sanity checks. The\ncombination reduces the drawbacks of both analyses, such as the subjective\nfailure propagation assumptions in the CFTs or the determination of relevant\nfault scenarios for the EES. Both CFTs and the EES provide a modular, reusable\nand compositional safety analysis and are applicable throughout the whole\ndesign process. They support continuous model refinement and the reuse of\nconducted safety analysis and simulation models. Hence, safety goal violations\ncan be identified in early design stages and the reuse of conducted safety\nanalyses reduces the overhead for safety assessment.",
    "descriptor": "",
    "authors": [
      "Sebastian Reiter",
      "Marc Zeller",
      "Kai Hoefig",
      "Alexander Viehl",
      "Oliver Bringmann",
      "Wolfgang Rosenstiel"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03368"
  },
  {
    "id": "arXiv:2106.03371",
    "title": "User Behavior Assessment Towards Biometric Facial Recognition System: A  SEM-Neural Network Approach",
    "abstract": "A smart home is grounded on the sensors that endure automation, safety, and\nstructural integration. The security mechanism in digital setup possesses\nvibrant prominence and the biometric facial recognition system is novel\naddition to accrue the smart home features. Understanding the implementation of\nsuch technology is the outcome of user behavior modeling. However, there is the\npaucity of empirical research that explains the role of cognitive, functional,\nand social aspects of end-users acceptance behavior towards biometric facial\nrecognition systems at homes. Therefore, a causal research survey was conducted\nto comprehend the behavioral intention towards the use of a biometric facial\nrecognition system. Technology Acceptance Model (TAM)was implied with Perceived\nSystem Quality (PSQ) and Social Influence (SI)to hypothesize the conceptual\nframework. Data was collected from 475respondents through online\nquestionnaires. Structural Equation Modeling(SEM) and Artificial Neural Network\n(ANN) were employed to analyze the surveyed data. The results showed that all\nthe variables of the proposed framework significantly affected the behavioral\nintention to use the system. The PSQ appeared as the noteworthy predictor\ntowards biometric facial recognition system usability through regression and\nsensitivity analyses. A multi-analytical approach towards understanding the\ntechnology user behavior will support the efficient decision-making process in\nHuman-centric computing.",
    "descriptor": "\nComments: 15 Pages, 04 Figures, 05 Tables, FICC-2021 Conference 29-30 April 2021, Vancouver, Canada\n",
    "authors": [
      "Sheikh Muhamad Hizam",
      "Waqas Ahmed",
      "Muhammad Fahad",
      "Habiba Akter",
      "Ilham Sentosa",
      "Jawad Ali"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.03371"
  },
  {
    "id": "arXiv:2106.03373",
    "title": "Pre-trained Language Model for Web-scale Retrieval in Baidu Search",
    "abstract": "Retrieval is a crucial stage in web search that identifies a small set of\nquery-relevant candidates from a billion-scale corpus. Discovering more\nsemantically-related candidates in the retrieval stage is very promising to\nexpose more high-quality results to the end users. However, it still remains\nnon-trivial challenges of building and deploying effective retrieval models for\nsemantic matching in real search engine. In this paper, we describe the\nretrieval system that we developed and deployed in Baidu Search. The system\nexploits the recent state-of-the-art Chinese pretrained language model, namely\nEnhanced Representation through kNowledge IntEgration (ERNIE), which\nfacilitates the system with expressive semantic matching. In particular, we\ndeveloped an ERNIE-based retrieval model, which is equipped with 1) expressive\nTransformer-based semantic encoders, and 2) a comprehensive multi-stage\ntraining paradigm. More importantly, we present a practical system workflow for\ndeploying the model in web-scale retrieval. Eventually, the system is fully\ndeployed into production, where rigorous offline and online experiments were\nconducted. The results show that the system can perform high-quality candidate\nretrieval, especially for those tail queries with uncommon demands. Overall,\nthe new retrieval system facilitated by pretrained language model (i.e., ERNIE)\ncan largely improve the usability and applicability of our search engine.",
    "descriptor": "",
    "authors": [
      "Yiding Liu",
      "Weixue Lu",
      "Suqi Cheng",
      "Daiting Shi",
      "Shuaiqiang Wang",
      "Zhicong Cheng",
      "Dawei Yin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03373"
  },
  {
    "id": "arXiv:2106.03374",
    "title": "MixRL: Data Mixing Augmentation for Regression using Reinforcement  Learning",
    "abstract": "Data augmentation is becoming essential for improving regression accuracy in\ncritical applications including manufacturing and finance. Existing techniques\nfor data augmentation largely focus on classification tasks and do not readily\napply to regression tasks. In particular, the recent Mixup techniques for\nclassification rely on the key assumption that linearity holds among training\nexamples, which is reasonable if the label space is discrete, but has\nlimitations when the label space is continuous as in regression. We show that\nmixing examples that either have a large data or label distance may have an\nincreasingly-negative effect on model performance. Hence, we use the stricter\nassumption that linearity only holds within certain data or label distances for\nregression where the degree may vary by each example. We then propose MixRL, a\ndata augmentation meta learning framework for regression that learns for each\nexample how many nearest neighbors it should be mixed with for the best model\nperformance using a small validation set. MixRL achieves these objectives using\nMonte Carlo policy gradient reinforcement learning. Our experiments conducted\nboth on synthetic and real datasets show that MixRL significantly outperforms\nstate-of-the-art data augmentation baselines. MixRL can also be integrated with\nother classification Mixup techniques for better results.",
    "descriptor": "\nComments: 14 pages, 8 figures, 6 tables\n",
    "authors": [
      "Seong-Hyeon Hwang",
      "Steven Euijong Whang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03374"
  },
  {
    "id": "arXiv:2106.03375",
    "title": "Commutative Lie Group VAE for Disentanglement Learning",
    "abstract": "We view disentanglement learning as discovering an underlying structure that\nequivariantly reflects the factorized variations shown in data. Traditionally,\nsuch a structure is fixed to be a vector space with data variations represented\nby translations along individual latent dimensions. We argue this simple\nstructure is suboptimal since it requires the model to learn to discard the\nproperties (e.g. different scales of changes, different levels of abstractness)\nof data variations, which is an extra work than equivariance learning. Instead,\nwe propose to encode the data variations with groups, a structure not only can\nequivariantly represent variations, but can also be adaptively optimized to\npreserve the properties of data variations. Considering it is hard to conduct\ntraining on group structures, we focus on Lie groups and adopt a\nparameterization using Lie algebra. Based on the parameterization, some\ndisentanglement learning constraints are naturally derived. A simple model\nnamed Commutative Lie Group VAE is introduced to realize the group-based\ndisentanglement learning. Experiments show that our model can effectively learn\ndisentangled representations without supervision, and can achieve\nstate-of-the-art performance without extra constraints.",
    "descriptor": "\nComments: Accepted in ICML2021\n",
    "authors": [
      "Xinqi Zhu",
      "Chang Xu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03375"
  },
  {
    "id": "arXiv:2106.03376",
    "title": "A Globally Normalized Neural Model for Semantic Parsing",
    "abstract": "In this paper, we propose a globally normalized model for context-free\ngrammar (CFG)-based semantic parsing. Instead of predicting a probability, our\nmodel predicts a real-valued score at each step and does not suffer from the\nlabel bias problem. Experiments show that our approach outperforms locally\nnormalized models on small datasets, but it does not yield improvement on a\nlarge dataset.",
    "descriptor": "",
    "authors": [
      "Chenyang Huang",
      "Wei Yang",
      "Yanshuai Cao",
      "Osmar Za\u00efane",
      "Lili Mou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03376"
  },
  {
    "id": "arXiv:2106.03377",
    "title": "On the Skew-Symmetric Binary Sequences and the Merit Factor Problem",
    "abstract": "The merit factor problem is of practical importance to manifold domains, such\nas digital communications engineering, radars, system modulation, system\ntesting, information theory, physics, chemistry. However, the merit factor\nproblem is referenced as one of the most difficult optimization problems and it\nwas further conjectured that stochastic search procedures will not yield merit\nfactors higher than 5 for long binary sequences (sequences with lengths greater\nthan 200). Some useful mathematical properties related to the flip operation of\nthe skew-symmetric binary sequences are presented in this work. By exploiting\nthose properties, the memory complexity of state-of-the-art stochastic merit\nfactor optimization algorithms could be reduced from $O(n^2)$ to $O(n)$. As a\nproof of concept, a lightweight stochastic algorithm was constructed, which can\noptimize pseudo-randomly generated skew-symmetric binary sequences with long\nlengths (up to ${10}^5+1$) to skew-symmetric binary sequences with a merit\nfactor greater than 5. An approximation of the required time is also provided.\nThe numerical experiments suggest that the algorithm is universal and could be\napplied to skew-symmetric binary sequences with arbitrary lengths.",
    "descriptor": "",
    "authors": [
      "Miroslav Dimitrov"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03377"
  },
  {
    "id": "arXiv:2106.03379",
    "title": "LAWDR: Language-Agnostic Weighted Document Representations from  Pre-trained Models",
    "abstract": "Cross-lingual document representations enable language understanding in\nmultilingual contexts and allow transfer learning from high-resource to\nlow-resource languages at the document level. Recently large pre-trained\nlanguage models such as BERT, XLM and XLM-RoBERTa have achieved great success\nwhen fine-tuned on sentence-level downstream tasks. It is tempting to apply\nthese cross-lingual models to document representation learning. However, there\nare two challenges: (1) these models impose high costs on long document\nprocessing and thus many of them have strict length limit; (2) model\nfine-tuning requires extra data and computational resources, which is not\npractical in resource-limited settings. In this work, we address these\nchallenges by proposing unsupervised Language-Agnostic Weighted Document\nRepresentations (LAWDR). We study the geometry of pre-trained sentence\nembeddings and leverage it to derive document representations without\nfine-tuning. Evaluated on cross-lingual document alignment, LAWDR demonstrates\ncomparable performance to state-of-the-art models on benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Hongyu Gong",
      "Vishrav Chaudhary",
      "Yuqing Tang",
      "Francisco Guzm\u00e1n"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03379"
  },
  {
    "id": "arXiv:2106.03382",
    "title": "ContourRender: Detecting Arbitrary Contour Shape For Instance  Segmentation In One Pass",
    "abstract": "Direct contour regression for instance segmentation is a challenging task.\nPrevious works usually achieve it by learning to progressively refine the\ncontour prediction or adopting a shape representation with limited\nexpressiveness. In this work, we argue that the difficulty in regressing the\ncontour points in one pass is mainly due to the ambiguity when discretizing a\nsmooth contour into a polygon. To address the ambiguity, we propose a novel\ndifferentiable rendering-based approach named \\textbf{ContourRender}. During\ntraining, it first predicts a contour generated by an invertible shape\nsignature, and then optimizes the contour with the more stable silhouette by\nconverting it to a contour mesh and rendering the mesh to a 2D map.\nThis method significantly improves the quality of contour without iterations\nor cascaded refinements. Moreover, as optimization is not needed during\ninference, the inference speed will not be influenced.\nExperiments show the proposed ContourRender outperforms all the contour-based\ninstance segmentation approaches on COCO, while stays competitive with the\niteration-based state-of-the-art on Cityscapes. In addition, we specifically\nselect a subset from COCO val2017 named COCO ContourHard-val to further\ndemonstrate the contour quality improvements. Codes, models, and dataset split\nwill be released.",
    "descriptor": "\nComments: Tech report\n",
    "authors": [
      "Tutian Tang",
      "Wenqiang Xu",
      "Ruolin Ye",
      "Yan-Feng Wang",
      "Cewu Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03382"
  },
  {
    "id": "arXiv:2106.03386",
    "title": "Corona Health -- A Study- and Sensor-based Mobile App Platform Exploring  Aspects of the COVID-19 Pandemic",
    "abstract": "Physical and mental well-being during the COVID-19 pandemic is typically\nassessed via surveys, which might make it difficult to conduct longitudinal\nstudies and might lead to data suffering from recall bias. Ecological momentary\nassessment (EMA) driven smartphone apps can help alleviate such issues,\nallowing for in situ recordings. Implementing such an app is not trivial,\nnecessitates strict regulatory and legal requirements, and requires short\ndevelopment cycles to appropriately react to abrupt changes in the pandemic.\nBased on an existing app framework, we developed Corona Health, an app that\nserves as a platform for deploying questionnaire-based studies in combination\nwith recordings of mobile sensors. In this paper, we present the technical\ndetails of Corona Health and provide first insights into the collected data.\nThrough collaborative efforts from experts from public health, medicine,\npsychology, and computer science, we released Corona Health publicly on Google\nPlay and the Apple App Store (in July, 2020) in 8 languages and attracted 7,290\ninstallations so far. Currently, five studies related to physical and mental\nwell-being are deployed and 17,241 questionnaires have been filled out. Corona\nHealth proves to be a viable tool for conducting research related to the\nCOVID-19 pandemic and can serve as a blueprint for future EMA-based studies.\nThe data we collected will substantially improve our knowledge on mental and\nphysical health states, traits and trajectories as well as its risk and\nprotective factors over the course of the COVID-19 pandemic and its diverse\nprevention measures.",
    "descriptor": "",
    "authors": [
      "Felix Beierle",
      "Johannes Schobel",
      "Carsten Vogel",
      "Johannes Allgaier",
      "Lena Mulansky",
      "Fabian Haug",
      "Julian Haug",
      "Winfried Schlee",
      "Marc Holfelder",
      "Michael Stach",
      "Marc Schickler",
      "Harald Baumeister",
      "Caroline Cohrdes",
      "J\u00fcrgen Deckert",
      "Lorenz Deserno",
      "Johanna-Sophie Edler",
      "Felizitas A. Eichner",
      "Helmut Greger",
      "Grit Hein",
      "Peter Heuschmann",
      "Dennis John",
      "Hans A. Kestler",
      "Dagmar Krefting",
      "Berthold Langguth",
      "Patrick Meybohm",
      "Thomas Probst",
      "Manfred Reichert",
      "Marcel Romanos",
      "Stefan St\u00f6rk",
      "Yannik Terhorst",
      "Martin Wei\u00df",
      "R\u00fcdiger Pryss"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.03386"
  },
  {
    "id": "arXiv:2106.03387",
    "title": "Difference methods for time discretization of stochastic wave equation",
    "abstract": "The time discretization of stochastic spectral fractional wave equation is\nstudied by using the difference methods. Firstly, we exploit rectangle formula\nto get a low order time discretization, whose the strong convergence order is\nsmaller than $1$ in the sense of mean-squared $L^2$-norm. Meanwhile, by\nmodifying the low order method with trapezoidal rule, the convergence rate is\nimproved at expenses of requiring some extra temporal regularity to the\nsolution. The modified scheme has superlinear convergence rate under the\nmean-squared $L^2$-norm. Several numerical experiments are provided to confirm\nthe theoretical error estimates.",
    "descriptor": "\nComments: 22 pages, 2 figures\n",
    "authors": [
      "Xing Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03387"
  },
  {
    "id": "arXiv:2106.03388",
    "title": "DINs: Deep Interactive Networks for Neurofibroma Segmentation in  Neurofibromatosis Type 1 on Whole-Body MRI",
    "abstract": "Neurofibromatosis type 1 (NF1) is an autosomal dominant tumor predisposition\nsyndrome that involves the central and peripheral nervous systems. Accurate\ndetection and segmentation of neurofibromas are essential for assessing tumor\nburden and longitudinal tumor size changes. Automatic convolutional neural\nnetworks (CNNs) are sensitive and vulnerable as tumors' variable anatomical\nlocation and heterogeneous appearance on MRI. In this study, we propose deep\ninteractive networks (DINs) to address the above limitations. User interactions\nguide the model to recognize complicated tumors and quickly adapt to\nheterogeneous tumors. We introduce a simple but effective Exponential Distance\nTransform (ExpDT) that converts user interactions into guide maps regarded as\nthe spatial and appearance prior. Comparing with popular Euclidean and geodesic\ndistances, ExpDT is more robust to various image sizes, which reserves the\ndistribution of interactive inputs. Furthermore, to enhance the tumor-related\nfeatures, we design a deep interactive module to propagate the guides into\ndeeper layers. We train and evaluate DINs on three MRI data sets from NF1\npatients. The experiment results yield significant improvements of 44% and 14%\nin DSC comparing with automated and other interactive methods, respectively. We\nalso experimentally demonstrate the efficiency of DINs in reducing user burden\nwhen comparing with conventional interactive methods. The source code of our\nmethod is available at \\url{https://github.com/Jarvis73/DINs}.",
    "descriptor": "\nComments: Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)\n",
    "authors": [
      "Jian-Wei Zhang",
      "Wei Chen",
      "K. Ina Ly",
      "Xubin Zhang",
      "Fan Yan",
      "Justin Jordan",
      "Gordon Harris",
      "Scott Plotkin",
      "Pengyi Hao",
      "Wenli Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03388"
  },
  {
    "id": "arXiv:2106.03389",
    "title": "Never guess what I heard... Rumor Detection in Finnish News: a Dataset  and a Baseline",
    "abstract": "This study presents a new dataset on rumor detection in Finnish language news\nheadlines. We have evaluated two different LSTM based models and two different\nBERT models, and have found very significant differences in the results. A\nfine-tuned FinBERT reaches the best overall accuracy of 94.3% and rumor label\naccuracy of 96.0% of the time. However, a model fine-tuned on Multilingual BERT\nreaches the best factual label accuracy of 97.2%. Our results suggest that the\nperformance difference is due to a difference in the original training data.\nFurthermore, we find that a regular LSTM model works better than one trained\nwith a pretrained word2vec model. These findings suggest that more work needs\nto be done for pretrained models in Finnish language as they have been trained\non small and biased corpora.",
    "descriptor": "\nComments: 2021 Workshop on NLP4IF: Censorship, Disinformation, and Propaganda\n",
    "authors": [
      "Mika H\u00e4m\u00e4l\u00e4inen",
      "Khalid Alnajjar",
      "Niko Partanen",
      "Jack Rueter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03389"
  },
  {
    "id": "arXiv:2106.03391",
    "title": "Apurin\u00e3 Universal Dependencies Treebank",
    "abstract": "This paper presents and discusses the first Universal Dependencies treebank\nfor the Apurin\\~a language. The treebank contains 76 fully annotated sentences,\napplies 14 parts-of-speech, as well as seven augmented or new features - some\nof which are unique to Apurin\\~a. The construction of the treebank has also\nserved as an opportunity to develop finite-state description of the language\nand facilitate the transfer of open-source infrastructure possibilities to an\nendangered language of the Amazon. The source materials used in the initial\ntreebank represent fieldwork practices where not all tokens of all sentences\nare equally annotated. For this reason, establishing regular annotation\npractices for the entire Apurin\\~a treebank is an ongoing project.",
    "descriptor": "\nComments: The First Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)\n",
    "authors": [
      "Jack Rueter",
      "Mar\u00edlia Fernanda Pereira de Freitas",
      "Sidney da Silva Facundes",
      "Mika H\u00e4m\u00e4l\u00e4inen",
      "Niko Partanen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03391"
  },
  {
    "id": "arXiv:2106.03393",
    "title": "Adversarially Regularized Graph Attention Networks for Inductive  Learning on Partially Labeled Graphs",
    "abstract": "Graph embedding is a general approach to tackling graph-analytic problems by\nencoding nodes into low-dimensional representations. Most existing embedding\nmethods are transductive since the information of all nodes is required in\ntraining, including those to be predicted. In this paper, we propose a novel\ninductive embedding method for semi-supervised learning on graphs. This method\ngenerates node representations by learning a parametric function to aggregate\ninformation from the neighborhood using an attention mechanism, and hence\nnaturally generalizes to previously unseen nodes. Furthermore, adversarial\ntraining serves as an external regularization enforcing the learned\nrepresentations to match a prior distribution for improving robustness and\ngeneralization ability. Experiments on real-world clean or noisy graphs are\nused to demonstrate the effectiveness of this approach.",
    "descriptor": "",
    "authors": [
      "Jiaren Xiao",
      "Quanyu Dai",
      "Xiaochen Xie",
      "James Lam",
      "Ka-Wai Kwok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03393"
  },
  {
    "id": "arXiv:2106.03394",
    "title": "A generative model for molecule generation based on chemical reaction  trees",
    "abstract": "Deep generative models have been shown powerful in generating novel molecules\nwith desired chemical properties via their representations such as strings,\ntrees or graphs. However, these models are limited in recommending synthetic\nroutes for the generated molecules in practice. We propose a generative model\nto generate molecules via multi-step chemical reaction trees. Specifically, our\nmodel first propose a chemical reaction tree with predicted reaction templates\nand commercially available molecules (starting molecules), and then perform\nforward synthetic steps to obtain product molecules. Experiments show that our\nmodel can generate chemical reactions whose product molecules are with desired\nchemical properties. Also, the complete synthetic routes for these product\nmolecules are provided.",
    "descriptor": "",
    "authors": [
      "Dai Hai Nguyen",
      "Koji Tsuda"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.03394"
  },
  {
    "id": "arXiv:2106.03399",
    "title": "Scientific Dataset Discovery via Topic-level Recommendation",
    "abstract": "Data intensive research requires the support of appropriate datasets.\nHowever, it is often time-consuming to discover usable datasets matching a\nspecific research topic. We formulate the dataset discovery problem on an\nattributed heterogeneous graph, which is composed of paper-paper citation,\npaper-dataset citation, and also paper content. We propose to characterize both\npaper and dataset nodes by their commonly shared latent topics, rather than\nlearning user and item representations via canonical graph embedding models,\nbecause the usage of datasets and the themes of research projects can be\nunderstood on the common base of research topics. The relevant datasets to a\ngiven research project can then be inferred in the shared topic space. The\nexperimental results show that our model can generate reasonable profiles for\ndatasets, and recommend proper datasets for a query, which represents a\nresearch project linked with several papers.",
    "descriptor": "",
    "authors": [
      "Basmah Altaf",
      "Shichao Pei",
      "Xiangliang Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03399"
  },
  {
    "id": "arXiv:2106.03400",
    "title": "Believe What You See: Implicit Constraint Approach for Offline  Multi-Agent Reinforcement Learning",
    "abstract": "Learning from datasets without interaction with environments (Offline\nLearning) is an essential step to apply Reinforcement Learning (RL) algorithms\nin real-world scenarios. However, compared with the single-agent counterpart,\noffline multi-agent RL introduces more agents with the larger state and action\nspace, which is more challenging but attracts little attention. We demonstrate\ncurrent offline RL algorithms are ineffective in multi-agent systems due to the\naccumulated extrapolation error. In this paper, we propose a novel offline RL\nalgorithm, named Implicit Constraint Q-learning (ICQ), which effectively\nalleviates the extrapolation error by only trusting the state-action pairs\ngiven in the dataset for value estimation. Moreover, we extend ICQ to\nmulti-agent tasks by decomposing the joint-policy under the implicit\nconstraint. Experimental results demonstrate that the extrapolation error is\nreduced to almost zero and insensitive to the number of agents. We further show\nthat ICQ achieves the state-of-the-art performance in the challenging\nmulti-agent offline tasks (StarCraft II).",
    "descriptor": "\nComments: The first two authors contributed equally to the work\n",
    "authors": [
      "Yiqin Yang",
      "Xiaoteng Ma",
      "Chenghao Li",
      "Zewu Zheng",
      "Qiyuan Zhang",
      "Gao Huang",
      "Jun Yang",
      "Qianchuan Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03400"
  },
  {
    "id": "arXiv:2106.03403",
    "title": "Network Inference and Influence Maximization from Samples",
    "abstract": "Influence maximization is the task of selecting a small number of seed nodes\nin a social network to maximize the spread of the influence from these seeds,\nand it has been widely investigated in the past two decades. In the canonical\nsetting, the whole social network as well as its diffusion parameters is given\nas input. In this paper, we consider the more realistic sampling setting where\nthe network is unknown and we only have a set of passively observed cascades\nthat record the set of activated nodes at each diffusion step. We study the\ntask of influence maximization from these cascade samples (IMS), and present\nconstant approximation algorithms for this task under mild conditions on the\nseed set distribution. To achieve the optimization goal, we also provide a\nnovel solution to the network inference problem, that is, learning diffusion\nparameters and the network structure from the cascade data. Comparing with\nprior solutions, our network inference algorithm requires weaker assumptions\nand does not rely on maximum-likelihood estimation and convex programming. Our\nIMS algorithms enhance the learning-and-then-optimization approach by allowing\na constant approximation ratio even when the diffusion parameters are hard to\nlearn, and we do not need any assumption related to the network structure or\ndiffusion parameters.",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Wei Chen",
      "Xiaoming Sun",
      "Jialin Zhang",
      "Zhijie Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03403"
  },
  {
    "id": "arXiv:2106.03407",
    "title": "Multi-goal path planning using multiple random trees",
    "abstract": "In this paper, we propose a novel sampling-based planner for multi-goal path\nplanning among obstacles, where the objective is to visit predefined target\nlocations while minimizing the travel costs. The order of visiting the targets\nis often achieved by solving the Traveling Salesman Problem (TSP) or its\nvariants. TSP requires to define costs between the individual targets, which -\nin a map with obstacles - requires to compute mutual paths between the targets.\nThese paths, found by path planning, are used both to define the costs (e.g.,\nbased on their length or time-to-traverse) and also they define paths that are\nlater used in the final solution. To enable TSP finding a good-quality\nsolution, it is necessary to find these target-to-target paths as short as\npossible. We propose a sampling-based planner called Space-Filling Forest\n(SFF*) that solves the part of finding collision-free paths. SFF* uses multiple\ntrees (forest) constructed gradually and simultaneously from the targets and\nattempts to find connections with other trees to form the paths. Unlike\nRapidly-exploring Random Tree (RRT), which uses the nearest-neighbor rule for\nselecting nodes for expansion, SFF* maintains an explicit list of nodes for\nexpansion. Individual trees are grown in a RRT* manner, i.e., with rewiring the\nnodes to minimize their cost. Computational results show that SFF* provides\nshorter target-to-target paths than existing approaches, and consequently, the\nfinal TSP solutions also have a lower cost.",
    "descriptor": "",
    "authors": [
      "Jaroslav Jano\u0161",
      "Vojt\u011bch Von\u00e1sek",
      "Robert P\u011bni\u010dka"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03407"
  },
  {
    "id": "arXiv:2106.03408",
    "title": "Antipodes of Label Differential Privacy: PATE and ALIBI",
    "abstract": "We consider the privacy-preserving machine learning (ML) setting where the\ntrained model must satisfy differential privacy (DP) with respect to the labels\nof the training examples. We propose two novel approaches based on,\nrespectively, the Laplace mechanism and the PATE framework, and demonstrate\ntheir effectiveness on standard benchmarks.\nWhile recent work by Ghazi et al. proposed Label DP schemes based on a\nrandomized response mechanism, we argue that additive Laplace noise coupled\nwith Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover,\nwe show how to achieve very strong privacy levels in some regimes, with our\nadaptation of the PATE framework that builds on recent advances in\nsemi-supervised learning.\nWe complement theoretical analysis of our algorithms' privacy guarantees with\nempirical evaluation of their memorization properties. Our evaluation suggests\nthat comparing different algorithms according to their provable DP guarantees\ncan be misleading and favor a less private algorithm with a tighter analysis.",
    "descriptor": "\nComments: Code for implementation of algorithms and memorization attacks is available from this https URL under MIT license\n",
    "authors": [
      "Mani Malek",
      "Ilya Mironov",
      "Karthik Prasad",
      "Igor Shilov",
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03408"
  },
  {
    "id": "arXiv:2106.03410",
    "title": "Generating Relevant and Coherent Dialogue Responses using Self-separated  Conditional Variational AutoEncoders",
    "abstract": "Conditional Variational AutoEncoder (CVAE) effectively increases the\ndiversity and informativeness of responses in open-ended dialogue generation\ntasks through enriching the context vector with sampled latent variables.\nHowever, due to the inherent one-to-many and many-to-one phenomena in human\ndialogues, the sampled latent variables may not correctly reflect the contexts'\nsemantics, leading to irrelevant and incoherent generated responses. To resolve\nthis problem, we propose Self-separated Conditional Variational AutoEncoder\n(abbreviated as SepaCVAE) that introduces group information to regularize the\nlatent variables, which enhances CVAE by improving the responses' relevance and\ncoherence while maintaining their diversity and informativeness. SepaCVAE\nactively divides the input data into groups, and then widens the absolute\ndifference between data pairs from distinct groups, while narrowing the\nrelative distance between data pairs in the same group. Empirical results from\nautomatic evaluation and detailed analysis demonstrate that SepaCVAE can\nsignificantly boost responses in well-established open-domain dialogue\ndatasets.",
    "descriptor": "",
    "authors": [
      "Bin Sun",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Jiamou Liu",
      "Kan Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03410"
  },
  {
    "id": "arXiv:2106.03412",
    "title": "Resolution learning in deep convolutional networks using scale-space  theory",
    "abstract": "Resolution in deep convolutional neural networks (CNNs) is typically bounded\nby the receptive field size through filter sizes, and subsampling layers or\nstrided convolutions on feature maps. The optimal resolution may vary\nsignificantly depending on the dataset. Modern CNNs hard-code their resolution\nhyper-parameters in the network architecture which makes tuning such\nhyper-parameters cumbersome. We propose to do away with hard-coded resolution\nhyper-parameters and aim to learn the appropriate resolution from data. We use\nscale-space theory to obtain a self-similar parametrization of filters and make\nuse of the N-Jet: a truncated Taylor series to approximate a filter by a\nlearned combination of Gaussian derivative filters. The parameter {\\sigma} of\nthe Gaussian basis controls both the amount of detail the filter encodes and\nthe spatial extent of the filter. Since {\\sigma} is a continuous parameter, we\ncan optimize it with respect to the loss. The proposed N-Jet layer achieves\ncomparable performance when used in state-of-the art architectures, while\nlearning the correct resolution in each layer automatically. We evaluate our\nN-Jet layer on both classification and segmentation, and we show that learning\n{\\sigma} is especially beneficial for inputs at multiple sizes.",
    "descriptor": "",
    "authors": [
      "Silvia L.Pintea",
      "Nergis Tomen",
      "Stanley F. Goes",
      "Marco Loog",
      "Jan C. van Gemert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03412"
  },
  {
    "id": "arXiv:2106.03415",
    "title": "Leveraging Tripartite Interaction Information from Live Stream  E-Commerce for Improving Product Recommendation",
    "abstract": "Recently, a new form of online shopping becomes more and more popular, which\ncombines live streaming with E-Commerce activity. The streamers introduce\nproducts and interact with their audiences, and hence greatly improve the\nperformance of selling products. Despite of the successful applications in\nindustries, the live stream E-commerce has not been well studied in the data\nscience community. To fill this gap, we investigate this brand-new scenario and\ncollect a real-world Live Stream E-Commerce (LSEC) dataset. Different from\nconventional E-commerce activities, the streamers play a pivotal role in the\nLSEC events. Hence, the key is to make full use of rich interaction information\namong streamers, users, and products. We first conduct data analysis on the\ntripartite interaction data and quantify the streamer's influence on users'\npurchase behavior. Based on the analysis results, we model the tripartite\ninformation as a heterogeneous graph, which can be decomposed to multiple\nbipartite graphs in order to better capture the influence. We propose a novel\nLive Stream E-Commerce Graph Neural Network framework (LSEC-GNN) to learn the\nnode representations of each bipartite graph, and further design a multi-task\nlearning approach to improve product recommendation. Extensive experiments on\ntwo real-world datasets with different scales show that our method can\nsignificantly outperform various baseline approaches.",
    "descriptor": "\nComments: To appear in KDD'21 ADS\n",
    "authors": [
      "Sanshi Yu",
      "Zhuoxuan Jiang",
      "Dong-Dong Chen",
      "Shanshan Feng",
      "Dongsheng Li",
      "Qi Liu",
      "Jinfeng Yi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03415"
  },
  {
    "id": "arXiv:2106.03418",
    "title": "Multi-Target Domain Adaptation with Collaborative Consistency Learning",
    "abstract": "Recently unsupervised domain adaptation for the semantic segmentation task\nhas become more and more popular due to high-cost of pixel-level annotation on\nreal-world images. However, most domain adaptation methods are only restricted\nto single-source-single-target pair, and can not be directly extended to\nmultiple target domains. In this work, we propose a collaborative learning\nframework to achieve unsupervised multi-target domain adaptation. An\nunsupervised domain adaptation expert model is first trained for each\nsource-target pair and is further encouraged to collaborate with each other\nthrough a bridge built between different target domains. These expert models\nare further improved by adding the regularization of making the consistent\npixel-wise prediction for each sample with the same structured context. To\nobtain a single model that works across multiple target domains, we propose to\nsimultaneously learn a student model which is trained to not only imitate the\noutput of each expert on the corresponding target domain, but also to pull\ndifferent expert close to each other with regularization on their weights.\nExtensive experiments demonstrate that the proposed method can effectively\nexploit rich structured information contained in both labeled source domain and\nmultiple unlabeled target domains. Not only does it perform well across\nmultiple target domains but also performs favorably against state-of-the-art\nunsupervised domain adaptation methods specially trained on a single\nsource-target pair",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Takashi Isobe",
      "Xu Jia",
      "Shuaijun Chen",
      "Jianzhong He",
      "Yongjie Shi",
      "Jianzhuang Liu",
      "Huchuan Lu",
      "Shengjin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03418"
  },
  {
    "id": "arXiv:2106.03422",
    "title": "Source-Free Open Compound Domain Adaptation in Semantic Segmentation",
    "abstract": "In this work, we introduce a new concept, named source-free open compound\ndomain adaptation (SF-OCDA), and study it in semantic segmentation. SF-OCDA is\nmore challenging than the traditional domain adaptation but it is more\npractical. It jointly considers (1) the issues of data privacy and data storage\nand (2) the scenario of multiple target domains and unseen open domains. In\nSF-OCDA, only the source pre-trained model and the target data are available to\nlearn the target model. The model is evaluated on the samples from the target\nand unseen open domains. To solve this problem, we present an effective\nframework by separating the training process into two stages: (1) pre-training\na generalized source model and (2) adapting a target model with self-supervised\nlearning. In our framework, we propose the Cross-Patch Style Swap (CPSS) to\ndiversify samples with various patch styles in the feature-level, which can\nbenefit the training of both stages. First, CPSS can significantly improve the\ngeneralization ability of the source model, providing more accurate\npseudo-labels for the latter stage. Second, CPSS can reduce the influence of\nnoisy pseudo-labels and also avoid the model overfitting to the target domain\nduring self-supervised learning, consistently boosting the performance on the\ntarget and open domains. Experiments demonstrate that our method produces\nstate-of-the-art results on the C-Driving dataset. Furthermore, our model also\nachieves the leading performance on CityScapes for domain generalization.",
    "descriptor": "",
    "authors": [
      "Yuyang Zhao",
      "Zhun Zhong",
      "Zhiming Luo",
      "Gim Hee Lee",
      "Nicu Sebe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03422"
  },
  {
    "id": "arXiv:2106.03425",
    "title": "An Algorithmic Meta-Theorem for Graph Modification to Planarity and FOL",
    "abstract": "In general, a graph modification problem is defined by a graph modification\noperation $\\boxtimes$ and a target graph property ${\\cal P}$. Typically, the\nmodification operation $\\boxtimes$ may be vertex removal}, edge removal}, edge\ncontraction}, or edge addition and the question is, given a graph $G$ and an\ninteger $k$, whether it is possible to transform $G$ to a graph in ${\\cal P}$\nafter applying $k$ times the operation $\\boxtimes$ on $G$. This problem has\nbeen extensively studied for particilar instantiations of $\\boxtimes$ and\n${\\cal P}$. In this paper we consider the general property ${\\cal P}_{{\\phi}}$\nof being planar and, moreover, being a model of some First-Order Logic sentence\n${\\phi}$ (an FOL-sentence). We call the corresponding meta-problem Graph\n$\\boxtimes$-Modification to Planarity and ${\\phi}$ and prove the following\nalgorithmic meta-theorem: there exists a function $f:\\Bbb{N}^{2}\\to\\Bbb{N}$\nsuch that, for every $\\boxtimes$ and every FOL sentence ${\\phi}$, the Graph\n$\\boxtimes$-Modification to Planarity and ${\\phi}$ is solvable in\n$f(k,|{\\phi}|)\\cdot n^2$ time. The proof constitutes a hybrid of two different\nclassic techniques in graph algorithms. The first is the irrelevant vertex\ntechnique that is typically used in the context of Graph Minors and deals with\nproperties such as planarity or surface-embeddability (that are not\nFOL-expressible)\nand the second is the use of Gaifman's Locality Theorem that is the\ntheoretical base for the meta-algorithmic study of FOL-expressible problems.",
    "descriptor": "",
    "authors": [
      "Fedor V. Fomin",
      "Petr A. Golovach",
      "Giannos Stamoulis",
      "Dimitrios M. Thilikos"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.03425"
  },
  {
    "id": "arXiv:2106.03427",
    "title": "Hierarchical Task Learning from Language Instructions with Unified  Transformers and Self-Monitoring",
    "abstract": "Despite recent progress, learning new tasks through language instructions\nremains an extremely challenging problem. On the ALFRED benchmark for task\nlearning, the published state-of-the-art system only achieves a task success\nrate of less than 10% in an unseen environment, compared to the human\nperformance of over 90%. To address this issue, this paper takes a closer look\nat task learning. In a departure from a widely applied end-to-end architecture,\nwe decomposed task learning into three sub-problems: sub-goal planning, scene\nnavigation, and object manipulation; and developed a model HiTUT (stands for\nHierarchical Tasks via Unified Transformers) that addresses each sub-problem in\na unified manner to learn a hierarchical task structure. On the ALFRED\nbenchmark, HiTUT has achieved the best performance with a remarkably higher\ngeneralization ability. In the unseen environment, HiTUT achieves over 160%\nperformance gain in success rate compared to the previous state of the art. The\nexplicit representation of task structures also enables an in-depth\nunderstanding of the nature of the problem and the ability of the agent, which\nprovides insight for future benchmark development and evaluation.",
    "descriptor": "\nComments: Accepted by ACL 2021 Findings\n",
    "authors": [
      "Yichi Zhang",
      "Joyce Chai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03427"
  },
  {
    "id": "arXiv:2106.03428",
    "title": "Automation for Interpretable Machine Learning Through a Comparison of  Loss Functions to Regularisers",
    "abstract": "To increase the ubiquity of machine learning it needs to be automated.\nAutomation is cost-effective as it allows experts to spend less time tuning the\napproach, which leads to shorter development times. However, while this\nautomation produces highly accurate architectures, they can be uninterpretable,\nacting as `black-boxes' which produce low conventional errors but fail to model\nthe underlying input-output relationships -- the ground truth. This paper\nexplores the use of the Fit to Median Error measure in machine learning\nregression automation, using evolutionary computation in order to improve the\napproximation of the ground truth. When used alongside conventional error\nmeasures it improves interpretability by regularising learnt input-output\nrelationships to the conditional median. It is compared to traditional\nregularisers to illustrate that the use of the Fit to Median Error produces\nregression neural networks which model more consistent input-output\nrelationships. The problem considered is ship power prediction using a\nfuel-saving air lubrication system, which is highly stochastic in nature. The\nnetworks optimised for their Fit to Median Error are shown to approximate the\nground truth more consistently, without sacrificing conventional Minkowski-r\nerror values.",
    "descriptor": "\nComments: 11 pages, 5 figures, under review,\n",
    "authors": [
      "A. I. Parkes",
      "J. Camilleri",
      "D. A. Hudson",
      "A. J. Sobey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03428"
  },
  {
    "id": "arXiv:2106.03432",
    "title": "Channel DropBlock: An Improved Regularization Method for Fine-Grained  Visual Classification",
    "abstract": "Classifying the sub-categories of an object from the same super-category\n(e.g., bird) in a fine-grained visual classification (FGVC) task highly relies\non mining multiple discriminative features. Existing approaches mainly tackle\nthis problem by introducing attention mechanisms to locate the discriminative\nparts or feature encoding approaches to extract the highly parameterized\nfeatures in a weakly-supervised fashion. In this work, we propose a lightweight\nyet effective regularization method named Channel DropBlock (CDB), in\ncombination with two alternative correlation metrics, to address this problem.\nThe key idea is to randomly mask out a group of correlated channels during\ntraining to destruct features from co-adaptations and thus enhance feature\nrepresentations. Extensive experiments on three benchmark FGVC datasets show\nthat CDB effectively improves the performance.",
    "descriptor": "",
    "authors": [
      "Yifeng Ding",
      "Shuwei Dong",
      "Yujun Tong",
      "Zhanyu Ma",
      "Bo Xiao",
      "Haibin Ling"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03432"
  },
  {
    "id": "arXiv:2106.03437",
    "title": "Unsupervised Learning for Cuboid Shape Abstraction via Joint  Segmentation from Point Clouds",
    "abstract": "Representing complex 3D objects as simple geometric primitives, known as\nshape abstraction, is important for geometric modeling, structural analysis,\nand shape synthesis. In this paper, we propose an unsupervised shape\nabstraction method to map a point cloud into a compact cuboid representation.\nWe jointly predict cuboid allocation as part segmentation and cuboid shapes and\nenforce the consistency between the segmentation and shape abstraction for\nself-learning. For the cuboid abstraction task, we transform the input point\ncloud into a set of parametric cuboids using a variational auto-encoder\nnetwork. The segmentation network allocates each point into a cuboid\nconsidering the point-cuboid affinity. Without manual annotations of parts in\npoint clouds, we design four novel losses to jointly supervise the two branches\nin terms of geometric similarity and cuboid compactness. We evaluate our method\non multiple shape collections and demonstrate its superiority over existing\nshape abstraction methods. Moreover, based on our network architecture and\nlearned representations, our approach supports various applications including\nstructured shape generation, shape interpolation, and structural shape\nclustering.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Kaizhi Yang",
      "Xuejin Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.03437"
  },
  {
    "id": "arXiv:2106.03441",
    "title": "Attention Temperature Matters in Abstractive Summarization Distillation",
    "abstract": "Recent progress of abstractive text summarization largely relies on large\npre-trained sequence-to-sequence Transformer models, which are computationally\nexpensive. This paper aims to distill these large models into smaller ones for\nfaster inference and minimal performance loss. Pseudo-labeling based methods\nare popular in sequence-to-sequence model distillation. In this paper, we find\nsimply manipulating attention temperatures in Transformers can make pseudo\nlabels easier to learn for student models. Our experiments on three\nsummarization datasets show our proposed method consistently improves over\nvanilla pseudo-labeling based methods. We also find that both the pseudo labels\nand summaries produced by our students are shorter and more abstractive. We\nwill make our code and models publicly available.",
    "descriptor": "\nComments: Submitted to NeurIPS 2021\n",
    "authors": [
      "Shengqiang Zhang",
      "Xingxing Zhang",
      "Hangbo Bao",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03441"
  },
  {
    "id": "arXiv:2106.03442",
    "title": "Average-Reward Reinforcement Learning with Trust Region Methods",
    "abstract": "Most of reinforcement learning algorithms optimize the discounted criterion\nwhich is beneficial to accelerate the convergence and reduce the variance of\nestimates. Although the discounted criterion is appropriate for certain tasks\nsuch as financial related problems, many engineering problems treat future\nrewards equally and prefer a long-run average criterion. In this paper, we\nstudy the reinforcement learning problem with the long-run average criterion.\nFirstly, we develop a unified trust region theory with discounted and average\ncriteria. With the average criterion, a novel performance bound within the\ntrust region is derived with the Perturbation Analysis (PA) theory. Secondly,\nwe propose a practical algorithm named Average Policy Optimization (APO), which\nimproves the value estimation with a novel technique named Average Value\nConstraint. To the best of our knowledge, our work is the first one to study\nthe trust region approach with the average criterion and it complements the\nframework of reinforcement learning beyond the discounted criterion. Finally,\nexperiments are conducted in the continuous control environment MuJoCo. In most\ntasks, APO performs better than the discounted PPO, which demonstrates the\neffectiveness of our approach.",
    "descriptor": "\nComments: Accepted by IJCAI2021\n",
    "authors": [
      "Xiaoteng Ma",
      "Xiaohang Tang",
      "Li Xia",
      "Jun Yang",
      "Qianchuan Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03442"
  },
  {
    "id": "arXiv:2106.03443",
    "title": "Causal Influence Detection for Improving Efficiency in Reinforcement  Learning",
    "abstract": "Many reinforcement learning (RL) environments consist of independent entities\nthat interact sparsely. In such environments, RL agents have only limited\ninfluence over other entities in any particular situation. Our idea in this\nwork is that learning can be efficiently guided by knowing when and what the\nagent can influence with its actions. To achieve this, we introduce a measure\nof situation-dependent causal influence based on conditional mutual information\nand show that it can reliably detect states of influence. We then propose\nseveral ways to integrate this measure into RL algorithms to improve\nexploration and off-policy learning. All modified algorithms show strong\nincreases in data efficiency on robotic manipulation tasks.",
    "descriptor": "",
    "authors": [
      "Maximilian Seitzer",
      "Bernhard Sch\u00f6lkopf",
      "Georg Martius"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03443"
  },
  {
    "id": "arXiv:2106.03450",
    "title": "supervised adptive threshold network for instance segmentation",
    "abstract": "Currently, instance segmentation is attracting more and more attention in\nmachine learning region. However, there exists some defects on the information\npropagation in previous Mask R-CNN and other network models. In this paper, we\npropose supervised adaptive threshold network for instance segmentation.\nSpecifically, we adopt the Mask R-CNN method based on adaptive threshold, and\nby establishing a layered adaptive network structure, it performs adaptive\nbinarization on the probability graph generated by Mask RCNN to obtain better\nsegmentation effect and reduce the error rate. At the same time, an adaptive\nfeature pool is designed to make the transmission between different layers of\nthe network more accurate and effective, reduce the loss in the process of\nfeature transmission, and further improve the mask method. Experiments on\nbenchmark data sets indicate that the effectiveness of the proposed model",
    "descriptor": "",
    "authors": [
      "Kuikun Liu",
      "Jie Yang",
      "Cai Sun",
      "Haoyuan Chi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03450"
  },
  {
    "id": "arXiv:2106.03452",
    "title": "Shape As Points: A Differentiable Poisson Solver",
    "abstract": "In recent years, neural implicit representations gained popularity in 3D\nreconstruction due to their expressiveness and flexibility. However, the\nimplicit nature of neural implicit representations results in slow inference\ntime and requires careful initialization. In this paper, we revisit the classic\nyet ubiquitous point cloud representation and introduce a differentiable\npoint-to-mesh layer using a differentiable formulation of Poisson Surface\nReconstruction (PSR) that allows for a GPU-accelerated fast solution of the\nindicator function given an oriented point cloud. The differentiable PSR layer\nallows us to efficiently and differentiably bridge the explicit 3D point\nrepresentation with the 3D mesh via the implicit indicator field, enabling\nend-to-end optimization of surface reconstruction metrics such as Chamfer\ndistance. This duality between points and meshes hence allows us to represent\nshapes as oriented point clouds, which are explicit, lightweight and\nexpressive. Compared to neural implicit representations, our Shape-As-Points\n(SAP) model is more interpretable, lightweight, and accelerates inference time\nby one order of magnitude. Compared to other explicit representations such as\npoints, patches, and meshes, SAP produces topology-agnostic, watertight\nmanifold surfaces. We demonstrate the effectiveness of SAP on the task of\nsurface reconstruction from unoriented point clouds and learning-based\nreconstruction.",
    "descriptor": "",
    "authors": [
      "Songyou Peng",
      "Chiyu \"Max\" Jiang",
      "Yiyi Liao",
      "Michael Niemeyer",
      "Marc Pollefeys",
      "Andreas Geiger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.03452"
  },
  {
    "id": "arXiv:2106.03456",
    "title": "Are best approximations really better than Chebyshev?",
    "abstract": "Best and Chebyshev approximations play an important role in approximation\ntheory. From the viewpoint of measuring approximation error in the maximum\nnorm, it is evident that best approximations are better than their Chebyshev\ncounterparts. However, the situation may be reversed if we compare the\napproximation quality from the viewpoint of either the rate of pointwise\nconvergence or the accuracy of spectral differentiation. We show that when the\nunderlying function has an algebraic singularity, the Chebyshev projection of\ndegree n converges one power of n faster than its best counterpart at each\npoint away from the singularity and both converge at the same rate at the\nsingularity. This gives a complete explanation for the phenomenon that the\naccuracy of Chebyshev projections is much better than that of best\napproximations except in a small neighborhood of the singularity. Extensions to\nsuperconvergence points and spectral differentiation, Chebyshev interpolants\nand other orthogonal projections are also discussed.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Haiyong Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03456"
  },
  {
    "id": "arXiv:2106.03457",
    "title": "Mechanism Design for Facility Location Problems: A Survey",
    "abstract": "The study of approximate mechanism design for facility location problems has\nbeen in the center of research at the intersection of artificial intelligence\nand economics for the last decades, largely due to its practical importance in\nvarious domains, such as social planning and clustering.At a high level, the\ngoal is to design mechanisms to select a set of locations on which to build a\nset of facilities, aiming to optimize some social objective and ensure\ndesirable properties based on the preferences of strategic agents, who might\nhave incentives to misreport their private information such as their locations.\nThis paper presents a comprehensive survey of the significant progress that has\nbeen made since the introduction of the problem, highlighting the different\nvariants and methodologies, as well as the most interesting directions for\nfuture research.",
    "descriptor": "\nComments: To appear in IJCAI 2021 (Survey Track)\n",
    "authors": [
      "Hau Chan",
      "Aris Filos-Ratsikas",
      "Bo Li",
      "Minming Li",
      "Chenhao Wang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03457"
  },
  {
    "id": "arXiv:2106.03458",
    "title": "A Grounded Theory of the Role of Coordination in Software Security Patch  Management",
    "abstract": "Several disastrous security attacks can be attributed to delays in patching\nsoftware vulnerabilities. While researchers and practitioners have paid\nsignificant attention to automate vulnerabilities identification and patch\ndevelopment activities of software security patch management, there has been\nrelatively little effort dedicated to gain an in-depth understanding of the\nsocio-technical aspects, e.g., coordination of interdependent activities of the\npatching process and patching decisions, that may cause delays in applying\nsecurity patches. We report on a Grounded Theory study of the role of\ncoordination in security patch management. The reported theory consists of four\ninter-related dimensions, i.e., causes, breakdowns, constraints, and\nmechanisms. The theory explains the causes that define the need for\ncoordination among interdependent software and hardware components and multiple\nstakeholders' decisions, the constraints that can negatively impact\ncoordination, the breakdowns in coordination, and the potential corrective\nmeasures. This study provides potentially useful insights for researchers and\npractitioners who can carefully consider the needs of and devise suitable\nsolutions for supporting the coordination of interdependencies involved in\nsecurity patch management.",
    "descriptor": "\nComments: Accepted for publication at the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE '21)\n",
    "authors": [
      "Nesara Dissanayake",
      "Mansooreh Zahedi",
      "Asangi Jayatilaka",
      "M. Ali Babar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03458"
  },
  {
    "id": "arXiv:2106.03459",
    "title": "Real-time Identification and Tuning of Multirotors Based on Deep Neural  Networks for Accurate Trajectory Tracking Under Wind Disturbances",
    "abstract": "High performance trajectory tracking for multirotor Unmanned Aerial Vehicles\n(UAVs) is a fast growing research area due to the increase in popularity and\ndemand. In many applications, the multirotor UAV dynamics would change\nin-flight resulting in performance degradation, or even instability, such that\nthe control system is required to adapt its parameters to the new dynamics. In\nthis paper, we developed a real-time identification approach based on Deep\nNeural Networks (DNNs) and the Modified Relay Feedback Test (MRFT) to optimally\ntune PID controllers suitable for aggressive trajectory tracking. We also\npropose a feedback linearization technique along with additional feedforward\nterms to achieve high trajectory tracking performance. In addition, we\ninvestigate and analyze different PID configurations for position controllers\nto maximize the tracking performance in the presence of wind disturbance and\nsystem parameter changes, and provide a systematic design methodology to\ntrade-off performance for robustness. We prove the effectiveness and\napplicability of our developed approach through a set of experiments where\naccurate trajectory tracking is maintained despite significant changes to the\nUAV aerodynamic characteristics and the application of external wind. We\ndemonstrate low discrepancy between simulation and experimental results which\nproves the potential of using the suggested approach for planning and fault\ndetection tasks. The achieved tracking results on figure-eight trajectory is on\npar with the state-of-the-art.",
    "descriptor": "",
    "authors": [
      "AbdulAziz Y. AlKayas",
      "Mohamad Chehadeh",
      "Abdulla Ayyad",
      "Yahya Zweiri"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03459"
  },
  {
    "id": "arXiv:2106.03461",
    "title": "Subject Independent Emotion Recognition using EEG Signals Employing  Attention Driven Neural Networks",
    "abstract": "Electroencephalogram (EEG) based emotional analysis has been employed in\nmedical science, security and human-computer interaction with good success. In\nthe recent past, deep learning-based approaches have significantly improved the\nclassification accuracy when compared to classical signal processing and\nmachine learning based frameworks. But most of them were subject-dependent\nstudies which were not able to generalize on the subject-independent tasks due\nto the inter-subject variability in EEG. In this work, a novel deep learning\nframework capable of doing subject-independent emotion recognition is\npresented, consisting of two parts. First, an unsupervised Long Short-Term\nMemory (LSTM) with channel-attention autoencoder is proposed for getting a\ncorrelated lower dimensional latent space representation of the EEG data for\neach subject. Secondly, a convolutional neural network (CNN) with attention\nframework, which takes the first component as input, is presented for\nperforming the task of subject-independent emotion recognition. With the\nattention mechanism, the proposed approach could highlight the channel of\ninterest as well as the temporal localization of the EEG signal, which\ncontributes to the emotion under consideration as validated by the results. The\nproposed approach has been validated using various widely employed datasets for\nEEG signals including DEAP dataset, SEED dataset and CHB-MIT dataset. With\nproposed methodology, average subject independent accuracies of 65.9%, 69.5%\nfor valence and arousal classification in the DEAP dataset, 76.7% for\npositive-negative classification in SEED dataset is obtained and further for\nthe CHB-MIT dataset average subject independent accuracies of 69.1%, 67.6%,\n72.3% for Pre-Ictal Vs Ictal, Inter-Ictal Vs Ictal, Pre-Ictal Vs Inter-Ictal\nclassification is obtained.",
    "descriptor": "\nComments: Under Review in IEEE Journal of Biomedical and Health Informatics\n",
    "authors": [
      "Arjun",
      "Aniket Singh Rajpoot",
      "Mahesh Raveendranatha Panicker"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03461"
  },
  {
    "id": "arXiv:2106.03462",
    "title": "SILVAN: Estimating Betweenness Centralities with Progressive Sampling  and Non-uniform Rademacher Bounds",
    "abstract": "Betweenness centrality is a popular centrality measure with applications in\nseveral domains, and whose exact computation is impractical for modern-sized\nnetworks. We present SILVAN, a novel, efficient algorithm to compute, with high\nprobability, accurate estimates of the betweenness centrality of all nodes of a\ngraph and a high-quality approximation of the k most central nodes of a graph.\nSILVAN follows a progressive sampling approach, and builds on recently improved\nbounds on Monte-Carlo Empirical Rademacher Averages, a fundamental tool from\nstatistical learning theory. SILVAN relies on a novel estimation scheme that\nleads to non-uniform bounds on the deviation of the estimates from the true\nvalues of the between centrality of all the nodes, providing tight guarantees\non the quality of the approximation. Our extensive experimental evaluation\nshows that SILVAN extracts high-quality approximations while outperforming, in\nterms of number of samples and accuracy, the state-of-the-art approximation\nalgorithm with comparable quality guarantees.",
    "descriptor": "",
    "authors": [
      "Leonardo Pellegrina",
      "Fabio Vandin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03462"
  },
  {
    "id": "arXiv:2106.03464",
    "title": "Learning stable reduced-order models for hybrid twins",
    "abstract": "The concept of Hybrid Twin (HT) has recently received a growing interest\nthanks to the availability of powerful machine learning techniques. This twin\nconcept combines physics-based models within a model-order reduction\nframework-to obtain real-time feedback rates-and data science. Thus, the main\nidea of the HT is to develop on-the-fly data-driven models to correct possible\ndeviations between measurements and physics-based model predictions. This paper\nis focused on the computation of stable, fast and accurate corrections in the\nHybrid Twin framework. Furthermore, regarding the delicate and important\nproblem of stability, a new approach is proposed, introducing several\nsub-variants and guaranteeing a low computational cost as well as the\nachievement of a stable time-integration.",
    "descriptor": "",
    "authors": [
      "Abel Sancarlos",
      "Morgan Cameron",
      "Jean-Marc Le Peuvedic",
      "Juliette Groulier",
      "Jean-Louis Duval",
      "Elias Cueto",
      "Francisco Chinesta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03464"
  },
  {
    "id": "arXiv:2106.03468",
    "title": "On Healthcare Robots: Concepts, definitions, and considerations for  healthcare robot governance",
    "abstract": "Although healthcare is a remarkably sensitive domain of application, and\nsystems that exert direct control over the world can cause harm in a way that\nhumans cannot necessarily correct or oversee, it is still unclear whether and\nhow healthcare robots are currently regulated or should be regulated. Existing\nregulations are primarily unprepared to provide guidance for such a rapidly\nevolving field and accommodate devices that rely on machine learning and AI.\nMoreover, the field of healthcare robotics is very rich and extensive, but it\nis still very much scattered and unclear in terms of definitions, medical and\ntechnical classifications, product characteristics, purpose, and intended use.\nAs a result, these devices often navigate between the medical device regulation\nor other non-medical norms, such as the ISO personal care standard. Before\nregulating the field of healthcare robots, it is therefore essential to map the\nmajor state-of-the-art developments in healthcare robotics, their capabilities\nand applications, and the challenges we face as a result of their integration\nwithin the healthcare environment.\nThis contribution fills in this gap and lack of clarity currently experienced\nwithin healthcare robotics and its governance by providing a structured\noverview of and further elaboration on the main categories now established,\ntheir intended purpose, use, and main characteristics. We explicitly focus on\nsurgical, assistive, and service robots to rightfully match the definition of\nhealthcare as the organized provision of medical care to individuals, including\nefforts to maintain, treat, or restore physical, mental, or emotional\nwell-being. We complement these findings with policy recommendations to help\npolicymakers unravel an optimal regulatory framing for healthcare robot\ntechnologies",
    "descriptor": "\nComments: 87 pages\n",
    "authors": [
      "Eduard Fosch-Villaronga",
      "Hadassah Drukarch"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03468"
  },
  {
    "id": "arXiv:2106.03469",
    "title": "Multilingual Neural Semantic Parsing for Low-Resourced Languages",
    "abstract": "Multilingual semantic parsing is a cost-effective method that allows a single\nmodel to understand different languages. However, researchers face a great\nimbalance of availability of training data, with English being resource rich,\nand other languages having much less data. To tackle the data limitation\nproblem, we propose using machine translation to bootstrap multilingual\ntraining data from the more abundant English data. To compensate for the data\nquality of machine translated training data, we utilize transfer learning from\npretrained multilingual encoders to further improve the model. To evaluate our\nmultilingual models on human-written sentences as opposed to machine translated\nones, we introduce a new multilingual semantic parsing dataset in English,\nItalian and Japanese based on the Facebook Task Oriented Parsing (TOP) dataset.\nWe show that joint multilingual training with pretrained encoders substantially\noutperforms our baselines on the TOP dataset and outperforms the\nstate-of-the-art model on the public NLMaps dataset. We also establish a new\nbaseline for zero-shot learning on the TOP dataset. We find that a semantic\nparser trained only on English data achieves a zero-shot performance of 44.9%\nexact-match accuracy on Italian sentences.",
    "descriptor": "\nComments: Accepted at *SEM2021\n",
    "authors": [
      "Menglin Xia",
      "Emilio Monti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03469"
  },
  {
    "id": "arXiv:2106.03470",
    "title": "Osiris: Automated Discovery of Microarchitectural Side Channels",
    "abstract": "In the last years, a series of side channels have been discovered on CPUs.\nThese side channels have been used in powerful attacks, e.g., on cryptographic\nimplementations, or as building blocks in transient-execution attacks such as\nSpectre or Meltdown. However, in many cases, discovering side channels is still\na tedious manual process.\nIn this paper, we present Osiris, a fuzzing-based framework to automatically\ndiscover microarchitectural side channels. Based on a machine-readable\nspecification of a CPU's ISA, Osiris generates instruction-sequence triples and\nautomatically tests whether they form a timing-based side channel. Furthermore,\nOsiris evaluates their usability as a side channel in transient-execution\nattacks, i.e., as the microarchitectural encoding for attacks like Spectre. In\ntotal, we discover four novel timing-based side channels on Intel and AMD CPUs.\nBased on these side channels, we demonstrate exploitation in three case\nstudies. We show that our microarchitectural KASLR break using non-temporal\nloads, FlushConflict, even works on the new Intel Ice Lake and Comet Lake\nmicroarchitectures. We present a cross-core cross-VM covert channel that is not\nrelying on the memory subsystem and transmits up to 1 kbit/s. We demonstrate\nthis channel on the AWS cloud, showing that it is stealthy and noise resistant.\nFinally, we demonstrate Stream+Reload, a covert channel for transient-execution\nattacks that, on average, allows leaking 7.83 bytes within a transient window,\nimproving state-of-the-art attacks that only leak up to 3 bytes.",
    "descriptor": "\nComments: Will be published at USENIX Security'21\n",
    "authors": [
      "Daniel Weber",
      "Ahmad Ibrahim",
      "Hamed Nemati",
      "Michael Schwarz",
      "Christian Rossow"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03470"
  },
  {
    "id": "arXiv:2106.03471",
    "title": "Relative Importance in Sentence Processing",
    "abstract": "Determining the relative importance of the elements in a sentence is a key\nfactor for effortless natural language understanding. For human language\nprocessing, we can approximate patterns of relative importance by measuring\nreading fixations using eye-tracking technology. In neural language models,\ngradient-based saliency methods indicate the relative importance of a token for\nthe target objective. In this work, we compare patterns of relative importance\nin English language processing by humans and models and analyze the underlying\nlinguistic patterns. We find that human processing patterns in English\ncorrelate strongly with saliency-based importance in language models and not\nwith attention-based importance. Our results indicate that saliency could be a\ncognitively more plausible metric for interpreting neural language models. The\ncode is available on GitHub: https://github.com/beinborn/relative_importance",
    "descriptor": "\nComments: accepted at ACL 2021\n",
    "authors": [
      "Nora Hollenstein",
      "Lisa Beinborn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03471"
  },
  {
    "id": "arXiv:2106.03476",
    "title": "Local Algorithms for Estimating Effective Resistance",
    "abstract": "Effective resistance is an important metric that measures the similarity of\ntwo vertices in a graph. It has found applications in graph clustering,\nrecommendation systems and network reliability, among others. In spite of the\nimportance of the effective resistances, we still lack efficient algorithms to\nexactly compute or approximate them on massive graphs.\nIn this work, we design several \\emph{local algorithms} for estimating\neffective resistances, which are algorithms that only read a small portion of\nthe input while still having provable performance guarantees. To illustrate,\nour main algorithm approximates the effective resistance between any vertex\npair $s,t$ with an arbitrarily small additive error $\\varepsilon$ in time\n$O(\\mathrm{poly}(\\log n/\\varepsilon))$, whenever the underlying graph has\nbounded mixing time. We perform an extensive empirical study on several\nbenchmark datasets, validating the performance of our algorithms.",
    "descriptor": "\nComments: KDD 2021\n",
    "authors": [
      "Pan Peng",
      "Daniel Lopatta",
      "Yuichi Yoshida",
      "Gramoz Goranci"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03476"
  },
  {
    "id": "arXiv:2106.03479",
    "title": "FINet: Dual Branches Feature Interaction for Partial-to-Partial Point  Cloud Registration",
    "abstract": "Data association is important in the point cloud registration. In this work,\nwe propose to solve the partial-to-partial registration from a new perspective,\nby introducing feature interactions between the source and the reference clouds\nat the feature extraction stage, such that the registration can be realized\nwithout the explicit mask estimation or attentions for the overlapping\ndetection as adopted previously. Specifically, we present FINet, a feature\ninteraction-based structure with the capability to enable and strengthen the\ninformation associating between the inputs at multiple stages. To achieve this,\nwe first split the features into two components, one for the rotation and one\nfor the translation, based on the fact that they belong to different solution\nspaces, yielding a dual branches structure. Second, we insert several\ninteraction modules at the feature extractor for the data association. Third,\nwe propose a transformation sensitivity loss to obtain rotation-attentive and\ntranslation-attentive features. Experiments demonstrate that our method\nperforms higher precision and robustness compared to the state-of-the-art\ntraditional and learning-based methods.",
    "descriptor": "",
    "authors": [
      "Hao Xu",
      "Nianjin Ye",
      "Shuaicheng Liu",
      "Guanghui Liu",
      "Bing Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03479"
  },
  {
    "id": "arXiv:2106.03484",
    "title": "BERTGEN: Multi-task Generation through BERT",
    "abstract": "We present BERTGEN, a novel generative, decoder-only model which extends BERT\nby fusing multimodal and multilingual pretrained models VL-BERT and M-BERT,\nrespectively. BERTGEN is auto-regressively trained for language generation\ntasks, namely image captioning, machine translation and multimodal machine\ntranslation, under a multitask setting. With a comprehensive set of\nevaluations, we show that BERTGEN outperforms many strong baselines across the\ntasks explored. We also show BERTGEN's ability for zero-shot language\ngeneration, where it exhibits competitive performance to supervised\ncounterparts. Finally, we conduct ablation studies which demonstrate that\nBERTGEN substantially benefits from multi-tasking and effectively transfers\nrelevant inductive biases from the pre-trained models.",
    "descriptor": "\nComments: Accepted to ACL 2021 Main Conference\n",
    "authors": [
      "Faidon Mitzalis",
      "Ozan Caglayan",
      "Pranava Madhyastha",
      "Lucia Specia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03484"
  },
  {
    "id": "arXiv:2106.03486",
    "title": "A new variational formulation with high order impedance boundary  condition for the scattering problem in electromagnetism",
    "abstract": "In this paper, we propose some variational formulations with the use of high\norder impedance boundary condition (HOIBC) to solve the scattering problem. We\nstudy the existence and uniqueness of the solution. Then, a discretization of\nthese formulations is done. We give validations of the HOIBC obtained with a\nMoM code that show the improvement in accuracy over the standard impedance\nboundary condition (SIBC) computations.",
    "descriptor": "\nComments: 37 pages, 21 figures\n",
    "authors": [
      "Soumaya Oueslati",
      "Christian Daveau",
      "Abil Aubakirov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03486"
  },
  {
    "id": "arXiv:2106.03487",
    "title": "Exploiting Emotional Dependencies with Graph Convolutional Networks for  Facial Expression Recognition",
    "abstract": "Over the past few years, deep learning methods have shown remarkable results\nin many face-related tasks including automatic facial expression recognition\n(FER) in-the-wild. Meanwhile, numerous models describing the human emotional\nstates have been proposed by the psychology community. However, we have no\nclear evidence as to which representation is more appropriate and the majority\nof FER systems use either the categorical or the dimensional model of affect.\nInspired by recent work in multi-label classification, this paper proposes a\nnovel multi-task learning (MTL) framework that exploits the dependencies\nbetween these two models using a Graph Convolutional Network (GCN) to recognize\nfacial expressions in-the-wild. Specifically, a shared feature representation\nis learned for both discrete and continuous recognition in a MTL setting.\nMoreover, the facial expression classifiers and the valence-arousal regressors\nare learned through a GCN that explicitly captures the dependencies between\nthem. To evaluate the performance of our method under real-world conditions we\ntrain our models on AffectNet dataset. The results of our experiments show that\nour method outperforms the current state-of-the-art methods on discrete FER.",
    "descriptor": "\nComments: 9 pages, 8 figures, 3 tables, submitted to the 16th IEEE International Conference on Automatic Face and Gesture Recognition\n",
    "authors": [
      "Panagiotis Antoniadis",
      "Panagiotis P. Filntisis",
      "Petros Maragos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03487"
  },
  {
    "id": "arXiv:2106.03489",
    "title": "Conditionally Exponential Prior in Focal Near- and Far-Field EEG Source  Localization via Randomized Multiresolution Scanning (RAMUS)",
    "abstract": "This paper develops mathematical methods for localizing focal sources at\ndifferent depths based on the non-invasive electro-/magnetoencephalography\nmeasurements. In the context of hierarchical Bayesian modelling, we introduce a\nconditionally exponential prior (CEP) which extends the concept of the\nconditionally Gaussian prior (CGP) and has been proposed to be advantageous in\nreconstructing far-field activity, in particular, when coupled with randomized\nmultiresolution scanning (RAMUS). An approach to obtain the shape and scale\nparameter of the gamma hyperprior steering the CEP is derived from the\nphysiological a priori knowledge of the brain activity. The core concept of\nthis study is to show that the first-degree CEP will yield and improve the\nfocality compared to the second-order case. The results of the present\nnumerical experiments suggest that sources reconstructed via a combination of\nthe first-degree CEP and RAMUS achieve an accuracy comparable to the\nsecond-degree case while being more focal for numerically simulated originators\nof human somatosensory evoked potentials (SEPs) related to human median nerve\nstimulation, including simultaneous thalamic and cortical activity, as well as\nfor a sub-thalamic dipolar and quadrupolar source configuration.",
    "descriptor": "\nComments: 19 pages, 12 figures, submitted to Journal of Mathematical Imaging and Vision\n",
    "authors": [
      "Joonas Lahtinen",
      "Alexandra Koulouri",
      "Atena Rezaei",
      "Sampsa Pursiainen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03489"
  },
  {
    "id": "arXiv:2106.03492",
    "title": "Empirical Bayesian Independent Deeply Learned Matrix Analysis For  Multichannel Audio Source Separation",
    "abstract": "Independent deeply learned matrix analysis (IDLMA) is one of the\nstate-of-the-art supervised multichannel audio source separation methods. It\nblindly estimates the demixing filters on the basis of source independence,\nusing the source model estimated by the deep neural network (DNN). However,\nsince the ratios of the source to interferer signals vary widely among\ntime-frequency (TF) slots, it is difficult to obtain reliable estimated power\nspectrograms of sources at all TF slots. In this paper, we propose an IDLMA\nextension, empirical Bayesian IDLMA (EB-IDLMA), by introducing a prior\ndistribution of source power spectrograms and treating the source power\nspectrograms as latent random variables. This treatment allows us to implicitly\nconsider the reliability of the estimated source power spectrograms for the\nestimation of demixing filters through the hyperparameters of the prior\ndistribution estimated by the DNN. Experimental evaluations show the\neffectiveness of EB-IDLMA and the importance of introducing the reliability of\nthe estimated source power spectrograms.",
    "descriptor": "\nComments: 5 pages, 4 figures, accepted for European Signal Processing Conference 2021 (EUSIPCO 2021)\n",
    "authors": [
      "Takuya Hasumi",
      "Tomohiko Nakamura",
      "Norihiro Takamune",
      "Hiroshi Saruwatari",
      "Daichi Kitamura",
      "Yu Takahashi",
      "Kazunobu Kondo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03492"
  },
  {
    "id": "arXiv:2106.03496",
    "title": "Self-Supervision & Meta-Learning for One-Shot Unsupervised Cross-Domain  Detection",
    "abstract": "Deep detection models have largely demonstrated to be extremely powerful in\ncontrolled settings, but appear brittle and fail when applied off-the-shelf on\nunseen domains. All the adaptive approaches developed to amend this issue\naccess a sizable amount of target samples at training time, a strategy not\nsuitable when the target is unknown and its data are not available in advance.\nConsider for instance the task of monitoring image feeds from social media: as\nevery image is uploaded by a different user it belongs to a different target\ndomain that is impossible to foresee during training. Our work addresses this\nsetting, presenting an object detection algorithm able to perform unsupervised\nadaptation across domains by using only one target sample, seen at test time.\nWe introduce a multi-task architecture that one-shot adapts to any incoming\nsample by iteratively solving a self-supervised task on it. We further exploit\nmeta-learning to simulate single-sample cross domain learning episodes and\nbetter align to the test condition. Moreover, a cross-task pseudo-labeling\nprocedure allows to focus on the image foreground and enhances the adaptation\nprocess. A thorough benchmark analysis against the most recent cross-domain\ndetection methods and a detailed ablation study show the advantage of our\napproach.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2005.11610\n",
    "authors": [
      "F. Cappio Borlino",
      "S. Polizzotto",
      "A. D'Innocente",
      "S. Bucci",
      "B. Caputo",
      "T. Tommasi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03496"
  },
  {
    "id": "arXiv:2106.03498",
    "title": "Identifiability in inverse reinforcement learning",
    "abstract": "Inverse reinforcement learning attempts to reconstruct the reward function in\na Markov decision problem, using observations of agent actions. As already\nobserved by Russell the problem is ill-posed, and the reward function is not\nidentifiable, even under the presence of perfect information about optimal\nbehavior. We provide a resolution to this non-identifiability for problems with\nentropy regularization. For a given environment, we fully characterize the\nreward functions leading to a given policy and demonstrate that, given\ndemonstrations of actions for the same reward under two distinct discount\nfactors, or under sufficiently different environments, the unobserved reward\ncan be recovered up to a constant. Through a simple numerical experiment, we\ndemonstrate the accurate reconstruction of the reward function through our\nproposed resolution.",
    "descriptor": "",
    "authors": [
      "Haoyang Cao",
      "Samuel N. Cohen",
      "Lukasz Szpruch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03498"
  },
  {
    "id": "arXiv:2106.03500",
    "title": "Multi-chart flows",
    "abstract": "We present Multi-chart flows, a flow-based model for concurrently learning\ntopologically non-trivial manifolds and statistical densities on them. Current\nmethods focus on manifolds that are topologically Euclidean, enforce strong\nstructural priors on the learned models or use operations that do not scale to\nhigh dimensions. In contrast, our model learns the local manifold topology\npiecewise by \"gluing\" it back together through a collection of learned\ncoordinate charts. We demonstrate the efficiency of our approach on synthetic\ndata of known manifolds, as well as higher dimensional manifolds of unknown\ntopology, where we show better sample efficiency and competitive or superior\nperformance against current state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Dimitris Kalatzis",
      "Johan Ziruo Ye",
      "Jesper Wohlert",
      "S\u00f8ren Hauberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03500"
  },
  {
    "id": "arXiv:2106.03501",
    "title": "Set-Estimation based Networked Model Predictive Control for Energy  Management of Faulty Microgrids",
    "abstract": "This paper addresses the issue of power flow control for partially faulty\nmicrogrids. In microgrid control systems, faults may occur in both electrical\nand communication layers. This may have severe effects on the operation of\nmicrogrids. In addition, disturbances always coexist with faults in microgrids,\nwhich may further deteriorate system performance. To address the faults and\ndisturbances simultaneously, a model predictive control (MPC) method based on\nset-membership estimation (SME) that transmits information via a communication\nnetwork is proposed. When electrical devices are nonfunctional or communication\nfailures occur, the corresponding system states will become unavailable. To\nthis end, the SME method is employed to estimate the states with the existence\nof unknown-but-bounded process and measurement disturbances. The networked MPC\nmethod is designed to schedule the power dispatch by using the forecasts of\nphotovoltaic (PV) generation and load demand. With these two methods, the\nfault-tolerant control can be achieved. Further, a deviation compensation\nmethod is proposed to compensate for the forecast errors. The effectiveness of\nthe proposed control strategy is demonstrated through wireless communication\ntests using Raspberry Pis.",
    "descriptor": "\nComments: 11 pages, 12 figures\n",
    "authors": [
      "Quanwei Qiu",
      "Fuwen Yang",
      "Yong Zhu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03501"
  },
  {
    "id": "arXiv:2106.03502",
    "title": "Efficient training for future video generation based on hierarchical  disentangled representation of latent variables",
    "abstract": "Generating videos predicting the future of a given sequence has been an area\nof active research in recent years. However, an essential problem remains\nunsolved: most of the methods require large computational cost and memory usage\nfor training. In this paper, we propose a novel method for generating future\nprediction videos with less memory usage than the conventional methods. This is\na critical stepping stone in the path towards generating videos with high image\nquality, similar to that of generated images in the latest works in the field\nof image generation. We achieve high-efficiency by training our method in two\nstages: (1) image reconstruction to encode video frames into latent variables,\nand (2) latent variable prediction to generate the future sequence. Our method\nachieves an efficient compression of video into low-dimensional latent\nvariables by decomposing each frame according to its hierarchical structure.\nThat is, we consider that video can be separated into background and foreground\nobjects, and that each object holds time-varying and time-independent\ninformation independently. Our experiments show that the proposed method can\nefficiently generate future prediction videos, even for complex datasets that\ncannot be handled by previous methods.",
    "descriptor": "",
    "authors": [
      "Naoya Fushishita",
      "Antonio Tejero-de-Pablos",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03502"
  },
  {
    "id": "arXiv:2106.03503",
    "title": "The Distance Transform and its Computation",
    "abstract": "Distance transformation is an image processing technique used for many\ndifferent applications. Related to a binary image, the general idea is to\ndetermine the distance of all background points to the nearest object point (or\nvice versa). In this tutorial, different approaches are explained in detail and\ncompared using examples. Corresponding source code is provided to facilitate\nown investigations. A particular objective of this tutorial is to clarify the\ndifference between arbitrary distance transforms and exact Euclidean distance\ntransformations.",
    "descriptor": "\nComments: 24 pages, 22 figures, 1 table, 9 listings\n",
    "authors": [
      "Tilo Strutz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.03503"
  },
  {
    "id": "arXiv:2106.03504",
    "title": "Boosting 5G mm-Wave IAB Reliability with Reconfigurable Intelligent  Surfaces",
    "abstract": "The introduction of the mm-Wave spectrum into 5G NR promises to bring about\nunprecedented data throughput to future mobile wireless networks, but comes\nwith several challenges. Network densification has been proposed as a viable\nsolution to increase RAN resilience and newly introduced IAB is considered as a\nkey enabling technology with compelling cost-reducing opportunities for such\ndense deployments. Reconfigurable Intelligent Surfaces (RISes) have recently\ngained extreme popularity as they can create Smart Radio Environments by EM\nwave manipulation. Recent studies have shown how this technology can behave as\ninexpensive passive relays. However, it is not yet clear what role this\ntechnology can play in a large RAN deployment. With the scope of filling this\ngap, we propose a new mm-Wave IAB planning tool where RISes can be installed\nalongside base stations to maximize the network resilience against blockages\ndue to nomadic obstacles and human self-blocking. Numerical results show how\nadding RISes to IAB deployments can provide high levels of blockage resistance,\nwhile they also significantly reduce the overall network planning cost.",
    "descriptor": "",
    "authors": [
      "Paolo Fiore",
      "Eugenio Moro",
      "Ilario Filippini",
      "Antonio Capone",
      "Danilo De Donno"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03504"
  },
  {
    "id": "arXiv:2106.03505",
    "title": "Self-supervised Depth Estimation Leveraging Global Perception and  Geometric Smoothness Using On-board Videos",
    "abstract": "Self-supervised depth estimation has drawn much attention in recent years as\nit does not require labeled data but image sequences. Moreover, it can be\nconveniently used in various applications, such as autonomous driving,\nrobotics, realistic navigation, and smart cities. However, extracting global\ncontextual information from images and predicting a geometrically natural depth\nmap remain challenging. In this paper, we present DLNet for pixel-wise depth\nestimation, which simultaneously extracts global and local features with the\naid of our depth Linformer block. This block consists of the Linformer and\ninnovative soft split multi-layer perceptron blocks. Moreover, a\nthree-dimensional geometry smoothness loss is proposed to predict a\ngeometrically natural depth map by imposing the second-order smoothness\nconstraint on the predicted three-dimensional point clouds, thereby realizing\nimproved performance as a byproduct. Finally, we explore the multi-scale\nprediction strategy and propose the maximum margin dual-scale prediction\nstrategy for further performance improvement. In experiments on the KITTI and\nMake3D benchmarks, the proposed DLNet achieves performance competitive to those\nof the state-of-the-art methods, reducing time and space complexities by more\nthan $62\\%$ and $56\\%$, respectively. Extensive testing on various real-world\nsituations further demonstrates the strong practicality and generalization\ncapability of the proposed model.",
    "descriptor": "",
    "authors": [
      "Shaocheng Jia",
      "Xin Pei",
      "Wei Yao",
      "S.C. Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03505"
  },
  {
    "id": "arXiv:2106.03514",
    "title": "Baseline Skinning for Point Sets of Articulated Bodies",
    "abstract": "General skinning techniques aim to deform the surface of an articulated model\nfollowing the pose change of a skeleton. Their rapidity makes them ideal tools\nfor real-time animation purposes. However, popular skinning algorithms are\nsimple, but they tend to generate undesirable geometric artefacts. In our work,\nwe consider skeletons given in the form of sphere-mesh models controlling both\nthe pose and morphology of the shape that is either described as a mesh or a\nraw point set. We propose a novel skinning method that encodes the point set\ndetails above a bundle of baselines covering the sphere-mesh. In particular, we\npropose a geometrical model of the baseline and detail direction evolution\nduring bone twisting and joints bending rotations. Our approach works directly\non point sets and thus preserves the accuracy of the initial sampling. It\nfurther avoids computing a weight per point or a costly explicit muscle\nmodelling step. We evaluate our method on several articulated body point sets,\nshowing that it creates fewer artefacts than classical methods.",
    "descriptor": "",
    "authors": [
      "Tong Fu",
      "Rapha\u00eblle Chaine",
      "Julie Digne"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.03514"
  },
  {
    "id": "arXiv:2106.03517",
    "title": "Top-KAST: Top-K Always Sparse Training",
    "abstract": "Sparse neural networks are becoming increasingly important as the field seeks\nto improve the performance of existing models by scaling them up, while\nsimultaneously trying to reduce power consumption and computational footprint.\nUnfortunately, most existing methods for inducing performant sparse models\nstill entail the instantiation of dense parameters, or dense gradients in the\nbackward-pass, during training. For very large models this requirement can be\nprohibitive. In this work we propose Top-KAST, a method that preserves constant\nsparsity throughout training (in both the forward and backward-passes). We\ndemonstrate the efficacy of our approach by showing that it performs comparably\nto or better than previous works when training models on the established\nImageNet benchmark, whilst fully maintaining sparsity. In addition to our\nImageNet results, we also demonstrate our approach in the domain of language\nmodeling where the current best performing architectures tend to have tens of\nbillions of parameters and scaling up does not yet seem to have saturated\nperformance. Sparse versions of these architectures can be run with\nsignificantly fewer resources, making them more widely accessible and\napplicable. Furthermore, in addition to being effective, our approach is\nstraightforward and can easily be implemented in a wide range of existing\nmachine learning frameworks with only a few additional lines of code. We\ntherefore hope that our contribution will help enable the broader community to\nexplore the potential held by massive models, without incurring massive\ncomputational cost.",
    "descriptor": "",
    "authors": [
      "Siddhant M. Jayakumar",
      "Razvan Pascanu",
      "Jack W. Rae",
      "Simon Osindero",
      "Erich Elsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03517"
  },
  {
    "id": "arXiv:2106.03518",
    "title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for EmotionCause  Extraction",
    "abstract": "The Emotion Cause Extraction (ECE)} task aims to identify clauses which\ncontain emotion-evoking information for a particular emotion expressed in text.\nWe observe that a widely-used ECE dataset exhibits a bias that the majority of\nannotated cause clauses are either directly before their associated emotion\nclauses or are the emotion clauses themselves. Existing models for ECE tend to\nexplore such relative position information and suffer from the dataset bias. To\ninvestigate the degree of reliance of existing ECE models on clause relative\npositions, we propose a novel strategy to generate adversarial examples in\nwhich the relative position information is no longer the indicative feature of\ncause clauses. We test the performance of existing models on such adversarial\nexamples and observe a significant performance drop. To address the dataset\nbias, we propose a novel graph-based method to explicitly model the emotion\ntriggering paths by leveraging the commonsense knowledge to enhance the\nsemantic dependencies between a candidate clause and an emotion clause.\nExperimental results show that our proposed approach performs on par with the\nexisting state-of-the-art methods on the original ECE dataset, and is more\nrobust against adversarial attacks compared to existing models.",
    "descriptor": "\nComments: ACL2021 Main Conference Long paper\n",
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Gabriele Pergola",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03518"
  },
  {
    "id": "arXiv:2106.03521",
    "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of  Conversational Language Models",
    "abstract": "Text representation models are prone to exhibit a range of societal biases,\nreflecting the non-controlled and biased nature of the underlying pretraining\ndata, which consequently leads to severe ethical issues and even bias\namplification. Recent work has predominantly focused on measuring and\nmitigating bias in pretrained language models. Surprisingly, the landscape of\nbias measurements and mitigation resources and methods for conversational\nlanguage models is still very scarce: it is limited to only a few types of\nbias, artificially constructed resources, and completely ignores the impact\nthat debiasing methods may have on the final performance in dialog tasks, e.g.,\nconversational response generation. In this work, we present RedditBias, the\nfirst conversational data set grounded in the actual human conversations from\nReddit, allowing for bias measurement and mitigation across four important bias\ndimensions: gender, race, religion, and queerness. Further, we develop an\nevaluation framework which simultaneously 1) measures bias on the developed\nRedditBias resource, and 2) evaluates model capability in dialog tasks after\nmodel debiasing. We use the evaluation framework to benchmark the widely used\nconversational DialoGPT model along with the adaptations of four debiasing\nmethods. Our results indicate that DialoGPT is biased with respect to religious\ngroups and that some debiasing techniques can remove this bias while preserving\ndownstream task performance.",
    "descriptor": "\nComments: Accepted for ACL21\n",
    "authors": [
      "Soumya Barikeri",
      "Anne Lauscher",
      "Ivan Vuli\u0107",
      "Goran Glava\u0161"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03521"
  },
  {
    "id": "arXiv:2106.03524",
    "title": "Smoothness-Aware Quantization Techniques",
    "abstract": "Distributed machine learning has become an indispensable tool for training\nlarge supervised machine learning models. To address the high communication\ncosts of distributed training, which is further exacerbated by the fact that\nmodern highly performing models are typically overparameterized, a large body\nof work has been devoted in recent years to the design of various compression\nstrategies, such as sparsification and quantization, and optimization\nalgorithms capable of using them. Recently, Safaryan et al (2021) pioneered a\ndramatically different compression design approach: they first use the local\ntraining data to form local {\\em smoothness matrices}, and then propose to\ndesign a compressor capable of exploiting the smoothness information contained\ntherein. While this novel approach leads to substantial savings in\ncommunication, it is limited to sparsification as it crucially depends on the\nlinearity of the compression operator. In this work, we resolve this problem by\nextending their smoothness-aware compression strategy to arbitrary unbiased\ncompression operators, which also includes sparsification. Specializing our\nresults to quantization, we observe significant savings in communication\ncomplexity compared to standard quantization. In particular, we show\ntheoretically that block quantization with $n$ blocks outperforms single block\nquantization, leading to a reduction in communication complexity by an\n$\\mathcal{O}(n)$ factor, where $n$ is the number of nodes in the distributed\nsystem. Finally, we provide extensive numerical evidence that our\nsmoothness-aware quantization strategies outperform existing quantization\nschemes as well the aforementioned smoothness-aware sparsification strategies\nwith respect to all relevant success measures: the number of iterations, the\ntotal amount of bits communicated, and wall-clock time.",
    "descriptor": "\nComments: 19+19 pages, 3 tables, 8 figures\n",
    "authors": [
      "Bokun Wang",
      "Mher Safaryan",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03524"
  },
  {
    "id": "arXiv:2106.03527",
    "title": "Multi-Exit Semantic Segmentation Networks",
    "abstract": "Semantic segmentation arises as the backbone of many vision systems, spanning\nfrom self-driving cars and robot navigation to augmented reality and\nteleconferencing. Frequently operating under stringent latency constraints\nwithin a limited resource envelope, optimising for efficient execution becomes\nimportant. To this end, we propose a framework for converting state-of-the-art\nsegmentation models to MESS networks; specially trained CNNs that employ\nparametrised early exits along their depth to save computation during inference\non easier samples. Designing and training such networks naively can hurt\nperformance. Thus, we propose a two-staged training process that pushes\nsemantically important features early in the network. We co-optimise the\nnumber, placement and architecture of the attached segmentation heads, along\nwith the exit policy, to adapt to the device capabilities and\napplication-specific requirements. Optimising for speed, MESS networks can\nachieve latency gains of up to 2.83x over state-of-the-art methods with no\naccuracy degradation. Accordingly, optimising for accuracy, we achieve an\nimprovement of up to 5.33 pp, under the same computational budget.",
    "descriptor": "",
    "authors": [
      "Alexandros Kouris",
      "Stylianos I. Venieris",
      "Stefanos Laskaridis",
      "Nicholas D. Lane"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03527"
  },
  {
    "id": "arXiv:2106.03530",
    "title": "CAiRE in DialDoc21: Data Augmentation for Information-Seeking Dialogue  System",
    "abstract": "Information-seeking dialogue systems, including knowledge identification and\nresponse generation, aim to respond to users with fluent, coherent, and\ninformative responses based on users' needs, which. To tackle this challenge,\nwe utilize data augmentation methods and several training techniques with the\npre-trained language models to learn a general pattern of the task and thus\nachieve promising performance. In DialDoc21 competition, our system achieved\n74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU\nscore in subtask 2. Empirical analysis is provided to explain the effectiveness\nof our approaches.",
    "descriptor": "\nComments: Accepted in DialDoc21 Workshop in ACL 2021\n",
    "authors": [
      "Etsuko Ishii",
      "Yan Xu",
      "Genta Indra Winata",
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Zihan Liu",
      "Peng Xu",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03530"
  },
  {
    "id": "arXiv:2106.03532",
    "title": "SizeFlags: Reducing Size and Fit Related Returns in Fashion E-Commerce",
    "abstract": "E-commerce is growing at an unprecedented rate and the fashion industry has\nrecently witnessed a noticeable shift in customers' order behaviour towards\nstronger online shopping. However, fashion articles ordered online do not\nalways find their way to a customer's wardrobe. In fact, a large share of them\nend up being returned. Finding clothes that fit online is very challenging and\naccounts for one of the main drivers of increased return rates in fashion\ne-commerce. Size and fit related returns severely impact 1. the customers\nexperience and their dissatisfaction with online shopping, 2. the environment\nthrough an increased carbon footprint, and 3. the profitability of online\nfashion platforms. Due to poor fit, customers often end up returning articles\nthat they like but do not fit them, which they have to re-order in a different\nsize. To tackle this issue we introduce SizeFlags, a probabilistic Bayesian\nmodel based on weakly annotated large-scale data from customers. Leveraging the\nadvantages of the Bayesian framework, we extend our model to successfully\nintegrate rich priors from human experts feedback and computer vision\nintelligence. Through extensive experimentation, large-scale A/B testing and\ncontinuous evaluation of the model in production, we demonstrate the strong\nimpact of the proposed approach in robustly reducing size-related returns in\nonline fashion over 14 countries.",
    "descriptor": "\nComments: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n",
    "authors": [
      "Andrea Nestler",
      "Nour Karessli",
      "Karl Hajjar",
      "Rodrigo Weffer",
      "Reza Shirvany"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03532"
  },
  {
    "id": "arXiv:2106.03535",
    "title": "Graph Neural Networks in Network Neuroscience",
    "abstract": "Noninvasive medical neuroimaging has yielded many discoveries about the brain\nconnectivity. Several substantial techniques mapping morphological, structural\nand functional brain connectivities were developed to create a comprehensive\nroad map of neuronal activities in the human brain -namely brain graph. Relying\non its non-Euclidean data type, graph neural network (GNN) provides a clever\nway of learning the deep graph structure and it is rapidly becoming the\nstate-of-the-art leading to enhanced performance in various network\nneuroscience tasks. Here we review current GNN-based methods, highlighting the\nways that they have been used in several applications related to brain graphs\nsuch as missing brain graph synthesis and disease classification. We conclude\nby charting a path toward a better application of GNN models in network\nneuroscience field for neurological disorder diagnosis and population graph\nintegration. The list of papers cited in our work is available at\nhttps://github.com/basiralab/GNNs-in-Network-Neuroscience.",
    "descriptor": "",
    "authors": [
      "Alaa Bessadok",
      "Mohamed Ali Mahjoub",
      "Islem Rekik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.03535"
  },
  {
    "id": "arXiv:2106.03536",
    "title": "Performance assessment of Synchronous Condensers vs Voltage Source  Converters providing grid-forming functions",
    "abstract": "Having sufficient grid-forming sources is one of the necessary conditions to\nguarantee the stability in a power system hosting a very large share of\ninverter-based generation. The grid-forming function has been historically\nfulfilled by synchronous machines. However, with the appropriate control, it\ncan also be provided by voltage source converters (VSC). This work presents a\ncomparison between two technologies with grid-forming capability: the VSC with\na grid-forming control coupled with an adequate energy storage system, and the\nsynchronous condensers (SC). Both devices are compared regarding their inertial\nresponse, as well as their contribution to the system strength and\nshort-circuit current for an equivalent capacity expressed in terms of apparent\npower and inertial reserve. Their behaviour following grid disturbances is\nassessed through time-domain simulations based on detailed electromagnetic\ntransient (EMT) models. The results show that both devices achieve similar\nperformance in the time-scale of seconds. For shorter time-windows, however,\nthey present a different behavior: the SC ensures a better stiffness in the\nfirst tens of ms following the disturbance, while the VSC offers a faster\nresynchronization.",
    "descriptor": "\nComments: 6 pages, 5 figures, PowerTech 2021 conference\n",
    "authors": [
      "Dorsan Lepour",
      "Mario Paolone",
      "Guillaume Denis",
      "Carmen Cardozo",
      "Thibault Prevost",
      "Emeline Guiu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03536"
  },
  {
    "id": "arXiv:2106.03538",
    "title": "End-to-end reconstruction meets data-driven regularization for inverse  problems",
    "abstract": "We propose an unsupervised approach for learning end-to-end reconstruction\noperators for ill-posed inverse problems. The proposed method combines the\nclassical variational framework with iterative unrolling, which essentially\nseeks to minimize a weighted combination of the expected distortion in the\nmeasurement space and the Wasserstein-1 distance between the distributions of\nthe reconstruction and ground-truth. More specifically, the regularizer in the\nvariational setting is parametrized by a deep neural network and learned\nsimultaneously with the unrolled reconstruction operator. The variational\nproblem is then initialized with the reconstruction of the unrolled operator\nand solved iteratively till convergence. Notably, it takes significantly fewer\niterations to converge, thanks to the excellent initialization obtained via the\nunrolled operator. The resulting approach combines the computational efficiency\nof end-to-end unrolled reconstruction with the well-posedness and\nnoise-stability guarantees of the variational setting. Moreover, we demonstrate\nwith the example of X-ray computed tomography (CT) that our approach\noutperforms state-of-the-art unsupervised methods, and that it outperforms or\nis on par with state-of-the-art supervised learned reconstruction approaches.",
    "descriptor": "",
    "authors": [
      "Subhadip Mukherjee",
      "Marcello Carioni",
      "Ozan \u00d6ktem",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03538"
  },
  {
    "id": "arXiv:2106.03539",
    "title": "How to Bake Quantum into Your Pet Petri Nets and Have Your Net Theory  Too",
    "abstract": "Petri nets have found widespread use among many application domains, not\nleast due to their human-friendly graphical syntax for the composition of\ninteracting distributed and asynchronous processes and services, based in\npartial-order dependencies and concurrent executions. Petri nets also come with\nabstract semantics, and mathematical methods for compositional synthesis,\nstructural checks and behavioural analysis. These have led to the use of\nvarious kinds of nets for real-time, distributed and parallel programming\nlanguages, software and services systems, with a view to their interfaces and\ninteraction protocols. These affordances make Petri nets invaluable for\ndistributed software architecture approaches focused on components, their\nmutual dependencies and environment-facing interactions. Quantum computing --\nand in particular quantum software engineering -- is in its infancy and could\nbenefit from the accumulated insights of software architecture research and of\nnet theory, its methods, and its applications. In this paper, we establish a\nconnection between Petri nets and quantum systems, such that net theory and the\ncomponent architecture of nets may help in the synthesis and analysis of\nabstract software models and their interface protocols in hybrid\nclassical-and-quantum programming languages and services systems. We leverage\nsome insights from net formalisms for software specification for a versatile\nrecipe to bake quantum into extant Petri net flavours, and prove universality\nand compositionality of Petri nets for quantum programming.",
    "descriptor": "\nComments: 24 pages incl. supplementary material in appendix. Accepted for the 15th Symposium and Summer School On Service-Oriented Computing (Submitted 2 April 2021, this https URL). Final revised and authenticated version to appear in Springer CCIS (this https URL)\n",
    "authors": [
      "Heinz W. Schmidt"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.03539"
  },
  {
    "id": "arXiv:2106.03540",
    "title": "Explicit numerical approximation for logistic models with regime  switching in finite and infinite horizons",
    "abstract": "The stochastic logistic model with regime switching is an important model in\nthe ecosystem. While analytic solution to this model is positive, current\nnumerical methods are unable to preserve such boundaries in the approximation.\nSo, proposing appropriate numerical method for solving this model which\npreserves positivity and dynamical behaviors of the model's solution is very\nimportant. In this paper, we present a positivity preserving truncated\nEuler-Maruyama scheme for this model, which taking advantages of being explicit\nand easily implementable. Without additional restriction conditions, strong\nconvergence of the numerical algorithm is studied, and 1/2 order convergence\nrate is obtained. In the particular case of this model without switching the\nfirst order strong convergence rate is obtained. Furthermore, the approximation\nof long-time dynamical properties is realized, including the stochastic\npermanence, extinctive and stability in distribution. Some simulations and\nexamples are provided to confirm the theoretical results and demonstrate the\nvalidity of the approach.",
    "descriptor": "\nComments: 62 pages, 12 figures\n",
    "authors": [
      "Xiaoyue Li",
      "Hongfu Yang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.03540"
  },
  {
    "id": "arXiv:2106.03541",
    "title": "Multi-agent Battery Storage Management using MPC-based Reinforcement  Learning",
    "abstract": "In this paper, we present the use of Model Predictive Control (MPC) based on\nReinforcement Learning (RL) to find the optimal policy for a multi-agent\nbattery storage system. A time-varying prediction of the power price and\nproduction-demand uncertainty are considered. We focus on optimizing an\neconomic objective cost while avoiding very low or very high state of charge,\nwhich can damage the battery. We consider the bounded power provided by the\nmain grid and the constraints on the power input and state of each agent. A\nparametrized MPC-scheme is used as a function approximator for the\ndeterministic policy gradient method and RL optimizes the closed-loop\nperformance by updating the parameters. Simulation results demonstrate that the\nproposed method is able to tackle the constraints and deliver the optimal\npolicy.",
    "descriptor": "\nComments: This paper has been accepted to be presented at 2021 Conference on Control Technology and Applications (CCTA), 6 pages, 8 Figs\n",
    "authors": [
      "A. Bahari Kordabad",
      "W. Cai",
      "S. Gros"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03541"
  },
  {
    "id": "arXiv:2106.03545",
    "title": "An Improved Approximation Algorithm for the Maximum Weight Independent  Set Problem in d-Claw Free Graphs",
    "abstract": "In this paper, we consider the task of computing an independent set of\nmaximum weight in a given $d$-claw free graph $G=(V,E)$ equipped with a\npositive weight function $w:V\\rightarrow\\mathbb{R}^+$. In doing so, $d\\geq 2$\nis considered a constant. The previously best known approximation algorithm for\nthis problem is the local improvement algorithm SquareImp proposed by Berman.\nIt achieves a performance ratio of $\\frac{d}{2}+\\epsilon$ in time\n$\\mathcal{O}(|V(G)|^{d+1}\\cdot(|V(G)|+|E(G)|)\\cdot (d-1)^2\\cdot\n\\left(\\frac{d}{2\\epsilon}+1\\right)^2)$ for any $\\epsilon>0$, which has remained\nunimproved for the last twenty years. By considering a broader class of local\nimprovements, we obtain an approximation ratio of\n$\\frac{d}{2}-\\frac{1}{63,700,992}+\\epsilon$ for any $\\epsilon>0$ at the cost of\nan additional factor of $\\mathcal{O}(|V(G)|^{(d-1)^2})$ in the running time. In\nparticular, our result implies a polynomial time $\\frac{d}{2}$-approximation\nalgorithm. Furthermore, the well-known reduction from the weighted $k$-Set\nPacking Problem to the Maximum Weight Independent Set Problem in $k+1$-claw\nfree graphs provides a\n$\\frac{k+1}{2}-\\frac{1}{63,700,992}+\\epsilon$-approximation algorithm for the\nweighted $k$-Set Packing Problem for any $\\epsilon>0$. This improves on the\npreviously best known approximation guarantee of $\\frac{k+1}{2}+\\epsilon$\noriginating from the result of Berman.",
    "descriptor": "\nComments: full version of the paper \"An Improved Approximation Algorithm for the Maximum Weight Independent Set Problem in d-Claw Free Graphs\" published in the proceedings of STACS 2021, 30 pages, 4 figures\n",
    "authors": [
      "Meike Neuwohner"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03545"
  },
  {
    "id": "arXiv:2106.03546",
    "title": "On Learning to Rank Long Sequences with Contextual Bandits",
    "abstract": "Motivated by problems of learning to rank long item sequences, we introduce a\nvariant of the cascading bandit model that considers flexible length sequences\nwith varying rewards and losses. We formulate two generative models for this\nproblem within the generalized linear setting, and design and analyze upper\nconfidence algorithms for it. Our analysis delivers tight regret bounds which,\nwhen specialized to vanilla cascading bandits, results in sharper guarantees\nthan previously available in the literature. We evaluate our algorithms on a\nnumber of real-world datasets, and show significantly improved empirical\nperformance as compared to known cascading bandit baselines.",
    "descriptor": "",
    "authors": [
      "Anirban Santara",
      "Claudio Gentile",
      "Gaurav Aggarwal",
      "Shuai Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03546"
  },
  {
    "id": "arXiv:2106.03548",
    "title": "Auction-based and Distributed Optimization Approaches for Scheduling  Observations in Satellite Constellations with Exclusive Orbit Portions",
    "abstract": "We investigate the use of multi-agent allocation techniques on problems\nrelated to Earth observation scenarios with multiple users and satellites. We\nfocus on the problem of coordinating users having reserved exclusive orbit\nportions and one central planner having several requests that may use some\nintervals of these exclusives. We define this problem as Earth Observation\nSatellite Constellation Scheduling Problem (EOSCSP) and map it to a Mixed\nInteger Linear Program. As to solve EOSCSP, we propose market-based techniques\nand a distributed problem solving technique based on Distributed Constraint\nOptimization (DCOP), where agents cooperate to allocate requests without\nsharing their own schedules. These contributions are experimentally evaluated\non randomly generated EOSCSP instances based on real large-scale or highly\nconflicting observation order books.",
    "descriptor": "",
    "authors": [
      "Gauthier Picard"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03548"
  },
  {
    "id": "arXiv:2106.03553",
    "title": "Playing with words: Do people exploit loaded language to affect others'  decisions for their own benefit?",
    "abstract": "In this article, we study whether people in the position of describing a\ndecision problem to decision-makers exploit this opportunity for their benefit,\nby choosing descriptions that may be potentially beneficial for themselves. To\nthis end, we design, pre-register, and conduct an experiment in which dictator\ngame recipients are asked to choose the instructions used to introduce the game\nto dictators, among six different instructions that are known from previous\nresearch to affect dictators' decisions. The results demonstrate that some\ndictator game recipients tend to choose instructions that make them more likely\nto receive a higher payoff. Finally, we found some evidence that young age and\ndeliberative thinking are associated with this tendency.",
    "descriptor": "",
    "authors": [
      "Valerio Capraro",
      "Andrea Vanzo",
      "Antonio Cabrales"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Physics and Society (physics.soc-ph)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2106.03553"
  },
  {
    "id": "arXiv:2106.03554",
    "title": "Free-Choice Nets With Home Clusters Are Lucent",
    "abstract": "A marked Petri net is lucent if there are no two different reachable markings\nenabling the same set of transitions, i.e., states are fully characterized by\nthe transitions they enable. Characterizing the class of systems that are\nlucent is a foundational and also challenging question. However, little\nresearch has been done on the topic. In this paper, it is shown that all\nfree-choice nets having a home cluster are lucent. These nets have a so-called\nhome marking such that it is always possible to reach this marking again. Such\na home marking can serve as a regeneration point or as an end-point. The result\nis highly relevant because in many applications, we want the system to be\nlucent and many well-behaved process models fall into the class identified in\nthis paper. Unlike previous work, we do not require the marked Petri net to be\nlive and strongly connected. Most of the analysis techniques for free-choice\nnets are tailored towards well-formed nets. The approach presented in this\npaper provides a novel perspective enabling new analysis techniques for\nfree-choice nets that do not need to be well-formed. Therefore, we can also\nmodel systems and processes that are terminating and/or have an initialization\nphase.",
    "descriptor": "",
    "authors": [
      "Wil M.P. van der Aalst"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Computation and Language (cs.CL)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03554"
  },
  {
    "id": "arXiv:2106.03555",
    "title": "The Limits of Local Search for the Maximum Weight Independent Set  Problem in d-Claw Free Graphs",
    "abstract": "We consider the Maximum Weight Independent Set Problem (MWIS) in $d$-claw\nfree graphs, i.e. the task of computing an independent set of maximum weight in\na given $d$-claw free graph $G=(V,E)$ equipped with a positive weight function\n$w:V\\rightarrow\\mathbb{R}_{>0}$. For $k\\geq 1$, the MWIS in $k+1$-claw free\ngraphs generalizes the weighted $k$-Set Packing Problem. Given that for $k\\geq\n3$, this problem does not permit a polynomial time $o(\\frac{k}{\\log\nk})$-approximation unless $P=NP$, most previous algorithms for both weighted\n$k$-Set Packing and the MWIS in $d$-claw free graphs rely on local search. For\nthe last twenty years, Berman's algorithm SquareImp, which yields a\n$\\frac{d}{2}+\\epsilon$-approximation for the MWIS in $d$-claw free graphs, has\nremained unchallenged for both problems. Recently, it was improved by\nNeuwohner, obtaining an approximation guarantee slightly below $\\frac{d}{2}$,\nand inevitably raising the question of how far one can get by using local\nsearch. In this paper, we finally answer this question asymptotically in the\nfollowing sense: By considering local improvements of logarithmic size, we\nobtain approximation ratios of $\\frac{d-1+\\epsilon_d}{2}$ for the MWIS in\n$d$-claw free graphs for $d\\geq 3$ in quasi-polynomial time, where $0\\leq\n\\epsilon_d\\leq 1$ and $\\lim_{d\\rightarrow\\infty}\\epsilon_d = 0$. By employing\nthe color coding technique, we can use the previous result to obtain a\npolynomial time $\\frac{k+\\epsilon_{k+1}}{2}$-approximation for weighted $k$-Set\nPacking. On the other hand, we provide examples showing that no local\nimprovement algorithm considering local improvements of size\n$\\mathcal{O}(\\log(|\\mathcal{S}|))$ with respect to some power $w^\\alpha$ of the\nweight function, where $\\alpha\\in\\mathbb{R}$ is chosen arbitrarily, but fixed,\ncan yield an approximation guarantee better than $\\frac{k}{2}$ for the weighted\n$k$-Set Packing Problem with $k\\geq 3$.",
    "descriptor": "\nComments: 50 pages, 2 figures\n",
    "authors": [
      "Meike Neuwohner"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03555"
  },
  {
    "id": "arXiv:2106.03557",
    "title": "Arrangements of orthogonal circles with many intersections",
    "abstract": "An arrangement of circles in which circles intersect only in angles of\n$\\pi/2$ is called an \\emph{arrangement of orthogonal circles}. We show that in\nthe case that no two circles are nested, the intersection graph of such an\narrangement is planar. The same result holds for arrangement of circles that\nintersect in an angle of at most $\\pi/2$.\nFor the general case we prove that the maximal number of edges in an\nintersection graph of an arrangement of orthogonal circles lies in between $4n\n- O\\left(\\sqrt{n}\\right)$ and $\\left(4+\\frac{5}{11}\\right)n$, for $n$ being the\nnumber of circles. Based on the lower bound we can also improve the bound for\nthe number of triangles in arrangements of orthogonal circles to $(3 +\n5/9)n-O\\left(\\sqrt{n}\\right)$.",
    "descriptor": "",
    "authors": [
      "Sarah Carmesin",
      "Andr\u00e9 Schulz"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.03557"
  },
  {
    "id": "arXiv:2106.03562",
    "title": "Robotic Electrospinning Actuated by Non-Circular Joint Continuum  Manipulator for Endoluminal Therapy",
    "abstract": "Electrospinning has exhibited excellent benefits to treat the trauma for\ntissue engineering due to its produced micro/nano fibrous structure. It can\neffectively adhere to the tissue surface for long-term continuous therapy. This\npaper develops a robotic electrospinning platform for endoluminal therapy. The\nplatform consists of a continuum manipulator, the electrospinning device, and\nthe actuation unit. The continuum manipulator has two bending sections to\nfacilitate the steering of the tip needle for a controllable spinning\ndirection. Non-circular joint profile is carefully designed to enable a\nconstant length of the centreline of a continuum manipulator for stable fluid\ntransmission inside it. Experiments are performed on a bronchus phantom, and\nthe steering ability and bending limitation in each direction are also\ninvestigated. The endoluminal electrospinning is also fulfilled by a trajectory\nfollowing and points targeting experiments. The effective adhesive area of the\nproduced fibre is also illustrated. The proposed robotic electrospinning shows\nits feasibility to precisely spread more therapeutic drug to construct fibrous\nstructure for potential endoluminal treatment.",
    "descriptor": "",
    "authors": [
      "Zicong Wu",
      "Chuqian Lou",
      "Zhu Jin",
      "Shaoping Huang",
      "Ning Liu",
      "Yun Zou",
      "Mirko Kovac",
      "Anzhu Gao",
      "Guang-Zhong Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03562"
  },
  {
    "id": "arXiv:2106.03567",
    "title": "AMV : Algorithm Metadata Vocabulary",
    "abstract": "Metadata vocabularies are used in various domains of study. It provides an\nin-depth description of the resources. In this work, we develop Algorithm\nMetadata Vocabulary (AMV), a vocabulary for capturing and storing the metadata\nabout the algorithms (a procedure or a set of rules that is followed\nstep-by-step to solve a problem, especially by a computer). The snag faced by\nthe researchers in the current time is the failure of getting relevant results\nwhen searching for algorithms in any search engine. AMV is represented as a\nsemantic model and produced OWL file, which can be directly used by anyone\ninterested to create and publish algorithm metadata as a knowledge graph, or to\nprovide metadata service through SPARQL endpoint. To design the vocabulary, we\npropose a well-defined methodology, which considers real issues faced by the\nalgorithm users and the practitioners. The evaluation shows a promising result.",
    "descriptor": "",
    "authors": [
      "Biswanath Dutta",
      "Jyotima Patel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03567"
  },
  {
    "id": "arXiv:2106.03569",
    "title": "Socially-Aware Self-Supervised Tri-Training for Recommendation",
    "abstract": "Self-supervised learning (SSL), which can automatically generate ground-truth\nsamples from raw data, holds vast potential to improve recommender systems.\nMost existing SSL-based methods perturb the raw data graph with uniform\nnode/edge dropout to generate new data views and then conduct the\nself-discrimination based contrastive learning over different views to learn\ngeneralizable representations. Under this scheme, only a bijective mapping is\nbuilt between nodes in two different views, which means that the\nself-supervision signals from other nodes are being neglected. Due to the\nwidely observed homophily in recommender systems, we argue that the supervisory\nsignals from other nodes are also highly likely to benefit the representation\nlearning for recommendation. To capture these signals, a general socially-aware\nSSL framework that integrates tri-training is proposed in this paper.\nTechnically, our framework first augments the user data views with the user\nsocial information. And then under the regime of tri-training for multi-view\nencoding, the framework builds three graph encoders (one for recommendation)\nupon the augmented views and iteratively improves each encoder with\nself-supervision signals from other users, generated by the other two encoders.\nSince the tri-training operates on the augmented views of the same data sources\nfor self-supervision signals, we name it self-supervised tri-training.\nExtensive experiments on multiple real-world datasets consistently validate the\neffectiveness of the self-supervised tri-training framework for improving\nrecommendation. The code is released at https://github.com/Coder-Yu/QRec.",
    "descriptor": "\nComments: 9 pages, accepted by KDD'21\n",
    "authors": [
      "Junliang Yu",
      "Hongzhi Yin",
      "Min Gao",
      "Xin Xia",
      "Xiangliang Zhang",
      "Nguyen Quoc Viet Hung"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.03569"
  },
  {
    "id": "arXiv:2106.03579",
    "title": "Forward Looking Best-Response Multiplicative Weights Update Methods",
    "abstract": "We propose a novel variant of the \\emph{multiplicative weights update method}\nwith forward-looking best-response strategies, that guarantees last-iterate\nconvergence for \\emph{zero-sum games} with a unique \\emph{Nash equilibrium}.\nParticularly, we show that the proposed algorithm converges to an\n$\\eta^{1/\\rho}$-approximate Nash equilibrium, with $\\rho > 1$, by decreasing\nthe Kullback-Leibler divergence of each iterate by a rate of at least\n$\\Omega(\\eta^{1+\\frac{1}{\\rho}})$, for sufficiently small learning rate $\\eta$.\nWhen our method enters a sufficiently small neighborhood of the solution, it\nbecomes a contraction and converges to the Nash equilibrium of the game.\nFurthermore, we perform an experimental comparison with the recently proposed\noptimistic variant of the multiplicative weights update method, by\n\\cite{Daskalakis2019LastIterateCZ}, which has also been proved to attain\nlast-iterate convergence. Our findings reveal that our algorithm offers\nsubstantial gains both in terms of the convergence rate and the region of\ncontraction relative to the previous approach.",
    "descriptor": "",
    "authors": [
      "Michail Fasoulakis",
      "Evangelos Markakis",
      "Yannis Pantazis",
      "Constantinos Varsos"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03579"
  },
  {
    "id": "arXiv:2106.03580",
    "title": "One-shot learning of paired associations by a reservoir computing model  with Hebbian plasticity",
    "abstract": "One-shot learning can be achieved by algorithms and animals, but how the\nlatter do it is poorly understood as most of the algorithms are not\nbiologically plausible. Experiments studying one-shot learning in rodents have\nshown that after initial gradual learning of associations between cues and\nlocations, new associations can be learned with just a single exposure to each\nnew cue-location pair. Foster, Morris and Dayan (2000) developed a hybrid\ntemporal difference - symbolic model that exhibited one-shot learning for dead\nreckoning to displaced single locations. While the temporal difference rule for\nlearning the agent's actual coordinates was biologically plausible, the model's\nsymbolic mechanism for learning target coordinates was not, and one-shot\nlearning for multiple target locations was not addressed. Here we extend the\nmodel by replacing the symbolic mechanism with a reservoir of recurrently\nconnected neurons resembling cortical microcircuitry. Biologically plausible\nlearning of target coordinates was achieved by subjecting the reservoir's\noutput weights to synaptic plasticity governed by a novel 4-factor variant of\nthe exploratory Hebbian (EH) rule. As with rodents, the reservoir model\nexhibited one-shot learning for multiple paired associations.",
    "descriptor": "\nComments: 16 pages, 6 figures. Code can be accessed at this https URL\n",
    "authors": [
      "M Ganesh Kumar",
      "Cheston Tan",
      "Camilo Libedinsky",
      "Shih-Cheng Yen",
      "Andrew Yong-Yi Tan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.03580"
  },
  {
    "id": "arXiv:2106.03593",
    "title": "Neural Auction: End-to-End Learning of Auction Mechanisms for E-Commerce  Advertising",
    "abstract": "In e-commerce advertising, it is crucial to jointly consider various\nperformance metrics, e.g., user experience, advertiser utility, and platform\nrevenue. Traditional auction mechanisms, such as GSP and VCG auctions, can be\nsuboptimal due to their fixed allocation rules to optimize a single performance\nmetric (e.g., revenue or social welfare). Recently, data-driven auctions,\nlearned directly from auction outcomes to optimize multiple performance\nmetrics, have attracted increasing research interests. However, the procedure\nof auction mechanisms involves various discrete calculation operations, making\nit challenging to be compatible with continuous optimization pipelines in\nmachine learning. In this paper, we design \\underline{D}eep \\underline{N}eural\n\\underline{A}uctions (DNAs) to enable end-to-end auction learning by proposing\na differentiable model to relax the discrete sorting operation, a key component\nin auctions. We optimize the performance metrics by developing deep models to\nefficiently extract contexts from auctions, providing rich features for auction\ndesign. We further integrate the game theoretical conditions within the model\ndesign, to guarantee the stability of the auctions. DNAs have been successfully\ndeployed in the e-commerce advertising system at Taobao. Experimental\nevaluation results on both large-scale data set as well as online A/B test\ndemonstrated that DNAs significantly outperformed other mechanisms widely\nadopted in industry.",
    "descriptor": "\nComments: To appear in the Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2021\n",
    "authors": [
      "Xiangyu Liu",
      "Chuan Yu",
      "Zhilin Zhang",
      "Zhenzhe Zheng",
      "Yu Rong",
      "Hongtao Lv",
      "Da Huo",
      "Yiqing Wang",
      "Dagui Chen",
      "Jian Xu",
      "Fan Wu",
      "Guihai Chen",
      "Xiaoqiang Zhu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03593"
  },
  {
    "id": "arXiv:2106.03594",
    "title": "Learning Combinatorial Node Labeling Algorithms",
    "abstract": "We present a graph neural network to learn graph coloring heuristics using\nreinforcement learning. Our learned deterministic heuristics give better\nsolutions than classical degree-based greedy heuristics and only take seconds\nto evaluate on graphs with tens of thousands of vertices. As our approach is\nbased on policy-gradients, it also learns a probabilistic policy as well. These\nprobabilistic policies outperform all greedy coloring baselines and a machine\nlearning baseline. Our approach generalizes several previous machine-learning\nframeworks, which applied to problems like minimum vertex cover. We also\ndemonstrate that our approach outperforms two greedy heuristics on minimum\nvertex cover.",
    "descriptor": "",
    "authors": [
      "Lukas Gianinazzi",
      "Maximilian Fries",
      "Nikoli Dryden",
      "Tal Ben-Nun",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03594"
  },
  {
    "id": "arXiv:2106.03596",
    "title": "Beyond Bandit Feedback in Online Multiclass Classification",
    "abstract": "We study the problem of online multiclass classification in a setting where\nthe learner's feedback is determined by an arbitrary directed graph. While\nincluding bandit feedback as a special case, feedback graphs allow a much\nricher set of applications, including filtering and label efficient\nclassification. We introduce Gappletron, the first online multiclass algorithm\nthat works with arbitrary feedback graphs. For this new algorithm, we prove\nsurrogate regret bounds that hold, both in expectation and with high\nprobability, for a large class of surrogate losses. Our bounds are of order\n$B\\sqrt{\\rho KT}$, where $B$ is the diameter of the prediction space, $K$ is\nthe number of classes, $T$ is the time horizon, and $\\rho$ is the domination\nnumber (a graph-theoretic parameter affecting the amount of exploration). In\nthe full information case, we show that Gappletron achieves a constant\nsurrogate regret of order $B^2K$. We also prove a general lower bound of order\n$\\max\\big\\{B^2K,\\sqrt{T}\\big\\}$ showing that our upper bounds are not\nsignificantly improvable. Experiments on synthetic data show that for various\nfeedback graphs, our algorithm is competitive against known baselines.",
    "descriptor": "",
    "authors": [
      "Dirk van der Hoeven",
      "Federico Fusco",
      "Nicol\u00f2 Cesa-Bianchi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03596"
  },
  {
    "id": "arXiv:2106.03598",
    "title": "SciFive: a text-to-text transformer model for biomedical literature",
    "abstract": "In this report, we introduce SciFive, a domain-specific T5 model that has\nbeen pre-trained on large biomedical corpora. Our model outperforms the current\nSOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation,\nrelation extraction, natural language inference, and question-answering. We\nshow that text-generation methods have significant potential in a broad array\nof biomedical NLP tasks, particularly those requiring longer, more complex\noutputs. Our results support the exploration of more difficult text generation\ntasks and the development of new methods in this area",
    "descriptor": "",
    "authors": [
      "Long N. Phan",
      "James T. Anibal",
      "Hieu Tran",
      "Shaurya Chanana",
      "Erol Bahadroglu",
      "Alec Peltekian",
      "Gr\u00e9goire Altan-Bonnet"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03598"
  },
  {
    "id": "arXiv:2106.03601",
    "title": "Analyzing Open-Source Serverless Platforms: Characteristics and  Performance",
    "abstract": "Serverless computing is increasingly popular because of its lower cost and\neasier deployment. Several cloud service providers (CSPs) offer serverless\ncomputing on their public clouds, but it may bring the vendor lock-in risk. To\navoid this limitation, many open-source serverless platforms come out to allow\ndevelopers to freely deploy and manage functions on self-hosted clouds.\nHowever, building effective functions requires much expertise and thorough\ncomprehension of platform frameworks and features that affect performance. It\nis a challenge for a service developer to differentiate and select the\nappropriate serverless platform for different demands and scenarios. Thus, we\nelaborate the frameworks and event processing models of four popular\nopen-source serverless platforms and identify their salient idiosyncrasies. We\nanalyze the root causes of performance differences between different service\nexporting and auto-scaling modes on those platforms. Further, we provide\nseveral insights for future work, such as auto-scaling and metric collection.",
    "descriptor": "",
    "authors": [
      "Junfeng Li",
      "Sameer G. Kulkarni",
      "K. K. Ramakrishnan",
      "Dan Li"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03601"
  },
  {
    "id": "arXiv:2106.03603",
    "title": "Deep Neural Network Modeling of Unknown Partial Differential Equations  in Nodal Space",
    "abstract": "We present a numerical framework for deep neural network (DNN) modeling of\nunknown time-dependent partial differential equations (PDE) using their\ntrajectory data. Unlike the recent work of [Wu and Xiu, J. Comput. Phys. 2020],\nwhere the learning takes place in modal/Fourier space, the current method\nconducts the learning and modeling in physical space and uses measurement data\nas nodal values. We present a DNN structure that has a direct correspondence to\nthe evolution operator of the underlying PDE, thus establishing the existence\nof the DNN model. The DNN model also does not require any geometric information\nof the data nodes. Consequently, a trained DNN defines a predictive model for\nthe underlying unknown PDE over structureless grids. A set of examples,\nincluding linear and nonlinear scalar PDE, system of PDEs, in both one\ndimension and two dimensions, over structured and unstructured grids, are\npresented to demonstrate the effectiveness of the proposed DNN modeling.\nExtension to other equations such as differential-integral equations is also\ndiscussed.",
    "descriptor": "",
    "authors": [
      "Zhen Chen",
      "Victor Churchill",
      "Kailiang Wu",
      "Dongbin Xiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03603"
  },
  {
    "id": "arXiv:2106.03609",
    "title": "High-Dimensional Bayesian Optimisation with Variational Autoencoders and  Deep Metric Learning",
    "abstract": "We introduce a method based on deep metric learning to perform Bayesian\noptimisation over high-dimensional, structured input spaces using variational\nautoencoders (VAEs). By extending ideas from supervised deep metric learning,\nwe address a longstanding problem in high-dimensional VAE Bayesian\noptimisation, namely how to enforce a discriminative latent space as an\ninductive bias. Importantly, we achieve such an inductive bias using just 1% of\nthe available labelled data relative to previous work, highlighting the sample\nefficiency of our approach. As a theoretical contribution, we present a proof\nof vanishing regret for our method. As an empirical contribution, we present\nstate-of-the-art results on real-world high-dimensional black-box optimisation\nproblems including property-guided molecule generation. It is the hope that the\nresults presented in this paper can act as a guiding principle for realising\neffective high-dimensional Bayesian optimisation.",
    "descriptor": "",
    "authors": [
      "Antoine Grosnit",
      "Rasul Tutunov",
      "Alexandre Max Maraval",
      "Ryan-Rhys Griffiths",
      "Alexander I. Cowen-Rivers",
      "Lin Yang",
      "Lin Zhu",
      "Wenlong Lyu",
      "Zhitang Chen",
      "Jun Wang",
      "Jan Peters",
      "Haitham Bou-Ammar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03609"
  },
  {
    "id": "arXiv:2106.03611",
    "title": "Inferring Objectives in Continuous Dynamic Games from Noise-Corrupted  Partial State Observations",
    "abstract": "Robots and autonomous systems must interact with one another and their\nenvironment to provide high-quality services to their users. Dynamic game\ntheory provides an expressive theoretical framework for modeling scenarios\ninvolving multiple agents with differing objectives interacting over time. A\ncore challenge when formulating a dynamic game is designing objectives for each\nagent that capture desired behavior. In this paper, we propose a method for\ninferring parametric objective models of multiple agents based on observed\ninteractions. Our inverse game solver jointly optimizes player objectives and\ncontinuous-state estimates by coupling them through Nash equilibrium\nconstraints. Hence, our method is able to directly maximize the observation\nlikelihood rather than other non-probabilistic surrogate criteria. Our method\ndoes not require full observations of game states or player strategies to\nidentify player objectives. Instead, it robustly recovers this information from\nnoisy, partial state observations. As a byproduct of estimating player\nobjectives, our method computes a Nash equilibrium trajectory corresponding to\nthose objectives. Thus, it is suitable for downstream trajectory forecasting\ntasks. We demonstrate our method in several simulated traffic scenarios.\nResults show that it reliably estimates player objectives from single short\nsequences of noisy, partially observed interactions. Furthermore, using the\nestimated objectives, our method makes accurate predictions of each player's\ntrajectory.",
    "descriptor": "\nComments: Submitted to RSS2021\n",
    "authors": [
      "Lasse Peters",
      "David Fridovich-Keil",
      "Vicen\u00e7 Rubies-Royo",
      "Claire J. Tomlin",
      "Cyrill Stachniss"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.03611"
  },
  {
    "id": "arXiv:2106.03613",
    "title": "RoSearch: Search for Robust Student Architectures When Distilling  Pre-trained Language Models",
    "abstract": "Pre-trained language models achieve outstanding performance in NLP tasks.\nVarious knowledge distillation methods have been proposed to reduce the heavy\ncomputation and storage requirements of pre-trained language models. However,\nfrom our observations, student models acquired by knowledge distillation suffer\nfrom adversarial attacks, which limits their usage in security sensitive\nscenarios. In order to overcome these security problems, RoSearch is proposed\nas a comprehensive framework to search the student models with better\nadversarial robustness when performing knowledge distillation. A directed\nacyclic graph based search space is built and an evolutionary search strategy\nis utilized to guide the searching approach. Each searched architecture is\ntrained by knowledge distillation on pre-trained language model and then\nevaluated under a robustness-, accuracy- and efficiency-aware metric as\nenvironmental fitness. Experimental results show that RoSearch can improve\nrobustness of student models from 7%~18% up to 45.8%~47.8% on different\ndatasets with comparable weight compression ratio to existing distillation\nmethods (4.6$\\times$~6.5$\\times$ improvement from teacher model BERT_BASE) and\nlow accuracy drop. In addition, we summarize the relationship between student\narchitecture and robustness through statistics of searched models.",
    "descriptor": "\nComments: 10 pages, 9 figures\n",
    "authors": [
      "Xin Guo",
      "Jianlei Yang",
      "Haoyi Zhou",
      "Xucheng Ye",
      "Jianxin Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03613"
  },
  {
    "id": "arXiv:2106.03614",
    "title": "Adversarial Attack and Defense in Deep Ranking",
    "abstract": "Deep Neural Network classifiers are vulnerable to adversarial attack, where\nan imperceptible perturbation could result in misclassification. However, the\nvulnerability of DNN-based image ranking systems remains under-explored. In\nthis paper, we propose two attacks against deep ranking systems, i.e.,\nCandidate Attack and Query Attack, that can raise or lower the rank of chosen\ncandidates by adversarial perturbations. Specifically, the expected ranking\norder is first represented as a set of inequalities, and then a triplet-like\nobjective function is designed to obtain the optimal perturbation. Conversely,\nan anti-collapse triplet defense is proposed to improve the ranking model\nrobustness against all proposed attacks, where the model learns to prevent the\npositive and negative samples being pulled close to each other by adversarial\nattack. To comprehensively measure the empirical adversarial robustness of a\nranking model with our defense, we propose an empirical robustness score, which\ninvolves a set of representative attacks against ranking models. Our\nadversarial ranking attacks and defenses are evaluated on MNIST, Fashion-MNIST,\nCUB200-2011, CARS196 and Stanford Online Products datasets. Experimental\nresults demonstrate that a typical deep ranking system can be effectively\ncompromised by our attacks. Nevertheless, our defense can significantly improve\nthe ranking system robustness, and simultaneously mitigate a wide range of\nattacks.",
    "descriptor": "",
    "authors": [
      "Mo Zhou",
      "Le Wang",
      "Zhenxing Niu",
      "Qilin Zhang",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03614"
  },
  {
    "id": "arXiv:2106.03616",
    "title": "Beamforming and Transmit Power Design for Intelligent Reconfigurable  Surface-aided Secure Spatial Modulation",
    "abstract": "Intelligent reflecting surface (IRS) is a promising solution to build a\nprogrammable wireless environment for future communication systems, in which\nthe reflector elements steer the incident signal in fully customizable ways by\npassive beamforming. In this paper, an IRS-aided secure spatial modulation (SM)\nis proposed, where the IRS perform passive beamforming and information transfer\nsimultaneously by adjusting the on-off states of the reflecting elements. We\nformulate an optimization problem to maximize the average secrecy rate (SR) by\njointly optimizing the passive beamforming at IRS and the transmit power at\ntransmitter under the consideration that the direct pathes channels from\ntransmitter to receivers are obstructed by obstacles. As the expression of SR\nis complex, we derive a newly fitting expression (NASR) for the expression of\ntraditional approximate SR (TASR), which has simpler closed-form and more\nconvenient for subsequent optimization. Based on the above two fitting\nexpressions, three beamforming methods, called maximizing NASR via successive\nconvex approximation (Max-NASR-SCA), maximizing NASR via dual ascent\n(Max-NASR-DA) and maximizing TASR via semi-definite relaxation (Max-TASR-SDR)\nare proposed to improve the SR performance. Additionally, two transmit power\ndesign (TPD) methods are proposed based on the above two approximate SR\nexpressions, called Max-NASR-TPD and Max-TASR-TPD. Simulation results show that\nthe proposed Max-NASR-DA and Max-NASR-SCA IRS beamformers harvest substantial\nSR performance gains over Max-TASR-SDR. For TPD, the proposed Max-NASR-TPD\nperforms better than Max-TASR-TPD. Particularly, the Max-NASR-TPD has a\nclosed-form solution.",
    "descriptor": "",
    "authors": [
      "Feng Shu",
      "Xinyi Jiang",
      "Wenlong Cai",
      "Weiping Shi",
      "Mengxing Huang",
      "Jiangzhou Wang",
      "Xiaohu You"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03616"
  },
  {
    "id": "arXiv:2106.03617",
    "title": "PAIO: A Software-Defined Storage Data Plane Framework",
    "abstract": "We propose PAIO, the first general-purpose framework that enables system\ndesigners to build custom-made Software-Defined Storage (SDS) data plane\nstages. It provides the means to implement storage optimizations adaptable to\ndifferent workflows and user-defined policies, and allows straightforward\nintegration with existing applications and I/O layers. PAIO allows stages to be\nintegrated with modern SDS control planes to ensure holistic control and\nsystem-wide optimal performance. We demonstrate the performance and\napplicability of PAIO with two use cases. The first improves 99th percentile\nlatency by 4x in industry-standard LSM-based key-value stores. The second\nensures dynamic per-application bandwidth guarantees under shared storage\nenvironments.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Ricardo Macedo",
      "Yusuke Tanimura",
      "Jason Haga",
      "Vijay Chidambaram",
      "Jos\u00e9 Pereira",
      "Jo\u00e3o Paulo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2106.03617"
  },
  {
    "id": "arXiv:2106.03618",
    "title": "Document-level Relation Extraction as Semantic Segmentation",
    "abstract": "Document-level relation extraction aims to extract relations among multiple\nentity pairs from a document. Previously proposed graph-based or\ntransformer-based models utilize the entities independently, regardless of\nglobal information among relational triples. This paper approaches the problem\nby predicting an entity-level relation matrix to capture local and global\ninformation, parallel to the semantic segmentation task in computer vision.\nHerein, we propose a Document U-shaped Network for document-level relation\nextraction. Specifically, we leverage an encoder module to capture the context\ninformation of entities and a U-shaped segmentation module over the image-style\nfeature map to capture global interdependency among triples. Experimental\nresults show that our approach can obtain state-of-the-art performance on three\nbenchmark datasets DocRED, CDR, and GDA.",
    "descriptor": "\nComments: Accepted by IJCAI 2021\n",
    "authors": [
      "Ningyu Zhang",
      "Xiang Chen",
      "Xin Xie",
      "Shumin Deng",
      "Chuanqi Tan",
      "Mosha Chen",
      "Fei Huang",
      "Luo Si",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03618"
  },
  {
    "id": "arXiv:2106.03619",
    "title": "Multi-modal Entity Alignment in Hyperbolic Space",
    "abstract": "Many AI-related tasks involve the interactions of data in multiple\nmodalities. It has been a new trend to merge multi-modal information into\nknowledge graph(KG), resulting in multi-modal knowledge graphs (MMKG). However,\nMMKGs usually suffer from low coverage and incompleteness. To mitigate this\nproblem, a viable approach is to integrate complementary knowledge from other\nMMKGs. To this end, although existing entity alignment approaches could be\nadopted, they operate in the Euclidean space, and the resulting Euclidean\nentity representations can lead to large distortion of KG's hierarchical\nstructure. Besides, the visual information has yet not been well exploited. In\nresponse to these issues, in this work, we propose a novel multi-modal entity\nalignment approach, Hyperbolic multi-modal entity alignment(HMEA), which\nextends the Euclidean representation to hyperboloid manifold. We first adopt\nthe Hyperbolic Graph Convolutional Networks (HGCNs) to learn structural\nrepresentations of entities. Regarding the visual information, we generate\nimage embeddings using the densenet model, which are also projected into the\nhyperbolic space using HGCNs. Finally, we combine the structure and visual\nrepresentations in the hyperbolic space and use the aggregated embeddings to\npredict potential alignment results. Extensive experiments and ablation studies\ndemonstrate the effectiveness of our proposed model and its components.",
    "descriptor": "\nComments: 24 pages,5 figures;\n",
    "authors": [
      "Hao Guo",
      "Jiuyang Tang",
      "Weixin Zeng",
      "Xiang Zhao",
      "Li Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03619"
  },
  {
    "id": "arXiv:2106.03620",
    "title": "PcDGAN: A Continuous Conditional Diverse Generative Adversarial Network  For Inverse Design",
    "abstract": "Engineering design tasks often require synthesizing new designs that meet\ndesired performance requirements. The conventional design process, which\nrequires iterative optimization and performance evaluation, is slow and\ndependent on initial designs. Past work has used conditional generative\nadversarial networks (cGANs) to enable direct design synthesis for given target\nperformances. However, most existing cGANs are restricted to categorical\nconditions. Recent work on Continuous conditional GAN (CcGAN) tries to address\nthis problem, but still faces two challenges: 1) it performs poorly on\nnon-uniform performance distributions, and 2) the generated designs may not\ncover the entire design space. We propose a new model, named Performance\nConditioned Diverse Generative Adversarial Network (PcDGAN), which introduces a\nsingular vicinal loss combined with a Determinantal Point Processes (DPP) based\nloss function to enhance diversity. PcDGAN uses a new self-reinforcing score\ncalled the Lambert Log Exponential Transition Score (LLETS) for improved\nconditioning. Experiments on synthetic problems and a real-world airfoil design\nproblem demonstrate that PcDGAN outperforms state-of-the-art GAN models and\nimproves the conditioning likelihood by 69% in an airfoil generation task and\nup to 78% in synthetic conditional generation tasks and achieves greater design\nspace coverage. The proposed method enables efficient design synthesis and\ndesign space exploration with applications ranging from CAD model generation to\nmetamaterial selection.",
    "descriptor": "",
    "authors": [
      "Amin Heyrani Nobari",
      "Wei Chen",
      "Faez Ahmed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03620"
  },
  {
    "id": "arXiv:2106.03626",
    "title": "Towards Formal Verification of Password Generation Algorithms used in  Password Managers",
    "abstract": "Password managers are important tools that enable us to use stronger\npasswords, freeing us from the cognitive burden of remembering them. Despite\nthis, there are still many users who do not fully trust password managers. In\nthis paper, we focus on a feature that most password managers offer that might\nimpact the user's trust, which is the process of generating a random password.\nWe survey which algorithms are most commonly used and we propose a solution for\na formally verified reference implementation of a password generation\nalgorithm. We use EasyCrypt as our framework to both specify the reference\nimplementation and to prove its functional correctness and security.",
    "descriptor": "\nComments: shortpaper\n",
    "authors": [
      "Miguel Grilo",
      "Jo\u00e3o F. Ferreira",
      "Jos\u00e9 Bacelar Almeida"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.03626"
  },
  {
    "id": "arXiv:2106.03630",
    "title": "Efficient Iterative Amortized Inference for Learning Symmetric and  Disentangled Multi-Object Representations",
    "abstract": "Unsupervised multi-object representation learning depends on inductive biases\nto guide the discovery of object-centric representations that generalize.\nHowever, we observe that methods for learning these representations are either\nimpractical due to long training times and large memory consumption or forego\nkey inductive biases. In this work, we introduce EfficientMORL, an efficient\nframework for the unsupervised learning of object-centric representations. We\nshow that optimization challenges caused by requiring both symmetry and\ndisentanglement can in fact be addressed by high-cost iterative amortized\ninference by designing the framework to minimize its dependence on it. We take\na two-stage approach to inference: first, a hierarchical variational\nautoencoder extracts symmetric and disentangled representations through\nbottom-up inference, and second, a lightweight network refines the\nrepresentations with top-down feedback. The number of refinement steps taken\nduring training is reduced following a curriculum, so that at test time with\nzero steps the model achieves 99.1% of the refined decomposition performance.\nWe demonstrate strong object decomposition and disentanglement on the standard\nmulti-object benchmark while achieving nearly an order of magnitude faster\ntraining and test time inference over the previous state-of-the-art model.",
    "descriptor": "\nComments: Published in ICML'21. Code and data: this https URL\n",
    "authors": [
      "Patrick Emami",
      "Pan He",
      "Sanjay Ranka",
      "Anand Rangarajan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03630"
  },
  {
    "id": "arXiv:2106.03631",
    "title": "Unsupervised Representation Disentanglement of Text: An Evaluation on  Synthetic Datasets",
    "abstract": "To highlight the challenges of achieving representation disentanglement for\ntext domain in an unsupervised setting, in this paper we select a\nrepresentative set of successfully applied models from the image domain. We\nevaluate these models on 6 disentanglement metrics, as well as on downstream\nclassification tasks and homotopy. To facilitate the evaluation, we propose two\nsynthetic datasets with known generative factors. Our experiments highlight the\nexisting gap in the text domain and illustrate that certain elements such as\nrepresentation sparsity (as an inductive bias), or representation coupling with\nthe decoder could impact disentanglement. To the best of our knowledge, our\nwork is the first attempt on the intersection of unsupervised representation\ndisentanglement and text, and provides the experimental framework and datasets\nfor examining future developments in this direction.",
    "descriptor": "\nComments: Accepted to RepL4NLP 2021\n",
    "authors": [
      "Lan Zhang",
      "Victor Prokhorov",
      "Ehsan Shareghi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03631"
  },
  {
    "id": "arXiv:2106.03632",
    "title": "Quantifying and Improving Transferability in Domain Generalization",
    "abstract": "Out-of-distribution generalization is one of the key challenges when\ntransferring a model from the lab to the real world. Existing efforts mostly\nfocus on building invariant features among source and target domains. Based on\ninvariant features, a high-performing classifier on source domains could\nhopefully behave equally well on a target domain. In other words, the invariant\nfeatures are \\emph{transferable}. However, in practice, there are no perfectly\ntransferable features, and some algorithms seem to learn ''more transferable''\nfeatures than others. How can we understand and quantify such\n\\emph{transferability}? In this paper, we formally define transferability that\none can quantify and compute in domain generalization. We point out the\ndifference and connection with common discrepancy measures between domains,\nsuch as total variation and Wasserstein distance. We then prove that our\ntransferability can be estimated with enough samples and give a new upper bound\nfor the target error based on our transferability. Empirically, we evaluate the\ntransferability of the feature embeddings learned by existing algorithms for\ndomain generalization. Surprisingly, we find that many algorithms are not quite\nlearning transferable features, although few could still survive. In light of\nthis, we propose a new algorithm for learning transferable features and test it\nover various benchmark datasets, including RotatedMNIST, PACS, Office-Home and\nWILDS-FMoW. Experimental results show that the proposed algorithm achieves\nconsistent improvement over many state-of-the-art algorithms, corroborating our\ntheoretical findings.",
    "descriptor": "\nComments: 36 pages, 7 figures\n",
    "authors": [
      "Guojun Zhang",
      "Han Zhao",
      "Yaoliang Yu",
      "Pascal Poupart"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03632"
  },
  {
    "id": "arXiv:2106.03634",
    "title": "PROST: Physical Reasoning of Objects through Space and Time",
    "abstract": "We present a new probing dataset named PROST: Physical Reasoning about\nObjects Through Space and Time. This dataset contains 18,736 multiple-choice\nquestions made from 14 manually curated templates, covering 10 physical\nreasoning concepts. All questions are designed to probe both causal and masked\nlanguage models in a zero-shot setting. We conduct an extensive analysis which\ndemonstrates that state-of-the-art pretrained models are inadequate at physical\nreasoning: they are influenced by the order in which answer options are\npresented to them, they struggle when the superlative in a question is inverted\n(e.g., most <-> least), and increasing the amount of pretraining data and\nparameters only yields minimal improvements. These results provide support for\nthe hypothesis that current pretrained models' ability to reason about physical\ninteractions is inherently limited by a lack of real world experience. By\nhighlighting these limitations, we hope to motivate the development of models\nwith a human-like understanding of the physical world.",
    "descriptor": "\nComments: Accepted to ACL-Findings 2021, 9 Pages\n",
    "authors": [
      "St\u00e9phane Aroca-Ouellette",
      "Cory Paik",
      "Alessandro Roncone",
      "Katharina Kann"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03634"
  },
  {
    "id": "arXiv:2106.03635",
    "title": "GTM: A Generative Triple-Wise Model for Conversational Question  Generation",
    "abstract": "Generating some appealing questions in open-domain conversations is an\neffective way to improve human-machine interactions and lead the topic to a\nbroader or deeper direction. To avoid dull or deviated questions, some\nresearchers tried to utilize answer, the \"future\" information, to guide\nquestion generation. However, they separate a post-question-answer (PQA) triple\ninto two parts: post-question (PQ) and question-answer (QA) pairs, which may\nhurt the overall coherence. Besides, the QA relationship is modeled as a\none-to-one mapping that is not reasonable in open-domain conversations. To\ntackle these problems, we propose a generative triple-wise model with\nhierarchical variations for open-domain conversational question generation\n(CQG). Latent variables in three hierarchies are used to represent the shared\nbackground of a triple and one-to-many semantic mappings in both PQ and QA\npairs. Experimental results on a large-scale CQG dataset show that our method\nsignificantly improves the quality of questions in terms of fluency, coherence\nand diversity over competitive baselines.",
    "descriptor": "\nComments: To appear at ACL 2021 main conference (long paper)\n",
    "authors": [
      "Lei Shen",
      "Fandong Meng",
      "Jinchao Zhang",
      "Yang Feng",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03635"
  },
  {
    "id": "arXiv:2106.03637",
    "title": "Deep Canonical Correlation Alignment for Sensor Signals",
    "abstract": "Sensor technology is becoming increasingly prevalent across a multitude of\nfields and industries. As a result, simultaneous recordings of multiple\ninter-correlated signals is becoming increasingly common. With this, more\nproblems of a practical nature emerge due to sensor clock-drift, offsets, and\nother complications. Processing of multiple sensor data is often dependent on\nthe data being properly aligned in the temporal dimension. The alignment\nprocess is a necessary step before the data can be evaluated properly but it is\na time consuming process, often involving significant manual labor and\nexpertise. Regularly used methods to align sensor signals have trouble\naddressing real-world issues such as morphological dissimilarities, excessive\nnoise, or very long, raw sensor signals. In this work, we present Deep\nCanonical Correlation Sensor Alignment (DCCA), a method that is specifically\ntailored to address these problems. It exploits common properties specific to\nmisalignments produced by sensor circuitry, such as clock-drift and offsets. On\na selection of artificial and real datasets we demonstrate the performance of\nDCCA under a variety of conditions.",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Narayan Sch\u00fctz",
      "Angela Botros",
      "Michael Single",
      "Philipp Buluschek",
      "Tobias Nef"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03637"
  },
  {
    "id": "arXiv:2106.03639",
    "title": "SNR optimization of multi-span fiber optic communication systems  employing EDFAs with non-flat gain and noise figure",
    "abstract": "Throughput optimization of optical communication systems is a key challenge\nfor current optical networks. The use of gain-flattening filters (GFFs)\nsimplifies the problem at the cost of insertion loss, higher power consumption\nand potentially poorer performance. In this work, we propose a component wise\nmodel of a multi-span transmission system for signal-to-noise (SNR)\noptimization. A machine-learning based model is trained for the gain and noise\nfigure spectral profile of a C-band amplifier without a GFF. The model is\ncombined with the Gaussian noise model for nonlinearities in optical fibers\nincluding stimulated Raman scattering and the implementation penalty spectral\nprofile measured in back-to-back in order to predict the SNR in each channel of\na multi-span wavelength division multiplexed system. All basic components in\nthe system model are differentiable and allow for the gradient descent-based\noptimization of a system of arbitrary configuration in terms of number of spans\nand length per span. When the input power profile is optimized for flat and\nmaximized received SNR per channel, the minimum performance in an arbitrary\n3-span experimental system is improved by up to 8 dB w.r.t. a system with flat\ninput power profile. An SNR flatness down to 1.2 dB is simultaneously achieved.\nThe model and optimization methods are used to optimize the performance of an\nexample core network, and 0.2 dB of gain is shown w.r.t. solutions that do not\ntake into account nonlinearities. The method is also shown to be beneficial for\nsystems with ideal gain flattening, achieving up to 0.3 dB of gain w.r.t. a\nflat input power profile.",
    "descriptor": "\nComments: submitted to JLT\n",
    "authors": [
      "Metodi Plamenov Yankov",
      "Pawel Marcin Kaminski",
      "Henrik Enggaard Hansen",
      "Francesco Da Ros"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03639"
  },
  {
    "id": "arXiv:2106.03640",
    "title": "Making EfficientNet More Efficient: Exploring Batch-Independent  Normalization, Group Convolutions and Reduced Resolution Training",
    "abstract": "Much recent research has been dedicated to improving the efficiency of\ntraining and inference for image classification. This effort has commonly\nfocused on explicitly improving theoretical efficiency, often measured as\nImageNet validation accuracy per FLOP. These theoretical savings have, however,\nproven challenging to achieve in practice, particularly on high-performance\ntraining accelerators.\nIn this work, we focus on improving the practical efficiency of the\nstate-of-the-art EfficientNet models on a new class of accelerator, the\nGraphcore IPU. We do this by extending this family of models in the following\nways: (i) generalising depthwise convolutions to group convolutions; (ii)\nadding proxy-normalized activations to match batch normalization performance\nwith batch-independent statistics; (iii) reducing compute by lowering the\ntraining resolution and inexpensively fine-tuning at higher resolution. We find\nthat these three methods improve the practical efficiency for both training and\ninference. Our code will be made available online.",
    "descriptor": "",
    "authors": [
      "Dominic Masters",
      "Antoine Labatie",
      "Zach Eaton-Rosen",
      "Carlo Luschi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03640"
  },
  {
    "id": "arXiv:2106.03645",
    "title": "Photonic Differential Privacy with Direct Feedback Alignment",
    "abstract": "Optical Processing Units (OPUs) -- low-power photonic chips dedicated to\nlarge scale random projections -- have been used in previous work to train deep\nneural networks using Direct Feedback Alignment (DFA), an effective alternative\nto backpropagation. Here, we demonstrate how to leverage the intrinsic noise of\noptical random projections to build a differentially private DFA mechanism,\nmaking OPUs a solution of choice to provide a private-by-design training. We\nprovide a theoretical analysis of our adaptive privacy mechanism, carefully\nmeasuring how the noise of optical random projections propagates in the process\nand gives rise to provable Differential Privacy. Finally, we conduct\nexperiments demonstrating the ability of our learning procedure to achieve\nsolid end-task performance.",
    "descriptor": "",
    "authors": [
      "Ruben Ohana",
      "Hamlet J. Medina Ruiz",
      "Julien Launay",
      "Alessandro Cappelli",
      "Iacopo Poli",
      "Liva Ralaivola",
      "Alain Rakotomamonjy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03645"
  },
  {
    "id": "arXiv:2106.03648",
    "title": "Cost-effective Mapping of Mobile Robot Based on the Fusion of UWB and  Short-range 2D LiDAR",
    "abstract": "Environment mapping is an essential prerequisite for mobile robots to perform\ndifferent tasks such as navigation and mission planning. With the availability\nof low-cost 2D LiDARs, there are increasing applications of such 2D LiDARs in\nindustrial environments. However, environment mapping in an unknown and\nfeature-less environment with such low-cost 2D LiDARs remains a challenge. The\nchallenge mainly originates from the short-range of LiDARs and complexities in\nperforming scan matching in these environments. In order to resolve these\nshortcomings, we propose to fuse the ultra-wideband (UWB) with 2D LiDARs to\nimprove the mapping quality of a mobile robot. The optimization-based approach\nis utilized for the fusion of UWB ranging information and odometry to first\noptimize the trajectory. Then the LiDAR-based loop closures are incorporated to\nimprove the accuracy of the trajectory estimation. Finally, the optimized\ntrajectory is combined with the LiDAR scans to produce the occupancy map of the\nenvironment. The performance of the proposed approach is evaluated in an indoor\nfeature-less environment with a size of 20m*20m. Obtained results show that the\nmapping error of the proposed scheme is 85.5% less than that of the\nconventional GMapping algorithm with short-range LiDAR (for example Hokuyo\nURG-04LX in our experiment with a maximum range of 5.6m).",
    "descriptor": "\nComments: Accepted by IEEE/ASME TRANSACTIONS ON MECHATRONICS\n",
    "authors": [
      "Ran Liu",
      "Yongping He",
      "Chau Yuen",
      "Billy Pik Lik Lau",
      "Rashid Ali",
      "Wenpeng Fu",
      "Zhiqiang Cao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.03648"
  },
  {
    "id": "arXiv:2106.03650",
    "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
    "abstract": "Very recently, Window-based Transformers, which computed self-attention\nwithin non-overlapping local windows, demonstrated promising results on image\nclassification, semantic segmentation, and object detection. However, less\nstudy has been devoted to the cross-window connection which is the key element\nto improve the representation ability. In this work, we revisit the spatial\nshuffle as an efficient way to build connections among windows. As a result, we\npropose a new vision transformer, named Shuffle Transformer, which is highly\nefficient and easy to implement by modifying two lines of code. Furthermore,\nthe depth-wise convolution is introduced to complement the spatial shuffle for\nenhancing neighbor-window connections. The proposed architectures achieve\nexcellent performance on a wide range of visual tasks including image-level\nclassification, object detection, and semantic segmentation. Code will be\nreleased for reproduction.",
    "descriptor": "",
    "authors": [
      "Zilong Huang",
      "Youcheng Ben",
      "Guozhong Luo",
      "Pei Cheng",
      "Gang Yu",
      "Bin Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03650"
  },
  {
    "id": "arXiv:2106.03654",
    "title": "The Convexity and Concavity of Envelopes of the Minimum-Relative-Entropy  Region for the DSBS",
    "abstract": "In this paper, we prove that for the doubly symmetric binary distribution,\nthe lower increasing envelope and the upper envelope of the\nminimum-relative-entropy region are respectively convex and concave. We also\nprove that another function induced the minimum-relative-entropy region is\nconcave. These two envelopes and this function were previously used to\ncharacterize the optimal exponents in strong small-set expansion problems and\nstrong Brascamp--Lieb inequalities. The results in this paper, combined with\nthe strong small-set expansion theorem derived by Yu, Anantharam, and Chen\n(2021), and the strong Brascamp--Lieb inequality derived by Yu (2021), confirm\npositively Ordentlich--Polyanskiy--Shayevitz's conjecture on the strong\nsmall-set expansion (2019) and Polyanskiy's conjecture on the strong\nBrascamp--Lieb inequality (2016). The proofs in this paper are based on the\nequivalence between the convexity of a function and the convexity of the set of\nminimizers of its Lagrangian dual.",
    "descriptor": "\nComments: 14 pages, 4 figures\n",
    "authors": [
      "Lei Yu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.03654"
  },
  {
    "id": "arXiv:2106.03658",
    "title": "Reduction Using Induced Subnets To Systematically Prove Properties For  Free-Choice Nets",
    "abstract": "We use sequences of t-induced T-nets and p-induced P-nets to convert\nfree-choice nets into T-nets and P-nets while preserving properties such as\nwell-formedness, liveness, lucency, pc-safety, and perpetuality. The approach\nis general and can be applied to different properties. This allows for more\nsystematic proofs that \"peel off\" non-trivial parts while retaining the essence\nof the problem (e.g., lifting properties from T-net and P-net to free-choice\nnets).",
    "descriptor": "\nComments: Keywords: Petri Nets, Free-Choice Nets, Net Reduction, Lucency\n",
    "authors": [
      "Wil M.P. van der Aalst"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.03658"
  },
  {
    "id": "arXiv:2106.03664",
    "title": "Optimal Transmit Power and Antenna Selection to Achieve Energy Efficient  and Low Complexity in fifth generation Massive MIMO Systems",
    "abstract": "This paper investigates joint antenna selection and optimal transmit power in\nmulti cell massive multiple input multiple output systems. The pilot\ninterference and activated transmit antenna selection plays an essential role\nin maximizing energy efficiency. We derived the closed-form of maximal energy\nefficiency with complete knowledge of large-scale fading with maximum ratio\ntransmission while accounting for channel estimation and eliminated pilot\ncontamination when the antennas approach infinity. We investigated joint\noptimal antenna selection and optimal transmit power under minimized reuse of\npilot sequences based on a novel iterative low-complexity algorithm for\nLagrange multiplayer and Newton methods. The two scenarios of achievable high\ndata rate and total transmit power allocation are critical to the performance\nmaximal energy efficiency. We propose new power consumption for each antenna\nbased on the transmit power amplifier and circuit power consumption to analyze\nexact power consumption. The simulation results show that maximal energy\nefficiency could be achieved using the iterative low complexity algorithm based\non the reasonable maximum transmit power when the noise power was less than the\npower received pilot. The proposed low complexity iterative algorithm offers\nmaximum energy efficiency by repeating a minimized pilot signal until the\noptimal antenna selection and transmission power are achieved.",
    "descriptor": "\nComments: 10Pages\n",
    "authors": [
      "Adeeb Salh",
      "Lukman Audah",
      "Nor Shahida Mohd Shah",
      "Qazwan Abdullah",
      "Noorsaliza Abdullah",
      "Jameel Mukred",
      "Shipun Hamzah"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03664"
  },
  {
    "id": "arXiv:2106.03665",
    "title": "Hierarchical Robot Navigation in Novel Environments using Rough 2-D Maps",
    "abstract": "In robot navigation, generalizing quickly to unseen environments is\nessential. Hierarchical methods inspired by human navigation have been\nproposed, typically consisting of a high-level landmark proposer and a\nlow-level controller. However, these methods either require precise high-level\ninformation to be given in advance or need to construct such guidance from\nextensive interaction with the environment. In this work, we propose an\napproach that leverages a rough 2-D map of the environment to navigate in novel\nenvironments without requiring further learning. In particular, we introduce a\ndynamic topological map that can be initialized from the rough 2-D map along\nwith a high-level planning approach for proposing reachable 2-D map patches of\nthe intermediate landmarks between the start and goal locations. To use\nproposed 2-D patches, we train a deep generative model to generate intermediate\nlandmarks in observation space which are used as subgoals by low-level\ngoal-conditioned reinforcement learning. Importantly, because the low-level\ncontroller is only trained with local behaviors (e.g. go across the\nintersection, turn left at a corner) on existing environments, this framework\nallows us to generalize to novel environments given only a rough 2-D map,\nwithout requiring further learning. Experimental results demonstrate the\neffectiveness of the proposed framework in both seen and novel environments.",
    "descriptor": "\nComments: 21 pages, Conference on Robot Learning 2020, Boston, MA\n",
    "authors": [
      "Chengguang Xu",
      "Christopher Amato",
      "Lawson L.S. Wong"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03665"
  },
  {
    "id": "arXiv:2106.03668",
    "title": "Recovery Analysis for Plug-and-Play Priors using the Restricted  Eigenvalue Condition",
    "abstract": "The plug-and-play priors (PnP) and regularization by denoising (RED) methods\nhave become widely used for solving inverse problems by leveraging pre-trained\ndeep denoisers as image priors. While the empirical imaging performance and the\ntheoretical convergence properties of these algorithms have been widely\ninvestigated, their recovery properties have not previously been theoretically\nanalyzed. We address this gap by showing how to establish theoretical recovery\nguarantees for PnP/RED by assuming that the solution of these methods lies near\nthe fixed-points of a deep neural network. We also present numerical results\ncomparing the recovery performance of PnP/RED in compressive sensing against\nthat of recent compressive sensing algorithms based on generative models. Our\nnumerical results suggest that PnP with a pre-trained artifact removal network\nprovides significantly better results compared to the existing state-of-the-art\nmethods.",
    "descriptor": "\nComments: 25 pages, 11 figures\n",
    "authors": [
      "Jiaming Liu",
      "M. Salman Asif",
      "Brendt Wohlberg",
      "Ulugbek S. Kamilov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03668"
  },
  {
    "id": "arXiv:2106.03669",
    "title": "Open source disease analysis system of cactus by artificial intelligence  and image processing",
    "abstract": "There is a growing interest in cactus cultivation because of numerous cacti\nuses from houseplants to food and medicinal applications. Various diseases\nimpact the growth of cacti. To develop an automated model for the analysis of\ncactus disease and to be able to quickly treat and prevent damage to the\ncactus. The Faster R-CNN and YOLO algorithm technique were used to analyze\ncactus diseases automatically distributed into six groups: 1) anthracnose, 2)\ncanker, 3) lack of care, 4) aphid, 5) rusts and 6) normal group. Based on the\nexperimental results the YOLOv5 algorithm was found to be more effective at\ndetecting and identifying cactus disease than the Faster R-CNN algorithm. Data\ntraining and testing with YOLOv5S model resulted in a precision of 89.7% and an\naccuracy (recall) of 98.5%, which is effective enough for further use in a\nnumber of applications in cactus cultivation. Overall the YOLOv5 algorithm had\na test time per image of only 26 milliseconds. Therefore, the YOLOv5 algorithm\nwas found to suitable for mobile applications and this model could be further\ndeveloped into a program for analyzing cactus disease.",
    "descriptor": "\nComments: Preprint for IAIT2021\n",
    "authors": [
      "Kanlayanee Kaweesinsakul",
      "Siranee Nuchitprasitchai",
      "Joshua M. Pearce"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03669"
  },
  {
    "id": "arXiv:2106.03673",
    "title": "Algorithms and Decision-Making in the Public Sector",
    "abstract": "This article surveys the use of algorithmic systems to support\ndecision-making in the public sector. Governments adopt, procure, and use\nalgorithmic systems to support their functions within several contexts --\nincluding criminal justice, education, and benefits provision -- with important\nconsequences for accountability, privacy, social inequity, and public\nparticipation in decision-making. We explore the social implications of\nmunicipal algorithmic systems across a variety of stages, including problem\nformulation, technology acquisition, deployment, and evaluation. We highlight\nseveral open questions that require further empirical research.",
    "descriptor": "",
    "authors": [
      "Karen Levy",
      "Kyla E. Chasalow",
      "Sarah Riley"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.03673"
  },
  {
    "id": "arXiv:2106.03679",
    "title": "The CESAW dataset: a conversation",
    "abstract": "An analysis of the 61,817 tasks performed by developers working on 45\nprojects, implemented using Team Software Process, is documented via a\nconversation between a data analyst and the person who collected, compiled, and\noriginally analyzed the data. Five projects were safety critical, containing a\ntotal of 28,899 tasks.\nProjects were broken down using a Work Breakdown Structure to create a\nhierarchical organization, with tasks at the leaf nodes. The WBS information\nenables task organization within a project to be investigated, e.g., how\nrelated tasks are sequenced together. Task data includes: kind of task,\nanonymous developer id, start/end time/date, as well as interruption and break\ntimes; a total of 203,621 time facts.\nTask effort estimation accuracy was found to be influenced by factors such as\nthe person making the estimate, the project involved, and the propensity to use\nround numbers.",
    "descriptor": "\nComments: 36 pages, 20 figures, all data available\n",
    "authors": [
      "Derek M. Jones",
      "William R. Nichols"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03679"
  },
  {
    "id": "arXiv:2106.03683",
    "title": "Towards a Multi-purpose Robotic Nursing Assistant",
    "abstract": "Robotic nursing aid is one of the heavily researched areas in robotics\nnowadays. Several robotic assistants exist that only focus on a specific\nfunction related to nurses assistance or functions related to patient aid.\nThere is a need for a unified system that not only performs tasks that would\nassist nurses and reduce their burden but also perform tasks that help a\npatient. In recent times, due to the COVID-19 pandemic, there is also an\nincrease in the need for robotic assistants that have teleoperation\ncapabilities to provide better protection against the virus spread. To address\nthese requirements, we propose a novel Multi-purpose Intelligent Nurse Aid\n(MINA) robotic system that is capable of providing walking assistance to the\npatients and perform teleoperation tasks with an easy-to-use and intuitive\nGraphical User Interface (GUI). This paper also presents preliminary results\nfrom the walking assistant task that improves upon the current state-of-the-art\nmethods and shows the developed GUI for teleoperation.",
    "descriptor": "\nComments: accepted at ICRA 2021 Workshop on No-Touch Care for Worker Safety During Pandemic Response\n",
    "authors": [
      "Krishna Chaitanya Kodur",
      "Kaustubh Rajpathak",
      "Akilesh Rajavenkatanarayanan",
      "Maria Kyrarini",
      "Fillia Makedon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03683"
  },
  {
    "id": "arXiv:2106.03684",
    "title": "Extending counterfactual accounts of intent to include oblique intent",
    "abstract": "One approach to defining Intention is to use the counterfactual tools\ndeveloped to define Causality. Direct Intention is considered the highest level\nof intent in the common law, and is a sufficient component for the most serious\ncrimes to be committed. Loosely defined it is the commission of actions to\nbring about a desired or targeted outcome. Direct Intention is not always\nnecessary for the most serious category of crimes because society has also\nfound it necessary to develop a theory of intention around side-effects, known\nas oblique intent or indirect intent. This is to prevent moral harms from going\nunpunished which were not the aim of the actor, but were natural consequences\nnevertheless. This paper uses a canonical example of a plane owner, planting a\nbomb on their own plane in order to collect insurance, to illustrate how two\naccounts of counterfactual intent do not conclude that murder of the plane's\npassengers and crew were directly intended. We extend both frameworks to\ninclude a definition of oblique intent developed in Ashton (2021)",
    "descriptor": "",
    "authors": [
      "Hal Ashton"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03684"
  },
  {
    "id": "arXiv:2106.03693",
    "title": "Increase and Conquer: Training Graph Neural Networks on Growing Graphs",
    "abstract": "Graph neural networks (GNNs) use graph convolutions to exploit network\ninvariances and learn meaningful features from network data. However, on\nlarge-scale graphs convolutions incur in high computational cost, leading to\nscalability limitations. Leveraging the graphon -- the limit object of a graph\n-- in this paper we consider the problem of learning a graphon neural network\n(WNN) -- the limit object of a GNN -- by training GNNs on graphs sampled\nBernoulli from the graphon. Under smoothness conditions, we show that: (i) the\nexpected distance between the learning steps on the GNN and on the WNN\ndecreases asymptotically with the size of the graph, and (ii) when training on\na sequence of growing graphs, gradient descent follows the learning direction\nof the WNN. Inspired by these results, we propose a novel algorithm to learn\nGNNs on large-scale graphs that, starting from a moderate number of nodes,\nsuccessively increases the size of the graph during training. This algorithm is\nbenchmarked on both a recommendation system and a decentralized control problem\nwhere it is shown to retain comparable performance, to its large-scale\ncounterpart, at a reduced computational cost.",
    "descriptor": "",
    "authors": [
      "Juan Cervino",
      "Luana Ruiz",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03693"
  },
  {
    "id": "arXiv:2106.03694",
    "title": "Detection of marine floating plastic using Sentinel-2 imagery and  machine learning models",
    "abstract": "The increasing level of marine plastic pollution poses severe threats to the\nmarine ecosystem and biodiversity. The present study attempted to explore the\nfull functionality of open Sentinel satellite data and ML models for detecting\nand classifying floating plastic debris in Mytilene (Greece), Limassol\n(Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support\nVector Machine (SVM) and Random Forest (RF) were utilized to carry out the\nclassification analysis. In-situ plastic location data was collected from the\ncontrol experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the\nsame was considered for training the models. Both remote sensing bands and\nspectral indices were used for developing the ML models. A spectral signature\nprofile for plastic was created for discriminating the floating plastic from\nother marine debris. A newly developed index, kernel Normalized Difference\nVegetation Index (kNDVI), was incorporated into the modelling to examine its\ncontribution to model performances. Both SVM and RF were performed well in five\nmodels and test case combinations. Among the two ML models, the highest\nperformance was measured for the RF. The inclusion of kNDVI was found effective\nand increased the model performances, reflected by high balanced accuracy\nmeasured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using\nthe best-performed model, an automated floating plastic detection system was\ndeveloped and tested in Calabria and Beirut. For both sites, the trained model\nhad detected the floating plastic with ~99% accuracy. Among the six predictors,\nthe FDI was found the most important variable for detecting marine floating\nplastic. These findings collectively suggest that high-resolution remote\nsensing imagery and the automated ML models can be an effective alternative for\nthe cost-effective detection of marine floating plastic.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Srikanta Sannigrahi",
      "Bidroha Basu",
      "Arunima Sarkar Basu",
      "Francesco Pilla"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03694"
  },
  {
    "id": "arXiv:2106.03699",
    "title": "Formalizing Distribution Inference Risks",
    "abstract": "Property inference attacks reveal statistical properties about a training set\nbut are difficult to distinguish from the primary purposes of statistical\nmachine learning, which is to produce models that capture statistical\nproperties about a distribution. Motivated by Yeom et al.'s membership\ninference framework, we propose a formal and generic definition of property\ninference attacks. The proposed notion describes attacks that can distinguish\nbetween possible training distributions, extending beyond previous property\ninference attacks that infer the ratio of a particular type of data in the\ntraining data set. In this paper, we show how our definition captures previous\nproperty inference attacks as well as a new attack that reveals the average\ndegree of nodes of a training graph and report on experiments giving insight\ninto the potential risks of property inference attacks.",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Anshuman Suri",
      "David Evans"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03699"
  },
  {
    "id": "arXiv:2106.03705",
    "title": "Deep Learning 3D Dose Prediction for Conventional Lung IMRT Using  Consistent/Unbiased Automated Plans",
    "abstract": "Deep learning (DL) 3D dose prediction has recently gained a lot of attention.\nHowever, the variability of plan quality in the training dataset, generated\nmanually by planners with wide range of expertise, can dramatically effect the\nquality of the final predictions. Moreover, any changes in the clinical\ncriteria requires a new set of manually generated plans by planners to build a\nnew prediction model. In this work, we instead use consistent plans generated\nby our in-house automated planning system (named ``ECHO'') to train the DL\nmodel. ECHO (expedited constrained hierarchical optimization) generates\nconsistent/unbiased plans by solving large-scale constrained optimization\nproblems sequentially. If the clinical criteria changes, a new training data\nset can be easily generated offline using ECHO, with no or limited human\nintervention, making the DL-based prediction model easily adaptable to the\nchanges in the clinical practice. We used 120 conventional lung patients (100\nfor training, 20 for testing) with different beam configurations and trained\nour DL-model using manually-generated as well as automated ECHO plans. We\nevaluated different inputs: (1) CT+(PTV/OAR)contours, and (2) CT+contours+beam\nconfigurations, and different loss functions: (1) MAE (mean absolute error),\nand (2) MAE+DVH (dose volume histograms). The quality of the predictions was\ncompared using different DVH metrics as well as dose-score and DVH-score,\nrecently introduced by the AAPM knowledge-based planning grand challenge. The\nbest results were obtained using automated ECHO plans and CT+contours+beam as\ntraining inputs and MAE+DVH as loss function.",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Navdeep Dahiya",
      "Gourav Jhanwar",
      "Anthony Yezzi",
      "Masoud Zarepisheh",
      "Saad Nadeem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03705"
  },
  {
    "id": "arXiv:2106.03706",
    "title": "A Comprehensive Assessment of Dialog Evaluation Metrics",
    "abstract": "Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 17\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.",
    "descriptor": "",
    "authors": [
      "Yi-Ting Yeh",
      "Maxine Eskenazi",
      "Shikib Mehri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03706"
  },
  {
    "id": "arXiv:2106.03710",
    "title": "Application of optimal spline subspaces for the removal of spurious  outliers in isogeometric discretizations",
    "abstract": "We show that isogeometric Galerkin discretizations of eigenvalue problems\nrelated to the Laplace operator subject to any standard type of homogeneous\nboundary conditions have no outliers in certain optimal spline subspaces that\npreserve full approximation power. Roughly speaking, these optimal subspaces\nare obtained from the full spline space defined on certain uniform knot\nsequences by imposing specific additional boundary conditions. The spline\nsubspaces of interest have been introduced in the literature some years ago\nwhen proving their optimality with respect to Kolmogorov $n$-widths in\n$L^2$-norm for some function classes. The eigenfunctions of the Laplacian --\nwith any standard type of homogeneous boundary conditions -- belong to such\nclasses. Here, we complete the analysis of the approximation properties of\nthese optimal spline subspaces. In particular, we provide explicit $L^2$ and\n$H^1$ error estimates for Ritz projectors in the univariate and in the\nmultivariate tensor-product setting. Besides their intrinsic interest, these\nestimates imply that, for a fixed number of degrees of freedom, all the\neigenfunctions and the corresponding eigenvalues are well approximated, without\nloss of accuracy in any frequency. Thus, there are no spurious numerical values\nin the approximated spectrum. In other words, the considered subspaces provide\nfully accurate outlier-free discretizations in the univariate and in the\nmultivariate tensor-product case. This main contribution is complemented by an\nexplicit construction of B-spline-like bases for the considered spline\nsubspaces. Their role as discretization spaces for addressing general problems\nwith non-homogeneous boundary behavior is discussed as well.",
    "descriptor": "\nComments: 41 pages, 18 figures\n",
    "authors": [
      "Carla Manni",
      "Espen Sande",
      "Hendrik Speleers"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03710"
  },
  {
    "id": "arXiv:2106.03714",
    "title": "Refiner: Refining Self-attention for Vision Transformers",
    "abstract": "Vision Transformers (ViTs) have shown competitive accuracy in image\nclassification tasks compared with CNNs. Yet, they generally require much more\ndata for model pre-training. Most of recent works thus are dedicated to\ndesigning more complex architectures or training methods to address the\ndata-efficiency issue of ViTs. However, few of them explore improving the\nself-attention mechanism, a key factor distinguishing ViTs from CNNs. Different\nfrom existing works, we introduce a conceptually simple scheme, called refiner,\nto directly refine the self-attention maps of ViTs. Specifically, refiner\nexplores attention expansion that projects the multi-head attention maps to a\nhigher-dimensional space to promote their diversity. Further, refiner applies\nconvolutions to augment local patterns of the attention maps, which we show is\nequivalent to a distributed local attention features are aggregated locally\nwith learnable kernels and then globally aggregated with self-attention.\nExtensive experiments demonstrate that refiner works surprisingly well.\nSignificantly, it enables ViTs to achieve 86% top-1 classification accuracy on\nImageNet with only 81M parameters.",
    "descriptor": "",
    "authors": [
      "Daquan Zhou",
      "Yujun Shi",
      "Bingyi Kang",
      "Weihao Yu",
      "Zihang Jiang",
      "Yuan Li",
      "Xiaojie Jin",
      "Qibin Hou",
      "Jiashi Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03714"
  },
  {
    "id": "arXiv:2106.03717",
    "title": "Diverse Pretrained Context Encodings Improve Document Translation",
    "abstract": "We propose a new architecture for adapting a sentence-level\nsequence-to-sequence transformer by incorporating multiple pretrained document\ncontext signals and assess the impact on translation performance of (1)\ndifferent pretraining approaches for generating these signals, (2) the quantity\nof parallel data for which document context is available, and (3) conditioning\non source, target, or source and target contexts. Experiments on the NIST\nChinese-English, and IWSLT and WMT English-German tasks support four general\nconclusions: that using pretrained context representations markedly improves\nsample efficiency, that adequate parallel data resources are crucial for\nlearning to use document context, that jointly conditioning on multiple context\nrepresentations outperforms any single representation, and that source context\nis more valuable for translation performance than target side context. Our best\nmulti-context model consistently outperforms the best existing context-aware\ntransformers.",
    "descriptor": "",
    "authors": [
      "Domenic Donato",
      "Lei Yu",
      "Chris Dyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03717"
  },
  {
    "id": "arXiv:2106.03719",
    "title": "Incremental False Negative Detection for Contrastive Learning",
    "abstract": "Self-supervised learning has recently shown great potential in vision tasks\nvia contrastive learning, which aims to discriminate each image, or instance,\nin the dataset. However, such instance-level learning ignores the semantic\nrelationship between instances and repels the anchor equally from the\nsemantically similar samples, termed as false negatives. In this work, we first\nempirically highlight that the unfavorable effect from false negatives is more\nsignificant for the datasets containing images with more semantic concepts. To\naddress the issue, we introduce a novel incremental false negative detection\nfor self-supervised contrastive learning. Following the training process, when\nthe encoder is gradually better-trained and the embedding space becomes more\nsemantically structural, our method incrementally detects more reliable false\nnegatives. Subsequently, during contrastive learning, we discuss two strategies\nto explicitly remove the detected false negatives. Extensive experiments show\nthat our proposed method outperforms other self-supervised contrastive learning\nframeworks on multiple benchmarks within a limited compute.",
    "descriptor": "\nComments: Submitted to NeurIPS 2021. Code: this https URL\n",
    "authors": [
      "Tsai-Shien Chen",
      "Wei-Chih Hung",
      "Hung-Yu Tseng",
      "Shao-Yi Chien",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03719"
  },
  {
    "id": "arXiv:2106.03720",
    "title": "Person Re-Identification with a Locally Aware Transformer",
    "abstract": "Person Re-Identification is an important problem in computer vision-based\nsurveillance applications, in which the same person is attempted to be\nidentified from surveillance photographs in a variety of nearby zones. At\npresent, the majority of Person re-ID techniques are based on Convolutional\nNeural Networks (CNNs), but Vision Transformers are beginning to displace pure\nCNNs for a variety of object recognition tasks. The primary output of a vision\ntransformer is a global classification token, but vision transformers also\nyield local tokens which contain additional information about local regions of\nthe image. Techniques to make use of these local tokens to improve\nclassification accuracy are an active area of research. We propose a novel\nLocally Aware Transformer (LA-Transformer) that employs a Parts-based\nConvolution Baseline (PCB)-inspired strategy for aggregating globally enhanced\nlocal classification tokens into an ensemble of $\\sqrt{N}$ classifiers, where\n$N$ is the number of patches. An additional novelty is that we incorporate\nblockwise fine-tuning which further improves re-ID accuracy. LA-Transformer\nwith blockwise fine-tuning achieves rank-1 accuracy of $98.27 \\%$ with standard\ndeviation of $0.13$ on the Market-1501 and $98.7\\%$ with standard deviation of\n$0.2$ on the CUHK03 dataset respectively, outperforming all other\nstate-of-the-art published methods at the time of writing.",
    "descriptor": "\nComments: 10 pages, 2 figure, submitted to NurIPS 2021\n",
    "authors": [
      "Charu Sharma",
      "Siddhant R. Kapil",
      "David Chapman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03720"
  },
  {
    "id": "arXiv:2106.03721",
    "title": "OoD-Bench: Benchmarking and Understanding Out-of-Distribution  Generalization Datasets and Algorithms",
    "abstract": "Deep learning has achieved tremendous success with independent and\nidentically distributed (i.i.d.) data. However, the performance of neural\nnetworks often degenerates drastically when encountering out-of-distribution\n(OoD) data, i.e., training and test data are sampled from different\ndistributions. While a plethora of algorithms has been proposed to deal with\nOoD generalization, our understanding of the data used to train and evaluate\nthese algorithms remains stagnant. In this work, we position existing datasets\nand algorithms from various research areas (e.g., domain generalization, stable\nlearning, invariant risk minimization) seemingly unconnected into the same\ncoherent picture. First, we identify and measure two distinct kinds of\ndistribution shifts that are ubiquitous in various datasets. Next, we compare\nvarious OoD generalization algorithms with a new benchmark dominated by the two\ndistribution shifts. Through extensive experiments, we show that existing OoD\nalgorithms that outperform empirical risk minimization on one distribution\nshift usually have limitations on the other distribution shift. The new\nbenchmark may serve as a strong foothold that can be resorted to by future OoD\ngeneralization research.",
    "descriptor": "",
    "authors": [
      "Nanyang Ye",
      "Kaican Li",
      "Lanqing Hong",
      "Haoyue Bai",
      "Yiting Chen",
      "Fengwei Zhou",
      "Zhenguo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03721"
  },
  {
    "id": "arXiv:2106.03722",
    "title": "Error Loss Networks",
    "abstract": "A novel model called error loss network (ELN) is proposed to build an error\nloss function for supervised learning. The ELN is in structure similar to a RBF\nneural network, but its input is an error sample and output is a loss\ncorresponding to that error sample. That means the nonlinear input-output\nmapper of ELN creates an error loss function. The proposed ELN provides a\nunified model for a large class of error loss functions, which includes some\ninformation theoretic learning (ITL) loss functions as special cases. The\nactivation function, weight parameters and network size of the ELN can be\npredetermined or learned from the error samples. On this basis, we propose a\nnew machine learning paradigm where the learning process is divided into two\nstages: first, learning a loss function using an ELN; second, using the learned\nloss function to continue to perform the learning. Experimental results are\npresented to demonstrate the desirable performance of the new method.",
    "descriptor": "",
    "authors": [
      "Badong Chen",
      "Yunfei Zheng",
      "Pengju Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03722"
  },
  {
    "id": "arXiv:2106.03723",
    "title": "Self-Supervised Graph Learning with Proximity-based Views and Channel  Contrast",
    "abstract": "We consider graph representation learning in a self-supervised manner. Graph\nneural networks (GNNs) use neighborhood aggregation as a core component that\nresults in feature smoothing among nodes in proximity. While successful in\nvarious prediction tasks, such a paradigm falls short of capturing nodes'\nsimilarities over a long distance, which proves to be important for\nhigh-quality learning. To tackle this problem, we strengthen the graph with two\nadditional graph views, in which nodes are directly linked to those with the\nmost similar features or local structures. Not restricted by connectivity in\nthe original graph, the generated views allow the model to enhance its\nexpressive power with new and complementary perspectives from which to look at\nthe relationship between nodes. Following a contrastive learning approach, We\npropose a method that aims to maximize the agreement between representations\nacross generated views and the original graph. We also propose a channel-level\ncontrast approach that greatly reduces computation cost, compared to the\ncommonly used node level contrast, which requires computation cost quadratic in\nthe number of nodes. Extensive experiments on seven assortative graphs and four\ndisassortative graphs demonstrate the effectiveness of our approach.",
    "descriptor": "\nComments: 17 pages, 8 figures\n",
    "authors": [
      "Wei Zhuo",
      "Guang Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03723"
  },
  {
    "id": "arXiv:2106.03724",
    "title": "Truthful allocation in graphs and hypergraphs",
    "abstract": "We study truthful mechanisms for allocation problems in graphs, both for the\nminimization (i.e., scheduling) and maximization (i.e., auctions) setting. The\nminimization problem is a special case of the well-studied unrelated machines\nscheduling problem, in which every given task can be executed only by two\npre-specified machines in the case of graphs or a given subset of machines in\nthe case of hypergraphs. This corresponds to a multigraph whose nodes are the\nmachines and its hyperedges are the tasks. This class of problems belongs to\nmultidimensional mechanism design, for which there are no known general\nmechanisms other than the VCG and its generalization to affine minimizers. We\npropose a new class of mechanisms that are truthful and have significantly\nbetter performance than affine minimizers in many settings. Specifically, we\nprovide upper and lower bounds for truthful mechanisms for general multigraphs,\nas well as special classes of graphs such as stars, trees, planar graphs,\n$k$-degenerate graphs, and graphs of a given treewidth. We also consider the\nobjective of minimizing or maximizing the $L^p$-norm of the values of the\nplayers, a generalization of the makespan minimization that corresponds to\n$p=\\infty$, and extend the results to any $p>0$.",
    "descriptor": "",
    "authors": [
      "George Christodoulou",
      "Elias Koutsoupias",
      "Annamaria Kovacs"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03724"
  },
  {
    "id": "arXiv:2106.03725",
    "title": "Stability of Manifold Neural Networks to Deformations",
    "abstract": "Stability is an important property of graph neural networks (GNNs) which\nexplains their success in many problems of practical interest. Existing GNN\nstability results depend on the size of the graph, restricting applicability to\ngraphs of moderate size. To understand the stability properties of GNNs on\nlarge graphs, we consider neural networks supported on manifolds. These are\ndefined in terms of manifold diffusions mediated by the Laplace-Beltrami (LB)\noperator and are interpreted as limits of GNNs running on graphs of growing\nsize. We define manifold deformations and show that they lead to perturbations\nof the manifold's LB operator that consist of an absolute and a relative\nperturbation term. We then define filters that split the infinite dimensional\nspectrum of the LB operator in finite partitions, and prove that manifold\nneural networks (MNNs) with these filters are stable to both, absolute and\nrelative perturbations of the LB operator. Stability results are illustrated\nnumerically in resource allocation problems in wireless networks.",
    "descriptor": "\nComments: 18 pages, 4 figures\n",
    "authors": [
      "Zhiyang Wang",
      "Luana Ruiz",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03725"
  },
  {
    "id": "arXiv:2106.03730",
    "title": "Encouraging Neural Machine Translation to Satisfy Terminology  Constraints",
    "abstract": "We present a new approach to encourage neural machine translation to satisfy\nlexical constraints. Our method acts at the training step and thereby avoiding\nthe introduction of any extra computational overhead at inference step. The\nproposed method combines three main ingredients. The first one consists in\naugmenting the training data to specify the constraints. Intuitively, this\nencourages the model to learn a copy behavior when it encounters constraint\nterms. Compared to previous work, we use a simplified augmentation strategy\nwithout source factors. The second ingredient is constraint token masking,\nwhich makes it even easier for the model to learn the copy behavior and\ngeneralize better. The third one, is a modification of the standard cross\nentropy loss to bias the model towards assigning high probabilities to\nconstraint words. Empirical results show that our method improves upon related\nbaselines in terms of both BLEU score and the percentage of generated\nconstraint terms.",
    "descriptor": "",
    "authors": [
      "Melissa Ailem",
      "Jinghsu Liu",
      "Raheel Qader"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03730"
  },
  {
    "id": "arXiv:2106.03731",
    "title": "Approximation of solutions of DDEs under nonstandard assumptions via  Euler scheme",
    "abstract": "We deal with approximation of solutions of delay differential equations\n(DDEs) via the classical Euler algorithm. We investigate the pointwise error of\nthe Euler scheme under nonstandard assumptions imposed on the right-hand side\nfunction $f$. Namely, we assume that $f$ is globally of at most linear growth,\nsatisfies globally one-side Lipschitz condition but it is only locally H\\\"older\ncontinuous. We provide a detailed error analysis of the Euler algorithm under\nsuch nonstandard regularity conditions. Moreover, we report results of\nnumerical experiments.",
    "descriptor": "\nComments: 19 pages, 6 figures\n",
    "authors": [
      "Natalia Czy\u017cewska",
      "Pawe\u0142 M. Morkisz",
      "Pawe\u0142 Przyby\u0142owicz"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03731"
  },
  {
    "id": "arXiv:2106.03732",
    "title": "Extending Reference Broadcast Infrastructure Synchronization Protocol in  IEEE 802.11 as Enabler for the IIoT",
    "abstract": "Realizing the industrial Internet of Things, more andmore mobile use cases\nwill emerge in the industrial landscape,requiring both novel concepts and\nsmooth integration into legacydeployments.Since accurate time synchronization\nis particularly challengingfor wireless devices, we propose a concept for\nsimple but accuratesynchronization in IEEE 802.11 wireless local area network\nthatextends the Reference Broadcast Infrastructure Synchronizationprotocol, and\na suitable integration of IEEE 802.1AS that is partof the IEEE time-sensitive\nnetworking standards. In addition,the concept is evaluated with a testbed using\ncommercial off-the-shelf hardware and a realistic discrete automation\ndemonstratorequipped mostly with industrial components. By using the\nafore-mentioned devices for wireless communications, this concept canbe\ndirectly applied in existing industrial solutions, thus achievingthe proposed\nresults. It is shown that the achieved synchronicity issuitable for a wide\nrange of mandatory mobile use cases, which aremost important for a fully\nfunctional industrial Internet of Things.",
    "descriptor": "",
    "authors": [
      "Michael Gundall",
      "Christopher Huber",
      "Sergiy Melnyk",
      "Hans D. Schotten"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03732"
  },
  {
    "id": "arXiv:2106.03734",
    "title": "Reveal of Vision Transformers Robustness against Adversarial Attacks",
    "abstract": "Attention-based networks have achieved state-of-the-art performance in many\ncomputer vision tasks, such as image classification. Unlike Convolutional\nNeural Network (CNN), the major part of the vanilla Vision Transformer (ViT) is\nthe attention block that brings the power of mimicking the global context of\nthe input image. This power is data hunger and hence, the larger the training\ndata the better the performance. To overcome this limitation, many ViT-based\nnetworks, or hybrid-ViT, have been proposed to include local context during the\ntraining. The robustness of ViTs and its variants against adversarial attacks\nhas not been widely invested in the literature. Some robustness attributes were\nrevealed in few previous works and hence, more insight robustness attributes\nare yet unrevealed. This work studies the robustness of ViT variants 1) against\ndifferent $L_p$-based adversarial attacks in comparison with CNNs and 2) under\nAdversarial Examples (AEs) after applying preprocessing defense methods. To\nthat end, we run a set of experiments on 1000 images from ImageNet-1k and then\nprovide an analysis that reveals that vanilla ViT or hybrid-ViT are more robust\nthan CNNs. For instance, we found that 1) Vanilla ViTs or hybrid-ViTs are more\nrobust than CNNs under $L_0$, $L_1$, $L_2$, $L_\\infty$-based, and Color Channel\nPerturbations (CCP) attacks. 2) Vanilla ViTs are not responding to\npreprocessing defenses that mainly reduce the high frequency components while,\nhybrid-ViTs are more responsive to such defense. 3) CCP can be used as a\npreprocessing defense and larger ViT variants are found to be more responsive\nthan other models. Furthermore, feature maps, attention maps, and Grad-CAM\nvisualization jointly with image quality measures, and perturbations' energy\nspectrum are provided for an insight understanding of attention-based models.",
    "descriptor": "",
    "authors": [
      "Ahmed Aldahdooh",
      "Wassim Hamidouche",
      "Olivier Deforges"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03734"
  },
  {
    "id": "arXiv:2106.03738",
    "title": "Unsupervised Action Segmentation for Instructional Videos",
    "abstract": "In this paper we address the problem of automatically discovering atomic\nactions in unsupervised manner from instructional videos, which are rarely\nannotated with atomic actions. We present an unsupervised approach to learn\natomic actions of structured human tasks from a variety of instructional videos\nbased on a sequential stochastic autoregressive model for temporal segmentation\nof videos. This learns to represent and discover the sequential relationship\nbetween different atomic actions of the task, and which provides automatic and\nunsupervised self-labeling.",
    "descriptor": "\nComments: 4 page abstract for LUV workshop\n",
    "authors": [
      "AJ Piergiovanni",
      "Anelia Angelova",
      "Michael S. Ryoo",
      "Irfan Essa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03738"
  },
  {
    "id": "arXiv:2106.03743",
    "title": "Proxy-Normalizing Activations to Match Batch Normalization while  Removing Batch Dependence",
    "abstract": "We investigate the reasons for the performance degradation incurred with\nbatch-independent normalization. We find that the prototypical techniques of\nlayer normalization and instance normalization both induce the appearance of\nfailure modes in the neural network's pre-activations: (i) layer normalization\ninduces a collapse towards channel-wise constant functions; (ii) instance\nnormalization induces a lack of variability in instance statistics, symptomatic\nof an alteration of the expressivity. To alleviate failure mode (i) without\naggravating failure mode (ii), we introduce the technique \"Proxy Normalization\"\nthat normalizes post-activations using a proxy distribution. When combined with\nlayer normalization or group normalization, this batch-independent\nnormalization emulates batch normalization's behavior and consistently matches\nor exceeds its performance.",
    "descriptor": "",
    "authors": [
      "Antoine Labatie",
      "Dominic Masters",
      "Zach Eaton-Rosen",
      "Carlo Luschi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03743"
  },
  {
    "id": "arXiv:2106.03746",
    "title": "Efficient Training of Visual Transformers with Small-Size Datasets",
    "abstract": "Visual Transformers (VTs) are emerging as an architectural paradigm\nalternative to Convolutional networks (CNNs). Differently from CNNs, VTs can\ncapture global relations between image elements and they potentially have a\nlarger representation capacity. However, the lack of the typical convolutional\ninductive bias makes these models more data-hungry than common CNNs. In fact,\nsome local properties of the visual domain which are embedded in the CNN\narchitectural design, in VTs should be learned from samples. In this paper, we\nempirically analyse different VTs, comparing their robustness in a small\ntraining-set regime, and we show that, despite having a comparable accuracy\nwhen trained on ImageNet, their performance on smaller datasets can be largely\ndifferent. Moreover, we propose a self-supervised task which can extract\nadditional information from images with only a negligible computational\noverhead. This task encourages the VTs to learn spatial relations within an\nimage and makes the VT training much more robust when training data are scarce.\nOur task is used jointly with the standard (supervised) training and it does\nnot depend on specific architectural choices, thus it can be easily plugged in\nthe existing VTs. Using an extensive evaluation with different VTs and\ndatasets, we show that our method can improve (sometimes dramatically) the\nfinal accuracy of the VTs. The code will be available upon acceptance.",
    "descriptor": "",
    "authors": [
      "Yahui Liu",
      "Enver Sangineto",
      "Wei Bi",
      "Nicu Sebe",
      "Bruno Lepri",
      "Marco De Nadai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03746"
  },
  {
    "id": "arXiv:2106.03748",
    "title": "Towards robust and domain agnostic reinforcement learning competitions",
    "abstract": "Reinforcement learning competitions have formed the basis for standard\nresearch benchmarks, galvanized advances in the state-of-the-art, and shaped\nthe direction of the field. Despite this, a majority of challenges suffer from\nthe same fundamental problems: participant solutions to the posed challenge are\nusually domain-specific, biased to maximally exploit compute resources, and not\nguaranteed to be reproducible. In this paper, we present a new framework of\ncompetition design that promotes the development of algorithms that overcome\nthese barriers. We propose four central mechanisms for achieving this end:\nsubmission retraining, domain randomization, desemantization through domain\nobfuscation, and the limitation of competition compute and environment-sample\nbudget. To demonstrate the efficacy of this design, we proposed, organized, and\nran the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning. In\nthis work, we describe the organizational outcomes of the competition and show\nthat the resulting participant submissions are reproducible, non-specific to\nthe competition environment, and sample/resource efficient, despite the\ndifficult competition task.",
    "descriptor": "\nComments: 20 pages, several figures, published PMLR\n",
    "authors": [
      "William Hebgen Guss",
      "Stephanie Milani",
      "Nicholay Topin",
      "Brandon Houghton",
      "Sharada Mohanty",
      "Andrew Melnik",
      "Augustin Harter",
      "Benoit Buschmaas",
      "Bjarne Jaster",
      "Christoph Berganski",
      "Dennis Heitkamp",
      "Marko Henning",
      "Helge Ritter",
      "Chengjie Wu",
      "Xiaotian Hao",
      "Yiming Lu",
      "Hangyu Mao",
      "Yihuan Mao",
      "Chao Wang",
      "Michal Opanowicz",
      "Anssi Kanervisto",
      "Yanick Schraner",
      "Christian Scheller",
      "Xiren Zhou",
      "Lu Liu",
      "Daichi Nishio",
      "Toi Tsuneda",
      "Karolis Ramanauskas",
      "Gabija Juceviciute"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03748"
  },
  {
    "id": "arXiv:2106.03750",
    "title": "Smart Village: An IoT Based Digital Transformation",
    "abstract": "Almost 46% of the world's population resides in a rural landscape. Smart\nvillages, alongside smart cities, are in need of time for future economic\ngrowth, improved agriculture, better health, and education. The smart village\nis a concept that improves the traditional rural aspects with the help of\ndigital transformation. The smart village is built up using heterogeneous\ndigital technologies pillared around the Internet-of-Thing (IoT). There exist\nmany opportunities in research to design a low-cost, secure, and efficient\ntechnical ecosystem. This article identifies the key application areas, where\nthe IoT can be applied in the smart village. The article also presents a\ncomparative study of communication technology options.",
    "descriptor": "\nComments: 5 Pages, Conference: IEEE 7th World Forum on Internet of Things (WF-IoT), New Orleans, June 2021\n",
    "authors": [
      "Amit Degada",
      "Himanshu Thapliyal",
      "Saraju P. Mohanty"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.03750"
  },
  {
    "id": "arXiv:2106.03753",
    "title": "Energy-Efficient Naming in Beeping Networks",
    "abstract": "A single-hop beeping network is a distributed communication model in which\nall stations can communicate with one another by transmitting only one-bit\nmessages, called beeps. This paper focuses on resolving the distributed\ncomputing area's two fundamental problems: naming and counting problems. We are\nparticularly interested in optimizing the energy complexity and the running\ntime of algorithms to resolve these problems. Our contribution is to design\nrandomized algorithms with an optimal running time of O(n log n) and an energy\ncomplexity of O(log n) for both the naming and counting problems on single-hop\nbeeping networks of n stations.",
    "descriptor": "",
    "authors": [
      "Ny Aina Andriambolamalala",
      "Vlady Ravelomanana"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03753"
  },
  {
    "id": "arXiv:2106.03755",
    "title": "HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate  Segmentation",
    "abstract": "Superpixels serve as a powerful preprocessing tool in many computer vision\ntasks. By using superpixel representation, the number of image primitives can\nbe largely reduced by orders of magnitudes. The majority of superpixel methods\nuse handcrafted features, which usually do not translate well into strong\nadherence to object boundaries. A few recent superpixel methods have introduced\ndeep learning into the superpixel segmentation process. However, none of these\nmethods is able to produce superpixels in near real-time, which is crucial to\nthe applicability of a superpixel method in practice. In this work, we propose\na two-stage graph-based framework for superpixel segmentation. In the first\nstage, we introduce an efficient Deep Affinity Learning (DAL) network that\nlearns pairwise pixel affinities by aggregating multi-scale information. In the\nsecond stage, we propose a highly efficient superpixel method called\nHierarchical Entropy Rate Segmentation (HERS). Using the learned affinities\nfrom the first stage, HERS builds a hierarchical tree structure that can\nproduce any number of highly adaptive superpixels instantaneously. We\ndemonstrate, through visual and numerical experiments, the effectiveness and\nefficiency of our method compared to various state-of-the-art superpixel\nmethods.",
    "descriptor": "",
    "authors": [
      "Hankui Peng",
      "Angelica I. Aviles-Rivero",
      "Carola-Bibiane Schonlieb"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03755"
  },
  {
    "id": "arXiv:2106.03760",
    "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with  Applications to Multi-Task Learning",
    "abstract": "The Mixture-of-experts (MoE) architecture is showing promising results in\nmulti-task learning (MTL) and in scaling high-capacity neural networks.\nState-of-the-art MoE models use a trainable sparse gate to select a subset of\nthe experts for each input example. While conceptually appealing, existing\nsparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to\nconvergence and statistical performance issues when training with\ngradient-based methods. In this paper, we develop DSelect-k: the first,\ncontinuously differentiable and sparse gate for MoE, based on a novel binary\nencoding formulation. Our gate can be trained using first-order methods, such\nas stochastic gradient descent, and offers explicit control over the number of\nexperts to select. We demonstrate the effectiveness of DSelect-k in the context\nof MTL, on both synthetic and real datasets with up to 128 tasks. Our\nexperiments indicate that MoE models based on DSelect-k can achieve\nstatistically significant improvements in predictive and expert selection\nperformance. Notably, on a real-world large-scale recommender system, DSelect-k\nachieves over 22% average improvement in predictive performance compared to the\nTop-k gate. We provide an open-source TensorFlow implementation of our gate.",
    "descriptor": "",
    "authors": [
      "Hussein Hazimeh",
      "Zhe Zhao",
      "Aakanksha Chowdhery",
      "Maheswaran Sathiamoorthy",
      "Yihua Chen",
      "Rahul Mazumder",
      "Lichan Hong",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03760"
  },
  {
    "id": "arXiv:2106.03761",
    "title": "Bias Mitigation of Face Recognition Models Through Calibration",
    "abstract": "Face recognition models suffer from bias: for example, the probability of a\nfalse positive (incorrect face match) strongly depends on sensitive attributes\nlike ethnicity. As a result, these models may disproportionately and negatively\nimpact minority groups when used in law enforcement. In this work, we introduce\nthe Bias Mitigation Calibration (BMC) method, which (i) increases model\naccuracy (improving the state-of-the-art), (ii) produces fairly-calibrated\nprobabilities, (iii) significantly reduces the gap in the false positive rates,\nand (iv) does not require knowledge of the sensitive attribute.",
    "descriptor": "\nComments: 22 pages, 20 tables, 13 figures\n",
    "authors": [
      "Tiago Salvador",
      "Stephanie Cairns",
      "Vikram Voleti",
      "Noah Marshall",
      "Adam Oberman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03761"
  },
  {
    "id": "arXiv:2106.03763",
    "title": "Vanishing Curvature and the Power of Adaptive Methods in Randomly  Initialized Deep Networks",
    "abstract": "This paper revisits the so-called vanishing gradient phenomenon, which\ncommonly occurs in deep randomly initialized neural networks. Leveraging an\nin-depth analysis of neural chains, we first show that vanishing gradients\ncannot be circumvented when the network width scales with less than O(depth),\neven when initialized with the popular Xavier and He initializations. Second,\nwe extend the analysis to second-order derivatives and show that random i.i.d.\ninitialization also gives rise to Hessian matrices with eigenspectra that\nvanish as networks grow in depth. Whenever this happens, optimizers are\ninitialized in a very flat, saddle point-like plateau, which is particularly\nhard to escape with stochastic gradient descent (SGD) as its escaping time is\ninversely related to curvature. We believe that this observation is crucial for\nfully understanding (a) historical difficulties of training deep nets with\nvanilla SGD, (b) the success of adaptive gradient methods (which naturally\nadapt to curvature and thus quickly escape flat plateaus) and (c) the\neffectiveness of modern architectural components like residual connections and\nnormalization layers.",
    "descriptor": "",
    "authors": [
      "Antonio Orvieto",
      "Jonas Kohler",
      "Dario Pavllo",
      "Thomas Hofmann",
      "Aurelien Lucchi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03763"
  },
  {
    "id": "arXiv:2106.03764",
    "title": "On the Expressive Power of Self-Attention Matrices",
    "abstract": "Transformer networks are able to capture patterns in data coming from many\ndomains (text, images, videos, proteins, etc.) with little or no change to\narchitecture components. We perform a theoretical analysis of the core\ncomponent responsible for signal propagation between elements, i.e. the\nself-attention matrix. In practice, this matrix typically exhibits two\nproperties: (1) it is sparse, meaning that each token only attends to a small\nsubset of other tokens; and (2) it changes dynamically depending on the input\nto the module. With these considerations in mind, we ask the following\nquestion: Can a fixed self-attention module approximate arbitrary sparse\npatterns depending on the input? How small is the hidden size $d$ required for\nsuch approximation? We make progress in answering this question and show that\nthe self-attention matrix can provably approximate sparse matrices, where\nsparsity is in terms of a bounded number of nonzero elements in each row and\ncolumn. While the parameters of self-attention are fixed, various sparse\nmatrices can be approximated by only modifying the inputs. Our proof is based\non the random projection technique and uses the seminal Johnson-Lindenstrauss\nlemma. Our proof is constructive, enabling us to propose an algorithm for\nfinding adaptive inputs and fixed self-attention parameters in order to\napproximate a given matrix. In particular, we show that, in order to\napproximate any sparse matrix up to a given precision defined in terms of\npreserving matrix element ratios, $d$ grows only logarithmically with the\nsequence length $L$ (i.e. $d = O(\\log L)$).",
    "descriptor": "",
    "authors": [
      "Valerii Likhosherstov",
      "Krzysztof Choromanski",
      "Adrian Weller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03764"
  },
  {
    "id": "arXiv:2106.03770",
    "title": "Few-Shot Unsupervised Image-to-Image Translation on complex scenes",
    "abstract": "Unsupervised image-to-image translation methods have received a lot of\nattention in the last few years. Multiple techniques emerged tackling the\ninitial challenge from different perspectives. Some focus on learning as much\nas possible from several target style images for translations while other make\nuse of object detection in order to produce more realistic results on\ncontent-rich scenes. In this work, we assess how a method that has initially\nbeen developed for single object translation performs on more diverse and\ncontent-rich images. Our work is based on the FUNIT[1] framework and we train\nit with a more diverse dataset. This helps understanding how such method\nbehaves beyond their initial frame of application. We present a way to extend a\ndataset based on object detection. Moreover, we propose a way to adapt the\nFUNIT framework in order to leverage the power of object detection that one can\nsee in other methods.",
    "descriptor": "",
    "authors": [
      "Luca Barras",
      "Samuel Chassot",
      "Daniel Filipe Nunes Silva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03770"
  },
  {
    "id": "arXiv:2106.03772",
    "title": "Learning Dynamics via Graph Neural Networks for Human Pose Estimation  and Tracking",
    "abstract": "Multi-person pose estimation and tracking serve as crucial steps for video\nunderstanding. Most state-of-the-art approaches rely on first estimating poses\nin each frame and only then implementing data association and refinement.\nDespite the promising results achieved, such a strategy is inevitably prone to\nmissed detections especially in heavily-cluttered scenes, since this\ntracking-by-detection paradigm is, by nature, largely dependent on visual\nevidences that are absent in the case of occlusion. In this paper, we propose a\nnovel online approach to learning the pose dynamics, which are independent of\npose detections in current fame, and hence may serve as a robust estimation\neven in challenging scenarios including occlusion. Specifically, we derive this\nprediction of dynamics through a graph neural network~(GNN) that explicitly\naccounts for both spatial-temporal and visual information. It takes as input\nthe historical pose tracklets and directly predicts the corresponding poses in\nthe following frame for each tracklet. The predicted poses will then be\naggregated with the detected poses, if any, at the same frame so as to produce\nthe final pose, potentially recovering the occluded joints missed by the\nestimator. Experiments on PoseTrack 2017 and PoseTrack 2018 datasets\ndemonstrate that the proposed method achieves results superior to the state of\nthe art on both human pose estimation and tracking tasks.",
    "descriptor": "\nComments: Accepted by CVPR 2021\n",
    "authors": [
      "Yiding Yang",
      "Zhou Ren",
      "Haoxiang Li",
      "Chunluan Zhou",
      "Xinchao Wang",
      "Gang Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03772"
  },
  {
    "id": "arXiv:2106.03774",
    "title": "Digital Taxonomist: Identifying Plant Species in Citizen Scientists'  Photographs",
    "abstract": "Automatic identification of plant specimens from amateur photographs could\nimprove species range maps, thus supporting ecosystems research as well as\nconservation efforts. However, classifying plant specimens based on image data\nalone is challenging: some species exhibit large variations in visual\nappearance, while at the same time different species are often visually\nsimilar; additionally, species observations follow a highly imbalanced,\nlong-tailed distribution due to differences in abundance as well as observer\nbiases. On the other hand, most species observations are accompanied by side\ninformation about the spatial, temporal and ecological context. Moreover,\nbiological species are not an unordered list of classes but embedded in a\nhierarchical taxonomic structure. We propose a machine learning model that\ntakes into account these additional cues in a unified framework. Our Digital\nTaxonomist is able to identify plant species in photographs more correctly.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Riccardo de Lutio",
      "Yihang She",
      "Stefano D'Aronco",
      "Stefania Russo",
      "Philipp Brun",
      "Jan D. Wegner",
      "Konrad Schindler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03774"
  },
  {
    "id": "arXiv:2106.03775",
    "title": "Explainable Artificial Intelligence (XAI) for Increasing User Trust in  Deep Reinforcement Learning Driven Autonomous Systems",
    "abstract": "We consider the problem of providing users of deep Reinforcement Learning\n(RL) based systems with a better understanding of when their output can be\ntrusted. We offer an explainable artificial intelligence (XAI) framework that\nprovides a three-fold explanation: a graphical depiction of the systems\ngeneralization and performance in the current game state, how well the agent\nwould play in semantically similar environments, and a narrative explanation of\nwhat the graphical information implies. We created a user-interface for our XAI\nframework and evaluated its efficacy via a human-user experiment. The results\ndemonstrate a statistically significant increase in user trust and acceptance\nof the AI system with explanation, versus the AI system without explanation.",
    "descriptor": "\nComments: NeurIPS Deep RL workshop, Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada\n",
    "authors": [
      "Jeff Druce",
      "Michael Harradon",
      "James Tittle"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03775"
  },
  {
    "id": "arXiv:2106.03776",
    "title": "CDN-MEDAL: Two-stage Density and Difference Approximation Framework for  Motion Analysis",
    "abstract": "Background modeling is a promising research area in video analysis with a\nvariety of video surveillance applications. Recent years have witnessed the\nproliferation of deep neural networks via effective learning-based approaches\nin motion analysis. However, these techniques only provide a limited\ndescription of the observed scenes' insufficient properties where a\nsingle-valued mapping is learned to approximate the temporal conditional\naverages of the target background. On the other hand, statistical learning in\nimagery domains has become one of the most prevalent approaches with high\nadaptation to dynamic context transformation, notably Gaussian Mixture Models,\ncombined with a foreground extraction step. In this work, we propose a novel,\ntwo-stage method of change detection with two convolutional neural networks.\nThe first architecture is grounded on the unsupervised Gaussian mixtures\nstatistical learning to describe the scenes' salient features. The second one\nimplements a light-weight pipeline of foreground detection. Our two-stage\nframework contains approximately 3.5K parameters in total but still maintains\nrapid convergence to intricate motion patterns. Our experiments on publicly\navailable datasets show that our proposed networks are not only capable of\ngeneralizing regions of moving objects in unseen cases with promising results\nbut also are competitive in performance efficiency and effectiveness regarding\nforeground segmentation.",
    "descriptor": "\nComments: 14 pages, 5 figures, to be submitted to IEEE TCSVT\n",
    "authors": [
      "Synh Viet-Uyen Ha",
      "Cuong Tien Nguyen",
      "Hung Ngoc Phan",
      "Nhat Minh Chung",
      "Phuong Hoai Ha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03776"
  },
  {
    "id": "arXiv:2106.03777",
    "title": "X2Parser: Cross-Lingual and Cross-Domain Framework for Task-Oriented  Compositional Semantic Parsing",
    "abstract": "Task-oriented compositional semantic parsing (TCSP) handles complex nested\nuser queries and serves as an essential component of virtual assistants.\nCurrent TCSP models rely on numerous training data to achieve decent\nperformance but fail to generalize to low-resource target languages or domains.\nIn this paper, we present X2Parser, a transferable Cross-lingual and\nCross-domain Parser for TCSP. Unlike previous models that learn to generate the\nhierarchical representations for nested intents and slots, we propose to\npredict flattened intents and slots representations separately and cast both\nprediction tasks into sequence labeling problems. After that, we further\npropose a fertility-based slot predictor that first learns to dynamically\ndetect the number of labels for each token, and then predicts the slot types.\nExperimental results illustrate that our model can significantly outperform\nexisting strong baselines in cross-lingual and cross-domain settings, and our\nmodel can also achieve a good generalization ability on target languages of\ntarget domains. Furthermore, our model tackles the problem in an efficient\nnon-autoregressive way that reduces the latency by up to 66% compared to the\ngenerative model.",
    "descriptor": "\nComments: Accepted in RepL4NLP 2021\n",
    "authors": [
      "Zihan Liu",
      "Genta Indra Winata",
      "Peng Xu",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03777"
  },
  {
    "id": "arXiv:2106.03780",
    "title": "Learning Stochastic Optimal Policies via Gradient Descent",
    "abstract": "We systematically develop a learning-based treatment of stochastic optimal\ncontrol (SOC), relying on direct optimization of parametric control policies.\nWe propose a derivation of adjoint sensitivity results for stochastic\ndifferential equations through direct application of variational calculus.\nThen, given an objective function for a predetermined task specifying the\ndesiderata for the controller, we optimize their parameters via iterative\ngradient descent methods. In doing so, we extend the range of applicability of\nclassical SOC techniques, often requiring strict assumptions on the functional\nform of system and control. We verify the performance of the proposed approach\non a continuous-time, finite horizon portfolio optimization with proportional\ntransaction costs.",
    "descriptor": "",
    "authors": [
      "Stefano Massaroli",
      "Michael Poli",
      "Stefano Peluchetti",
      "Jinkyoo Park",
      "Atsushi Yamashita",
      "Hajime Asama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03780"
  },
  {
    "id": "arXiv:2106.03783",
    "title": "An Information-theoretic Approach to Distribution Shifts",
    "abstract": "Safely deploying machine learning models to the real world is often a\nchallenging process. Models trained with data obtained from a specific\ngeographic location tend to fail when queried with data obtained elsewhere,\nagents trained in a simulation can struggle to adapt when deployed in the real\nworld or novel environments, and neural networks that are fit to a subset of\nthe population might carry some selection bias into their decision process. In\nthis work, we describe the problem of data shift from a novel\ninformation-theoretic perspective by (i) identifying and describing the\ndifferent sources of error, (ii) comparing some of the most promising\nobjectives explored in the recent domain generalization, and fair\nclassification literature. From our theoretical analysis and empirical\nevaluation, we conclude that the model selection procedure needs to be guided\nby careful considerations regarding the observed data, the factors used for\ncorrection, and the structure of the data-generating process.",
    "descriptor": "",
    "authors": [
      "Marco Federici",
      "Ryota Tomioka",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03783"
  },
  {
    "id": "arXiv:2106.03785",
    "title": "Generative Adversarial Networks: A Survey Towards Private and Secure  Applications",
    "abstract": "Generative Adversarial Networks (GAN) have promoted a variety of applications\nin computer vision, natural language processing, etc. due to its generative\nmodel's compelling ability to generate realistic examples plausibly drawn from\nan existing distribution of samples. GAN not only provides impressive\nperformance on data generation-based tasks but also stimulates fertilization\nfor privacy and security oriented research because of its game theoretic\noptimization strategy. Unfortunately, there are no comprehensive surveys on GAN\nin privacy and security, which motivates this survey paper to summarize those\nstate-of-the-art works systematically. The existing works are classified into\nproper categories based on privacy and security functions, and this survey\npaper conducts a comprehensive analysis of their advantages and drawbacks.\nConsidering that GAN in privacy and security is still at a very initial stage\nand has imposed unique challenges that are yet to be well addressed, this paper\nalso sheds light on some potential privacy and security applications with GAN\nand elaborates on some future research directions.",
    "descriptor": "",
    "authors": [
      "Zhipeng Cai",
      "Zuobin Xiong",
      "Honghui Xu",
      "Peng Wang",
      "Wei Li",
      "Yi Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03785"
  },
  {
    "id": "arXiv:2106.03787",
    "title": "Concave Utility Reinforcement Learning: the Mean-field Game viewpoint",
    "abstract": "Concave Utility Reinforcement Learning (CURL) extends RL from linear to\nconcave utilities in the occupancy measure induced by the agent's policy. This\nencompasses not only RL but also imitation learning and exploration, among\nothers. Yet, this more general paradigm invalidates the classical Bellman\nequations, and calls for new algorithms. Mean-field Games (MFGs) are a\ncontinuous approximation of many-agent RL. They consider the limit case of a\ncontinuous distribution of identical agents, anonymous with symmetric\ninterests, and reduce the problem to the study of a single representative agent\nin interaction with the full population. Our core contribution consists in\nshowing that CURL is a subclass of MFGs. We think this important to bridge\ntogether both communities. It also allows to shed light on aspects of both\nfields: we show the equivalence between concavity in CURL and monotonicity in\nthe associated MFG, between optimality conditions in CURL and Nash equilibrium\nin MFG, or that Fictitious Play (FP) for this class of MFGs is simply\nFrank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs.\nWe also experimentally demonstrate that, using algorithms recently introduced\nfor solving MFGs, we can address the CURL problem more efficiently.",
    "descriptor": "",
    "authors": [
      "Matthieu Geist",
      "Julien P\u00e9rolat",
      "Mathieu Lauri\u00e8re",
      "Romuald Elie",
      "Sarah Perrin",
      "Olivier Bachem",
      "R\u00e9mi Munos",
      "Olivier Pietquin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.03787"
  },
  {
    "id": "arXiv:2106.03790",
    "title": "Multi-armed Bandit Requiring Monotone Arm Sequences",
    "abstract": "In many online learning or multi-armed bandit problems, the taken actions or\npulled arms are ordinal and required to be monotone over time. Examples include\ndynamic pricing, in which the firms use markup pricing policies to please early\nadopters and deter strategic waiting, and clinical trials, in which the dose\nallocation usually follows the dose escalation principle to prevent dose\nlimiting toxicities. We consider the continuum-armed bandit problem when the\narm sequence is required to be monotone. We show that when the unknown\nobjective function is Lipschitz continuous, the regret is $O(T)$. When in\naddition the objective function is unimodal or quasiconcave, the regret is\n$\\tilde O(T^{3/4})$ under the proposed algorithm, which is also shown to be the\noptimal rate. This deviates from the optimal rate $\\tilde O(T^{2/3})$ in the\ncontinuous-armed bandit literature and demonstrates the cost to the learning\nefficiency brought by the monotonicity requirement.",
    "descriptor": "",
    "authors": [
      "Ningyuan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03790"
  },
  {
    "id": "arXiv:2106.03792",
    "title": "Preference Discovery in Large Product Lines",
    "abstract": "When AI tools can generate many solutions, some human preference must be\napplied to determine which solution is relevant to the current project. One way\nto find those preferences is interactive search-based software engineering\n(iSBSE) where humans can influence the search process. Current iSBSE methods\ncan lead to cognitive fatigue (when they overwhelm humans with too many overly\nelaborate questions). WHUN is an iSBSE algorithm that avoids that problem. Due\nto its recursive clustering procedure, WHUN only pesters humans for\n$O(log_2{N})$ interactions. Further, each interaction is mediated via a feature\nselection procedure that reduces the number of asked questions. When compared\nto prior state-of-the-art iSBSE systems, WHUN runs faster, asks fewer\nquestions, and achieves better solutions that are within $0.1\\%$ of the best\nsolutions seen in our sample space. More importantly, WHUN scales to large\nproblems (in our experiments, models with 1000 variables can be explored with\nhalf a dozen interactions where, each time, we ask only four questions).\nAccordingly, we recommend WHUN as a baseline against which future iSBSE work\nshould be compared. To facilitate that, all our scripts are online at\nhttps://github.com/ai-se/whun.",
    "descriptor": "",
    "authors": [
      "Andre Lustosa",
      "Tim Menzies"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.03792"
  },
  {
    "id": "arXiv:2106.03794",
    "title": "COVID-Fact: Fact Extraction and Verification of Real-World Claims on  COVID-19 Pandemic",
    "abstract": "We introduce a FEVER-like dataset COVID-Fact of $4,086$ claims concerning the\nCOVID-19 pandemic. The dataset contains claims, evidence for the claims, and\ncontradictory claims refuted by the evidence. Unlike previous approaches, we\nautomatically detect true claims and their source articles and then generate\ncounter-claims using automatic methods rather than employing human annotators.\nAlong with our constructed resource, we formally present the task of\nidentifying relevant evidence for the claims and verifying whether the evidence\nrefutes or supports a given claim. In addition to scientific claims, our data\ncontains simplified general claims from media sources, making it better suited\nfor detecting general misinformation regarding COVID-19. Our experiments\nindicate that COVID-Fact will provide a challenging testbed for the development\nof new systems and our approach will reduce the costs of building\ndomain-specific datasets for detecting misinformation.",
    "descriptor": "\nComments: ACL 2021 Camera Ready\n",
    "authors": [
      "Arkadiy Saakyan",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03794"
  },
  {
    "id": "arXiv:2106.03796",
    "title": "Enabling On-Device Self-Supervised Contrastive Learning With Selective  Data Contrast",
    "abstract": "After a model is deployed on edge devices, it is desirable for these devices\nto learn from unlabeled data to continuously improve accuracy. Contrastive\nlearning has demonstrated its great potential in learning from unlabeled data.\nHowever, the online input data are usually none independent and identically\ndistributed (non-iid) and storages of edge devices are usually too limited to\nstore enough representative data from different data classes. We propose a\nframework to automatically select the most representative data from the\nunlabeled input stream, which only requires a small data buffer for dynamic\nlearning. Experiments show that accuracy and learning speed are greatly\nimproved.",
    "descriptor": "",
    "authors": [
      "Yawen Wu",
      "Zhepeng Wang",
      "Dewen Zeng",
      "Yiyu Shi",
      "Jingtong Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03796"
  },
  {
    "id": "arXiv:2106.03797",
    "title": "Drone-based AI and 3D Reconstruction for Digital Twin Augmentation",
    "abstract": "Digital Twin is an emerging technology at the forefront of Industry 4.0, with\nthe ultimate goal of combining the physical space and the virtual space. To\ndate, the Digital Twin concept has been applied in many engineering fields,\nproviding useful insights in the areas of engineering design, manufacturing,\nautomation, and construction industry. While the nexus of various technologies\nopens up new opportunities with Digital Twin, the technology requires a\nframework to integrate the different technologies, such as the Building\nInformation Model used in the Building and Construction industry. In this work,\nan Information Fusion framework is proposed to seamlessly fuse heterogeneous\ncomponents in a Digital Twin framework from the variety of technologies\ninvolved. This study aims to augment Digital Twin in buildings with the use of\nAI and 3D reconstruction empowered by unmanned aviation vehicles. We proposed a\ndrone-based Digital Twin augmentation framework with reusable and customisable\ncomponents. A proof of concept is also developed, and extensive evaluation is\nconducted for 3D reconstruction and applications of AI for defect detection.",
    "descriptor": "",
    "authors": [
      "Alex To",
      "Maican Liu",
      "Muhammad Hazeeq Bin Muhammad Hairul",
      "Joseph G. Davis",
      "Jeannie S.A. Lee",
      "Henrik Hesse",
      "Hoang D. Nguyen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.03797"
  },
  {
    "id": "arXiv:2106.03798",
    "title": "DoubleField: Bridging the Neural Surface and Radiance Fields for  High-fidelity Human Rendering",
    "abstract": "We introduce DoubleField, a novel representation combining the merits of both\nsurface field and radiance field for high-fidelity human rendering. Within\nDoubleField, the surface field and radiance field are associated together by a\nshared feature embedding and a surface-guided sampling strategy. In this way,\nDoubleField has a continuous but disentangled learning space for geometry and\nappearance modeling, which supports fast training, inference, and finetuning.\nTo achieve high-fidelity free-viewpoint rendering, DoubleField is further\naugmented to leverage ultra-high-resolution inputs, where a view-to-view\ntransformer and a transfer learning scheme are introduced for more efficient\nlearning and finetuning from sparse-view inputs at original resolutions. The\nefficacy of DoubleField is validated by the quantitative evaluations on several\ndatasets and the qualitative results in a real-world sparse multi-view system,\nshowing its superior capability for photo-realistic free-viewpoint human\nrendering. For code and demo video, please refer to our project page:\nthis http URL",
    "descriptor": "",
    "authors": [
      "Ruizhi Shao",
      "Hongwen Zhang",
      "He Zhang",
      "Yanpei Cao",
      "Tao Yu",
      "Yebin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03798"
  },
  {
    "id": "arXiv:2106.03799",
    "title": "Deterministic Iteratively Built KD-Tree with KNN Search for Exact  Applications",
    "abstract": "K-Nearest Neighbors (KNN) search is a fundamental algorithm in artificial\nintelligence software with applications in robotics, and autonomous vehicles.\nThese wide-ranging applications utilize KNN either directly for simple\nclassification or combine KNN results as input to other algorithms such as\nLocally Weighted Learning (LWL). Similar to binary trees, kd-trees become\nunbalanced as new data is added in online applications which can lead to rapid\ndegradation in search performance unless the tree is rebuilt. Although\napproximate methods are suitable for graphics applications, which prioritize\nquery speed over query accuracy, they are unsuitable for certain applications\nin autonomous systems, aeronautics, and robotic manipulation where exact\nsolutions are desired. In this paper, we will attempt to assess the performance\nof non-recursive deterministic kd-tree functions and KNN functions. We will\nalso present a \"forest of interval kd-trees\" which reduces the number of tree\nrebuilds, without compromising the exactness of query results.",
    "descriptor": "",
    "authors": [
      "Aryan Naim",
      "Joseph Bowkett",
      "Sisir Karumanchi",
      "Peyman Tavallali",
      "Brett Kennedy"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03799"
  },
  {
    "id": "arXiv:2106.03801",
    "title": "Visual Transformer for Task-aware Active Learning",
    "abstract": "Pool-based sampling in active learning (AL) represents a key framework for\nan-notating informative data when dealing with deep learning models. In this\npaper, we present a novel pipeline for pool-based Active Learning. Unlike most\nprevious works, our method exploits accessible unlabelled examples during\ntraining to estimate their co-relation with the labelled examples. Another\ncontribution of this paper is to adapt Visual Transformer as a sampler in the\nAL pipeline. Visual Transformer models non-local visual concept dependency\nbetween labelled and unlabelled examples, which is crucial to identifying the\ninfluencing unlabelled examples. Also, compared to existing methods where the\nlearner and the sampler are trained in a multi-stage manner, we propose to\ntrain them in a task-aware jointly manner which enables transforming the latent\nspace into two separate tasks: one that classifies the labelled examples; the\nother that distinguishes the labelling direction. We evaluated our work on four\ndifferent challenging benchmarks of classification and detection tasks viz.\nCIFAR10, CIFAR100,FashionMNIST, RaFD, and Pascal VOC 2007. Our extensive\nempirical and qualitative evaluations demonstrate the superiority of our method\ncompared to the existing methods. Code available:\nhttps://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning",
    "descriptor": "",
    "authors": [
      "Razvan Caramalau",
      "Binod Bhattarai",
      "Tae-Kyun Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03801"
  },
  {
    "id": "arXiv:2106.03802",
    "title": "Learning to Efficiently Sample from Diffusion Probabilistic Models",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a powerful\nfamily of generative models that can yield high-fidelity samples and\ncompetitive log-likelihoods across a range of domains, including image and\nspeech synthesis. Key advantages of DDPMs include ease of training, in contrast\nto generative adversarial networks, and speed of generation, in contrast to\nautoregressive models. However, DDPMs typically require hundreds-to-thousands\nof steps to generate a high fidelity sample, making them prohibitively\nexpensive for high dimensional problems. Fortunately, DDPMs allow trading\ngeneration speed for sample quality through adjusting the number of refinement\nsteps as a post process. Prior work has been successful in improving generation\nspeed through handcrafting the time schedule by trial and error. We instead\nview the selection of the inference time schedules as an optimization problem,\nand introduce an exact dynamic programming algorithm that finds the optimal\ndiscrete time schedules for any pre-trained DDPM. Our method exploits the fact\nthat ELBO can be decomposed into separate KL terms, and given any computation\nbudget, discovers the time schedule that maximizes the training ELBO exactly.\nOur method is efficient, has no hyper-parameters of its own, and can be applied\nto any pre-trained DDPM with no retraining. We discover inference time\nschedules requiring as few as 32 refinement steps, while sacrificing less than\n0.1 bits per dimension compared to the default 4,000 steps used on ImageNet\n64x64 [Ho et al., 2020; Nichol and Dhariwal, 2021].",
    "descriptor": "",
    "authors": [
      "Daniel Watson",
      "Jonathan Ho",
      "Mohammad Norouzi",
      "William Chan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03802"
  },
  {
    "id": "arXiv:2106.03804",
    "title": "Deep Medial Fields",
    "abstract": "Implicit representations of geometry, such as occupancy fields or signed\ndistance fields (SDF), have recently re-gained popularity in encoding 3D solid\nshape in a functional form. In this work, we introduce medial fields: a field\nfunction derived from the medial axis transform (MAT) that makes available\ninformation about the underlying 3D geometry that is immediately useful for a\nnumber of downstream tasks. In particular, the medial field encodes the local\nthickness of a 3D shape, and enables O(1) projection of a query point onto the\nmedial axis. To construct the medial field we require nothing but the SDF of\nthe shape itself, thus allowing its straightforward incorporation in any\napplication that relies on signed distance fields. Working in unison with the\nO(1) surface projection supported by the SDF, the medial field opens the door\nfor an entirely new set of efficient, shape-aware operations on implicit\nrepresentations. We present three such applications, including a modification\nto sphere tracing that renders implicit representations with better convergence\nproperties, a fast construction method for memory-efficient rigid-body\ncollision proxies, and an efficient approximation of ambient occlusion that\nremains stable with respect to viewpoint variations.",
    "descriptor": "",
    "authors": [
      "Daniel Rebain",
      "Ke Li",
      "Vincent Sitzmann",
      "Soroosh Yazdani",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03804"
  },
  {
    "id": "arXiv:2106.03805",
    "title": "3DB: A Framework for Debugging Computer Vision Models",
    "abstract": "We introduce 3DB: an extendable, unified framework for testing and debugging\nvision models using photorealistic simulation. We demonstrate, through a wide\nrange of use cases, that 3DB allows users to discover vulnerabilities in\ncomputer vision systems and gain insights into how models make decisions. 3DB\ncaptures and generalizes many robustness analyses from prior work, and enables\none to study their interplay. Finally, we find that the insights generated by\nthe system transfer to the physical world.\nWe are releasing 3DB as a library (https://github.com/3db/3db) alongside a\nset of example analyses, guides, and documentation: https://3db.github.io/3db/ .",
    "descriptor": "",
    "authors": [
      "Guillaume Leclerc",
      "Hadi Salman",
      "Andrew Ilyas",
      "Sai Vemprala",
      "Logan Engstrom",
      "Vibhav Vineet",
      "Kai Xiao",
      "Pengchuan Zhang",
      "Shibani Santurkar",
      "Greg Yang",
      "Ashish Kapoor",
      "Aleksander Madry"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03805"
  },
  {
    "id": "arXiv:2106.03806",
    "title": "Deep Context- and Relation-Aware Learning for Aspect-based Sentiment  Analysis",
    "abstract": "Existing works for aspect-based sentiment analysis (ABSA) have adopted a\nunified approach, which allows the interactive relations among subtasks.\nHowever, we observe that these methods tend to predict polarities based on the\nliteral meaning of aspect and opinion terms and mainly consider relations\nimplicitly among subtasks at the word level. In addition, identifying multiple\naspect-opinion pairs with their polarities is much more challenging. Therefore,\na comprehensive understanding of contextual information w.r.t. the aspect and\nopinion are further required in ABSA. In this paper, we propose Deep\nContextualized Relation-Aware Network (DCRAN), which allows interactive\nrelations among subtasks with deep contextual information based on two modules\n(i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies).\nEspecially, we design novel self-supervised strategies for ABSA, which have\nstrengths in dealing with multiple aspects. Experimental results show that\nDCRAN significantly outperforms previous state-of-the-art methods by large\nmargins on three widely used benchmarks.",
    "descriptor": "\nComments: Accepted to ACL-IJCNLP 2021\n",
    "authors": [
      "Shinhyeok Oh",
      "Dongyub Lee",
      "Taesun Whang",
      "IlNam Park",
      "Gaeun Seo",
      "EungGyun Kim",
      "Harksoo Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03806"
  },
  {
    "id": "arXiv:2106.03812",
    "title": "Scalable Computation of Monge Maps with General Costs",
    "abstract": "Monge map refers to the optimal transport map between two probability\ndistributions and provides a principled approach to transform one distribution\nto another. In spite of the rapid developments of the numerical methods for\noptimal transport problems, computing the Monge maps remains challenging,\nespecially for high dimensional problems. In this paper, we present a scalable\nalgorithm for computing the Monge map between two probability distributions.\nOur algorithm is based on a weak form of the optimal transport problem, thus it\nonly requires samples from the marginals instead of their analytic expressions,\nand can accommodate optimal transport between two distributions with different\ndimensions. Our algorithm is suitable for general cost functions, compared with\nother existing methods for estimating Monge maps using samples, which are\nusually for quadratic costs. The performance of our algorithms is demonstrated\nthrough a series of experiments with both synthetic and realistic data.",
    "descriptor": "",
    "authors": [
      "Jiaojiao Fan",
      "Shu Liu",
      "Shaojun Ma",
      "Yongxin Chen",
      "Haomin Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03812"
  },
  {
    "id": "arXiv:2106.03814",
    "title": "High Resolution Solar Image Generation using Generative Adversarial  Networks",
    "abstract": "We applied Deep Learning algorithm known as Generative Adversarial Networks\n(GANs) to perform solar image-to-image translation. That is, from Solar\nDynamics Observatory (SDO)/Helioseismic and Magnetic Imager(HMI) line of sight\nmagnetogram images to SDO/Atmospheric Imaging Assembly(AIA) 0304-{\\AA} images.\nThe Ultraviolet(UV)/Extreme Ultraviolet(EUV) observations like the\nSDO/AIA0304-{\\AA} images were only made available to scientists in the late\n1990s even though the magenetic field observations like the SDO/HMI have been\navailable since the 1970s. Therefore by leveraging Deep Learning algorithms\nlike GANs we can give scientists access to complete datasets for analysis. For\ngenerating high resolution solar images we use the Pix2PixHD and Pix2Pix\nalgorithms. The Pix2PixHD algorithm was specifically designed for high\nresolution image generation tasks, and the Pix2Pix algorithm is by far the most\nwidely used image to image translation algorithm. For training and testing we\nused the data for the year 2012, 2013 and 2014. The results show that our deep\nlearning models are capable of generating high resolution(1024 x 1024 pixels)\nAIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson\nCorrelation Coefficient of the images generated by Pix2PixHD and original\nimages is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate\nimages. The results we get for our Pix2PixHD model is better than the results\nobtained by previous works done by others to generate AIA0304 images. Thus, we\ncan use these models to generate AIA0304 images when the AIA0304 data is not\navailable which can be used for understanding space weather and giving\nresearchers the capability to predict solar events such as Solar Flares and\nCoronal Mass Ejections. As far as we know, our work is the first attempt to\nleverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image\ntranslation.",
    "descriptor": "\nComments: 10 pages, 3 figures\n",
    "authors": [
      "Ankan Dash",
      "Junyi Ye",
      "Guiling Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.03814"
  },
  {
    "id": "arXiv:2106.03816",
    "title": "Diversity driven Query Rewriting in Search Advertising",
    "abstract": "Retrieving keywords (bidwords) with the same intent as query, referred to as\nclose variant keywords, is of prime importance for effective targeted search\nadvertising. For head and torso search queries, sponsored search engines use a\nhuge repository of same intent queries and keywords, mined ahead of time.\nOnline, this repository is used to rewrite the query and then lookup the\nrewrite in a repository of bid keywords contributing to significant revenue.\nRecently generative retrieval models have been shown to be effective at the\ntask of generating such query rewrites. We observe two main limitations of such\ngenerative models. First, rewrites generated by these models exhibit low\nlexical diversity, and hence the rewrites fail to retrieve relevant keywords\nthat have diverse linguistic variations. Second, there is a misalignment\nbetween the training objective - the likelihood of training data, v/s what we\ndesire - improved quality and coverage of rewrites. In this work, we introduce\nCLOVER, a framework to generate both high-quality and diverse rewrites by\noptimizing for human assessment of rewrite quality using our diversity-driven\nreinforcement learning algorithm. We use an evaluation model, trained to\npredict human judgments, as the reward function to finetune the generation\npolicy. We empirically show the effectiveness of our proposed approach through\noffline experiments on search queries across geographies spanning three major\nlanguages. We also perform online A/B experiments on Bing, a large commercial\nsearch engine, which shows (i) better user engagement with an average increase\nin clicks by 12.83% accompanied with an average defect reduction by 13.97%, and\n(ii) improved revenue by 21.29%.",
    "descriptor": "\nComments: Accepted in KDD 2021, 9 pages\n",
    "authors": [
      "Akash Kumar Mohankumar",
      "Nikit Begwani",
      "Amit Singh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03816"
  },
  {
    "id": "arXiv:2106.03819",
    "title": "A Semi-Personalized System for User Cold Start Recommendation on Music  Streaming Apps",
    "abstract": "Music streaming services heavily rely on recommender systems to improve their\nusers' experience, by helping them navigate through a large musical catalog and\ndiscover new songs, albums or artists. However, recommending relevant and\npersonalized content to new users, with few to no interactions with the\ncatalog, is challenging. This is commonly referred to as the user cold start\nproblem. In this applied paper, we present the system recently deployed on the\nmusic streaming service Deezer to address this problem. The solution leverages\na semi-personalized recommendation strategy, based on a deep neural network\narchitecture and on a clustering of users from heterogeneous sources of\ninformation. We extensively show the practical impact of this system and its\neffectiveness at predicting the future musical preferences of cold start users\non Deezer, through both offline and online large-scale experiments. Besides, we\npublicly release our code as well as anonymized usage data from our\nexperiments. We hope that this release of industrial resources will benefit\nfuture research on user cold start recommendation.",
    "descriptor": "\nComments: 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2021)\n",
    "authors": [
      "L\u00e9a Briand",
      "Guillaume Salha-Galvan",
      "Walid Bendada",
      "Mathieu Morlon",
      "Viet-Anh Tran"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03819"
  },
  {
    "id": "arXiv:2106.03821",
    "title": "Active Speaker Detection as a Multi-Objective Optimization with  Uncertainty-based Multimodal Fusion",
    "abstract": "It is now well established from a variety of studies that there is a\nsignificant benefit from combining video and audio data in detecting active\nspeakers. However, either of the modalities can potentially mislead audiovisual\nfusion by inducing unreliable or deceptive information. This paper outlines\nactive speaker detection as a multi-objective learning problem to leverage best\nof each modalities using a novel self-attention, uncertainty-based multimodal\nfusion scheme. Results obtained show that the proposed multi-objective learning\narchitecture outperforms traditional approaches in improving both mAP and AUC\nscores. We further demonstrate that our fusion strategy surpasses, in active\nspeaker detection, other modality fusion methods reported in various\ndisciplines. We finally show that the proposed method significantly improves\nthe state-of-the-art on the AVA-ActiveSpeaker dataset.",
    "descriptor": "\nComments: In INTERSPEECH 2021\n",
    "authors": [
      "Baptiste Pouthier",
      "Laurent Pilati",
      "Leela K. Gudupudi",
      "Charles Bouveyron",
      "Frederic Precioso"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.03821"
  },
  {
    "id": "arXiv:2106.03822",
    "title": "Energy and Age Pareto Optimal Trajectories in UAV-assisted Wireless Data  Collection",
    "abstract": "This paper studies an unmanned aerial vehicle (UAV)-assisted wireless\nnetwork, where a UAV is dispatched to gather information from ground sensor\nnodes (SN) and transfer the collected data to the depot. The information\nfreshness is captured by the age of information (AoI) metric, whilst the energy\nconsumption of the UAV is seen as another performance criterion. Most\nimportantly, the AoI and energy efficiency are inherently competing metrics,\nsince decreasing the AoI requires the UAV returning to the depot more\nfrequently, leading to a higher energy consumption. To this end, we design UAV\npaths that optimize these two competing metrics and reveal the Pareto frontier.\nTo formulate this problem, a multi-objective mixed integer linear programming\n(MILP) is proposed with a flow-based constraint set and we apply Bender's\ndecomposition on the proposed formulation. The overall outcome shows that the\nproposed method allows deriving non-dominated solutions for decision making for\nUAV based wireless data collection. Numerical results are provided to\ncorroborate our study by presenting the Pareto front of the two objectives and\nthe effect on the UAV trajectory.",
    "descriptor": "",
    "authors": [
      "Yuan Liao",
      "Vasilis Friderikos"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.03822"
  },
  {
    "id": "arXiv:2106.03824",
    "title": "Parallel Batch-Dynamic $k$-Core Decomposition",
    "abstract": "Maintaining a $k$-core decomposition quickly in a dynamic graph is an\nimportant problem in many applications, including social network analytics,\ngraph visualization, centrality measure computations, and community detection\nalgorithms. The main challenge for designing efficient $k$-core decomposition\nalgorithms is that a single change to the graph can cause the decomposition to\nchange significantly.\nWe present the first parallel batch-dynamic algorithm for maintaining an\napproximate $k$-core decomposition that is efficient in both theory and\npractice. Given an initial graph with $m$ edges, and a batch of $B$ updates,\nour algorithm maintains a $(2 + \\delta)$-approximation of the coreness values\nfor all vertices (for any constant $\\delta > 0$) in $O(B\\log^2 m)$ amortized\nwork and $O(\\log^2 m \\log\\log m)$ depth (parallel time) with high probability.\nOur algorithm also maintains a low out-degree orientation of the graph in the\nsame bounds. We implemented and experimentally evaluated our algorithm on a\n30-core machine with two-way hyper-threading on $11$ graphs of varying\ndensities and sizes. Compared to the state-of-the-art algorithms, our algorithm\nachieves up to a 114.52x speedup against the best multicore implementation and\nup to a 497.63x speedup against the best sequential algorithm, obtaining\nresults for graphs that are orders-of-magnitude larger than those used in\nprevious studies.\nIn addition, we present the first approximate static $k$-core algorithm with\nlinear work and polylogarithmic depth. We show that on a 30-core machine with\ntwo-way hyper-threading, our implementation achieves up to a 3.9x speedup in\nthe static case over the previous state-of-the-art parallel algorithm.",
    "descriptor": "",
    "authors": [
      "Quanquan C. Liu",
      "Jessica Shi",
      "Shangdi Yu",
      "Laxman Dhulipala",
      "Julian Shun"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03824"
  },
  {
    "id": "arXiv:2106.03826",
    "title": "Narrative Question Answering with Cutting-Edge Open-Domain QA  Techniques: A Comprehensive Study",
    "abstract": "Recent advancements in open-domain question answering (ODQA), i.e., finding\nanswers from large open-domain corpus like Wikipedia, have led to human-level\nperformance on many datasets. However, progress in QA over book stories (Book\nQA) lags behind despite its similar task formulation to ODQA. This work\nprovides a comprehensive and quantitative analysis about the difficulty of Book\nQA: (1) We benchmark the research on the NarrativeQA dataset with extensive\nexperiments with cutting-edge ODQA techniques. This quantifies the challenges\nBook QA poses, as well as advances the published state-of-the-art with a\n$\\sim$7\\% absolute improvement on Rouge-L. (2) We further analyze the detailed\nchallenges in Book QA through human\nstudies.\\footnote{\\url{https://github.com/gorov/BookQA}.} Our findings indicate\nthat the event-centric questions dominate this task, which exemplifies the\ninability of existing QA models to handle event-oriented scenarios.",
    "descriptor": "\nComments: Accepted to TACL\n",
    "authors": [
      "Xiangyang Mou",
      "Chenghao Yang",
      "Mo Yu",
      "Bingsheng Yao",
      "Xiaoxiao Guo",
      "Saloni Potdar",
      "Hui Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03826"
  },
  {
    "id": "arXiv:2106.03827",
    "title": "Stateful Strategic Regression",
    "abstract": "Automated decision-making tools increasingly assess individuals to determine\nif they qualify for high-stakes opportunities. A recent line of research\ninvestigates how strategic agents may respond to such scoring tools to receive\nfavorable assessments. While prior work has focused on the short-term strategic\ninteractions between a decision-making institution (modeled as a principal) and\nindividual decision-subjects (modeled as agents), we investigate interactions\nspanning multiple time-steps. In particular, we consider settings in which the\nagent's effort investment today can accumulate over time in the form of an\ninternal state - impacting both his future rewards and that of the principal.\nWe characterize the Stackelberg equilibrium of the resulting game and provide\nnovel algorithms for computing it. Our analysis reveals several intriguing\ninsights about the role of multiple interactions in shaping the game's outcome:\nFirst, we establish that in our stateful setting, the class of all linear\nassessment policies remains as powerful as the larger class of all monotonic\nassessment policies. While recovering the principal's optimal policy requires\nsolving a non-convex optimization problem, we provide polynomial-time\nalgorithms for recovering both the principal and agent's optimal policies under\ncommon assumptions about the process by which effort investments convert to\nobservable features. Most importantly, we show that with multiple rounds of\ninteraction at her disposal, the principal is more effective at incentivizing\nthe agent to accumulate effort in her desired direction. Our work addresses\nseveral critical gaps in the growing literature on the societal impacts of\nautomated decision-making - by focusing on longer time horizons and accounting\nfor the compounding nature of decisions individuals receive over time.",
    "descriptor": "",
    "authors": [
      "Keegan Harris",
      "Hoda Heidari",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03827"
  },
  {
    "id": "arXiv:2106.03830",
    "title": "A Simple Recipe for Multilingual Grammatical Error Correction",
    "abstract": "This paper presents a simple recipe to train state-of-the-art multilingual\nGrammatical Error Correction (GEC) models. We achieve this by first proposing a\nlanguage-agnostic method to generate a large number of synthetic examples. The\nsecond ingredient is to use large-scale multilingual language models (up to 11B\nparameters). Once fine-tuned on language-specific supervised sets we surpass\nthe previous state-of-the-art results on GEC benchmarks in four languages:\nEnglish, Czech, German and Russian. Having established a new set of baselines\nfor GEC, we make our results easily reproducible and accessible by releasing a\ncLang-8 dataset. It is produced by using our best model, which we call gT5, to\nclean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly\nsimplifies typical GEC training pipelines composed of multiple fine-tuning\nstages -- we demonstrate that performing a single fine-tuning step on cLang-8\nwith the off-the-shelf language models yields further accuracy improvements\nover an already top-performing gT5 model for English.",
    "descriptor": "",
    "authors": [
      "Sascha Rothe",
      "Jonathan Mallinson",
      "Eric Malmi",
      "Sebastian Krause",
      "Aliaksei Severyn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03830"
  },
  {
    "id": "arXiv:2106.03831",
    "title": "Counterfactual Maximum Likelihood Estimation for Training Deep Networks",
    "abstract": "Although deep learning models have driven state-of-the-art performance on a\nwide array of tasks, they are prone to learning spurious correlations that\nshould not be learned as predictive clues. To mitigate this problem, we propose\na causality-based training framework to reduce the spurious correlations caused\nby observable confounders. We give theoretical analysis on the underlying\ngeneral Structural Causal Model (SCM) and propose to perform Maximum Likelihood\nEstimation (MLE) on the interventional distribution instead of the\nobservational distribution, namely Counterfactual Maximum Likelihood Estimation\n(CMLE). As the interventional distribution, in general, is hidden from the\nobservational data, we then derive two different upper bounds of the expected\nnegative log-likelihood and propose two general algorithms, Implicit CMLE and\nExplicit CMLE, for causal predictions of deep learning models using\nobservational data. We conduct experiments on two real-world tasks: Natural\nLanguage Inference (NLI) and Image Captioning. The results show that CMLE\nmethods outperform the regular MLE method in terms of out-of-domain\ngeneralization performance and reducing spurious correlations, while\nmaintaining comparable performance on the regular evaluations.",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Xinyi Wang",
      "Wenhu Chen",
      "Michael Saxon",
      "William Yang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03831"
  },
  {
    "id": "arXiv:2106.03833",
    "title": "Learning without Knowing: Unobserved Context in Continuous Transfer  Reinforcement Learning",
    "abstract": "In this paper, we consider a transfer Reinforcement Learning (RL) problem in\ncontinuous state and action spaces, under unobserved contextual information.\nFor example, the context can represent the mental view of the world that an\nexpert agent has formed through past interactions with this world. We assume\nthat this context is not accessible to a learner agent who can only observe the\nexpert data. Then, our goal is to use the context-aware expert data to learn an\noptimal context-unaware policy for the learner using only a few new data\nsamples. Such problems are typically solved using imitation learning that\nassumes that both the expert and learner agents have access to the same\ninformation. However, if the learner does not know the expert context, using\nthe expert data alone will result in a biased learner policy and will require\nmany new data samples to improve. To address this challenge, in this paper, we\nformulate the learning problem as a causal bound-constrained Multi-Armed-Bandit\n(MAB) problem. The arms of this MAB correspond to a set of basis policy\nfunctions that can be initialized in an unsupervised way using the expert data\nand represent the different expert behaviors affected by the unobserved\ncontext. On the other hand, the MAB constraints correspond to causal bounds on\nthe accumulated rewards of these basis policy functions that we also compute\nfrom the expert data. The solution to this MAB allows the learner agent to\nselect the best basis policy and improve it online. And the use of causal\nbounds reduces the exploration variance and, therefore, improves the learning\nrate. We provide numerical experiments on an autonomous driving example that\nshow that our proposed transfer RL method improves the learner's policy faster\ncompared to existing imitation learning methods and enjoys much lower variance\nduring training.",
    "descriptor": "",
    "authors": [
      "Chenyu Liu",
      "Yan Zhang",
      "Yi Shen",
      "Michael M. Zavlanos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03833"
  },
  {
    "id": "arXiv:2106.03836",
    "title": "Tunable Trajectory Planner Using G3 Curves",
    "abstract": "Trajectory planning is commonly used as part of a local planner in autonomous\ndriving. This paper considers the problem of planning a\ncontinuous-curvature-rate trajectory between fixed start and goal states that\nminimizes a tunable trade-off between passenger comfort and travel time. The\nproblem is an instance of infinite dimensional optimization over two continuous\nfunctions: a path, and a velocity profile. We propose a simplification of this\nproblem that facilitates the discretization of both functions. This paper also\nproposes a method to quickly generate minimal-length paths between start and\ngoal states based on a single tuning parameter: the second derivative of\ncurvature. Furthermore, we discretize the set of velocity profiles along a\ngiven path into a selection of acceleration way-points along the path.\nGradient-descent is then employed to minimize cost over feasible choices of the\nsecond derivative of curvature, and acceleration way-points, resulting in a\nmethod that repeatedly solves the path and velocity profiles in an iterative\nfashion. Numerical examples are provided to illustrate the benefits of the\nproposed methods.",
    "descriptor": "\nComments: 13 pages, 11 figures, submitted to IEEE Transactions on Intelligent Vehicles\n",
    "authors": [
      "Alexander Botros",
      "Stephen L. Smith"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03836"
  },
  {
    "id": "arXiv:2106.03837",
    "title": "MemStream: Memory-Based Anomaly Detection in Multi-Aspect Streams with  Concept Drift",
    "abstract": "Given a stream of entries over time in a multi-aspect data setting where\nconcept drift is present, how can we detect anomalous activities? Most of the\nexisting unsupervised anomaly detection approaches seek to detect anomalous\nevents in an offline fashion and require a large amount of data for training.\nThis is not practical in real-life scenarios where we receive the data in a\nstreaming manner and do not know the size of the stream beforehand. Thus, we\nneed a data-efficient method that can detect and adapt to changing data trends,\nor concept drift, in an online manner. In this work, we propose MemStream, a\nstreaming multi-aspect anomaly detection framework, allowing us to detect\nunusual events as they occur while being resilient to concept drift. We\nleverage the power of a denoising autoencoder to learn representations and a\nmemory module to learn the dynamically changing trend in data without the need\nfor labels. We prove the optimum memory size required for effective drift\nhandling. Furthermore, MemStream makes use of two architecture design choices\nto be robust to memory poisoning. Experimental results show the effectiveness\nof our approach compared to state-of-the-art streaming baselines using 2\nsynthetic datasets and 11 real-world datasets.",
    "descriptor": "",
    "authors": [
      "Siddharth Bhatia",
      "Arjit Jain",
      "Shivin Srivastava",
      "Kenji Kawaguchi",
      "Bryan Hooi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03837"
  },
  {
    "id": "arXiv:2106.03839",
    "title": "NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results",
    "abstract": "This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a\nRAW noisy burst as input, the task in the challenge was to generate a clean RGB\nimage with 4 times higher resolution. The challenge contained two tracks; Track\n1 evaluating on synthetically generated data, and Track 2 using real-world\nbursts from mobile camera. In the final testing phase, 6 teams submitted\nresults using a diverse set of solutions. The top-performing methods set a new\nstate-of-the-art for the burst super-resolution task.",
    "descriptor": "\nComments: NTIRE 2021 Burst Super-Resolution challenge report\n",
    "authors": [
      "Goutam Bhat",
      "Martin Danelljan",
      "Radu Timofte",
      "Kazutoshi Akita",
      "Wooyeong Cho",
      "Haoqiang Fan",
      "Lanpeng Jia",
      "Daeshik Kim",
      "Bruno Lecouat",
      "Youwei Li",
      "Shuaicheng Liu",
      "Ziluan Liu",
      "Ziwei Luo",
      "Takahiro Maeda",
      "Julien Mairal",
      "Christian Micheloni",
      "Xuan Mo",
      "Takeru Oba",
      "Pavel Ostyakov",
      "Jean Ponce",
      "Sanghyeok Son",
      "Jian Sun",
      "Norimichi Ukita",
      "Rao Muhammad Umer",
      "Youliang Yan",
      "Lei Yu",
      "Magauiya Zhussip",
      "Xueyi Zou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03839"
  },
  {
    "id": "arXiv:2106.03840",
    "title": "Balancing Garbage Collection vs I/O Amplification using dynamic KV  separation",
    "abstract": "Key-value (KV) separation is a technique that introduces randomness in the\nI/O access patterns to reduce I/O amplification in LSM-based key-value stores\nfor fast storage devices (NVMe). KV separation has a significant drawback that\nmakes it less attractive: Delete and especially update operations that are\nimportant in modern workloads result in frequent and expensive garbage\ncollection (GC) in the value log. In this paper, we design and implement\nParallax, which proposes hybrid KV placement that reduces GC overhead\nsignificantly and maximizes the benefits of using a log. We first model the\nbenefits of KV separation for different KV pair sizes. We use this model to\nclassify KV pairs in three categories small, medium, and large. Then, Parallax\nuses different approaches for each KV category: It always places large values\nin a log and small values in place. For medium values it uses a mixed strategy\nthat combines the benefits of using a log and eliminates GC overhead as\nfollows: It places medium values in a log for all but the last few (typically\none or two) levels in the LSM structure, where it performs a full compaction,\nmerges values in place, and reclaims log space without the need for GC. We\nevaluate Parallax against RocksDB that places all values in place and BlobDB\nthat always performs KV separation. We find that Parallax increases throughput\nby up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x,\nand increases CPU efficiency by up to 18.7x and 28x respectively, for all but\nscan-based YCSB workloads.",
    "descriptor": "\nComments: 14 pages, 8 figures\n",
    "authors": [
      "Giorgos Xanthakis",
      "Giorgos Saloustros",
      "Nikos Batsaras",
      "Anastasios Papagiannis",
      "Angelos Bilas"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.03840"
  },
  {
    "id": "arXiv:2106.03843",
    "title": "Equivariant Graph Neural Networks for 3D Macromolecular Structure",
    "abstract": "Representing and reasoning about 3D structures of macromolecules is emerging\nas a distinct challenge in machine learning. Here, we extend recent work on\ngeometric vector perceptrons and apply equivariant graph neural networks to a\nwide range of tasks from structural biology. Our method outperforms all\nreference architectures on 4 out of 8 tasks in the ATOM3D benchmark and broadly\nimproves over rotation-invariant graph neural networks. We also demonstrate\nthat transfer learning can improve performance in learning from macromolecular\nstructure.",
    "descriptor": "",
    "authors": [
      "Bowen Jing",
      "Stephan Eismann",
      "Pratham N. Soni",
      "Ron O. Dror"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2106.03843"
  },
  {
    "id": "arXiv:2106.03844",
    "title": "Mean-Shifted Contrastive Loss for Anomaly Detection",
    "abstract": "Deep anomaly detection methods learn representations that separate between\nnormal and anomalous samples. Very effective representations are obtained when\npowerful externally trained feature extractors (e.g. ResNets pre-trained on\nImageNet) are fine-tuned on the training data which consists of normal samples\nand no anomalies. However, this is a difficult task that can suffer from\ncatastrophic collapse, i.e. it is prone to learning trivial and non-specific\nfeatures. In this paper, we propose a new loss function which can overcome\nfailure modes of both center-loss and contrastive-loss methods. Furthermore, we\ncombine it with a confidence-invariant angular center loss, which replaces the\nEuclidean distance used in previous work, that was sensitive to prediction\nconfidence. Our improvements yield a new anomaly detection approach, based on\n$\\textit{Mean-Shifted Contrastive Loss}$, which is both more accurate and less\nsensitive to catastrophic collapse than previous methods. Our method achieves\nstate-of-the-art anomaly detection performance on multiple benchmarks including\n$97.5\\%$ ROC-AUC on the CIFAR-10 dataset.",
    "descriptor": "",
    "authors": [
      "Tal Reiss",
      "Yedid Hoshen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03844"
  },
  {
    "id": "arXiv:2106.03845",
    "title": "A Matrix Trickle-Down Theorem on Simplicial Complexes and Applications  to Sampling Colorings",
    "abstract": "We show that the natural Glauber dynamics mixes rapidly and generates a\nrandom proper edge-coloring of a graph with maximum degree $\\Delta$ whenever\nthe number of colors is at least $q\\geq (\\frac{10}{3} + \\epsilon)\\Delta$, where\n$\\epsilon>0$ is arbitrary and the maximum degree satisfies $\\Delta \\geq C$ for\na constant $C = C(\\epsilon)$ depending only on $\\epsilon$. For edge-colorings,\nthis improves upon prior work \\cite{Vig99, CDMPP19} which show rapid mixing\nwhen $q\\geq (\\frac{11}{3}-\\epsilon_0 ) \\Delta$, where $\\epsilon_0 \\approx\n10^{-5}$ is a small fixed constant. At the heart of our proof, we establish a\nmatrix trickle-down theorem, generalizing Oppenheim's influential result, as a\nnew technique to prove that a high dimensional simplical complex is a local\nspectral expander.",
    "descriptor": "",
    "authors": [
      "Dorna Abdolazimi",
      "Kuikui Liu",
      "Shayan Oveis Gharan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.03845"
  },
  {
    "id": "arXiv:2106.03847",
    "title": "GAN Cocktail: mixing GANs without dataset access",
    "abstract": "Today's generative models are capable of synthesizing high-fidelity images,\nbut each model specializes on a specific target domain. This raises the need\nfor model merging: combining two or more pretrained generative models into a\nsingle unified one. In this work we tackle the problem of model merging, given\ntwo constraints that often come up in the real world: (1) no access to the\noriginal training data, and (2) without increasing the size of the neural\nnetwork. To the best of our knowledge, model merging under these constraints\nhas not been studied thus far. We propose a novel, two-stage solution. In the\nfirst stage, we transform the weights of all the models to the same parameter\nspace by a technique we term model rooting. In the second stage, we merge the\nrooted models by averaging their weights and fine-tuning them for each specific\ndomain, using only data generated by the original trained models. We\ndemonstrate that our approach is superior to baseline methods and to existing\ntransfer learning techniques, and investigate several applications.",
    "descriptor": "",
    "authors": [
      "Omri Avrahami",
      "Dani Lischinski",
      "Ohad Fried"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.03847"
  },
  {
    "id": "arXiv:2106.03849",
    "title": "SIMONe: View-Invariant, Temporally-Abstracted Object Representations via  Unsupervised Video Decomposition",
    "abstract": "To help agents reason about scenes in terms of their building blocks, we wish\nto extract the compositional structure of any given scene (in particular, the\nconfiguration and characteristics of objects comprising the scene). This\nproblem is especially difficult when scene structure needs to be inferred while\nalso estimating the agent's location/viewpoint, as the two variables jointly\ngive rise to the agent's observations. We present an unsupervised variational\napproach to this problem. Leveraging the shared structure that exists across\ndifferent scenes, our model learns to infer two sets of latent representations\nfrom RGB video input alone: a set of \"object\" latents, corresponding to the\ntime-invariant, object-level contents of the scene, as well as a set of \"frame\"\nlatents, corresponding to global time-varying elements such as viewpoint. This\nfactorization of latents allows our model, SIMONe, to represent object\nattributes in an allocentric manner which does not depend on viewpoint.\nMoreover, it allows us to disentangle object dynamics and summarize their\ntrajectories as time-abstracted, view-invariant, per-object properties. We\ndemonstrate these capabilities, as well as the model's performance in terms of\nview synthesis and instance segmentation, across three procedurally generated\nvideo datasets.",
    "descriptor": "\nComments: Animated figures are available at this https URL\n",
    "authors": [
      "Rishabh Kabra",
      "Daniel Zoran",
      "Goker Erdogan",
      "Loic Matthey",
      "Antonia Creswell",
      "Matthew Botvinick",
      "Alexander Lerchner",
      "Christopher P. Burgess"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03849"
  },
  {
    "id": "arXiv:1912.04088",
    "title": "Grover Adaptive Search for Constrained Polynomial Binary Optimization",
    "abstract": "In this paper we discuss Grover Adaptive Search (GAS) for Constrained\nPolynomial Binary Optimization (CPBO) problems, and in particular, Quadratic\nUnconstrained Binary Optimization (QUBO) problems, as a special case. GAS can\nprovide a quadratic speed-up for combinatorial optimization problems compared\nto brute force search. However, this requires the development of efficient\noracles to represent problems and flag states that satisfy certain search\ncriteria. In general, this can be achieved using quantum arithmetic, however,\nthis is expensive in terms of Toffoli gates as well as required ancilla qubits,\nwhich can be prohibitive in the near-term. Within this work, we develop a way\nto construct efficient oracles to solve CPBO problems using GAS algorithms. We\ndemonstrate this approach and the potential speed-up for the portfolio\noptimization problem, i.e. a QUBO, using simulation and experimental results\nobtained on real quantum hardware. However, our approach applies to\nhigher-degree polynomial objective functions as well as constrained\noptimization problems.",
    "descriptor": "\nComments: 11 pages, 15 figures\n",
    "authors": [
      "Austin Gilliam",
      "Stefan Woerner",
      "Constantin Gonciulea"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/1912.04088"
  },
  {
    "id": "arXiv:2106.02669",
    "title": "Real Time Video based Heart and Respiration Rate Monitoring",
    "abstract": "In recent years, research about monitoring vital signs by smartphones grows\nsignificantly. There are some special sensors like Electrocardiogram (ECG) and\nPhotoplethysmographic (PPG) to detect heart rate (HR) and respiration rate\n(RR). Smartphone cameras also can measure HR by detecting and processing\nimaging Photoplethysmographic (iPPG) signals from the video of a user's face.\nIndeed, the variation in the intensity of the green channel can be measured by\nthe iPPG signals of the video. This study aimed to provide a method to extract\nheart rate and respiration rate using the video of individuals' faces. The\nproposed method is based on measuring fluctuations in the Hue, and can\ntherefore extract both HR and RR from the video of a user's face. The proposed\nmethod is evaluated by performing on 25 healthy individuals. For each subject,\n20 seconds video of his/her face is recorded. Results show that the proposed\napproach of measuring iPPG using Hue gives more accurate rates than the Green\nchannel.",
    "descriptor": "",
    "authors": [
      "Jafar Pourbemany",
      "Almabrok Essa",
      "Ye Zhu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02669"
  },
  {
    "id": "arXiv:2106.02686",
    "title": "Ensemble Markov chain Monte Carlo with teleporting walkers",
    "abstract": "We introduce an ensemble Markov chain Monte Carlo approach to sampling from a\nprobability density with known likelihood. This method upgrades an underlying\nMarkov chain by allowing an ensemble of such chains to interact via a process\nin which one chain's state is cloned as another's is deleted. This effective\nteleportation of states can overcome issues of metastability in the underlying\nchain, as the scheme enjoys rapid mixing once the modes of the target density\nhave been populated. We derive a mean-field limit for the evolution of the\nensemble. We analyze the global and local convergence of this mean-field limit,\nshowing asymptotic convergence independent of the spectral gap of the\nunderlying Markov chain, and moreover we interpret the limiting evolution as a\ngradient flow. We explain how interaction can be applied selectively to a\nsubset of state variables in order to maintain advantage on very\nhigh-dimensional problems. Finally we present the application of our\nmethodology to Bayesian hyperparameter estimation for Gaussian process\nregression.",
    "descriptor": "",
    "authors": [
      "Michael Lindsey",
      "Jonathan Weare",
      "Anna Zhang"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.02686"
  },
  {
    "id": "arXiv:2106.02693",
    "title": "Safe Tests and Always-Valid Confidence Intervals for contingency tables  and beyond",
    "abstract": "We develop E variables for testing whether two data streams come from the\nsame source or not, and more generally, whether the difference between the\nsources is larger than some minimal effect size. These E variables lead to\ntests that remain safe, i.e. keep their Type-I error guarantees, under flexible\nsampling scenarios such as optional stopping and continuation. We also develop\nthe corresponding always-valid confidence intervals. In special cases our E\nvariables also have an optimal `growth' property under the alternative. We\nillustrate the generic construction through the special case of 2x2 contingency\ntables, where we also allow for the incorporation of different restrictions on\na composite alternative. Comparison to p-value analysis in simulations and a\nreal-world example show that E variables, through their flexibility, often\nallow for early stopping of data collection, thereby retaining similar power as\nclassical methods.",
    "descriptor": "",
    "authors": [
      "Rosanne Turner",
      "Alexander Ly",
      "Peter Gr\u00fcnwald"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.02693"
  },
  {
    "id": "arXiv:2106.02700",
    "title": "A Discrete Variational Derivation of Accelerated Methods in Optimization",
    "abstract": "Many of the new developments in machine learning are connected with\ngradient-based optimization methods. Recently, these methods have been studied\nusing a variational perspective. This has opened up the possibility of\nintroducing variational and symplectic integration methods using geometric\nintegrators. In particular, in this paper, we introduce variational integrators\nwhich allow us to derive different methods for optimization. Using both,\nHamilton's principle and Lagrange-d'Alembert's, we derive two families of\noptimization methods in one-to-one correspondence that generalize Polyak's\nheavy ball and the well known Nesterov accelerated gradient method, mimicking\nthe behavior of the latter which reduces the oscillations of typical momentum\nmethods. However, since the systems considered are explicitly time-dependent,\nthe preservation of symplecticity of autonomous systems occurs here solely on\nthe fibers. Several experiments exemplify the result.",
    "descriptor": "\nComments: 29 pages, 11 figures\n",
    "authors": [
      "C\u00e9dric M. Campos",
      "Alejandro Mahillo",
      "David Mart\u00edn de Diego"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Differential Geometry (math.DG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.02700"
  },
  {
    "id": "arXiv:2106.02703",
    "title": "Dissipative search of an unstructured database",
    "abstract": "The search of an unstructured database amounts to finding one element having\na certain property out of $N$ elements. The classical search with an oracle\nchecking one element at a time requires on average $N/2$ steps. The Grover\nalgorithm for the quantum search, and its unitary Hamiltonian evolution\nanalogue, accomplish the search asymptotically optimally in $\\mathcal{O}\n(\\sqrt{N})$ time steps. We reformulate the search problem as a dissipative\nMarkov process acting on an $N$-level system weakly coupled to a thermal bath.\nAssuming that the energy levels of the system represent the database elements,\nwe show that, with a proper choice of the spectrum and physically admissible,\nlong-range transition rates between the energy levels, the system relaxes to\nthe ground state, corresponding to the sought element, in time $\\mathcal{O}\n(\\ln N)$.",
    "descriptor": "\nComments: 4+2 pages, 2 figures\n",
    "authors": [
      "Armen E. Allahverdyan",
      "David Petrosyan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02703"
  },
  {
    "id": "arXiv:2106.02713",
    "title": "Learning Curves for SGD on Structured Features",
    "abstract": "The generalization performance of a machine learning algorithm such as a\nneural network depends in a non-trivial way on the structure of the data\ndistribution. Models of generalization in machine learning theory often ignore\nthe low-dimensional structure of natural signals, either by considering\ndata-agnostic bounds or by studying the performance of the algorithm when\ntrained on uncorrelated features. To analyze the influence of data structure on\ntest loss dynamics, we study an exactly solveable model of stochastic gradient\ndescent (SGD) which predicts test loss when training on features with arbitrary\ncovariance structure. We solve the theory exactly for both Gaussian features\nand arbitrary features and we show that the simpler Gaussian model accurately\npredicts test loss of nonlinear random-feature models and deep neural networks\ntrained with SGD on real datasets such as MNIST and CIFAR-10. We show that\nmodeling the geometry of the data in the induced feature space is indeed\ncrucial to accurately predict the test error throughout learning.",
    "descriptor": "",
    "authors": [
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02713"
  },
  {
    "id": "arXiv:2106.02735",
    "title": "Data-driven discovery of interacting particle systems using Gaussian  processes",
    "abstract": "Interacting particle or agent systems that display a rich variety of\ncollection motions are ubiquitous in science and engineering. A fundamental and\nchallenging goal is to understand the link between individual interaction rules\nand collective behaviors. In this paper, we study the data-driven discovery of\ndistance-based interaction laws in second-order interacting particle systems.\nWe propose a learning approach that models the latent interaction kernel\nfunctions as Gaussian processes, which can simultaneously fulfill two inference\ngoals: one is the nonparametric inference of interaction kernel function with\nthe pointwise uncertainty quantification, and the other one is the inference of\nunknown parameters in the non-collective forces of the system. We formulate\nlearning interaction kernel functions as a statistical inverse problem and\nprovide a detailed analysis of recoverability conditions, establishing that a\ncoercivity condition is sufficient for recoverability. We provide a\nfinite-sample analysis, showing that our posterior mean estimator converges at\nan optimal rate equal to the one in the classical 1-dimensional Kernel Ridge\nregression. Numerical results on systems that exhibit different collective\nbehaviors demonstrate efficient learning of our approach from scarce noisy\ntrajectory data.",
    "descriptor": "\nComments: 10 pages; Appendix 19 pages;\n",
    "authors": [
      "Jinchao Feng",
      "Yunxiang Ren",
      "Sui Tang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.02735"
  },
  {
    "id": "arXiv:2106.02750",
    "title": "Do You Listen with One or Two Microphones? A Unified ASR Model for  Single and Multi-Channel Audio",
    "abstract": "Automatic speech recognition (ASR) models are typically designed to operate\non a single input data type, e.g. a single or multi-channel audio streamed from\na device. This design decision assumes the \\textit{primary} input data source\ndoes not change and if an additional (\\textit{auxiliary}) data source is\noccasionally available, it cannot be used. An ASR model that operates on both\nprimary and auxiliary data can achieve better accuracy compared to a\nprimary-only solution; and a model that can serve both \\textit{primary-only}\n(PO) and \\textit{primary-plus-auxiliary} (PPA) modes is highly desirable. In\nthis work, we propose a unified ASR model that can serve both modes. We\ndemonstrate its efficacy in a realistic scenario where a set of devices\ntypically stream a single primary audio channel, and two additional auxiliary\nchannels \\textit{only when} upload bandwidth allows it. The architecture\nenables a unique methodology that uses both types of input audio during\ntraining time. Our proposed approach achieves up to 12.5\\% relative\nword-error-rate reduction (WERR) compared to a PO baseline, and up to 16.0\\%\nrelative WERR in low-SNR conditions. The unique training methodology achieves\nup to 2.5\\% relative WERR compared to a PPA baseline.",
    "descriptor": "",
    "authors": [
      "Gokce Keskin",
      "Minhua Wu",
      "Brian King",
      "Harish Mallidi",
      "Yang Gao",
      "\\\\Jasha Droppo",
      "Ariya Rastrow",
      "Roland Maas"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02750"
  },
  {
    "id": "arXiv:2106.02780",
    "title": "Learning Treatment Effects in Panels with General Intervention Patterns",
    "abstract": "The problem of causal inference with panel data is a central econometric\nquestion. The following is a fundamental version of this problem: Let $M^*$ be\na low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix\n$Z$ with entries in $\\{0,1\\}$ we observe the matrix $O$ with entries $O_{ij} :=\nM^*_{ij} + E_{ij} + \\mathcal{T}_{ij} Z_{ij}$ where $\\mathcal{T}_{ij} $ are\nunknown, heterogenous treatment effects. The problem requires we estimate the\naverage treatment effect $\\tau^* := \\sum_{ij} \\mathcal{T}_{ij} Z_{ij} /\n\\sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to\nestimating $\\tau^*$ when $Z$ places support on a single row. This paper extends\nthat framework to allow rate-optimal recovery of $\\tau^*$ for general $Z$, thus\nbroadly expanding its applicability. Our guarantees are the first of their type\nin this general setting. Computational experiments on synthetic and real-world\ndata show a substantial advantage over competing estimators.",
    "descriptor": "",
    "authors": [
      "Vivek F. Farias",
      "Andrew A. Li",
      "Tianyi Peng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2106.02780"
  },
  {
    "id": "arXiv:2106.02800",
    "title": "AOSLO-net: A deep learning-based method for automatic segmentation of  retinal microaneurysms from adaptive optics scanning laser ophthalmoscope  images",
    "abstract": "Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time\nretinal images with high resolution down to 2 $\\mu m$. This technique enables\ndetection of the morphologies of individual microaneurysms (MAs), which are one\nof the earliest signs of diabetic retinopathy (DR), a frequent complication of\ndiabetes that can lead to visual impairment and blindness. In contrast to\nprevious automatic models developed for MA detection on standard fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policy,\nincluding preprocessing, data augmentation and transfer learning, to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and\nsegmentation, leading to correct MA morphological classification, while\noutperforming the state-of-the-art both in accuracy and cost.",
    "descriptor": "",
    "authors": [
      "Qian Zhang",
      "Konstantina Sampani",
      "Mengjia Xu",
      "Shengze Cai",
      "Yixiang Deng",
      "He Li",
      "Jennifer K. Sun",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02800"
  },
  {
    "id": "arXiv:2106.02803",
    "title": "Network Estimation by Mixing: Adaptivity and More",
    "abstract": "Networks analysis has been commonly used to study the interactions between\nunits of complex systems. One problem of particular interest is learning the\nnetwork's underlying connection pattern given a single and noisy instantiation.\nWhile many methods have been proposed to address this problem in recent years,\nthey usually assume that the true model belongs to a known class, which is not\nverifiable in most real-world applications. Consequently, network modeling\nbased on these methods either suffers from model misspecification or relies on\nadditional model selection procedures that are not well understood in theory\nand can potentially be unstable in practice. To address this difficulty, we\npropose a mixing strategy that leverages available arbitrary models to improve\ntheir individual performances. The proposed method is computationally efficient\nand almost tuning-free; thus, it can be used as an off-the-shelf method for\nnetwork modeling. We show that the proposed method performs equally well as the\noracle estimate when the true model is included as individual candidates. More\nimportantly, the method remains robust and outperforms all current estimates\neven when the models are misspecified. Extensive simulation examples are used\nto verify the advantage of the proposed mixing method. Evaluation of link\nprediction performance on 385 real-world networks from six domains also\ndemonstrates the universal competitiveness of the mixing method across multiple\ndomains.",
    "descriptor": "",
    "authors": [
      "Tianxi Li",
      "Can M. Le"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.02803"
  },
  {
    "id": "arXiv:2106.02810",
    "title": "An Attribute-Aligned Strategy for Learning Speech Representation",
    "abstract": "Advancement in speech technology has brought convenience to our life.\nHowever, the concern is on the rise as speech signal contains multiple personal\nattributes, which would lead to either sensitive information leakage or bias\ntoward decision. In this work, we propose an attribute-aligned learning\nstrategy to derive speech representation that can flexibly address these issues\nby attribute-selection mechanism. Specifically, we propose a\nlayered-representation variational autoencoder (LR-VAE), which factorizes\nspeech representation into attribute-sensitive nodes, to derive an\nidentity-free representation for speech emotion recognition (SER), and an\nemotionless representation for speaker verification (SV). Our proposed method\nachieves competitive performances on identity-free SER and a better performance\non emotionless SV, comparing to the current state-of-the-art method of using\nadversarial learning applied on a large emotion corpora, the MSP-Podcast. Also,\nour proposed learning strategy reduces the model and training process needed to\nachieve multiple privacy-preserving tasks.",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Yu-Lin Huang",
      "Bo-Hao Su",
      "Y.-W. Peter Hong",
      "Chi-Chun Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02810"
  },
  {
    "id": "arXiv:2106.02812",
    "title": "Optimizing Ansatz Design in QAOA for Max-cut",
    "abstract": "Quantum Approximate Optimization Algorithm (QAOA) has been studied widely in\nthe literature, primarily for finding an approximate value of the maximum cut\nsize of a graph. QAOA is composed of a problem hamiltonian and a mixer\nhamiltonian which are applied alternately for $p \\geq 1$ layers. The circuit\nfor this algorithm requires $2m$ CNOT gates in each layer, where $m$ is the\nnumber of edges in the graph. CNOT gate is one of the primary sources of error\nin modern quantum computers. In this paper, we propose two techniques for\nreducing the number of CNOT gates in the circuit which are independent of the\nhardware architecture. For a graph with $n$ vertices, we first propose a\ntechnique based on edge coloring that can reduce upto $\\lfloor \\frac{n}{2}\n\\rfloor$ CNOT gates in the circuit. Next, we propose another technique based on\nDepth First Search (DFS) that can reduce $n-1$ CNOT gates at the cost of some\nincreased depth. We analytically derive the criteria for which the reduction in\nthe number of CNOT gates due to the DFS based technique can provide lower error\nprobability even with some increased depth, and show that all graphs conform to\nthis criteria, making this technique universal. We further show that this\nproposed optimization holds even in the post transpilation stage of the\ncircuit, which is actually executed in the IBM Quantum hardware. We simulate\nthese two techniques for graphs of various sparsity with the ibmq_manhattan\nnoise model and show that the DFS based technique outperforms the edge coloring\nbased technique, which in turn, outperforms the traditional QAOA circuit in\nterms of reduction in the number of CNOT gates, and hence the probability of\nerror of the circuit.",
    "descriptor": "\nComments: 14 pages; single column (without reference)\n",
    "authors": [
      "Ritajit Majumdar",
      "Dhiraj Madan",
      "Debasmita Bhoumik",
      "Dhinakaran Vinayagamurthy",
      "Shesha Raghunathan",
      "Susmita Sur-Kolay"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02812"
  },
  {
    "id": "arXiv:2106.02847",
    "title": "Navigating to the Best Policy in Markov Decision Processes",
    "abstract": "We investigate the classical active pure exploration problem in Markov\nDecision Processes, where the agent sequentially selects actions and, from the\nresulting system trajectory, aims at identifying the best policy as fast as\npossible. We propose an information-theoretic lower bound on the average number\nof steps required before a correct answer can be given with probability at\nleast $1-\\delta$. This lower bound involves a non-convex optimization problem,\nfor which we propose a convex relaxation. We further provide an algorithm whose\nsample complexity matches the relaxed lower bound up to a factor $2$. This\nalgorithm addresses general communicating MDPs; we propose a variant with\nreduced exploration rate (and hence faster convergence) under an additional\nergodicity assumption. This work extends previous results relative to the\n\\emph{generative setting}~\\cite{marjani2020adaptive}, where the agent could at\neach step observe the random outcome of any (state, action) pair. In contrast,\nwe show here how to deal with the \\emph{navigation constraints}. Our analysis\nrelies on an ergodic theorem for non-homogeneous Markov chains which we\nconsider of wide interest in the analysis of Markov Decision Processes.",
    "descriptor": "",
    "authors": [
      "Aymen Al Marjani",
      "Aur\u00e9lien Garivier",
      "Alexandre Proutiere"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02847"
  },
  {
    "id": "arXiv:2106.02884",
    "title": "A Deep Variational Bayesian Framework for Blind Image Deblurring",
    "abstract": "Blind image deblurring is an important yet very challenging problem in\nlow-level vision. Traditional optimization based methods generally formulate\nthis task as a maximum-a-posteriori estimation or variational inference\nproblem, whose performance highly relies on the handcraft priors for both the\nlatent image and the blur kernel. In contrast, recent deep learning methods\ngenerally learn, from a large collection of training images, deep neural\nnetworks (DNNs) directly mapping the blurry image to the clean one or to the\nblur kernel, paying less attention to the physical degradation process of the\nblurry image. In this paper, we present a deep variational Bayesian framework\nfor blind image deblurring. Under this framework, the posterior of the latent\nclean image and blur kernel can be jointly estimated in an amortized inference\nfashion with DNNs, and the involved inference DNNs can be trained by fully\nconsidering the physical blur model, together with the supervision of data\ndriven priors for the clean image and blur kernel, which is naturally led to by\nthe evidence lower bound objective. Comprehensive experiments are conducted to\nsubstantiate the effectiveness of the proposed framework. The results show that\nit can not only achieve a promising performance with relatively simple\nnetworks, but also enhance the performance of existing DNNs for deblurring.",
    "descriptor": "",
    "authors": [
      "Hui Wang",
      "Zongsheng Yue",
      "Qian Zhao",
      "Deyu Meng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02884"
  },
  {
    "id": "arXiv:2106.02901",
    "title": "Hierarchical Temperature Imaging Using Pseudo-Inversed Convolutional  Neural Network Aided TDLAS Tomography",
    "abstract": "As an in situ combustion diagnostic tool, Tunable Diode Laser Absorption\nSpectroscopy (TDLAS) tomography has been widely used for imaging of\ntwo-dimensional temperature distributions in reactive flows. Compared with the\ncomputational tomographic algorithms, Convolutional Neural Networks (CNNs) have\nbeen proofed to be more robust and accurate for image reconstruction,\nparticularly in case of limited access of laser beams in the Region of Interest\n(RoI). In practice, flame in the RoI that requires to be reconstructed with\ngood spatial resolution is commonly surrounded by low-temperature background.\nAlthough the background is not of high interest, spectroscopic absorption still\nexists due to heat dissipation and gas convection. Therefore, we propose a\nPseudo-Inversed CNN (PI-CNN) for hierarchical temperature imaging that (a) uses\nefficiently the training and learning resources for temperature imaging in the\nRoI with good spatial resolution, and (b) reconstructs the less spatially\nresolved background temperature by adequately addressing the integrity of the\nspectroscopic absorption model. In comparison with the traditional CNN, the\nnewly introduced pseudo inversion of the RoI sensitivity matrix is more\npenetrating for revealing the inherent correlation between the projection data\nand the RoI to be reconstructed, thus prioritising the temperature imaging in\nthe RoI with high accuracy and high computational efficiency. In this paper,\nthe proposed algorithm was validated by both numerical simulation and lab-scale\nexperiment, indicating good agreement between the phantoms and the\nhigh-fidelity reconstructions.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Instrumentation and Measurement\n",
    "authors": [
      "Jingjing Si",
      "Guoliang Li",
      "Yinbo Cheng",
      "Rui Zhang",
      "Godwin Enemali",
      "Chang Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.02901"
  },
  {
    "id": "arXiv:2106.02929",
    "title": "The topological Dirac equation of networks and simplicial complexes",
    "abstract": "We define the topological Dirac equation describing the evolution of a\ntopological wave function on networks or on simplicial complexes. On networks,\nthe topological wave function describes the dynamics of topological signals or\ncochains, i.e. dynamical signals defined both on nodes and on links. On\nsimplicial complexes the wave function is also defined on higher-dimensional\nsimplices. Therefore the topological wave function satisfies a relaxed\ncondition of locality as it acquires the same value along simplices of\ndimension larger than zero. The topological Dirac equation defines eigenstates\nwhose dispersion relation is determined by the spectral properties of the Dirac\n(or chiral) operator defined on networks and generalized network structures\nincluding simplicial complexes and multiplex networks. On simplicial complexes\nthe Dirac equation leads to multiple energy bands. On multiplex networks the\ntopological Dirac equation can be generalized to distinguish between different\nmutlilinks leading to a natural definition of rotations of the topological\nspinor. The topological Dirac equation is here initially formulated on a\nspatial network or simplicial complex for describing the evolution of the\ntopological wave function in continuous time. This framework is also extended\nto treat the topological Dirac equation on $1+d$ spaces describing a discrete\nspace-time with one temporal dimension and $d$ spatial dimensions with $d\\in\n\\{1,2,3\\}$. This work includes also the discussion of numerical results\nobtained by implementing the topological Dirac equation on simplicial complex\nmodels and on real simple and multiplex network data.",
    "descriptor": "\nComments: (34 pages, 5 figures)\n",
    "authors": [
      "Ginestra Bianconi"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Social and Information Networks (cs.SI)",
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Mathematical Physics (math-ph)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.02929"
  },
  {
    "id": "arXiv:2106.02948",
    "title": "Neural dSCA: demixing multimodal interaction among brain areas during  naturalistic experiments",
    "abstract": "Multi-regional interaction among neuronal populations underlies the brain's\nprocessing of rich sensory information in our daily lives. Recent neuroscience\nand neuroimaging studies have increasingly used naturalistic stimuli and\nexperimental design to identify such realistic sensory computation in the\nbrain. However, existing methods for cross-areal interaction analysis with\ndimensionality reduction, such as reduced-rank regression and canonical\ncorrelation analysis, have limited applicability and interpretability in\nnaturalistic settings because they usually do not appropriately 'demix' neural\ninteractions into those associated with different types of task parameters or\nstimulus features (e.g., visual or audio). In this paper, we develop a new\nmethod for cross-areal interaction analysis that uses the rich task or stimulus\nparameters to reveal how and what types of information are shared by different\nneural populations. The proposed neural demixed shared component analysis\ncombines existing dimensionality reduction methods with a practical neural\nnetwork implementation of functional analysis of variance with latent\nvariables, thereby efficiently demixing nonlinear effects of continuous and\nmultimodal stimuli. We also propose a simplifying alternative under the\nassumptions of linear effects and unimodal stimuli. To demonstrate our methods,\nwe analyzed two human neuroimaging datasets of participants watching\nnaturalistic videos of movies and dance movements. The results demonstrate that\nour methods provide new insights into multi-regional interaction in the brain\nduring naturalistic sensory inputs, which cannot be captured by conventional\ntechniques.",
    "descriptor": "",
    "authors": [
      "Yu Takagi",
      "Laurence T. Hunt",
      "Ryu Ohata",
      "Hiroshi Imamizu",
      "Jun-ichiro Hirayama"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02948"
  },
  {
    "id": "arXiv:2106.02964",
    "title": "A Review of Machine Learning Classification Using Quantum Annealing for  Real-world Applications",
    "abstract": "Optimizing the training of a machine learning pipeline helps in reducing\ntraining costs and improving model performance. One such optimizing strategy is\nquantum annealing, which is an emerging computing paradigm that has shown\npotential in optimizing the training of a machine learning model. The\nimplementation of a physical quantum annealer has been realized by D-Wave\nsystems and is available to the research community for experiments. Recent\nexperimental results on a variety of machine learning applications using\nquantum annealing have shown interesting results where the performance of\nclassical machine learning techniques is limited by limited training data and\nhigh dimensional features. This article explores the application of D-Wave's\nquantum annealer for optimizing machine learning pipelines for real-world\nclassification problems. We review the application domains on which a physical\nquantum annealer has been used to train machine learning classifiers. We\ndiscuss and analyze the experiments performed on the D-Wave quantum annealer\nfor applications such as image recognition, remote sensing imagery,\ncomputational biology, and particle physics. We discuss the possible advantages\nand the problems for which quantum annealing is likely to be advantageous over\nclassical computation.",
    "descriptor": "\nComments: 13 Pages\n",
    "authors": [
      "Rajdeep Kumar Nath",
      "Himanshu Thapliyal",
      "Travis S. Humble"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02964"
  },
  {
    "id": "arXiv:2106.02978",
    "title": "Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks",
    "abstract": "Stochastic linear contextual bandit algorithms have substantial applications\nin practice, such as recommender systems, online advertising, clinical trials,\netc. Recent works show that optimal bandit algorithms are vulnerable to\nadversarial attacks and can fail completely in the presence of attacks.\nExisting robust bandit algorithms only work for the non-contextual setting\nunder the attack of rewards and cannot improve the robustness in the general\nand popular contextual bandit environment. In addition, none of the existing\nmethods can defend against attacked context. In this work, we provide the first\nrobust bandit algorithm for stochastic linear contextual bandit setting under a\nfully adaptive and omniscient attack. Our algorithm not only works under the\nattack of rewards, but also under attacked context. Moreover, it does not need\nany information about the attack budget or the particular form of the attack.\nWe provide theoretical guarantees for our proposed algorithm and show by\nextensive experiments that our proposed algorithm significantly improves the\nrobustness against various kinds of popular attacks.",
    "descriptor": "",
    "authors": [
      "Qin Ding",
      "Cho-Jui Hsieh",
      "James Sharpnack"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02978"
  },
  {
    "id": "arXiv:2106.02979",
    "title": "Syndicated Bandits: A Framework for Auto Tuning Hyper-parameters in  Contextual Bandit Algorithms",
    "abstract": "The stochastic contextual bandit problem, which models the trade-off between\nexploration and exploitation, has many real applications, including recommender\nsystems, online advertising and clinical trials. As many other machine learning\nalgorithms, contextual bandit algorithms often have one or more\nhyper-parameters. As an example, in most optimal stochastic contextual bandit\nalgorithms, there is an unknown exploration parameter which controls the\ntrade-off between exploration and exploitation. A proper choice of the\nhyper-parameters is essential for contextual bandit algorithms to perform well.\nHowever, it is infeasible to use offline tuning methods to select\nhyper-parameters in contextual bandit environment since there is no\npre-collected dataset and the decisions have to be made in real time. To tackle\nthis problem, we first propose a two-layer bandit structure for auto tuning the\nexploration parameter and further generalize it to the Syndicated Bandits\nframework which can learn multiple hyper-parameters dynamically in contextual\nbandit environment. We show our Syndicated Bandits framework can achieve the\noptimal regret upper bounds and is general enough to handle the tuning tasks in\nmany popular contextual bandit algorithms, such as LinUCB, LinTS, UCB-GLM, etc.\nExperiments on both synthetic and real datasets validate the effectiveness of\nour proposed framework.",
    "descriptor": "",
    "authors": [
      "Qin Ding",
      "Yi-Wei Liu",
      "Cho-Jui Hsieh",
      "James Sharpnack"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02979"
  },
  {
    "id": "arXiv:2106.02988",
    "title": "Causal Bandits with Unknown Graph Structure",
    "abstract": "In causal bandit problems, the action set consists of interventions on\nvariables of a causal graph. Several researchers have recently studied such\nbandit problems and pointed out their practical applications. However, all\nexisting works rely on a restrictive and impractical assumption that the\nlearner is given full knowledge of the causal graph structure upfront. In this\npaper, we develop novel causal bandit algorithms without knowing the causal\ngraph. Our algorithms work well for causal trees, causal forests and a general\nclass of causal graphs. The regret guarantees of our algorithms greatly improve\nupon those of standard multi-armed bandit (MAB) algorithms under mild\nconditions. Lastly, we prove our mild conditions are necessary: without them\none cannot do better than standard MAB bandit algorithms.",
    "descriptor": "",
    "authors": [
      "Yangyi Lu",
      "Amirhossein Meisami",
      "Ambuj Tewari"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02988"
  },
  {
    "id": "arXiv:2106.03007",
    "title": "Unbiased Self-Play",
    "abstract": "We present a general optimization framework for emergent belief-state\nrepresentation without any supervision. We employed the common configuration of\nmultiagent reinforcement learning and communication to improve exploration\ncoverage over an environment by leveraging the knowledge of each agent. In this\npaper, we obtained that recurrent neural nets (RNNs) with shared weights are\nhighly biased in partially observable environments because of their\nnoncooperativity. To address this, we designated an unbiased version of\nself-play via mechanism design, also known as reverse game theory, to clarify\nunbiased knowledge at the Bayesian Nash equilibrium. The key idea is to add\nimaginary rewards using the peer prediction mechanism, i.e., a mechanism for\nmutually criticizing information in a decentralized environment. Numerical\nanalyses, including StarCraft exploration tasks with up to 20 agents and\noff-the-shelf RNNs, demonstrate the state-of-the-art performance.",
    "descriptor": "",
    "authors": [
      "Shohei Ohsawa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2106.03007"
  },
  {
    "id": "arXiv:2106.03012",
    "title": "On Irreversible Metropolis Sampling Related to Langevin Dynamics",
    "abstract": "There has been considerable interest in designing Markov chain Monte Carlo\nalgorithms by exploiting numerical methods for Langevin dynamics, which\nincludes Hamiltonian dynamics as a deterministic case. A prominent approach is\nHamiltonian Monte Carlo (HMC), where a leapfrog discretization of Hamiltonian\ndynamics is employed. We investigate a recently proposed class of irreversible\nsampling algorithms, called Hamiltonian assisted Metropolis sampling (HAMS),\nwhich uses an augmented target density similarly as in HMC, but involves a\nflexible proposal scheme and a carefully formulated acceptance-rejection scheme\nto achieve generalized reversibility. We show that as the step size tends to 0,\nthe HAMS proposal satisfies a class of stochastic differential equations\nincluding Langevin dynamics as a special case. We provide theoretical results\nfor HAMS under the univariate Gaussian setting, including the stationary\nvariance, the expected acceptance rate, and the spectral radius. From these\nresults, we derive default choices of tuning parameters for HAMS, such that\nonly the step size needs to be tuned in applications. Various relatively recent\nalgorithms for Langevin dynamics are also shown to fall in the class of HAMS\nproposals up to negligible differences. Our numerical experiments on sampling\nhigh-dimensional latent variables confirm that the HAMS algorithms consistently\nachieve superior performance, compared with several Metropolis-adjusted\nalgorithms based on popular integrators of Langevin dynamics.",
    "descriptor": "",
    "authors": [
      "Zexi Song",
      "Zhiqiang Tan"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03012"
  },
  {
    "id": "arXiv:2106.03013",
    "title": "Inverse design of two-dimensional materials with invertible neural  networks",
    "abstract": "The ability to readily design novel materials with chosen functional\nproperties on-demand represents a next frontier in materials discovery.\nHowever, thoroughly and efficiently sampling the entire design space in a\ncomputationally tractable manner remains a highly challenging task. To tackle\nthis problem, we propose an inverse design framework (MatDesINNe) utilizing\ninvertible neural networks which can map both forward and reverse processes\nbetween the design space and target property. This approach can be used to\ngenerate materials candidates for a designated property, thereby satisfying the\nhighly sought-after goal of inverse design. We then apply this framework to the\ntask of band gap engineering in two-dimensional materials, starting with MoS2.\nWithin the design space encompassing six degrees of freedom in applied tensile,\ncompressive and shear strain plus an external electric field, we show the\nframework can generate novel, high fidelity, and diverse candidates with\nnear-chemical accuracy. We extend this generative capability further to provide\ninsights regarding metal-insulator transition, important for memristive\nneuromorphic applications among others, in MoS2 which is not otherwise possible\nwith brute force screening. This approach is general and can be directly\nextended to other materials and their corresponding design spaces and target\nproperties.",
    "descriptor": "",
    "authors": [
      "Victor Fung",
      "Jiaxin Zhang",
      "Guoxiang Hu",
      "P. Ganesh",
      "Bobby G. Sumpter"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03013"
  },
  {
    "id": "arXiv:2106.03019",
    "title": "Machine Learning Based Anxiety Detection in Older Adults using Wristband  Sensors and Context Feature",
    "abstract": "This paper explores a novel method for anxiety detection in older adults\nusing simple wristband sensors such as Electrodermal Activity (EDA) and\nPhotoplethysmogram (PPG) and a context-based feature. The proposed method for\nanxiety detection combines features from a single physiological signal with an\nexperimental context-based feature to improve the performance of the anxiety\ndetection model. The experimental data for this work is obtained from a\nyear-long experiment on 41 healthy older adults (26 females and 15 males) in\nthe age range 60-80 with mean age 73.36+-5.25 during a Trier Social Stress Test\n(TSST) protocol. The anxiety level ground truth was obtained from State-Trait\nAnxiety Inventory (STAI), which is regarded as the gold standard to measure\nperceived anxiety. EDA and Blood Volume Pulse (BVP) signals were recorded using\na wrist-worn EDA and PPG sensor respectively. 47 features were computed from\nEDA and BVP signal, out of which a final set of 24 significantly correlated\nfeatures were selected for analysis. The phases of the experimental study are\nencoded as unique integers to generate the context feature vector. A\ncombination of features from a single sensor with the context feature vector is\nused for training a machine learning model to distinguish between anxious and\nnot-anxious states. Results and analysis showed that the EDA and BVP machine\nlearning models that combined the context feature along with the physiological\nfeatures achieved 3.37% and 6.41% higher accuracy respectively than the models\nthat used only physiological features. Further, end-to-end processing of EDA\nand BVP signals was simulated for real-time anxiety level detection. This work\ndemonstrates the practicality of the proposed anxiety detection method in\nfacilitating long-term monitoring of anxiety in older adults using low-cost\nconsumer devices.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Rajdeep Kumar Nath",
      "Himanshu Thapliyal"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03019"
  },
  {
    "id": "arXiv:2106.03022",
    "title": "Fisher-Pitman permutation tests based on nonparametric Poisson mixtures  with application to single cell genomics",
    "abstract": "This paper investigates the theoretical and empirical performance of\nFisher-Pitman-type permutation tests for assessing the equality of unknown\nPoisson mixture distributions. Building on nonparametric maximum likelihood\nestimators (NPMLEs) of the mixing distribution, these tests are theoretically\nshown to be able to adapt to complicated unspecified structures of count data\nand also consistent against their corresponding ANOVA-type alternatives; the\nlatter is a result in parallel to classic claims made by Robinson (Robinson,\n1973). The studied methods are then applied to a single-cell RNA-seq data\nobtained from different cell types from brain samples of autism subjects and\nhealthy controls; empirically, they unveil genes that are differentially\nexpressed between autism and control subjects yet are missed using common\ntests. For justifying their use, rate optimality of NPMLEs is also established\nin settings similar to nonparametric Gaussian (Wu and Yang, 2020a) and binomial\nmixtures (Tian et al., 2017; Vinayak et al., 2019).",
    "descriptor": "\nComments: 52 pages\n",
    "authors": [
      "Zhen Miao",
      "Weihao Kong",
      "Ramya Korlakai Vinayak",
      "Wei Sun",
      "Fang Han"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03022"
  },
  {
    "id": "arXiv:2106.03032",
    "title": "Deep Particulate Matter Forecasting Model Using Correntropy-Induced Loss",
    "abstract": "Forecasting the particulate matter (PM) concentration in South Korea has\nbecome urgently necessary owing to its strong negative impact on human life. In\nmost statistical or machine learning methods, independent and identically\ndistributed data, for example, a Gaussian distribution, are assumed; however,\ntime series such as air pollution and weather data do not meet this assumption.\nIn this study, the maximum correntropy criterion for regression (MCCR) loss is\nused in an analysis of the statistical characteristics of air pollution and\nweather data. Rigorous seasonality adjustment of the air pollution and weather\ndata was performed because of their complex seasonality patterns and the\nheavy-tailed distribution of data even after deseasonalization. The MCCR loss\nwas applied to multiple models including conventional statistical models and\nstate-of-the-art machine learning models. The results show that the MCCR loss\nis more appropriate than the conventional mean squared error loss for\nforecasting extreme values.",
    "descriptor": "\nComments: Submitted to Journal of Mechanical Science and Technology (In review)\n",
    "authors": [
      "Jongsu Kim",
      "Changhoon Lee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.03032"
  },
  {
    "id": "arXiv:2106.03034",
    "title": "Minibatch and Momentum Model-based Methods for Stochastic Non-smooth  Non-convex Optimization",
    "abstract": "Stochastic model-based methods have received increasing attention lately due\nto their appealing robustness to the stepsize selection and provable efficiency\nguarantee for non-smooth non-convex optimization. To further improve the\nperformance of stochastic model-based methods, we make two important\nextensions. First, we propose a new minibatch algorithm which takes a set of\nsamples to approximate the model function in each iteration. For the first\ntime, we show that stochastic algorithms achieve linear speedup over the batch\nsize even for non-smooth and non-convex problems. To this end, we develop a\nnovel sensitivity analysis of the proximal mapping involved in each algorithm\niteration. Our analysis can be of independent interests in more general\nsettings. Second, motivated by the success of momentum techniques for convex\noptimization, we propose a new stochastic extrapolated model-based method to\npossibly improve the convergence in the non-smooth and non-convex setting. We\nobtain complexity guarantees for a fairly flexible range of extrapolation term.\nIn addition, we conduct experiments to show the empirical advantage of our\nproposed methods.",
    "descriptor": "\nComments: 36 pages, 9 figures\n",
    "authors": [
      "Qi Deng",
      "Wenzhi Gao"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03034"
  },
  {
    "id": "arXiv:2106.03035",
    "title": "Online Trading Models in the Forex Market Considering Transaction Costs",
    "abstract": "In recent years, a wide range of investment models have been created using\nartificial intelligence. Automatic trading by artificial intelligence can\nexpand the range of trading methods, such as by conferring the ability to\noperate 24 hours a day and the ability to trade with high frequency. Automatic\ntrading can also be expected to trade with more information than is available\nto humans if it can sufficiently consider past data. In this paper, we propose\nan investment agent based on a deep reinforcement learning model, which is an\nartificial intelligence model. The model considers the transaction costs\ninvolved in actual trading and creates a framework for trading over a long\nperiod of time so that it can make a large profit on a single trade. In doing\nso, it can maximize the profit while keeping transaction costs low. In\naddition, in consideration of actual operations, we use online learning so that\nthe system can continue to learn by constantly updating the latest online data\ninstead of learning with static data. This makes it possible to trade in\nnon-stationary financial markets by always incorporating current market trend\ninformation.",
    "descriptor": "\nComments: 7 pages, 2 figures, 6 tables\n",
    "authors": [
      "Koya Ishikawa",
      "Kazuhide Nakata"
    ],
    "subjectives": [
      "Trading and Market Microstructure (q-fin.TR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03035"
  },
  {
    "id": "arXiv:2106.03052",
    "title": "Brain Age Estimation From MRI Using Cascade Networks with Ranking Loss",
    "abstract": "Chronological age of healthy people is able to be predicted accurately using\ndeep neural networks from neuroimaging data, and the predicted brain age could\nserve as a biomarker for detecting aging-related diseases. In this paper, a\nnovel 3D convolutional network, called two-stage-age-network (TSAN), is\nproposed to estimate brain age from T1-weighted MRI data. Compared with\nexisting methods, TSAN has the following improvements. First, TSAN uses a\ntwo-stage cascade network architecture, where the first-stage network estimates\na rough brain age, then the second-stage network estimates the brain age more\naccurately from the discretized brain age by the first-stage network. Second,\nto our knowledge, TSAN is the first work to apply novel ranking losses in brain\nage estimation, together with the traditional mean square error (MSE) loss.\nThird, densely connected paths are used to combine feature maps with different\nscales. The experiments with $6586$ MRIs showed that TSAN could provide\naccurate brain age estimation, yielding mean absolute error (MAE) of $2.428$\nand Pearson's correlation coefficient (PCC) of $0.985$, between the estimated\nand chronological ages. Furthermore, using the brain age gap between brain age\nand chronological age as a biomarker, Alzheimer's disease (AD) and Mild\nCognitive Impairment (MCI) can be distinguished from healthy control (HC)\nsubjects by support vector machine (SVM). Classification AUC in AD/HC and\nMCI/HC was $0.904$ and $0.823$, respectively. It showed that brain age gap is\nan effective biomarker associated with risk of dementia, and has potential for\nearly-stage dementia risk screening. The codes and trained models have been\nreleased on GitHub: https://github.com/Milan-BUAA/TSAN-brain-age-estimation.",
    "descriptor": "\nComments: Accepted by IEEE transactions on Medical Imaging, 13 pages, 6 figures\n",
    "authors": [
      "Jian Cheng",
      "Ziyang Liu",
      "Hao Guan",
      "Zhenzhou Wu",
      "Haogang Zhu",
      "Jiyang Jiang",
      "Wei Wen",
      "Dacheng Tao",
      "Tao Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03052"
  },
  {
    "id": "arXiv:2106.03056",
    "title": "MURANA: A Generic Framework for Stochastic Variance-Reduced Optimization",
    "abstract": "We propose a generic variance-reduced algorithm, which we call MUltiple\nRANdomized Algorithm (MURANA), for minimizing a sum of several smooth functions\nplus a regularizer, in a sequential or distributed manner. Our method is\nformulated with general stochastic operators, which allow us to model various\nstrategies for reducing the computational complexity. For example, MURANA\nsupports sparse activation of the gradients, and also reduction of the\ncommunication load via compression of the update vectors. This versatility\nallows MURANA to cover many existing randomization mechanisms within a unified\nframework. However, MURANA also encodes new methods as special cases. We\nhighlight one of them, which we call ELVIRA, and show that it improves upon\nLoopless SVRG.",
    "descriptor": "",
    "authors": [
      "Laurent Condat",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03056"
  },
  {
    "id": "arXiv:2106.03061",
    "title": "Lawvere-Tierney topologies for computability theorists",
    "abstract": "In this article, we introduce certain kinds of computable reduction games\nwith imperfect information. One can view such a game as an extension of the\nnotion of Turing reduction, and generalized Weihrauch reduction as well. Based\non the work by Lee and van Oosten, we utilize these games for providing a\nconcrete description of the lattice of the Lawvere-Tierney topologies on the\neffective topos (equivalently, the subtoposes of the effective topos preordered\nby geometric inclusion). As an application, for instance, we show that there\nexists no minimal Lawvere-Tierney topology which is strictly above the identity\ntopology on the effective topos.",
    "descriptor": "",
    "authors": [
      "Takayuki Kihara"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2106.03061"
  },
  {
    "id": "arXiv:2106.03091",
    "title": "Regularization in ResNet with Stochastic Depth",
    "abstract": "Regularization plays a major role in modern deep learning. From classic\ntechniques such as L1,L2 penalties to other noise-based methods such as\nDropout, regularization often yields better generalization properties by\navoiding overfitting. Recently, Stochastic Depth (SD) has emerged as an\nalternative regularization technique for residual neural networks (ResNets) and\nhas proven to boost the performance of ResNet on many tasks [Huang et al.,\n2016]. Despite the recent success of SD, little is known about this technique\nfrom a theoretical perspective. This paper provides a hybrid analysis combining\nperturbation analysis and signal propagation to shed light on different\nregularization effects of SD. Our analysis allows us to derive principled\nguidelines for choosing the survival rates used for training with SD.",
    "descriptor": "\nComments: 24 pages, 15 figures\n",
    "authors": [
      "Soufiane Hayou",
      "Fadhel Ayed"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03091"
  },
  {
    "id": "arXiv:2106.03107",
    "title": "New complexity results and algorithms for min-max-min robust  combinatorial optimization",
    "abstract": "In this work we investigate the min-max-min robust optimization problem\napplied to combinatorial problems with uncertain cost-vectors which are\ncontained in a convex uncertainty set. The idea of the approach is to calculate\na set of k feasible solutions which are worst-case optimal if in each possible\nscenario the best of the k solutions would be implemented. It is known that the\nmin-max-min robust problem can be solved efficiently if k is at least the\ndimension of the problem, while it is theoretically and computationally hard if\nk is small. While both cases are well studied in the literature nothing is\nknown about the intermediate case, namely if k is smaller than but close to the\ndimension of the problem. We approach this open question and show that for a\nselection of combinatorial problems the min-max-min problem can be solved\nexactly and approximately in polynomial time if some problem specific values\nare fixed. Furthermore we approach a second open question and present the first\nimplementable algorithm with pseudopolynomial runtime for the case that k is at\nleast the dimension of the problem. The algorithm is based on a projected\nsubgradient method where the projection problem is solved by the classical\nFrank-Wolfe algorithm. Additionally we derive a branch & bound method to solve\nthe min-max-min problem for arbitrary values of k and perform tests on knapsack\nand shortest path instances. The experiments show that despite its theoretical\nimpact the projected subgradient method cannot compete with an already existing\nmethod. On the other hand the performance of the branch & bound method scales\nvery well with the number of solutions. Thus we are able to solve instances\nwhere k is above some small threshold very efficiently.",
    "descriptor": "",
    "authors": [
      "Jannis Kurtz"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.03107"
  },
  {
    "id": "arXiv:2106.03126",
    "title": "Predicting Quantum Potentials by Deep Neural Network and Metropolis  Sampling",
    "abstract": "The hybridizations of machine learning and quantum physics have caused\nessential impacts to the methodology in both fields. Inspired by quantum\npotential neural network, we here propose to solve the potential in the\nSchrodinger equation provided the eigenstate, by combining Metropolis sampling\nwith deep neural network, which we dub as Metropolis potential neural network\n(MPNN). A loss function is proposed to explicitly involve the energy in the\noptimization for its accurate evaluation. Benchmarking on the harmonic\noscillator and hydrogen atom, MPNN shows excellent accuracy and stability on\npredicting not just the potential to satisfy the Schrodinger equation, but also\nthe eigen-energy. Our proposal could be potentially applied to the ab-initio\nsimulations, and to inversely solving other partial differential equations in\nphysics and beyond.",
    "descriptor": "",
    "authors": [
      "Rui Hong",
      "Peng-Fei Zhou",
      "Bin Xi",
      "Jie Hu",
      "An-Chun Ji",
      "Shi-Ju Ran"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03126"
  },
  {
    "id": "arXiv:2106.03129",
    "title": "3D UAV Trajectory and Data Collection Optimisation via Deep  Reinforcement Learning",
    "abstract": "Unmanned aerial vehicles (UAVs) are now beginning to be deployed for\nenhancing the network performance and coverage in wireless communication.\nHowever, due to the limitation of their on-board power and flight time, it is\nchallenging to obtain an optimal resource allocation scheme for the\nUAV-assisted Internet of Things (IoT). In this paper, we design a new\nUAV-assisted IoT systems relying on the shortest flight path of the UAVs while\nmaximising the amount of data collected from IoT devices. Then, a deep\nreinforcement learning-based technique is conceived for finding the optimal\ntrajectory and throughput in a specific coverage area. After training, the UAV\nhas the ability to autonomously collect all the data from user nodes at a\nsignificant total sum-rate improvement while minimising the associated\nresources used. Numerical results are provided to highlight how our techniques\nstrike a balance between the throughput attained, trajectory, and the time\nspent. More explicitly, we characterise the attainable performance in terms of\nthe UAV trajectory, the expected reward and the total sum-rate.",
    "descriptor": "\nComments: 30 pages, UAV-assisted wireless network, trajectory, data collection, and deep reinforcement learning\n",
    "authors": [
      "Khoi Khac Nguyen",
      "Trung Q. Duong",
      "Tan Do-Duy",
      "Holger Claussen",
      "and Lajos Hanzo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03129"
  },
  {
    "id": "arXiv:2106.03153",
    "title": "Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation",
    "abstract": "With rapid progress in neural text-to-speech (TTS) models, personalized\nspeech generation is now in high demand for many applications. For practical\napplicability, a TTS model should generate high-quality speech with only a few\naudio samples from the given speaker, that are also short in length. However,\nexisting methods either require to fine-tune the model or achieve low\nadaptation quality without fine-tuning. In this work, we propose StyleSpeech, a\nnew TTS model which not only synthesizes high-quality speech but also\neffectively adapts to new speakers. Specifically, we propose Style-Adaptive\nLayer Normalization (SALN) which aligns gain and bias of the text input\naccording to the style extracted from a reference speech audio. With SALN, our\nmodel effectively synthesizes speech in the style of the target speaker even\nfrom single speech audio. Furthermore, to enhance StyleSpeech's adaptation to\nspeech from new speakers, we extend it to Meta-StyleSpeech by introducing two\ndiscriminators trained with style prototypes, and performing episodic training.\nThe experimental results show that our models generate high-quality speech\nwhich accurately follows the speaker's voice with single short-duration (1-3\nsec) speech audio, significantly outperforming baselines.",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Dongchan Min",
      "Dong Bok Lee",
      "Eunho Yang",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.03153"
  },
  {
    "id": "arXiv:2106.03156",
    "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via  Random Scaling",
    "abstract": "We develop a new method of online inference for a vector of parameters\nestimated by the Polyak-Ruppert averaging procedure of stochastic gradient\ndescent (SGD) algorithms. We leverage insights from time series regression in\neconometrics and construct asymptotically pivotal statistics via random\nscaling. Our approach is fully operational with online data and is rigorously\nunderpinned by a functional central limit theorem. Our proposed inference\nmethod has a couple of key advantages over the existing methods. First, the\ntest statistic is computed in an online fashion with only SGD iterates and the\ncritical values can be obtained without any resampling methods, thereby\nallowing for efficient implementation suitable for massive online data. Second,\nthere is no need to estimate the asymptotic variance and our inference method\nis shown to be robust to changes in the tuning parameters for SGD algorithms in\nsimulation experiments with synthetic data.",
    "descriptor": "\nComments: 16 pages, 5 figures, 5 tables\n",
    "authors": [
      "Sokbae Lee",
      "Yuan Liao",
      "Myung Hwan Seo",
      "Youngki Shin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.03156"
  },
  {
    "id": "arXiv:2106.03172",
    "title": "Stochastic EM methods with Variance Reduction for Penalised PET  Reconstructions",
    "abstract": "Expectation-maximization (EM) is a popular and well-established method for\nimage reconstruction in positron emission tomography (PET) but it often suffers\nfrom slow convergence. Ordered subset EM (OSEM) is an effective reconstruction\nalgorithm that provides significant acceleration during initial iterations, but\nit has been observed to enter a limit cycle. In this work, we investigate two\nclasses of algorithms for accelerating OSEM based on variance reduction for\npenalised PET reconstructions. The first is a stochastic variance reduced EM\nalgorithm, termed as SVREM, an extension of the classical EM to the stochastic\ncontext, by combining classical OSEM with insights from variance reduction\ntechniques for gradient descent. The second views OSEM as a preconditioned\nstochastic gradient ascent, and applies variance reduction techniques, i.e.,\nSAGA and SVRG, to estimate the update direction. We present several numerical\nexperiments to illustrate the efficiency and accuracy of the approaches. The\nnumerical results show that these approaches significantly outperform existing\nOSEM type methods for penalised PET reconstructions, and hold great potential.",
    "descriptor": "",
    "authors": [
      "Zeljko Kereta",
      "Robert Twyman",
      "Simon Arridge",
      "Kris Thielemans",
      "Bangti Jin"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03172"
  },
  {
    "id": "arXiv:2106.03202",
    "title": "Closed Ziv-Lempel factorization of the $m$-bonacci words",
    "abstract": "A word $w$ is said to be closed if it has a proper factor $x$ which occurs\nexactly twice in $w$, as a prefix and as a suffix of $w$. Based on the concept\nof Ziv-Lempel factorization, we define the closed $z$-factorization of finite\nand infinite words. Then we find the closed $z$-factorization of the infinite\n$m$-bonacci words for all $m \\geq 2$. We also classify closed prefixes of the\ninfinite $m$-bonacci words.",
    "descriptor": "",
    "authors": [
      "Marieh Jahannia",
      "Morteza Mohammad-noori",
      "Narad Rampersad",
      "Manon Stipulanti"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.03202"
  },
  {
    "id": "arXiv:2106.03208",
    "title": "Deep Learning-based Type Identification of Volumetric MRI Sequences",
    "abstract": "The analysis of Magnetic Resonance Imaging (MRI) sequences enables clinical\nprofessionals to monitor the progression of a brain tumor. As the interest for\nautomatizing brain volume MRI analysis increases, it becomes convenient to have\neach sequence well identified. However, the unstandardized naming of MRI\nsequences makes their identification difficult for automated systems, as well\nas makes it difficult for researches to generate or use datasets for machine\nlearning research. In the face of that, we propose a system for identifying\ntypes of brain MRI sequences based on deep learning. By training a\nConvolutional Neural Network (CNN) based on 18-layer ResNet architecture, our\nsystem can classify a volumetric brain MRI as a FLAIR, T1, T1c or T2 sequence,\nor whether it does not belong to any of these classes. The network was\nevaluated on publicly available datasets comprising both, pre-processed (BraTS\ndataset) and non-pre-processed (TCGA-GBM dataset), image types with diverse\nacquisition protocols, requiring only a few slices of the volume for training.\nOur system can classify among sequence types with an accuracy of 96.81%.",
    "descriptor": "",
    "authors": [
      "Jean Pablo Vieira de Mello",
      "Thiago M. Paix\u00e3o",
      "Rodrigo Berriel",
      "Mauricio Reyes",
      "Claudine Badue",
      "Alberto F. De Souza",
      "Thiago Oliveira-Santos"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03208"
  },
  {
    "id": "arXiv:2106.03212",
    "title": "Towards an Understanding of Benign Overfitting in Neural Networks",
    "abstract": "Modern machine learning models often employ a huge number of parameters and\nare typically optimized to have zero training loss; yet surprisingly, they\npossess near-optimal prediction performance, contradicting classical learning\ntheory. We examine how these benign overfitting phenomena occur in a two-layer\nneural network setting where sample covariates are corrupted with noise. We\naddress the high dimensional regime, where the data dimension $d$ grows with\nthe number $n$ of data points. Our analysis combines an upper bound on the bias\nwith matching upper and lower bounds on the variance of the interpolator (an\nestimator that interpolates the data). These results indicate that the excess\nlearning risk of the interpolator decays under mild conditions. We further show\nthat it is possible for the two-layer ReLU network interpolator to achieve a\nnear minimax-optimal learning rate, which to our knowledge is the first\ngeneralization result for such networks. Finally, our theory predicts that the\nexcess learning risk starts to increase once the number of parameters $s$ grows\nbeyond $O(n^2)$, matching recent empirical findings.",
    "descriptor": "",
    "authors": [
      "Zhu Li",
      "Zhi-Hua Zhou",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03212"
  },
  {
    "id": "arXiv:2106.03214",
    "title": "Lower Bounds on Stabilizer Rank",
    "abstract": "The stabilizer rank of a quantum state $\\psi$ is the minimal $r$ such that\n$\\left| \\psi \\right \\rangle = \\sum_{j=1}^r c_j \\left|\\varphi_j \\right\\rangle$\nfor $c_j \\in \\mathbb{C}$ and stabilizer states $\\varphi_j$. The running time of\nseveral classical simulation methods for quantum circuits is determined by the\nstabilizer rank of the $n$-th tensor power of single-qubit magic states.\nWe prove a lower bound of $\\Omega(n)$ on the stabilizer rank of such states,\nimproving a previous lower bound of $\\Omega(\\sqrt{n})$ of Bravyi, Smith and\nSmolin (arXiv:1506.01396). Further, we prove that for a sufficiently small\nconstant $\\delta$, the stabilizer rank of any state which is $\\delta$-close to\nthose states is $\\Omega(\\sqrt{n}/\\log n)$. This is the first non-trivial lower\nbound for approximate stabilizer rank.\nOur techniques rely on the representation of stabilizer states as quadratic\nfunctions over affine subspaces of $\\mathbb{F}_2^n$, and we use tools from\nanalysis of boolean functions and complexity theory. The proof of the first\nresult involves a careful analysis of directional derivatives of quadratic\npolynomials, whereas the proof of the second result uses Razborov-Smolensky low\ndegree polynomial approximations and correlation bounds against the majority\nfunction.",
    "descriptor": "",
    "authors": [
      "Shir Peleg",
      "Amir Shpilka",
      "Ben Lee Volk"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.03214"
  },
  {
    "id": "arXiv:2106.03227",
    "title": "Neural Tangent Kernel Maximum Mean Discrepancy",
    "abstract": "We present a novel neural network Maximum Mean Discrepancy (MMD) statistic by\nidentifying a connection between neural tangent kernel (NTK) and MMD statistic.\nThis connection enables us to develop a computationally efficient and\nmemory-efficient approach to compute the MMD statistic and perform neural\nnetwork based two-sample tests towards addressing the long-standing challenge\nof memory and computational complexity of the MMD statistic, which is essential\nfor online implementation to assimilate new samples. Theoretically, such a\nconnection allows us to understand the properties of the new test statistic,\nsuch as Type-I error and testing power for performing the two-sample test, by\nleveraging analysis tools for kernel MMD. Numerical experiments on synthetic\nand real-world datasets validate the theory and demonstrate the effectiveness\nof the proposed NTK-MMD statistic.",
    "descriptor": "",
    "authors": [
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.03227"
  },
  {
    "id": "arXiv:2106.03235",
    "title": "On the Optimality of Backward Regression: Sparse Recovery and Subset  Selection",
    "abstract": "Sparse recovery and subset selection are fundamental problems in varied\ncommunities, including signal processing, statistics and machine learning.\nHerein, we focus on an important greedy algorithm for these problems: Backward\nStepwise Regression. We present novel guarantees for the algorithm, propose an\nefficient, numerically stable implementation, and put forth Stepwise Regression\nwith Replacement (SRR), a new family of two-stage algorithms that employs both\nforward and backward steps for compressed sensing problems. Prior work on the\nbackward algorithm has proven its optimality for the subset selection problem,\nprovided the residual associated with the optimal solution is small enough.\nHowever, the existing bounds on the residual magnitude are NP-hard to compute.\nIn contrast, our main theoretical result includes a bound that can be computed\nin polynomial time, depends chiefly on the smallest singular value of the\nmatrix, and also extends to the method of magnitude pruning. In addition, we\nreport numerical experiments highlighting crucial differences between forward\nand backward greedy algorithms and compare SRR against popular two-stage\nalgorithms for compressed sensing. Remarkably, SRR algorithms generally\nmaintain good sparse recovery performance on coherent dictionaries. Further, a\nparticular SRR algorithm has an edge over Subspace Pursuit.",
    "descriptor": "",
    "authors": [
      "Sebatian Ament",
      "Carla Gomes"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.03235"
  },
  {
    "id": "arXiv:2106.03240",
    "title": "A Projection-based Reduced-order Method for Electron Transport Problems  with Long-range Interactions",
    "abstract": "Long-range interactions play a central role in electron transport. At the\nsame time, they present a challenge for direct computer simulations, since\nsufficiently large portions of the bath have to be included in the computation\nto accurately compute the Coulomb potential. This article presents a\nreduced-order approach, by deriving an open quantum model for the reduced\ndensity-matrix. To treat the transient dynamics, the problem is placed in a\nreduced-order framework. The dynamics, described by the Liouville von Neumann\nequation, is projected to subspaces using a Petrov-Galerkin projection. In\norder to recover the global electron density profile as a vehicle to compute\nthe Coulomb potential, we propose a domain decomposition approach, where the\ncomputational domain also includes segments of the bath that are selected using\nlogarithmic grids. This approach leads to a multi-component self-energy that\nenters the effective Hamiltonian. We demonstrate the accuracy of the reduced\nmodel using a molecular junction built from a Lithium chains.",
    "descriptor": "",
    "authors": [
      "Weiqi Chu",
      "Xiantao Li"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Numerical Analysis (math.NA)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03240"
  },
  {
    "id": "arXiv:2106.03249",
    "title": "Accurate Self-Configuration of Rectangular Multiport Interferometers",
    "abstract": "Multiport interferometers based on integrated beamsplitter meshes are widely\nused in photonic technologies. While the rectangular mesh is favored for its\ncompactness and uniformity, its geometry resists conventional\nself-configuration approaches, which are essential to programming large meshes\nin the presence of fabrication error. Here, we present a new configuration\nalgorithm, related to the $2\\times 2$ block decomposition of a unitary matrix,\nthat overcomes this limitation. Our proposed algorithm is robust to errors,\nrequires no prior knowledge of the process variations, and relies only on\nexternal sources and detectors. We show that self-configuration using this\ntechnique reduces the effect of fabrication errors by the same quadratic factor\nobserved in triangular meshes. This relaxes a significant limit to the size of\nmultiport interferometers, removing a major roadblock to the scaling of optical\nquantum and machine-learning hardware.",
    "descriptor": "\nComments: 5 pages, 5 figures. SM: 3 pages, 3, figures\n",
    "authors": [
      "Ryan Hamerly",
      "Saumil Bandyopadhyay",
      "Dirk Englund"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.03249"
  },
  {
    "id": "arXiv:2106.03268",
    "title": "Method of Alternating Projection for the Absolute Value Equation",
    "abstract": "A novel approach for solving the general absolute value equation $Ax+B|x| =\nc$ where $A,B\\in \\mathbb{R}^{m\\times n}$ and $c\\in \\mathbb{R}^m$ is presented.\nWe reformulate the equation as a feasibility problem which we solve via the\nmethod of alternating projections (MAP). The fixed points set of the\nalternating projections map is characterized under nondegeneracy conditions on\n$A$ and $B$. Furthermore, we prove linear convergence of the algorithm. Unlike\nmost of the existing approaches in the literature, the algorithm presented here\nis capable of handling problems with $m\\neq n$, both theoretically and\nnumerically.",
    "descriptor": "\nComments: 38 pages, 1 figure, 3 tables\n",
    "authors": [
      "Jan Harold Alcantara",
      "Jein-Shan Chen",
      "Matthew K. Tam"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03268"
  },
  {
    "id": "arXiv:2106.03272",
    "title": "Signatured Deep Fictitious Play for Mean Field Games with Common Noise",
    "abstract": "Existing deep learning methods for solving mean-field games (MFGs) with\ncommon noise fix the sampling common noise paths and then solve the\ncorresponding MFGs. This leads to a nested-loop structure with millions of\nsimulations of common noise paths in order to produce accurate solutions, which\nresults in prohibitive computational cost and limits the applications to a\nlarge extent. In this paper, based on the rough path theory, we propose a novel\nsingle-loop algorithm, named signatured deep fictitious play, by which we can\nwork with the unfixed common noise setup to avoid the nested-loop structure and\nreduce the computational complexity significantly. The proposed algorithm can\naccurately capture the effect of common uncertainty changes on mean-field\nequilibria without further training of neural networks, as previously needed in\nthe existing machine learning algorithms. The efficiency is supported by three\napplications, including linear-quadratic MFGs, mean-field portfolio game, and\nmean-field game of optimal consumption and investment. Overall, we provide a\nnew point of view from the rough path theory to solve MFGs with common noise\nwith significantly improved efficiency and an extensive range of applications.\nIn addition, we report the first deep learning work to deal with extended MFGs\n(a mean-field interaction via both the states and controls) with common noise.",
    "descriptor": "\nComments: Published at ICML 2021\n",
    "authors": [
      "Ming Min",
      "Ruimeng Hu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2106.03272"
  },
  {
    "id": "arXiv:2106.03325",
    "title": "Routing optimization on power packet dispatching system based on energy  loss minimization",
    "abstract": "Power packet dispatching system has been proposed for smart power management\nin the form of discretized packet. In this paper, we discuss the routing\noptimization of power packets on the network of power routers. We propose a\ncost metric for the power packet delivery by circuit analysis of the router\nnetwork. Using the metric, we formulate the optimization problem as a general\nshortest path problem from a source node to a load node. The result of\nnumerical simulations shows that the proposed algorithm can allocate\ndistributed power sources to load demands and identify the optimal path for the\npower delivery.",
    "descriptor": "\nComments: This paper was submitted to Nonlinear Theory and Its Applications, IEICE on May 21, 2021\n",
    "authors": [
      "Shiu Mochiyama",
      "Kazuhiro Koto",
      "Takashi Hikihara"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.03325"
  },
  {
    "id": "arXiv:2106.03357",
    "title": "Evaluating State-of-the-Art Classification Models Against Bayes  Optimality",
    "abstract": "Evaluating the inherent difficulty of a given data-driven classification\nproblem is important for establishing absolute benchmarks and evaluating\nprogress in the field. To this end, a natural quantity to consider is the\n\\emph{Bayes error}, which measures the optimal classification error\ntheoretically achievable for a given data distribution. While generally an\nintractable quantity, we show that we can compute the exact Bayes error of\ngenerative models learned using normalizing flows. Our technique relies on a\nfundamental result, which states that the Bayes error is invariant under\ninvertible transformation. Therefore, we can compute the exact Bayes error of\nthe learned flow models by computing it for Gaussian base distributions, which\ncan be done efficiently using Holmes-Diaconis-Ross integration. Moreover, we\nshow that by varying the temperature of the learned flow models, we can\ngenerate synthetic datasets that closely resemble standard benchmark datasets,\nbut with almost any desired Bayes error. We use our approach to conduct a\nthorough investigation of state-of-the-art classification models, and find that\nin some -- but not all -- cases, these models are capable of obtaining accuracy\nvery near optimal. Finally, we use our method to evaluate the intrinsic\n\"hardness\" of standard benchmark datasets, and classes within those datasets.",
    "descriptor": "",
    "authors": [
      "Ryan Theisen",
      "Huan Wang",
      "Lav R. Varshney",
      "Caiming Xiong",
      "Richard Socher"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03357"
  },
  {
    "id": "arXiv:2106.03361",
    "title": "Application of neural networks to classification of data of the TUS  orbital telescope",
    "abstract": "We employ neural networks for classification of data of the TUS fluorescence\ntelescope, the world's first orbital detector of ultra-high energy cosmic rays.\nWe focus on two particular types of signals in the TUS data: track-like flashes\nproduced by cosmic ray hits of the photodetector and flashes that originated\nfrom distant lightnings. We demonstrate that even simple neural networks\ncombined with certain conventional methods of data analysis can be highly\neffective in tasks of classification of data of fluorescence telescopes.",
    "descriptor": "\nComments: 21 pages, 15 figures\n",
    "authors": [
      "Mikhail Zotov"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03361"
  },
  {
    "id": "arXiv:2106.03365",
    "title": "Generalized Linear Bandits with Local Differential Privacy",
    "abstract": "Contextual bandit algorithms are useful in personalized online\ndecision-making. However, many applications such as personalized medicine and\nonline advertising require the utilization of individual-specific information\nfor effective learning, while user's data should remain private from the server\ndue to privacy concerns. This motivates the introduction of local differential\nprivacy (LDP), a stringent notion in privacy, to contextual bandits. In this\npaper, we design LDP algorithms for stochastic generalized linear bandits to\nachieve the same regret bound as in non-privacy settings. Our main idea is to\ndevelop a stochastic gradient-based estimator and update mechanism to ensure\nLDP. We then exploit the flexibility of stochastic gradient descent (SGD),\nwhose theoretical guarantee for bandit problems is rarely explored, in dealing\nwith generalized linear bandits. We also develop an estimator and update\nmechanism based on Ordinary Least Square (OLS) for linear bandits. Finally, we\nconduct experiments with both simulation and real-world datasets to demonstrate\nthe consistently superb performance of our algorithms under LDP constraints\nwith reasonably small parameters $(\\varepsilon, \\delta)$ to ensure strong\nprivacy protection.",
    "descriptor": "",
    "authors": [
      "Yuxuan Han",
      "Zhipeng Liang",
      "Yang Wang",
      "Jiheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03365"
  },
  {
    "id": "arXiv:2106.03395",
    "title": "How to Evaluate Uncertainty Estimates in Machine Learning for  Regression?",
    "abstract": "As neural networks become more popular, the need for accompanying uncertainty\nestimates increases. The current testing methodology focusses on how good the\npredictive uncertainty estimates explain the differences between predictions\nand observations in a previously unseen test set. Intuitively this is a logical\napproach. The current setup of benchmark data sets also allows easy comparison\nbetween the different methods. We demonstrate, however, through both\ntheoretical arguments and simulations that this way of evaluating the quality\nof uncertainty estimates has serious flaws. Firstly, it cannot disentangle the\naleatoric from the epistemic uncertainty. Secondly, the current methodology\nconsiders the uncertainty averaged over all test samples, implicitly averaging\nout overconfident and underconfident predictions. When checking if the correct\nfraction of test points falls inside prediction intervals, a good score on\naverage gives no guarantee that the intervals are sensible for individual\npoints. We demonstrate through practical examples that these effects can result\nin favoring a method, based on the predictive uncertainty, that has undesirable\nbehaviour of the confidence intervals. Finally, we propose a simulation-based\ntesting approach that addresses these problems while still allowing easy\ncomparison between different methods.",
    "descriptor": "\nComments: 22 pages, 10 figures\n",
    "authors": [
      "Laurens Sluijterman",
      "Eric Cator",
      "Tom Heskes"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03395"
  },
  {
    "id": "arXiv:2106.03409",
    "title": "Improving Lower Bounds for Equitable Chromatic Number",
    "abstract": "In many practical applications the underlying graph must be as equitable\ncolored as possible. A coloring is called equitable if the number of vertices\ncolored with each color differs by at most one, and the least number of colors\nfor which a graph has such a coloring is called its equitable chromatic number.\nWe introduce a new integer linear programming approach for studying the\nequitable coloring number of a graph and show how to use it for improving lower\nbounds for this number. The two stage method is based on finding or upper\nbounding the maximum cardinality of an equitable color class in a valid\nequitable coloring and, then, sequentially improving the lower bound for the\nequitable coloring number. The computational experiments were carried out on\nDIMACS graphs and other graphs from the literature.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Emanuel Florentin Olariu",
      "Cristian Frasinaru"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.03409"
  },
  {
    "id": "arXiv:2106.03419",
    "title": "Data Augmentation Methods for End-to-end Speech Recognition on  Distant-Talk Scenarios",
    "abstract": "Although end-to-end automatic speech recognition (E2E ASR) has achieved great\nperformance in tasks that have numerous paired data, it is still challenging to\nmake E2E ASR robust against noisy and low-resource conditions. In this study,\nwe investigated data augmentation methods for E2E ASR in distant-talk\nscenarios. E2E ASR models are trained on the series of CHiME challenge\ndatasets, which are suitable tasks for studying robustness against noisy and\nspontaneous speech. We propose to use three augmentation methods and thier\ncombinations: 1) data augmentation using text-to-speech (TTS) data, 2)\ncycle-consistent generative adversarial network (Cycle-GAN) augmentation\ntrained to map two different audio characteristics, the one of clean speech and\nof noisy recordings, to match the testing condition, and 3) pseudo-label\naugmentation provided by the pretrained ASR module for smoothing label\ndistributions. Experimental results using the CHiME-6/CHiME-4 datasets show\nthat each augmentation method individually improves the accuracy on top of the\nconventional SpecAugment; further improvements are obtained by combining these\napproaches. We achieved 4.3\\% word error rate (WER) reduction, which was more\nsignificant than that of the SpecAugment, when we combine all three\naugmentations for the CHiME-6 task.",
    "descriptor": "\nComments: Accepted for Interspeech2021\n",
    "authors": [
      "Emiru Tsunoo",
      "Kentaro Shibata",
      "Chaitanya Narisetty",
      "Yosuke Kashiwagi",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.03419"
  },
  {
    "id": "arXiv:2106.03445",
    "title": "Detecting plainness in PSPACE",
    "abstract": "We show that groups presented by inverse-closed finite convergent\nlength-reducing rewriting systems are characterised by a striking geometric\nproperty: non-degenerate triangles in their Cayley graph have uniformly bounded\nsize. We then define a simple relation on the set of non-trivial finite-order\nelements of the group, and prove that the group is plain if and only if this\nrelation is transitive on a bounded set. This in turn gives rise to a\ndeterministic quadratic space algorithm (in the size of the longest right-hand\nside of a rule in the rewriting system) to decide whether or not the group is\nplain.",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Murray Elder",
      "Adam Piggott"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.03445"
  },
  {
    "id": "arXiv:2106.03455",
    "title": "Knowledge-aware Deep Framework for Collaborative Skin Lesion  Segmentation and Melanoma Recognition",
    "abstract": "Deep learning techniques have shown their superior performance in\ndermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a\nchallenging task due to the difficulty of incorporating the useful\ndermatologist clinical knowledge into the learning process. In this paper, we\npropose a novel knowledge-aware deep framework that incorporates some clinical\nknowledge into collaborative learning of two important melanoma diagnosis\ntasks, i.e., skin lesion segmentation and melanoma recognition. Specifically,\nto exploit the knowledge of morphological expressions of the lesion region and\nalso the periphery region for melanoma identification, a lesion-based pooling\nand shape extraction (LPSE) scheme is designed, which transfers the structure\ninformation obtained from skin lesion segmentation into melanoma recognition.\nMeanwhile, to pass the skin lesion diagnosis knowledge from melanoma\nrecognition to skin lesion segmentation, an effective diagnosis guided feature\nfusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual\nlearning mechanism that further promotes the inter-task cooperation, and thus\niteratively improves the joint learning capability of the model for both skin\nlesion segmentation and melanoma recognition. Experimental results on two\npublicly available skin lesion datasets show the effectiveness of the proposed\nmethod for melanoma analysis.",
    "descriptor": "\nComments: Pattern Recognition\n",
    "authors": [
      "Xiaohong Wang",
      "Xudong Jiang",
      "Henghui Ding",
      "Yuqian Zhao",
      "Jun Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03455"
  },
  {
    "id": "arXiv:2106.03477",
    "title": "BayesIMP: Uncertainty Quantification for Causal Data Fusion",
    "abstract": "While causal models are becoming one of the mainstays of machine learning,\nthe problem of uncertainty quantification in causal inference remains\nchallenging. In this paper, we study the causal data fusion problem, where\ndatasets pertaining to multiple causal graphs are combined to estimate the\naverage treatment effect of a target variable. As data arises from multiple\nsources and can vary in quality and quantity, principled uncertainty\nquantification becomes essential. To that end, we introduce Bayesian\nInterventional Mean Processes, a framework which combines ideas from\nprobabilistic integration and kernel mean embeddings to represent\ninterventional distributions in the reproducing kernel Hilbert space, while\ntaking into account the uncertainty within each causal graph. To demonstrate\nthe utility of our uncertainty estimation, we apply our method to the Causal\nBayesian Optimisation task and show improvements over state-of-the-art methods.",
    "descriptor": "\nComments: 10 pages main text, 10 pages supplementary materials\n",
    "authors": [
      "Siu Lun Chau",
      "Jean-Fran\u00e7ois Ton",
      "Javier Gonz\u00e1lez",
      "Yee Whye Teh",
      "Dino Sejdinovic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03477"
  },
  {
    "id": "arXiv:2106.03480",
    "title": "A Distance Covariance-based Kernel for Nonlinear Causal Clustering in  Heterogeneous Populations",
    "abstract": "We consider the problem of causal structure learning in the setting of\nheterogeneous populations, i.e., populations in which a single causal structure\ndoes not adequately represent all population members, as is common in\nbiological and social sciences. To this end, we introduce a distance\ncovariance-based kernel designed specifically to measure the similarity between\nthe underlying nonlinear causal structures of different samples. This kernel\nenables us to perform clustering to identify the homogeneous subpopulations.\nIndeed, we prove the corresponding feature map is a statistically consistent\nestimator of nonlinear independence structure, rendering the kernel itself a\nstatistical test for the hypothesis that sets of samples come from different\ngenerating causal structures. We can then use existing methods to learn a\ncausal structure for each of these subpopulations. We demonstrate using our\nkernel for causal clustering with an application in genetics, allowing us to\nreason about the latent transcription factor networks regulating measured gene\nexpression levels.",
    "descriptor": "",
    "authors": [
      "Alex Markham",
      "Moritz Grosse-Wentrup"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03480"
  },
  {
    "id": "arXiv:2106.03485",
    "title": "Representation mitosis in wide neural networks",
    "abstract": "Deep neural networks (DNNs) defy the classical bias-variance trade-off:\nadding parameters to a DNN that exactly interpolates its training data will\ntypically improve its generalisation performance. Explaining the mechanism\nbehind the benefit of such over-parameterisation is an outstanding challenge\nfor deep learning theory. Here, we study the last layer representation of\nvarious deep architectures such as Wide-ResNets for image classification and\nfind evidence for an underlying mechanism that we call *representation\nmitosis*: if the last hidden representation is wide enough, its neurons tend to\nsplit into groups which carry identical information, and differ from each other\nonly by a statistically independent noise. Like in a mitosis process, the\nnumber of such groups, or ``clones'', increases linearly with the width of the\nlayer, but only if the width is above a critical value. We show that a key\ningredient to activate mitosis is continuing the training process until the\ntraining error is zero. Finally, we show that in one of the learning tasks we\nconsidered, a wide model with several automatically developed clones performs\nsignificantly better than a deep ensemble based on architectures in which the\nlast layer has the same size as the clones.",
    "descriptor": "",
    "authors": [
      "Diego Doimo",
      "Aldo Glielmo",
      "Sebastian Goldt",
      "Alessandro Laio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03485"
  },
  {
    "id": "arXiv:2106.03494",
    "title": "Weakly-supervised word-level pronunciation error detection in non-native  English speech",
    "abstract": "We propose a weakly-supervised model for word-level mispronunciation\ndetection in non-native (L2) English speech. To train this model, phonetically\ntranscribed L2 speech is not required and we only need to mark mispronounced\nwords. The lack of phonetic transcriptions for L2 speech means that the model\nhas to learn only from a weak signal of word-level mispronunciations. Because\nof that and due to the limited amount of mispronounced L2 speech, the model is\nmore likely to overfit. To limit this risk, we train it in a multi-task setup.\nIn the first task, we estimate the probabilities of word-level\nmispronunciation. For the second task, we use a phoneme recognizer trained on\nphonetically transcribed L1 speech that is easily accessible and can be\nautomatically annotated. Compared to state-of-the-art approaches, we improve\nthe accuracy of detecting word-level pronunciation errors in AUC metric by 30%\non the GUT Isle Corpus of L2 Polish speakers, and by 21.5% on the Isle Corpus\nof L2 German and Italian speakers.",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Daniel Korzekwa",
      "Jaime Lorenzo-Trueba",
      "Thomas Drugman",
      "Shira Calamaro",
      "Bozena Kostek"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03494"
  },
  {
    "id": "arXiv:2106.03511",
    "title": "Task-driven Semantic Coding via Reinforcement Learning",
    "abstract": "Task-driven semantic video/image coding has drawn considerable attention with\nthe development of intelligent media applications, such as license plate\ndetection, face detection, and medical diagnosis, which focuses on maintaining\nthe semantic information of videos/images. Deep neural network (DNN)-based\ncodecs have been studied for this purpose due to their inherent end-to-end\noptimization mechanism. However, the traditional hybrid coding framework cannot\nbe optimized in an end-to-end manner, which makes task-driven semantic fidelity\nmetric unable to be automatically integrated into the rate-distortion\noptimization process. Therefore, it is still attractive and challenging to\nimplement task-driven semantic coding with the traditional hybrid coding\nframework, which should still be widely used in practical industry for a long\ntime. To solve this challenge, we design semantic maps for different tasks to\nextract the pixelwise semantic fidelity for videos/images. Instead of directly\nintegrating the semantic fidelity metric into traditional hybrid coding\nframework, we implement task-driven semantic coding by implementing semantic\nbit allocation based on reinforcement learning (RL). We formulate the semantic\nbit allocation problem as a Markov decision process (MDP) and utilize one RL\nagent to automatically determine the quantization parameters (QPs) for\ndifferent coding units (CUs) according to the task-driven semantic fidelity\nmetric. Extensive experiments on different tasks, such as classification,\ndetection and segmentation, have demonstrated the superior performance of our\napproach by achieving an average bitrate saving of 34.39% to 52.62% over the\nHigh Efficiency Video Coding (H.265/HEVC) anchor under equivalent task-related\nsemantic fidelity.",
    "descriptor": "\nComments: 13 pages, accepted by IEEE Transactions on Image Processing (TIP)\n",
    "authors": [
      "Xin Li",
      "Jun Shi",
      "Zhibo Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.03511"
  },
  {
    "id": "arXiv:2106.03519",
    "title": "Closed-Loop Wireless Power Transfer with Adaptive Waveform and  Beamforming: Design, Prototype, and Experiment",
    "abstract": "In this paper, we design, prototype, and experiment a closed-loop radiative\nwireless power transfer (WPT) system with adaptive waveform and beamforming\nusing limited feedback. Spatial and frequency domains are exploited by jointly\nutilizing multi-sine waveform and multi-antenna beamforming at the transmitter\nin WPT system to adapt to the multipath fading channel and boost the output dc\npower. A closed-loop architecture based on a codebook design and a low\ncomplexity over-the-air limited feedback using an IEEE 802.15.4 RF interface is\nproposed. The codebook consists of multiple codewords where each codeword\nrepresents particular waveform and beamforming. The transmitter sweeps through\nthe codebook and then the receiver feeds back the index of the optimal\ncodeword, so that the waveform and beamforming can be adapted to the multipath\nfading channel to maximize the output dc power without requiring explicit\nchannel estimation and the knowledge of accurate Channel State Information. The\nproposed closed-loop WPT with adaptive waveform and beamforming using limited\nfeedback is prototyped using a Software Defined Radio equipment and measured in\na real indoor environment. The measurement results show that the proposed\nclosed-loop WPT with adaptive waveform and beamforming can increase the output\ndc power by up to 14.7 dB compared with the conventional single-tone and\nsingle-antenna WPT system.",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Shanpu Shen",
      "Junghoon Kim",
      "Bruno Clerckx"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.03519"
  },
  {
    "id": "arXiv:2106.03542",
    "title": "How Tight Can PAC-Bayes be in the Small Data Regime?",
    "abstract": "In this paper, we investigate the question: Given a small number of\ndatapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be\nmade? For such small datasets, test set bounds adversely affect generalisation\nperformance by discarding data. In this setting, PAC-Bayes bounds are\nespecially attractive, due to their ability to use all the data to\nsimultaneously learn a posterior and bound its generalisation risk. We focus on\nthe case of i.i.d. data with a bounded loss and consider the generic PAC-Bayes\ntheorem of Germain et al. (2009) and Begin et al. (2016). While their theorem\nis known to recover many existing PAC-Bayes bounds, it is unclear what the\ntightest bound derivable from their framework is. Surprisingly, we show that\nfor a fixed learning algorithm and dataset, the tightest bound of this form\ncoincides with the tightest bound of the more restrictive family of bounds\nconsidered in Catoni (2007). In contrast, in the more natural case of\ndistributions over datasets, we give examples (both analytic and numerical)\nshowing that the family of bounds in Catoni (2007) can be suboptimal. Within\nthe proof framework of Germain et al. (2009) and Begin et al. (2016), we\nestablish a lower bound on the best bound achievable in expectation, which\nrecovers the Chernoff test set bound in the case when the posterior is equal to\nthe prior. Finally, to illustrate how tight these bounds can potentially be, we\nstudy a synthetic one-dimensional classification task in which it is feasible\nto meta-learn both the prior and the form of the bound to obtain the tightest\nPAC-Bayes and test set bounds possible. We find that in this simple, controlled\nscenario, PAC-Bayes bounds are surprisingly competitive with comparable,\ncommonly used Chernoff test set bounds. However, the sharpest test set bounds\nstill lead to better guarantees on the generalisation error than the PAC-Bayes\nbounds we consider.",
    "descriptor": "\nComments: Preprint. Under review\n",
    "authors": [
      "Andrew Y. K. Foong",
      "Wessel P. Bruinsma",
      "David R. Burt",
      "Richard E. Turner"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.03542"
  },
  {
    "id": "arXiv:2106.03565",
    "title": "Comparative study and limits of different level-set formulations for the  modeling of anisotropic grain growth",
    "abstract": "Four different finite element level-set (FE-LS) formulations are compared for\nthe modeling of grain growth in the context of polycrystalline structures and,\nmoreover, two of them are presented for the first time using anisotropic grain\nboundary (GB) energy and mobility. Mean values and distributions are compared\nusing the four formulations. First, we present the strong and weak formulations\nfor the different models and the crystallographic parameters used at the\nmesoscopic scale. Second, some Grim Reaper analytical cases are presented and\ncompared with the simulation results, here the evolutions of individual\nmultiple junctions are followed. Additionally, large scale simulations are\npresented. Anisotropic GB energy and mobility are respectively defined as\nfunctions of the misorientation/inclination and disorientation. The evolution\nof the disorientation distribution function (DDF) is computed and its evolution\nis in accordance with prior works. We found that the formulation called\n\"Anisotropic\" is the more physical one but it could be replaced at the\nmesoscopic scale by an Isotropic formulation for simple microstructures\npresenting an initial Mackenzie-type DDF.",
    "descriptor": "",
    "authors": [
      "Brayan Murgas",
      "Sebastian Florez",
      "Nathalie Bozzolo",
      "Julien Fausty",
      "Marc Bernacki"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.03565"
  },
  {
    "id": "arXiv:2106.03585",
    "title": "Decentralized Optimization with Heterogeneous Delays: a Continuous-Time  Approach",
    "abstract": "In decentralized optimization, nodes of a communication network privately\npossess a local objective function, and communicate using gossip-based methods\nin order to minimize the average of these per-node objectives. While\nsynchronous algorithms can be heavily slowed down by a few nodes and edges in\nthe graph (the straggler problem), their asynchronous counterparts lack from a\nsharp analysis taking into account heterogeneous delays in the communication\nnetwork. In this paper, we propose a novel continuous-time framework to analyze\nasynchronous algorithms, which does not require to define a global ordering of\nthe events, and allows to finely characterize the time complexity in the\npresence of (heterogeneous) delays. Using this framework, we describe a fully\nasynchronous decentralized algorithm to minimize the sum of smooth and strongly\nconvex functions. Our algorithm (DCDM, Delayed Coordinate Dual Method), based\non delayed randomized gossip communications and local computational updates,\nachieves an asynchronous speed-up: the rate of convergence is tightly\ncharacterized in terms of the eigengap of the graph weighted by local delays\nonly, instead of the global worst-case delays as in previous analyses.",
    "descriptor": "",
    "authors": [
      "Mathieu Even",
      "Hadrien Hendrikx",
      "Laurent Massoulie"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Multiagent Systems (cs.MA)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03585"
  },
  {
    "id": "arXiv:2106.03587",
    "title": "2-distance 4-coloring of planar subcubic graphs with girth at least 21",
    "abstract": "A $2$-distance $k$-coloring of a graph is a proper vertex $k$-coloring where\nvertices at distance at most 2 cannot share the same color. We prove the\nexistence of a $2$-distance $4$-coloring for planar subcubic graphs with girth\nat least 21. We also show a construction of a planar subcubic graph of girth 11\nthat is not $2$-distance $4$-colorable.",
    "descriptor": "\nComments: 21 pages, 14 figures. arXiv admin note: text overlap with arXiv:2103.11687\n",
    "authors": [
      "Hoang La",
      "Mickael Montassier"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.03587"
  },
  {
    "id": "arXiv:2106.03589",
    "title": "Random features for adaptive nonlinear control and prediction",
    "abstract": "A key assumption in the theory of adaptive control for nonlinear systems is\nthat the uncertainty of the system can be expressed in the linear span of a set\nof known basis functions. While this assumption leads to efficient algorithms,\nverifying it in practice can be difficult, particularly for complex systems.\nHere we leverage connections between reproducing kernel Hilbert spaces, random\nFourier features, and universal approximation theory to propose a\ncomputationally tractable algorithm for both adaptive control and adaptive\nprediction that does not rely on a linearly parameterized unknown.\nSpecifically, we approximate the unknown dynamics with a finite expansion in\n$\\textit{random}$ basis functions, and provide an explicit guarantee on the\nnumber of random features needed to track a desired trajectory with high\nprobability. Remarkably, our explicit bounds only depend\n$\\textit{polynomially}$ on the underlying parameters of the system, allowing\nour proposed algorithms to efficiently scale to high-dimensional systems. We\nstudy a setting where the unknown dynamics splits into a component that can be\nmodeled through available physical knowledge of the system and a component that\nlives in a reproducing kernel Hilbert space. Our algorithms simultaneously\nadapt over parameters for physical basis functions and random features to learn\nboth components of the dynamics online.",
    "descriptor": "",
    "authors": [
      "Nicholas M. Boffi",
      "Stephen Tu",
      "Jean-Jacques E. Slotine"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03589"
  },
  {
    "id": "arXiv:2106.03591",
    "title": "Calibrating multi-dimensional complex ODE from noisy data via deep  neural networks",
    "abstract": "Ordinary differential equations (ODEs) are widely used to model complex\ndynamics that arises in biology, chemistry, engineering, finance, physics, etc.\nCalibration of a complicated ODE system using noisy data is generally very\ndifficult. In this work, we propose a two-stage nonparametric approach to\naddress this problem. We first extract the de-noised data and their higher\norder derivatives using boundary kernel method, and then feed them into a\nsparsely connected deep neural network with ReLU activation function. Our\nmethod is able to recover the ODE system without being subject to the curse of\ndimensionality and complicated ODE structure. When the ODE possesses a general\nmodular structure, with each modular component involving only a few input\nvariables, and the network architecture is properly chosen, our method is\nproven to be consistent. Theoretical properties are corroborated by an\nextensive simulation study that demonstrates the validity and effectiveness of\nthe proposed method. Finally, we use our method to simultaneously characterize\nthe growth rate of Covid-19 infection cases from 50 states of the USA.",
    "descriptor": "",
    "authors": [
      "Kexuan Li",
      "Fangfang Wang",
      "Ruiqi Liu",
      "Fan Yang",
      "Zuofeng Shang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.03591"
  },
  {
    "id": "arXiv:2106.03604",
    "title": "Mutual control of stochastic switching for two electrically coupled  superparamagnetic tunnel junctions",
    "abstract": "Superparamagnetic tunnel junctions (SMTJs) are promising sources for the\nrandomness required by some compact and energy-efficient computing schemes.\nCoupling SMTJs gives rise to collective behavior that could be useful for\ncognitive computing. We use a simple linear electrical circuit to mutually\ncouple two SMTJs through their stochastic electrical transitions. When one SMTJ\nmakes a thermally induced transition, the voltage across both SMTJs changes,\nmodifying the transition rates of both. This coupling leads to significant\ncorrelation between the states of the two devices. Using fits to a generalized\nN\\'eel-Brown model for the individual thermally bistable magnetic devices, we\ncan accurately reproduce the behavior of the coupled devices with a Markov\nmodel.",
    "descriptor": "\nComments: 12 pages, 11 figures\n",
    "authors": [
      "Philippe Talatchian",
      "Matthew W. Daniels",
      "Advait Madhavan",
      "Matthew R. Pufall",
      "Emilie Ju\u00e9",
      "William H. Rippard",
      "Jabez J. McClelland",
      "Mark D. Stiles"
    ],
    "subjectives": [
      "Mesoscale and Nanoscale Physics (cond-mat.mes-hall)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Emerging Technologies (cs.ET)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03604"
  },
  {
    "id": "arXiv:2106.03671",
    "title": "Unsupervised Clustered Federated Learning in Complex Multi-source  Acoustic Environments",
    "abstract": "In this paper we introduce a realistic and challenging, multi-source and\nmulti-room acoustic environment and an improved algorithm for the estimation of\nsource-dominated microphone clusters in acoustic sensor networks. Our proposed\nclustering method is based on a single microphone per node and on unsupervised\nclustered federated learning which employs a light-weight autoencoder model. We\npresent an improved clustering control strategy that takes into account the\nvariability of the acoustic scene and allows the estimation of a dynamic range\nof clusters using reduced amounts of training data. The proposed approach is\noptimized using clustering-based measures and validated via a network-wide\nclassification task.",
    "descriptor": "\nComments: Accepted at EUSIPCO2021\n",
    "authors": [
      "Alexandru Nelus",
      "Rene Glitza",
      "Rainer Martin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03671"
  },
  {
    "id": "arXiv:2106.03676",
    "title": "Learning a performance metric of Buchberger's algorithm",
    "abstract": "What can be (machine) learned about the complexity of Buchberger's algorithm?\nGiven a system of polynomials, Buchberger's algorithm computes a Gr\\\"obner\nbasis of the ideal these polynomials generate using an iterative procedure\nbased on multivariate long division. The runtime of each step of the algorithm\nis typically dominated by a series of polynomial additions, and the total\nnumber of these additions is a hardware independent performance metric that is\noften used to evaluate and optimize various implementation choices. In this\nwork we attempt to predict, using just the starting input, the number of\npolynomial additions that take place during one run of Buchberger's algorithm.\nGood predictions are useful for quickly estimating difficulty and understanding\nwhat features make Gr\\\"obner basis computation hard. Our features and methods\ncould also be used for value models in the reinforcement learning approach to\noptimize Buchberger's algorithm introduced in [Peifer, Stillman, and\nHalpern-Leistner, 2020].\nWe show that a multiple linear regression model built from a set of\neasy-to-compute ideal generator statistics can predict the number of polynomial\nadditions somewhat well, better than an uninformed model, and better than\nregression models built on some intuitive commutative algebra invariants that\nare more difficult to compute. We also train a simple recursive neural network\nthat outperforms these linear models. Our work serves as a proof of concept,\ndemonstrating that predicting the number of polynomial additions in\nBuchberger's algorithm is a feasible problem from the point of view of machine\nlearning.",
    "descriptor": "",
    "authors": [
      "Jelena Mojsilovi\u0107",
      "Dylan Peifer",
      "Sonja Petrovi\u0107"
    ],
    "subjectives": [
      "Commutative Algebra (math.AC)",
      "Machine Learning (cs.LG)",
      "Symbolic Computation (cs.SC)",
      "Algebraic Geometry (math.AG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03676"
  },
  {
    "id": "arXiv:2106.03686",
    "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing",
    "abstract": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-output (MIMO)\nwireless radar. Here, the strong clutter due to the reflection of the layered\nstructure's surface often makes the detection of the defects challenging. Thus,\nsophisticated signal separation methods are required for improved defect\ndetection. In many scenarios, the number of defects that we are interested in\nis limited and the signaling response of the layered structure can be modeled\nas a low-rank structure. Therefore, we propose joint rank and sparsity\nminimization for defect detection. In particular, we propose a non-convex\napproach based on the iteratively reweighted nuclear and $\\ell_1-$norm (a\ndouble-reweighted approach) to obtain a higher accuracy compared to the\nconventional nuclear norm and $\\ell_1-$norm minimization. To this end, an\niterative algorithm is designed to estimate the low-rank and sparse\ncontributions. Further, we propose deep learning to learn the parameters of the\nalgorithm (i.e., algorithm unfolding) to improve the accuracy and the speed of\nconvergence of the algorithm. Our numerical results show that the proposed\napproach outperforms the conventional approaches in terms of mean square errors\nof the recovered low-rank and sparse components and the speed of convergence.",
    "descriptor": "",
    "authors": [
      "Udaya S.K.P. Miriya Thanthrige",
      "Peter Jung",
      "Aydin Sezgin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03686"
  },
  {
    "id": "arXiv:2106.03688",
    "title": "A Computational Model of Representation Learning in the Brain Cortex,  Integrating Unsupervised and Reinforcement Learning",
    "abstract": "A common view on the brain learning processes proposes that the three classic\nlearning paradigms -- unsupervised, reinforcement, and supervised -- take place\nin respectively the cortex, the basal-ganglia, and the cerebellum. However,\ndopamine outbursts, usually assumed to encode reward, are not limited to the\nbasal ganglia but also reach prefrontal, motor, and higher sensory cortices. We\npropose that in the cortex the same reward-based trial-and-error processes\nmight support not only the acquisition of motor representations but also of\nsensory representations. In particular, reward signals might guide\ntrial-and-error processes that mix with associative learning processes to\nsupport the acquisition of representations better serving downstream action\nselection. We tested the soundness of this hypothesis with a computational\nmodel that integrates unsupervised learning (Contrastive Divergence) and\nreinforcement learning (REINFORCE). The model was tested with a task requiring\ndifferent responses to different visual images grouped in categories involving\neither colour, shape, or size. Results show that a balanced mix of unsupervised\nand reinforcement learning processes leads to the best performance. Indeed,\nexcessive unsupervised learning tends to under-represent task-relevant features\nwhile excessive reinforcement learning tends to initially learn slowly and then\nto incur in local minima. These results stimulate future empirical studies on\ncategory learning directed to investigate similar effects in the extrastriate\nvisual cortices. Moreover, they prompt further computational investigations\ndirected to study the possible advantages of integrating unsupervised and\nreinforcement learning processes.",
    "descriptor": "",
    "authors": [
      "Giovanni Granato",
      "Emilio Cartoni",
      "Federico Da Rold",
      "Andrea Mattera",
      "Gianluca Baldassarre"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03688"
  },
  {
    "id": "arXiv:2106.03695",
    "title": "Neurons on Amoebae",
    "abstract": "We apply methods of machine-learning, such as neural networks, manifold\nlearning and image processing, in order to study amoebae in algebraic geometry\nand string theory. With the help of embedding manifold projection, we recover\ncomplicated conditions obtained from so-called lopsidedness. For certain cases\n(e.g. lopsided amoeba with positive coefficients for $F_0$), it could even\nreach $\\sim99\\%$ accuracy. Using weights and biases, we also find good\napproximations to determine the genus for an amoeba at lower computational\ncost. In general, the models could easily predict the genus with over $90\\%$\naccuracies. With similar techniques, we also investigate the membership\nproblem.",
    "descriptor": "\nComments: 50 pages\n",
    "authors": [
      "Jiakang Bao",
      "Yang-Hui He",
      "Edward Hirst"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2106.03695"
  },
  {
    "id": "arXiv:2106.03696",
    "title": "Dynamics of Stochastic Momentum Methods on Large-scale, Quadratic Models",
    "abstract": "We analyze a class of stochastic gradient algorithms with momentum on a\nhigh-dimensional random least squares problem. Our framework, inspired by\nrandom matrix theory, provides an exact (deterministic) characterization for\nthe sequence of loss values produced by these algorithms which is expressed\nonly in terms of the eigenvalues of the Hessian. This leads to simple\nexpressions for nearly-optimal hyperparameters, a description of the limiting\nneighborhood, and average-case complexity.\nAs a consequence, we show that (small-batch) stochastic heavy-ball momentum\nwith a fixed momentum parameter provides no actual performance improvement over\nSGD when step sizes are adjusted correctly. For contrast, in the non-strongly\nconvex setting, it is possible to get a large improvement over SGD using\nmomentum. By introducing hyperparameters that depend on the number of samples,\nwe propose a new algorithm sDANA (stochastic dimension adjusted Nesterov\nacceleration) which obtains an asymptotically optimal average-case complexity\nwhile remaining linearly convergent in the strongly convex setting without\nadjusting parameters.",
    "descriptor": "\nComments: 39 pages, 7 figures\n",
    "authors": [
      "Courtney Paquette",
      "Elliot Paquette"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03696"
  },
  {
    "id": "arXiv:2106.03701",
    "title": "Synthesis of standard 12-lead electrocardiograms using two dimensional  generative adversarial network",
    "abstract": "This paper proposes a two-dimensional (2D) bidirectional long short-term\nmemory generative adversarial network (GAN) to produce synthetic standard\n12-lead ECGs corresponding to four types of signals: left ventricular\nhypertrophy (LVH), left branch bundle block (LBBB), acute myocardial infarction\n(ACUTMI), and Normal. It uses a fully automatic end-to-end process to generate\nand verify the synthetic ECGs that does not require any visual inspection. The\nproposed model is able to produce synthetic standard 12-lead ECG signals with\nsuccess rates of 98% for LVH, 93% for LBBB, 79% for ACUTMI, and 59% for Normal.\nStatistical evaluation of the data confirms that the synthetic ECGs are not\nbiased towards or overfitted to the training ECGs, and span a wide range of\nmorphological features. This study demonstrates that it is feasible to use a 2D\nGAN to produce standard 12-lead ECGs suitable to augment artificially a diverse\ndatabase of real ECGs, thus providing a possible solution to the demand for\nextensive ECG datasets.",
    "descriptor": "",
    "authors": [
      "Yu-He Zhang",
      "Saeed Babaeizadeh"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03701"
  },
  {
    "id": "arXiv:2106.03702",
    "title": "Can a single neuron learn quantiles?",
    "abstract": "A novel non-parametric quantile estimation method for continuous random\nvariables is introduced, based on a minimal neural network architecture\nconsisting of a single unit. Its advantage over estimations from ranking the\norder statistics is shown, specifically for small sample size. In a regression\ncontext, the method can be used to quantify predictive uncertainty under the\nsplit conformal prediction setting, where prediction intervals are estimated\nfrom the residuals of a pre-trained model on a held-out validation set to\nquantify the uncertainty in future predictions. Benchmarking experiments\ndemonstrate that the method is competitive in quality and coverage with\nstate-of-the-art solutions, with the added benefit of being more\ncomputationally efficient.",
    "descriptor": "\nComments: 20 pages (10 of contents + 10 of supplementary material)\n",
    "authors": [
      "Edgardo Solano-Carrillo"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03702"
  },
  {
    "id": "arXiv:2106.03727",
    "title": "Deep Neural Network-based Enhancement for Image and Video Streaming  Systems: A Survey and Future Directions",
    "abstract": "Internet-enabled smartphones and ultra-wide displays are transforming a\nvariety of visual apps spanning from on-demand movies and 360{\\deg} videos to\nvideo-conferencing and live streaming. However, robustly delivering visual\ncontent under fluctuating networking conditions on devices of diverse\ncapabilities remains an open problem. In recent years, advances in the field of\ndeep learning on tasks such as super-resolution and image enhancement have led\nto unprecedented performance in generating high-quality images from low-quality\nones, a process we refer to as neural enhancement. In this paper, we survey\nstate-of-the-art content delivery systems that employ neural enhancement as a\nkey component in achieving both fast response time and high visual quality. We\nfirst present the components and architecture of existing content delivery\nsystems, highlighting their challenges and motivating the use of neural\nenhancement models as a countermeasure. We then cover the deployment challenges\nof these models and analyze existing systems and their design decisions in\nefficiently overcoming these technical challenges. Additionally, we underline\nthe key trends and common approaches across systems that target diverse\nuse-cases. Finally, we present promising future directions based on the latest\ninsights from deep learning research to further boost the quality of experience\nof content delivery systems.",
    "descriptor": "\nComments: Accepted for publication at the ACM Computing Surveys (CSUR) journal, 2021. arXiv admin note: text overlap with arXiv:2010.05838\n",
    "authors": [
      "Royson Lee",
      "Stylianos I. Venieris",
      "Nicholas D. Lane"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03727"
  },
  {
    "id": "arXiv:2106.03736",
    "title": "The effect of phased recurrent units in the classification of multiple  catalogs of astronomical lightcurves",
    "abstract": "In the new era of very large telescopes, where data is crucial to expand\nscientific knowledge, we have witnessed many deep learning applications for the\nautomatic classification of lightcurves. Recurrent neural networks (RNNs) are\none of the models used for these applications, and the LSTM unit stands out for\nbeing an excellent choice for the representation of long time series. In\ngeneral, RNNs assume observations at discrete times, which may not suit the\nirregular sampling of lightcurves. A traditional technique to address irregular\nsequences consists of adding the sampling time to the network's input, but this\nis not guaranteed to capture sampling irregularities during training.\nAlternatively, the Phased LSTM unit has been created to address this problem by\nupdating its state using the sampling times explicitly. In this work, we study\nthe effectiveness of the LSTM and Phased LSTM based architectures for the\nclassification of astronomical lightcurves. We use seven catalogs containing\nperiodic and nonperiodic astronomical objects. Our findings show that LSTM\noutperformed PLSTM on 6/7 datasets. However, the combination of both units\nenhances the results in all datasets.",
    "descriptor": "",
    "authors": [
      "C. Donoso-Oliva",
      "G. Cabrera-Vives",
      "P. Protopapas",
      "R. Carrasco-Davis",
      "P.A. Estevez"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03736"
  },
  {
    "id": "arXiv:2106.03741",
    "title": "Training Strategies for Deep Learning Gravitational-Wave Searches",
    "abstract": "Compact binary systems emit gravitational radiation which is potentially\ndetectable by current Earth bound detectors. Extracting these signals from the\ninstruments' background noise is a complex problem and the computational cost\nof most current searches depends on the complexity of the source model. Deep\nlearning may be capable of finding signals where current algorithms hit\ncomputational limits. Here we restrict our analysis to signals from\nnon-spinning binary black holes and systematically test different strategies by\nwhich training data is presented to the networks. To assess the impact of the\ntraining strategies, we re-analyze the first published networks and directly\ncompare them to an equivalent matched-filter search. We find that the deep\nlearning algorithms can generalize low signal-to-noise ratio (SNR) signals to\nhigh SNR ones but not vice versa. As such, it is not beneficial to provide high\nSNR signals during training, and fastest convergence is achieved when low SNR\nsamples are provided early on. During testing we found that the networks are\nsometimes unable to recover any signals when a false alarm probability\n$<10^{-3}$ is required. We resolve this restriction by applying a modification\nwe call unbounded Softmax replacement (USR) after training. With this\nalteration we find that the machine learning search retains $\\geq 97.5\\%$ of\nthe sensitivity of the matched-filter search down to a false-alarm rate of 1\nper month.",
    "descriptor": "\nComments: 17 pages, 11 figures, 3 tables, supplemental materials at this https URL\n",
    "authors": [
      "Marlin B. Sch\u00e4fer",
      "Ond\u0159ej Zelenka",
      "Alexander H. Nitz",
      "Frank Ohme",
      "Bernd Br\u00fcgmann"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "General Relativity and Quantum Cosmology (gr-qc)"
    ],
    "url": "https://arxiv.org/abs/2106.03741"
  },
  {
    "id": "arXiv:2106.03762",
    "title": "Improved Predictive Uncertainty using Corruption-based Calibration",
    "abstract": "We propose a simple post hoc calibration method to estimate the\nconfidence/uncertainty that a model prediction is correct on data with\ncovariate shift, as represented by the large-scale corrupted data benchmark\n[Ovadia et al, 2019]. We achieve this by synthesizing surrogate calibration\nsets by corrupting the calibration set with varying intensities of a known\ncorruption. Our method demonstrates significant improvements on the benchmark\non a wide range of covariate shifts.",
    "descriptor": "\nComments: 21 pages, 6 Tables, 17 Figures\n",
    "authors": [
      "Tiago Salvador",
      "Vikram Voleti",
      "Alexander Iannantuono",
      "Adam Oberman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03762"
  },
  {
    "id": "arXiv:2106.03765",
    "title": "On Inductive Biases for Heterogeneous Treatment Effect Estimation",
    "abstract": "We investigate how to exploit structural similarities of an individual's\npotential outcomes (POs) under different treatments to obtain better estimates\nof conditional average treatment effects in finite samples. Especially when it\nis unknown whether a treatment has an effect at all, it is natural to\nhypothesize that the POs are similar - yet, some existing strategies for\ntreatment effect estimation employ regularization schemes that implicitly\nencourage heterogeneity even when it does not exist and fail to fully make use\nof shared structure. In this paper, we investigate and compare three end-to-end\nlearning strategies to overcome this problem - based on regularization,\nreparametrization and a flexible multi-task architecture - each encoding\ninductive bias favoring shared behavior across POs. To build understanding of\ntheir relative strengths, we implement all strategies using neural networks and\nconduct a wide range of semi-synthetic experiments. We observe that all three\napproaches can lead to substantial improvements upon numerous baselines and\ngain insight into performance differences across various experimental settings.",
    "descriptor": "",
    "authors": [
      "Alicia Curth",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03765"
  },
  {
    "id": "arXiv:2106.03771",
    "title": "Traction chain networks: Insights beyond force chain networks for  non-spherical particle systems",
    "abstract": "Force chain networks are generally applied in granular materials to gain\ninsight into inter-particle granular contact. For conservative spherical\nparticle systems, i.e. frictionless and undamped, force chains are information\ncomplete due to symmetries resulting from isotropy and constant curvature of a\nsphere. In fact, for conservative spherical particle systems, given the\ngeometry and material, the force chain network uniquely defines the contact\nstate that includes elastic forces, penetration distance, overlap volume,\ncontact areas and contact pressures in a particle system. This is, however, not\nthe case for conservative non-spherical particle systems. The reason is that a\nforce chain network is not sufficient to uniquely define the contact state in a\nconservative non-spherical particle system. Additional information is required\nto define the contact state of non-spherical granular systems. Traction chain\nnetworks are proposed to complement force chain networks for the improved\nquantification of the state of contact of a granular system.",
    "descriptor": "\nComments: 19 pages; 17 figures\n",
    "authors": [
      "Daniel N. Wilke"
    ],
    "subjectives": [
      "Soft Condensed Matter (cond-mat.soft)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.03771"
  },
  {
    "id": "arXiv:2106.03791",
    "title": "Learning Gaussian Mixtures with Generalised Linear Models: Precise  Asymptotics in High-dimensions",
    "abstract": "Generalised linear models for multi-class classification problems are one of\nthe fundamental building blocks of modern machine learning tasks. In this\nmanuscript, we characterise the learning of a mixture of $K$ Gaussians with\ngeneric means and covariances via empirical risk minimisation (ERM) with any\nconvex loss and regularisation. In particular, we prove exact asymptotics\ncharacterising the ERM estimator in high-dimensions, extending several previous\nresults about Gaussian mixture classification in the literature. We exemplify\nour result in two tasks of interest in statistical learning: a) classification\nfor a mixture with sparse means, where we study the efficiency of $\\ell_1$\npenalty with respect to $\\ell_2$; b) max-margin multi-class classification,\nwhere we characterise the phase transition on the existence of the multi-class\nlogistic maximum likelihood estimator for $K>2$. Finally, we discuss how our\ntheory can be applied beyond the scope of synthetic data, showing that in\ndifferent cases Gaussian mixtures capture closely the learning curve of\nclassification tasks in real data sets.",
    "descriptor": "\nComments: 12 pages + 34 pages of Appendix, 10 figures\n",
    "authors": [
      "Bruno Loureiro",
      "Gabriele Sicuro",
      "C\u00e9dric Gerbelot",
      "Alessandro Pacco",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03791"
  },
  {
    "id": "arXiv:2106.03793",
    "title": "Pointwise visual field estimation from optical coherence tomography in  glaucoma: a structure-function analysis using deep learning",
    "abstract": "Background/Aims: Standard Automated Perimetry (SAP) is the gold standard to\nmonitor visual field (VF) loss in glaucoma management, but is prone to\nintra-subject variability. We developed and validated a deep learning (DL)\nregression model that estimates pointwise and overall VF loss from unsegmented\noptical coherence tomography (OCT) scans. Methods: Eight DL regression models\nwere trained with various retinal imaging modalities: circumpapillary OCT at\n3.5mm, 4.1mm, 4.7mm diameter, and scanning laser ophthalmoscopy (SLO) en face\nimages to estimate mean deviation (MD) and 52 threshold values. This\nretrospective study used data from patients who underwent a complete glaucoma\nexamination, including a reliable Humphrey Field Analyzer (HFA) 24-2 SITA\nStandard VF exam and a SPECTRALIS OCT scan using the Glaucoma Module Premium\nEdition. Results: A total of 1378 matched OCT-VF pairs of 496 patients (863\neyes) were included for training and evaluation of the DL models. Average\nsample MD was -7.53dB (from -33.8dB to +2.0dB). For 52 VF threshold values\nestimation, the circumpapillary OCT scan with the largest radius (4.7mm)\nachieved the best performance among all individual models (Pearson r=0.77, 95%\nCI=[0.72-0.82]). For MD, prediction averaging of OCT-trained models (3.5mm,\n4.1mm, 4.7mm) resulted in a Pearson r of 0.78 [0.73-0.83] on the validation set\nand comparable performance on the test set (Pearson r=0.79 [0.75-0.82]).\nConclusion: DL on unsegmented OCT scans accurately predicts pointwise and mean\ndeviation of 24-2 VF in glaucoma patients. Automated VF from unsegmented OCT\ncould be a solution for patients unable to produce reliable perimetry results.",
    "descriptor": "",
    "authors": [
      "Ruben Hemelings",
      "Bart Elen",
      "Jo\u00e3o Barbosa Breda",
      "Erwin Bellon",
      "Matthew B Blaschko",
      "Patrick De Boever",
      "Ingeborg Stalmans"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03793"
  },
  {
    "id": "arXiv:2106.03795",
    "title": "Heavy Tails in SGD and Compressibility of Overparametrized Neural  Networks",
    "abstract": "Neural network compression techniques have become increasingly popular as\nthey can drastically reduce the storage and computation requirements for very\nlarge networks. Recent empirical studies have illustrated that even simple\npruning strategies can be surprisingly effective, and several theoretical\nstudies have shown that compressible networks (in specific senses) should\nachieve a low generalization error. Yet, a theoretical characterization of the\nunderlying cause that makes the networks amenable to such simple compression\nschemes is still missing. In this study, we address this fundamental question\nand reveal that the dynamics of the training algorithm has a key role in\nobtaining such compressible networks. Focusing our attention on stochastic\ngradient descent (SGD), our main contribution is to link compressibility to two\nrecently established properties of SGD: (i) as the network size goes to\ninfinity, the system can converge to a mean-field limit, where the network\nweights behave independently, (ii) for a large step-size/batch-size ratio, the\nSGD iterates can converge to a heavy-tailed stationary distribution. In the\ncase where these two phenomena occur simultaneously, we prove that the networks\nare guaranteed to be '$\\ell_p$-compressible', and the compression errors of\ndifferent pruning techniques (magnitude, singular value, or node pruning)\nbecome arbitrarily small as the network size increases. We further prove\ngeneralization bounds adapted to our theoretical framework, which indeed\nconfirm that the generalization error will be lower for more compressible\nnetworks. Our theory and numerical study on various neural networks show that\nlarge step-size/batch-size ratios introduce heavy-tails, which, in combination\nwith overparametrization, result in compressibility.",
    "descriptor": "",
    "authors": [
      "Melih Barsbey",
      "Milad Sefidgaran",
      "Murat A. Erdogdu",
      "Ga\u00ebl Richard",
      "Umut \u015eim\u015fekli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03795"
  },
  {
    "id": "arXiv:2106.03815",
    "title": "QKD based on symmetric entangled Bernstein-Vazirani",
    "abstract": "This paper introduces a novel entanglement-based QKD protocol, that makes use\nof a modified symmetric version of the Bernstein-Vazirani algorithm, in order\nto achieve a secure and efficient key distribution. Two variants of the\nprotocol, one fully symmetric and one semi-symmetric, are presented. In both\ncases, the spatially separated Alice and Bob share multiple EPR pairs, one\nqubit of the pair each. The fully symmetric version allows both parties to\ninput a secret key from the irrespective location and, finally, acquire in the\nend a totally new and original key, an idea which was inspired by the\nDiffie-Hellman key exchange protocol. In the semi-symmetric version, Alice\nsends her chosen secret key to Bob (or vice versa). Furthermore, their\nperformance against an eavesdropper's attack is analyzed. Finally, in order to\nillustrate the operation of the protocols in practice, two small scale but\ndetailed examples are given.",
    "descriptor": "\nComments: 16 pages, 8 figures\n",
    "authors": [
      "Michael Ampatzis",
      "Theodore Andronikos"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.03815"
  },
  {
    "id": "arXiv:2106.03820",
    "title": "Accurate and robust Shapley Values for explaining predictions and  focusing on local important variables",
    "abstract": "Although Shapley Values (SV) are widely used in explainable AI, they can be\npoorly understood and estimated, which implies that their analysis may lead to\nspurious inferences and explanations. As a starting point, we remind an\ninvariance principle for SV and derive the correct approach for computing the\nSV of categorical variables that are particularly sensitive to the encoding\nused. In the case of tree-based models, we introduce two estimators of Shapley\nValues that exploit efficiently the tree structure and are more accurate than\nstate-of-the-art methods. For interpreting additive explanations, we recommend\nto filter the non-influential variables and to compute the Shapley Values only\nfor groups of influential variables. For this purpose, we use the concept of\n\"Same Decision Probability\" (SDP) that evaluates the robustness of a prediction\nwhen some variables are missing. This prior selection procedure produces sparse\nadditive explanations easier to visualize and analyse. Simulations and\ncomparisons are performed with state-of-the-art algorithm, and show the\npractical gain of our approach.",
    "descriptor": "\nComments: 9 pages, 4 figures, 2 tables. arXiv admin note: substantial text overlap with arXiv:2103.13342\n",
    "authors": [
      "Salim I. Amoukou",
      "Nicolas J-B. Brunel",
      "Tangi Sala\u00fcn"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03820"
  },
  {
    "id": "arXiv:2106.03823",
    "title": "Multivariate Probabilistic Regression with Natural Gradient Boosting",
    "abstract": "Many single-target regression problems require estimates of uncertainty along\nwith the point predictions. Probabilistic regression algorithms are well-suited\nfor these tasks. However, the options are much more limited when the prediction\ntarget is multivariate and a joint measure of uncertainty is required. For\nexample, in predicting a 2D velocity vector a joint uncertainty would quantify\nthe probability of any vector in the plane, which would be more expressive than\ntwo separate uncertainties on the x- and y- components. To enable joint\nprobabilistic regression, we propose a Natural Gradient Boosting (NGBoost)\napproach based on nonparametrically modeling the conditional parameters of the\nmultivariate predictive distribution. Our method is robust, works\nout-of-the-box without extensive tuning, is modular with respect to the assumed\ntarget distribution, and performs competitively in comparison to existing\napproaches. We demonstrate these claims in simulation and with a case study\npredicting two-dimensional oceanographic velocity data. An implementation of\nour method is available at https://github.com/stanfordmlgroup/ngboost.",
    "descriptor": "",
    "authors": [
      "Michael O'Malley",
      "Adam M. Sykulski",
      "Rick Lumpkin",
      "Alejandro Schuler"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.03823"
  },
  {
    "id": "arXiv:1505.05630",
    "title": "Very Sparse Additive Spanners and Emulators",
    "abstract": "Comments: ITCS 2015",
    "descriptor": "\nComments: ITCS 2015\n",
    "authors": [
      "Greg Bodwin",
      "Virginia Vassilevska Williams"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1505.05630"
  },
  {
    "id": "arXiv:1605.04344",
    "title": "On the Effects of Measurement Uncertainty in Optimal Control of Contact  Interactions",
    "abstract": "Comments: 17 pages, 5 figures - this version is the one published at WAFR 2016 to fulfill the open access requirements of the EU commission, please refer to the previous version for the complete derivation of the algorithm",
    "descriptor": "\nComments: 17 pages, 5 figures - this version is the one published at WAFR 2016 to fulfill the open access requirements of the EU commission, please refer to the previous version for the complete derivation of the algorithm\n",
    "authors": [
      "Brahayam Ponton",
      "Stefan Schaal",
      "Ludovic Righetti"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/1605.04344"
  },
  {
    "id": "arXiv:1701.01207",
    "title": "Learning Semidefinite Regularizers",
    "abstract": "Comments: 51 pages, 9 figures",
    "descriptor": "\nComments: 51 pages, 9 figures\n",
    "authors": [
      "Yong Sheng Soh",
      "Venkat Chandrasekaran"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1701.01207"
  },
  {
    "id": "arXiv:1708.04203",
    "title": "Optimal Algorithms for Separating a Polyhedron from Its Single-Part Mold",
    "abstract": "Comments: 13 pages. This version includes a proof that establishes the optimality of our algorithms, which extends our CASE2017 version. Submitted to Computer-Aided Design",
    "descriptor": "\nComments: 13 pages. This version includes a proof that establishes the optimality of our algorithms, which extends our CASE2017 version. Submitted to Computer-Aided Design\n",
    "authors": [
      "Prosenjit Bose",
      "Tzvika Geft",
      "Dan Halperin",
      "Shahar Shamai"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/1708.04203"
  },
  {
    "id": "arXiv:1802.04064",
    "title": "A Contextual Bandit Bake-off",
    "abstract": "Comments: JMLR",
    "descriptor": "\nComments: JMLR\n",
    "authors": [
      "Alberto Bietti",
      "Alekh Agarwal",
      "John Langford"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1802.04064"
  },
  {
    "id": "arXiv:1811.11849",
    "title": "Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on  Crowd Videos",
    "abstract": "Comments: Under review at Patter Recognition",
    "descriptor": "\nComments: Under review at Patter Recognition\n",
    "authors": [
      "Kha Gia Quach",
      "Ngan Le",
      "Chi Nhan Duong",
      "Ibsa Jalata",
      "Kaushik Roy",
      "Khoa Luu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1811.11849"
  },
  {
    "id": "arXiv:1812.00426",
    "title": "GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View  Fundamental Matrices",
    "abstract": "GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View  Fundamental Matrices",
    "descriptor": "",
    "authors": [
      "Yoni Kasten",
      "Amnon Geifman",
      "Meirav Galun",
      "Ronen Basri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1812.00426"
  },
  {
    "id": "arXiv:1812.10249",
    "title": "Constructing Faithful Homomorphisms over Fields of Finite Characteristic",
    "abstract": "Constructing Faithful Homomorphisms over Fields of Finite Characteristic",
    "descriptor": "",
    "authors": [
      "Prerona Chatterjee",
      "Ramprasad Saptharishi"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/1812.10249"
  },
  {
    "id": "arXiv:1902.00009",
    "title": "Descriptor system techniques and software tools",
    "abstract": "Comments: 11 pages. A shorter version of this article appeared in the Encyclopedia of Systems and Control (2019). arXiv admin note: text overlap with arXiv:1707.07140",
    "descriptor": "\nComments: 11 pages. A shorter version of this article appeared in the Encyclopedia of Systems and Control (2019). arXiv admin note: text overlap with arXiv:1707.07140\n",
    "authors": [
      "Andreas Varga"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/1902.00009"
  },
  {
    "id": "arXiv:1902.07436",
    "title": "Perfect reconstruction of sparse signals with piecewise continuous  nonconvex penalties and nonconvexity control",
    "abstract": "Comments: 25 pages, 17 figures",
    "descriptor": "\nComments: 25 pages, 17 figures\n",
    "authors": [
      "Ayaka Sakata",
      "Tomoyuki Obuchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1902.07436"
  },
  {
    "id": "arXiv:1902.07670",
    "title": "Intelligent Surface-Aided Transmitter Architectures for Millimeter Wave  Ultra Massive MIMO Systems",
    "abstract": "Comments: Journal version of arXiv:1811.02948",
    "descriptor": "\nComments: Journal version of arXiv:1811.02948\n",
    "authors": [
      "Vahid Jamali",
      "Antonia M. Tulino",
      "Georg Fischer",
      "Ralf M\u00fcller",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1902.07670"
  },
  {
    "id": "arXiv:1904.07903",
    "title": "Fully computable a posteriori error bounds for eigenfunctions",
    "abstract": "Comments: 27 pages, 8 tables, 9 figures",
    "descriptor": "\nComments: 27 pages, 8 tables, 9 figures\n",
    "authors": [
      "Xuefeng Liu",
      "Tom\u00e1\u0161 Vejchodsk\u00fd"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1904.07903"
  },
  {
    "id": "arXiv:1905.00993",
    "title": "Game Semantics of Martin-L\u00f6f Type Theory",
    "abstract": "Game Semantics of Martin-L\u00f6f Type Theory",
    "descriptor": "",
    "authors": [
      "Norihiro Yamada"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/1905.00993"
  },
  {
    "id": "arXiv:1905.11824",
    "title": "Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov  Models",
    "abstract": "Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov  Models",
    "descriptor": "",
    "authors": [
      "Soham Deshmukh",
      "Rahul Rade",
      "Dr. Faruk Kazi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1905.11824"
  },
  {
    "id": "arXiv:1907.06022",
    "title": "Multiscale Principle of Relevant Information for Hyperspectral Image  Classification",
    "abstract": "Comments: Mansucript to be published in Machine Learning Journal (Springer). Code available at this https URL",
    "descriptor": "\nComments: Mansucript to be published in Machine Learning Journal (Springer). Code available at this https URL\n",
    "authors": [
      "Yantao Wei",
      "Shujian Yu",
      "Luis Sanchez Giraldo",
      "Jose C. Principe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1907.06022"
  },
  {
    "id": "arXiv:1908.03202",
    "title": "Received Signal Strength Based Wireless Source Localization with  Inaccurate Anchor Position",
    "abstract": "Comments: 12 pages, 8 figures",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Yang Liu",
      "Guojun Han",
      "Yonghua Wang",
      "Zheng Xue",
      "Jing Chen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/1908.03202"
  },
  {
    "id": "arXiv:1910.02610",
    "title": "Multi-hop Question Answering via Reasoning Chains",
    "abstract": "Multi-hop Question Answering via Reasoning Chains",
    "descriptor": "",
    "authors": [
      "Jifan Chen",
      "Shih-ting Lin",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1910.02610"
  },
  {
    "id": "arXiv:1910.11829",
    "title": "String Matching with Wildcards in the Massively Parallel Computation  Model",
    "abstract": "String Matching with Wildcards in the Massively Parallel Computation  Model",
    "descriptor": "",
    "authors": [
      "MohammadTaghi Hajiaghayi",
      "Hamed Saleh",
      "Saeed Seddighin",
      "Xiaorui Sun"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/1910.11829"
  },
  {
    "id": "arXiv:1910.13671",
    "title": "Facial Image Deformation Based on Landmark Detection",
    "abstract": "Facial Image Deformation Based on Landmark Detection",
    "descriptor": "",
    "authors": [
      "Chaoyue Song",
      "Yugang Chen",
      "Shulai Zhang",
      "Bingbing Ni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/1910.13671"
  },
  {
    "id": "arXiv:1911.07255",
    "title": "Spectral Geometric Matrix Completion",
    "abstract": "Comments: Accepted to Mathematical and Scientific Machine Learning (MSML) 2021 this https URL",
    "descriptor": "\nComments: Accepted to Mathematical and Scientific Machine Learning (MSML) 2021 this https URL\n",
    "authors": [
      "Amit Boyarski",
      "Sanketh Vedula",
      "Alex Bronstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1911.07255"
  },
  {
    "id": "arXiv:1912.01756",
    "title": "Conv-MPN: Convolutional Message Passing Neural Network for Structured  Outdoor Architecture Reconstruction",
    "abstract": "Comments: Accepted by CVPR2020",
    "descriptor": "\nComments: Accepted by CVPR2020\n",
    "authors": [
      "Fuyang Zhang",
      "Nelson Nauata",
      "Yasutaka Furukawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1912.01756"
  },
  {
    "id": "arXiv:2001.06471",
    "title": "Learning Sparse Classifiers: Continuous and Mixed Integer Optimization  Perspectives",
    "abstract": "Comments: To appear in JMLR",
    "descriptor": "\nComments: To appear in JMLR\n",
    "authors": [
      "Antoine Dedieu",
      "Hussein Hazimeh",
      "Rahul Mazumder"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2001.06471"
  },
  {
    "id": "arXiv:2001.10980",
    "title": "Multimodal Story Generation on Plural Images",
    "abstract": "Comments: This is an undergraduate project report. Completed Dec. 2019 at the Cooper Union",
    "descriptor": "\nComments: This is an undergraduate project report. Completed Dec. 2019 at the Cooper Union\n",
    "authors": [
      "Jing Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2001.10980"
  },
  {
    "id": "arXiv:2002.00558",
    "title": "The Price of Incentivizing Exploration: A Characterization via Thompson  Sampling and Sample Complexity",
    "abstract": "The Price of Incentivizing Exploration: A Characterization via Thompson  Sampling and Sample Complexity",
    "descriptor": "",
    "authors": [
      "Mark Sellke",
      "Aleksandrs Slivkins"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2002.00558"
  },
  {
    "id": "arXiv:2002.04238",
    "title": "HMRL: Hyper-Meta Learning for Sparse Reward Reinforcement Learning  Problem",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Yun Hua",
      "Xiangfeng Wang",
      "Bo Jin",
      "Wenhao Li",
      "Junchi Yan",
      "Xiaofeng He",
      "Hongyuan Zha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.04238"
  },
  {
    "id": "arXiv:2002.05123",
    "title": "Over-the-Air Adversarial Flickering Attacks against Video Recognition  Networks",
    "abstract": "Over-the-Air Adversarial Flickering Attacks against Video Recognition  Networks",
    "descriptor": "",
    "authors": [
      "Roi Pony",
      "Itay Naeh",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.05123"
  },
  {
    "id": "arXiv:2002.10045",
    "title": "Optimal Advertising for Information Products",
    "abstract": "Optimal Advertising for Information Products",
    "descriptor": "",
    "authors": [
      "Shuran Zheng",
      "Yiling Chen"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2002.10045"
  },
  {
    "id": "arXiv:2002.10085",
    "title": "Temporal Spike Sequence Learning via Backpropagation for Deep Spiking  Neural Networks",
    "abstract": "Comments: Accepted for spotlight presentation of NeurIPS (Neural Information Processing System) 2020: this https URL",
    "descriptor": "\nComments: Accepted for spotlight presentation of NeurIPS (Neural Information Processing System) 2020: this https URL\n",
    "authors": [
      "Wenrui Zhang",
      "Peng Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2002.10085"
  },
  {
    "id": "arXiv:2002.11169",
    "title": "Unsupervised Discovery, Control, and Disentanglement of Semantic  Attributes with Applications to Anomaly Detection",
    "abstract": "Comments: MIT Neural Computation 2021, Vol 33(3), pp. 802--826",
    "descriptor": "\nComments: MIT Neural Computation 2021, Vol 33(3), pp. 802--826\n",
    "authors": [
      "William Paul",
      "I-Jeng Wang",
      "Fady Alajaji",
      "Philippe Burlina"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2002.11169"
  },
  {
    "id": "arXiv:2003.00239",
    "title": "Improving THz Quality-of-Transmission with Systematic RLNC and Auxiliary  Channels",
    "abstract": "Comments: 7 pages, 6 figures, accepted at IEEE ICC'20 Workshop - TeraCom",
    "descriptor": "\nComments: 7 pages, 6 figures, accepted at IEEE ICC'20 Workshop - TeraCom\n",
    "authors": [
      "Cao Vien Phung",
      "Anna Engelmann",
      "Thomas Kuerner",
      "Admela Jukan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2003.00239"
  },
  {
    "id": "arXiv:2003.00314",
    "title": "A complexity chasm for solving univariate sparse polynomial equations  over $p$-adic fields",
    "abstract": "Comments: 19 pages, 3 figures. This version contains an Appendix missing from the ISSAC 2021 conference version, as well as some corrections and improvements",
    "descriptor": "\nComments: 19 pages, 3 figures. This version contains an Appendix missing from the ISSAC 2021 conference version, as well as some corrections and improvements\n",
    "authors": [
      "J. Maurice Rojas",
      "Yuyu Zhu"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Computational Complexity (cs.CC)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2003.00314"
  },
  {
    "id": "arXiv:2003.03674",
    "title": "Error Correction with Systematic RLNC in Multi-Channel THz Communication  Systems",
    "abstract": "Comments: 6 pages, 5 figures",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Cao Vien Phung",
      "Anna Engelmann",
      "Admela Jukan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2003.03674"
  },
  {
    "id": "arXiv:2003.05853",
    "title": "An autonomous swarm of micro flying robots with range-based relative  localization",
    "abstract": "Comments: Submitted to TRO. Project link: this https URL",
    "descriptor": "\nComments: Submitted to TRO. Project link: this https URL\n",
    "authors": [
      "Shushuai Li",
      "Mario Coppola",
      "Christophe De Wagter",
      "Guido C. H. E. de Croon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2003.05853"
  },
  {
    "id": "arXiv:2003.12725",
    "title": "A Graph to Graphs Framework for Retrosynthesis Prediction",
    "abstract": "Comments: ICML 2020",
    "descriptor": "\nComments: ICML 2020\n",
    "authors": [
      "Chence Shi",
      "Minkai Xu",
      "Hongyu Guo",
      "Ming Zhang",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.12725"
  },
  {
    "id": "arXiv:2004.01302",
    "title": "Distributed Inference with Sparse and Quantized Communication",
    "abstract": "Comments: Accepted for publication in the IEEE Transactions of Signal Processing as a regular paper",
    "descriptor": "\nComments: Accepted for publication in the IEEE Transactions of Signal Processing as a regular paper\n",
    "authors": [
      "Aritra Mitra",
      "John A. Richards",
      "Saurabh Bagchi",
      "Shreyas Sundaram"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computer Science and Game Theory (cs.GT)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2004.01302"
  },
  {
    "id": "arXiv:2004.02995",
    "title": "Multi-Step Inference for Reasoning Over Paragraphs",
    "abstract": "Comments: accepted by EMNLP 2020",
    "descriptor": "\nComments: accepted by EMNLP 2020\n",
    "authors": [
      "Jiangming Liu",
      "Matt Gardner",
      "Shay B. Cohen",
      "Mirella Lapata"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2004.02995"
  },
  {
    "id": "arXiv:2004.07162",
    "title": "On Linear Optimization over Wasserstein Balls",
    "abstract": "On Linear Optimization over Wasserstein Balls",
    "descriptor": "",
    "authors": [
      "Man-Chung Yue",
      "Daniel Kuhn",
      "Wolfram Wiesemann"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2004.07162"
  },
  {
    "id": "arXiv:2004.08107",
    "title": "Cascaded Context Enhancement Network for Automatic Skin Lesion  Segmentation",
    "abstract": "Cascaded Context Enhancement Network for Automatic Skin Lesion  Segmentation",
    "descriptor": "",
    "authors": [
      "Ruxin Wang",
      "Shuyuan Chen",
      "Chaojie Ji",
      "Ye Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2004.08107"
  },
  {
    "id": "arXiv:2004.09764",
    "title": "Discrete Auto-regressive Variational Attention Models for Text Modeling",
    "abstract": "Comments: IJCNN 2021",
    "descriptor": "\nComments: IJCNN 2021\n",
    "authors": [
      "Xianghong Fang",
      "Haoli Bai",
      "Jian Li",
      "Zenglin Xu",
      "Michael Lyu",
      "Irwin King"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.09764"
  },
  {
    "id": "arXiv:2004.13954",
    "title": "Rethink the Connections among Generalization, Memorization and the  Spectral Bias of DNNs",
    "abstract": "Comments: IJCAI-21",
    "descriptor": "\nComments: IJCAI-21\n",
    "authors": [
      "Xiao Zhang",
      "Haoyi Xiong",
      "Dongrui Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.13954"
  },
  {
    "id": "arXiv:2005.00694",
    "title": "Optimal Beam Association for High Mobility mmWave Vehicular Networks:  Lightweight Parallel Reinforcement Learning Approach",
    "abstract": "Optimal Beam Association for High Mobility mmWave Vehicular Networks:  Lightweight Parallel Reinforcement Learning Approach",
    "descriptor": "",
    "authors": [
      "Nguyen Van Huynh",
      "Diep N. Nguyen",
      "Dinh Thai Hoang",
      "Eryk Dutkiewicz"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2005.00694"
  },
  {
    "id": "arXiv:2005.00777",
    "title": "Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor  Imagery Recognition",
    "abstract": "Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor  Imagery Recognition",
    "descriptor": "",
    "authors": [
      "Yimin Hou",
      "Shuyue Jia",
      "Xiangmin Lun",
      "Yan Shi",
      "Yang Li"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2005.00777"
  },
  {
    "id": "arXiv:2005.01529",
    "title": "Accelerated Learning with Robustness to Adversarial Regressors",
    "abstract": "Comments: L4DC 2021 Full Version",
    "descriptor": "\nComments: L4DC 2021 Full Version\n",
    "authors": [
      "Joseph E. Gaudio",
      "Anuradha M. Annaswamy",
      "Jos\u00e9 M. Moreu",
      "Michael A. Bolender",
      "Travis E. Gibson"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2005.01529"
  },
  {
    "id": "arXiv:2005.01757",
    "title": "Sample Complexity of Uniform Convergence for Multicalibration",
    "abstract": "Comments: NeurIPS 2020",
    "descriptor": "\nComments: NeurIPS 2020\n",
    "authors": [
      "Eliran Shabat",
      "Lee Cohen",
      "Yishay Mansour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.01757"
  },
  {
    "id": "arXiv:2005.03120",
    "title": "Electricity-Aware Bid Format for Heat Commitment and Dispatch",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1910.08617",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1910.08617\n",
    "authors": [
      "Lesia Mitridati",
      "Pascal Van Hentenryck",
      "Jalal Kazempour"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2005.03120"
  },
  {
    "id": "arXiv:2005.07871",
    "title": "Remote State Estimation with Smart Sensors over Markov Fading Channels",
    "abstract": "Comments: The paper has been accepted by IEEE Transactions on Automatic Control. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: The paper has been accepted by IEEE Transactions on Automatic Control. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Wanchun Liu",
      "Daniel E. Quevedo",
      "Yonghui Li",
      "Karl Henrik Johansson",
      "Branka Vucetic"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2005.07871"
  },
  {
    "id": "arXiv:2005.09814",
    "title": "Mirror Descent Policy Optimization",
    "abstract": "Mirror Descent Policy Optimization",
    "descriptor": "",
    "authors": [
      "Manan Tomar",
      "Lior Shani",
      "Yonathan Efroni",
      "Mohammad Ghavamzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.09814"
  },
  {
    "id": "arXiv:2005.10224",
    "title": "The Random Feature Model for Input-Output Maps between Banach Spaces",
    "abstract": "Comments: To appear in SIAM Journal on Scientific Computing; 32 pages, 9 figures",
    "descriptor": "\nComments: To appear in SIAM Journal on Scientific Computing; 32 pages, 9 figures\n",
    "authors": [
      "Nicholas H. Nelsen",
      "Andrew M. Stuart"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.10224"
  },
  {
    "id": "arXiv:2005.10435",
    "title": "Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators  with Massive Data",
    "abstract": "Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators  with Massive Data",
    "descriptor": "",
    "authors": [
      "Jun Yu",
      "HaiYing Wang",
      "Mingyao Ai",
      "Huiming Zhang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.10435"
  },
  {
    "id": "arXiv:2005.11516",
    "title": "Frontal Attack: Leaking Control-Flow in SGX via the CPU Frontend",
    "abstract": "Comments: Accepted for publication at the 30th USENIX Security Symposium (USENIX Security 21) Change w.r.t. v3: Corrected list of exploited libraries in the conclusions",
    "descriptor": "\nComments: Accepted for publication at the 30th USENIX Security Symposium (USENIX Security 21) Change w.r.t. v3: Corrected list of exploited libraries in the conclusions\n",
    "authors": [
      "Ivan Puddu",
      "Moritz Schneider",
      "Miro Haller",
      "Srdjan \u010capkun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2005.11516"
  },
  {
    "id": "arXiv:2005.13273",
    "title": "Selective Inference for Latent Block Models",
    "abstract": "Selective Inference for Latent Block Models",
    "descriptor": "",
    "authors": [
      "Chihiro Watanabe",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2005.13273"
  },
  {
    "id": "arXiv:2006.02181",
    "title": "Information Consumption and Social Response in a Segregated Environment:  the Case of Gab",
    "abstract": "Comments: Major flaws in the analysis have been found",
    "descriptor": "\nComments: Major flaws in the analysis have been found\n",
    "authors": [
      "Gabriele Etta",
      "Alessandro Galeazzi",
      "Matteo Cinelli",
      "Mauro Conti",
      "Walter Quattrociocchi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2006.02181"
  },
  {
    "id": "arXiv:2006.03631",
    "title": "UFO-BLO: Unbiased First-Order Bilevel Optimization",
    "abstract": "UFO-BLO: Unbiased First-Order Bilevel Optimization",
    "descriptor": "",
    "authors": [
      "Valerii Likhosherstov",
      "Xingyou Song",
      "Krzysztof Choromanski",
      "Jared Davis",
      "Adrian Weller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.03631"
  },
  {
    "id": "arXiv:2006.04012",
    "title": "An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic  Gradient Descent and Thompson Sampling",
    "abstract": "An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic  Gradient Descent and Thompson Sampling",
    "descriptor": "",
    "authors": [
      "Qin Ding",
      "Cho-Jui Hsieh",
      "James Sharpnack"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.04012"
  },
  {
    "id": "arXiv:2006.04373",
    "title": "MC2G: An Efficient Algorithm for Matrix Completion with Social and Item  Similarity Graphs",
    "abstract": "MC2G: An Efficient Algorithm for Matrix Completion with Social and Item  Similarity Graphs",
    "descriptor": "",
    "authors": [
      "Qiaosheng Zhang",
      "Geewon Suh",
      "Changho Suh",
      "Vincent Y. F. Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2006.04373"
  },
  {
    "id": "arXiv:2006.05051",
    "title": "Constrained episodic reinforcement learning in concave-convex and  knapsack settings",
    "abstract": "Comments: The NeurIPS 2020 version of this paper includes a small bug, leading to an incorrect dependence on H in Theorem 3.4. This version fixes it by adjusting Eq. (9), Theorem 3.4 and the relevant proofs. Changes in the main text are noted in red. Changes in the appendix are limited to Appendices B.1, B.5, and B.6 and the statement of Lemma F.3",
    "descriptor": "\nComments: The NeurIPS 2020 version of this paper includes a small bug, leading to an incorrect dependence on H in Theorem 3.4. This version fixes it by adjusting Eq. (9), Theorem 3.4 and the relevant proofs. Changes in the main text are noted in red. Changes in the appendix are limited to Appendices B.1, B.5, and B.6 and the statement of Lemma F.3\n",
    "authors": [
      "Kiant\u00e9 Brantley",
      "Miroslav Dudik",
      "Thodoris Lykouris",
      "Sobhan Miryoosefi",
      "Max Simchowitz",
      "Aleksandrs Slivkins",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.05051"
  },
  {
    "id": "arXiv:2006.05734",
    "title": "3D Human Mesh Regression with Dense Correspondence",
    "abstract": "Comments: To appear at CVPR 2020",
    "descriptor": "\nComments: To appear at CVPR 2020\n",
    "authors": [
      "Wang Zeng",
      "Wanli Ouyang",
      "Ping Luo",
      "Wentao Liu",
      "Xiaogang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2006.05734"
  },
  {
    "id": "arXiv:2006.05973",
    "title": "Optimal Bounds between $f$-Divergences and Integral Probability Metrics",
    "abstract": "Optimal Bounds between $f$-Divergences and Integral Probability Metrics",
    "descriptor": "",
    "authors": [
      "Rohit Agrawal",
      "Thibaut Horel"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2006.05973"
  },
  {
    "id": "arXiv:2006.06466",
    "title": "How Interpretable and Trustworthy are GAMs?",
    "abstract": "Comments: Accepted in 2021 KDD",
    "descriptor": "\nComments: Accepted in 2021 KDD\n",
    "authors": [
      "Chun-Hao Chang",
      "Sarah Tan",
      "Ben Lengerich",
      "Anna Goldenberg",
      "Rich Caruana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.06466"
  },
  {
    "id": "arXiv:2006.06518",
    "title": "Towards Expedited Impedance Tuning of a Robotic Prosthesis for  Personalized Gait Assistance by Reinforcement Learning Control",
    "abstract": "Towards Expedited Impedance Tuning of a Robotic Prosthesis for  Personalized Gait Assistance by Reinforcement Learning Control",
    "descriptor": "",
    "authors": [
      "Minhan Li",
      "Yue Wen",
      "Xiang Gao",
      "Jennie Si",
      "He Helen Huang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2006.06518"
  },
  {
    "id": "arXiv:2006.08251",
    "title": "Weighting Adversarial Neural Network for Domain Adaptation in Regression",
    "abstract": "Comments: 18 pages, 3 figures",
    "descriptor": "\nComments: 18 pages, 3 figures\n",
    "authors": [
      "Antoine de Mathelin",
      "Guillaume Richard",
      "Francois Deheeger",
      "Mathilde Mougeot",
      "Nicolas Vayatis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.08251"
  },
  {
    "id": "arXiv:2006.08267",
    "title": "Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking  Fairness and Algorithm Utility",
    "abstract": "Comments: add the appendix",
    "descriptor": "\nComments: add the appendix\n",
    "authors": [
      "Sen Cui",
      "Weishen Pan",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.08267"
  },
  {
    "id": "arXiv:2006.08924",
    "title": "GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding  Time-resolved EEG Motor Imagery Signals",
    "abstract": "GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding  Time-resolved EEG Motor Imagery Signals",
    "descriptor": "",
    "authors": [
      "Xiangmin Lun",
      "Shuyue Jia",
      "Yimin Hou",
      "Yan Shi",
      "Yang Li"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2006.08924"
  },
  {
    "id": "arXiv:2006.08950",
    "title": "Federated Accelerated Stochastic Gradient Descent",
    "abstract": "Comments: Accepted to NeurIPS 2020. Best paper in International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2020 (FL-ICML'20). Code repository see this https URL",
    "descriptor": "\nComments: Accepted to NeurIPS 2020. Best paper in International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2020 (FL-ICML'20). Code repository see this https URL\n",
    "authors": [
      "Honglin Yuan",
      "Tengyu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.08950"
  },
  {
    "id": "arXiv:2006.09500",
    "title": "Logic of Machine Learning",
    "abstract": "Logic of Machine Learning",
    "descriptor": "",
    "authors": [
      "Marina Sapir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.09500"
  },
  {
    "id": "arXiv:2006.13019",
    "title": "Optimal Network Slicing for Service-Oriented Networks with Flexible  Routing and Guaranteed E2E Latency",
    "abstract": "Comments: 16 pages, 8 figures, accepted for publication in IEEE Transactions on Network and Service Management, code available here: this https URL arXiv admin note: text overlap with arXiv:2002.07380",
    "descriptor": "\nComments: 16 pages, 8 figures, accepted for publication in IEEE Transactions on Network and Service Management, code available here: this https URL arXiv admin note: text overlap with arXiv:2002.07380\n",
    "authors": [
      "Wei-Kun Chen",
      "Ya-Feng Liu",
      "Antonio De Domenico",
      "Zhi-Quan Luo",
      "Yu-Hong Dai"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2006.13019"
  },
  {
    "id": "arXiv:2006.13125",
    "title": "DeepQTMT: A Deep Learning Approach for Fast QTMT-based CU Partition of  Intra-mode VVC",
    "abstract": "Comments: 14 pages, 10 figures, 7 tables. Published in IEEE Transactions on Image Processing (TIP), 2021",
    "descriptor": "\nComments: 14 pages, 10 figures, 7 tables. Published in IEEE Transactions on Image Processing (TIP), 2021\n",
    "authors": [
      "Tianyi Li",
      "Mai Xu",
      "Runzhi Tang",
      "Ying Chen",
      "Qunliang Xing"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2006.13125"
  },
  {
    "id": "arXiv:2006.15646",
    "title": "Expressive Power of Invariant and Equivariant Graph Neural Networks",
    "abstract": "Comments: Appears in: Proceedings of the 9th International Conference on Learning Representations, ICLR 2021. 39 pages",
    "descriptor": "\nComments: Appears in: Proceedings of the 9th International Conference on Learning Representations, ICLR 2021. 39 pages\n",
    "authors": [
      "Wa\u00efss Azizian",
      "Marc Lelarge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.15646"
  },
  {
    "id": "arXiv:2006.16664",
    "title": "Constructive Universal High-Dimensional Distribution Generation through  Deep ReLU Networks",
    "abstract": "Constructive Universal High-Dimensional Distribution Generation through  Deep ReLU Networks",
    "descriptor": "",
    "authors": [
      "Dmytro Perekrestenko",
      "Stephan M\u00fcller",
      "Helmut B\u00f6lcskei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.16664"
  },
  {
    "id": "arXiv:2007.04865",
    "title": "A Deep Joint Sparse Non-negative Matrix Factorization Framework for  Identifying the Common and Subject-specific Functional Units of Tongue Motion  During Speech",
    "abstract": "Comments: Accepted by Medical Image Analysis",
    "descriptor": "\nComments: Accepted by Medical Image Analysis\n",
    "authors": [
      "Jonghye Woo",
      "Fangxu Xing",
      "Jerry L. Prince",
      "Maureen Stone",
      "Arnold Gomez",
      "Timothy G. Reese",
      "Van J. Wedeen",
      "Georges El Fakhri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2007.04865"
  },
  {
    "id": "arXiv:2007.05975",
    "title": "A Graph Symmetrisation Bound on Channel Information Leakage under  Blowfish Privacy",
    "abstract": "Comments: 14 pages, 4 figures",
    "descriptor": "\nComments: 14 pages, 4 figures\n",
    "authors": [
      "Tobias Edwards",
      "Benjamin I. P. Rubinstein",
      "Zuhe Zhang",
      "Sanming Zhou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.05975"
  },
  {
    "id": "arXiv:2007.07550",
    "title": "Group Invariant Dictionary Learning",
    "abstract": "Comments: 30 pages, 23 figures",
    "descriptor": "\nComments: 30 pages, 23 figures\n",
    "authors": [
      "Yong Sheng Soh"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2007.07550"
  },
  {
    "id": "arXiv:2007.08911",
    "title": "Technologies for Trustworthy Machine Learning: A Survey in a  Socio-Technical Context",
    "abstract": "Comments: We are updating some sections to include more recent advances",
    "descriptor": "\nComments: We are updating some sections to include more recent advances\n",
    "authors": [
      "Ehsan Toreini",
      "Mhairi Aitken",
      "Kovila P. L. Coopamootoo",
      "Karen Elliott",
      "Vladimiro Gonzalez Zelaya",
      "Paolo Missier",
      "Magdalene Ng",
      "Aad van Moorsel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.08911"
  },
  {
    "id": "arXiv:2007.08956",
    "title": "Vertex distinction with subgraph centrality: a proof of Estrada's  conjecture and some generalizations",
    "abstract": "Comments: 8 pages, no figures",
    "descriptor": "\nComments: 8 pages, no figures\n",
    "authors": [
      "Francesco Ballini",
      "Nikita Deniskin"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2007.08956"
  },
  {
    "id": "arXiv:2007.09890",
    "title": "Learning the Positions in CountSketch",
    "abstract": "Learning the Positions in CountSketch",
    "descriptor": "",
    "authors": [
      "Simin Liu",
      "Tianrui Liu",
      "Ali Vakilian",
      "Yulin Wan",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.09890"
  },
  {
    "id": "arXiv:2007.11622",
    "title": "TinyTL: Reduce Activations, Not Trainable Parameters for Efficient  On-Device Learning",
    "abstract": "Comments: NeurIPS 2020",
    "descriptor": "\nComments: NeurIPS 2020\n",
    "authors": [
      "Han Cai",
      "Chuang Gan",
      "Ligeng Zhu",
      "Song Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.11622"
  },
  {
    "id": "arXiv:2007.13086",
    "title": "Anonymizing Machine Learning Models",
    "abstract": "Anonymizing Machine Learning Models",
    "descriptor": "",
    "authors": [
      "Abigail Goldsteen",
      "Gilad Ezov",
      "Ron Shmelkin",
      "Micha Moffie",
      "Ariel Farkash"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.13086"
  },
  {
    "id": "arXiv:2007.14268",
    "title": "On the Convergence of Tsetlin Machines for the IDENTITY- and NOT  Operators",
    "abstract": "Comments: 23 pages, 10 figures",
    "descriptor": "\nComments: 23 pages, 10 figures\n",
    "authors": [
      "Xuan Zhang",
      "Lei Jiao",
      "Ole-Christoffer Granmo",
      "Morten Goodwin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.14268"
  },
  {
    "id": "arXiv:2007.15478",
    "title": "Quadratic Word Equations with Length Constraints, Counter Systems, and  Presburger Arithmetic with Divisibility",
    "abstract": "Comments: 19 pages, 3 figures, journal submission of ATVA'18 paper [arXiv:1805.06701]",
    "descriptor": "\nComments: 19 pages, 3 figures, journal submission of ATVA'18 paper [arXiv:1805.06701]\n",
    "authors": [
      "Anthony W. Lin",
      "Rupak Majumdar"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2007.15478"
  },
  {
    "id": "arXiv:2008.00742",
    "title": "Collaborative Learning in the Jungle",
    "abstract": "Comments: 34 pages, 1 figure",
    "descriptor": "\nComments: 34 pages, 1 figure\n",
    "authors": [
      "El-Mahdi El-Mhamdi",
      "Sadegh Farhadkhani",
      "Rachid Guerraoui",
      "Arsany Guirguis",
      "L\u00ea Nguy\u00ean Hoang",
      "S\u00e9bastien Rouault"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.00742"
  },
  {
    "id": "arXiv:2008.01739",
    "title": "Select, Extract and Generate: Neural Keyphrase Generation with  Layer-wise Coverage Attention",
    "abstract": "Comments: ACL 2021 (camera ready)",
    "descriptor": "\nComments: ACL 2021 (camera ready)\n",
    "authors": [
      "Wasi Uddin Ahmad",
      "Xiao Bai",
      "Soomin Lee",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2008.01739"
  },
  {
    "id": "arXiv:2008.02437",
    "title": "A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration",
    "abstract": "A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration",
    "descriptor": "",
    "authors": [
      "Yuetian Luo",
      "Garvesh Raskutti",
      "Ming Yuan",
      "Anru R. Zhang"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.02437"
  },
  {
    "id": "arXiv:2008.05558",
    "title": "On the complexity of finding a local minimizer of a quadratic function  over a polytope",
    "abstract": "Comments: 9 pages",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Amir Ali Ahmadi",
      "Jeffrey Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.05558"
  },
  {
    "id": "arXiv:2008.09214",
    "title": "Recorp: Receiver-Oriented Policies for Industrial Wireless Networks",
    "abstract": "Recorp: Receiver-Oriented Policies for Industrial Wireless Networks",
    "descriptor": "",
    "authors": [
      "Ryan Brummet",
      "Md Kowsar Hossain",
      "Octav Chipara",
      "Ted Herman",
      "David E. Steward"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2008.09214"
  },
  {
    "id": "arXiv:2008.10728",
    "title": "Constructive Spherical Codes by Hopf Foliations",
    "abstract": "Comments: 29 pages, 9 figures, submitted to the IEEE Transactions on Information Theory. Minor improvements",
    "descriptor": "\nComments: 29 pages, 9 figures, submitted to the IEEE Transactions on Information Theory. Minor improvements\n",
    "authors": [
      "Henrique K. Miyamoto",
      "Sueli I. R. Costa",
      "Henrique N. S\u00e1 Earp"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2008.10728"
  },
  {
    "id": "arXiv:2008.11015",
    "title": "Table2Charts: Recommending Charts by Learning Shared Table  Representations",
    "abstract": "Comments: 9 + 2(appendix) pages, accepted by KDD'21 conference",
    "descriptor": "\nComments: 9 + 2(appendix) pages, accepted by KDD'21 conference\n",
    "authors": [
      "Mengyu Zhou",
      "Qingtao Li",
      "Xinyi He",
      "Yuejiang Li",
      "Yibo Liu",
      "Wei Ji",
      "Shi Han",
      "Yining Chen",
      "Daxin Jiang",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2008.11015"
  },
  {
    "id": "arXiv:2008.11646",
    "title": "Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization",
    "abstract": "Comments: accepted by TCSVT",
    "descriptor": "\nComments: accepted by TCSVT\n",
    "authors": [
      "Tingyu Wang",
      "Zhedong Zheng",
      "Chenggang Yan",
      "Jiyong Zhang",
      "Yaoqi Sun",
      "Bolun Zheng",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.11646"
  },
  {
    "id": "arXiv:2008.12091",
    "title": "Limitations of Implicit Bias in Matrix Sensing: Initialization Rank  Matters",
    "abstract": "Limitations of Implicit Bias in Matrix Sensing: Initialization Rank  Matters",
    "descriptor": "",
    "authors": [
      "Armin Eftekhari",
      "Konstantinos Zygalakis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2008.12091"
  },
  {
    "id": "arXiv:2008.12315",
    "title": "Low-rank Characteristic Tensor Density Estimation Part I: Foundations",
    "abstract": "Low-rank Characteristic Tensor Density Estimation Part I: Foundations",
    "descriptor": "",
    "authors": [
      "Magda Amiridi",
      "Nikos Kargas",
      "Nicholas D. Sidiropoulos"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.12315"
  },
  {
    "id": "arXiv:2009.01197",
    "title": "An enhanced simulation-based iterated local search metaheuristic for  gravity fed water distribution network design optimization",
    "abstract": "An enhanced simulation-based iterated local search metaheuristic for  gravity fed water distribution network design optimization",
    "descriptor": "",
    "authors": [
      "Willian C. S. Martinho",
      "Rafael A. Melo",
      "Kenneth S\u00f6rensen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2009.01197"
  },
  {
    "id": "arXiv:2009.04324",
    "title": "Overcoming the curse of dimensionality with Laplacian regularization in  semi-supervised learning",
    "abstract": "Comments: 40 pages, 6 figures",
    "descriptor": "\nComments: 40 pages, 6 figures\n",
    "authors": [
      "Vivien Cabannes",
      "Loucas Pillaud-Vivien",
      "Francis Bach",
      "Alessandro Rudi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.04324"
  },
  {
    "id": "arXiv:2009.06097",
    "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range  Dependency Encoding",
    "abstract": "Comments: ACL Findings 2021, 11 pages",
    "descriptor": "\nComments: ACL Findings 2021, 11 pages\n",
    "authors": [
      "Shuohang Wang",
      "Luowei Zhou",
      "Zhe Gan",
      "Yen-Chun Chen",
      "Yuwei Fang",
      "Siqi Sun",
      "Yu Cheng",
      "Jingjing Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2009.06097"
  },
  {
    "id": "arXiv:2009.06303",
    "title": "Fed+: A Unified Approach to Robust Personalized Federated Learning",
    "abstract": "Fed+: A Unified Approach to Robust Personalized Federated Learning",
    "descriptor": "",
    "authors": [
      "Pengqian Yu",
      "Achintya Kundu",
      "Laura Wynter",
      "Shiau Hong Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.06303"
  },
  {
    "id": "arXiv:2009.06429",
    "title": "Into the Unknown: Active Monitoring of Neural Networks",
    "abstract": "Into the Unknown: Active Monitoring of Neural Networks",
    "descriptor": "",
    "authors": [
      "Anna Lukina",
      "Christian Schilling",
      "Thomas A. Henzinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2009.06429"
  },
  {
    "id": "arXiv:2009.06828",
    "title": "Matching in Selective and Balanced Representation Space for Treatment  Effects Estimation",
    "abstract": "Comments: Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)",
    "descriptor": "\nComments: Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)\n",
    "authors": [
      "Zhixuan Chu",
      "Stephen L. Rathbun",
      "Sheng Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2009.06828"
  },
  {
    "id": "arXiv:2009.06891",
    "title": "Global-aware Beam Search for Neural Abstractive Summarization",
    "abstract": "Comments: 27 pages",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Ye Ma",
      "Zixun Lan",
      "Lu Zong",
      "Kaizhu Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2009.06891"
  },
  {
    "id": "arXiv:2009.07536",
    "title": "Hybrid-Attention Guided Network with Multiple Resolution Features for  Person Re-Identification",
    "abstract": "Comments: 11 pages, 8 figures, 66 conferences",
    "descriptor": "\nComments: 11 pages, 8 figures, 66 conferences\n",
    "authors": [
      "Guoqing Zhang",
      "Junchuan Yang",
      "Yuhui Zheng",
      "Yi Wu",
      "Shengyong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.07536"
  },
  {
    "id": "arXiv:2009.07999",
    "title": "Distilled One-Shot Federated Learning",
    "abstract": "Distilled One-Shot Federated Learning",
    "descriptor": "",
    "authors": [
      "Yanlin Zhou",
      "George Pu",
      "Xiyao Ma",
      "Xiaolin Li",
      "Dapeng Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.07999"
  },
  {
    "id": "arXiv:2009.08228",
    "title": "LeadCache: Regret-Optimal Caching in Networks",
    "abstract": "LeadCache: Regret-Optimal Caching in Networks",
    "descriptor": "",
    "authors": [
      "Debjit Paria",
      "Abhishek Sinha"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2009.08228"
  },
  {
    "id": "arXiv:2009.08348",
    "title": "S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric  Learning",
    "abstract": "Comments: Accepted to ICML2021",
    "descriptor": "\nComments: Accepted to ICML2021\n",
    "authors": [
      "Karsten Roth",
      "Timo Milbich",
      "Bj\u00f6rn Ommer",
      "Joseph Paul Cohen",
      "Marzyeh Ghassemi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.08348"
  },
  {
    "id": "arXiv:2009.09229",
    "title": "Detailed Dynamic Model of Antagonistic PAM System and its Experimental  Validation: Sensor-less Angle and Torque Control with UKF",
    "abstract": "Comments: Accepted at IEEE/ASME Transactions on Mechatronics",
    "descriptor": "\nComments: Accepted at IEEE/ASME Transactions on Mechatronics\n",
    "authors": [
      "Takaya Shin",
      "Takumi Ibayashi",
      "Kiminao Kogiso"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2009.09229"
  },
  {
    "id": "arXiv:2009.11622",
    "title": "On Tractability of Ulams Metric in Highier Dimensions and Dually Related  Hierarchies",
    "abstract": "On Tractability of Ulams Metric in Highier Dimensions and Dually Related  Hierarchies",
    "descriptor": "",
    "authors": [
      "Sebastian Bala",
      "Andrzej Kozik"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2009.11622"
  },
  {
    "id": "arXiv:2009.12029",
    "title": "Privacy-Preserving Push-sum Average Consensus via State Decomposition",
    "abstract": "Privacy-Preserving Push-sum Average Consensus via State Decomposition",
    "descriptor": "",
    "authors": [
      "Xiaomeng Chen",
      "Lingying Huang",
      "Kemi Ding",
      "Subhrakanti Dey",
      "Ling Shi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2009.12029"
  },
  {
    "id": "arXiv:2009.13457",
    "title": "Drift Estimation of Multiscale Diffusions Based on Filtered Data",
    "abstract": "Drift Estimation of Multiscale Diffusions Based on Filtered Data",
    "descriptor": "",
    "authors": [
      "Assyr Abdulle",
      "Giacomo Garegnani",
      "Grigorios A. Pavliotis",
      "Andrew M. Stuart",
      "Andrea Zanoni"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2009.13457"
  },
  {
    "id": "arXiv:2010.00148",
    "title": "DEEPMIR: A DEEP neural network for differential detection of cerebral  Microbleeds and IRon deposits in MRI",
    "abstract": "DEEPMIR: A DEEP neural network for differential detection of cerebral  Microbleeds and IRon deposits in MRI",
    "descriptor": "",
    "authors": [
      "Tanweer Rashid",
      "Ahmed Abdulkadir",
      "Ilya M. Nasrallah",
      "Jeffrey B. Ware",
      "Hangfan Liu",
      "Pascal Spincemaille",
      "J. Rafael Romero",
      "R. Nick Bryan",
      "Susan R. Heckbert",
      "Mohamad Habes"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.00148"
  },
  {
    "id": "arXiv:2010.01056",
    "title": "AMR:Autonomous Coin Mixer with Privacy Preserving Reward Distribution",
    "abstract": "AMR:Autonomous Coin Mixer with Privacy Preserving Reward Distribution",
    "descriptor": "",
    "authors": [
      "Duc V. Le",
      "Arthur Gervais"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2010.01056"
  },
  {
    "id": "arXiv:2010.01618",
    "title": "A Modular Analysis of Provable Acceleration via Polyak's Momentum:  Training a Wide ReLU Network and a Deep Linear Network",
    "abstract": "Comments: Accepted at ICML 2021",
    "descriptor": "\nComments: Accepted at ICML 2021\n",
    "authors": [
      "Jun-Kun Wang",
      "Chi-Heng Lin",
      "Jacob Abernethy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.01618"
  },
  {
    "id": "arXiv:2010.02036",
    "title": "BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal  Transfer",
    "abstract": "BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal  Transfer",
    "descriptor": "",
    "authors": [
      "Or Patashnik",
      "Dov Danon",
      "Hao Zhang",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.02036"
  },
  {
    "id": "arXiv:2010.02720",
    "title": "Learnable Uncertainty under Laplace Approximations",
    "abstract": "Comments: UAI 2021",
    "descriptor": "\nComments: UAI 2021\n",
    "authors": [
      "Agustinus Kristiadi",
      "Matthias Hein",
      "Philipp Hennig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.02720"
  },
  {
    "id": "arXiv:2010.04230",
    "title": "No MCMC for me: Amortized sampling for fast and stable training of  energy-based models",
    "abstract": "No MCMC for me: Amortized sampling for fast and stable training of  energy-based models",
    "descriptor": "",
    "authors": [
      "Will Grathwohl",
      "Jacob Kelly",
      "Milad Hashemi",
      "Mohammad Norouzi",
      "Kevin Swersky",
      "David Duvenaud"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2010.04230"
  },
  {
    "id": "arXiv:2010.06255",
    "title": "Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A  Review and Experimental Evaluation",
    "abstract": "Comments: 28 pages, 10 figures, submitted to GRSM",
    "descriptor": "\nComments: 28 pages, 10 figures, submitted to GRSM\n",
    "authors": [
      "Changhong Fu",
      "Bowen Li",
      "Fangqiang Ding",
      "Fuling Lin",
      "Geng Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2010.06255"
  },
  {
    "id": "arXiv:2010.06997",
    "title": "Measuring the originality of intellectual property assets based on  machine learning outputs",
    "abstract": "Comments: 23 pages, 6 tables, 2 figures",
    "descriptor": "\nComments: 23 pages, 6 tables, 2 figures\n",
    "authors": [
      "S\u00e9bastien Ragot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2010.06997"
  },
  {
    "id": "arXiv:2010.07873",
    "title": "Neograd: Near-Ideal Gradient Descent",
    "abstract": "Comments: 22 pages, 12 figures; preprint",
    "descriptor": "\nComments: 22 pages, 12 figures; preprint\n",
    "authors": [
      "Michael F. Zimmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.07873"
  },
  {
    "id": "arXiv:2010.08047",
    "title": "Orbital MCMC",
    "abstract": "Orbital MCMC",
    "descriptor": "",
    "authors": [
      "Kirill Neklyudov",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2010.08047"
  },
  {
    "id": "arXiv:2010.08115",
    "title": "Pinball-OCSVM for early-stage COVID-19 diagnosis with limited  posteroanterior chest X-ray images",
    "abstract": "Pinball-OCSVM for early-stage COVID-19 diagnosis with limited  posteroanterior chest X-ray images",
    "descriptor": "",
    "authors": [
      "Sanjay Kumar Sonbhadra",
      "Sonali Agarwal",
      "P. Nagabhushan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.08115"
  },
  {
    "id": "arXiv:2010.08258",
    "title": "Variational (Gradient) Estimate of the Score Function in Energy-based  Latent Variable Models",
    "abstract": "Variational (Gradient) Estimate of the Score Function in Energy-based  Latent Variable Models",
    "descriptor": "",
    "authors": [
      "Fan Bao",
      "Kun Xu",
      "Chongxuan Li",
      "Lanqing Hong",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.08258"
  },
  {
    "id": "arXiv:2010.08548",
    "title": "Characterizing the Latent Space of Molecular Deep Generative Models with  Persistent Homology Metrics",
    "abstract": "Comments: Accepted to and presented as spotlight poster at the Topological Data Analysis and Beyond Workshop at the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)",
    "descriptor": "\nComments: Accepted to and presented as spotlight poster at the Topological Data Analysis and Beyond Workshop at the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)\n",
    "authors": [
      "Yair Schiff",
      "Vijil Chenthamarakshan",
      "Karthikeyan Natesan Ramamurthy",
      "Payel Das"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.08548"
  },
  {
    "id": "arXiv:2010.09345",
    "title": "A Framework to Learn with Interpretation",
    "abstract": "A Framework to Learn with Interpretation",
    "descriptor": "",
    "authors": [
      "Jayneel Parekh",
      "Pavlo Mozharovskyi",
      "Florence d'Alch\u00e9-Buc"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.09345"
  },
  {
    "id": "arXiv:2010.09610",
    "title": "Increasing Depth Leads to U-Shaped Test Risk in Over-parameterized  Convolutional Networks",
    "abstract": "Comments: 27 pages, 23 figures",
    "descriptor": "\nComments: 27 pages, 23 figures\n",
    "authors": [
      "Eshaan Nichani",
      "Adityanarayanan Radhakrishnan",
      "Caroline Uhler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.09610"
  },
  {
    "id": "arXiv:2010.09656",
    "title": "Operator Augmentation for Noisy Elliptic Systems",
    "abstract": "Operator Augmentation for Noisy Elliptic Systems",
    "descriptor": "",
    "authors": [
      "Philip A. Etter",
      "Lexing Ying"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2010.09656"
  },
  {
    "id": "arXiv:2010.10637",
    "title": "Mutual Information Regularized Identity-aware Facial  ExpressionRecognition in Compressed Video",
    "abstract": "Comments: Published in Pattern Recognition",
    "descriptor": "\nComments: Published in Pattern Recognition\n",
    "authors": [
      "Xiaofeng Liu",
      "Linghao Jin",
      "Xu Han",
      "Jane You"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2010.10637"
  },
  {
    "id": "arXiv:2010.10784",
    "title": "Learning to Embed Categorical Features without Embedding Tables for  Recommendation",
    "abstract": "Comments: Accepted to KDD'21, Research Track",
    "descriptor": "\nComments: Accepted to KDD'21, Research Track\n",
    "authors": [
      "Wang-Cheng Kang",
      "Derek Zhiyuan Cheng",
      "Tiansheng Yao",
      "Xinyang Yi",
      "Ting Chen",
      "Lichan Hong",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2010.10784"
  },
  {
    "id": "arXiv:2010.11791",
    "title": "ConVEx: Data-Efficient and Few-Shot Slot Labeling",
    "abstract": "Comments: NAACL 2021 (long)",
    "descriptor": "\nComments: NAACL 2021 (long)\n",
    "authors": [
      "Matthew Henderson",
      "Ivan Vuli\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2010.11791"
  },
  {
    "id": "arXiv:2010.11797",
    "title": "Should Graph Convolution Trust Neighbors? A Simple Causal Inference  Method",
    "abstract": "Comments: Accepted by SIGIR'21",
    "descriptor": "\nComments: Accepted by SIGIR'21\n",
    "authors": [
      "Fuli Feng",
      "Weiran Huang",
      "Xiangnan He",
      "Xin Xin",
      "Qifan Wang",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.11797"
  },
  {
    "id": "arXiv:2010.11915",
    "title": "Challenges in Information-Seeking QA: Unanswerable Questions and  Paragraph Retrieval",
    "abstract": "Comments: Published as a conference paper at ACL 2021 (long). Our code and annotated data are publicly available at this https URL",
    "descriptor": "\nComments: Published as a conference paper at ACL 2021 (long). Our code and annotated data are publicly available at this https URL\n",
    "authors": [
      "Akari Asai",
      "Eunsol Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2010.11915"
  },
  {
    "id": "arXiv:2010.11994",
    "title": "Thresholded Lasso Bandit",
    "abstract": "Thresholded Lasso Bandit",
    "descriptor": "",
    "authors": [
      "Kaito Ariu",
      "Kenshi Abe",
      "Alexandre Prouti\u00e8re"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.11994"
  },
  {
    "id": "arXiv:2010.12834",
    "title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization",
    "abstract": "Comments: ACL 2021 Findings",
    "descriptor": "\nComments: ACL 2021 Findings\n",
    "authors": [
      "Saadia Gabriel",
      "Asli Celikyilmaz",
      "Rahul Jha",
      "Yejin Choi",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2010.12834"
  },
  {
    "id": "arXiv:2010.13523",
    "title": "Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional  Data",
    "abstract": "Comments: 92 pages, 11 figures. Accepted to the Journal of Machine Learning Research",
    "descriptor": "\nComments: 92 pages, 11 figures. Accepted to the Journal of Machine Learning Research\n",
    "authors": [
      "Yikun Zhang",
      "Yen-Chi Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2010.13523"
  },
  {
    "id": "arXiv:2010.14184",
    "title": "Spatio-temporal encoding improves neuromorphic tactile texture  classification",
    "abstract": "Comments: 8 pages, 8 figures, accepted for publication to IEEE Sensors",
    "descriptor": "\nComments: 8 pages, 8 figures, accepted for publication to IEEE Sensors\n",
    "authors": [
      "Anupam K. Gupta",
      "Andrei Nakagawa",
      "Nathan F. Lepora",
      "Nitish V. Thakor"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2010.14184"
  },
  {
    "id": "arXiv:2010.14439",
    "title": "Differentiable Open-Ended Commonsense Reasoning",
    "abstract": "Comments: Accepted to NAACL 2021. Project website: this https URL",
    "descriptor": "\nComments: Accepted to NAACL 2021. Project website: this https URL\n",
    "authors": [
      "Bill Yuchen Lin",
      "Haitian Sun",
      "Bhuwan Dhingra",
      "Manzil Zaheer",
      "Xiang Ren",
      "William W. Cohen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.14439"
  },
  {
    "id": "arXiv:2010.14605",
    "title": "Traffic Refinery: Cost-Aware Data Representation for Machine Learning on  Network Traffic",
    "abstract": "Traffic Refinery: Cost-Aware Data Representation for Machine Learning on  Network Traffic",
    "descriptor": "",
    "authors": [
      "Francesco Bronzino",
      "Paul Schmitt",
      "Sara Ayoubi",
      "Hyojoon Kim",
      "Renata Teixeira",
      "Nick Feamster"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.14605"
  },
  {
    "id": "arXiv:2010.15162",
    "title": "Sizeless: Predicting the optimal size of serverless functions",
    "abstract": "Comments: 11 pages, 6 figures, conference",
    "descriptor": "\nComments: 11 pages, 6 figures, conference\n",
    "authors": [
      "Simon Eismann",
      "Long Bui",
      "Johannes Grohmann",
      "Cristina L. Abad",
      "Nikolas Herbst",
      "Samuel Kounev"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2010.15162"
  },
  {
    "id": "arXiv:2010.15350",
    "title": "A Hybrid Position/Force Controller for Joint Robots",
    "abstract": "Comments: To appear in ICRA 2021",
    "descriptor": "\nComments: To appear in ICRA 2021\n",
    "authors": [
      "Shengwen Xie",
      "Juan Ren"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2010.15350"
  },
  {
    "id": "arXiv:2010.15363",
    "title": "Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias  in Recommender System",
    "abstract": "Comments: To Appear in SIGKDD 2021",
    "descriptor": "\nComments: To Appear in SIGKDD 2021\n",
    "authors": [
      "Tianxin Wei",
      "Fuli Feng",
      "Jiawei Chen",
      "Ziwei Wu",
      "Jinfeng Yi",
      "Xiangnan He"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2010.15363"
  },
  {
    "id": "arXiv:2011.00095",
    "title": "Adversarial Attacks on Optimization based Planners",
    "abstract": "Comments: 7 pages. Presented at ICRA 2021",
    "descriptor": "\nComments: 7 pages. Presented at ICRA 2021\n",
    "authors": [
      "Sai Vemprala",
      "Ashish Kapoor"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2011.00095"
  },
  {
    "id": "arXiv:2011.00330",
    "title": "Resource Allocation in Multi-armed Bandit Exploration: Overcoming  Sublinear Scaling with Adaptive Parallelism",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Brijen Thananjeyan",
      "Kirthevasan Kandasamy",
      "Ion Stoica",
      "Michael I. Jordan",
      "Ken Goldberg",
      "Joseph E. Gonzalez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.00330"
  },
  {
    "id": "arXiv:2011.03609",
    "title": "A Few Shot Adaptation of Visual Navigation Skills to New Observations  using Meta-Learning",
    "abstract": "Comments: ICRA 2021",
    "descriptor": "\nComments: ICRA 2021\n",
    "authors": [
      "Qian Luo",
      "Maks Sorokin",
      "Sehoon Ha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.03609"
  },
  {
    "id": "arXiv:2011.03978",
    "title": "Smooth approximations and CSPs over finitely bounded homogeneous  structures",
    "abstract": "Comments: 42 pages. Latest version: added details to many proofs",
    "descriptor": "\nComments: 42 pages. Latest version: added details to many proofs\n",
    "authors": [
      "Antoine Mottet",
      "Michael Pinsker"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)",
      "Logic (math.LO)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2011.03978"
  },
  {
    "id": "arXiv:2011.05001",
    "title": "Integral Probability Metric based Regularization for Optimal Transport",
    "abstract": "Integral Probability Metric based Regularization for Optimal Transport",
    "descriptor": "",
    "authors": [
      "Piyushi Manupriya",
      "J. Saketha Nath",
      "Pratik Jawanpuria"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2011.05001"
  },
  {
    "id": "arXiv:2011.07423",
    "title": "Declarative Approaches to Counterfactual Explanations for Classification",
    "abstract": "Comments: Revised and considerably extended version of journal submission after reviews, by invitation. Based on RuleML-RR'20 paper [arXiv:2004.13237]",
    "descriptor": "\nComments: Revised and considerably extended version of journal submission after reviews, by invitation. Based on RuleML-RR'20 paper [arXiv:2004.13237]\n",
    "authors": [
      "Leopoldo Bertossi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2011.07423"
  },
  {
    "id": "arXiv:2011.07954",
    "title": "Using a Supervised Method without supervision for foreground  segmentation",
    "abstract": "Using a Supervised Method without supervision for foreground  segmentation",
    "descriptor": "",
    "authors": [
      "Levi Kassel",
      "Michael Werman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.07954"
  },
  {
    "id": "arXiv:2011.08474",
    "title": "Federated Composite Optimization",
    "abstract": "Comments: Accepted to ICML 2021. Code repository see this https URL",
    "descriptor": "\nComments: Accepted to ICML 2021. Code repository see this https URL\n",
    "authors": [
      "Honglin Yuan",
      "Manzil Zaheer",
      "Sashank Reddi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.08474"
  },
  {
    "id": "arXiv:2011.10115",
    "title": "Deep Neural Networks using a Single Neuron: Folded-in-Time Architecture  using Feedback-Modulated Delay Loops",
    "abstract": "Deep Neural Networks using a Single Neuron: Folded-in-Time Architecture  using Feedback-Modulated Delay Loops",
    "descriptor": "",
    "authors": [
      "Florian Stelzer",
      "Andr\u00e9 R\u00f6hm",
      "Raul Vicente",
      "Ingo Fischer",
      "Serhiy Yanchuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2011.10115"
  },
  {
    "id": "arXiv:2011.11060",
    "title": "Registration of serial sections: An evaluation method based on  distortions of the ground truths",
    "abstract": "Comments: Supplemental data available under this https URL",
    "descriptor": "\nComments: Supplemental data available under this https URL\n",
    "authors": [
      "Oleg Lobachev",
      "Takuya Funatomi",
      "Alexander Pfaffenroth",
      "Reinhold F\u00f6rster",
      "Lars Knudsen",
      "Christoph Wrede",
      "Michael Guthe",
      "David Haberth\u00fcr",
      "Ruslan Hlushchuk",
      "Thomas Salaets",
      "Jaan Toelen",
      "Simone Gaffling",
      "Christian M\u00fchlfeld",
      "Roman Grothausmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.11060"
  },
  {
    "id": "arXiv:2011.12454",
    "title": "Supercharging Imbalanced Data Learning With Energy-based Contrastive  Representation Transfer",
    "abstract": "Supercharging Imbalanced Data Learning With Energy-based Contrastive  Representation Transfer",
    "descriptor": "",
    "authors": [
      "Zidi Xiu",
      "Junya Chen",
      "Ricardo Henao",
      "Benjamin Goldstein",
      "Lawrence Carin",
      "Chenyang Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.12454"
  },
  {
    "id": "arXiv:2011.12979",
    "title": "De-STT: De-entaglement of unwanted Nuisances and Biases in Speech to  Text System using Adversarial Forgetting",
    "abstract": "Comments: Need to add substantial findings during the new experiments, which would mean to update the claim made in the paper",
    "descriptor": "\nComments: Need to add substantial findings during the new experiments, which would mean to update the claim made in the paper\n",
    "authors": [
      "Hemant Yadav",
      "Janvijay Singh",
      "Atul Anshuman Singh",
      "Rachit Mittal",
      "Rajiv Ratn Shah"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.12979"
  },
  {
    "id": "arXiv:2011.14251",
    "title": "Importance Weight Estimation and Generalization in Domain Adaptation  under Label Shift",
    "abstract": "Importance Weight Estimation and Generalization in Domain Adaptation  under Label Shift",
    "descriptor": "",
    "authors": [
      "Kamyar Azizzadenesheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.14251"
  },
  {
    "id": "arXiv:2011.14434",
    "title": "On the Nisan-Ronen conjecture",
    "abstract": "Comments: Improvements on presentation",
    "descriptor": "\nComments: Improvements on presentation\n",
    "authors": [
      "George Christodoulou",
      "Elias Koutsoupias",
      "Annamaria Kovacs"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2011.14434"
  },
  {
    "id": "arXiv:2011.14757",
    "title": "Message Passing Based Structured Sparse Signal Recovery for Estimation  of OTFS Channels with Fractional Doppler Shifts",
    "abstract": "Comments: Accepted by IEEE TWC",
    "descriptor": "\nComments: Accepted by IEEE TWC\n",
    "authors": [
      "Fei Liu",
      "Zhengdao Yuan",
      "Qinghua Guo",
      "Zhongyong Wang",
      "Peng Sun"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2011.14757"
  },
  {
    "id": "arXiv:2011.15003",
    "title": "Convolutive Transfer Function Invariant SDR training criteria for  Multi-Channel Reverberant Speech Separation",
    "abstract": "Comments: Accepted by ICASSP 2021",
    "descriptor": "\nComments: Accepted by ICASSP 2021\n",
    "authors": [
      "Christoph Boeddeker",
      "Wangyou Zhang",
      "Tomohiro Nakatani",
      "Keisuke Kinoshita",
      "Tsubasa Ochiai",
      "Marc Delcroix",
      "Naoyuki Kamo",
      "Yanmin Qian",
      "Reinhold Haeb-Umbach"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2011.15003"
  },
  {
    "id": "arXiv:2012.00780",
    "title": "Refining Deep Generative Models via Discriminator Gradient Flow",
    "abstract": "Comments: ICLR 2021 Camera Ready; Code available at this https URL; Updated Related Work",
    "descriptor": "\nComments: ICLR 2021 Camera Ready; Code available at this https URL; Updated Related Work\n",
    "authors": [
      "Abdul Fatir Ansari",
      "Ming Liang Ang",
      "Harold Soh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.00780"
  },
  {
    "id": "arXiv:2012.00889",
    "title": "Revisiting Maximum Entropy Inverse Reinforcement Learning: New  Perspectives and Algorithms",
    "abstract": "Comments: Published as a conference paper at the 2020 IEEE Symposium Series on Computational Intelligence (SSCI)",
    "descriptor": "\nComments: Published as a conference paper at the 2020 IEEE Symposium Series on Computational Intelligence (SSCI)\n",
    "authors": [
      "Aaron J. Snoswell",
      "Surya P. N. Singh",
      "Nan Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2012.00889"
  },
  {
    "id": "arXiv:2012.02119",
    "title": "Robustly Learning Mixtures of $k$ Arbitrary Gaussians",
    "abstract": "Comments: This version extends the previous one to yield 1) robust proper learning algorithm with poly(eps) error and 2) an information theoretic argument proving that the same algorithms in fact also yield parameter recovery guarantees. The updates are included in Sections 7,8, and 9 and the main result from the previous version (Thm 1.4) is presented and proved in Section 6",
    "descriptor": "\nComments: This version extends the previous one to yield 1) robust proper learning algorithm with poly(eps) error and 2) an information theoretic argument proving that the same algorithms in fact also yield parameter recovery guarantees. The updates are included in Sections 7,8, and 9 and the main result from the previous version (Thm 1.4) is presented and proved in Section 6\n",
    "authors": [
      "Ainesh Bakshi",
      "Ilias Diakonikolas",
      "He Jia",
      "Daniel M. Kane",
      "Pravesh K. Kothari",
      "Santosh S. Vempala"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.02119"
  },
  {
    "id": "arXiv:2012.02507",
    "title": "Coarse-to-Fine Entity Representations for Document-level Relation  Extraction",
    "abstract": "Coarse-to-Fine Entity Representations for Document-level Relation  Extraction",
    "descriptor": "",
    "authors": [
      "Damai Dai",
      "Jing Ren",
      "Shuang Zeng",
      "Baobao Chang",
      "Zhifang Sui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.02507"
  },
  {
    "id": "arXiv:2012.03176",
    "title": "Maximum Entropy Subspace Clustering Network",
    "abstract": "Maximum Entropy Subspace Clustering Network",
    "descriptor": "",
    "authors": [
      "Zhihao Peng",
      "Yuheng Jia",
      "Hui Liu",
      "Junhui Hou",
      "Qingfu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.03176"
  },
  {
    "id": "arXiv:2012.03515",
    "title": "Fine-grained Angular Contrastive Learning with Coarse Labels",
    "abstract": "Fine-grained Angular Contrastive Learning with Coarse Labels",
    "descriptor": "",
    "authors": [
      "Guy Bukchin",
      "Eli Schwartz",
      "Kate Saenko",
      "Ori Shahar",
      "Rogerio Feris",
      "Raja Giryes",
      "Leonid Karlinsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.03515"
  },
  {
    "id": "arXiv:2012.03955",
    "title": "Disentangling a Deep Learned Volume Formula",
    "abstract": "Comments: v1: 26 + 19 pages, 15 figures; v2: 27 + 18 pages, figures updated, references added, journal version",
    "descriptor": "\nComments: v1: 26 + 19 pages, 15 figures; v2: 27 + 18 pages, figures updated, references added, journal version\n",
    "authors": [
      "Jessica Craven",
      "Vishnu Jejjala",
      "Arjun Kar"
    ],
    "subjectives": [
      "High Energy Physics - Theory (hep-th)",
      "Machine Learning (cs.LG)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2012.03955"
  },
  {
    "id": "arXiv:2012.04337",
    "title": "Robust Learning by Self-Transition for Handling Noisy Labels",
    "abstract": "Comments: Accepted at KDD 2021",
    "descriptor": "\nComments: Accepted at KDD 2021\n",
    "authors": [
      "Hwanjun Song",
      "Minseok Kim",
      "Dongmin Park",
      "Yooju Shin",
      "Jae-Gil Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.04337"
  },
  {
    "id": "arXiv:2012.04483",
    "title": "A Novel Transformation Approach of Shared-link Coded Caching Schemes for  Multiaccess Networks",
    "abstract": "Comments: 19 pages",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Minquan Cheng",
      "Kai Wan",
      "Dequan Liang",
      "Mingming Zhang",
      "Giuseppe Caire"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.04483"
  },
  {
    "id": "arXiv:2012.05419",
    "title": "A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based  Neuromorphic Processors",
    "abstract": "Comments: This work is dated and will be superseded by a forthcoming work",
    "descriptor": "\nComments: This work is dated and will be superseded by a forthcoming work\n",
    "authors": [
      "Harideep Nair",
      "Prabhu Vellaisamy",
      "Santha Bhasuthkar",
      "John Paul Shen"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2012.05419"
  },
  {
    "id": "arXiv:2012.05460",
    "title": "Quasi-polynomial time approximation of output probabilities of  geometrically-local, shallow quantum circuits",
    "abstract": "Quasi-polynomial time approximation of output probabilities of  geometrically-local, shallow quantum circuits",
    "descriptor": "",
    "authors": [
      "Nolan J. Coble",
      "Matthew Coudron"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2012.05460"
  },
  {
    "id": "arXiv:2012.06387",
    "title": "TARA: Training and Representation Alteration for AI Fairness and Domain  Generalization",
    "abstract": "Comments: Submitted to MIT Neural Computation",
    "descriptor": "\nComments: Submitted to MIT Neural Computation\n",
    "authors": [
      "William Paul",
      "Armin Hadzic",
      "Neil Joshi",
      "Fady Alajaji",
      "Phil Burlina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.06387"
  },
  {
    "id": "arXiv:2012.06609",
    "title": "RegulaTor: A Straightforward Website Fingerprinting Defense",
    "abstract": "RegulaTor: A Straightforward Website Fingerprinting Defense",
    "descriptor": "",
    "authors": [
      "James K Holland",
      "Nicholas Hopper"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2012.06609"
  },
  {
    "id": "arXiv:2012.06675",
    "title": "Clustered Sparse Channel Estimation for Massive MIMO Systems by  Expectation Maximization-Propagation (EM-EP)",
    "abstract": "Clustered Sparse Channel Estimation for Massive MIMO Systems by  Expectation Maximization-Propagation (EM-EP)",
    "descriptor": "",
    "authors": [
      "Mohammed Rashid",
      "Mort Naraghi-Pour"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.06675"
  },
  {
    "id": "arXiv:2012.09688",
    "title": "PCT: Point cloud transformer",
    "abstract": "Comments: 11 pages, 5 figures",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Meng-Hao Guo",
      "Jun-Xiong Cai",
      "Zheng-Ning Liu",
      "Tai-Jiang Mu",
      "Ralph R. Martin",
      "Shi-Min Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.09688"
  },
  {
    "id": "arXiv:2012.14788",
    "title": "Detection of Lexical Stress Errors in Non-Native (L2) English with Data  Augmentation and Attention",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Daniel Korzekwa",
      "Roberto Barra-Chicote",
      "Szymon Zaporowski",
      "Grzegorz Beringer",
      "Jaime Lorenzo-Trueba",
      "Alicja Serafinowicz",
      "Jasha Droppo",
      "Thomas Drugman",
      "Bozena Kostek"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2012.14788"
  },
  {
    "id": "arXiv:2012.14897",
    "title": "$\\mathcal{PT}$-Symmetric Quantum Discrimination of Three States",
    "abstract": "$\\mathcal{PT}$-Symmetric Quantum Discrimination of Three States",
    "descriptor": "",
    "authors": [
      "Yaroslav Balytskyi",
      "Manohar Raavi",
      "Anatoliy Pinchuk",
      "Sang-Yoon Chang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.14897"
  },
  {
    "id": "arXiv:2012.15010",
    "title": "PMGT-VR: A decentralized proximal-gradient algorithmic framework with  variance reduction",
    "abstract": "Comments: 16 pages, 4 figures",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Haishan Ye",
      "Wei Xiong",
      "Tong Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.15010"
  },
  {
    "id": "arXiv:2012.15699",
    "title": "Better Robustness by More Coverage: Adversarial Training with Mixup  Augmentation for Robust Fine-tuning",
    "abstract": "Comments: ACL 2021 (Findings)",
    "descriptor": "\nComments: ACL 2021 (Findings)\n",
    "authors": [
      "Chenglei Si",
      "Zhengyan Zhang",
      "Fanchao Qi",
      "Zhiyuan Liu",
      "Yasheng Wang",
      "Qun Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.15699"
  },
  {
    "id": "arXiv:2012.15859",
    "title": "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    "abstract": "Comments: In Proceedings of ACL 2021, 9 pages",
    "descriptor": "\nComments: In Proceedings of ACL 2021, 9 pages\n",
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Rebecca Marchant",
      "Ricardo Mu\u00f1oz Sanchez",
      "Mugdha Pandya",
      "Adam Lopez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.15859"
  },
  {
    "id": "arXiv:2101.00123",
    "title": "Intent Classification and Slot Filling for Privacy Policies",
    "abstract": "Comments: ACL 2021 (camera ready)",
    "descriptor": "\nComments: ACL 2021 (camera ready)\n",
    "authors": [
      "Wasi Uddin Ahmad",
      "Jianfeng Chi",
      "Tu Le",
      "Thomas Norton",
      "Yuan Tian",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.00123"
  },
  {
    "id": "arXiv:2101.02615",
    "title": "On the Expected Value of Buffer Size in IoT Devices Deploying REST HTTP",
    "abstract": "Comments: This paper is uploaded here for research community, thus it is for non-commercial purposes",
    "descriptor": "\nComments: This paper is uploaded here for research community, thus it is for non-commercial purposes\n",
    "authors": [
      "Cao Vien Phung",
      "Mounir Bensalem",
      "Admela Jukan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2101.02615"
  },
  {
    "id": "arXiv:2101.03727",
    "title": "Computer-assisted proof for the stationary solution existence of the  Navier-Stokes equation over 3D domains",
    "abstract": "Comments: 1 figures",
    "descriptor": "\nComments: 1 figures\n",
    "authors": [
      "Xuefeng Liu",
      "Mitsuhiro T. Nakao",
      "Shin'ichi Oishi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2101.03727"
  },
  {
    "id": "arXiv:2101.04792",
    "title": "Learning Efficient Representations for Keyword Spotting with Triplet  Loss",
    "abstract": "Comments: Submitted to SPECOM 2021",
    "descriptor": "\nComments: Submitted to SPECOM 2021\n",
    "authors": [
      "Roman Vygon",
      "Nikolay Mikhaylovskiy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.04792"
  },
  {
    "id": "arXiv:2101.05434",
    "title": "A Unified Conditional Disentanglement Framework for Multimodal Brain MR  Image Translation",
    "abstract": "Comments: Published in IEEE International Symposium on Biomedical Imaging (ISBI) 2021 for Oral presentation",
    "descriptor": "\nComments: Published in IEEE International Symposium on Biomedical Imaging (ISBI) 2021 for Oral presentation\n",
    "authors": [
      "Xiaofeng Liu",
      "Fangxu Xing",
      "Georges El Fakhri",
      "Jonghye Woo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.05434"
  },
  {
    "id": "arXiv:2101.07524",
    "title": "PeerGAN: Generative Adversarial Networks with a Competing Peer  Discriminator",
    "abstract": "Comments: Under Review",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Jiaheng Wei",
      "Minghao Liu",
      "Jiahao Luo",
      "Qiutong Li",
      "James Davis",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.07524"
  },
  {
    "id": "arXiv:2101.08894",
    "title": "Generative Replay-based Continual Zero-Shot Learning",
    "abstract": "Generative Replay-based Continual Zero-Shot Learning",
    "descriptor": "",
    "authors": [
      "Chandan Gautam",
      "Sethupathy Parameswaran",
      "Ashish Mishra",
      "Suresh Sundaram"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.08894"
  },
  {
    "id": "arXiv:2101.09612",
    "title": "On the Proof of Global Convergence of Gradient Descent for Deep ReLU  Networks with Linear Widths",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Quynh Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.09612"
  },
  {
    "id": "arXiv:2101.10318",
    "title": "Multi-Time Attention Networks for Irregularly Sampled Time Series",
    "abstract": "Comments: Accepted at International Conference on Learning Representations (ICLR) 2021",
    "descriptor": "\nComments: Accepted at International Conference on Learning Representations (ICLR) 2021\n",
    "authors": [
      "Satya Narayan Shukla",
      "Benjamin M. Marlin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.10318"
  },
  {
    "id": "arXiv:2101.12169",
    "title": "Rate-Energy Balanced Precoding Design for SWIPT based Two-Way Relay  Systems",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2101.12161",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2101.12161\n",
    "authors": [
      "Navneet Garg",
      "Junkai Zhang",
      "Tharmalingam Ratnarajah"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2101.12169"
  },
  {
    "id": "arXiv:2102.00643",
    "title": "An Exhaustive Survey on P4 Programmable Data Plane Switches: Taxonomy,  Applications, Challenges, and Future Trends",
    "abstract": "An Exhaustive Survey on P4 Programmable Data Plane Switches: Taxonomy,  Applications, Challenges, and Future Trends",
    "descriptor": "",
    "authors": [
      "Elie F. Kfoury",
      "Jorge Crichigno",
      "Elias Bou-Harb"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2102.00643"
  },
  {
    "id": "arXiv:2102.00815",
    "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and  Sample-Efficient Algorithms",
    "abstract": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and  Sample-Efficient Algorithms",
    "descriptor": "",
    "authors": [
      "Chi Jin",
      "Qinghua Liu",
      "Sobhan Miryoosefi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.00815"
  },
  {
    "id": "arXiv:2102.01197",
    "title": "Common Randomness Generation over Slow Fading Channels",
    "abstract": "Common Randomness Generation over Slow Fading Channels",
    "descriptor": "",
    "authors": [
      "Rami Ezzine",
      "Moritz Wiese",
      "Christian Deppe",
      "Holger Boche"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.01197"
  },
  {
    "id": "arXiv:2102.01412",
    "title": "On Codes for the Noisy Substring Channel",
    "abstract": "Comments: ISIT 2021 version (including all proofs)",
    "descriptor": "\nComments: ISIT 2021 version (including all proofs)\n",
    "authors": [
      "Yonatan Yehezkeally",
      "Nikita Polyanskii"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.01412"
  },
  {
    "id": "arXiv:2102.02267",
    "title": "DEFT: Detection Embeddings for Tracking",
    "abstract": "Comments: Accepted at CVPR 2021, ADP3 Workshop on Autonomous Driving: Perception, Prediction and Planning",
    "descriptor": "\nComments: Accepted at CVPR 2021, ADP3 Workshop on Autonomous Driving: Perception, Prediction and Planning\n",
    "authors": [
      "Mohamed Chaabane",
      "Peter Zhang",
      "J. Ross Beveridge",
      "Stephen O'Hara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.02267"
  },
  {
    "id": "arXiv:2102.02400",
    "title": "Provably End-to-end Label-Noise Learning without Anchor Points",
    "abstract": "Provably End-to-end Label-Noise Learning without Anchor Points",
    "descriptor": "",
    "authors": [
      "Xuefeng Li",
      "Tongliang Liu",
      "Bo Han",
      "Gang Niu",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.02400"
  },
  {
    "id": "arXiv:2102.02992",
    "title": "Learning High Dimensional Wasserstein Geodesics",
    "abstract": "Learning High Dimensional Wasserstein Geodesics",
    "descriptor": "",
    "authors": [
      "Shu Liu",
      "Shaojun Ma",
      "Yongxin Chen",
      "Hongyuan Zha",
      "Haomin Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.02992"
  },
  {
    "id": "arXiv:2102.03150",
    "title": "Equivariant message passing for the prediction of tensorial properties  and molecular spectra",
    "abstract": "Comments: Accepted at ICML 2021",
    "descriptor": "\nComments: Accepted at ICML 2021\n",
    "authors": [
      "Kristof T. Sch\u00fctt",
      "Oliver T. Unke",
      "Michael Gastegger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2102.03150"
  },
  {
    "id": "arXiv:2102.03448",
    "title": "Federated Reconstruction: Partially Local Federated Learning",
    "abstract": "Federated Reconstruction: Partially Local Federated Learning",
    "descriptor": "",
    "authors": [
      "Karan Singhal",
      "Hakim Sidahmed",
      "Zachary Garrett",
      "Shanshan Wu",
      "Keith Rush",
      "Sushant Prakash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.03448"
  },
  {
    "id": "arXiv:2102.03521",
    "title": "Haptic-enabled Mixed Reality System for Mixed-initiative Remote Robot  Control",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Yuan Tian",
      "Lianjun Li",
      "Andrea Fumagalli",
      "Yonas Tadesse",
      "Balakrishnan Prabhakaran"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2102.03521"
  },
  {
    "id": "arXiv:2102.04050",
    "title": "A Constant Approximation Algorithm for Sequential Random-Order  No-Substitution k-Median Clustering",
    "abstract": "A Constant Approximation Algorithm for Sequential Random-Order  No-Substitution k-Median Clustering",
    "descriptor": "",
    "authors": [
      "Tom Hess",
      "Michal Moshkovitz",
      "Sivan Sabato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.04050"
  },
  {
    "id": "arXiv:2102.04159",
    "title": "Deep Residual Learning in Spiking Neural Networks",
    "abstract": "Deep Residual Learning in Spiking Neural Networks",
    "descriptor": "",
    "authors": [
      "Wei Fang",
      "Zhaofei Yu",
      "Yanqi Chen",
      "Tiejun Huang",
      "Timoth\u00e9e Masquelier",
      "Yonghong Tian"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2102.04159"
  },
  {
    "id": "arXiv:2102.04509",
    "title": "Oops I Took A Gradient: Scalable Sampling for Discrete Distributions",
    "abstract": "Comments: Energy-Based Models, Deep generative models, MCMC sampling",
    "descriptor": "\nComments: Energy-Based Models, Deep generative models, MCMC sampling\n",
    "authors": [
      "Will Grathwohl",
      "Kevin Swersky",
      "Milad Hashemi",
      "David Duvenaud",
      "Chris J. Maddison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.04509"
  },
  {
    "id": "arXiv:2102.04832",
    "title": "Fast and Accurate Amplitude Demodulation of Wideband Signals",
    "abstract": "Comments: Accepted for publication in IEEE Transactions on Signal Processing",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Signal Processing\n",
    "authors": [
      "Mantas Gabrielaitis"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2102.04832"
  },
  {
    "id": "arXiv:2102.05114",
    "title": "A note on (matricial and fast) ways to compute Burt's structural holes",
    "abstract": "Comments: 8 pages, 3 figures",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Alessio Muscillo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2102.05114"
  },
  {
    "id": "arXiv:2102.05209",
    "title": "Quantum State Classification via Quantum Fourier",
    "abstract": "Quantum State Classification via Quantum Fourier",
    "descriptor": "",
    "authors": [
      "Mohsen Heidari",
      "Wojciech Szpankowski"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.05209"
  },
  {
    "id": "arXiv:2102.05313",
    "title": "Conditional Versus Adversarial Euler-based Generators For Time Series",
    "abstract": "Comments: 14 page, 9 Figures",
    "descriptor": "\nComments: 14 page, 9 Figures\n",
    "authors": [
      "Carl Remlinger",
      "Joseph Mikael",
      "Romuald Elie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2102.05313"
  },
  {
    "id": "arXiv:2102.05375",
    "title": "Strength of Minibatch Noise in SGD",
    "abstract": "Comments: The first two authors contributed equally",
    "descriptor": "\nComments: The first two authors contributed equally\n",
    "authors": [
      "Liu Ziyin",
      "Kangqiao Liu",
      "Takashi Mori",
      "Masahito Ueda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.05375"
  },
  {
    "id": "arXiv:2102.05573",
    "title": "A Witness Two-Sample Test",
    "abstract": "Comments: Under Review - Code available upon personal request",
    "descriptor": "\nComments: Under Review - Code available upon personal request\n",
    "authors": [
      "Jonas M. K\u00fcbler",
      "Wittawat Jitkrittum",
      "Bernhard Sch\u00f6lkopf",
      "Krikamol Muandet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.05573"
  },
  {
    "id": "arXiv:2102.05640",
    "title": "An Exact Solver for the Weston-Watkins SVM Subproblem",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Yutong Wang",
      "Clayton D. Scott"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05640"
  },
  {
    "id": "arXiv:2102.06522",
    "title": "Sequential Neural Posterior and Likelihood Approximation",
    "abstract": "Comments: 28 pages, 8 tables, 14 figures. The supplementary material is attached to the main paper",
    "descriptor": "\nComments: 28 pages, 8 tables, 14 figures. The supplementary material is attached to the main paper\n",
    "authors": [
      "Samuel Wiqvist",
      "Jes Frellsen",
      "Umberto Picchini"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2102.06522"
  },
  {
    "id": "arXiv:2102.06663",
    "title": "Potential Singularity Formation of 3D Axisymmetric Navier-Stokes  Equations with Degenerate Diffusion Coefficients",
    "abstract": "Potential Singularity Formation of 3D Axisymmetric Navier-Stokes  Equations with Degenerate Diffusion Coefficients",
    "descriptor": "",
    "authors": [
      "Thomas Y. Hou",
      "De Huang"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2102.06663"
  },
  {
    "id": "arXiv:2102.06790",
    "title": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
    "abstract": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Tianlong Chen",
      "Yongduo Sui",
      "Xuxi Chen",
      "Aston Zhang",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.06790"
  },
  {
    "id": "arXiv:2102.06866",
    "title": "Understanding Negative Samples in Instance Discriminative  Self-supervised Representation Learning",
    "abstract": "Comments: 24 pages, 5 figures, and 4 tables",
    "descriptor": "\nComments: 24 pages, 5 figures, and 4 tables\n",
    "authors": [
      "Kento Nozawa",
      "Issei Sato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.06866"
  },
  {
    "id": "arXiv:2102.07868",
    "title": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning",
    "abstract": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning",
    "descriptor": "",
    "authors": [
      "Idan Achituve",
      "Aviv Navon",
      "Yochai Yemini",
      "Gal Chechik",
      "Ethan Fetaya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07868"
  },
  {
    "id": "arXiv:2102.07945",
    "title": "Local Hyper-Flow Diffusion",
    "abstract": "Comments: 47 pages, 10 figures, 12 tables",
    "descriptor": "\nComments: 47 pages, 10 figures, 12 tables\n",
    "authors": [
      "Kimon Fountoulakis",
      "Pan Li",
      "Shenghao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.07945"
  },
  {
    "id": "arXiv:2102.07960",
    "title": "A Combination of Multi-Objective Genetic Algorithm and Deep Learning for  Music Harmony Generation",
    "abstract": "Comments: 14 pages, 8 figures, 1 table",
    "descriptor": "\nComments: 14 pages, 8 figures, 1 table\n",
    "authors": [
      "Maryam Majidi",
      "Rahil Mahdian Toroghi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.07960"
  },
  {
    "id": "arXiv:2102.08348",
    "title": "Unambiguous DNFs and Alon-Saks-Seymour",
    "abstract": "Comments: v1: 12 pages, 2 figures. v2: Added an author; improved result; 15 pages",
    "descriptor": "\nComments: v1: 12 pages, 2 figures. v2: Added an author; improved result; 15 pages\n",
    "authors": [
      "Kaspars Balodis",
      "Shalev Ben-David",
      "Mika G\u00f6\u00f6s",
      "Siddhartha Jain",
      "Robin Kothari"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2102.08348"
  },
  {
    "id": "arXiv:2102.08786",
    "title": "Graph Learning with 1D Convolutions on Random Walks",
    "abstract": "Graph Learning with 1D Convolutions on Random Walks",
    "descriptor": "",
    "authors": [
      "Jan Toenshoff",
      "Martin Ritzert",
      "Hinrikus Wolf",
      "Martin Grohe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2102.08786"
  },
  {
    "id": "arXiv:2102.08868",
    "title": "Bridging the Gap Between Adversarial Robustness and Optimization Bias",
    "abstract": "Comments: New CIFAR-10 experiments and Fourier attack variations",
    "descriptor": "\nComments: New CIFAR-10 experiments and Fourier attack variations\n",
    "authors": [
      "Fartash Faghri",
      "Sven Gowal",
      "Cristina Vasconcelos",
      "David J. Fleet",
      "Fabian Pedregosa",
      "Nicolas Le Roux"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.08868"
  },
  {
    "id": "arXiv:2102.09041",
    "title": "Reaching Consensus for Asynchronous Distributed Key Generation",
    "abstract": "Reaching Consensus for Asynchronous Distributed Key Generation",
    "descriptor": "",
    "authors": [
      "Ittai Abraham",
      "Philipp Jovanovic",
      "Mary Maller",
      "Sarah Meiklejohn",
      "Gilad Stern",
      "Alin Tomescu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.09041"
  },
  {
    "id": "arXiv:2102.09050",
    "title": "End-to-end learnable EEG channel selection for deep neural networks with  Gumbel-softmax",
    "abstract": "Comments: Updated revisions",
    "descriptor": "\nComments: Updated revisions\n",
    "authors": [
      "Thomas Strypsteen",
      "Alexander Bertrand"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09050"
  },
  {
    "id": "arXiv:2102.09161",
    "title": "On the Sample Complexity of Stability Constrained Imitation Learning",
    "abstract": "On the Sample Complexity of Stability Constrained Imitation Learning",
    "descriptor": "",
    "authors": [
      "Stephen Tu",
      "Alexander Robey",
      "Tingnan Zhang",
      "Nikolai Matni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.09161"
  },
  {
    "id": "arXiv:2102.09594",
    "title": "Embedding a Deterministic BFT Protocol in a Block DAG",
    "abstract": "Embedding a Deterministic BFT Protocol in a Block DAG",
    "descriptor": "",
    "authors": [
      "Maria A Schett",
      "George Danezis"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.09594"
  },
  {
    "id": "arXiv:2102.09599",
    "title": "Privacy-Preserving Kickstarting Deep Reinforcement Learning with  Privacy-Aware Learners",
    "abstract": "Comments: Under double-blind review",
    "descriptor": "\nComments: Under double-blind review\n",
    "authors": [
      "Parham Gohari",
      "Bo Chen",
      "Bo Wu",
      "Matthew Hale",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2102.09599"
  },
  {
    "id": "arXiv:2102.09820",
    "title": "Strong-Diameter Network Decomposition",
    "abstract": "Strong-Diameter Network Decomposition",
    "descriptor": "",
    "authors": [
      "Yi-Jun Chang",
      "Mohsen Ghaffari"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.09820"
  },
  {
    "id": "arXiv:2102.09850",
    "title": "Model-Invariant State Abstractions for Model-Based Reinforcement  Learning",
    "abstract": "Model-Invariant State Abstractions for Model-Based Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Manan Tomar",
      "Amy Zhang",
      "Roberto Calandra",
      "Matthew E. Taylor",
      "Joelle Pineau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2102.09850"
  },
  {
    "id": "arXiv:2102.09949",
    "title": "Fundamentals of Semantic Numeration Systems. Can the Context be  Calculated?",
    "abstract": "Comments: 15 pages, 8 figures",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Alexander Chunikhin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2102.09949"
  },
  {
    "id": "arXiv:2102.10058",
    "title": "Principled Simplicial Neural Networks for Trajectory Prediction",
    "abstract": "Comments: To appear in ICML 2021",
    "descriptor": "\nComments: To appear in ICML 2021\n",
    "authors": [
      "T. Mitchell Roddenberry",
      "Nicholas Glaze",
      "Santiago Segarra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2102.10058"
  },
  {
    "id": "arXiv:2102.10567",
    "title": "Dynamical Analysis of the EIP-1559 Ethereum Fee Market",
    "abstract": "Dynamical Analysis of the EIP-1559 Ethereum Fee Market",
    "descriptor": "",
    "authors": [
      "Stefanos Leonardos",
      "Barnab\u00e9 Monnot",
      "Dani\u00ebl Reijsbergen",
      "Stratis Skoulakis",
      "Georgios Piliouras"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.10567"
  },
  {
    "id": "arXiv:2102.12886",
    "title": "Generalized Parametric Path Problems",
    "abstract": "Generalized Parametric Path Problems",
    "descriptor": "",
    "authors": [
      "Prerona Chatterjee",
      "Kshitij Gajjar",
      "Jaikumar Radhakrishnan",
      "Girish Varma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.12886"
  },
  {
    "id": "arXiv:2102.13515",
    "title": "Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning",
    "abstract": "Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "V\u00edctor Campos",
      "Pablo Sprechmann",
      "Steven Hansen",
      "Andre Barreto",
      "Steven Kapturowski",
      "Alex Vitvitskyi",
      "Adri\u00e0 Puigdom\u00e8nech Badia",
      "Charles Blundell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13515"
  },
  {
    "id": "arXiv:2103.00210",
    "title": "Application of the unified control and detection framework to detecting  stealthy integrity cyber-attacks on feedback control systems",
    "abstract": "Application of the unified control and detection framework to detecting  stealthy integrity cyber-attacks on feedback control systems",
    "descriptor": "",
    "authors": [
      "Steven X. Ding",
      "Linlin Li",
      "Dong Zhao",
      "Chris Louen",
      "Tianyu Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.00210"
  },
  {
    "id": "arXiv:2103.00223",
    "title": "Generalized Universe Hierarchies and First-Class Universe Levels",
    "abstract": "Generalized Universe Hierarchies and First-Class Universe Levels",
    "descriptor": "",
    "authors": [
      "Andr\u00e1s Kov\u00e1cs"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2103.00223"
  },
  {
    "id": "arXiv:2103.01024",
    "title": "Periodic trajectories in P-time event graphs and the non-positive  circuit weight problem",
    "abstract": "Comments: Minor corrections",
    "descriptor": "\nComments: Minor corrections\n",
    "authors": [
      "Davide Zorzenon",
      "Jan Komenda",
      "Joerg Raisch"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2103.01024"
  },
  {
    "id": "arXiv:2103.01028",
    "title": "Information Discrepancy in Strategic Learning",
    "abstract": "Information Discrepancy in Strategic Learning",
    "descriptor": "",
    "authors": [
      "Yahav Bechavod",
      "Chara Podimata",
      "Zhiwei Steven Wu",
      "Juba Ziani"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.01028"
  },
  {
    "id": "arXiv:2103.01379",
    "title": "Practical Privacy Filters and Odometers with R\u00e9nyi Differential  Privacy and Applications to Differentially Private Deep Learning",
    "abstract": "Practical Privacy Filters and Odometers with R\u00e9nyi Differential  Privacy and Applications to Differentially Private Deep Learning",
    "descriptor": "",
    "authors": [
      "Mathias L\u00e9cuyer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.01379"
  },
  {
    "id": "arXiv:2103.02014",
    "title": "Online Adversarial Attacks",
    "abstract": "Comments: Preprint",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Andjela Mladenovic",
      "Avishek Joey Bose",
      "Hugo Berard",
      "William L. Hamilton",
      "Simon Lacoste-Julien",
      "Pascal Vincent",
      "Gauthier Gidel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2103.02014"
  },
  {
    "id": "arXiv:2103.03399",
    "title": "Representation Matters: Assessing the Importance of Subgroup Allocations  in Training Data",
    "abstract": "Comments: Accepted to ICML 2021; 31 pages,9 figures",
    "descriptor": "\nComments: Accepted to ICML 2021; 31 pages,9 figures\n",
    "authors": [
      "Esther Rolf",
      "Theodora Worledge",
      "Benjamin Recht",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.03399"
  },
  {
    "id": "arXiv:2103.03417",
    "title": "Measuring Model Biases in the Absence of Ground Truth",
    "abstract": "Measuring Model Biases in the Absence of Ground Truth",
    "descriptor": "",
    "authors": [
      "Osman Aka",
      "Ken Burke",
      "Alex B\u00e4uerle",
      "Christina Greer",
      "Margaret Mitchell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.03417"
  },
  {
    "id": "arXiv:2103.03757",
    "title": "Discrepancy-Based Active Learning for Domain Adaptation",
    "abstract": "Comments: 28 pages, 11 figures",
    "descriptor": "\nComments: 28 pages, 11 figures\n",
    "authors": [
      "Antoine de Mathelin",
      "Francois Deheeger",
      "Mathilde Mougeot",
      "Nicolas Vayatis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.03757"
  },
  {
    "id": "arXiv:2103.04250",
    "title": "Greedy Approximation Algorithms for Active Sequential Hypothesis Testing",
    "abstract": "Greedy Approximation Algorithms for Active Sequential Hypothesis Testing",
    "descriptor": "",
    "authors": [
      "Kyra Gan",
      "Su Jia",
      "Andrew Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.04250"
  },
  {
    "id": "arXiv:2103.04304",
    "title": "Fair-Share Allocations for Agents with Arbitrary Entitlements",
    "abstract": "Fair-Share Allocations for Agents with Arbitrary Entitlements",
    "descriptor": "",
    "authors": [
      "Moshe Babaioff",
      "Tomer Ezra",
      "Uriel Feige"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2103.04304"
  },
  {
    "id": "arXiv:2103.04400",
    "title": "What If We Only Use Real Datasets for Scene Text Recognition? Toward  Scene Text Recognition With Fewer Labels",
    "abstract": "Comments: CVPR 2021",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Jeonghun Baek",
      "Yusuke Matsui",
      "Kiyoharu Aizawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.04400"
  },
  {
    "id": "arXiv:2103.05126",
    "title": "Exact Distribution-Free Hypothesis Tests for the Regression Function of  Binary Classification via Conditional Kernel Mean Embeddings",
    "abstract": "Exact Distribution-Free Hypothesis Tests for the Regression Function of  Binary Classification via Conditional Kernel Mean Embeddings",
    "descriptor": "",
    "authors": [
      "Ambrus Tam\u00e1s",
      "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.05126"
  },
  {
    "id": "arXiv:2103.05908",
    "title": "DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End  Information Extraction",
    "abstract": "DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End  Information Extraction",
    "descriptor": "",
    "authors": [
      "Freddy C. Chua",
      "Nigel P. Duffy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.05908"
  },
  {
    "id": "arXiv:2103.06408",
    "title": "Geometric Approaches on Persistent Homology",
    "abstract": "Geometric Approaches on Persistent Homology",
    "descriptor": "",
    "authors": [
      "Henry Adams",
      "Baris Coskunuzer"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2103.06408"
  },
  {
    "id": "arXiv:2103.06419",
    "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid  pooling based residual U-Net for automatic liver segmentation in Computed  Tomography",
    "abstract": "Comments: 25 pages, 17 figures; revised manuscript",
    "descriptor": "\nComments: 25 pages, 17 figures; revised manuscript\n",
    "authors": [
      "Jinke Wang",
      "Peiqing Lv",
      "Haiying Wang",
      "Changfa Shi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.06419"
  },
  {
    "id": "arXiv:2103.07054",
    "title": "FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose  Estimation with Decoupled Rotation Mechanism",
    "abstract": "Comments: accepted by CVPR2021, oral",
    "descriptor": "\nComments: accepted by CVPR2021, oral\n",
    "authors": [
      "Wei Chen",
      "Xi Jia",
      "Hyung Jin Chang",
      "Jinming Duan",
      "Linlin Shen",
      "Ales Leonardis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.07054"
  },
  {
    "id": "arXiv:2103.07230",
    "title": "Sequential Random Network for Fine-grained Image Classification",
    "abstract": "Comments: The performance of the model is very severely affected by the order of the test samples",
    "descriptor": "\nComments: The performance of the model is very severely affected by the order of the test samples\n",
    "authors": [
      "Chaorong Li",
      "Malu Zhang",
      "Wei Huang",
      "Fengqing Qin",
      "Anping Zeng",
      "Yuanyuan Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.07230"
  },
  {
    "id": "arXiv:2103.07309",
    "title": "Vectorial Parameterizations of Pose",
    "abstract": "Comments: 12 pages, 5 figures",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Timothy D. Barfoot",
      "James R. Forbes",
      "Gabriele M. T. D'Eleuterio"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.07309"
  },
  {
    "id": "arXiv:2103.07780",
    "title": "Online Double Oracle",
    "abstract": "Comments: yaodong.yang@outlook.com",
    "descriptor": "\nComments: yaodong.yang@outlook.com\n",
    "authors": [
      "Le Cong Dinh",
      "Yaodong Yang",
      "Zheng Tian",
      "Nicolas Perez Nieves",
      "Oliver Slumbers",
      "David Henry Mguni",
      "Haitham Bou Ammar",
      "Jun Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2103.07780"
  },
  {
    "id": "arXiv:2103.08463",
    "title": "How to distribute data across tasks for meta-learning?",
    "abstract": "How to distribute data across tasks for meta-learning?",
    "descriptor": "",
    "authors": [
      "Alexandru Cioba",
      "Michael Bromberg",
      "Qian Wang",
      "Ritwik Niyogi",
      "Georgios Batzolis",
      "Jezabel Garcia",
      "Da-shan Shiu",
      "Alberto Bernacchia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.08463"
  },
  {
    "id": "arXiv:2103.09593",
    "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots",
    "abstract": "Comments: To be presented at NAACL-HLT 2021. Abstract also published in the Rising Stars Track of the Workshop on Computational Approaches to Linguistic Code-Switching (CALCS 2021)",
    "descriptor": "\nComments: To be presented at NAACL-HLT 2021. Abstract also published in the Rising Stars Track of the Workshop on Computational Approaches to Linguistic Code-Switching (CALCS 2021)\n",
    "authors": [
      "Samson Tan",
      "Shafiq Joty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2103.09593"
  },
  {
    "id": "arXiv:2103.09943",
    "title": "Fast and High-Quality Blind Multi-Spectral Image Pansharpening",
    "abstract": "Comments: 17 pages, 47 figures, journal, accepted by IEEE Transactions on Geoscience of Remote Sensing",
    "descriptor": "\nComments: 17 pages, 47 figures, journal, accepted by IEEE Transactions on Geoscience of Remote Sensing\n",
    "authors": [
      "Lantao Yu",
      "Dehong Liu",
      "Hassan Mansour",
      "Petros T. Boufounos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.09943"
  },
  {
    "id": "arXiv:2103.10325",
    "title": "Contextual Biasing of Language Models for Speech Recognition in  Goal-Oriented Conversational Agents",
    "abstract": "Comments: Updated version with extensions are uploaded here arXiv:2104.11070",
    "descriptor": "\nComments: Updated version with extensions are uploaded here arXiv:2104.11070\n",
    "authors": [
      "Ashish Shenoy",
      "Sravan Bodapati",
      "Katrin Kirchhoff"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2103.10325"
  },
  {
    "id": "arXiv:2103.10702",
    "title": "ClawCraneNet: Leveraging Object-level Relation for Text-based Video  Segmentation",
    "abstract": "ClawCraneNet: Leveraging Object-level Relation for Text-based Video  Segmentation",
    "descriptor": "",
    "authors": [
      "Chen Liang",
      "Yu Wu",
      "Yawei Luo",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.10702"
  },
  {
    "id": "arXiv:2103.11055",
    "title": "Online Robust Control of Nonlinear Systems with Large Uncertainty",
    "abstract": "Comments: 58 pages, 5 figures",
    "descriptor": "\nComments: 58 pages, 5 figures\n",
    "authors": [
      "Dimitar Ho",
      "Hoang M. Le",
      "John C. Doyle",
      "Yisong Yue"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.11055"
  },
  {
    "id": "arXiv:2103.11804",
    "title": "Detection of fake news on CoViD-19 on Web Search Engines",
    "abstract": "Detection of fake news on CoViD-19 on Web Search Engines",
    "descriptor": "",
    "authors": [
      "V. Mazzeo",
      "A. Rapisarda",
      "G. Giuffrida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2103.11804"
  },
  {
    "id": "arXiv:2103.12731",
    "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones",
    "abstract": "Comments: CVPR 2021 Oral",
    "descriptor": "\nComments: CVPR 2021 Oral\n",
    "authors": [
      "Ashish Vaswani",
      "Prajit Ramachandran",
      "Aravind Srinivas",
      "Niki Parmar",
      "Blake Hechtman",
      "Jonathon Shlens"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.12731"
  },
  {
    "id": "arXiv:2103.14250",
    "title": "Evaluation of deep learning models for multi-step ahead time series  prediction",
    "abstract": "Evaluation of deep learning models for multi-step ahead time series  prediction",
    "descriptor": "",
    "authors": [
      "Rohitash Chandra",
      "Shaurya Goyal",
      "Rishabh Gupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.14250"
  },
  {
    "id": "arXiv:2103.14625",
    "title": "Dodrio: Exploring Transformer Models with Interactive Visualization",
    "abstract": "Comments: 10 pages, 8 figures, Accepted to ACL 2021. For a demo video, see this https URL . For a live demo, see this https URL",
    "descriptor": "\nComments: 10 pages, 8 figures, Accepted to ACL 2021. For a demo video, see this https URL . For a live demo, see this https URL\n",
    "authors": [
      "Zijie J. Wang",
      "Robert Turko",
      "Duen Horng Chau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.14625"
  },
  {
    "id": "arXiv:2103.15060",
    "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Ye Jia",
      "Heiga Zen",
      "Jonathan Shen",
      "Yu Zhang",
      "Yonghui Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2103.15060"
  },
  {
    "id": "arXiv:2103.15422",
    "title": "Reduced Basis Methods for Efficient Simulation of a Rigid Robot Hand  Interacting with Soft Tissue",
    "abstract": "Reduced Basis Methods for Efficient Simulation of a Rigid Robot Hand  Interacting with Soft Tissue",
    "descriptor": "",
    "authors": [
      "Shahnewaz Shuva",
      "Patrick Buchfink",
      "Oliver R\u00f6hrle",
      "Bernard Haasdonk"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.15422"
  },
  {
    "id": "arXiv:2103.16689",
    "title": "Multi-Source Causal Inference Using Control Variates",
    "abstract": "Multi-Source Causal Inference Using Control Variates",
    "descriptor": "",
    "authors": [
      "Wenshuo Guo",
      "Serena Wang",
      "Peng Ding",
      "Yixin Wang",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2103.16689"
  },
  {
    "id": "arXiv:2103.17182",
    "title": "Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to  Improve Generalization",
    "abstract": "Comments: ICML 2021; 20 pages; 13 figures; Key Words: deep learning theory, optimizer, momentum, generalization, gradient noise",
    "descriptor": "\nComments: ICML 2021; 20 pages; 13 figures; Key Words: deep learning theory, optimizer, momentum, generalization, gradient noise\n",
    "authors": [
      "Zeke Xie",
      "Li Yuan",
      "Zhanxing Zhu",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.17182"
  },
  {
    "id": "arXiv:2104.00520",
    "title": "DIVERSE: Bayesian Data IntegratiVE learning for precise drug ResponSE  prediction",
    "abstract": "DIVERSE: Bayesian Data IntegratiVE learning for precise drug ResponSE  prediction",
    "descriptor": "",
    "authors": [
      "Bet\u00fcl G\u00fcven\u00e7 Paltun",
      "Samuel Kaski",
      "Hiroshi Mamitsuka"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)"
    ],
    "url": "https://arxiv.org/abs/2104.00520"
  },
  {
    "id": "arXiv:2104.00825",
    "title": "Towards High Fidelity Face Relighting with Realistic Shadows",
    "abstract": "Comments: Accepted to CVPR 2021",
    "descriptor": "\nComments: Accepted to CVPR 2021\n",
    "authors": [
      "Andrew Hou",
      "Ze Zhang",
      "Michel Sarkis",
      "Ning Bi",
      "Yiying Tong",
      "Xiaoming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.00825"
  },
  {
    "id": "arXiv:2104.00861",
    "title": "Algorithms for Poisson Phase Retrieval",
    "abstract": "Comments: Submitted to IEEE TCI",
    "descriptor": "\nComments: Submitted to IEEE TCI\n",
    "authors": [
      "Zongyu Li",
      "Kenneth Lange",
      "Jeffrey A. Fessler"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2104.00861"
  },
  {
    "id": "arXiv:2104.00994",
    "title": "Unsupervised Acoustic Unit Discovery by Leveraging a  Language-Independent Subword Discriminative Feature Representation",
    "abstract": "Comments: Accepted for publication in INTERSPEECH 2021",
    "descriptor": "\nComments: Accepted for publication in INTERSPEECH 2021\n",
    "authors": [
      "Siyuan Feng",
      "Piotr \u017belasko",
      "Laureano Moro-Vel\u00e1zquez",
      "Odette Scharenborg"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.00994"
  },
  {
    "id": "arXiv:2104.01532",
    "title": "Fitting Splines to Axonal Arbors Quantifies Relationship between Branch  Order and Geometry",
    "abstract": "Fitting Splines to Axonal Arbors Quantifies Relationship between Branch  Order and Geometry",
    "descriptor": "",
    "authors": [
      "Thomas L. Athey",
      "Jacopo Teneggi",
      "Joshua T. Vogelstein",
      "Daniel Tward",
      "Ulrich Mueller",
      "Michael I. Miller"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Mathematical Software (cs.MS)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2104.01532"
  },
  {
    "id": "arXiv:2104.01978",
    "title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in  Audio-Visual Emotion Recognition",
    "abstract": "Comments: paper accepted by INTERSPEECH2021",
    "descriptor": "\nComments: paper accepted by INTERSPEECH2021\n",
    "authors": [
      "Haoqi Li",
      "Yelin Kim",
      "Cheng-Hao Kuo",
      "Shrikanth Narayanan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.01978"
  },
  {
    "id": "arXiv:2104.02242",
    "title": "hBert + BiasCorp -- Fighting Racism on the Web",
    "abstract": "hBert + BiasCorp -- Fighting Racism on the Web",
    "descriptor": "",
    "authors": [
      "Olawale Onabola",
      "Zhuang Ma",
      "Yang Xie",
      "Benjamin Akera",
      "Abdulrahman Ibraheem",
      "Jia Xue",
      "Dianbo Liu",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.02242"
  },
  {
    "id": "arXiv:2104.02610",
    "title": "On the Robustness of Vision Transformers to Adversarial Examples",
    "abstract": "On the Robustness of Vision Transformers to Adversarial Examples",
    "descriptor": "",
    "authors": [
      "Kaleel Mahmood",
      "Rigel Mahmood",
      "Marten van Dijk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.02610"
  },
  {
    "id": "arXiv:2104.02822",
    "title": "Low-Regret Active learning",
    "abstract": "Low-Regret Active learning",
    "descriptor": "",
    "authors": [
      "Cenk Baykal",
      "Lucas Liebenwein",
      "Dan Feldman",
      "Daniela Rus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.02822"
  },
  {
    "id": "arXiv:2104.02951",
    "title": "A Hybrid Inference System for Improved Curvature Estimation in the  Level-Set Method Using Machine Learning",
    "abstract": "Comments: Submitted",
    "descriptor": "\nComments: Submitted\n",
    "authors": [
      "Luis \u00c1ngel Larios-C\u00e1rdenas",
      "Frederic Gibou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2104.02951"
  },
  {
    "id": "arXiv:2104.03279",
    "title": "Modern Hopfield Networks for Few- and Zero-Shot Reaction Template  Prediction",
    "abstract": "Comments: 14 pages + 12 pages appendix",
    "descriptor": "\nComments: 14 pages + 12 pages appendix\n",
    "authors": [
      "Philipp Seidl",
      "Philipp Renz",
      "Natalia Dyubankova",
      "Paulo Neves",
      "Jonas Verhoeven",
      "Marwin Segler",
      "J\u00f6rg K. Wegner",
      "Sepp Hochreiter",
      "G\u00fcnter Klambauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.03279"
  },
  {
    "id": "arXiv:2104.03640",
    "title": "Semantic Scene Completion via Integrating Instances and Scene  in-the-Loop",
    "abstract": "Comments: CVPR 2021",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Yingjie Cai",
      "Xuesong Chen",
      "Chao Zhang",
      "Kwan-Yee Lin",
      "Xiaogang Wang",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.03640"
  },
  {
    "id": "arXiv:2104.03736",
    "title": "Towards Enabling Meta-Learning from Target Models",
    "abstract": "Towards Enabling Meta-Learning from Target Models",
    "descriptor": "",
    "authors": [
      "Su Lu",
      "Han-Jia Ye",
      "Le Gan",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.03736"
  },
  {
    "id": "arXiv:2104.04552",
    "title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition",
    "abstract": "Comments: Presented as conference paper at Interspeech 2021",
    "descriptor": "\nComments: Presented as conference paper at Interspeech 2021\n",
    "authors": [
      "W. Ronny Huang",
      "Tara N. Sainath",
      "Cal Peyser",
      "Shankar Kumar",
      "David Rybach",
      "Trevor Strohman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.04552"
  },
  {
    "id": "arXiv:2104.04940",
    "title": "Dissecting the square into seven or nine congruent parts",
    "abstract": "Dissecting the square into seven or nine congruent parts",
    "descriptor": "",
    "authors": [
      "Gerardo L. Maldonado",
      "Edgardo Rold\u00e1n-Pensado"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2104.04940"
  },
  {
    "id": "arXiv:2104.05207",
    "title": "Online Machine Learning Techniques for Coq: A Comparison",
    "abstract": "Comments: Intelligent Computer Mathematics 14th International Conference, CICM 2021",
    "descriptor": "\nComments: Intelligent Computer Mathematics 14th International Conference, CICM 2021\n",
    "authors": [
      "Liao Zhang",
      "Lasse Blaauwbroek",
      "Bartosz Piotrowski",
      "Prokop \u010cern\u00fd",
      "Cezary Kaliszyk",
      "Josef Urban"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.05207"
  },
  {
    "id": "arXiv:2104.05784",
    "title": "Extremely Low Footprint End-to-End ASR System for Smart Device",
    "abstract": "Comments: 5 pages, 2 figures, accepted by INTERSPEECH 2021",
    "descriptor": "\nComments: 5 pages, 2 figures, accepted by INTERSPEECH 2021\n",
    "authors": [
      "Zhifu Gao",
      "Yiwu Yao",
      "Shiliang Zhang",
      "Jun Yang",
      "Ming Lei",
      "Ian McLoughlin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.05784"
  },
  {
    "id": "arXiv:2104.06249",
    "title": "Prediction of Apophis Asteroid Flyby Optimal Trajectories and Data  Fusion of Earth-Apophis Mission Launch Windows using Deep Neural Networks",
    "abstract": "Comments: 6 pages, 5 figures",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Manuel Ntumba",
      "Saurabh Gore",
      "Jean-Baptiste Awanyo"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.06249"
  },
  {
    "id": "arXiv:2104.06643",
    "title": "Generative Causal Explanations for Graph Neural Networks",
    "abstract": "Comments: To appear in the Thirty-eighth International Conference on Machine Learning (ICML2021). Source code: this https URL",
    "descriptor": "\nComments: To appear in the Thirty-eighth International Conference on Machine Learning (ICML2021). Source code: this https URL\n",
    "authors": [
      "Wanyu Lin",
      "Hao Lan",
      "Baochun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.06643"
  },
  {
    "id": "arXiv:2104.07085",
    "title": "Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary  Layers in Deep Neural Networks",
    "abstract": "Comments: The paper has been accepted to CVPR 2021 BiVision Workshop. We notice the final Conv2D is also a 1x1 convolution layer so we update the result with changing it",
    "descriptor": "\nComments: The paper has been accepted to CVPR 2021 BiVision Workshop. We notice the final Conv2D is also a 1x1 convolution layer so we update the result with changing it\n",
    "authors": [
      "Hongyi Pan",
      "Diaa Dabawi",
      "Ahmet Enis Cetin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2104.07085"
  },
  {
    "id": "arXiv:2104.07365",
    "title": "D-Cliques: Compensating NonIIDness in Decentralized Federated Learning  with Topology",
    "abstract": "D-Cliques: Compensating NonIIDness in Decentralized Federated Learning  with Topology",
    "descriptor": "",
    "authors": [
      "Aur\u00e9lien Bellet",
      "Anne-Marie Kermarrec",
      "Erick Lavoie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2104.07365"
  },
  {
    "id": "arXiv:2104.08382",
    "title": "Lower Bounds on Cross-Entropy Loss in the Presence of Test-time  Adversaries",
    "abstract": "Comments: 16 pages, 12 figures; Accepted to ICML 2021",
    "descriptor": "\nComments: 16 pages, 12 figures; Accepted to ICML 2021\n",
    "authors": [
      "Arjun Nitin Bhagoji",
      "Daniel Cullina",
      "Vikash Sehwag",
      "Prateek Mittal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.08382"
  },
  {
    "id": "arXiv:2104.08669",
    "title": "Fifty Three Matrix Factorizations: A systematic approach",
    "abstract": "Comments: 73 pages",
    "descriptor": "\nComments: 73 pages\n",
    "authors": [
      "Alan Edelman",
      "Sungwoo Jeong"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2104.08669"
  },
  {
    "id": "arXiv:2104.08736",
    "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with  Provable Convergence",
    "abstract": "Comments: 24 pages, 8 figures",
    "descriptor": "\nComments: 24 pages, 8 figures\n",
    "authors": [
      "Qi Qi",
      "Youzhi Luo",
      "Zhao Xu",
      "Shuiwang Ji",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2104.08736"
  },
  {
    "id": "arXiv:2104.08776",
    "title": "Federated Learning of User Verification Models Without Sharing  Embeddings",
    "abstract": "Federated Learning of User Verification Models Without Sharing  Embeddings",
    "descriptor": "",
    "authors": [
      "Hossein Hosseini",
      "Hyunsin Park",
      "Sungrack Yun",
      "Christos Louizos",
      "Joseph Soriaga",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.08776"
  },
  {
    "id": "arXiv:2104.09137",
    "title": "Community Detection for Access-Control Decisions: Analysing the Role of  Homophily and Information Diffusion in Online Social Networks",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Nicolas E. Diaz Ferreyra",
      "Tobias Hecking",
      "Esma A\u00efmeur",
      "Maritta Heisel",
      "H. Ulrich Hoppe"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2104.09137"
  },
  {
    "id": "arXiv:2104.09435",
    "title": "Deep learning enables reference-free isotropic super-resolution for  volumetric fluorescence microscopy",
    "abstract": "Deep learning enables reference-free isotropic super-resolution for  volumetric fluorescence microscopy",
    "descriptor": "",
    "authors": [
      "Hyoungjun Park",
      "Myeongsu Na",
      "Bumju Kim",
      "Soohyun Park",
      "Ki Hean Kim",
      "Sunghoe Chang",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.09435"
  },
  {
    "id": "arXiv:2104.09667",
    "title": "Manipulating SGD with Data Ordering Attacks",
    "abstract": "Manipulating SGD with Data Ordering Attacks",
    "descriptor": "",
    "authors": [
      "Ilia Shumailov",
      "Zakhar Shumaylov",
      "Dmitry Kazhdan",
      "Yiren Zhao",
      "Nicolas Papernot",
      "Murat A. Erdogdu",
      "Ross Anderson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.09667"
  },
  {
    "id": "arXiv:2104.10336",
    "title": "MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor  Based on Multi-Task Adversarial Training",
    "abstract": "Comments: 7 pages, 1 figure, 4 tables",
    "descriptor": "\nComments: 7 pages, 1 figure, 4 tables\n",
    "authors": [
      "Jian Ma",
      "Shuyi Xie",
      "Haiqin Yang",
      "Lianxin Jiang",
      "Mengyuan Zhou",
      "Xiaoyi Ruan",
      "Yang Mo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.10336"
  },
  {
    "id": "arXiv:2104.10366",
    "title": "Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement  Verification and Evidence Finding with Tables",
    "abstract": "Comments: 7 pages, 1 figure, 3 tables, the champion solution for SemEval-21 Task 9",
    "descriptor": "\nComments: 7 pages, 1 figure, 3 tables, the champion solution for SemEval-21 Task 9\n",
    "authors": [
      "Xiaoyi Ruan",
      "Meizhi Jin",
      "Jian Ma",
      "Haiqin Yang",
      "Lianxin Jiang",
      "Yang Mo",
      "Mengyuan Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.10366"
  },
  {
    "id": "arXiv:2104.10375",
    "title": "PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context  Disambiguation",
    "abstract": "Comments: 7 pages, 2 figures, 2 tables, winning solution on En-Ar, En-Fr, En-Ru, and En-Zh cross-lingual tasks of SemEval'21 Task 2",
    "descriptor": "\nComments: 7 pages, 2 figures, 2 tables, winning solution on En-Ar, En-Fr, En-Ru, and En-Zh cross-lingual tasks of SemEval'21 Task 2\n",
    "authors": [
      "Shuyi Xie",
      "Jian Ma",
      "Haiqin Yang",
      "Lianxin Jiang",
      "Yang Mo",
      "Jianping Shen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.10375"
  },
  {
    "id": "arXiv:2104.10972",
    "title": "ImageNet-21K Pretraining for the Masses",
    "abstract": "ImageNet-21K Pretraining for the Masses",
    "descriptor": "",
    "authors": [
      "Tal Ridnik",
      "Emanuel Ben-Baruch",
      "Asaf Noy",
      "Lihi Zelnik-Manor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.10972"
  },
  {
    "id": "arXiv:2104.11070",
    "title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents",
    "abstract": "Comments: Accepted to Interspeech 2021. arXiv admin note: text overlap with arXiv:2103.10325",
    "descriptor": "\nComments: Accepted to Interspeech 2021. arXiv admin note: text overlap with arXiv:2103.10325\n",
    "authors": [
      "Ashish Shenoy",
      "Sravan Bodapati",
      "Monica Sunkara",
      "Srikanth Ronanki",
      "Katrin Kirchhoff"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.11070"
  },
  {
    "id": "arXiv:2104.11455",
    "title": "Birds of a Feather Flock Together: A Close Look at Cooperation Emergence  via Multi-Agent RL",
    "abstract": "Birds of a Feather Flock Together: A Close Look at Cooperation Emergence  via Multi-Agent RL",
    "descriptor": "",
    "authors": [
      "Heng Dong",
      "Tonghan Wang",
      "Jiayuan Liu",
      "Chi Han",
      "Chongjie Zhang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.11455"
  },
  {
    "id": "arXiv:2104.11543",
    "title": "Middle-level Fusion for Lightweight RGB-D Salient Object Detection",
    "abstract": "Comments: 11 pages, 6 figures",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Nianchang Huang",
      "Qiang Zhang",
      "Jungong Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.11543"
  },
  {
    "id": "arXiv:2104.12055",
    "title": "Machine Learning Approaches for Binary Classification to Discover Liver  Diseases using Clinical Data",
    "abstract": "Machine Learning Approaches for Binary Classification to Discover Liver  Diseases using Clinical Data",
    "descriptor": "",
    "authors": [
      "Fahad B. Mostafa",
      "Md Easin Hasan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2104.12055"
  },
  {
    "id": "arXiv:2104.13095",
    "title": "Relational Learning with Gated and Attentive Neighbor Aggregator for  Few-Shot Knowledge Graph Completion",
    "abstract": "Comments: The full version of a paper accepted to SIGIR 2021",
    "descriptor": "\nComments: The full version of a paper accepted to SIGIR 2021\n",
    "authors": [
      "Guanglin Niu",
      "Yang Li",
      "Chengguang Tang",
      "Ruiying Geng",
      "Jian Dai",
      "Qiao Liu",
      "Hao Wang",
      "Jian Sun",
      "Fei Huang",
      "Luo Si"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.13095"
  },
  {
    "id": "arXiv:2104.13100",
    "title": "Shellcode_IA32: A Dataset for Automatic Shellcode Generation",
    "abstract": "Shellcode_IA32: A Dataset for Automatic Shellcode Generation",
    "descriptor": "",
    "authors": [
      "Pietro Liguori",
      "Erfan Al-Hossami",
      "Domenico Cotroneo",
      "Roberto Natella",
      "Bojan Cukic",
      "Samira Shaikh"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.13100"
  },
  {
    "id": "arXiv:2104.13785",
    "title": "Mutualized oblivious DNS ($\u03bc$ODNS): Hiding a tree in the wild forest",
    "abstract": "Comments: article class, 17pages, 3 figures, 2 tables, the citation [16] is corrected in version 2",
    "descriptor": "\nComments: article class, 17pages, 3 figures, 2 tables, the citation [16] is corrected in version 2\n",
    "authors": [
      "Jun Kurihara",
      "Takeshi Kubo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2104.13785"
  },
  {
    "id": "arXiv:2104.14716",
    "title": "A Novel Provably Secure Key-Agreement Using Secret Subgroup Generator",
    "abstract": "A Novel Provably Secure Key-Agreement Using Secret Subgroup Generator",
    "descriptor": "",
    "authors": [
      "Abdelhaliem Babiker"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.14716"
  },
  {
    "id": "arXiv:2105.00793",
    "title": "Tubal Matrices",
    "abstract": "Tubal Matrices",
    "descriptor": "",
    "authors": [
      "Liqun Qi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.00793"
  },
  {
    "id": "arXiv:2105.01757",
    "title": "A Survey on End-User Robot Programming",
    "abstract": "Comments: 35 pages, 1 figure",
    "descriptor": "\nComments: 35 pages, 1 figure\n",
    "authors": [
      "Gopika Ajaykumar",
      "Maureen Steele",
      "Chien-Ming Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2105.01757"
  },
  {
    "id": "arXiv:2105.01941",
    "title": "Monotonicity-Based Regularization for Shape Reconstruction in Linear  Elasticity",
    "abstract": "Comments: 25 pages, 14 figures",
    "descriptor": "\nComments: 25 pages, 14 figures\n",
    "authors": [
      "Sarah Eberle",
      "Bastian Harrach"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.01941"
  },
  {
    "id": "arXiv:2105.02468",
    "title": "Relative stability toward diffeomorphisms indicates performance in deep  nets",
    "abstract": "Relative stability toward diffeomorphisms indicates performance in deep  nets",
    "descriptor": "",
    "authors": [
      "Leonardo Petrini",
      "Alessandro Favero",
      "Mario Geiger",
      "Matthieu Wyart"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.02468"
  },
  {
    "id": "arXiv:2105.03019",
    "title": "Imitation Learning via Simultaneous Optimization of Policies and  Auxiliary Trajectories",
    "abstract": "Imitation Learning via Simultaneous Optimization of Policies and  Auxiliary Trajectories",
    "descriptor": "",
    "authors": [
      "Mandy Xie",
      "Anqi Li",
      "Karl Van Wyk",
      "Frank Dellaert",
      "Byron Boots",
      "Nathan Ratliff"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03019"
  },
  {
    "id": "arXiv:2105.03358",
    "title": "Soft-Attention Improves Skin Cancer Classification Performance",
    "abstract": "Comments: 8 pages, 9 figures, 4 tables",
    "descriptor": "\nComments: 8 pages, 9 figures, 4 tables\n",
    "authors": [
      "Soumyya Kanti Datta",
      "Mohammad Abuzar Shaikh",
      "Sargur N. Srihari",
      "Mingchen Gao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03358"
  },
  {
    "id": "arXiv:2105.03584",
    "title": "Adaptive Latent Space Tuning for Non-Stationary Distributions",
    "abstract": "Adaptive Latent Space Tuning for Non-Stationary Distributions",
    "descriptor": "",
    "authors": [
      "Alexander Scheinker",
      "Frederick Cropp",
      "Sergio Paiagua",
      "Daniele Filippetto"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Accelerator Physics (physics.acc-ph)"
    ],
    "url": "https://arxiv.org/abs/2105.03584"
  },
  {
    "id": "arXiv:2105.03902",
    "title": "Learning Gradient Fields for Molecular Conformation Generation",
    "abstract": "Comments: ICML 2021, Long talk",
    "descriptor": "\nComments: ICML 2021, Long talk\n",
    "authors": [
      "Chence Shi",
      "Shitong Luo",
      "Minkai Xu",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2105.03902"
  },
  {
    "id": "arXiv:2105.03966",
    "title": "Unit Ball Model for Embedding Hierarchical Structures in the Complex  Hyperbolic Space",
    "abstract": "Unit Ball Model for Embedding Hierarchical Structures in the Complex  Hyperbolic Space",
    "descriptor": "",
    "authors": [
      "Huiru Xiao",
      "Caigao Jiang",
      "Yangqiu Song",
      "James Zhang",
      "Junwu Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03966"
  },
  {
    "id": "arXiv:2105.04522",
    "title": "Generalized Jensen-Shannon Divergence Loss for Learning with Noisy  Labels",
    "abstract": "Generalized Jensen-Shannon Divergence Loss for Learning with Noisy  Labels",
    "descriptor": "",
    "authors": [
      "Erik Englesson",
      "Hossein Azizpour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.04522"
  },
  {
    "id": "arXiv:2105.04544",
    "title": "Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment  Restriction",
    "abstract": "Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment  Restriction",
    "descriptor": "",
    "authors": [
      "Afsaneh Mastouri",
      "Yuchen Zhu",
      "Limor Gultchin",
      "Anna Korba",
      "Ricardo Silva",
      "Matt J. Kusner",
      "Arthur Gretton",
      "Krikamol Muandet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.04544"
  },
  {
    "id": "arXiv:2105.06464",
    "title": "DiscoBox: Weakly Supervised Instance Segmentation and Semantic  Correspondence from Box Supervision",
    "abstract": "Comments: Tech Report",
    "descriptor": "\nComments: Tech Report\n",
    "authors": [
      "Shiyi Lan",
      "Zhiding Yu",
      "Christopher Choy",
      "Subhashree Radhakrishnan",
      "Guilin Liu",
      "Yuke Zhu",
      "Larry S. Davis",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.06464"
  },
  {
    "id": "arXiv:2105.07253",
    "title": "Regret Minimization Experience Replay",
    "abstract": "Regret Minimization Experience Replay",
    "descriptor": "",
    "authors": [
      "Zhenghai Xue",
      "Xu-Hui Liu",
      "Jing-Cheng Pang",
      "Shengyi Jiang",
      "Feng Xu",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.07253"
  },
  {
    "id": "arXiv:2105.07260",
    "title": "The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with  Exponential Noise",
    "abstract": "The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with  Exponential Noise",
    "descriptor": "",
    "authors": [
      "Zeyu Ding",
      "Daniel Kifer",
      "Sayed M. Saghaian N. E.",
      "Thomas Steinke",
      "Yuxin Wang",
      "Yingtai Xiao",
      "Danfeng Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.07260"
  },
  {
    "id": "arXiv:2105.07454",
    "title": "A Synchronized Action Framework for Responsible Detection of  Coordination on Social Media",
    "abstract": "A Synchronized Action Framework for Responsible Detection of  Coordination on Social Media",
    "descriptor": "",
    "authors": [
      "Thomas Magelinski",
      "Lynnette Hui Xian Ng",
      "Kathleen M. Carley"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2105.07454"
  },
  {
    "id": "arXiv:2105.09291",
    "title": "Deciding FO2 Alternation for Automata over Finite and Infinite Words",
    "abstract": "Deciding FO2 Alternation for Automata over Finite and Infinite Words",
    "descriptor": "",
    "authors": [
      "Viktor Henriksson",
      "Manfred Kufleitner"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2105.09291"
  },
  {
    "id": "arXiv:2105.09926",
    "title": "Diversity, Fairness, and Sustainability in Population Protocols",
    "abstract": "Diversity, Fairness, and Sustainability in Population Protocols",
    "descriptor": "",
    "authors": [
      "Nan Kang",
      "Frederik Mallmann-Trenn",
      "Nicol\u00e1s Rivera"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2105.09926"
  },
  {
    "id": "arXiv:2105.10110",
    "title": "Guidance and Teaching Network for Video Salient Object Detection",
    "abstract": "Comments: Accepted at IEEE ICIP 2021",
    "descriptor": "\nComments: Accepted at IEEE ICIP 2021\n",
    "authors": [
      "Yingxia Jiao",
      "Xiao Wang",
      "Yu-Cheng Chou",
      "Shouyuan Yang",
      "Ge-Peng Ji",
      "Rong Zhu",
      "Ge Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.10110"
  },
  {
    "id": "arXiv:2105.10466",
    "title": "Setting Out a Software Stack Capable of Hosting a Virtual ROS-based  Competition",
    "abstract": "Comments: The article is being currently withdrawn owing to a crucial error made whilst setting up the experimentation and due to presence of some broken dependencies within the stack",
    "descriptor": "\nComments: The article is being currently withdrawn owing to a crucial error made whilst setting up the experimentation and due to presence of some broken dependencies within the stack\n",
    "authors": [
      "Nishesh Singh"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2105.10466"
  },
  {
    "id": "arXiv:2105.10488",
    "title": "Understanding the Performance of Knowledge Graph Embeddings in Drug  Discovery",
    "abstract": "Understanding the Performance of Knowledge Graph Embeddings in Drug  Discovery",
    "descriptor": "",
    "authors": [
      "Stephen Bonner",
      "Ian P Barrett",
      "Cheng Ye",
      "Rowan Swiers",
      "Ola Engkvist",
      "Charles Tapley Hoyt",
      "William L Hamilton"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.10488"
  },
  {
    "id": "arXiv:2105.10731",
    "title": "A systematic review of physical-digital play technology and  developmentally relevant child behaviour",
    "abstract": "Comments: 11 Tables, 1 Figure, 4 Appendices; Keywords: Systematic review, digital play, child development, child behaviour, child-computer interactions *Corresponding author info: Faculty of Education, University of Cambridge. Email: pelt2@cam.ac.uk; torresp.uk@gmail.com",
    "descriptor": "\nComments: 11 Tables, 1 Figure, 4 Appendices; Keywords: Systematic review, digital play, child development, child behaviour, child-computer interactions *Corresponding author info: Faculty of Education, University of Cambridge. Email: pelt2@cam.ac.uk; torresp.uk@gmail.com\n",
    "authors": [
      "Pablo E. Torres",
      "Philip I. N. Ulrich",
      "Veronica Cucuiat",
      "Mutlu Cukurova",
      "Maria Fercovic De la Presa",
      "Rose Luckin",
      "Amanda Carr",
      "Thomas Dylan",
      "Abigail Durrant",
      "John Vines",
      "Shaun Lawson"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2105.10731"
  },
  {
    "id": "arXiv:2105.11152",
    "title": "Dynamic Hawkes Processes for Discovering Time-evolving Communities'  States behind Diffusion Processes",
    "abstract": "Comments: 11 pages, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21)",
    "descriptor": "\nComments: 11 pages, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21)\n",
    "authors": [
      "Maya Okawa",
      "Tomoharu Iwata",
      "Yusuke Tanaka",
      "Hiroyuki Toda",
      "Takeshi Kurashima",
      "Hisashi Kashima"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.11152"
  },
  {
    "id": "arXiv:2105.11601",
    "title": "Personalized Transformer for Explainable Recommendation",
    "abstract": "Comments: Published as a conference paper at ACL-IJCNLP 2021",
    "descriptor": "\nComments: Published as a conference paper at ACL-IJCNLP 2021\n",
    "authors": [
      "Lei Li",
      "Yongfeng Zhang",
      "Li Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.11601"
  },
  {
    "id": "arXiv:2105.12254",
    "title": "Interpretable UAV Collision Avoidance using Deep Reinforcement Learning",
    "abstract": "Interpretable UAV Collision Avoidance using Deep Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Deepak-George Thomas",
      "Daniil Olshanskyi",
      "Karter Krueger",
      "Tichakorn Wongpiromsarn",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.12254"
  },
  {
    "id": "arXiv:2105.12883",
    "title": "i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent  Environmental Conditions",
    "abstract": "Comments: 8 Pages, 8 Figures, Accepted Robotics: Science and Systems 2021 paper",
    "descriptor": "\nComments: 8 Pages, 8 Figures, Accepted Robotics: Science and Systems 2021 paper\n",
    "authors": [
      "Peng Yin",
      "Lingyun Xu",
      "Ji Zhang",
      "Howie Choset",
      "Sebastian Scherer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2105.12883"
  },
  {
    "id": "arXiv:2105.12885",
    "title": "3D Segmentation Learning from Sparse Annotations and Hierarchical  Descriptors",
    "abstract": "Comments: 8 pages, 7 figures, Accepted in IEEE Robotics and Automation Letters, 2021",
    "descriptor": "\nComments: 8 pages, 7 figures, Accepted in IEEE Robotics and Automation Letters, 2021\n",
    "authors": [
      "Peng Yin",
      "Lingyun Xu",
      "Jianmin Ji",
      "Sebastian Scherer",
      "Howie Choset"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2105.12885"
  },
  {
    "id": "arXiv:2105.12909",
    "title": "Deconditional Downscaling with Gaussian Processes",
    "abstract": "Deconditional Downscaling with Gaussian Processes",
    "descriptor": "",
    "authors": [
      "Siu Lun Chau",
      "Shahine Bouabid",
      "Dino Sejdinovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.12909"
  },
  {
    "id": "arXiv:2105.12973",
    "title": "$H^m$-Conforming Virtual Elements in Arbitrary Dimension",
    "abstract": "Comments: 32 pages, 1 figure",
    "descriptor": "\nComments: 32 pages, 1 figure\n",
    "authors": [
      "Xuehai Huang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.12973"
  },
  {
    "id": "arXiv:2105.13016",
    "title": "Stylizing 3D Scene via Implicit Representation and HyperNetwork",
    "abstract": "Comments: Project page: this https URL; typos corrected, Figure11, 12 revised",
    "descriptor": "\nComments: Project page: this https URL; typos corrected, Figure11, 12 revised\n",
    "authors": [
      "Pei-Ze Chiang",
      "Meng-Shiun Tsai",
      "Hung-Yu Tseng",
      "Wei-sheng Lai",
      "Wei-Chen Chiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.13016"
  },
  {
    "id": "arXiv:2105.13228",
    "title": "Optimization Induced Equilibrium Networks",
    "abstract": "Optimization Induced Equilibrium Networks",
    "descriptor": "",
    "authors": [
      "Xingyu Xie",
      "Qiuhao Wang",
      "Zenan Ling",
      "Xia Li",
      "Yisen Wang",
      "Guangcan Liu",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2105.13228"
  },
  {
    "id": "arXiv:2105.13280",
    "title": "Coarse-Grid Selection Using Simulated Annealing",
    "abstract": "Comments: 22 pages, 12 figures",
    "descriptor": "\nComments: 22 pages, 12 figures\n",
    "authors": [
      "Tareq. U. Zaman",
      "Scott P. MacLachlan",
      "Luke N. Olson",
      "Matt West"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.13280"
  },
  {
    "id": "arXiv:2105.13553",
    "title": "Autonomous Optimization of Fluid Systems at Varying Length Scales",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Alexander E. Siemenn",
      "Evyatar Shaulsky",
      "Matthew Beveridge",
      "Tonio Buonassisi",
      "Sara M. Hashmi",
      "Iddo Drori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2105.13553"
  },
  {
    "id": "arXiv:2105.13677",
    "title": "ResT: An Efficient Transformer for Visual Recognition",
    "abstract": "Comments: ResT is an efficient multi-scale vision Transformer that can tackle input images with arbitrary size. arXiv admin note: text overlap with arXiv:2103.14030 by other authors",
    "descriptor": "\nComments: ResT is an efficient multi-scale vision Transformer that can tackle input images with arbitrary size. arXiv admin note: text overlap with arXiv:2103.14030 by other authors\n",
    "authors": [
      "Qinglong Zhang",
      "Yubin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.13677"
  },
  {
    "id": "arXiv:2105.13678",
    "title": "An efficient hybrid hash based privacy amplification algorithm for  quantum key distribution",
    "abstract": "Comments: 14 pages, 4 figures",
    "descriptor": "\nComments: 14 pages, 4 figures\n",
    "authors": [
      "Yan Bingze",
      "Li Qiong",
      "Mao Haokun",
      "Chen Nan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.13678"
  },
  {
    "id": "arXiv:2105.13771",
    "title": "Chromatic and spatial analysis of one-pixel attacks against an image  classifier",
    "abstract": "Chromatic and spatial analysis of one-pixel attacks against an image  classifier",
    "descriptor": "",
    "authors": [
      "Janne Alatalo",
      "Joni Korpihalkola",
      "Tuomo Sipola",
      "Tero Kokkonen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.13771"
  },
  {
    "id": "arXiv:2105.14083",
    "title": "Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial  Awareness",
    "abstract": "Comments: 9 pages, 3 figures, 3 algorithms. Currently under blind review at NeurIPS 2021",
    "descriptor": "\nComments: 9 pages, 3 figures, 3 algorithms. Currently under blind review at NeurIPS 2021\n",
    "authors": [
      "Glenn Dawson",
      "Robi Polikar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2105.14083"
  },
  {
    "id": "arXiv:2105.14167",
    "title": "NeuralLog: Natural Language Inference with Joint Neural and Logical  Reasoning",
    "abstract": "Comments: 8 pages, 4 figures, The 10th Joint Conference on Lexical and Computational Semantics (*SEM2021) @ ACL2021",
    "descriptor": "\nComments: 8 pages, 4 figures, The 10th Joint Conference on Lexical and Computational Semantics (*SEM2021) @ ACL2021\n",
    "authors": [
      "Zeming Chen",
      "Qiyue Gao",
      "Lawrence S. Moss"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.14167"
  },
  {
    "id": "arXiv:2105.14217",
    "title": "Less is More: Pay Less Attention in Vision Transformers",
    "abstract": "Comments: 12 pages, 3 figures",
    "descriptor": "\nComments: 12 pages, 3 figures\n",
    "authors": [
      "Zizheng Pan",
      "Bohan Zhuang",
      "Haoyu He",
      "Jing Liu",
      "Jianfei Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.14217"
  },
  {
    "id": "arXiv:2105.14363",
    "title": "On the Theory of Reinforcement Learning with Once-per-Episode Feedback",
    "abstract": "On the Theory of Reinforcement Learning with Once-per-Episode Feedback",
    "descriptor": "",
    "authors": [
      "Niladri S. Chatterji",
      "Aldo Pacchiano",
      "Peter L. Bartlett",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.14363"
  },
  {
    "id": "arXiv:2105.14565",
    "title": "SPI: Automated Identification of Security Patches via Commits",
    "abstract": "Comments: Accepted By ACM Transactions on Software Engineering and Methodology (TOSEM), Continuous Special Section: AI and SE",
    "descriptor": "\nComments: Accepted By ACM Transactions on Software Engineering and Methodology (TOSEM), Continuous Special Section: AI and SE\n",
    "authors": [
      "Yaqin Zhou",
      "Jing Kai Siow",
      "Chenyu Wang",
      "Shangqing Liu",
      "Yang Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2105.14565"
  },
  {
    "id": "arXiv:2105.14655",
    "title": "UNiTE: Unitary N-body Tensor Equivariant Network with Applications to  Quantum Chemistry",
    "abstract": "UNiTE: Unitary N-body Tensor Equivariant Network with Applications to  Quantum Chemistry",
    "descriptor": "",
    "authors": [
      "Zhuoran Qiao",
      "Anders S. Christensen",
      "Matthew Welborn",
      "Frederick R. Manby",
      "Anima Anandkumar",
      "Thomas F. Miller III"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2105.14655"
  },
  {
    "id": "arXiv:2105.14964",
    "title": "A Capacity Region Outer Bound for the Two-User Dispersive Nonlinear  Fiber Optical Channel",
    "abstract": "Comments: Corrected typos",
    "descriptor": "\nComments: Corrected typos\n",
    "authors": [
      "Viswanathan Ramachandran",
      "Astrid Barreiro",
      "Gabriele Liga",
      "Alex Alvarado"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2105.14964"
  },
  {
    "id": "arXiv:2105.15076",
    "title": "Large-Scale Spatio-Temporal Person Re-identification: Algorithm and  Benchmark",
    "abstract": "Large-Scale Spatio-Temporal Person Re-identification: Algorithm and  Benchmark",
    "descriptor": "",
    "authors": [
      "Xiujun Shu",
      "Xiao Wang",
      "Shiliang Zhang",
      "Xianghao Zhang",
      "Yuanqi Chen",
      "Ge Li",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.15076"
  },
  {
    "id": "arXiv:2105.15203",
    "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with  Transformers",
    "abstract": "Comments: Tech Report",
    "descriptor": "\nComments: Tech Report\n",
    "authors": [
      "Enze Xie",
      "Wenhai Wang",
      "Zhiding Yu",
      "Anima Anandkumar",
      "Jose M. Alvarez",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.15203"
  },
  {
    "id": "arXiv:2106.00106",
    "title": "Anti-Koopmanism",
    "abstract": "Anti-Koopmanism",
    "descriptor": "",
    "authors": [
      "Efrain Gonzalez",
      "Moad Abudia",
      "Michael Jury",
      "Rushikesh Kamalapurkar",
      "Joel A. Rosenfeld"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.00106"
  },
  {
    "id": "arXiv:2106.00133",
    "title": "AppBuddy: Learning to Accomplish Tasks in Mobile Apps via Reinforcement  Learning",
    "abstract": "AppBuddy: Learning to Accomplish Tasks in Mobile Apps via Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Maayan Shvo",
      "Zhiming Hu",
      "Rodrigo Toro Icarte",
      "Iqbal Mohomed",
      "Allan Jepson",
      "Sheila A. McIlraith"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.00133"
  },
  {
    "id": "arXiv:2106.00399",
    "title": "Computing Least and Greatest Fixed Points in Absorptive Semirings",
    "abstract": "Comments: submitted to RAMiCS, full version",
    "descriptor": "\nComments: submitted to RAMiCS, full version\n",
    "authors": [
      "Matthias Naaf"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.00399"
  },
  {
    "id": "arXiv:2106.00421",
    "title": "OpenBox: A Generalized Black-box Optimization Service",
    "abstract": "OpenBox: A Generalized Black-box Optimization Service",
    "descriptor": "",
    "authors": [
      "Yang Li",
      "Yu Shen",
      "Wentao Zhang",
      "Yuanwei Chen",
      "Huaijun Jiang",
      "Mingchao Liu",
      "Jiawei Jiang",
      "Jinyang Gao",
      "Wentao Wu",
      "Zhi Yang",
      "Ce Zhang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.00421"
  },
  {
    "id": "arXiv:2106.00444",
    "title": "Minimax Regret for Bandit Convex Optimisation of Ridge Functions",
    "abstract": "Comments: Correcting an (instructive) error that leads to a weaker result",
    "descriptor": "\nComments: Correcting an (instructive) error that leads to a weaker result\n",
    "authors": [
      "Tor Lattimore"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.00444"
  },
  {
    "id": "arXiv:2106.00459",
    "title": "KGPool: Dynamic Knowledge Graph Context Selection for Relation  Extraction",
    "abstract": "Comments: ACL 2021 (findings)",
    "descriptor": "\nComments: ACL 2021 (findings)\n",
    "authors": [
      "Abhishek Nadgeri",
      "Anson Bastos",
      "Kuldeep Singh",
      "Isaiah Onando Mulang'",
      "Johannes Hoffart",
      "Saeedeh Shekarpour",
      "Vijay Saraswat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.00459"
  },
  {
    "id": "arXiv:2106.00526",
    "title": "A Compression-Compilation Framework for On-mobile Real-time BERT  Applications",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2009.06823",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2009.06823\n",
    "authors": [
      "Wei Niu",
      "Zhenglun Kong",
      "Geng Yuan",
      "Weiwen Jiang",
      "Jiexiong Guan",
      "Caiwen Ding",
      "Pu Zhao",
      "Sijia Liu",
      "Bin Ren",
      "Yanzhi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.00526"
  },
  {
    "id": "arXiv:2106.00590",
    "title": "NewsEmbed: Modeling News through Pre-trained Document Representations",
    "abstract": "Comments: Accepted in SIGKDD 2021",
    "descriptor": "\nComments: Accepted in SIGKDD 2021\n",
    "authors": [
      "Jialu Liu",
      "Tianqi Liu",
      "Cong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.00590"
  },
  {
    "id": "arXiv:2106.00596",
    "title": "Look Wide and Interpret Twice: Improving Performance on Interactive  Instruction-following Tasks",
    "abstract": "Comments: To appear in IJCAI2021. 8-page main paper and Appendix following. Appendix E for details of entry submission to EAI 2021. Github: this https URL",
    "descriptor": "\nComments: To appear in IJCAI2021. 8-page main paper and Appendix following. Appendix E for details of entry submission to EAI 2021. Github: this https URL\n",
    "authors": [
      "Van-Quang Nguyen",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.00596"
  },
  {
    "id": "arXiv:2106.00639",
    "title": "Multi-modal Point-of-Care Diagnostics for COVID-19 Based On Acoustics  and Symptoms",
    "abstract": "Comments: The Manuscript is submitted to IEEE-EMBS Journal of Biomedical and Health Informatics on June 1, 2021",
    "descriptor": "\nComments: The Manuscript is submitted to IEEE-EMBS Journal of Biomedical and Health Informatics on June 1, 2021\n",
    "authors": [
      "Srikanth Raj Chetupalli",
      "Prashant Krishnan",
      "Neeraj Sharma",
      "Ananya Muguli",
      "Rohit Kumar",
      "Viral Nanda",
      "Lancelot Mark Pinto",
      "Prasanta Kumar Ghosh",
      "Sriram Ganapathy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.00639"
  },
  {
    "id": "arXiv:2106.00641",
    "title": "SpanNER: Named Entity Re-/Recognition as Span Prediction",
    "abstract": "Comments: Accepted by ACL 2021 (Main track)",
    "descriptor": "\nComments: Accepted by ACL 2021 (Main track)\n",
    "authors": [
      "Jinlan Fu",
      "Xuanjing Huang",
      "Pengfei Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.00641"
  },
  {
    "id": "arXiv:2106.00761",
    "title": "Motif Prediction with Graph Neural Networks",
    "abstract": "Motif Prediction with Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Maciej Besta",
      "Raphael Grob",
      "Cesare Miglioli",
      "Nicola Bernold",
      "Grzegorz Kwasniewski",
      "Gabriel Gjini",
      "Raghavendra Kanakagiri",
      "Saleh Ashkboos",
      "Lukas Gianinazzi",
      "Nikoli Dryden",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.00761"
  },
  {
    "id": "arXiv:2106.00795",
    "title": "Classification of MIMO Equalizers",
    "abstract": "Comments: This work was submitted to ECOC 2021. This theoretical paper also explains the principle of the experimental demonstration in Joint Transmitter and Receiver IQ Differential Phase Calibration using a single 4x8 MIMO Equalizer, Proc. Advanced Photonics Congress 2021 (SPPCom), SpTh1D.4",
    "descriptor": "\nComments: This work was submitted to ECOC 2021. This theoretical paper also explains the principle of the experimental demonstration in Joint Transmitter and Receiver IQ Differential Phase Calibration using a single 4x8 MIMO Equalizer, Proc. Advanced Photonics Congress 2021 (SPPCom), SpTh1D.4\n",
    "authors": [
      "Wing Chau Ng",
      "Chuandong Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.00795"
  },
  {
    "id": "arXiv:2106.00808",
    "title": "Invariant Policy Learning: A Causal Perspective",
    "abstract": "Invariant Policy Learning: A Causal Perspective",
    "descriptor": "",
    "authors": [
      "Sorawit Saengkyongam",
      "Nikolaj Thams",
      "Jonas Peters",
      "Niklas Pfister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00808"
  },
  {
    "id": "arXiv:2106.01016",
    "title": "Deep Reinforcement Learning-based UAV Navigation and Control: A Soft  Actor-Critic with Hindsight Experience Replay Approach",
    "abstract": "Comments: 12 page, 9 figures",
    "descriptor": "\nComments: 12 page, 9 figures\n",
    "authors": [
      "Myoung Hoon Lee",
      "Jun Moon"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.01016"
  },
  {
    "id": "arXiv:2106.01084",
    "title": "Asymptotic Characterisation of Regularised Zero-Forcing Receiver for  Imperfect and Correlated Massive MIMO Systems with Optimal Power Allocation",
    "abstract": "Asymptotic Characterisation of Regularised Zero-Forcing Receiver for  Imperfect and Correlated Massive MIMO Systems with Optimal Power Allocation",
    "descriptor": "",
    "authors": [
      "Ayed M. Alrashdi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.01084"
  },
  {
    "id": "arXiv:2106.01093",
    "title": "LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and  Non-Local Relations",
    "abstract": "Comments: 15 pages, 8 figures, accepted to ACL 2021 main conference",
    "descriptor": "\nComments: 15 pages, 8 figures, accepted to ACL 2021 main conference\n",
    "authors": [
      "Ruisheng Cao",
      "Lu Chen",
      "Zhi Chen",
      "Yanbin Zhao",
      "Su Zhu",
      "Kai Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.01093"
  },
  {
    "id": "arXiv:2106.01112",
    "title": "DynaEval: Unifying Turn and Dialogue Level Evaluation",
    "abstract": "Comments: ACL-IJCNLP 2021 (Main conference, Long paper)",
    "descriptor": "\nComments: ACL-IJCNLP 2021 (Main conference, Long paper)\n",
    "authors": [
      "Chen Zhang",
      "Yiming Chen",
      "Luis Fernando D'Haro",
      "Yan Zhang",
      "Thomas Friedrichs",
      "Grandee Lee",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.01112"
  },
  {
    "id": "arXiv:2106.01124",
    "title": "Opening the Black Box of Deep Neural Networks in Physical Layer  Communication",
    "abstract": "Comments: 5 pages, 5 figures, submitted to IEEE Wireless Communications Letters",
    "descriptor": "\nComments: 5 pages, 5 figures, submitted to IEEE Wireless Communications Letters\n",
    "authors": [
      "Jun Liu",
      "Kai Mei",
      "Dongtang Ma",
      "Jibo Wei"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01124"
  },
  {
    "id": "arXiv:2106.01271",
    "title": "Deep learning-based multi-output quantile forecasting of PV generation",
    "abstract": "Deep learning-based multi-output quantile forecasting of PV generation",
    "descriptor": "",
    "authors": [
      "Jonathan Dumas",
      "Colin Cointe",
      "Xavier Fettweis",
      "Bertrand Corn\u00e9lusse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.01271"
  },
  {
    "id": "arXiv:2106.01352",
    "title": "NeRP: Neural Rearrangement Planning for Unknown Objects",
    "abstract": "Comments: Please refer to our supplementary video: this https URL",
    "descriptor": "\nComments: Please refer to our supplementary video: this https URL\n",
    "authors": [
      "Ahmed H. Qureshi",
      "Arsalan Mousavian",
      "Chris Paxton",
      "Michael C. Yip",
      "Dieter Fox"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01352"
  },
  {
    "id": "arXiv:2106.01438",
    "title": "A Leader-Follower Game Theoretic Approach to Arrest Cascading Failure in  Smart Grid",
    "abstract": "Comments: This paper is accepted for publication in American Journal of Science and Engineering. arXiv admin note: text overlap with arXiv:2101.08896",
    "descriptor": "\nComments: This paper is accepted for publication in American Journal of Science and Engineering. arXiv admin note: text overlap with arXiv:2101.08896\n",
    "authors": [
      "Sohini Roy",
      "Arunabha Sen"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.01438"
  },
  {
    "id": "arXiv:2106.01680",
    "title": "Convergent Graph Solvers",
    "abstract": "Comments: 12 pages, 5 figures",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Junyoung Park",
      "Jinhyun Choo",
      "Jinkyoo Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.01680"
  },
  {
    "id": "arXiv:2106.01682",
    "title": "Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic  Regression",
    "abstract": "Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic  Regression",
    "descriptor": "",
    "authors": [
      "Olivier Sprangers",
      "Sebastian Schelter",
      "Maarten de Rijke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.01682"
  },
  {
    "id": "arXiv:2106.01894",
    "title": "Low-Congestion Shortcuts in Constant Diameter Graphs",
    "abstract": "Comments: To appear in PODC 2021",
    "descriptor": "\nComments: To appear in PODC 2021\n",
    "authors": [
      "Shimon Kogan",
      "Merav Parter"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.01894"
  },
  {
    "id": "arXiv:2106.02024",
    "title": "Combinatorial Algorithms for Matching Markets via Nash Bargaining:  One-Sided, Two-Sided and Non-Bipartite",
    "abstract": "Comments: 51 pages",
    "descriptor": "\nComments: 51 pages\n",
    "authors": [
      "Ioannis Panageas",
      "Thorben Tr\u00f6bst",
      "Vijay V. Vazirani"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2106.02024"
  },
  {
    "id": "arXiv:2106.02078",
    "title": "Robust Learning via Persistency of Excitation",
    "abstract": "Robust Learning via Persistency of Excitation",
    "descriptor": "",
    "authors": [
      "Kaustubh Sridhar",
      "Oleg Sokolsky",
      "Insup Lee",
      "James Weimer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02078"
  },
  {
    "id": "arXiv:2106.02096",
    "title": "Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of  Topological Equivalence",
    "abstract": "Comments: 18 pages, 2 figures",
    "descriptor": "\nComments: 18 pages, 2 figures\n",
    "authors": [
      "Byeongsu Yu",
      "Kisung You"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02096"
  },
  {
    "id": "arXiv:2106.02116",
    "title": "Rotor Thermal Monitoring Scheme for Direct-Torque-Controlled Interior  Permanent Magnet Synchronous Machines via High-Frequency Rotating Flux or  Torque Injection",
    "abstract": "Rotor Thermal Monitoring Scheme for Direct-Torque-Controlled Interior  Permanent Magnet Synchronous Machines via High-Frequency Rotating Flux or  Torque Injection",
    "descriptor": "",
    "authors": [
      "Shen Zhang",
      "Sufei Li",
      "Lijun He",
      "Jose A. Restrepo",
      "Thomas G. Habetler"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02116"
  },
  {
    "id": "arXiv:2106.02118",
    "title": "A Prospective Observational Study to Investigate Performance of a Chest  X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S.  Hospitals",
    "abstract": "Comments: Check out the medRxiv version at this https URL for updates",
    "descriptor": "\nComments: Check out the medRxiv version at this https URL for updates\n",
    "authors": [
      "Ju Sun",
      "Le Peng",
      "Taihui Li",
      "Dyah Adila",
      "Zach Zaiman",
      "Genevieve B. Melton",
      "Nicholas Ingraham",
      "Eric Murray",
      "Daniel Boley",
      "Sean Switzer",
      "John L. Burns",
      "Kun Huang",
      "Tadashi Allen",
      "Scott D. Steenburg",
      "Judy Wawira Gichoya",
      "Erich Kummerfeld",
      "Christopher Tignanelli"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02118"
  },
  {
    "id": "arXiv:2106.02225",
    "title": "Materials Representation and Transfer Learning for Multi-Property  Prediction",
    "abstract": "Comments: The is accepted at the Applied Physics Reviews journal",
    "descriptor": "\nComments: The is accepted at the Applied Physics Reviews journal\n",
    "authors": [
      "Shufeng Kong",
      "Dan Guevarra",
      "Carla P. Gomes",
      "John M. Gregoire"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.02225"
  },
  {
    "id": "arXiv:2106.02253",
    "title": "X-volution: On the unification of convolution and self-attention",
    "abstract": "X-volution: On the unification of convolution and self-attention",
    "descriptor": "",
    "authors": [
      "Xuanhong Chen",
      "Hang Wang",
      "Bingbing Ni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02253"
  },
  {
    "id": "arXiv:2106.02262",
    "title": "Envy-free division of multi-layered cakes",
    "abstract": "Comments: 18 pages",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Ayumi Igarashi",
      "Fr\u00e9d\u00e9ric Meunier"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.02262"
  },
  {
    "id": "arXiv:2106.02300",
    "title": "AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial  Discriminator for Cross-Lingual NER",
    "abstract": "Comments: This paper has been accepted at ACL-IJCNLP 2021",
    "descriptor": "\nComments: This paper has been accepted at ACL-IJCNLP 2021\n",
    "authors": [
      "Weile Chen",
      "Huiqiang Jiang",
      "Qianhui Wu",
      "B\u00f6rje F. Karlsson",
      "Yi Guan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.02300"
  },
  {
    "id": "arXiv:2106.02377",
    "title": "A Survey on Deep Domain Adaptation for LiDAR Perception",
    "abstract": "Comments: Accepted at IEEE Intelligent Vehicles Symposium (IV) 2021 Workshop on Autonomy at Scale. 8 pages, 5 figures",
    "descriptor": "\nComments: Accepted at IEEE Intelligent Vehicles Symposium (IV) 2021 Workshop on Autonomy at Scale. 8 pages, 5 figures\n",
    "authors": [
      "Larissa T. Triess",
      "Mariella Dreissig",
      "Christoph B. Rist",
      "J. Marius Z\u00f6llner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02377"
  },
  {
    "id": "arXiv:2106.02566",
    "title": "Improve the Interpretability of Attention: A Fast, Accurate, and  Interpretable High-Resolution Attention Model",
    "abstract": "Improve the Interpretability of Attention: A Fast, Accurate, and  Interpretable High-Resolution Attention Model",
    "descriptor": "",
    "authors": [
      "Tristan Gomez",
      "Suiyi Ling",
      "Thomas Fr\u00e9our",
      "Harold Mouch\u00e8re"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02566"
  },
  {
    "id": "arXiv:2106.02575",
    "title": "Optimal Rates of (Locally) Differentially Private Heavy-tailed  Multi-Armed Bandits",
    "abstract": "Optimal Rates of (Locally) Differentially Private Heavy-tailed  Multi-Armed Bandits",
    "descriptor": "",
    "authors": [
      "Youming Tao",
      "Yulian Wu",
      "Peng Zhao",
      "Di Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02575"
  },
  {
    "id": "arXiv:2106.02594",
    "title": "Self-Supervised Learning of Domain Invariant Features for Depth  Estimation",
    "abstract": "Comments: 16 pages",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Hiroyasu Akada",
      "Shariq Farooq Bhat",
      "Ibraheem Alhashim",
      "Peter Wonka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02594"
  },
  {
    "id": "arXiv:2106.02623",
    "title": "The Closer You Look, The More You Learn: A Grey-box Approach to Protocol  State Machine Learning",
    "abstract": "The Closer You Look, The More You Learn: A Grey-box Approach to Protocol  State Machine Learning",
    "descriptor": "",
    "authors": [
      "Chris McMahon Stone",
      "Sam L. Thomas",
      "Mathy Vanhoef",
      "James Henderson",
      "Nicolas Bailluet",
      "Tom Chothia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02623"
  }
]