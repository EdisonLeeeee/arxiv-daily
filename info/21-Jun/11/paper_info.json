[
  {
    "id": "arXiv:2106.05268",
    "title": "Vector Symbolic Architectures as a Computing Framework for Nanoscale  Hardware",
    "abstract": "This article reviews recent progress in the development of the computing\nframework Vector Symbolic Architectures (also known as Hyperdimensional\nComputing). This framework is well suited for implementation in stochastic,\nnanoscale hardware and it naturally expresses the types of cognitive operations\nrequired for Artificial Intelligence (AI). We demonstrate in this article that\nthe ring-like algebraic structure of Vector Symbolic Architectures offers\nsimple but powerful operations on high-dimensional vectors that can support all\ndata structures and manipulations relevant in modern computing. In addition, we\nillustrate the distinguishing feature of Vector Symbolic Architectures,\n\"computing in superposition,\" which sets it apart from conventional computing.\nThis latter property opens the door to efficient solutions to the difficult\ncombinatorial search problems inherent in AI applications. Vector Symbolic\nArchitectures are Turing complete, as we show, and we see them acting as a\nframework for computing with distributed representations in myriad AI settings.\nThis paper serves as a reference for computer architects by illustrating\ntechniques and philosophy of VSAs for distributed computing and relevance to\nemerging computing hardware, such as neuromorphic computing.",
    "descriptor": "\nComments: 28 pages, 15 figures, 1 Table\n",
    "authors": [
      "Denis Kleyko",
      "Mike Davies",
      "E. Paxon Frady",
      "Pentti Kanerva",
      "Spencer J. Kent",
      "Bruno A. Olshausen",
      "Evgeny Osipov",
      "Jan M. Rabaey",
      "Dmitri A. Rachkovskij",
      "Abbas Rahimi",
      "Friedrich T. Sommer"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05268"
  },
  {
    "id": "arXiv:2106.05303",
    "title": "Explaining Time Series Predictions with Dynamic Masks",
    "abstract": "How can we explain the predictions of a machine learning model? When the data\nis structured as a multivariate time series, this question induces additional\ndifficulties such as the necessity for the explanation to embody the time\ndependency and the large number of inputs. To address these challenges, we\npropose dynamic masks (Dynamask). This method produces instance-wise importance\nscores for each feature at each time step by fitting a perturbation mask to the\ninput sequence. In order to incorporate the time dependency of the data,\nDynamask studies the effects of dynamic perturbation operators. In order to\ntackle the large number of inputs, we propose a scheme to make the feature\nselection parsimonious (to select no more feature than necessary) and legible\n(a notion that we detail by making a parallel with information theory). With\nsynthetic and real-world data, we demonstrate that the dynamic underpinning of\nDynamask, together with its parsimony, offer a neat improvement in the\nidentification of feature importance over time. The modularity of Dynamask\nmakes it ideal as a plug-in to increase the transparency of a wide range of\nmachine learning models in areas such as medicine and finance, where time\nseries are abundant.",
    "descriptor": "\nComments: Presented at the Thirty-eighth International Conference on Machine Learning (ICML 2021)\n",
    "authors": [
      "Jonathan Crabb\u00e9",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05303"
  },
  {
    "id": "arXiv:2106.05304",
    "title": "Revisiting Point Cloud Shape Classification with a Simple and Effective  Baseline",
    "abstract": "Processing point cloud data is an important component of many real-world\nsystems. As such, a wide variety of point-based approaches have been proposed,\nreporting steady benchmark improvements over time. We study the key ingredients\nof this progress and uncover two critical results. First, we find that\nauxiliary factors like different evaluation schemes, data augmentation\nstrategies, and loss functions, which are independent of the model\narchitecture, make a large difference in performance. The differences are large\nenough that they obscure the effect of architecture. When these factors are\ncontrolled for, PointNet++, a relatively older network, performs competitively\nwith recent methods. Second, a very simple projection-based method, which we\nrefer to as SimpleView, performs surprisingly well. It achieves on par or\nbetter results than sophisticated state-of-the-art methods on ModelNet40 while\nbeing half the size of PointNet++. It also outperforms state-of-the-art methods\non ScanObjectNN, a real-world point cloud benchmark, and demonstrates better\ncross-dataset generalization. Code is available at\nhttps://github.com/princeton-vl/SimpleView.",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Ankit Goyal",
      "Hei Law",
      "Bowei Liu",
      "Alejandro Newell",
      "Jia Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05304"
  },
  {
    "id": "arXiv:2106.05306",
    "title": "DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact",
    "abstract": "Cloth simulation has wide applications including computer animation, garment\ndesign, and robot-assisted dressing. In this work, we present a differentiable\ncloth simulator whose additional gradient information facilitates cloth-related\napplications. Our differentiable simulator extends the state-of-the-art cloth\nsimulator based on Projective Dynamics and with dry frictional contact governed\nby the Signorini-Coulomb law. We derive gradients with contact in this forward\nsimulation framework and speed up the computation with Jacobi iteration\ninspired by previous differentiable simulation work. To our best knowledge, we\npresent the first differentiable cloth simulator with the Coulomb law of\nfriction. We demonstrate the efficacy of our simulator in various applications,\nincluding system identification, manipulation, inverse design, and a\nreal-to-sim task. Many of our applications have not been demonstrated in\nprevious differentiable cloth simulators. The gradient information from our\nsimulator enables efficient gradient-based task solvers from which we observe a\nsubstantial speedup over standard gradient-free methods.",
    "descriptor": "",
    "authors": [
      "Yifei Li",
      "Tao Du",
      "Kui Wu",
      "Jie Xu",
      "Wojciech Matusik"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05306"
  },
  {
    "id": "arXiv:2106.05308",
    "title": "Visual Sensor Pose Optimisation Using Rendering-based Visibility Models  for Robust Cooperative Perception",
    "abstract": "Visual Sensor Networks can be used in a variety of perception applications\nsuch as infrastructure support for autonomous driving in complex road segments.\nThe pose of the sensors in such networks directly determines the coverage of\nthe environment and objects therein, which impacts the performance of\napplications such as object detection and tracking. Existing sensor pose\noptimisation methods in the literature either maximise the coverage of ground\nsurfaces, or consider the visibility of the target objects as binary variables,\nwhich cannot represent various degrees of visibility. Such formulations cannot\nguarantee the visibility of the target objects as they fail to consider\nocclusions. This paper proposes two novel sensor pose optimisation methods,\nbased on gradient-ascent and Integer Programming techniques, which maximise the\nvisibility of multiple target objects in cluttered environments. Both methods\nconsider a realistic visibility model based on a rendering engine that provides\npixel-level visibility information about the target objects. The proposed\nmethods are evaluated in a complex environment and compared to existing methods\nin the literature. The evaluation results indicate that explicitly modelling\nthe visibility of target objects is critical to avoid occlusions in cluttered\nenvironments. Furthermore, both methods significantly outperform existing\nmethods in terms of object visibility.",
    "descriptor": "\nComments: 15 pages, 10 figures, 4 tables\n",
    "authors": [
      "Eduardo Arnold",
      "Sajjad Mozaffari",
      "Mehrdad Dianati",
      "Paul Jennings"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.05308"
  },
  {
    "id": "arXiv:2106.05316",
    "title": "Raman spectral analysis of mixtures with one-dimensional convolutional  neural network",
    "abstract": "Recently, the combination of robust one-dimensional convolutional neural\nnetworks (1-D CNNs) and Raman spectroscopy has shown great promise in rapid\nidentification of unknown substances with good accuracy. Using this technique,\nresearchers can recognize a pure compound and distinguish it from unknown\nsubstances in a mixture. The novelty of this approach is that the trained\nneural network operates automatically without any pre- or post-processing of\ndata. Some studies have attempted to extend this technique to the\nclassification of pure compounds in an unknown mixture. However, the\napplication of 1-D CNNs has typically been restricted to binary classifications\nof pure compounds. Here we will highlight a new approach in spectral\nrecognition and quantification of chemical components in a multicomponent\nmixture. Two 1-D CNN models, RaMixNet I and II, have been developed for this\npurpose. The former is for rapid classification of components in a mixture\nwhile the latter is for quantitative determination of those constituents. In\nthe proposed method, there is no limit to the number of compounds in a mixture.\nA data augmentation method is also introduced by adding random baselines to the\nRaman spectra. The experimental results revealed that the classification\naccuracy of RaMixNet I and II is 100% for analysis of unknown test mixtures; at\nthe same time, the RaMixNet II model may achieve a regression accuracy of 88%\nfor the quantification of each component.",
    "descriptor": "\nComments: 9 pages, 5 tables, 3 figures\n",
    "authors": [
      "M. Hamed Mozaffari",
      "Li-Lin Tay"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.05316"
  },
  {
    "id": "arXiv:2106.05317",
    "title": "Attentional meta-learners are polythetic classifiers",
    "abstract": "Polythetic classifications, based on shared patterns of features that need\nneither be universal nor constant among members of a class, are common in the\nnatural world and greatly outnumber monothetic classifications over a set of\nfeatures. We show that threshold meta-learners require an embedding dimension\nthat is exponential in the number of features to emulate these functions. In\ncontrast, attentional classifiers are polythetic by default and able to solve\nthese problems with a linear embedding dimension. However, we find that in the\npresence of task-irrelevant features, inherent to meta-learning problems,\nattentional models are susceptible to misclassification. To address this\nchallenge, we further propose a self-attention feature-selection mechanism that\nadaptively dilutes non-discriminative features. We demonstrate the\neffectiveness of our approach in meta-learning Boolean functions, and synthetic\nand real-world few-shot learning tasks.",
    "descriptor": "\nComments: 10 main pages, 11 pages of appendices and additional figures\n",
    "authors": [
      "Ben Day",
      "Ramon Vi\u00f1as",
      "Nikola Simidjievski",
      "Pietro Li\u00f2"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05317"
  },
  {
    "id": "arXiv:2106.05318",
    "title": "Distributed Mean-Field Density Estimation for Large-Scale Systems",
    "abstract": "This work studies how to estimate the mean-field density of large-scale\nsystems in a distributed manner. Such problems are motivated by the recent\nswarm control technique that uses mean-field approximations to represent the\ncollective effect of the swarm, wherein the mean-field density (and its\ngradient) is usually used in feedback control design. In the first part, we\nformulate the density estimation problem as a filtering problem of the\nassociated mean-field partial differential equation (PDE), for which we employ\nkernel density estimation (KDE) to construct noisy observations and use\nfiltering theory of PDE systems to design an optimal (centralized) density\nfilter. It turns out that the covariance operator of observation noise depends\non the unknown density. Hence, we use approximations for the covariance\noperator to obtain a suboptimal density filter, and prove that both the density\nestimates and their gradient are convergent and remain close to the optimal one\nusing the notion of input-to-state stability (ISS). In the second part, we\ncontinue to study how to decentralize the density filter such that each agent\ncan estimate the mean-field density based on only its own position and local\ninformation exchange with neighbors. We prove that the local density filter is\nalso convergent and remains close to the centralized one in the sense of ISS.\nSimulation results suggest that the (centralized) suboptimal density filter is\nable to generate convergent density estimates, and the local density filter is\nable to converge and remain close to the centralized filter.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2009.05366\n",
    "authors": [
      "Tongjia Zheng",
      "Qing Han",
      "Hai Lin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05318"
  },
  {
    "id": "arXiv:2106.05319",
    "title": "Stein Latent Optimization for GANs",
    "abstract": "Generative adversarial networks (GANs) with clustered latent spaces can\nperform conditional generation in a completely unsupervised manner. However,\nthe salient attributes of unlabeled data in the real-world are mostly\nimbalanced. Existing unsupervised conditional GANs cannot properly cluster the\nattributes in their latent spaces because they assume uniform distributions of\nthe attributes. To address this problem, we theoretically derive Stein latent\noptimization that provides reparameterizable gradient estimations of the latent\ndistribution parameters assuming a Gaussian mixture prior in a continuous\nlatent space. Structurally, we introduce an encoder network and a novel\ncontrastive loss to help generated data from a single mixture component to\nrepresent a single attribute. We confirm that the proposed method, named Stein\nLatent Optimization for GANs (SLOGAN), successfully learns the balanced or\nimbalanced attributes and performs unsupervised tasks such as unsupervised\nconditional generation, unconditional generation, and cluster assignment even\nin the absence of information of the attributes (e.g. the imbalance ratio).\nMoreover, we demonstrate that the attributes to be learned can be manipulated\nusing a small amount of probe data.",
    "descriptor": "",
    "authors": [
      "Uiwon Hwang",
      "Heeseung Kim",
      "Dahuin Jung",
      "Hyemi Jang",
      "Hyungyu Lee",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05319"
  },
  {
    "id": "arXiv:2106.05321",
    "title": "Tensor feature hallucination for few-shot learning",
    "abstract": "Few-shot classification addresses the challenge of classifying examples given\nnot just limited supervision but limited data as well. An attractive solution\nis synthetic data generation. However, most such methods are overly\nsophisticated, focusing on high-quality, realistic data in the input space. It\nis unclear whether adapting them to the few-shot regime and using them for the\ndownstream task of classification is the right approach. Previous works on\nsynthetic data generation for few-shot classification focus on exploiting\ncomplex models, e.g. a Wasserstein GAN with multiple regularizers or a network\nthat transfers latent diversities from known to novel classes.\nWe follow a different approach and investigate how a simple and\nstraightforward synthetic data generation method can be used effectively. We\nmake two contributions, namely we show that: (1) using a simple loss function\nis more than enough for training a feature generator in the few-shot setting;\nand (2) learning to generate tensor features instead of vector features is\nsuperior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show\nthat our method sets a new state of the art, outperforming more sophisticated\nfew-shot data augmentation methods.",
    "descriptor": "\nComments: This is an extended work based on arXiv:2104.09467 including new experiments, new experimental settings and further analysis\n",
    "authors": [
      "Michalis Lazarou",
      "Yannis Avrithis",
      "Tania Stathaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05321"
  },
  {
    "id": "arXiv:2106.05325",
    "title": "ZoPE: A Fast Optimizer for ReLU Networks with Low-Dimensional Inputs",
    "abstract": "Deep neural networks often lack the safety and robustness guarantees needed\nto be deployed in safety critical systems. Formal verification techniques can\nbe used to prove input-output safety properties of networks, but when\nproperties are difficult to specify, we rely on the solution to various\noptimization problems. In this work, we present an algorithm called ZoPE that\nsolves optimization problems over the output of feedforward ReLU networks with\nlow-dimensional inputs. The algorithm eagerly splits the input space, bounding\nthe objective using zonotope propagation at each step, and improves\ncomputational efficiency compared to existing mixed integer programming\napproaches. We demonstrate how to formulate and solve three types of\noptimization problems: (i) minimization of any convex function over the output\nspace, (ii) minimization of a convex function over the output of two networks\nin series with an adversarial perturbation in the layer between them, and (iii)\nmaximization of the difference in output between two networks. Using ZoPE, we\nobserve a $25\\times$ speedup on property 1 of the ACAS Xu neural network\nverification benchmark and an $85\\times$ speedup on a set of linear\noptimization problems. We demonstrate the versatility of the optimizer in\nanalyzing networks by projecting onto the range of a generative adversarial\nnetwork and visualizing the differences between a compressed and uncompressed\nnetwork.",
    "descriptor": "\nComments: 9 pages, 3 figures\n",
    "authors": [
      "Christopher A. Strong",
      "Sydney M. Katz",
      "Anthony L. Corso",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05325"
  },
  {
    "id": "arXiv:2106.05326",
    "title": "Solution of Wiener-Hopf and Fredholm integral equations by fast Hilbert  and Fourier transforms",
    "abstract": "We present numerical methods based on the fast Fourier transform (FFT) to\nsolve convolution integral equations on a semi-infinite interval (Wiener-Hopf\nequation) or on a finite interval (Fredholm equation). We extend and improve a\nFFT-based method for the Wiener-Hopf equation due to Henery, expressing it in\nterms of the Hilbert transform, and computing the latter in a more\nsophisticated way with sinc functions. We then generalise our method to the\nFredholm equation reformulating it as two coupled Wiener-Hopf equations and\nsolving them iteratively. We provide numerical tests and open-source code.",
    "descriptor": "\nComments: 30 pages, 19 figures\n",
    "authors": [
      "Guido Germano",
      "Carolyn E Phelan",
      "Daniele Marazzina",
      "Gianluca Fusai"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05326"
  },
  {
    "id": "arXiv:2106.05332",
    "title": "Reinforcement Learning for Industrial Control Network Cyber Security  Orchestration",
    "abstract": "Defending computer networks from cyber attack requires coordinating actions\nacross multiple nodes based on imperfect indicators of compromise while\nminimizing disruptions to network operations. Advanced attacks can progress\nwith few observable signals over several months before execution. The resulting\nsequential decision problem has large observation and action spaces and a long\ntime-horizon, making it difficult to solve with existing methods. In this work,\nwe present techniques to scale deep reinforcement learning to solve the cyber\nsecurity orchestration problem for large industrial control networks. We\npropose a novel attention-based neural architecture with size complexity that\nis invariant to the size of the network under protection. A pre-training\ncurriculum is presented to overcome early exploration difficulty. Experiments\nshow in that the proposed approaches greatly improve both the learning sample\ncomplexity and converged policy performance over baseline methods in\nsimulation.",
    "descriptor": "\nComments: 12 pages, submitted to NeurIPS 2021\n",
    "authors": [
      "John Mern",
      "Kyle Hatch",
      "Ryan Silva",
      "Jeff Brush",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05332"
  },
  {
    "id": "arXiv:2106.05335",
    "title": "Online Learning for Stochastic Shortest Path Model via Posterior  Sampling",
    "abstract": "We consider the problem of online reinforcement learning for the Stochastic\nShortest Path (SSP) problem modeled as an unknown MDP with an absorbing state.\nWe propose PSRL-SSP, a simple posterior sampling-based reinforcement learning\nalgorithm for the SSP problem. The algorithm operates in epochs. At the\nbeginning of each epoch, a sample is drawn from the posterior distribution on\nthe unknown model dynamics, and the optimal policy with respect to the drawn\nsample is followed during that epoch. An epoch completes if either the number\nof visits to the goal state in the current epoch exceeds that of the previous\nepoch, or the number of visits to any of the state-action pairs is doubled. We\nestablish a Bayesian regret bound of $O(B_\\star S\\sqrt{AK})$, where $B_\\star$\nis an upper bound on the expected cost of the optimal policy, $S$ is the size\nof the state space, $A$ is the size of the action space, and $K$ is the number\nof episodes. The algorithm only requires the knowledge of the prior\ndistribution, and has no hyper-parameters to tune. It is the first such\nposterior sampling algorithm and outperforms numerically previously proposed\noptimism-based algorithms.",
    "descriptor": "",
    "authors": [
      "Mehdi Jafarnia-Jahromi",
      "Liyu Chen",
      "Rahul Jain",
      "Haipeng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05335"
  },
  {
    "id": "arXiv:2106.05341",
    "title": "Instantaneous Local Control Barrier Function: An Online Learning  Approach for Collision Avoidance",
    "abstract": "This paper presents a new formulation for provable safety under partial model\nuncertainty with guaranteed performance. A collision-free control strategy is\ndeveloped for an uncertain multi-agent system that navigates through a prior\nunknown environment populated with static and dynamic obstacles. Our novel\ninstantaneous local control barrier functions (IL-CBFs), constructed based on\nnoisy data from limited horizon sensors online, are adopted to characterize\npotential agent-to-obstacle collisions. These data-based IL-CBFs further serve\nas the constraints of a quadratic programming (QP) optimization framework to\ngenerate safe control inputs. The required model information during the QP\noptimization process is identified within a finite time by our proposed\nparameter estimation update law. Numerical simulations based on the reach-avoid\ngame and the formation keeping task are conducted to reveal the effectiveness\nof the proposed collision-free control strategy.",
    "descriptor": "",
    "authors": [
      "Cong Li",
      "Zengjie Zhang",
      "Ahmed Nesrin",
      "Qingchen Liu",
      "Fangzhou Liu",
      "Martin Buss"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05341"
  },
  {
    "id": "arXiv:2106.05345",
    "title": "Cocktail: Leveraging Ensemble Learning for Optimized Model Serving in  Public Cloud",
    "abstract": "With a growing demand for adopting ML models for a varietyof application\nservices, it is vital that the frameworks servingthese models are capable of\ndelivering highly accurate predic-tions with minimal latency along with reduced\ndeploymentcosts in a public cloud environment. Despite high latency,prior works\nin this domain are crucially limited by the accu-racy offered by individual\nmodels. Intuitively, model ensem-bling can address the accuracy gap by\nintelligently combiningdifferent models in parallel. However, selecting the\nappro-priate models dynamically at runtime to meet the desiredaccuracy with low\nlatency at minimal deployment cost is anontrivial problem. Towards this, we\nproposeCocktail, a costeffective ensembling-based model serving\nframework.Cock-tailcomprises of two key components: (i) a dynamic\nmodelselection framework, which reduces the number of modelsin the ensemble,\nwhile satisfying the accuracy and latencyrequirements; (ii) an adaptive\nresource management (RM)framework that employs a distributed proactive\nautoscalingpolicy combined with importance sampling, to efficiently allo-cate\nresources for the models. The RM framework leveragestransient virtual machine\n(VM) instances to reduce the de-ployment cost in a public cloud. A prototype\nimplementationofCocktailon the AWS EC2 platform and exhaustive evalua-tions\nusing a variety of workloads demonstrate thatCocktailcan reduce deployment cost\nby 1.45x, while providing 2xreduction in latency and satisfying the target\naccuracy for upto 96% of the requests, when compared to\nstate-of-the-artmodel-serving frameworks.",
    "descriptor": "\nComments: Accepeted at NSDI' 2022\n",
    "authors": [
      "Jashwant Raj Gunasekaran",
      "Cyan Subhra Mishra",
      "Prashanth Thinakaran",
      "Mahmut Taylan Kandemir",
      "Chita R. Das"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05345"
  },
  {
    "id": "arXiv:2106.05346",
    "title": "End-to-End Training of Multi-Document Reader and Retriever for  Open-Domain Question Answering",
    "abstract": "We present an end-to-end differentiable training method for\nretrieval-augmented open-domain question answering systems that combine\ninformation from multiple retrieved documents when generating answers. We model\nretrieval decisions as latent variables over sets of relevant documents. Since\nmarginalizing over sets of retrieved documents is computationally hard, we\napproximate this using an expectation-maximization algorithm. We iteratively\nestimate the value of our latent variable (the set of relevant documents for a\ngiven question) and then use this estimate to update the retriever and reader\nparameters. We hypothesize that such end-to-end training allows training\nsignals to flow to the reader and then to the retriever better than staged-wise\ntraining. This results in a retriever that is able to select more relevant\ndocuments for a question and a reader that is trained on more accurate\ndocuments to generate an answer. Experiments on three benchmark datasets\ndemonstrate that our proposed method outperforms all existing approaches of\ncomparable size by 2-3% absolute exact match points, achieving new\nstate-of-the-art results. Our results also demonstrate the feasibility of\nlearning to retrieve to improve answer generation without explicit supervision\nof retrieval decisions.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Devendra Singh Sachan",
      "Siva Reddy",
      "William Hamilton",
      "Chris Dyer",
      "Dani Yogatama"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05346"
  },
  {
    "id": "arXiv:2106.05348",
    "title": "SCARI: Separate and Conquer Algorithm for Action Rules and  Recommendations Induction",
    "abstract": "This article describes an action rule induction algorithm based on a\nsequential covering approach. Two variants of the algorithm are presented. The\nalgorithm allows the action rule induction from a source and a target decision\nclass point of view. The application of rule quality measures enables the\ninduction of action rules that meet various quality criteria. The article also\npresents a method for recommendation induction. The recommendations indicate\nthe actions to be taken to move a given test example, representing the source\nclass, to the target one. The recommendation method is based on a set of\ninduced action rules. The experimental part of the article presents the results\nof the algorithm operation on sixteen data sets. As a result of the conducted\nresearch the Ac-Rules package was made available.",
    "descriptor": "\nComments: 47 pages, 6 figures\n",
    "authors": [
      "Marek Sikora",
      "Pawe\u0142 Matyszok",
      "\u0141ukasz Wr\u00f3bel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05348"
  },
  {
    "id": "arXiv:2106.05350",
    "title": "Match What Matters: Generative Implicit Feature Replay for Continual  Learning",
    "abstract": "Neural networks are prone to catastrophic forgetting when trained\nincrementally on different tasks. In order to prevent forgetting, most existing\nmethods retain a small subset of previously seen samples, which in turn can be\nused for joint training with new tasks. While this is indeed effective, it may\nnot always be possible to store such samples, e.g., due to data protection\nregulations. In these cases, one can instead employ generative models to create\nartificial samples or features representing memories from previous tasks.\nFollowing a similar direction, we propose GenIFeR (Generative Implicit Feature\nReplay) for class-incremental learning. The main idea is to train a generative\nadversarial network (GAN) to generate images that contain realistic features.\nWhile the generator creates images at full resolution, the discriminator only\nsees the corresponding features extracted by the continually trained\nclassifier. Since the classifier compresses raw images into features that are\nactually relevant for classification, the GAN can match this target\ndistribution more accurately. On the other hand, allowing the generator to\ncreate full resolution images has several benefits: In contrast to previous\napproaches, the feature extractor of the classifier does not have to be frozen.\nIn addition, we can employ augmentations on generated images, which not only\nboosts classification performance, but also mitigates discriminator overfitting\nduring GAN training. We empirically show that GenIFeR is superior to both\nconventional generative image and feature replay. In particular, we\nsignificantly outperform the state-of-the-art in generative replay for various\nsettings on the CIFAR-100 and CUB-200 datasets.",
    "descriptor": "",
    "authors": [
      "Kevin Thandiackal",
      "Tiziano Portenier",
      "Andrea Giovannini",
      "Maria Gabrani",
      "Orcun Goksel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05350"
  },
  {
    "id": "arXiv:2106.05353",
    "title": "An Upper Bound on the State-Space Complexity of Brandubh",
    "abstract": "Before chess came to Northern Europe there was Tafl, a family of asymmetric\nstrategy board games associated strongly with the Vikings. The purpose of this\npaper is to study the combinatorial state-space complexity of an Irish\nvariation of Tafl called Brandubh. Brandubh was chosen because of its\nasymmetric goals for the two players, but also its overall complexity well\nbelow that of chess, which should make it tractable for strong solving.\nBrandubh's rules and characteristics are used to gain an understanding of the\noverall state-space complexity of the game. State-spaces will consider valid\npiece positions, a generalized rule set, and accepted final state conditions.\nFrom these states the upper bound for the complexity of strongly solving\nBrandubh is derived. Great effort has been placed on thoroughly accounting for\nall potential states and excluding invalid ones for the game. Overall, the\nupper bound complexity for solving the game is around 10^14 states, between\nthat of connect four and draughts (checkers).",
    "descriptor": "\nComments: 7 pages, 9 figures, Version 1.1 of article with correction on citation oversight of Galassi's prior work\n",
    "authors": [
      "Kiernan Compy",
      "Alana Evey",
      "Hunter McCullough",
      "Lindsay Allen",
      "Aaron S. Crandall"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.05353"
  },
  {
    "id": "arXiv:2106.05357",
    "title": "An Extensible Dashboard Architecture For Visualizing Base And Analyzed  Data",
    "abstract": "Any data analysis, especially the data sets that may be changing often or in\nreal-time, consists of at least three important synchronized components: i)\nfiguring out what to infer (objectives), ii) analysis or computation of\nobjectives, and iii) understanding of the results which may require drill-down\nand/or visualization. There is a lot of attention paid to the first two of the\nabove components as part of research whereas the understanding as well as\nderiving actionable decisions is quite tricky. Visualization is an important\nstep towards both understanding (even by non-experts) and inferring the actions\nthat need to be taken. As an example, for Covid-19, knowing regions (say, at\nthe county or state level) that have seen a spike or prone to a spike in cases\nin the near future may warrant additional actions with respect to gatherings,\nbusiness opening hours, etc. This paper focuses on an extensible architecture\nfor visualization of base as well as analyzed data. This paper proposes a\nmodular architecture of a dashboard for user-interaction, visualization\nmanagement, and complex analysis of base data. The contributions of this paper\nare: i) extensibility of the architecture providing flexibility to add\nadditional analysis, visualizations, and user interactions without changing the\nworkflow, ii) decoupling of the functional modules to ease and speedup\ndevelopment by different groups, and iii) address efficiency issues for display\nresponse time. This paper uses Multilayer Networks (or MLNs) for analysis. To\nshowcase the above, we present the implementation of a visualization dashboard,\ntermed CoWiz++ (for Covid Wizard), and elaborate on how web-based user\ninteraction and display components are interfaced seamlessly with the back end\nmodules.",
    "descriptor": "",
    "authors": [
      "Abhishek Santra",
      "Kunal Samant",
      "Endrit Memeti",
      "Enamul Karim",
      "Sharma Chakravarthy"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.05357"
  },
  {
    "id": "arXiv:2106.05358",
    "title": "Self-triggered min-max DMPC for asynchronous multi-agent systems with  communication delays",
    "abstract": "This paper studies the formation stabilization problem of asynchronous\nnonlinear multi-agent systems (MAS) subject to parametric uncertainties,\nexternal disturbances and bounded time-varying communication delays. A\nself-triggered min-max distributed model predictive control (DMPC) approach is\nproposed to handle these practical issues. At triggering instants, each agent\nsolves a local min-max optimization problem based on local system states and\npredicted system states of neighbors, determines its next triggering instant\nand broadcasts its predicted state trajectory to its neighbors. As a result,\nthe communication load is greatly alleviated while retaining robustness and\ncomparable control performance compared to periodic algorithms. In order to\nhandle time-varying delays, a novel consistency constraint is incorporated into\neach local optimization problem to restrict the deviation between the newest\npredicted states and previously broadcast predicted states. Consequently, each\nagent can utilize previously predicted states of its neighbors to achieve\ncooperation in the presence of time-varying delays and asynchronous\ncommunication induced by the distributed triggered scheduler. The recursive\nfeasibility of the proposed algorithm and the closed-loop stability of MAS at\ntriggering time instants are proven. Finally, numerical simulations are\nconducted to verify the efficiency of the proposed control method.",
    "descriptor": "",
    "authors": [
      "Henglai Wei",
      "Kunwu Zhang",
      "Yang Shi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05358"
  },
  {
    "id": "arXiv:2106.05360",
    "title": "Proportional Participatory Budgeting with Substitute Projects",
    "abstract": "Participatory budgeting is a democratic process for allocating funds to\nprojects based on the votes of members of the community. However, most input\nmethods of voters' preferences prevent the voters from expressing complex\nrelationships among projects, leading to outcomes that do not reflect their\npreferences well enough. In this paper, we propose an input method that begins\nto address this challenge, by allowing participants to express substitutes over\nprojects. Then, we extend a known aggregation mechanism from the literature\n(Rule X) to handle substitute projects. We prove that our extended rule\npreserves proportionality under natural conditions, and show empirically that\nit obtains substantially more welfare than the original mechanism on instances\nwith substitutes.",
    "descriptor": "",
    "authors": [
      "Roy Fairstein",
      "Reshef Meir",
      "Kobi Gal"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.05360"
  },
  {
    "id": "arXiv:2106.05363",
    "title": "On Clusters that are Separated but Large",
    "abstract": "$\\renewcommand{\\Re}{\\mathbb{R}}$Given a set $P$ of $n$ points in $\\Re^d$,\nconsider the problem of computing $k$ subsets of $P$ that form clusters that\nare well-separated from each other, and each of them is large (cardinality\nwise). We provide tight upper and lower bounds, and corresponding algorithms,\non the quality of separation, and the size of the clusters that can be\ncomputed, as a function of $n,d,k,s$, and $\\Phi$, where $s$ is the desired\nseparation, and $\\Phi$ is the spread of the point set $P$.",
    "descriptor": "",
    "authors": [
      "Sariel Har-Peled",
      "Joseph Rogge"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.05363"
  },
  {
    "id": "arXiv:2106.05365",
    "title": "DESCGEN: A Distantly Supervised Datasetfor Generating Abstractive Entity  Descriptions",
    "abstract": "Short textual descriptions of entities provide summaries of their key\nattributes and have been shown to be useful sources of background knowledge for\ntasks such as entity linking and question answering. However, generating entity\ndescriptions, especially for new and long-tail entities, can be challenging\nsince relevant information is often scattered across multiple sources with\nvaried content and style. We introduce DESCGEN: given mentions spread over\nmultiple documents, the goal is to generate an entity summary description.\nDESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each\npaired with nine evidence documents on average. The documents were collected\nusing a combination of entity linking and hyperlinks to the Wikipedia and\nFandom entity pages, which together provide high-quality distant supervision.\nThe resulting summaries are more abstractive than those found in existing\ndatasets and provide a better proxy for the challenge of describing new and\nemerging entities. We also propose a two-stage extract-then-generate baseline\nand show that there exists a large gap (19.9% in ROUGE-L) between\nstate-of-the-art models and human performance, suggesting that the data will\nsupport significant future work.",
    "descriptor": "",
    "authors": [
      "Weijia Shi",
      "Mandar Joshi",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05365"
  },
  {
    "id": "arXiv:2106.05367",
    "title": "Pulling back information geometry",
    "abstract": "Latent space geometry has shown itself to provide a rich and rigorous\nframework for interacting with the latent variables of deep generative models.\nThe existing theory, however, relies on the decoder being a Gaussian\ndistribution as its simple reparametrization allows us to interpret the\ngenerating process as a random projection of a deterministic manifold.\nConsequently, this approach breaks down when applied to decoders that are not\nas easily reparametrized. We here propose to use the Fisher-Rao metric\nassociated with the space of decoder distributions as a reference metric, which\nwe pull back to the latent space. We show that we can achieve meaningful latent\ngeometries for a wide range of decoder distributions for which the previous\ntheory was not applicable, opening the door to `black box' latent geometries.",
    "descriptor": "",
    "authors": [
      "Georgios Arvanitidis",
      "Miguel Gonz\u00e1lez-Duque",
      "Alison Pouplin",
      "Dimitris Kalatzis",
      "S\u00f8ren Hauberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05367"
  },
  {
    "id": "arXiv:2106.05373",
    "title": "StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs,  GPUs and FPGAs",
    "abstract": "The modern deep learning method based on backpropagation has surged in\npopularity and has been used in multiple domains and application areas. At the\nsame time, there are other -- less-known -- machine learning algorithms with a\nmature and solid theoretical foundation whose performance remains unexplored.\nOne such example is the brain-like Bayesian Confidence Propagation Neural\nNetwork (BCPNN). In this paper, we introduce StreamBrain -- a framework that\nallows neural networks based on BCPNN to be practically deployed in\nHigh-Performance Computing systems. StreamBrain is a domain-specific language\n(DSL), similar in concept to existing machine learning (ML) frameworks, and\nsupports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate\nthat StreamBrain can train the well-known ML benchmark dataset MNIST within\nseconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We\nalso show how StreamBrain can be used to train with custom floating-point\nformats and illustrate the impact of using different bfloat variations on BCPNN\nusing FPGAs.",
    "descriptor": "\nComments: Accepted for publication at the International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies (HEART 2021)\n",
    "authors": [
      "Artur Podobas",
      "Martin Svedin",
      "Steven W. D. Chien",
      "Ivy B. Peng",
      "Naresh Balaji Ravichandran",
      "Pawel Herman",
      "Anders Lansner",
      "Stefano Markidis"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.05373"
  },
  {
    "id": "arXiv:2106.05375",
    "title": "Plan2Scene: Converting Floorplans to 3D Scenes",
    "abstract": "We address the task of converting a floorplan and a set of associated photos\nof a residence into a textured 3D mesh model, a task which we call Plan2Scene.\nOur system 1) lifts a floorplan image to a 3D mesh model; 2) synthesizes\nsurface textures based on the input photos; and 3) infers textures for\nunobserved surfaces using a graph neural network architecture. To train and\nevaluate our system we create indoor surface texture datasets, and augment a\ndataset of floorplans and photos from prior work with rectified surface crops\nand additional annotations. Our approach handles the challenge of producing\ntileable textures for dominant surfaces such as floors, walls, and ceilings\nfrom a sparse set of unaligned photos that only partially cover the residence.\nQualitative and quantitative evaluations show that our system produces\nrealistic 3D interior models, outperforming baseline approaches on a suite of\ntexture quality metrics and as measured by a holistic user study.",
    "descriptor": "\nComments: This paper is accepted to CVPR 2021. For code, data and pretrained models, see this https URL\n",
    "authors": [
      "Madhawa Vidanapathirana",
      "Qirui Wu",
      "Yasutaka Furukawa",
      "Angel X. Chang",
      "Manolis Savva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.05375"
  },
  {
    "id": "arXiv:2106.05378",
    "title": "Parameter and Feature Selection in Stochastic Linear Bandits",
    "abstract": "We study two model selection settings in stochastic linear bandits (LB). In\nthe first setting, the reward parameter of the LB problem is arbitrarily\nselected from $M$ models represented as (possibly) overlapping balls in\n$\\mathbb R^d$. However, the agent only has access to misspecified models, i.e.,\nestimates of the centers and radii of the balls. We refer to this setting as\nparameter selection. In the second setting, which we refer to as feature\nselection, the expected reward of the LB problem is in the linear span of at\nleast one of $M$ feature maps (models). For each setting, we develop and\nanalyze an algorithm that is based on a reduction from bandits to\nfull-information problems. This allows us to obtain regret bounds that are not\nworse (up to a $\\sqrt{\\log M}$ factor) than the case where the true model is\nknown. Our parameter selection algorithm is OFUL-style and the one for feature\nselection is based on the SquareCB algorithm. We also show that the regret of\nour parameter selection algorithm scales logarithmically with model\nmisspecification.",
    "descriptor": "",
    "authors": [
      "Ahmadreza Moradipari",
      "Yasin Abbasi-Yadkori",
      "Mahnoosh Alizadeh",
      "Mohammad Ghavamzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05378"
  },
  {
    "id": "arXiv:2106.05380",
    "title": "Aerial Reconfigurable Intelligent Surface-Aided Wireless Communication  Systems",
    "abstract": "In this paper, we propose and investigate an aerial reconfigurable\nintelligent surface (aerial-RIS)-aided wireless communication system.\nSpecifically, considering practical composite fading channels, we characterize\nthe air-to-ground (A2G) links by Namkagami-m small-scale fading and\ninverse-Gamma large-scale shadowing. To investigate the delay-limited\nperformance of the proposed system, we derive a tight approximate closed-form\nexpression for the end-to-end outage probability (OP). Next, considering a\nmobile environment, where performance analysis is intractable, we rely on\nmachine learning-based performance prediction to evaluate the performance of\nthe mobile aerial-RIS-aided system. Specifically, taking into account the\nthree-dimensional (3D) spatial movement of the aerial-RIS, we build a deep\nneural network (DNN) to accurately predict the OP. We show that: (i) fading and\nshadowing conditions have strong impact on the OP, (ii) as the number of\nreflecting elements increases, aerial-RIS achieves higher energy efficiency\n(EE), and (iii) the aerial-RIS-aided system outperforms conventional relaying\nsystems.",
    "descriptor": "",
    "authors": [
      "Tri Nhu Do",
      "Georges Kaddoum",
      "Thanh Luan Nguyen",
      "Daniel Benevides da Costa",
      "Zygmunt J. Haas"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05380"
  },
  {
    "id": "arXiv:2106.05384",
    "title": "Long-time integration of parametric evolution equations with  physics-informed DeepONets",
    "abstract": "Ordinary and partial differential equations (ODEs/PDEs) play a paramount role\nin analyzing and simulating complex dynamic processes across all corners of\nscience and engineering. In recent years machine learning tools are aspiring to\nintroduce new effective ways of simulating PDEs, however existing approaches\nare not able to reliably return stable and accurate predictions across long\ntemporal horizons. We aim to address this challenge by introducing an effective\nframework for learning infinite-dimensional operators that map random initial\nconditions to associated PDE solutions within a short time interval. Such\nlatent operators can be parametrized by deep neural networks that are trained\nin an entirely self-supervised manner without requiring any paired input-output\nobservations. Global long-time predictions across a range of initial conditions\ncan be then obtained by iteratively evaluating the trained model using each\nprediction as the initial condition for the next evaluation step. This\nintroduces a new approach to temporal domain decomposition that is shown to be\neffective in performing accurate long-time simulations for a wide range of\nparametric ODE and PDE systems, from wave propagation, to reaction-diffusion\ndynamics and stiff chemical kinetics, all at a fraction of the computational\ncost needed by classical numerical solvers.",
    "descriptor": "\nComments: 29 pages, 22 figures\n",
    "authors": [
      "Sifan Wang",
      "Paris Perdikaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.05384"
  },
  {
    "id": "arXiv:2106.05385",
    "title": "Multirate Exponential Rosenbrock Methods",
    "abstract": "In this paper we present a novel class of methods for high order accurate\nintegration of multirate systems of ordinary differential equation\ninitial-value problems. Following from recent work on multirate exponential\nRunge--Kutta (MERK) methods, that construct multirate schemes by approximating\nthe action of matrix $\\varphi$-functions within explicit exponential\nRunge--Kutta methods, the proposed methods similarly build off of explicit\nexponential Rosenbrock (ExpRB) methods. By leveraging the exponential\nRosenbrock structure, the proposed Multirate Exponential Rosenbrock (MERB)\nmethods consist of the solution to a sequence of modified ``fast''\ninitial-value problems, that may themselves be approximated through subcycling\nany desired IVP solver. In addition to proving how to construct MERB methods\nfrom certain classes of ExpRB methods, we provide rigorous convergence analysis\nof the resulting schemes, and present candidate MERB schemes of orders two\nthrough six. We then present numerical simulations to confirm these theoretical\nconvergence rates, and to compare the efficiency of MERB methods against\nrecently-introduced multirate MERK and MRI-GARK methods.",
    "descriptor": "\nComments: Submitted to Mathematics of Computation\n",
    "authors": [
      "Vu Thai Luan",
      "Rujeko Chinomona",
      "Daniel R. Reynolds"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05385"
  },
  {
    "id": "arXiv:2106.05386",
    "title": "Artificial Intelligence in Drug Discovery:Applications and Techniques",
    "abstract": "Artificial intelligence has transformed the practice of drug discovery in the\npast decade. Various artificial intelligence techniques have been used in a\nwide range of applications. In this perspective, we present major applications\nof AI in drug discovery and discuss the relevant AI techniques, covering most\nrecent progress in AI-driven drug discovery. We expect that the perspective\nwill serve as a guide for researchers who are interested in working at this\nintersected area of artificial intelligence and drug discovery. We also provide\na GitHub repository summarizing the surveyed papers as a learning resource,\nwhich will be regularly updated.",
    "descriptor": "",
    "authors": [
      "Jianyuan Deng",
      "Zhibo Yang",
      "Dimitris Samaras",
      "Fusheng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05386"
  },
  {
    "id": "arXiv:2106.05387",
    "title": "Eye of the Beholder: Improved Relation Generalization for Text-based  Reinforcement Learning Agents",
    "abstract": "Text-based games (TBGs) have become a popular proving ground for the\ndemonstration of learning-based agents that make decisions in quasi real-world\nsettings. The crux of the problem for a reinforcement learning agent in such\nTBGs is identifying the objects in the world, and those objects' relations with\nthat world. While the recent use of text-based resources for increasing an\nagent's knowledge and improving its generalization have shown promise, we posit\nin this paper that there is much yet to be learned from visual representations\nof these same worlds. Specifically, we propose to retrieve images that\nrepresent specific instances of text observations from the world and train our\nagents on such images. This improves the agent's overall understanding of the\ngame 'scene' and objects' relationships to the world around them, and the\nvariety of visual representations on offer allow the agent to generate a better\ngeneralization of a relationship. We show that incorporating such images\nimproves the performance of agents in various TBG settings.",
    "descriptor": "",
    "authors": [
      "Keerthiram Murugesan",
      "Subhajit Chaudhury",
      "Kartik Talamadupula"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05387"
  },
  {
    "id": "arXiv:2106.05390",
    "title": "Optimizing Reusable Knowledge for Continual Learning via Metalearning",
    "abstract": "When learning tasks over time, artificial neural networks suffer from a\nproblem known as Catastrophic Forgetting (CF). This happens when the weights of\na network are overwritten during the training of a new task causing forgetting\nof old information. To address this issue, we propose MetA Reusable Knowledge\nor MARK, a new method that fosters weight reusability instead of overwriting\nwhen learning a new task. Specifically, MARK keeps a set of shared weights\namong tasks. We envision these shared weights as a common Knowledge Base (KB)\nthat is not only used to learn new tasks, but also enriched with new knowledge\nas the model learns new tasks. Key components behind MARK are two-fold. On the\none hand, a metalearning approach provides the key mechanism to incrementally\nenrich the KB with new knowledge and to foster weight reusability among tasks.\nOn the other hand, a set of trainable masks provides the key mechanism to\nselectively choose from the KB relevant weights to solve each task. By using\nMARK, we achieve state of the art results in several popular benchmarks,\nsurpassing the best performing methods in terms of average accuracy by over 10%\non the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness\nusing 55% of the number of parameters. Furthermore, an ablation study provides\nevidence that, indeed, MARK is learning reusable knowledge that is selectively\nused by each task.",
    "descriptor": "",
    "authors": [
      "Julio Hurtado",
      "Alain Raymond-Saez",
      "Alvaro Soto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05390"
  },
  {
    "id": "arXiv:2106.05391",
    "title": "Fairness-Aware Node Representation Learning",
    "abstract": "Node representation learning has demonstrated its effectiveness for various\napplications on graphs. Particularly, recent developments in contrastive\nlearning have led to promising results in unsupervised node representation\nlearning for a number of tasks. Despite the success of graph contrastive\nlearning and consequent growing interest, fairness is largely under-explored in\nthe field. To this end, this study addresses fairness issues in graph\ncontrastive learning with fairness-aware graph augmentation designs, through\nadaptive feature masking and edge deletion. In the study, different fairness\nnotions on graphs are introduced, which serve as guidelines for the proposed\ngraph augmentations. Furthermore, theoretical analysis is provided to\nquantitatively prove that the proposed feature masking approach can reduce\nintrinsic bias. Experimental results on real social networks are presented to\ndemonstrate that the proposed augmentations can enhance fairness in terms of\nstatistical parity and equal opportunity, while providing comparable\nclassification accuracy to state-of-the-art contrastive methods for node\nclassification.",
    "descriptor": "\nComments: 15 pages, 4 tables, submitted manuscript\n",
    "authors": [
      "\u00d6yk\u00fc Deniz K\u00f6se",
      "Yanning Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05391"
  },
  {
    "id": "arXiv:2106.05392",
    "title": "Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
    "abstract": "In video transformers, the time dimension is often treated in the same way as\nthe two spatial dimensions. However, in a scene where objects or the camera may\nmove, a physical point imaged at one location in frame $t$ may be entirely\nunrelated to what is found at that location in frame $t+k$. These temporal\ncorrespondences should be modeled to facilitate learning about dynamic scenes.\nTo this end, we propose a new drop-in block for video transformers --\ntrajectory attention -- that aggregates information along implicitly determined\nmotion paths. We additionally propose a new method to address the quadratic\ndependence of computation and memory on the input size, which is particularly\nimportant for high resolution or long videos. While these ideas are useful in a\nrange of settings, we apply them to the specific task of video action\nrecognition with a transformer model and obtain state-of-the-art results on the\nKinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models\nare available at: https://github.com/facebookresearch/Motionformer",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Mandela Patrick",
      "Dylan Campbell",
      "Yuki M. Asano",
      "Ishan Misra Florian Metze",
      "Christoph Feichtenhofer",
      "Andrea Vedaldi",
      "Jo\\\u00e3o F. Henriques"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05392"
  },
  {
    "id": "arXiv:2106.05395",
    "title": "Impact of Blockchain Technology on Electric Power Grids -- A case study  in LO3 Energy",
    "abstract": "The increasing amount of distributed energy resources including renewable\nenergy systems and electric vehicles is expected to change electric power grids\nsignificantly, where conventional consumers are transformed to prosumers since\nthey can produce electricity as well. In such an ecosystem, prosumers can start\noffering their excess energy to supply demands of the other customers on the\ngrids behind the meter without interference of distribution system operators\n(DSO). Besides, DSOs require more accurate and more frequent data form\nprosumers' net demand to be able to operate their network efficiently. The main\nchallenge in these new distribution grids is the amount of data that needs to\nbe collected in this platform is unbelievably high, and more immortally,\nprosumers will likely refuse to share their information with DSOs due to their\npotential privacy and economic concerns. Blockchain technology as an efficient\ndistributed solution for management of data and financial transactions, has\nbeen considered to solve this trust issue. With blockchain-based solutions,\ndata and financial transactions between all parties will take placed through\ndistributed ledgers without any interference from an intermediary. In this\npaper, impacts of blockchain technologies on electric power industry is\nstudied. The paper specifically focuses on LO3 Energy -- one of startups\napplying blockchain to electric power grids -- their blockchain-based solution\ncalled Exergy, and their use cases to implement such solutions.",
    "descriptor": "",
    "authors": [
      "Sakineh Khalili",
      "Vahid Disfani",
      "Mo Ahmadi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05395"
  },
  {
    "id": "arXiv:2106.05401",
    "title": "Mechanisms and Attributes of Echo Chambers in Social Media",
    "abstract": "Echo chambers may exclude social media users from being exposed to other\nopinions, therefore, can cause rampant negative effects. Among abundant\nevidence are the 2016 and 2020 US presidential elections conspiracy theories\nand polarization, as well as the COVID-19 disinfodemic. To help better detect\necho chambers and mitigate its negative effects, this paper explores the\nmechanisms and attributes of echo chambers in social media. In particular, we\nfirst illustrate four primary mechanisms related to three main factors: human\npsychology, social networks, and automatic systems. We then depict common\nattributes of echo chambers with a focus on the diffusion of misinformation,\nspreading of conspiracy theory, creation of social trends, political\npolarization, and emotional contagion of users. We illustrate each mechanism\nand attribute in a multi-perspective of sociology, psychology, and social\ncomputing with recent case studies. Our analysis suggest an emerging need to\ndetect echo chambers and mitigate their negative effects.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Bohan Jiang",
      "Mansooreh Karami",
      "Lu Cheng",
      "Tyler Black",
      "Huan Liu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05401"
  },
  {
    "id": "arXiv:2106.05402",
    "title": "Deception in Social Learning: A Multi-Agent Reinforcement Learning  Perspective",
    "abstract": "Within the framework of Multi-Agent Reinforcement Learning, Social Learning\nis a new class of algorithms that enables agents to reshape the reward function\nof other agents with the goal of promoting cooperation and achieving higher\nglobal rewards in mixed-motive games. However, this new modification allows\nagents unprecedented access to each other's learning process, which can\ndrastically increase the risk of manipulation when an agent does not realize it\nis being deceived into adopting policies which are not actually in its own best\ninterest. This research review introduces the problem statement, defines key\nconcepts, critically evaluates existing evidence and addresses open problems\nthat should be addressed in future research.",
    "descriptor": "",
    "authors": [
      "Paul Chelarescu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05402"
  },
  {
    "id": "arXiv:2106.05407",
    "title": "Auditing Network Traffic and Privacy Policies in Oculus VR",
    "abstract": "Virtual reality (VR) is an emerging technology that enables new applications\nbut also introduces privacy risks. In this paper, we focus on Oculus VR (OVR),\nthe leading platform in the VR space, and we provide the first comprehensive\nanalysis of personal data exposed by OVR apps and the platform itself, from a\ncombined networking and privacy policy perspective. We experimented with the\nQuest 2 headset, and we tested the most popular VR apps available on the\nofficial Oculus and the SideQuest app stores. We developed OVRseen, a\nmethodology and system for collecting, analyzing, and comparing network traffic\nand privacy policies on OVR. On the networking side, we captured and decrypted\nnetwork traffic of VR apps, which was previously not possible on OVR, and we\nextracted data flows (defined as <app, data type, destination>). We found that\nthe OVR ecosystem (compared to the mobile and other app ecosystems) is more\ncentralized, and driven by tracking and analytics, rather than by third-party\nadvertising. We show that the data types exposed by VR apps include personally\nidentifiable information (PII), device information that can be used for\nfingerprinting, and VR-specific data types. By comparing the data flows found\nin the network traffic with statements made in the apps' privacy policies, we\ndiscovered that approximately 70% of OVR data flows were not properly\ndisclosed. Furthermore, we provided additional context for these data flows,\nincluding the purpose, which we extracted from the privacy policies, and\nobserved that 69% were sent for purposes unrelated to the core functionality of\napps.",
    "descriptor": "\nComments: 13 pages (and 4 pages of references), 7 figures, 4 tables\n",
    "authors": [
      "Rahmadi Trimananda",
      "Hieu Le",
      "Hao Cui",
      "Janice Tran Ho",
      "Anastasia Shuba",
      "Athina Markopoulou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05407"
  },
  {
    "id": "arXiv:2106.05409",
    "title": "Zero Time Waste: Recycling Predictions in Early Exit Neural Networks",
    "abstract": "The problem of reducing processing time of large deep learning models is a\nfundamental challenge in many real-world applications. Early exit methods\nstrive towards this goal by attaching additional Internal Classifiers (ICs) to\nintermediate layers of a neural network. ICs can quickly return predictions for\neasy examples and, as a result, reduce the average inference time of the whole\nmodel. However, if a particular IC does not decide to return an answer early,\nits predictions are discarded, with its computations effectively being wasted.\nTo solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in\nwhich each IC reuses predictions returned by its predecessors by (1) adding\ndirect connections between ICs and (2) combining previous outputs in an\nensemble-like manner. We conduct extensive experiments across various datasets\nand architectures to demonstrate that ZTW achieves a significantly better\naccuracy vs. inference time trade-off than other recently proposed early exit\nmethods.",
    "descriptor": "",
    "authors": [
      "Maciej Wo\u0142czyk",
      "Bartosz W\u00f3jcik",
      "Klaudia Ba\u0142azy",
      "Igor Podolak",
      "Jacek Tabor",
      "Marek \u015amieja",
      "Tomasz Trzci\u0144ski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05409"
  },
  {
    "id": "arXiv:2106.05410",
    "title": "DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly  Detection",
    "abstract": "Semi-supervised anomaly detection, which aims to detect anomalies from normal\nsamples using a model that is solely trained on normal data, has been an active\nfield of research in the past decade. With recent advancements in deep\nlearning, particularly generative adversarial networks and autoencoders,\nresearchers have designed efficient deep anomaly detection methods. Existing\nworks commonly use neural networks such as an autoencoder to map the data into\na new representation that is easier to work with and then apply an anomaly\ndetection algorithm. In this paper, we propose a method, DASVDD, that jointly\nlearns the parameters of an autoencoder while minimizing the volume of an\nenclosing hyper-sphere on its latent representation. We propose a customized\nanomaly score which is a combination of autoencoder's reconstruction error and\ndistance of the lower-dimensional representation of a sample from the center of\nthe enclosing hyper-sphere. Minimizing this anomaly score on the normal data\nduring training aids us in learning the underlying distribution of normal data.\nIncluding the reconstruction error in the anomaly score ensures that DASVDD\ndoes not suffer from the common hyper-sphere collapse issue since the proposed\nDASVDD model does not converge to the trivial solution of mapping all inputs to\na constant point in the latent representation. Experimental evaluations on\nseveral benchmark datasets from different domains show that the proposed method\noutperforms most of the commonly used state-of-the-art anomaly detection\nalgorithms while maintaining robust and accurate performance across different\nanomaly classes.",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Hadi Hojjati",
      "Narges Armanfard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05410"
  },
  {
    "id": "arXiv:2106.05418",
    "title": "Probing transfer learning with a model of synthetic correlated datasets",
    "abstract": "Transfer learning can significantly improve the sample efficiency of neural\nnetworks, by exploiting the relatedness between a data-scarce target task and a\ndata-abundant source task. Despite years of successful applications, transfer\nlearning practice often relies on ad-hoc solutions, while theoretical\nunderstanding of these procedures is still limited. In the present work, we\nre-think a solvable model of synthetic data as a framework for modeling\ncorrelation between data-sets. This setup allows for an analytic\ncharacterization of the generalization performance obtained when transferring\nthe learned feature map from the source to the target task. Focusing on the\nproblem of training two-layer networks in a binary classification setting, we\nshow that our model can capture a range of salient features of transfer\nlearning with real data. Moreover, by exploiting parametric control over the\ncorrelation between the two data-sets, we systematically investigate under\nwhich conditions the transfer of features is beneficial for generalization.",
    "descriptor": "",
    "authors": [
      "Federica Gerace",
      "Luca Saglietti",
      "Stefano Sarao Mannelli",
      "Andrew Saxe",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)"
    ],
    "url": "https://arxiv.org/abs/2106.05418"
  },
  {
    "id": "arXiv:2106.05419",
    "title": "Natural Factor Based Solvers",
    "abstract": "We consider parametric families of partial differential equations--PDEs where\nthe parameter $\\kappa$ modifies only the (1,1) block of a saddle point matrix\nproduct of a discretization below. The main goal is to develop an algorithm\nthat removes, as much as possible, the dependence of iterative solvers on the\nparameter $\\kappa$. The algorithm we propose requires only one matrix\nfactorization which does not depend on $\\kappa$, therefore, allows to reuse it\nfor solving very fast a large number of discrete PDEs for different $\\kappa$\nand forcing terms. The design of the proposed algorithm is motivated by\nprevious works on natural factor of formulation of the stiffness matrices and\ntheir stable numerical solvers. As an application, in two dimensions, we\nconsider an iterative preconditioned solver based on the null space of\nCrouzeix-Raviart discrete gradient represented as the discrete curl of $P_1$\nconforming finite element functions. For the numerical examples, we consider\nthe case of random coefficient pressure equation where the permeability is\nmodeled by an stochastic process. We note that contrarily from recycling Krylov\nsubspace techniques, the proposed algorithm does not require fixed forcing\nterms.",
    "descriptor": "",
    "authors": [
      "O. Andr\u00e9s Cuervo",
      "Juan Galvis",
      "Marcus Sarkis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05419"
  },
  {
    "id": "arXiv:2106.05420",
    "title": "DynamiQ: Planning for Dynamics in Network Streaming Analytics Systems",
    "abstract": "The emergence of programmable data-plane targets has motivated a new hybrid\ndesign for network streaming analytics systems that combine these targets' fast\npacket processing speeds with the rich compute resources available at modern\nstream processors. However, these systems require careful query planning; that\nis, specifying the minute details of executing a given set of queries in a way\nthat makes the best use of the limited resources and programmability offered by\ndata-plane targets. We use such an existing system, Sonata, and real-world\npacket traces to understand how executing a fixed query workload is affected by\nthe unknown dynamics of the traffic that defines the target's input workload.\nWe observe that static query planning, as employed by Sonata, cannot handle\neven small changes in the input workload, wasting data-plane resources to the\npoint where query execution is confined mainly to userspace.\nThis paper presents the design and implementation of DynamiQ, a new network\nstreaming analytics platform that employs dynamic query planning to deal with\nthe dynamics of real-world input workloads. Specifically, we develop a suite of\npractical algorithms for (i) computing effective initial query plans (to start\nquery execution) and (ii) enabling efficient updating of portions of such an\ninitial query plan at runtime (to adapt to changes in the input workload).\nUsing real-world packet traces as input workload, we show that compared to\nSonata, DynamiQ reduces the stream processor's workload by two orders of\nmagnitude.",
    "descriptor": "",
    "authors": [
      "Rohan Bhatia",
      "Arpit Gupta",
      "Rob Harrison",
      "Daniel Lokshtanov",
      "Walter Willinger"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05420"
  },
  {
    "id": "arXiv:2106.05421",
    "title": "Data-Driven Invariant Learning for Probabilistic Programs",
    "abstract": "Morgan and McIver's weakest pre-expectation framework is one of the most\nwell-established methods for deductive verification of probabilistic programs.\nRoughly, the idea is to generalize binary state assertions to real-valued\nexpectations. While loop-free programs can be analyzed by mechanically\ntransforming expectations, verifying loops usually requires finding an\ninvariant expectation, a difficult task. We propose a new view of invariant\nexpectation synthesis as a regression problem: given an input state, predict\nthe average value of the post-expectation. Guided by this perspective, we\ndevelop the first data-driven invariant synthesis method for probabilistic\nprograms. Unlike prior work on probabilistic invariant inference, our approach\ncan learn piecewise continuous invariants without relying on template\nexpectations, and also works when only given black-box access to the program.\nWe implement our approach and demonstrate its effectiveness on a variety of\nbenchmarks from the probabilistic programming literature.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Jialu Bao",
      "Drashti Pathak",
      "Justin Hsu",
      "Subhajit Roy"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.05421"
  },
  {
    "id": "arXiv:2106.05423",
    "title": "A New Notion of Individually Fair Clustering: $\u03b1$-Equitable  $k$-Center",
    "abstract": "Clustering is a fundamental problem in unsupervised machine learning, and\nfair variants of it have recently received significant attention. In this work\nwe introduce a novel definition of fairness for clustering problems.\nSpecifically, in our model each point $j$ has a set of other points\n$\\mathcal{S}_j$ that it perceives as similar to itself, and it feels that it is\nfairly treated, if the quality of service it receives in the solution is\n$\\alpha$-close to that of the points in $\\mathcal{S}_j$. We begin our study by\nanswering questions regarding the structure of the problem, namely for what\nvalues of $\\alpha$ the problem is well-defined, and what the behavior of the\nPrice of Fairness (PoF) for it is. For the well-defined region of $\\alpha$, we\nprovide efficient and easily implementable approximation algorithms for the\n$k$-center objective, which in certain cases also enjoy bounded PoF guarantees.\nWe finally complement our analysis by an extensive suite of experiments that\nvalidates the effectiveness of our theoretical results.",
    "descriptor": "",
    "authors": [
      "Darshan Chakrabarti",
      "John P. Dickerson",
      "Seyed A. Esmaeili",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05423"
  },
  {
    "id": "arXiv:2106.05424",
    "title": "Fair Disaster Containment via Graph-Cut Problems",
    "abstract": "Graph cut problems form a fundamental problem type in combinatorial\noptimization, and are a central object of study in both theory and practice. In\naddition, the study of fairness in Algorithmic Design and Machine Learning has\nrecently received significant attention, with many different notions proposed\nand analyzed in a variety of contexts. In this paper we initiate the study of\nfairness for graph cut problems by giving the first fair definitions for them,\nand subsequently we demonstrate appropriate algorithmic techniques that yield a\nrigorous theoretical analysis. Specifically, we incorporate two different\ndefinitions of fairness, namely demographic and probabilistic individual\nfairness, in a particular cut problem modeling disaster containment scenarios.\nOur results include a variety of approximation algorithms with provable\ntheoretical guarantees.",
    "descriptor": "",
    "authors": [
      "Amy Babay",
      "Michael Dinitz",
      "Prathyush Sambaturu",
      "Aravind Srinivasan",
      "Leonidas Tsepenekas",
      "Anil Vullikanti"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05424"
  },
  {
    "id": "arXiv:2106.05426",
    "title": "Low-Dimensional Structure in the Space of Language Representations is  Reflected in Brain Responses",
    "abstract": "How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.",
    "descriptor": "\nComments: Preprint, submitted for review\n",
    "authors": [
      "Richard Antonello",
      "Javier Turek",
      "Vy Vo",
      "Alexander Huth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05426"
  },
  {
    "id": "arXiv:2106.05427",
    "title": "Observation data compression for variational assimilation of dynamical  systems",
    "abstract": "Accurate estimation of error covariances (both background and observation) is\ncrucial for efficient observation compression approaches in data assimilation\nof large-scale dynamical problems. We propose a new combination of a covariance\ntuning algorithm with existing PCA-type data compression approaches, either\nobservation- or information-based, with the aim of reducing the computational\ncost of real-time updating at each assimilation step. Relying on a local\nassumption of flow-independent error covariances, dynamical assimilation\nresiduals are used to adjust the covariance in each assimilation window. The\nestimated covariances then contribute to better specify the principal\ncomponents of either the observation dynamics or the state-observation\nsensitivity. The proposed approaches are first validated on a shallow water\ntwin experiment with correlated and non-homogeneous observation error. Proper\nselection of flow-independent assimilation windows, together with sampling\ndensity for background error estimation, and sensitivity of the approaches to\nthe observations error covariance knowledge, are also discussed and illustrated\nwith various numerical tests and results. The method is then applied to a more\nchallenging industrial hydrological model with real-world data and a non-linear\ntransformation operator provided by an operational precipitation-flow\nsimulation software.",
    "descriptor": "",
    "authors": [
      "Sibo Cheng",
      "Didier Lucor",
      "Jean-Philippe Argaud"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05427"
  },
  {
    "id": "arXiv:2106.05429",
    "title": "Deep Direct Volume Rendering: Learning Visual Feature Mappings From  Exemplary Images",
    "abstract": "Volume Rendering is an important technique for visualizing three-dimensional\nscalar data grids and is commonly employed for scientific and medical image\ndata. Direct Volume Rendering (DVR) is a well established and efficient\nrendering algorithm for volumetric data. Neural rendering uses deep neural\nnetworks to solve inverse rendering tasks and applies techniques similar to\nDVR. However, it has not been demonstrated successfully for the rendering of\nscientific volume data.\nIn this work, we introduce Deep Direct Volume Rendering (DeepDVR), a\ngeneralization of DVR that allows for the integration of deep neural networks\ninto the DVR algorithm. We conceptualize the rendering in a latent color space,\nthus enabling the use of deep architectures to learn implicit mappings for\nfeature extraction and classification, replacing explicit feature design and\nhand-crafted transfer functions. Our generalization serves to derive novel\nvolume rendering architectures that can be trained end-to-end directly from\nexamples in image space, obviating the need to manually define and fine-tune\nmultidimensional transfer functions while providing superior classification\nstrength. We further introduce a novel stepsize annealing scheme to accelerate\nthe training of DeepDVR models and validate its effectiveness in a set of\nexperiments. We validate our architectures on two example use cases: (1)\nlearning an optimized rendering from manually adjusted reference images for a\nsingle volume and (2) learning advanced visualization concepts like shading and\nsemantic colorization that generalize to unseen volume data.\nWe find that deep volume rendering architectures with explicit modeling of\nthe DVR pipeline effectively enable end-to-end learning of scientific volume\nrendering tasks from target images.",
    "descriptor": "",
    "authors": [
      "Jakob Weiss",
      "Nassir Navab"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05429"
  },
  {
    "id": "arXiv:2106.05430",
    "title": "Very Compact Clusters with Structural Regularization via Similarity and  Connectivity",
    "abstract": "Clustering algorithms have significantly improved along with Deep Neural\nNetworks which provide effective representation of data. Existing methods are\nbuilt upon deep autoencoder and self-training process that leverages the\ndistribution of cluster assignments of samples. However, as the fundamental\nobjective of the autoencoder is focused on efficient data reconstruction, the\nlearnt space may be sub-optimal for clustering. Moreover, it requires highly\neffective codes (i.e., representation) of data, otherwise the initial cluster\ncenters often cause stability issues during self-training. Many\nstate-of-the-art clustering algorithms use convolution operation to extract\nefficient codes but their applications are limited to image data. In this\nregard, we propose an end-to-end deep clustering algorithm, i.e., Very Compact\nClusters (VCC), for the general datasets, which takes advantage of\ndistributions of local relationships of samples near the boundary of clusters,\nso that they can be properly separated and pulled to cluster centers to form\ncompact clusters. Experimental results on various datasets illustrate that our\nproposed approach achieves better clustering performance over most of the\nstate-of-the-art clustering methods, and the data embeddings learned by VCC\nwithout convolution for image data are even comparable with specialized\nconvolutional methods.",
    "descriptor": "\nComments: 12 pages, 5 figures. arXiv admin note: text overlap with arXiv:1704.06327 by other authors\n",
    "authors": [
      "Xin Ma",
      "Won Hwa Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05430"
  },
  {
    "id": "arXiv:2106.05434",
    "title": "FedDICE: A ransomware spread detection in a distributed integrated  clinical environment using federated learning and SDN based mitigation",
    "abstract": "An integrated clinical environment (ICE) enables the connection and\ncoordination of the internet of medical things around the care of patients in\nhospitals. However, ransomware attacks and their spread on hospital\ninfrastructures, including ICE, are rising. Often the adversaries are targeting\nmultiple hospitals with the same ransomware attacks. These attacks are detected\nby using machine learning algorithms. But the challenge is devising the\nanti-ransomware learning mechanisms and services under the following\nconditions: (1) provide immunity to other hospitals if one of them got the\nattack, (2) hospitals are usually distributed over geographical locations, and\n(3) direct data sharing is avoided due to privacy concerns. In this regard,\nthis paper presents a federated distributed integrated clinical environment,\naka. FedDICE. FedDICE integrates federated learning (FL), which is\nprivacy-preserving learning, to SDN-oriented security architecture to enable\ncollaborative learning, detection, and mitigation of ransomware attacks. We\ndemonstrate the importance of FedDICE in a collaborative environment with up to\nfour hospitals and four popular ransomware families, namely WannaCry, Petya,\nBadRabbit, and PowerGhost. Our results find that in both IID and non-IID data\nsetups, FedDICE achieves the centralized baseline performance that needs direct\ndata sharing for detection. However, as a trade-off to data privacy, FedDICE\nobserves overhead in the anti-ransomware model training, e.g., 28x for the\nlogistic regression model. Besides, FedDICE utilizes SDN's dynamic network\nprogrammability feature to remove the infected devices in ICE.",
    "descriptor": "",
    "authors": [
      "Chandra Thapa",
      "Kallol Krishna Karmakar",
      "Alberto Huertas Celdran",
      "Seyit Camtepe",
      "Vijay Varadharajan",
      "Surya Nepal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05434"
  },
  {
    "id": "arXiv:2106.05436",
    "title": "An adaptive Origin-Destination flows cluster-detecting method to  identify urban mobility trends",
    "abstract": "Origin-Destination (OD) flow, as an abstract representation of the object`s\nmovement or interaction, has been used to reveal the urban mobility and\nhuman-land interaction pattern. As an important spatial analysis approach, the\nclustering methods of point events have been extended to OD flows to identify\nthe dominant trends and spatial structures of urban mobility. However, the\nexisting methods for OD flow cluster-detecting are limited both in specific\nspatial scale and the uncertain result due to different parameters setting,\nwhich is difficult for complicated OD flows clustering under spatial\nheterogeneity. To address these limitations, in this paper, we proposed a novel\nOD flows cluster-detecting method based on the OPTICS algorithm which can\nidentify OD flow clusters with various aggregation scales. The method can\nadaptively determine parameter value from the dataset without prior knowledge\nand artificial intervention. Experiments indicated that our method outperformed\nthree state-of-the-art methods with more accurate and complete of clusters and\nless noise. As a case study, our method is applied to identify the potential\nroutes for public transport service settings by detecting OD flow clusters\nwithin urban travel data.",
    "descriptor": "",
    "authors": [
      "Mengyuan Fang",
      "Luliang Tang",
      "Zihan Kan",
      "Xue Yang",
      "Tao Pei",
      "Qingquan Li",
      "Chaokui Li"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05436"
  },
  {
    "id": "arXiv:2106.05437",
    "title": "Data augmentation to improve robustness of image captioning solutions",
    "abstract": "In this paper, we study the impact of motion blur, a common quality flaw in\nreal world images, on a state-of-the-art two-stage image captioning solution,\nand notice a degradation in solution performance as blur intensity increases.\nWe investigate techniques to improve the robustness of the solution to motion\nblur using training data augmentation at each or both stages of the solution,\ni.e., object detection and captioning, and observe improved results. In\nparticular, augmenting both the stages reduces the CIDEr-D degradation for high\nmotion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to\n6.8 on Vizwiz dataset.",
    "descriptor": "\nComments: CVPR VizWiz 2021 workshop\n",
    "authors": [
      "Shashank Bujimalla",
      "Mahesh Subedar",
      "Omesh Tickoo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05437"
  },
  {
    "id": "arXiv:2106.05438",
    "title": "Cross-Modal Discrete Representation Learning",
    "abstract": "Recent advances in representation learning have demonstrated an ability to\nrepresent information from different modalities such as video, text, and audio\nin a single high-level embedding vector. In this work we present a\nself-supervised learning framework that is able to learn a representation that\ncaptures finer levels of granularity across different modalities such as\nconcepts or events represented by visual objects or spoken words. Our framework\nrelies on a discretized embedding space created via vector quantization that is\nshared across different modalities. Beyond the shared embedding space, we\npropose a Cross-Modal Code Matching objective that forces the representations\nfrom different views (modalities) to have a similar distribution over the\ndiscrete embedding space such that cross-modal objects/actions localization can\nbe performed without direct supervision. In our experiments we show that the\nproposed discretized multi-modal fine-grained representation (e.g.,\npixel/word/frame) can complement high-level summary representations (e.g.,\nvideo/sentence/waveform) for improved performance on cross-modal retrieval\ntasks. We also observe that the discretized representation uses individual\nclusters to represent the same semantic concept across modalities.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Alexander H. Liu",
      "SouYoung Jin",
      "Cheng-I Jeff Lai",
      "Andrew Rouditchenko",
      "Aude Oliva",
      "James Glass"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05438"
  },
  {
    "id": "arXiv:2106.05441",
    "title": "Unsupervised Video Person Re-identification via Noise and Hard frame  Aware Clustering",
    "abstract": "Unsupervised video-based person re-identification (re-ID) methods extract\nricher features from video tracklets than image-based ones. The\nstate-of-the-art methods utilize clustering to obtain pseudo-labels and train\nthe models iteratively. However, they underestimate the influence of two kinds\nof frames in the tracklet: 1) noise frames caused by detection errors or heavy\nocclusions exist in the tracklet, which may be allocated with unreliable labels\nduring clustering; 2) the tracklet also contains hard frames caused by pose\nchanges or partial occlusions, which are difficult to distinguish but\ninformative. This paper proposes a Noise and Hard frame Aware Clustering (NHAC)\nmethod. NHAC consists of a graph trimming module and a node re-sampling module.\nThe graph trimming module obtains stable graphs by removing noise frame nodes\nto improve the clustering accuracy. The node re-sampling module enhances the\ntraining of hard frame nodes to learn rich tracklet information. Experiments\nconducted on two video-based datasets demonstrate the effectiveness of the\nproposed NHAC under the unsupervised re-ID setting.",
    "descriptor": "\nComments: Appearing at ICME 2021\n",
    "authors": [
      "Pengyu Xie",
      "Xin Xu",
      "Zheng Wang",
      "Toshihiko Yamasaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05441"
  },
  {
    "id": "arXiv:2106.05444",
    "title": "Multiplicative perturbation bounds for the Generalized block Cholesky  downdating problem",
    "abstract": "The explicit expressions for the strong and the weak rigorous multiplicative\nperturbation bounds for the Generalized block Cholesky downdating problem are\nobtained. By bringing together the modified matrix-vector equation approach\nwith the method of Lyapunov majorant function and the Banach fixed point\ntheorem, we derived the strong rigorous multiplicative perturbation bounds. By\nusing the matrix-equation approach the weak rigorous multiplicative bounds are\npresented. Numerical experiments are provided to illustrate the obtained\nresults.",
    "descriptor": "",
    "authors": [
      "Mahvish Samara",
      "Aamir Farooq"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05444"
  },
  {
    "id": "arXiv:2106.05449",
    "title": "Investigating Alternatives to the Root Mean Square for Adaptive Gradient  Methods",
    "abstract": "Adam is an adaptive gradient method that has experienced widespread adoption\ndue to its fast and reliable training performance. Recent approaches have not\noffered significant improvement over Adam, often because they do not innovate\nupon one of its core features: normalization by the root mean square (RMS) of\nrecent gradients. However, as noted by Kingma and Ba (2015), any number of\n$L^p$ normalizations are possible, with the RMS corresponding to the specific\ncase of $p=2$. In our work, we theoretically and empirically characterize the\ninfluence of different $L^p$ norms on adaptive gradient methods for the first\ntime. We show mathematically how the choice of $p$ influences the size of the\nsteps taken, while leaving other desirable properties unaffected. We evaluate\nAdam with various $L^p$ norms on a suite of deep learning benchmarks, and find\nthat $p > 2$ consistently leads to improved learning speed and final\nperformance. The choices of $p=3$ or $p=6$ also match or outperform\nstate-of-the-art methods in all of our experiments.",
    "descriptor": "\nComments: 12 pages, 6 figures, 3 tables\n",
    "authors": [
      "Brett Daley",
      "Christopher Amato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05449"
  },
  {
    "id": "arXiv:2106.05450",
    "title": "Input Augmentation Improves Constrained Beam Search for Neural Machine  Translation: NTT at WAT 2021",
    "abstract": "This paper describes our systems that were submitted to the restricted\ntranslation task at WAT 2021. In this task, the systems are required to output\ntranslated sentences that contain all given word constraints. Our system\ncombined input augmentation and constrained beam search algorithms. Through\nexperiments, we found that this combination significantly improves translation\naccuracy and can save inference time while containing all the constraints in\nthe output. For both En->Ja and Ja->En, our systems obtained the best\nevaluation performances in automatic evaluation.",
    "descriptor": "\nComments: 9 pages, 4 figures, WAT 2021 Restricted Translation Task\n",
    "authors": [
      "Katsuki Chousa",
      "Makoto Morishita"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05450"
  },
  {
    "id": "arXiv:2106.05452",
    "title": "Nonlinear mixed-dimension model for embedded tubular networks with  application to root water uptake",
    "abstract": "We present a numerical scheme for the solution of nonlinear mixed-dimensional\nPDEs describing coupled processes in embedded tubular network system in\nexchange with a bulk domain. Such problems arise in various biological and\ntechnical applications such as in the modeling of root-water uptake, heat\nexchangers, or geothermal wells. The nonlinearity appears in form of\nsolution-dependent parameters such as pressure-dependent permeability or\ntemperature-dependent thermal conductivity. We derive and analyse a numerical\nscheme based on distributing the bulk-network coupling source term by a\nsmoothing kernel with local support. By the use of local analytical solutions,\ninterface unknowns and fluxes at the bulk-network interface can be accurately\nreconstructed from coarsely resolved numerical solutions in the bulk domain.\nNumerical examples give confidence in the robustness of the method and show the\nresults in comparison to previously published methods. The new method\noutperforms these existing methods in accuracy and efficiency. In a root water\nuptake scenario, we accurately estimate the transpiration rate using only a few\nthousand 3D mesh cells and a structured cube grid whereas other\nstate-of-the-art numerical schemes require millions of cells and local grid\nrefinement to reach comparable accuracy.",
    "descriptor": "\nComments: 34 pages, 16 figures\n",
    "authors": [
      "Timo Koch",
      "Hanchuan Wu",
      "Martin Schneider"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.05452"
  },
  {
    "id": "arXiv:2106.05453",
    "title": "Improving White-box Robustness of Pre-processing Defenses via Joint  Adversarial Training",
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. A range of\nadversarial defense techniques have been proposed to mitigate the interference\nof adversarial noise, among which the input pre-processing methods are scalable\nand show great potential to safeguard DNNs. However, pre-processing methods may\nsuffer from the robustness degradation effect, in which the defense reduces\nrather than improving the adversarial robustness of a target model in a\nwhite-box setting. A potential cause of this negative effect is that\nadversarial training examples are static and independent to the pre-processing\nmodel. To solve this problem, we investigate the influence of full adversarial\nexamples which are crafted against the full model, and find they indeed have a\npositive impact on the robustness of defenses. Furthermore, we find that simply\nchanging the adversarial training examples in pre-processing methods does not\ncompletely alleviate the robustness degradation effect. This is due to the\nadversarial risk of the pre-processed model being neglected, which is another\ncause of the robustness degradation effect. Motivated by above analyses, we\npropose a method called Joint Adversarial Training based Pre-processing (JATP)\ndefense. Specifically, we formulate a feature similarity based adversarial risk\nfor the pre-processing model by using full adversarial examples found in a\nfeature space. Unlike standard adversarial training, we only update the\npre-processing model, which prompts us to introduce a pixel-wise loss to\nimprove its cross-model transferability. We then conduct a joint adversarial\ntraining on the pre-processing model to minimize this overall risk. Empirical\nresults show that our method could effectively mitigate the robustness\ndegradation effect across different target models in comparison to previous\nstate-of-the-art approaches.",
    "descriptor": "",
    "authors": [
      "Dawei Zhou",
      "Nannan Wang",
      "Xinbo Gao",
      "Bo Han",
      "Jun Yu",
      "Xiaoyu Wang",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05453"
  },
  {
    "id": "arXiv:2106.05455",
    "title": "Graph Symbiosis Learning",
    "abstract": "We introduce a framework for learning from multiple generated graph views,\nnamed graph symbiosis learning (GraphSym). In GraphSym, graph neural networks\n(GNN) developed in multiple generated graph views can adaptively exchange\nparameters with each other and fuse information stored in linkage structures\nand node features. Specifically, we propose a novel adaptive exchange method to\niteratively substitute redundant channels in the weight matrix of one GNN with\ninformative channels of another GNN in a layer-by-layer manner. GraphSym does\nnot rely on specific methods to generate multiple graph views and GNN\narchitectures. Thus, existing GNNs can be seamlessly integrated into our\nframework. On 3 semi-supervised node classification datasets, GraphSym\noutperforms previous single-graph and multiple-graph GNNs without knowledge\ndistillation, and achieves new state-of-the-art results. We also conduct a\nseries of experiments on 15 public benchmarks, 8 popular GNN models, and 3\ngraph tasks -- node classification, graph classification, and edge prediction\n-- and show that GraphSym consistently achieves better performance than\nexisting popular GNNs by 1.9\\%$\\sim$3.9\\% on average and their ensembles.\nExtensive ablation studies and experiments on the few-shot setting also\ndemonstrate the effectiveness of GraphSym.",
    "descriptor": "",
    "authors": [
      "Liang Zeng",
      "Jin Xu",
      "Zijun Yao",
      "Yanqiao Zhu",
      "Jian Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05455"
  },
  {
    "id": "arXiv:2106.05458",
    "title": "Joint Landmark and Structure Learning for Automatic Evaluation of  Developmental Dysplasia of the Hip",
    "abstract": "The ultrasound (US) screening of the infant hip is vital for the early\ndiagnosis of developmental dysplasia of the hip (DDH). The US diagnosis of DDH\nrefers to measuring alpha and beta angles that quantify hip joint development.\nThese two angles are calculated from key anatomical landmarks and structures of\nthe hip. However, this measurement process is not trivial for sonographers and\nusually requires a thorough understanding of complex anatomical structures. In\nthis study, we propose a multi-task framework to learn the relationships among\nlandmarks and structures jointly and automatically evaluate DDH. Our multi-task\nnetworks are equipped with three novel modules. Firstly, we adopt Mask R-CNN as\nthe basic framework to detect and segment key anatomical structures and add one\nlandmark detection branch to form a new multi-task framework. Secondly, we\npropose a novel shape similarity loss to refine the incomplete anatomical\nstructure prediction robustly and accurately. Thirdly, we further incorporate\nthe landmark-structure consistent prior to ensure the consistency of the bony\nrim estimated from the segmented structure and the detected landmark. In our\nexperiments, 1,231 US images of the infant hip from 632 patients are collected,\nof which 247 images from 126 patients are tested. The average errors in alpha\nand beta angles are 2.221 degrees and 2.899 degrees. About 93% and 85%\nestimates of alpha and beta angles have errors less than 5 degrees,\nrespectively. Experimental results demonstrate that the proposed method can\naccurately and robustly realize the automatic evaluation of DDH, showing great\npotential for clinical application.",
    "descriptor": "\nComments: Accepted by IEEE Journal of Biomedical and Health Informatics. 14 pages, 10 figures and 10 tables\n",
    "authors": [
      "Xindi Hu",
      "Limin Wang",
      "Xin Yang",
      "Xu Zhou",
      "Wufeng Xue",
      "Yan Cao",
      "Shengfeng Liu",
      "Yuhao Huang",
      "Shuangping Guo",
      "Ning Shang",
      "Dong Ni",
      "Ning Gu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05458"
  },
  {
    "id": "arXiv:2106.05459",
    "title": "Mode recovery in neural autoregressive sequence modeling",
    "abstract": "Despite its wide use, recent studies have revealed unexpected and undesirable\nproperties of neural autoregressive sequence models trained with maximum\nlikelihood, such as an unreasonably high affinity to short sequences after\ntraining and to infinitely long sequences at decoding time. We propose to study\nthese phenomena by investigating how the modes, or local maxima, of a\ndistribution are maintained throughout the full learning chain of the\nground-truth, empirical, learned and decoding-induced distributions, via the\nnewly proposed mode recovery cost. We design a tractable testbed where we build\nthree types of ground-truth distributions: (1) an LSTM based structured\ndistribution, (2) an unstructured distribution where probability of a sequence\ndoes not depend on its content, and (3) a product of these two which we call a\nsemi-structured distribution. Our study reveals both expected and unexpected\nfindings. First, starting with data collection, mode recovery cost strongly\nrelies on the ground-truth distribution and is most costly with the\nsemi-structured distribution. Second, after learning, mode recovery cost from\nthe ground-truth distribution may increase or decrease compared to data\ncollection, with the largest cost degradation occurring with the\nsemi-structured ground-truth distribution. Finally, the ability of the\ndecoding-induced distribution to recover modes from the learned distribution is\nhighly impacted by the choices made earlier in the learning chain. We conclude\nthat future research must consider the entire learning chain in order to fully\nunderstand the potentials and perils and to further improve neural\nautoregressive sequence models.",
    "descriptor": "\nComments: ACL-IJCNLP 2021 5th Workshop on Structured Prediction for NLP\n",
    "authors": [
      "Ilia Kulikov",
      "Sean Welleck",
      "Kyunghyun Cho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05459"
  },
  {
    "id": "arXiv:2106.05463",
    "title": "Cross-chain Interaction Model In a Fully Verified Way",
    "abstract": "There are different kinds of blockchains, which have been applied in various\nareas. Blockchains are relatively independent systems that are apt to form\nisolated data islands. Then cross-chain interaction is proposed to connect\ndifferent blockchains. However, the current cross-chain methods do not maintain\nthe security of the original blockchain. They either depend on a less secure\nthird-party system or a less secure method. This makes the cross-chain\ninteraction less secure than the original blockchains (the security downgrade\nissues), or the cross-chain interaction can be done even if the paired\nblockchain does not exist (the blockchain invisible issue). In this paper, we\nfirst propose a system interaction model and use it to analyze the possible\nsecurity issues. Based on conclusions got from the proposed model, we propose\nthe cross-chain method that verifies the data of the paired blockchain by the\nconsensus algorithm of the paired blockchain (the CIFuV method). With this\nmethod, the cross-chain interaction can be as the same security as in the\npaired blockchain. At last, we evaluate the security issues during the system\ninteraction process, and the possibility to have the CIFuV model on the public\nblockchains.",
    "descriptor": "",
    "authors": [
      "Hong Su"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.05463"
  },
  {
    "id": "arXiv:2106.05467",
    "title": "NetFC: enabling accurate floating-point arithmetic on programmable  switches",
    "abstract": "In-network computation has been widely used to accelerate data-intensive\ndistributed applications. Some computational tasks, traditional performed on\nservers, are offloaded to the network (i.e. programmable switches). However,\nthe computational capacity of programmable switches is limited to simple\ninteger arithmetic operations while many of applications require on-the-fly\nfloating-point operations. To address this issue, prior approaches either adopt\na float-to-integer method or directly offload computational tasks to the local\nCPUs of switches, incurring accuracy loss and delayed processing. To this end,\nwe propose NetFC, a table-lookup method to achieve on-the-fly in-network\nfloating-point arithmetic operations nearly without accuracy loss. NetFC adopts\na divide-and-conquer mechanism that converts the original huge table into\nseveral much small tables together with some integer operations. NetFC adopts a\nscaling-factor mechanism for computational accuracy improvement, and a\nprefix-based lossless table compression method to reduce the memory overhead.\nWe use different types of datasets to evaluate NetFC. The experimental results\nshow that the average accuracy of NetFC can be as high as up to 99.94% at worst\nwith only 448KB memory consumption. Furthermore, we integrate NetFC into Sonata\nfor detecting Slowloris attack, yielding significant decrease of detection\ndelay.",
    "descriptor": "\nComments: 10 pages body, 11 pages total\n",
    "authors": [
      "Penglai Cui",
      "Heng Pan",
      "Zhenyu Li",
      "Jiaoren Wu",
      "Shengzhuo Zhang",
      "Xingwu Yang",
      "Hongtao Guan",
      "Gaogang Xie"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05467"
  },
  {
    "id": "arXiv:2106.05468",
    "title": "Multi-VFL: A Vertical Federated Learning System for Multiple Data and  Label Owners",
    "abstract": "Vertical Federated Learning (VFL) refers to the collaborative training of a\nmodel on a dataset where the features of the dataset are split among multiple\ndata owners, while label information is owned by a single data owner. In this\npaper, we propose a novel method, Multi Vertical Federated Learning\n(Multi-VFL), to train VFL models when there are multiple data and label owners.\nOur approach is the first to consider the setting where $D$-data owners (across\nwhich features are distributed) and $K$-label owners (across which labels are\ndistributed) exist. This proposed configuration allows different entities to\ntrain and learn optimal models without having to share their data. Our\nframework makes use of split learning and adaptive federated optimizers to\nsolve this problem. For empirical evaluation, we run experiments on the MNIST\nand FashionMNIST datasets. Our results show that using adaptive optimizers for\nmodel aggregation fastens convergence and improves accuracy.",
    "descriptor": "",
    "authors": [
      "Vaikkunth Mugunthan",
      "Pawan Goyal",
      "Lalana Kagal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05468"
  },
  {
    "id": "arXiv:2106.05469",
    "title": "Variational Information Bottleneck for Effective Low-Resource  Fine-Tuning",
    "abstract": "While large-scale pretrained language models have obtained impressive results\nwhen fine-tuned on a wide variety of tasks, they still often suffer from\noverfitting in low-resource scenarios. Since such models are general-purpose\nfeature extractors, many of these features are inevitably irrelevant for a\ngiven target task. We propose to use Variational Information Bottleneck (VIB)\nto suppress irrelevant features when fine-tuning on low-resource target tasks,\nand show that our method successfully reduces overfitting. Moreover, we show\nthat our VIB model finds sentence representations that are more robust to\nbiases in natural language inference datasets, and thereby obtains better\ngeneralization to out-of-domain datasets. Evaluation on seven low-resource\ndatasets in different tasks shows that our method significantly improves\ntransfer learning in low-resource scenarios, surpassing prior work. Moreover,\nit improves generalization on 13 out of 15 out-of-domain natural language\ninference benchmarks. Our code is publicly available in\nhttps://github.com/rabeehk/vibert.",
    "descriptor": "\nComments: ICLR, 2021\n",
    "authors": [
      "Rabeeh Karimi Mahabadi",
      "Yonatan Belinkov",
      "James Henderson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05469"
  },
  {
    "id": "arXiv:2106.05470",
    "title": "Automated Self-Supervised Learning for Graphs",
    "abstract": "Graph self-supervised learning has gained increasing attention due to its\ncapacity to learn expressive node representations. Many pretext tasks, or loss\nfunctions have been designed from distinct perspectives. However, we observe\nthat different pretext tasks affect downstream tasks differently cross\ndatasets, which suggests that searching pretext tasks is crucial for graph\nself-supervised learning. Different from existing works focusing on designing\nsingle pretext tasks, this work aims to investigate how to automatically\nleverage multiple pretext tasks effectively. Nevertheless, evaluating\nrepresentations derived from multiple pretext tasks without direct access to\nground truth labels makes this problem challenging. To address this obstacle,\nwe make use of a key principle of many real-world graphs, i.e., homophily, or\nthe principle that ``like attracts like,'' as the guidance to effectively\nsearch various self-supervised pretext tasks. We provide theoretical\nunderstanding and empirical evidence to justify the flexibility of homophily in\nthis search task. Then we propose the AutoSSL framework which can automatically\nsearch over combinations of various self-supervised tasks. By evaluating the\nframework on 7 real-world datasets, our experimental results show that AutoSSL\ncan significantly boost the performance on downstream tasks including node\nclustering and node classification compared with training under individual\ntasks. Code will be released at https://github.com/ChandlerBang/AutoSSL.",
    "descriptor": "",
    "authors": [
      "Wei Jin",
      "Xiaorui Liu",
      "Xiangyu Zhao",
      "Yao Ma",
      "Neil Shah",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05470"
  },
  {
    "id": "arXiv:2106.05473",
    "title": "Stream processors and comodels",
    "abstract": "In 2009, Ghani, Hancock and Pattinson gave a coalgebraic characterisation of\nstream processors $A^\\mathbb{N} \\to B^\\mathbb{N}$ drawing on ideas of\nBrouwerian constructivism. Their stream processors have an intensional\ncharacter; in this paper, we give a corresponding coalgebraic characterisation\nof extensional stream processors, i.e., the set of continuous functions\n$A^\\mathbb{N} \\to B^\\mathbb{N}$. Our account sites both our result and that of\nop. cit. within the apparatus of comodels for algebraic effects originating\nwith Power-Shkaravska.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Richard Garner"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2106.05473"
  },
  {
    "id": "arXiv:2106.05475",
    "title": "Jointly Optimize Coding and Node Selection for Distributed Computing  over Wireless Edge Networks",
    "abstract": "This work aims to jointly optimize the coding and node selection to minimize\nthe processing time for distributed computing tasks over wireless edge\nnetworks. Since the joint optimization problem formulation is NP-hard and\nnonlinear, we leverage the discrete characteristic of its decision variables to\ntransform the problem into an equivalent linear formulation. This linearization\ncan guarantee to find the optimal solutions and significantly reduce the\nproblem's complexity. Simulations based on real-world datasets show that the\nproposed approach can reduce the total processing time up to 2.3 times compared\nwith that of state-of-the-art approach.",
    "descriptor": "",
    "authors": [
      "Cong T. Nguyen",
      "Diep N. Nguyen",
      "Dinh Thai Hoang",
      "Hoang-Anh Pham",
      "Eryk Dutkiewicz"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.05475"
  },
  {
    "id": "arXiv:2106.05476",
    "title": "Learning Based Proximity Matrix Factorization for Node Embedding",
    "abstract": "Node embedding learns a low-dimensional representation for each node in the\ngraph. Recent progress on node embedding shows that proximity matrix\nfactorization methods gain superb performance and scale to large graphs with\nmillions of nodes. Existing approaches first define a proximity matrix and then\nlearn the embeddings that fit the proximity by matrix factorization. Most\nexisting matrix factorization methods adopt the same proximity for different\ntasks, while it is observed that different tasks and datasets may require\ndifferent proximity, limiting their representation power.\nMotivated by this, we propose {\\em Lemane}, a framework with trainable\nproximity measures, which can be learned to best suit the datasets and tasks at\nhand automatically. Our method is end-to-end, which incorporates differentiable\nSVD in the pipeline so that the parameters can be trained via backpropagation.\nHowever, this learning process is still expensive on large graphs. To improve\nthe scalability, we train proximity measures only on carefully subsampled\ngraphs, and then apply standard proximity matrix factorization on the original\ngraph using the learned proximity. Note that, computing the learned proximities\nfor each pair is still expensive for large graphs, and existing techniques for\ncomputing proximities are not applicable to the learned proximities. Thus, we\npresent generalized push techniques to make our solution scalable to large\ngraphs with millions of nodes. Extensive experiments show that our proposed\nsolution outperforms existing solutions on both link prediction and node\nclassification tasks on almost all datasets.",
    "descriptor": "\nComments: To appear in SIGKDD 2021\n",
    "authors": [
      "Xingyi Zhang",
      "Kun Xie",
      "Sibo Wang",
      "Zengfeng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05476"
  },
  {
    "id": "arXiv:2106.05478",
    "title": "Semantic-aware Binary Code Representation with BERT",
    "abstract": "A wide range of binary analysis applications, such as bug discovery, malware\nanalysis and code clone detection, require recovery of contextual meanings on a\nbinary code. Recently, binary analysis techniques based on machine learning\nhave been proposed to automatically reconstruct the code representation of a\nbinary instead of manually crafting specifics of the analysis algorithm.\nHowever, the existing approaches utilizing machine learning are still\nspecialized to solve one domain of problems, rendering recreation of models for\ndifferent types of binary analysis. In this paper, we propose DeepSemantic\nutilizing BERT in producing the semantic-aware code representation of a binary\ncode.\nTo this end, we introduce well-balanced instruction normalization that holds\nrich information for each of instructions yet minimizing an out-of-vocabulary\n(OOV) problem. DeepSemantic has been carefully designed based on our study with\nlarge swaths of binaries. Besides, DeepSemantic leverages the essence of the\nBERT architecture into re-purposing a pre-trained generic model that is readily\navailable as a one-time processing, followed by quickly applying specific\ndownstream tasks with a fine-tuning process. We demonstrate DeepSemantic with\ntwo downstream tasks, namely, binary similarity comparison and compiler\nprovenance (i.e., compiler and optimization level) prediction. Our experimental\nresults show that the binary similarity model outperforms two state-of-the-art\nbinary similarity tools, DeepBinDiff and SAFE, 49.84% and 15.83% on average,\nrespectively.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Hyungjoon Koo",
      "Soyeon Park",
      "Daejin Choi",
      "Taesoo Kim"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05478"
  },
  {
    "id": "arXiv:2106.05480",
    "title": "Lower Bounds on Metropolized Sampling Methods for Well-Conditioned  Distributions",
    "abstract": "We give lower bounds on the performance of two of the most popular sampling\nmethods in practice, the Metropolis-adjusted Langevin algorithm (MALA) and\nmulti-step Hamiltonian Monte Carlo (HMC) with a leapfrog integrator, when\napplied to well-conditioned distributions. Our main result is a nearly-tight\nlower bound of $\\widetilde{\\Omega}(\\kappa d)$ on the mixing time of MALA from\nan exponentially warm start, matching a line of algorithmic results up to\nlogarithmic factors and answering an open question of Chewi et. al. We also\nshow that a polynomial dependence on dimension is necessary for the relaxation\ntime of HMC under any number of leapfrog steps, and bound the gains achievable\nby changing the step count. Our HMC analysis draws upon a novel connection\nbetween leapfrog integration and Chebyshev polynomials, which may be of\nindependent interest.",
    "descriptor": "\nComments: 47 pages, 1 figure, comments welcome!\n",
    "authors": [
      "Yin Tat Lee",
      "Ruoqi Shen",
      "Kevin Tian"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05480"
  },
  {
    "id": "arXiv:2106.05481",
    "title": "Data Clustering-Driven Neural Network for Intra Prediction",
    "abstract": "As a crucial part of video compression, intra prediction utilizes local\ninformation of images to eliminate the redundancy in spatial domain. In both\nH.265/HEVC and H.266/VVC, multiple directional prediction modes are employed to\nfind the texture trend of each small block and then the prediction is made\nbased on reference samples in the selected direction. Recently, the intra\nprediction schemes based on neural networks have achieved great success. In\nthese methods, the networks are trained and applied to intra prediction in\naddition to the directional prediction modes. In this paper, we propose a novel\ndata clustering-driven neural network (dubbed DCDNN) for intra prediction,\nwhich can learn deep features of the clustered data. In DCDNN, each network can\nbe split into two networks by adding or subtracting Gaussian random noise. Then\na data clustering-driven training is applied to train all the derived networks\nrecursively. In each iteration, the entire training dataset is partitioned\naccording to the recovery qualities of the derived networks. For the\nexperiment, DCDNN is implemented into HEVC reference software HM-16.9. The\nexperimental results demonstrate that DCDNN can reach an average of 4.2%\nBjontegaard distortion rate (BDrate) improvement (up to 7.0%) over HEVC with\nall intra configuration. Compared with existing fully connected networkbased\nintra prediction methods, the bitrate saving performance is further improved.",
    "descriptor": "",
    "authors": [
      "Hengyu Man",
      "Xiaopeng Fan",
      "Ruiqin Xiong",
      "Debin Zhao"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.05481"
  },
  {
    "id": "arXiv:2106.05482",
    "title": "Deep Position-wise Interaction Network for CTR Prediction",
    "abstract": "Click-through rate (CTR) prediction plays an important role in online\nadvertising and recommender systems. In practice, the training of CTR models\ndepends on click data which is intrinsically biased towards higher positions\nsince higher position has higher CTR by nature. Existing methods such as actual\nposition training with fixed position inference and inverse propensity weighted\ntraining with no position inference alleviate the bias problem to some extend.\nHowever, the different treatment of position information between training and\ninference will inevitably lead to inconsistency and sub-optimal online\nperformance. Meanwhile, the basic assumption of these methods, i.e., the click\nprobability is the product of examination probability and relevance\nprobability, is oversimplified and insufficient to model the rich interaction\nbetween position and other information. In this paper, we propose a Deep\nPosition-wise Interaction Network (DPIN) to efficiently combine all candidate\nitems and positions for estimating CTR at each position, achieving consistency\nbetween offline and online as well as modeling the deep non-linear interaction\namong position, user, context and item under the limit of serving performance.\nFollowing our new treatment to the position bias in CTR prediction, we propose\na new evaluation metrics named PAUC (position-wise AUC) that is suitable for\nmeasuring the ranking quality at a given position. Through extensive\nexperiments on a real world dataset, we show empirically that our method is\nboth effective and efficient in solving position bias problem. We have also\ndeployed our method in production and observed statistically significant\nimprovement over a highly optimized baseline in a rigorous A/B test.",
    "descriptor": "\nComments: Accepted by SIGIR 2021\n",
    "authors": [
      "Jianqiang Huang",
      "Ke Hu",
      "Qingtao Tang",
      "Mingjian Chen",
      "Yi Qi",
      "Jia Cheng",
      "Jun Lei"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05482"
  },
  {
    "id": "arXiv:2106.05483",
    "title": "On the 4-adic complexity of the two-prime quaternary generator",
    "abstract": "R. Hofer and A. Winterhof proved that the 2-adic complexity of the two-prime\n(binary) generator of period $pq$ with two odd primes $p\\neq q$ is close to its\nperiod and it can attain the maximum in many cases.\nWhen the two-prime generator is applied to producing quaternary sequences, we\nneed to determine the 4-adic complexity. We present the formulae of possible\nvalues of the 4-adic complexity, which is larger than $pq-\\log_4(pq^2)-1$ if\n$p<q$. So it is good enough to resist the attack of the rational approximation\nalgorithm.",
    "descriptor": "",
    "authors": [
      "Vladimir Edemskiy",
      "Zhixiong Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2106.05483"
  },
  {
    "id": "arXiv:2106.05484",
    "title": "A Unified Framework for Task-Driven Data Quality Management",
    "abstract": "High-quality data is critical to train performant Machine Learning (ML)\nmodels, highlighting the importance of Data Quality Management (DQM). Existing\nDQM schemes often cannot satisfactorily improve ML performance because, by\ndesign, they are oblivious to downstream ML tasks. Besides, they cannot handle\nvarious data quality issues (especially those caused by adversarial attacks)\nand have limited applications to only certain types of ML models. Recently,\ndata valuation approaches (e.g., based on the Shapley value) have been\nleveraged to perform DQM; yet, empirical studies have observed that their\nperformance varies considerably based on the underlying data and training\nprocess. In this paper, we propose a task-driven, multi-purpose, model-agnostic\nDQM framework, DataSifter, which is optimized towards a given downstream ML\ntask, capable of effectively removing data points with various defects, and\napplicable to diverse models. Specifically, we formulate DQM as an optimization\nproblem and devise a scalable algorithm to solve it. Furthermore, we propose a\ntheoretical framework for comparing the worst-case performance of different DQM\nstrategies. Remarkably, our results show that the popular strategy based on the\nShapley value may end up choosing the worst data subset in certain practical\nscenarios. Our evaluation shows that DataSifter achieves and most often\nsignificantly improves the state-of-the-art performance over a wide range of\nDQM tasks, including backdoor, poison, noisy/mislabel data detection, data\nsummarization, and data debiasing.",
    "descriptor": "",
    "authors": [
      "Tianhao Wang",
      "Yi Zeng",
      "Ming Jin",
      "Ruoxi Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05484"
  },
  {
    "id": "arXiv:2106.05485",
    "title": "VaLiPro: Linear Programming Validator for Cluster Computing Systems",
    "abstract": "The article presents and evaluates a scalable algorithm for validating\nsolutions of linear programming problems on cluster computing systems. The main\nidea of the method is to generate a regular set of points (validation set) on a\nsmall-radius hypersphere centered at the point of the solution under\nvalidation. The objective function is calculated for each point of the\nvalidation set that belongs to the feasible region. If all these values are\nless than or equal to the value of the objective function at the point under\nvalidation, then this point is the correct solution. The parallel\nimplementation of the VaLiPro algorithm is performed in C++ through the\nparallel BSF-skeleton, which encapsulates all aspects related to the MPI-based\nparallelization of the program. We provide the results of large-scale\ncomputational experiments on a cluster computing system to study the\nscalability of the VaLiPro algorithm.",
    "descriptor": "\nComments: Submitted to \"Supercomputing Frontiers and Innovations\" journal\n",
    "authors": [
      "Leonid B. Sokolinsky",
      "Irina M. Sokolinskaya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.05485"
  },
  {
    "id": "arXiv:2106.05487",
    "title": "RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation",
    "abstract": "The segmentation of nanoscale electron microscopy (EM) images is crucial but\nchallenging in connectomics. Recent advances in deep learning have demonstrated\nthe significant potential of automatic segmentation for tera-scale EM images.\nHowever, none of the existing segmentation methods are error-free, and they\nrequire proofreading, which is typically implemented as an interactive,\nsemi-automatic process via manual intervention. Herein, we propose a fully\nautomatic proofreading method based on reinforcement learning. The main idea is\nto model the human decision process in proofreading using a reinforcement agent\nto achieve fully automatic proofreading. We systematically design the proposed\nsystem by combining multiple reinforcement learning agents in a hierarchical\nmanner, where each agent focuses only on a specific task while preserving\ndependency between agents. Furthermore, we also demonstrate that the episodic\ntask setting of reinforcement learning can efficiently manage a combination of\nmerge and split errors concurrently presented in the input. We demonstrate the\nefficacy of the proposed system by comparing it with state-of-the-art\nproofreading methods using various testing examples.",
    "descriptor": "\nComments: Submitted to MICCAI 2021\n",
    "authors": [
      "Khoa Tuan Nguyen",
      "Ganghee Jang",
      "Won-ki Jeong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05487"
  },
  {
    "id": "arXiv:2106.05489",
    "title": "Convex Risk Bounded Continuous-Time Trajectory Planning in Uncertain  Nonconvex Environments",
    "abstract": "In this paper, we address the trajectory planning problem in uncertain\nnonconvex static and dynamic environments that contain obstacles with\nprobabilistic location, size, and geometry. To address this problem, we provide\na risk bounded trajectory planning method that looks for continuous-time\ntrajectories with guaranteed bounded risk over the planning time horizon. Risk\nis defined as the probability of collision with uncertain obstacles. Existing\napproaches to address risk bounded trajectory planning problems either are\nlimited to Gaussian uncertainties and convex obstacles or rely on\nsampling-based methods that need uncertainty samples and time discretization.\nTo address the risk bounded trajectory planning problem, we leverage the notion\nof risk contours to transform the risk bounded planning problem into a\ndeterministic optimization problem. Risk contours are the set of all points in\nthe uncertain environment with guaranteed bounded risk. The obtained\ndeterministic optimization is, in general, nonlinear and nonconvex time-varying\noptimization. We provide convex methods based on sum-of-squares optimization to\nefficiently solve the obtained nonconvex time-varying optimization problem and\nobtain the continuous-time risk bounded trajectories without time\ndiscretization. The provided approach deals with arbitrary probabilistic\nuncertainties, nonconvex and nonlinear, static and dynamic obstacles, and is\nsuitable for online trajectory planning problems.",
    "descriptor": "\nComments: Robotics: Science and Systems (RSS) 2021\n",
    "authors": [
      "Ashkan Jasour",
      "Weiqiao Han",
      "Brian Williams"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05489"
  },
  {
    "id": "arXiv:2106.05491",
    "title": "Hybrid Spherical- and Planar-Wave Channel Modeling and DCNN-powered  Estimation for Terahertz Ultra-massive MIMO Systems",
    "abstract": "The Terahertz band is envisioned to meet the demanding 100 Gbps data rates\nfor 6G wireless communications. Aiming at combating the distance limitation\nproblem with low hardware-cost, ultra-massive MIMO with hybrid beamforming is\npromising. However, relationships among wavelength, array size and antenna\nspacing give rise to the inaccuracy of planar-wave channel model (PWM), while\nan enlarged channel matrix dimension leads to excessive parameters of applying\nspherical-wave channel model (SWM). Moreover, due to the adoption of hybrid\nbeamforming, channel estimation (CE) needs to recover high-dimensional channels\nfrom severely compressed channel observation. In this paper, a hybrid\nspherical- and planar-wave channel model (HSPM) is investigated and proved to\nbe accurate and efficient by adopting PWM within subarray and SWM among\nsubarray. Furthermore, a two-phase HSPM CE mechanism is developed. A deep\nconvolutional-neural-network (DCNN) is designed in the first phase for\nparameter estimation of reference subarrays, while geometric relationships of\nthe remaining channel parameters between reference subarrays are leveraged to\ncomplete CE in the second phase. Extensive numerical results demonstrate the\nHSPM is accurate at various communication distances, array sizes and carrier\nfrequencies.The DCNN converges fast and achieves high accuracy with 5.2 dB\nimproved normalized-mean-square-error compared to literature methods, and owns\nsubstantially low complexity.",
    "descriptor": "",
    "authors": [
      "Yuhang Chen",
      "Longfei Yan",
      "Chong Han"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.05491"
  },
  {
    "id": "arXiv:2106.05492",
    "title": "ERMAS: Becoming Robust to Reward Function Sim-to-Real Gaps in  Multi-Agent Simulations",
    "abstract": "Multi-agent simulations provide a scalable environment for learning policies\nthat interact with rational agents. However, such policies may fail to\ngeneralize to the real-world where agents may differ from simulated\ncounterparts due to unmodeled irrationality and misspecified reward functions.\nWe introduce Epsilon-Robust Multi-Agent Simulation (ERMAS), a robust\noptimization framework for learning AI policies that are robust to such\nmultiagent sim-to-real gaps. While existing notions of multi-agent robustness\nconcern perturbations in the actions of agents, we address a novel robustness\nobjective concerning perturbations in the reward functions of agents. ERMAS\nprovides this robustness by anticipating suboptimal behaviors from other\nagents, formalized as the worst-case epsilon-equilibrium. We show empirically\nthat ERMAS yields robust policies for repeated bimatrix games and optimal\ntaxation problems in economic simulations. In particular, in the two-level RL\nproblem posed by the AI Economist (Zheng et al., 2020) ERMAS learns tax\npolicies that are robust to changes in agent risk aversion, improving social\nwelfare by up to 15% in complex spatiotemporal simulations.",
    "descriptor": "",
    "authors": [
      "Eric Zhao",
      "Alexander R. Trott",
      "Caiming Xiong",
      "Stephan Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05492"
  },
  {
    "id": "arXiv:2106.05495",
    "title": "Distance Metric Learning through Minimization of the Free Energy",
    "abstract": "Distance metric learning has attracted a lot of interest for solving machine\nlearning and pattern recognition problems over the last decades. In this work\nwe present a simple approach based on concepts from statistical physics to\nlearn optimal distance metric for a given problem. We formulate the task as a\ntypical statistical physics problem: distances between patterns represent\nconstituents of a physical system and the objective function corresponds to\nenergy. Then we express the problem as a minimization of the free energy of a\ncomplex system, which is equivalent to distance metric learning. Much like for\nmany problems in physics, we propose an approach based on Metropolis Monte\nCarlo to find the best distance metric. This provides a natural way to learn\nthe distance metric, where the learning process can be intuitively seen as\nstretching and rotating the metric space until some heuristic is satisfied. Our\nproposed method can handle a wide variety of constraints including those with\nspurious local minima. The approach works surprisingly well with stochastic\nnearest neighbors from neighborhood component analysis (NCA). Experimental\nresults on artificial and real-world data sets reveal a clear superiority over\na number of state-of-the-art distance metric learning methods for nearest\nneighbors classification.",
    "descriptor": "",
    "authors": [
      "Dusan Stosic",
      "Darko Stosic",
      "Teresa B. Ludermir",
      "Borko Stosic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05495"
  },
  {
    "id": "arXiv:2106.05497",
    "title": "Hyperspace Neighbor Penetration Approach to Dynamic Programming for  Model-Based Reinforcement Learning Problems with Slowly Changing Variables in  A Continuous State Space",
    "abstract": "Slowly changing variables in a continuous state space constitute an important\ncategory of reinforcement learning and see its application in many domains,\nsuch as modeling a climate control system where temperature, humidity, etc.\nchange slowly over time. However, this subject is less addressed in recent\nstudies. Classical methods with certain variants, such as Dynamic Programming\nwith Tile Coding which discretizes the state space, fail to handle slowly\nchanging variables because those methods cannot capture the tiny changes in\neach transition step, as it is computationally expensive or impossible to\nestablish an extremely granular grid system. In this paper, we introduce a\nHyperspace Neighbor Penetration (HNP) approach that solves the problem. HNP\ncaptures in each transition step the state's partial \"penetration\" into its\nneighboring hyper-tiles in the gridded hyperspace, thus does not require the\ntransition to be inter-tile in order for the change to be captured. Therefore,\nHNP allows for a very coarse grid system, which makes the computation feasible.\nHNP assumes near linearity of the transition function in a local space, which\nis commonly satisfied. In summary, HNP can be orders of magnitude more\nefficient than classical method in handling slowly changing variables in\nreinforcement learning. We have made an industrial implementation of NHP with a\ngreat success.",
    "descriptor": "\nComments: 8 pages, 7 figures\n",
    "authors": [
      "Vincent Zha",
      "Ivey Chiu",
      "Alexandre Guilbault",
      "Jaime Tatis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05497"
  },
  {
    "id": "arXiv:2106.05498",
    "title": "It's COMPASlicated: The Messy Relationship between RAI Datasets and  Algorithmic Fairness Benchmarks",
    "abstract": "Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS\ndataset, are commonly used in algorithmic fairness papers due to benchmarking\npractices of comparing algorithms on datasets used in prior work. In many\ncases, this data is used as a benchmark to demonstrate good performance without\naccounting for the complexities of criminal justice (CJ) processes. We show\nthat pretrial RAI datasets contain numerous measurement biases and errors\ninherent to CJ pretrial evidence and due to disparities in discretion and\ndeployment, are limited in making claims about real-world outcomes, making the\ndatasets a poor fit for benchmarking under assumptions of ground truth and\nreal-world impact. Conventional practices of simply replicating previous data\nexperiments may implicitly inherit or edify normative positions without\nexplicitly interrogating assumptions. With context of how interdisciplinary\nfields have engaged in CJ research, algorithmic fairness practices are\nmisaligned for meaningful contribution in the context of CJ, and would benefit\nfrom transparent engagement with normative considerations and values related to\nfairness, justice, and equality. These factors prompt questions about whether\nbenchmarks for intrinsically socio-technical systems like the CJ system can\nexist in a beneficial and ethical way.",
    "descriptor": "",
    "authors": [
      "Michelle Bao",
      "Angela Zhou",
      "Samantha Zottola",
      "Brian Brubach",
      "Sarah Desmarais",
      "Aaron Horowitz",
      "Kristian Lum",
      "Suresh Venkatasubramanian"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05498"
  },
  {
    "id": "arXiv:2106.05499",
    "title": "AFAN: Augmented Feature Alignment Network for Cross-Domain Object  Detection",
    "abstract": "Unsupervised domain adaptation for object detection is a challenging problem\nwith many real-world applications. Unfortunately, it has received much less\nattention than supervised object detection. Models that try to address this\ntask tend to suffer from a shortage of annotated training samples. Moreover,\nexisting methods of feature alignments are not sufficient to learn\ndomain-invariant representations. To address these limitations, we propose a\nnovel augmented feature alignment network (AFAN) which integrates intermediate\ndomain image generation and domain-adversarial training into a unified\nframework. An intermediate domain image generator is proposed to enhance\nfeature alignments by domain-adversarial training with automatically generated\nsoft domain labels. The synthetic intermediate domain images progressively\nbridge the domain divergence and augment the annotated source domain training\ndata. A feature pyramid alignment is designed and the corresponding feature\ndiscriminator is used to align multi-scale convolutional features of different\nsemantic levels. Last but not least, we introduce a region feature alignment\nand an instance discriminator to learn domain-invariant features for object\nproposals. Our approach significantly outperforms the state-of-the-art methods\non standard benchmarks for both similar and dissimilar domain adaptations.\nFurther extensive experiments verify the effectiveness of each component and\ndemonstrate that the proposed network can learn domain-invariant\nrepresentations.",
    "descriptor": "",
    "authors": [
      "Hongsong Wang",
      "Shengcai Liao",
      "Ling Shao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05499"
  },
  {
    "id": "arXiv:2106.05505",
    "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in  Pre-trained Language Models",
    "abstract": "In this paper, we detail the relationship between convolutions and\nself-attention in natural language tasks. We show that relative position\nembeddings in self-attention layers are equivalent to recently-proposed dynamic\nlightweight convolutions, and we consider multiple new ways of integrating\nconvolutions into Transformer self-attention. Specifically, we propose\ncomposite attention, which unites previous relative position embedding methods\nunder a convolutional framework. We conduct experiments by training BERT with\ncomposite attention, finding that convolutions consistently improve performance\non multiple downstream tasks, replacing absolute position embeddings. To inform\nfuture work, we present results comparing lightweight convolutions, dynamic\nconvolutions, and depthwise-separable convolutions in language model\npre-training, considering multiple injection points for convolutions in\nself-attention layers.",
    "descriptor": "\nComments: Accepted to ACL-IJCNLP 2021\n",
    "authors": [
      "Tyler A. Chang",
      "Yifan Xu",
      "Weijian Xu",
      "Zhuowen Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05505"
  },
  {
    "id": "arXiv:2106.05506",
    "title": "Brittle AI, Causal Confusion, and Bad Mental Models: Challenges and  Successes in the XAI Program",
    "abstract": "The advances in artificial intelligence enabled by deep learning\narchitectures are undeniable. In several cases, deep neural network driven\nmodels have surpassed human level performance in benchmark autonomy tasks. The\nunderlying policies for these agents, however, are not easily interpretable. In\nfact, given their underlying deep models, it is impossible to directly\nunderstand the mapping from observations to actions for any reasonably complex\nagent. Producing this supporting technology to \"open the black box\" of these AI\nsystems, while not sacrificing performance, was the fundamental goal of the\nDARPA XAI program. In our journey through this program, we have several \"big\npicture\" takeaways: 1) Explanations need to be highly tailored to their\nscenario; 2) many seemingly high performing RL agents are extremely brittle and\nare not amendable to explanation; 3) causal models allow for rich explanations,\nbut how to present them isn't always straightforward; and 4) human subjects\nconjure fantastically wrong mental models for AIs, and these models are often\nhard to break. This paper discusses the origins of these takeaways, provides\namplifying information, and suggestions for future work.",
    "descriptor": "",
    "authors": [
      "Jeff Druce",
      "James Niehaus",
      "Vanessa Moody",
      "David Jensen",
      "Michael L. Littman"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05506"
  },
  {
    "id": "arXiv:2106.05508",
    "title": "Vertical Federated Learning without Revealing Intersection Membership",
    "abstract": "Vertical Federated Learning (vFL) allows multiple parties that own different\nattributes (e.g. features and labels) of the same data entity (e.g. a person)\nto jointly train a model. To prepare the training data, vFL needs to identify\nthe common data entities shared by all parties. It is usually achieved by\nPrivate Set Intersection (PSI) which identifies the intersection of training\nsamples from all parties by using personal identifiable information (e.g.\nemail) as sample IDs to align data instances. As a result, PSI would make\nsample IDs of the intersection visible to all parties, and therefore each party\ncan know that the data entities shown in the intersection also appear in the\nother parties, i.e. intersection membership. However, in many real-world\nprivacy-sensitive organizations, e.g. banks and hospitals, revealing membership\nof their data entities is prohibited. In this paper, we propose a vFL framework\nbased on Private Set Union (PSU) that allows each party to keep sensitive\nmembership information to itself. Instead of identifying the intersection of\nall training samples, our PSU protocol generates the union of samples as\ntraining instances. In addition, we propose strategies to generate synthetic\nfeatures and labels to handle samples that belong to the union but not the\nintersection. Through extensive experiments on two real-world datasets, we show\nour framework can protect the privacy of the intersection membership while\nmaintaining the model utility.",
    "descriptor": "",
    "authors": [
      "Jiankai Sun",
      "Xin Yang",
      "Yuanshun Yao",
      "Aonan Zhang",
      "Weihao Gao",
      "Junyuan Xie",
      "Chong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05508"
  },
  {
    "id": "arXiv:2106.05513",
    "title": "Deterministic Mincut in Almost-Linear Time",
    "abstract": "We present a deterministic (global) mincut algorithm for weighted, undirected\ngraphs that runs in $m^{1+o(1)}$ time, answering an open question of Karger\nfrom the 1990s. To obtain our result, we de-randomize the construction of the\n\\emph{skeleton} graph in Karger's near-linear time mincut algorithm, which is\nits only randomized component. In particular, we partially de-randomize the\nwell-known Benczur-Karger graph sparsification technique by random sampling,\nwhich we accomplish by the method of pessimistic estimators. Our main technical\ncomponent is designing an efficient pessimistic estimator to capture the cuts\nof a graph, which involves harnessing the expander decomposition framework\nintroduced in recent work by Goranci et al. (SODA 2021). As a side-effect, we\nobtain a structural representation of all approximate mincuts in a graph, which\nmay have future applications.",
    "descriptor": "\nComments: STOC 2021, 24 pages\n",
    "authors": [
      "Jason Li"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05513"
  },
  {
    "id": "arXiv:2106.05515",
    "title": "Understanding the Under-Coverage Bias in Uncertainty Estimation",
    "abstract": "Estimating the data uncertainty in regression tasks is often done by learning\na quantile function or a prediction interval of the true label conditioned on\nthe input. It is frequently observed that quantile regression -- a vanilla\nalgorithm for learning quantiles with asymptotic guarantees -- tends to\n\\emph{under-cover} than the desired coverage level in reality. While various\nfixes have been proposed, a more fundamental understanding of why this\nunder-coverage bias happens in the first place remains elusive.\nIn this paper, we present a rigorous theoretical study on the coverage of\nuncertainty estimation algorithms in learning quantiles. We prove that quantile\nregression suffers from an inherent under-coverage bias, in a vanilla setting\nwhere we learn a realizable linear quantile function and there is more data\nthan parameters. More quantitatively, for $\\alpha>0.5$ and small $d/n$, the\n$\\alpha$-quantile learned by quantile regression roughly achieves coverage\n$\\alpha - (\\alpha-1/2)\\cdot d/n$ regardless of the noise distribution, where\n$d$ is the input dimension and $n$ is the number of training data. Our theory\nreveals that this under-coverage bias stems from a certain high-dimensional\nparameter estimation error that is not implied by existing theories on quantile\nregression. Experiments on simulated and real data verify our theory and\nfurther illustrate the effect of various factors such as sample size and model\ncapacity on the under-coverage bias in more practical setups.",
    "descriptor": "",
    "authors": [
      "Yu Bai",
      "Song Mei",
      "Huan Wang",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05515"
  },
  {
    "id": "arXiv:2106.05517",
    "title": "Learning to Affiliate: Mutual Centralized Learning for Few-shot  Classification",
    "abstract": "Few-shot learning (FSL) aims to learn a classifier that can be easily adapted\nto accommodate new tasks not seen during training, given only a few examples.\nTo handle the limited-data problem in few-shot regimes, recent methods tend to\ncollectively use a set of local features to densely represent an image instead\nof using a mixed global feature. They generally explore a unidirectional\nquery-to-support paradigm in FSL, e.g., find the nearest/optimal support\nfeature for each query feature and aggregate these local matches for a joint\nclassification. In this paper, we propose a new method Mutual Centralized\nLearning (MCL) to fully affiliate the two disjoint sets of dense features in a\nbidirectional paradigm. We associate each local feature with a particle that\ncan bidirectionally random walk in a discrete feature space by the\naffiliations. To estimate the class probability, we propose the features'\naccessibility that measures the expected number of visits to the support\nfeatures of that class in a Markov process. We relate our method to learning a\ncentrality on an affiliation network and demonstrate its capability to be\nplugged in existing methods by highlighting centralized local features.\nExperiments show that our method achieves the state-of-the-art on both\nminiImageNet and tieredImageNet.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Yang Liu",
      "Weifeng Zhang",
      "Chao Xiang",
      "Tu Zheng",
      "Deng Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05517"
  },
  {
    "id": "arXiv:2106.05519",
    "title": "Consistent Instance False Positive Improves Fairness in Face Recognition",
    "abstract": "Demographic bias is a significant challenge in practical face recognition\nsystems. Existing methods heavily rely on accurate demographic annotations.\nHowever, such annotations are usually unavailable in real scenarios. Moreover,\nthese methods are typically designed for a specific demographic group and are\nnot general enough. In this paper, we propose a false positive rate penalty\nloss, which mitigates face recognition bias by increasing the consistency of\ninstance False Positive Rate (FPR). Specifically, we first define the instance\nFPR as the ratio between the number of the non-target similarities above a\nunified threshold and the total number of the non-target similarities. The\nunified threshold is estimated for a given total FPR. Then, an additional\npenalty term, which is in proportion to the ratio of instance FPR overall FPR,\nis introduced into the denominator of the softmax-based loss. The larger the\ninstance FPR, the larger the penalty. By such unequal penalties, the instance\nFPRs are supposed to be consistent. Compared with the previous debiasing\nmethods, our method requires no demographic annotations. Thus, it can mitigate\nthe bias among demographic groups divided by various attributes, and these\nattributes are not needed to be previously predefined during training.\nExtensive experimental results on popular benchmarks demonstrate the\nsuperiority of our method over state-of-the-art competitors. Code and trained\nmodels are available at https://github.com/Tencent/TFace.",
    "descriptor": "\nComments: CVPR2021\n",
    "authors": [
      "Xingkun Xu",
      "Yuge Huang",
      "Pengcheng Shen",
      "Shaoxin Li",
      "Jilin Li",
      "Feiyue Huang",
      "Yong Li",
      "Zhen Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05519"
  },
  {
    "id": "arXiv:2106.05521",
    "title": "Swarm Intelligence for Self-Organized Clustering",
    "abstract": "Algorithms implementing populations of agents which interact with one another\nand sense their environment may exhibit emergent behavior such as\nself-organization and swarm intelligence. Here a swarm system, called\nDatabionic swarm (DBS), is introduced which is able to adapt itself to\nstructures of high-dimensional data characterized by distance and/or\ndensity-based structures in the data space. By exploiting the interrelations of\nswarm intelligence, self-organization and emergence, DBS serves as an\nalternative approach to the optimization of a global objective function in the\ntask of clustering. The swarm omits the usage of a global objective function\nand is parameter-free because it searches for the Nash equilibrium during its\nannealing process. To our knowledge, DBS is the first swarm combining these\napproaches. Its clustering can outperform common clustering methods such as\nK-means, PAM, single linkage, spectral clustering, model-based clustering, and\nWard, if no prior knowledge about the data is available. A central problem in\nclustering is the correct estimation of the number of clusters. This is\naddressed by a DBS visualization called topographic map which allows assessing\nthe number of clusters. It is known that all clustering algorithms construct\nclusters, irrespective of the data set contains clusters or not. In contrast to\nmost other clustering algorithms, the topographic map identifies, that\nclustering of the data is meaningless if the data contains no (natural)\nclusters. The performance of DBS is demonstrated on a set of benchmark data,\nwhich are constructed to pose difficult clustering problems and in two\nreal-world applications.",
    "descriptor": "\nComments: 54 pages, 21 figures\n",
    "authors": [
      "Michael C. Thrun",
      "Alfred Ultsch"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05521"
  },
  {
    "id": "arXiv:2106.05522",
    "title": "A Mathematical Foundation for Robust Machine Learning based on  Bias-Variance Trade-off",
    "abstract": "A common assumption in machine learning is that samples are independently and\nidentically distributed (i.i.d). However, the contributions of different\nsamples are not identical in training. Some samples are difficult to learn and\nsome samples are noisy. The unequal contributions of samples has a considerable\neffect on training performances. Studies focusing on unequal sample\ncontributions (e.g., easy, hard, noisy) in learning usually refer to these\ncontributions as robust machine learning (RML). Weighing and regularization are\ntwo common techniques in RML. Numerous learning algorithms have been proposed\nbut the strategies for dealing with easy/hard/noisy samples differ or even\ncontradict with different learning algorithms. For example, some strategies\ntake the hard samples first, whereas some strategies take easy first.\nConducting a clear comparison for existing RML algorithms in dealing with\ndifferent samples is difficult due to lack of a unified theoretical framework\nfor RML. This study attempts to construct a mathematical foundation for RML\nbased on the bias-variance trade-off theory. A series of definitions and\nproperties are presented and proved. Several classical learning algorithms are\nalso explained and compared. Improvements of existing methods are obtained\nbased on the comparison. A unified method that combines two classical learning\nstrategies is proposed.",
    "descriptor": "",
    "authors": [
      "Ou Wu",
      "Weiyao Zhu",
      "Yingjun Deng",
      "Haixiang Zhang",
      "Qinghu Hou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05522"
  },
  {
    "id": "arXiv:2106.05525",
    "title": "3D Semantic Mapping from Arthroscopy using Out-of-distribution Pose and  Depth and In-distribution Segmentation Training",
    "abstract": "Minimally invasive surgery (MIS) has many documented advantages, but the\nsurgeon's limited visual contact with the scene can be problematic. Hence,\nsystems that can help surgeons navigate, such as a method that can produce a 3D\nsemantic map, can compensate for the limitation above. In theory, we can borrow\n3D semantic mapping techniques developed for robotics, but this requires\nfinding solutions to the following challenges in MIS: 1) semantic segmentation,\n2) depth estimation, and 3) pose estimation. In this paper, we propose the\nfirst 3D semantic mapping system from knee arthroscopy that solves the three\nchallenges above. Using out-of-distribution non-human datasets, where pose\ncould be labeled, we jointly train depth+pose estimators using selfsupervised\nand supervised losses. Using an in-distribution human knee dataset, we train a\nfully-supervised semantic segmentation system to label arthroscopic image\npixels into femur, ACL, and meniscus. Taking testing images from human knees,\nwe combine the results from these two systems to automatically create 3D\nsemantic maps of the human knee. The result of this work opens the pathway to\nthe generation of intraoperative 3D semantic mapping, registration with\npre-operative data, and robotic-assisted arthroscopy",
    "descriptor": "",
    "authors": [
      "Yaqub Jonmohamadi",
      "Shahnewaz Ali",
      "Fengbei Liu",
      "Jonathan Roberts",
      "Ross Crawford",
      "Gustavo Carneiro",
      "Ajay K. Pandey"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05525"
  },
  {
    "id": "arXiv:2106.05526",
    "title": "Simplifying Deep Reinforcement Learning via Self-Supervision",
    "abstract": "Supervised regression to demonstrations has been demonstrated to be a stable\nway to train deep policy networks. We are motivated to study how we can take\nfull advantage of supervised loss functions for stably training deep\nreinforcement learning agents. This is a challenging task because it is unclear\nhow the training data could be collected to enable policy improvement. In this\nwork, we propose Self-Supervised Reinforcement Learning (SSRL), a simple\nalgorithm that optimizes policies with purely supervised losses. We demonstrate\nthat, without policy gradient or value estimation, an iterative procedure of\n``labeling\" data and supervised regression is sufficient to drive stable policy\nimprovement. By selecting and imitating trajectories with high episodic\nrewards, SSRL is surprisingly competitive to contemporary algorithms with more\nstable performance and less running time, showing the potential of solving\nreinforcement learning with supervised learning techniques. The code is\navailable at https://github.com/daochenzha/SSRL",
    "descriptor": "",
    "authors": [
      "Daochen Zha",
      "Kwei-Herng Lai",
      "Kaixiong Zhou",
      "Xia Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05526"
  },
  {
    "id": "arXiv:2106.05527",
    "title": "Score Matching Model for Unbounded Data Score",
    "abstract": "Recent advance in score-based models incorporates the stochastic differential\nequation (SDE), which brings the state-of-the art performance on image\ngeneration tasks. This paper improves such score-based models by analyzing the\nmodel at the zero perturbation noise. In real datasets, the score function\ndiverges as the perturbation noise ($\\sigma$) decreases to zero, and this\nobservation leads an argument that the score estimation fails at $\\sigma=0$\nwith any neural network structure. Subsequently, we introduce Unbounded Noise\nConditional Score Network (UNCSN) that resolves the score diverging problem\nwith an easily applicable modification to any noise conditional score-based\nmodels. Additionally, we introduce a new type of SDE, so the exact log\nlikelihood can be calculated from the newly suggested SDE. On top of that, the\nassociated loss function mitigates the loss imbalance issue in a mini-batch,\nand we present a theoretic analysis on the proposed loss to uncover the behind\nmechanism of the data distribution modeling by the score-based models.",
    "descriptor": "\nComments: 24 pages, 14 figures\n",
    "authors": [
      "Dongjun Kim",
      "Seungjae Shin",
      "Kyungwoo Song",
      "Wanmo Kang",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05527"
  },
  {
    "id": "arXiv:2106.05528",
    "title": "Cross-domain Contrastive Learning for Unsupervised Domain Adaptation",
    "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na fully-labeled source domain to a different unlabeled target domain. Most\nexisting UDA methods learn domain-invariant feature representations by\nminimizing feature distances across domains. In this work, we build upon\ncontrastive self-supervised learning to align features so as to reduce the\ndomain discrepancy between training and testing sets. Exploring the same set of\ncategories shared by both domains, we introduce a simple yet effective\nframework CDCL, for domain alignment. In particular, given an anchor image from\none domain, we minimize its distances to cross-domain samples from the same\nclass relative to those from different categories. Since target labels are\nunavailable, we use a clustering-based approach with carefully initialized\ncenters to produce pseudo labels. In addition, we demonstrate that CDCL is a\ngeneral framework and can be adapted to the data-free setting, where the source\ndata are unavailable during training, with minimal modification. We conduct\nexperiments on two widely used domain adaptation benchmarks, i.e., Office-31\nand VisDA-2017, and demonstrate that CDCL achieves state-of-the-art performance\non both datasets.",
    "descriptor": "",
    "authors": [
      "Rui Wang",
      "Zuxuan Wu",
      "Zejia Weng",
      "Jingjing Chen",
      "Guo-Jun Qi",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05528"
  },
  {
    "id": "arXiv:2106.05529",
    "title": "Independent Deeply Learned Tensor Analysis for Determined Audio Source  Separation",
    "abstract": "We address the determined audio source separation problem in the\ntime-frequency domain. In independent deeply learned matrix analysis (IDLMA),\nit is assumed that the inter-frequency correlation of each source spectrum is\nzero, which is inappropriate for modeling nonstationary signals such as music\nsignals. To account for the correlation between frequencies, independent\npositive semidefinite tensor analysis has been proposed. This unsupervised\n(blind) method, however, severely restrict the structure of frequency\ncovariance matrices (FCMs) to reduce the number of model parameters. As an\nextension of these conventional approaches, we here propose a supervised method\nthat models FCMs using deep neural networks (DNNs). It is difficult to directly\ninfer FCMs using DNNs. Therefore, we also propose a new FCM model represented\nas a convex combination of a diagonal FCM and a rank-1 FCM. Our FCM model is\nflexible enough to not only consider inter-frequency correlation, but also\ncapture the dynamics of time-varying FCMs of nonstationary signals. We infer\nthe proposed FCMs using two DNNs: DNN for power spectrum estimation and DNN for\ntime-domain signal estimation. An experimental result of separating music\nsignals shows that the proposed method provides higher separation performance\nthan IDLMA.",
    "descriptor": "\nComments: 5 pages, 2 figures, accepted for European Signal Processing Conference 2021 (EUSIPCO 2021)\n",
    "authors": [
      "Naoki Narisawa",
      "Rintaro Ikeshita",
      "Norihiro Takamune",
      "Daichi Kitamura",
      "Tomohiko Nakamura",
      "Hiroshi Saruwatari",
      "Tomohiro Nakatani"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05529"
  },
  {
    "id": "arXiv:2106.05530",
    "title": "Adversarial Option-Aware Hierarchical Imitation Learning",
    "abstract": "It has been a challenge to learning skills for an agent from long-horizon\nunannotated demonstrations. Existing approaches like Hierarchical Imitation\nLearning(HIL) are prone to compounding errors or suboptimal solutions. In this\npaper, we propose Option-GAIL, a novel method to learn skills at long horizon.\nThe key idea of Option-GAIL is modeling the task hierarchy by options and train\nthe policy via generative adversarial optimization. In particular, we propose\nan Expectation-Maximization(EM)-style algorithm: an E-step that samples the\noptions of expert conditioned on the current learned policy, and an M-step that\nupdates the low- and high-level policies of agent simultaneously to minimize\nthe newly proposed option-occupancy measurement between the expert and the\nagent. We theoretically prove the convergence of the proposed algorithm.\nExperiments show that Option-GAIL outperforms other counterparts consistently\nacross a variety of tasks.",
    "descriptor": "\nComments: accepted by ICML 2021\n",
    "authors": [
      "Mingxuan Jing",
      "Wenbing Huang",
      "Fuchun Sun",
      "Xiaojian Ma",
      "Tao Kong",
      "Chuang Gan",
      "Lei Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05530"
  },
  {
    "id": "arXiv:2106.05532",
    "title": "How Robust are Model Rankings: A Leaderboard Customization Approach for  Equitable Evaluation",
    "abstract": "Models that top leaderboards often perform unsatisfactorily when deployed in\nreal world applications; this has necessitated rigorous and expensive\npre-deployment model testing. A hitherto unexplored facet of model performance\nis: Are our leaderboards doing equitable evaluation? In this paper, we\nintroduce a task-agnostic method to probe leaderboards by weighting samples\nbased on their `difficulty' level. We find that leaderboards can be\nadversarially attacked and top performing models may not always be the best\nmodels. We subsequently propose alternate evaluation metrics. Our experiments\non 10 models show changes in model ranking and an overall reduction in\npreviously reported performance -- thus rectifying the overestimation of AI\nsystems' capabilities. Inspired by behavioral testing principles, we further\ndevelop a prototype of a visual analytics tool that enables leaderboard\nrevamping through customization, based on an end user's focus area. This helps\nusers analyze models' strengths and weaknesses, and guides them in the\nselection of a model best suited for their application scenario. In a user\nstudy, members of various commercial product development teams, covering 5\nfocus areas, find that our prototype reduces pre-deployment development and\ntesting effort by 41% on average.",
    "descriptor": "\nComments: AAAI 2021\n",
    "authors": [
      "Swaroop Mishra",
      "Anjana Arunkumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05532"
  },
  {
    "id": "arXiv:2106.05535",
    "title": "Differentiable Robust LQR Layers",
    "abstract": "This paper proposes a differentiable robust LQR layer for reinforcement\nlearning and imitation learning under model uncertainty and stochastic\ndynamics. The robust LQR layer can exploit the advantages of robust optimal\ncontrol and model-free learning. It provides a new type of inductive bias for\nstochasticity and uncertainty modeling in control systems. In particular, we\npropose an efficient way to differentiate through a robust LQR optimization\nprogram by rewriting it as a convex program (i.e. semi-definite program) of the\nworst-case cost. Based on recent work on using convex optimization inside\nneural network layers, we develop a fully differentiable layer for optimizing\nthis worst-case cost, i.e. we compute the derivative of a performance measure\nw.r.t the model's unknown parameters, model uncertainty and stochasticity\nparameters. We demonstrate the proposed method on imitation learning and\napproximate dynamic programming on stochastic and uncertain domains. The\nexperiment results show that the proposed method can optimize robust policies\nunder uncertain situations, and are able to achieve a significantly better\nperformance than existing methods that do not model uncertainty directly.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Ngo Anh Vien",
      "Gerhard Neumann"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05535"
  },
  {
    "id": "arXiv:2106.05542",
    "title": "DUET: Detection Utilizing Enhancement for Text in Scanned or Captured  Documents",
    "abstract": "We present a novel deep neural model for text detection in document images.\nFor robust text detection in noisy scanned documents, the advantages of\nmulti-task learning are adopted by adding an auxiliary task of text\nenhancement. Namely, our proposed model is designed to perform noise reduction\nand text region enhancement as well as text detection. Moreover, we enrich the\ntraining data for the model with synthesized document images that are fully\nlabeled for text detection and enhancement, thus overcome the insufficiency of\nlabeled document image data. For the effective exploitation of the synthetic\nand real data, the training process is separated in two phases. The first phase\nis training only synthetic data in a fully-supervised manner. Then real data\nwith only detection labels are added in the second phase. The enhancement task\nfor the real data is weakly-supervised with information from their detection\nlabels. Our methods are demonstrated in a real document dataset with\nperformances exceeding those of other text detection methods. Moreover,\nablations are conducted and the results confirm the effectiveness of the\nsynthetic data, auxiliary task, and weak-supervision. Whereas the existing text\ndetection studies mostly focus on the text in scenes, our proposed method is\noptimized to the applications for the text in scanned documents.",
    "descriptor": "",
    "authors": [
      "Eun-Soo Jung",
      "HyeongGwan Son",
      "Kyusam Oh",
      "Yongkeun Yun",
      "Soonhwan Kwon",
      "Min Soo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05542"
  },
  {
    "id": "arXiv:2106.05544",
    "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive  Language Processing Signals",
    "abstract": "Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.",
    "descriptor": "",
    "authors": [
      "Yuqi Ren",
      "Deyi Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05544"
  },
  {
    "id": "arXiv:2106.05545",
    "title": "Super-Resolution Image Reconstruction Based on Self-Calibrated  Convolutional GAN",
    "abstract": "With the effective application of deep learning in computer vision,\nbreakthroughs have been made in the research of super-resolution images\nreconstruction. However, many researches have pointed out that the\ninsufficiency of the neural network extraction on image features may bring the\ndeteriorating of newly reconstructed image. On the other hand, the generated\npictures are sometimes too artificial because of over-smoothing. In order to\nsolve the above problems, we propose a novel self-calibrated convolutional\ngenerative adversarial networks. The generator consists of feature extraction\nand image reconstruction. Feature extraction uses self-calibrated convolutions,\nwhich contains four portions, and each portion has specific functions. It can\nnot only expand the range of receptive fields, but also obtain long-range\nspatial and inter-channel dependencies. Then image reconstruction is performed,\nand finally a super-resolution image is reconstructed. We have conducted\nthorough experiments on different datasets including set5, set14 and BSD100\nunder the SSIM evaluation method. The experimental results prove the\neffectiveness of the proposed network.",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Yibo Guo",
      "Haidi Wang",
      "Yiming Fan",
      "Shunyao Li",
      "Mingliang Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05545"
  },
  {
    "id": "arXiv:2106.05546",
    "title": "Progressive Multi-Granularity Training for Non-Autoregressive  Translation",
    "abstract": "Non-autoregressive translation (NAT) significantly accelerates the inference\nprocess via predicting the entire target sequence. However, recent studies show\nthat NAT is weak at learning high-mode of knowledge such as one-to-many\ntranslations. We argue that modes can be divided into various granularities\nwhich can be learned from easy to hard. In this study, we empirically show that\nNAT models are prone to learn fine-grained lower-mode knowledge, such as words\nand phrases, compared with sentences. Based on this observation, we propose\nprogressive multi-granularity training for NAT. More specifically, to make the\nmost of the training data, we break down the sentence-level examples into three\ntypes, i.e. words, phrases, sentences, and with the training goes, we\nprogressively increase the granularities. Experiments on Romanian-English,\nEnglish-German, Chinese-English, and Japanese-English demonstrate that our\napproach improves the phrase translation accuracy and model reordering ability,\ntherefore resulting in better translation quality against strong NAT baselines.\nAlso, we show that more deterministic fine-grained knowledge can further\nenhance performance.",
    "descriptor": "\nComments: findings of ACL 2021\n",
    "authors": [
      "Liang Ding",
      "Longyue Wang",
      "Xuebo Liu",
      "Derek F. Wong",
      "Dacheng Tao",
      "Zhaopeng Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05546"
  },
  {
    "id": "arXiv:2106.05549",
    "title": "Validation of Simulation-Based Testing: Bypassing Domain Shift with  Label-to-Image Synthesis",
    "abstract": "Many machine learning applications can benefit from simulated data for\nsystematic validation - in particular if real-life data is difficult to obtain\nor annotate. However, since simulations are prone to domain shift w.r.t.\nreal-life data, it is crucial to verify the transferability of the obtained\nresults. We propose a novel framework consisting of a generative label-to-image\nsynthesis model together with different transferability measures to inspect to\nwhat extent we can transfer testing results of semantic segmentation models\nfrom synthetic data to equivalent real-life data. With slight modifications,\nour approach is extendable to, e.g., general multi-class classification tasks.\nGrounded on the transferability analysis, our approach additionally allows for\nextensive testing by incorporating controlled simulations. We validate our\napproach empirically on a semantic segmentation task on driving scenes.\nTransferability is tested using correlation analysis of IoU and a learned\ndiscriminator. Although the latter can distinguish between real-life and\nsynthetic tests, in the former we observe surprisingly strong correlations of\n0.7 for both cars and pedestrians.",
    "descriptor": "\nComments: The first two authors contributed equally. Accepted at the 4th Workshop on \"Ensuring and Validating Safety for Automated Vehicles\" (WS13), IV2021. Under IEEE Copyright\n",
    "authors": [
      "Julia Rosenzweig",
      "Eduardo Brito",
      "Hans-Ulrich Kobialka",
      "Maram Akila",
      "Nico M. Schmidt",
      "Peter Schlicht",
      "Jan David Schneider",
      "Fabian H\u00fcger",
      "Matthias Rottmann",
      "Sebastian Houben",
      "Tim Wirtz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05549"
  },
  {
    "id": "arXiv:2106.05553",
    "title": "Stateless Reinforcement Learning for Multi-Agent Systems: the Case of  Spectrum Allocation in Dynamic Channel Bonding WLANs",
    "abstract": "Spectrum allocation in the form of primary channel and bandwidth selection is\na key factor for dynamic channel bonding (DCB) wireless local area networks\n(WLANs). To cope with varying environments, where networks change their\nconfigurations on their own, the wireless community is looking towards\nsolutions aided by machine learning (ML), and especially reinforcement learning\n(RL) given its trial-and-error approach. However, strong assumptions are\nnormally made to let complex RL models converge to near-optimal solutions. Our\ngoal with this paper is two-fold: justify in a comprehensible way why RL should\nbe the approach for wireless networks problems like decentralized spectrum\nallocation, and call into question whether the use of complex RL algorithms\nhelps the quest of rapid learning in realistic scenarios. We derive that\nstateless RL in the form of lightweight multi-armed-bandits (MABs) is an\nefficient solution for rapid adaptation avoiding the definition of extensive or\nmeaningless RL states.",
    "descriptor": "",
    "authors": [
      "Sergio Barrachina-Mu\u00f1oz",
      "Alessandro Chiumento",
      "Boris Bellalta"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05553"
  },
  {
    "id": "arXiv:2106.05554",
    "title": "Progressive Stage-wise Learning for Unsupervised Feature Representation  Enhancement",
    "abstract": "Unsupervised learning methods have recently shown their competitiveness\nagainst supervised training. Typically, these methods use a single objective to\ntrain the entire network. But one distinct advantage of unsupervised over\nsupervised learning is that the former possesses more variety and freedom in\ndesigning the objective. In this work, we explore new dimensions of\nunsupervised learning by proposing the Progressive Stage-wise Learning (PSL)\nframework. For a given unsupervised task, we design multilevel tasks and define\ndifferent learning stages for the deep network. Early learning stages are\nforced to focus on lowlevel tasks while late stages are guided to extract\ndeeper information through harder tasks. We discover that by progressive\nstage-wise learning, unsupervised feature representation can be effectively\nenhanced. Our extensive experiments show that PSL consistently improves results\nfor the leading unsupervised learning methods.",
    "descriptor": "\nComments: Accepted by the IEEE conference on computer vision and pattern recognition. 2021\n",
    "authors": [
      "Zefan Li",
      "Chenxi Li",
      "Alan Yuille",
      "Bingbing Ni",
      "Wenjun Zhang",
      "Wen Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05554"
  },
  {
    "id": "arXiv:2106.05555",
    "title": "Shades of BLEU, Flavours of Success: The Case of MultiWOZ",
    "abstract": "The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for\nbenchmarking context-to-response abilities of task-oriented dialogue systems.\nIn this work, we identify inconsistencies in data preprocessing and reporting\nof three corpus-based metrics used on this dataset, i.e., BLEU score and Inform\n& Success rates. We point out a few problems of the MultiWOZ benchmark such as\nunsatisfactory preprocessing, insufficient or under-specified evaluation\nmetrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy\noptimization models in as-fair-as-possible setups, and we show that their\nreported scores cannot be directly compared. To facilitate comparison of future\nsystems, we release our stand-alone standardized evaluation scripts. We also\ngive basic recommendations for corpus-based benchmarking in future works.",
    "descriptor": "\nComments: Accepted to GEM Workshop at ACL 2021; for the source files, see this https URL\n",
    "authors": [
      "Tom\u00e1\u0161 Nekvinda",
      "Ond\u0159ej Du\u0161ek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05555"
  },
  {
    "id": "arXiv:2106.05564",
    "title": "FRI-TEM: Time Encoding Sampling of Finite-Rate-of-Innovation Signals",
    "abstract": "Classical sampling is based on acquiring signal amplitudes at specific points\nin time, with the minimal sampling rate dictated by the degrees of freedom in\nthe signal. The samplers in this framework are controlled by a global clock\nthat operates at a rate greater than or equal to the minimal sampling rate. At\nhigh sampling rates, clocks are power-consuming and prone to electromagnetic\ninterference. An integrate-and-fire time encoding machine (IF-TEM) is an\nalternative power-efficient sampling mechanism which does not require a global\nclock. Here, the samples are irregularly spaced threshold-based samples. In\nthis paper, we investigate the problem of sampling finite-rate-of-innovation\n(FRI) signals using an IF-TEM. We provide theoretical recovery guarantees for\nan FRI signal with arbitrary pulse shape and without any constraint on the\nminimum separation between the pulses. In particular, we show how to design a\nsampling kernel, IF-TEM, and recovery method such that the FRI signals are\nperfectly reconstructed. We then propose a modification to the sampling kernel\nto improve noise robustness. Our results enable designing low-cost and\nenergy-efficient analog-to-digital converters for FRI signals.",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Hila Namman",
      "Satish Mulleti",
      "Yonina C. Eldar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.05564"
  },
  {
    "id": "arXiv:2106.05566",
    "title": "A Neural Tangent Kernel Perspective of GANs",
    "abstract": "Theoretical analyses for Generative Adversarial Networks (GANs) generally\nassume an arbitrarily large family of discriminators and do not consider the\ncharacteristics of the architectures used in practice. We show that this\nframework of analysis is too simplistic to properly analyze GAN training. To\ntackle this issue, we leverage the theory of infinite-width neural networks to\nmodel neural discriminator training for a wide range of adversarial losses via\nits Neural Tangent Kernel (NTK). Our analytical results show that GAN\ntrainability primarily depends on the discriminator's architecture. We further\nstudy the discriminator for specific architectures and losses, and highlight\nproperties providing a new understanding of GAN training. For example, we find\nthat GANs trained with the integral probability metric loss minimize the\nmaximum mean discrepancy with the NTK as kernel. Our conclusions demonstrate\nthe analysis opportunities provided by the proposed framework, which paves the\nway for better and more principled GAN models. We release a generic GAN\nanalysis toolkit based on our framework that supports the empirical part of our\nstudy.",
    "descriptor": "",
    "authors": [
      "Jean-Yves Franceschi",
      "Emmanuel de B\u00e9zenac",
      "Ibrahim Ayed",
      "Micka\u00ebl Chen",
      "Sylvain Lamprier",
      "Patrick Gallinari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05566"
  },
  {
    "id": "arXiv:2106.05568",
    "title": "Explainable AI, but explainable to whom?",
    "abstract": "Advances in AI technologies have resulted in superior levels of AI-based\nmodel performance. However, this has also led to a greater degree of model\ncomplexity, resulting in 'black box' models. In response to the AI black box\nproblem, the field of explainable AI (xAI) has emerged with the aim of\nproviding explanations catered to human understanding, trust, and transparency.\nYet, we still have a limited understanding of how xAI addresses the need for\nexplainable AI in the context of healthcare. Our research explores the\ndiffering explanation needs amongst stakeholders during the development of an\nAI-system for classifying COVID-19 patients for the ICU. We demonstrate that\nthere is a constellation of stakeholders who have different explanation needs,\nnot just the 'user'. Further, the findings demonstrate how the need for xAI\nemerges through concerns associated with specific stakeholder groups i.e., the\ndevelopment team, subject matter experts, decision makers, and the audience.\nOur findings contribute to the expansion of xAI by highlighting that different\nstakeholders have different explanation needs. From a practical perspective,\nthe study provides insights on how AI systems can be adjusted to support\ndifferent stakeholders needs, ensuring better implementation and operation in a\nhealthcare context.",
    "descriptor": "\nComments: accepted book chapter for AI in Healthcare DOI TBD\n",
    "authors": [
      "Julie Gerlings",
      "Millie S\u00f8ndergaard Jensen",
      "Arisa Shollo"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05568"
  },
  {
    "id": "arXiv:2106.05569",
    "title": "Front Contribution instead of Back Propagation",
    "abstract": "Deep Learning's outstanding track record across several domains has stemmed\nfrom the use of error backpropagation (BP). Several studies, however, have\nshown that it is impossible to execute BP in a real brain. Also, BP still\nserves as an important and unsolved bottleneck for memory usage and speed. We\npropose a simple, novel algorithm, the Front-Contribution algorithm, as a\ncompact alternative to BP. The contributions of all weights with respect to the\nfinal layer weights are calculated before training commences and all the\ncontributions are appended to weights of the final layer, i.e., the effective\nfinal layer weights are a non-linear function of themselves. Our algorithm then\nessentially collapses the network, precluding the necessity for weight updation\nof all weights not in the final layer. This reduction in parameters results in\nlower memory usage and higher training speed. We show that our algorithm\nproduces the exact same output as BP, in contrast to several recently proposed\nalgorithms approximating BP. Our preliminary experiments demonstrate the\nefficacy of the proposed algorithm. Our work provides a foundation to\neffectively utilize these presently under-explored \"front contributions\", and\nserves to inspire the next generation of training algorithms.",
    "descriptor": "\nComments: NeurIPS 2020 - Beyond Backpropagation Workshop\n",
    "authors": [
      "Swaroop Mishra",
      "Anjana Arunkumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.05569"
  },
  {
    "id": "arXiv:2106.05577",
    "title": "Quantum-Resistant Security for Software Updates on Low-power Networked  Embedded Devices",
    "abstract": "As the Internet of Things (IoT) rolls out today to devices whose lifetime may\nwell exceed a decade, conservative threat models should consider attackers with\naccess to quantum computing power. The SUIT standard (specified by the IETF)\ndefines a security architecture for IoT software updates, standardizing the\nmetadata and the cryptographic tools-namely, digital signatures and hash\nfunctions-that guarantee the legitimacy of software updates. While the\nperformance of SUIT has previously been evaluated in the pre-quantum context,\nit has not yet been studied in a post-quantum context. Taking the open-source\nimplementation of SUIT available in RIOT as a case study, we overview\npost-quantum considerations, and quantum-resistant digital signatures in\nparticular, focusing on lowpower, microcontroller-based IoT devices which have\nstringent resource constraints in terms of memory, CPU, and energy consumption.\nWe benchmark a selection of proposed post-quantum signature schemes (LMS,\nFalcon, and Dilithium) and compare them with current pre-quantum signature\nschemes (Ed25519 and ECDSA). Our benchmarks are carried out on a variety of IoT\nhardware including ARM Cortex-M, RISC-V, and Espressif (ESP32), which form the\nbulk of modern 32-bit microcontroller architectures. We interpret our benchmark\nresults in the context of SUIT, and estimate the real-world impact of\npost-quantum alternatives for a range of typical software update categories.\nCCS CONCEPTS $\\bullet$ Computer systems organization $\\rightarrow$ Embedded\nsystems.",
    "descriptor": "",
    "authors": [
      "Gustavo Banegas",
      "Koen Zandberg",
      "Adrian Herrmann",
      "Emmanuel Baccelli",
      "Benjamin Smith"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05577"
  },
  {
    "id": "arXiv:2106.05579",
    "title": "Multiway Online Correlated Selection",
    "abstract": "We give a $0.5368$-competitive algorithm for edge-weighted online bipartite\nmatching. Prior to our work, the best competitive ratio was $0.5086$ due to\nFahrbach, Huang, Tao, and Zadimoghaddam (FOCS 2020). They achieved their\nbreakthrough result by developing a subroutine called \\emph{online correlated\nselection} (OCS) which takes as input a sequence of pairs and selects one item\nfrom each pair. Importantly, the selections the OCS makes are negatively\ncorrelated.\nWe achieve our result by defining \\emph{multiway} OCSes which receive\narbitrarily many elements at each step, rather than just two. In addition to\nbetter competitive ratios, our formulation allows for a simpler reduction from\nedge-weighted online bipartite matching to OCSes. While Fahrbach et al. used a\nfactor-revealing linear program to optimize the competitive ratio, our analysis\ndirectly connects the competitive ratio to the parameters of the multiway OCS.\nFinally, we show that the formulation of Farhbach et al. can achieve a\ncompetitive ratio of at most $0.5239$, confirming that multiway OCSes are\nstrictly more powerful.",
    "descriptor": "",
    "authors": [
      "Guy Blanc",
      "Moses Charikar"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05579"
  },
  {
    "id": "arXiv:2106.05580",
    "title": "AGGGEN: Ordering and Aggregating while Generating",
    "abstract": "We present AGGGEN (pronounced 'again'), a data-to-text model which\nre-introduces two explicit sentence planning stages into neural data-to-text\nsystems: input ordering and input aggregation. In contrast to previous work\nusing sentence planning, our model is still end-to-end: AGGGEN performs\nsentence planning at the same time as generating text by learning latent\nalignments (via semantic facts) between input representation and target text.\nExperiments on the WebNLG and E2E challenge data show that by using fact-based\nalignments our approach is more interpretable, expressive, robust to noise, and\neasier to control, while retaining the advantages of end-to-end systems in\nterms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.",
    "descriptor": "",
    "authors": [
      "Xinnuo Xu",
      "Ond\u0159ej Du\u0161ek",
      "Verena Rieser",
      "Ioannis Konstas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05580"
  },
  {
    "id": "arXiv:2106.05581",
    "title": "Studying the characteristics of scientific communities using  individual-level bibliometrics: the case of Big Data research",
    "abstract": "Unlike most bibliometric studies focusing on publications, taking Big Data\nresearch as a case study, we introduce a novel bibliometric approach to unfold\nthe status of a given scientific community from an individual level\nperspective. We study the academic age, production, and research focus of the\ncommunity of authors active in Big Data research. Artificial Intelligence (AI)\nis selected as a reference area for comparative purposes. Results show that the\nacademic realm of \"Big Data\" is a growing topic with an expanding community of\nauthors, particularly of new authors every year. Compared to AI, Big Data\nattracts authors with a longer academic age, who can be regarded to have\naccumulated some publishing experience before entering the community. Despite\nthe highly skewed distribution of productivity amongst researchers in both\ncommunities, Big Data authors have higher values of both research focus and\nproduction than those of AI. Considering the community size, overall academic\nage, and persistence of publishing on the topic, our results support the idea\nof Big Data as a research topic with attractiveness for researchers. We argue\nthat the community-focused indicators proposed in this study could be\ngeneralized to investigate the development and dynamics of other research\nfields and topics.",
    "descriptor": "",
    "authors": [
      "Xiaozan Lyu",
      "Rodrigo Costas"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05581"
  },
  {
    "id": "arXiv:2106.05584",
    "title": "PDMA: Probabilistic Service Migration Approach for Delay-aware and  Mobility-aware Mobile Edge Computing",
    "abstract": "As a key technology in the 5G era, Mobile Edge Computing (MEC) has developed\nrapidly in recent years. MEC aims to reduce the service delay of mobile users,\nwhile alleviating the processing pressure on the core network. MEC can be\nregarded as an extension of cloud computing on the user side, which can deploy\nedge servers and bring computing resources closer to mobile users, and provide\nmore efficient interactions. However, due to the user's dynamic mobility, the\ndistance between the user and the edge server will change dynamically, which\nmay cause fluctuations in Quality of Service (QoS). Therefore, when a mobile\nuser moves in the MEC environment, certain approaches are needed to schedule\nservices deployed on the edge server to ensure the user experience. In this\npaper, we model service scheduling in MEC scenarios and propose a delay-aware\nand mobility-aware service management approach based on concise probabilistic\nmethods. This approach has low computational complexity and can effectively\nreduce service delay and migration costs. Furthermore, we conduct experiments\nby utilizing multiple realistic datasets and use iFogSim to evaluate the\nperformance of the algorithm. The results show that our proposed approach can\noptimize the performance on service delay, with 8% to 20% improvement and\nreduce the migration cost by more than 75% compared with baselines during the\nrush hours.",
    "descriptor": "\nComments: 22 pages, 10 figures, accepted by Software: Practice and Experience, 2021\n",
    "authors": [
      "Minxian Xu",
      "Qiheng Zhou",
      "Huaming Wu",
      "Weiwei Lin",
      "Kejiang Ye",
      "Chengzhong Xu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.05584"
  },
  {
    "id": "arXiv:2106.05587",
    "title": "A Discontinuity Capturing Shallow Neural Network for Elliptic Interface  Problems",
    "abstract": "In this paper, a new Discontinuity Capturing Shallow Neural Network (DCSNN)\nfor approximating $d$-dimensional piecewise continuous functions and for\nsolving elliptic interface problems is developed. There are three novel\nfeatures in the present network; namely, (i) jump discontinuity is captured\nsharply, (ii) it is completely shallow consisting of only one hidden layer,\n(iii) it is completely mesh-free for solving partial differential equations\n(PDEs). We first continuously extend the $d$-dimensional piecewise continuous\nfunction in $(d+1)$-dimensional space by augmenting one coordinate variable to\nlabel the pieces of discontinuous function, and then construct a shallow neural\nnetwork to express this new augmented function. Since only one hidden layer is\nemployed, the number of training parameters (weights and biases) scales\nlinearly with the dimension and the neurons used in the hidden layer. For\nsolving elliptic interface equations, the network is trained by minimizing the\nmean squared error loss that consists of the residual of governing equation,\nboundary condition, and the interface jump conditions. We perform a series of\nnumerical tests to compare the accuracy and efficiency of the present network.\nOur DCSNN model is comparably efficient due to only moderate number of\nparameters needed to be trained (a few hundreds of parameters used throughout\nall numerical examples here), and the result shows better accuracy (and less\nparameters) than other method using piecewise deep neural network in\nliterature. We also compare the results obtained by the traditional grid-based\nimmersed interface method (IIM) which is designed particularly for elliptic\ninterface problems. Again, the present results show better accuracy than the\nones obtained by IIM. We conclude by solving a six-dimensional problem to show\nthe capability of the present network for high-dimensional applications.",
    "descriptor": "",
    "authors": [
      "Wei-Fan Hu",
      "Te-Sheng Lin",
      "Ming-Chih Lai"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05587"
  },
  {
    "id": "arXiv:2106.05589",
    "title": "AUGNLG: Few-shot Natural Language Generation using Self-trained Data  Augmentation",
    "abstract": "Natural Language Generation (NLG) is a key component in a task-oriented\ndialogue system, which converts the structured meaning representation (MR) to\nthe natural language. For large-scale conversational systems, where it is\ncommon to have over hundreds of intents and thousands of slots, neither\ntemplate-based approaches nor model-based approaches are scalable. Recently,\nneural NLGs started leveraging transfer learning and showed promising results\nin few-shot settings. This paper proposes AUGNLG, a novel data augmentation\napproach that combines a self-trained neural retrieval model with a few-shot\nlearned NLU model, to automatically create MR-to-Text data from open-domain\ntexts. The proposed system mostly outperforms the state-of-the-art methods on\nthe FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm\nimproved results on the FewShotSGD data and provide comprehensive analysis\nresults on key components of our system. Our code and data are available at\nhttps://github.com/XinnuoXu/AugNLG.",
    "descriptor": "",
    "authors": [
      "Xinnuo Xu",
      "Guoyin Wang",
      "Young-Bum Kim",
      "Sungjin Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05589"
  },
  {
    "id": "arXiv:2106.05596",
    "title": "Multi-Dataset Benchmarks for Masked Identification using Contrastive  Representation Learning",
    "abstract": "The COVID-19 pandemic has drastically changed accepted norms globally. Within\nthe past year, masks have been used as a public health response to limit the\nspread of the virus. This sudden change has rendered many face recognition\nbased access control, authentication and surveillance systems ineffective.\nOfficial documents such as passports, driving license and national identity\ncards are enrolled with fully uncovered face images. However, in the current\nglobal situation, face matching systems should be able to match these reference\nimages with masked face images. As an example, in an airport or security\ncheckpoint it is safer to match the unmasked image of the identifying document\nto the masked person rather than asking them to remove the mask. We find that\ncurrent facial recognition techniques are not robust to this form of occlusion.\nTo address this unique requirement presented due to the current circumstance,\nwe propose a set of re-purposed datasets and a benchmark for researchers to\nuse. We also propose a contrastive visual representation learning based\npre-training workflow which is specialized to masked vs unmasked face matching.\nWe ensure that our method learns robust features to differentiate people across\nvarying data collection scenarios. We achieve this by training over many\ndifferent datasets and validating our result by testing on various holdout\ndatasets. The specialized weights trained by our method outperform standard\nface recognition features for masked to unmasked face matching. We believe the\nprovided synthetic mask generating code, our novel training approach and the\ntrained weights from the masked face models will help in adopting existing face\nrecognition systems to operate in the current global environment. We\nopen-source all contributions for broader use by the research community.",
    "descriptor": "",
    "authors": [
      "Sachith Seneviratne",
      "Nuran Kasthuriaarachchi",
      "Sanka Rasnayaka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.05596"
  },
  {
    "id": "arXiv:2106.05597",
    "title": "Supervising the Transfer of Reasoning Patterns in VQA",
    "abstract": "Methods for Visual Question Anwering (VQA) are notorious for leveraging\ndataset biases rather than performing reasoning, hindering generalization. It\nhas been recently shown that better reasoning patterns emerge in attention\nlayers of a state-of-the-art VQA model when they are trained on perfect\n(oracle) visual inputs. This provides evidence that deep neural networks can\nlearn to reason when training conditions are favorable enough. However,\ntransferring this learned knowledge to deployable models is a challenge, as\nmuch of it is lost during the transfer. We propose a method for knowledge\ntransfer based on a regularization term in our loss function, supervising the\nsequence of required reasoning operations. We provide a theoretical analysis\nbased on PAC-learning, showing that such program prediction can lead to\ndecreased sample complexity under mild hypotheses. We also demonstrate the\neffectiveness of this approach experimentally on the GQA dataset and show its\ncomplementarity to BERT-like self-supervised pre-training.",
    "descriptor": "",
    "authors": [
      "Corentin Kervadec",
      "Christian Wolf",
      "Grigory Antipov",
      "Moez Baccouche",
      "Madiha Nadri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05597"
  },
  {
    "id": "arXiv:2106.05600",
    "title": "The Isometry-Dual Property in Flags of Many-Point Algebraic Geometry  Codes",
    "abstract": "Let ${\\mathbb F}_q$ be the finite field with $q$ elements and let ${\\mathbb\nN}$ be the set of non-negative integers. A flag of linear codes $C_0 \\subsetneq\nC_1 \\subsetneq \\cdots \\subsetneq C_s$ is said to have the {\\it isometry-dual\nproperty} if there exists a vector ${\\bf x}\\in (\\mathbb{F}_q^*)^n$ such that\n$C_i={\\bf x} \\cdot C_{s-i}^\\perp$, where $C_i^\\perp$ denotes the dual code of\nthe code $C_i$. Consider ${\\mathcal F}$ a function field over ${\\mathbb F}_q$,\nand let $P$ and $Q_1,\\ldots, Q_t$ be rational places in ${\\mathcal F}$. Let the\ndivisor $D$ be the sum of pairwise different places of ${\\mathcal F}$ such that\n$P, Q_1,\\ldots, Q_t$ are not in $\\mbox{supp}(D)$, and let ${\\bf\nG}_{\\boldsymbol\\beta}$ be the divisor $\\sum_{i=1}^t\\beta_iQ_i$, for given\n$\\beta_i's \\in {\\mathbb Z}$. For suitable values of $\\beta_i's$ in ${\\mathbb\nZ}$ and varying an integer $a$ we investigate the existence of isometry-dual\nflags of codes in the families of many-point algebraic geometry codes\n$$C_\\mathcal L(D, a_0P+{\\bf G}_{\\boldsymbol\\beta})\\subsetneq C_\\mathcal L(D,\na_1P+{\\bf G}_{\\boldsymbol\\beta}))\\subsetneq \\dots \\subsetneq C_\\mathcal L(D,\na_sP+{\\bf G}_{\\boldsymbol\\beta})).$$ We then apply the obtained results to the\nbroad class of Kummer extensions ${\\mathcal F}$ defined by affine equations of\nthe form $y^m=f(x)$, for $f(x)$ a separable polynomial of degree $r$, where\n$\\gcd(r, m)=1$. In particular, depending on the place $P$, we obtain necessary\nand sufficient conditions depending on $m$ and $\\beta_i$'s such that the flag\nhas the isometry-dual property.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2005.12239\n",
    "authors": [
      "Maria Bras-Amor\u00f3s",
      "Alonso S. Castellanos",
      "Luciane Quoos"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2106.05600"
  },
  {
    "id": "arXiv:2106.05601",
    "title": "MiDeCon: Unsupervised and Accurate Fingerprint and Minutia Quality  Assessment based on Minutia Detection Confidence",
    "abstract": "An essential factor to achieve high accuracies in fingerprint recognition\nsystems is the quality of its samples. Previous works mainly proposed\nsupervised solutions based on image properties that neglects the minutiae\nextraction process, despite that most fingerprint recognition techniques are\nbased on detected minutiae. Consequently, a fingerprint image might be assigned\na high quality even if the utilized minutia extractor produces unreliable\ninformation. In this work, we propose a novel concept of assessing minutia and\nfingerprint quality based on minutia detection confidence (MiDeCon). MiDeCon\ncan be applied to an arbitrary deep learning based minutia extractor and does\nnot require quality labels for learning. We propose using the detection\nreliability of the extracted minutia as its quality indicator. By combining the\nhighest minutia qualities, MiDeCon also accurately determines the quality of a\nfull fingerprint. Experiments are conducted on the publicly available databases\nof the FVC 2006 and compared against several baselines, such as NIST's\nwidely-used fingerprint image quality software NFIQ1 and NFIQ2. The results\ndemonstrate a significantly stronger quality assessment performance of the\nproposed MiDeCon-qualities as related works on both, minutia- and\nfingerprint-level. The implementation is publicly available.",
    "descriptor": "\nComments: Accepted at IJCB 2021\n",
    "authors": [
      "Philipp Terh\u00f6rst",
      "Andr\u00e9 Boller",
      "Naser Damer",
      "Florian Kirchbuchner",
      "Arjan Kuijper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05601"
  },
  {
    "id": "arXiv:2106.05606",
    "title": "VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and  Summarization",
    "abstract": "Video transcript summarization is a fundamental task for video understanding.\nConventional approaches for transcript summarization are usually built upon the\nsummarization data for written language such as news articles, while the domain\ndiscrepancy may degrade the model performance on spoken text. In this paper, we\npresent VT-SSum, a benchmark dataset with spoken language for video transcript\nsegmentation and summarization, which includes 125K transcript-summary pairs\nfrom 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET\nby leveraging the slides content as the weak supervision to generate the\nextractive summary for video transcripts. Experiments with a state-of-the-art\ndeep learning approach show that the model trained with VT-SSum brings a\nsignificant improvement on the AMI spoken text summarization benchmark. VT-SSum\nwill be publicly available to support the future research of video transcript\nsegmentation and summarization tasks.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Tengchao Lv",
      "Lei Cui",
      "Momcilo Vasilijevic",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05606"
  },
  {
    "id": "arXiv:2106.05607",
    "title": "Spatially Invariant Unsupervised 3D Object Segmentation with Graph  Neural Networks",
    "abstract": "In this paper, we tackle the problem of unsupervised 3D object segmentation\nfrom a point cloud without RGB information. In particular, we propose a\nframework,~{\\bf SPAIR3D}, to model a point cloud as a spatial mixture model and\njointly learn the multiple-object representation and segmentation in 3D via\nVariational Autoencoders (VAE). Inspired by SPAIR, we adopt an\nobject-specification scheme that describes each object's location relative to\nits local voxel grid cell rather than the point cloud as a whole. To model the\nspatial mixture model on point clouds, we derive the~\\emph{Chamfer Likelihood},\nwhich fits naturally into the variational training pipeline. We further design\na new spatially invariant graph neural network to generate a varying number of\n3D points as a decoder within our VAE.~Experimental results demonstrate\nthat~{\\bf SPAIR3D} is capable of detecting and segmenting variable number of\nobjects without appearance information across diverse scenes.",
    "descriptor": "",
    "authors": [
      "Tianyu Wang",
      "Kee Siong Ng",
      "Miaomiao Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05607"
  },
  {
    "id": "arXiv:2106.05608",
    "title": "Thompson Sampling with a Mixture Prior",
    "abstract": "We study Thompson sampling (TS) in online decision-making problems where the\nuncertain environment is sampled from a mixture distribution. This is relevant\nto multi-task settings, where a learning agent is faced with different classes\nof problems. We incorporate this structure in a natural way by initializing TS\nwith a mixture prior -- dubbed MixTS -- and develop a novel, general technique\nfor analyzing the regret of TS with such priors. We apply this technique to\nderive Bayes regret bounds for MixTS in both linear bandits and tabular Markov\ndecision processes (MDPs). Our regret bounds reflect the structure of the\nproblem and depend on the number of components and confidence width of each\ncomponent of the prior. Finally, we demonstrate the empirical effectiveness of\nMixTS in both synthetic and real-world experiments.",
    "descriptor": "\nComments: 22 pages, 3 figures\n",
    "authors": [
      "Joey Hong",
      "Branislav Kveton",
      "Manzil Zaheer",
      "Mohammad Ghavamzadeh",
      "Craig Boutilier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05608"
  },
  {
    "id": "arXiv:2106.05609",
    "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via  Historical Embeddings",
    "abstract": "We present GNNAutoScale (GAS), a framework for scaling arbitrary\nmessage-passing GNNs to large graphs. GAS prunes entire sub-trees of the\ncomputation graph by utilizing historical embeddings from prior training\niterations, leading to constant GPU memory consumption in respect to input node\nsize without dropping any data. While existing solutions weaken the expressive\npower of message passing due to sub-sampling of edges or non-trainable\npropagations, our approach is provably able to maintain the expressive power of\nthe original GNN. We achieve this by providing approximation error bounds of\nhistorical embeddings and show how to tighten them in practice. Empirically, we\nshow that the practical realization of our framework, PyGAS, an easy-to-use\nextension for PyTorch Geometric, is both fast and memory-efficient, learns\nexpressive node representations, closely resembles the performance of their\nnon-scaling counterparts, and reaches state-of-the-art performance on\nlarge-scale graphs.",
    "descriptor": "\nComments: Published as a conference paper at ICML 2021\n",
    "authors": [
      "Matthias Fey",
      "Jan E. Lenssen",
      "Frank Weichert",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05609"
  },
  {
    "id": "arXiv:2106.05610",
    "title": "Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time",
    "abstract": "We study the widely used hierarchical agglomerative clustering (HAC)\nalgorithm on edge-weighted graphs. We define an algorithmic framework for\nhierarchical agglomerative graph clustering that provides the first efficient\n$\\tilde{O}(m)$ time exact algorithms for classic linkage measures, such as\ncomplete- and WPGMA-linkage, as well as other measures. Furthermore, for\naverage-linkage, arguably the most popular variant of HAC, we provide an\nalgorithm that runs in $\\tilde{O}(n\\sqrt{m})$ time. For this variant, this is\nthe first exact algorithm that runs in subquadratic time, as long as\n$m=n^{2-\\epsilon}$ for some constant $\\epsilon > 0$. We complement this result\nwith a simple $\\epsilon$-close approximation algorithm for average-linkage in\nour framework that runs in $\\tilde{O}(m)$ time. As an application of our\nalgorithms, we consider clustering points in a metric space by first using\n$k$-NN to generate a graph from the point set, and then running our algorithms\non the resulting weighted graph. We validate the performance of our algorithms\non publicly available datasets, and show that our approach can speed up\nclustering of point datasets by a factor of 20.7--76.5x.",
    "descriptor": "\nComments: This is the full version of the paper appearing in ICML'21\n",
    "authors": [
      "Laxman Dhulipala",
      "David Eisenstat",
      "Jakub \u0141\u0105cki",
      "Vahab Mirrokni",
      "Jessica Shi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05610"
  },
  {
    "id": "arXiv:2106.05611",
    "title": "Context-Free TextSpotter for Real-Time and Mobile End-to-End Text  Detection and Recognition",
    "abstract": "In the deployment of scene-text spotting systems on mobile platforms,\nlightweight models with low computation are preferable. In concept, end-to-end\n(E2E) text spotting is suitable for such purposes because it performs text\ndetection and recognition in a single model. However, current state-of-the-art\nE2E methods rely on heavy feature extractors, recurrent sequence modellings,\nand complex shape aligners to pursue accuracy, which means their computations\nare still heavy. We explore the opposite direction: How far can we go without\nbells and whistles in E2E text spotting? To this end, we propose a\ntext-spotting method that consists of simple convolutions and a few\npost-processes, named Context-Free TextSpotter. Experiments using standard\nbenchmarks show that Context-Free TextSpotter achieves real-time text spotting\non a GPU with only three million parameters, which is the smallest and fastest\namong existing deep text spotters, with an acceptable transcription quality\ndegradation compared to heavier ones. Further, we demonstrate that our text\nspotter can run on a smartphone with affordable latency, which is valuable for\nbuilding stand-alone OCR applications.",
    "descriptor": "\nComments: To appear in ICDAR2021\n",
    "authors": [
      "Ryota Yoshihashi",
      "Tomohiro Tanaka",
      "Kenji Doi",
      "Takumi Fujino",
      "Naoaki Yamashita"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05611"
  },
  {
    "id": "arXiv:2106.05616",
    "title": "SVMA: A GAN-based model for Monocular 3D Human Pose Estimation",
    "abstract": "Recovering 3D human pose from 2D joints is a highly unconstrained problem,\nespecially without any video or multi-view information. We present an\nunsupervised GAN-based model to recover 3D human pose from 2D joint locations\nextracted from a single image. Our model uses a GAN to learn the mapping of\ndistribution from 2D poses to 3D poses, not the simple 2D-3D correspondence.\nConsidering the reprojection constraint, our model can estimate the camera so\nthat we can reproject the estimated 3D pose to the original 2D pose. Based on\nthis reprojection method, we can rotate and reproject the generated pose to get\nour \"new\" 2D pose and then use a weight sharing generator to estimate the \"new\"\n3D pose and a \"new\" camera. Through the above estimation process, we can define\nthe single-view-multi-angle consistency loss during training to simulate\nmulti-view consistency, which means the 3D poses and cameras estimated from two\nangles of a single view should be able to be mixed to generate rich 2D\nreprojections, and the 2D reprojections reprojected from the same 3D pose\nshould be consistent. The experimental results on Human3.6M show that our\nmethod outperforms all the state-of-the-art methods, and results on\nMPI-INF-3DHP show that our method outperforms state-of-the-art by approximately\n15.0%.",
    "descriptor": "",
    "authors": [
      "Yicheng Deng",
      "Yongqi Sun",
      "Jiahui Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05616"
  },
  {
    "id": "arXiv:2106.05618",
    "title": "Date Estimation in the Wild of Scanned Historical Photos: An Image  Retrieval Approach",
    "abstract": "This paper presents a novel method for date estimation of historical\nphotographs from archival sources. The main contribution is to formulate the\ndate estimation as a retrieval task, where given a query, the retrieved images\nare ranked in terms of the estimated date similarity. The closer are their\nembedded representations the closer are their dates. Contrary to the\ntraditional models that design a neural network that learns a classifier or a\nregressor, we propose a learning objective based on the nDCG ranking metric. We\nhave experimentally evaluated the performance of the method in two different\ntasks: date estimation and date-sensitive image retrieval, using the DEW public\ndatabase, overcoming the baseline methods.",
    "descriptor": "\nComments: Accepted at ICDAR 2021\n",
    "authors": [
      "Adri\u00e0 Molina",
      "Pau Riba",
      "Lluis Gomez",
      "Oriol Ramos-Terrades",
      "Josep Llad\u00f3s"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05618"
  },
  {
    "id": "arXiv:2106.05620",
    "title": "Efficient Exact k-Flexible Aggregate Nearest Neighbor Search in Road  Networks Using the M-tree",
    "abstract": "This study proposes an efficient exact k-flexible aggregate nearest neighbor\n(k-FANN) search algorithm in road networks using the M-tree. The IER-kNN\nalgorithm, which previously showed the highest FANN search performance, used\nthe R-tree and pruned off unnecessary nodes based on the Euclidean coordinates\nof objects in road networks. However, IER-kNN made many unnecessary accesses to\nindex nodes since the Euclidean distance between objects is much different from\nthe actual shortest-path distance between them. In contrast, our algorithm\nproposed in this study can greatly reduce unnecessary accesses to index nodes\ncompared to IER-kNN since the M-tree is constructed based on the actual\nshortest-path distances between objects. To the best of our knowledge, our\nalgorithm is the first exact FANN algorithm using the M-tree. We prove that our\nalgorithm does not cause any false drop. As a result of a series of experiments\nusing various real road network datasets, our algorithm always showed a better\nperformance than IER-kNN and was improved by up to 6.92 times.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Moonyoung Chung",
      "Soon J. Hyun",
      "Woong-Kee Loh"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.05620"
  },
  {
    "id": "arXiv:2106.05625",
    "title": "Towards an Automated Pipeline for Detecting and Classifying Malware  through Machine Learning",
    "abstract": "The constant growth in the number of malware - software or code fragment\npotentially harmful for computers and information networks - and the use of\nsophisticated evasion and obfuscation techniques have seriously hindered\nclassic signature-based approaches. On the other hand, malware detection\nsystems based on machine learning techniques started offering a promising\nalternative to standard approaches, drastically reducing analysis time and\nturning out to be more robust against evasion and obfuscation techniques. In\nthis paper, we propose a malware taxonomic classification pipeline able to\nclassify Windows Portable Executable files (PEs). Given an input PE sample, it\nis first classified as either malicious or benign. If malicious, the pipeline\nfurther analyzes it in order to establish its threat type, family, and\nbehavior(s). We tested the proposed pipeline on the open source dataset EMBER,\ncontaining approximately 1 million PE samples, analyzed through static\nanalysis. Obtained malware detection results are comparable to other academic\nworks in the current state of art and, in addition, we provide an in-depth\nclassification of malicious samples. Models used in the pipeline provides\ninterpretable results which can help security analysts in better understanding\ndecisions taken by the automated pipeline.",
    "descriptor": "\nComments: 12 pages and 6 figures. Presented at ITASEC'21\n",
    "authors": [
      "Nicola Loi",
      "Claudio Borile",
      "Daniele Ucci"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05625"
  },
  {
    "id": "arXiv:2106.05626",
    "title": "Citation Swing Factor: An Indicator to Measure the Diffusion of Cited  Items",
    "abstract": "The h-index, introduced by Hirsch, is based on the mutual variation between\nthe number of cited and source items. The temporally continuous nature of the\ncitation accretion process causes a shift of cited items from the h-core zone\nto the adjacent citation-asymmetric zones, viz. h-excess zone, or h-tail zone.\nThe name given to this shifting phenomenon is the Diffusion of Cited Items\n(DCI). In this paper, two new variables are introduced, i.e., the Fold of\nExcess citation over Total citations (FET), denoted by $\\epsilon^2$ and the\nFold of h-core citation over Excess citations (FHE), denoted by $\\theta^2$. On\nthe basis of $\\theta$ and $\\epsilon$, another indicator is introduced, i.e.,\nthe Citation Swing Factor (CSF), defined as $d\\theta/d\\epsilon$, which\nindicates the differential coefficient of $\\theta$ with respect to $\\epsilon$.\nThe time dependence of FET and FHE is also discussed. The possible solutions of\nare derived here. The functionality of CSF ($d\\theta/d\\epsilon$) to measure the\ndiffusion process quantitatively will be tested later on for journals, authors\nand institutions.",
    "descriptor": "\nComments: 5 pages, 1 Figure\n",
    "authors": [
      "Bidyarthi Dutta"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.05626"
  },
  {
    "id": "arXiv:2106.05627",
    "title": "A Comparison and Combination of Unsupervised Blind Source Separation  Techniques",
    "abstract": "Unsupervised blind source separation methods do not require a training phase\nand thus cannot suffer from a train-test mismatch, which is a common concern in\nneural network based source separation. The unsupervised techniques can be\ncategorized in two classes, those building upon the sparsity of speech in the\nShort-Time Fourier transform domain and those exploiting non-Gaussianity or\nnon-stationarity of the source signals. In this contribution, spatial mixture\nmodels which fall in the first category and independent vector analysis (IVA)\nas a representative of the second category are compared w.r.t. their separation\nperformance and the performance of a downstream speech recognizer on a\nreverberant dataset of reasonable size. Furthermore, we introduce a serial\nconcatenation of the two, where the result of the mixture model serves as\ninitialization of IVA, which achieves significantly better WER performance than\neach algorithm individually and even approaches the performance of a much more\ncomplex neural network based technique.",
    "descriptor": "\nComments: Submitted to ITG 2021\n",
    "authors": [
      "Christoph Boeddeker",
      "Frederik Rautenberg",
      "Reinhold Haeb-Umbach"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05627"
  },
  {
    "id": "arXiv:2106.05630",
    "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training",
    "abstract": "Symbolic music understanding, which refers to the understanding of music from\nthe symbolic data (e.g., MIDI format, but not audio), covers many music\napplications such as genre classification, emotion classification, and music\npieces matching. While good music representations are beneficial for these\napplications, the lack of training data hinders representation learning.\nInspired by the success of pre-training models in natural language processing,\nin this paper, we develop MusicBERT, a large-scale pre-trained model for music\nunderstanding. To this end, we construct a large-scale symbolic music corpus\nthat contains more than 1 million music songs. Since symbolic music contains\nmore structural (e.g., bar, position) and diverse information (e.g., tempo,\ninstrument, and pitch), simply adopting the pre-training techniques from NLP to\nsymbolic music only brings marginal gains. Therefore, we design several\nmechanisms, including OctupleMIDI encoding and bar-level masking strategy, to\nenhance pre-training with symbolic music data. Experiments demonstrate the\nadvantages of MusicBERT on four music understanding tasks, including melody\ncompletion, accompaniment suggestion, genre classification, and style\nclassification. Ablation studies also verify the effectiveness of our designs\nof OctupleMIDI encoding and bar-level masking strategy in MusicBERT.",
    "descriptor": "\nComments: Accepted by ACL 2021 Findings\n",
    "authors": [
      "Mingliang Zeng",
      "Xu Tan",
      "Rui Wang",
      "Zeqian Ju",
      "Tao Qin",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05630"
  },
  {
    "id": "arXiv:2106.05631",
    "title": "Finding normal binary floating-point factors in constant time",
    "abstract": "Solving the floating-point equation $x \\otimes y = z$, where $x$, $y$ and $z$\nbelong to floating-point intervals, is a common task in automated reasoning for\nwhich no efficient algorithm is known in general. We show that it can be solved\nby computing a constant number of floating-point factors, and give a\nconstant-time algorithm for computing successive normal floating-point factors\nof normal floating-point numbers in radix 2. This leads to a constant-time\nprocedure for solving the given equation.",
    "descriptor": "\nComments: 47 pages, 4 figures\n",
    "authors": [
      "Mak Andrlon"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05631"
  },
  {
    "id": "arXiv:2106.05632",
    "title": "CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities  and Optimizations",
    "abstract": "DRAM is the dominant main memory technology used in modern computing systems.\nComputing systems implement a memory controller that interfaces with DRAM via\nDRAM commands. DRAM executes the given commands using internal components\n(e.g., access transistors, sense amplifiers) that are orchestrated by DRAM\ninternal timings, which are fixed foreach DRAM command. Unfortunately, the use\nof fixed internal timings limits the types of operations that DRAM can perform\nand hinders the implementation of new functionalities and custom mechanisms\nthat improve DRAM reliability, performance and energy. To overcome these\nlimitations, we propose enabling programmable DRAM internal timings for\ncontrolling in-DRAM components. To this end, we design CODIC, a new low-cost\nDRAM substrate that enables fine-grained control over four previously fixed\ninternal DRAM timings that are key to many DRAM operations. We implement CODIC\nwith only minimal changes to the DRAM chip and the DDRx interface. To\ndemonstrate the potential of CODIC, we propose two new CODIC-based security\nmechanisms that outperform state-of-the-art mechanisms in several ways: (1) a\nnew DRAM Physical Unclonable Function (PUF) that is more robust and has\nsignificantly higher throughput than state-of-the-art DRAM PUFs, and (2) the\nfirst cold boot attack prevention mechanism that does not introduce any\nperformance or energy overheads at runtime.",
    "descriptor": "\nComments: Extended version of an ISCA 2021 paper\n",
    "authors": [
      "Lois Orosa",
      "Yaohua Wang",
      "Mohammad Sadrosadati",
      "Jeremie S. Kim",
      "Minesh Patel",
      "Ivan Puddu",
      "Haocong Luo",
      "Kaveh Razavi",
      "Juan G\u00f3mez-Luna",
      "Hasan Hassan",
      "Nika Mansouri-Ghiasi",
      "Saugata Ghose",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05632"
  },
  {
    "id": "arXiv:2106.05633",
    "title": "Citation Recommendation for Research Papers via Knowledge Graphs",
    "abstract": "Citation recommendation for research papers is a valuable task that can help\nresearchers improve the quality of their work by suggesting relevant related\nwork. Current approaches for this task rely primarily on the text of the papers\nand the citation network. In this paper, we propose to exploit an additional\nsource of information, namely research knowledge graphs (KG) that interlink\nresearch papers based on mentioned scientific concepts. Our experimental\nresults demonstrate that the combination of information from research KGs with\nexisting state-of-the-art approaches is beneficial. Experimental results are\npresented for the STM-KG (STM: Science, Technology, Medicine), which is an\nautomatically populated knowledge graph based on the scientific concepts\nextracted from papers of ten domains. The proposed approach outperforms the\nstate of the art with a mean average precision of 20.6% (+0.8) for the top-50\nretrieved results.",
    "descriptor": "\nComments: Accepted for publication in 25th International Conference on Theory and Practice of Digital Libraries (TPDL), 2021\n",
    "authors": [
      "Arthur Brack",
      "Anett Hoppe",
      "Ralph Ewerth"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05633"
  },
  {
    "id": "arXiv:2106.05634",
    "title": "Exploring Unsupervised Pretraining Objectives for Machine Translation",
    "abstract": "Unsupervised cross-lingual pretraining has achieved strong results in neural\nmachine translation (NMT), by drastically reducing the need for large parallel\ndata. Most approaches adapt masked-language modeling (MLM) to\nsequence-to-sequence architectures, by masking parts of the input and\nreconstructing them in the decoder. In this work, we systematically compare\nmasking with alternative objectives that produce inputs resembling real (full)\nsentences, by reordering and replacing words based on their context. We\npretrain models with different methods on English$\\leftrightarrow$German,\nEnglish$\\leftrightarrow$Nepali and English$\\leftrightarrow$Sinhala monolingual\ndata, and evaluate them on NMT. In (semi-) supervised NMT, varying the\npretraining objective leads to surprisingly small differences in the finetuned\nperformance, whereas unsupervised NMT is much more sensitive to it. To\nunderstand these results, we thoroughly study the pretrained models using a\nseries of probes and verify that they encode and use information in different\nways. We conclude that finetuning on parallel data is mostly sensitive to few\nproperties that are shared by most models, such as a strong decoder, in\ncontrast to unsupervised NMT that also requires models with strong\ncross-lingual abilities.",
    "descriptor": "\nComments: Findings of ACL 2021\n",
    "authors": [
      "Christos Baziotis",
      "Ivan Titov",
      "Alexandra Birch",
      "Barry Haddow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05634"
  },
  {
    "id": "arXiv:2106.05635",
    "title": "Contraction Analysis of Discrete-time Stochastic Systems",
    "abstract": "In this paper, we develop a novel contraction framework for stability\nanalysis of discrete-time nonlinear systems with parameters following\nstochastic processes. For general stochastic processes, we first provide a\nsufficient condition for uniform incremental exponential stability (UIES) in\nthe first moment with respect to a Riemannian metric. Then, focusing on the\nEuclidean distance, we present a necessary and sufficient condition for UIES in\nthe second moment. By virtue of studying general stochastic processes, we can\nreadily derive UIES conditions for special classes of processes, e.g., i.i.d.\nprocesses and Markov processes, which is demonstrated as selected applications\nof our results.",
    "descriptor": "",
    "authors": [
      "Yu Kawano",
      "Yohei Hosoe"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05635"
  },
  {
    "id": "arXiv:2106.05638",
    "title": "An Instance-optimal Algorithm for Bichromatic Rectangular Visibility",
    "abstract": "Afshani, Barbay and Chan (2017) introduced the notion of instance-optimal\nalgorithm in the order-oblivious setting. An algorithm A is instance-optimal in\nthe order-oblivious setting for a certain class of algorithms A* if the\nfollowing hold:\n- A takes as input a sequence of objects from some domain;\n- for any instance $\\sigma$ and any algorithm A' in A*, the runtime of A on\n$\\sigma$ is at most a constant factor removed from the runtime of A' on the\nworst possible permutation of $\\sigma$. If we identify permutations of a\nsequence as representing the same instance, this essentially states that A is\noptimal on every possible input (and not only in the worst case).\nWe design instance-optimal algorithms for the problem of reporting, given a\nbichromatic set of points in the plane S, all pairs consisting of points of\ndifferent color which span an empty axis-aligned rectangle (or reporting all\npoints which appear in such a pair). This problem has applications for\ntraining-set reduction in nearest-neighbour classifiers. It is also related to\nthe problem consisting of finding the decision boundaries of a euclidean\nnearest-neighbour classifier, for which Bremner et al. (2005) gave an optimal\noutput-sensitive algorithm.\nBy showing the existence of an instance-optimal algorithm in the\norder-oblivious setting for this problem we push the methods of Afshani et al.\ncloser to their limits by adapting and extending them to a setting which\nexhibits highly non-local features. Previous problems for which\ninstance-optimal algorithms were proven to exist were based solely on local\nrelationships between points in a set.",
    "descriptor": "",
    "authors": [
      "Jean Cardinal",
      "Justin Dallant",
      "John Iacono"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.05638"
  },
  {
    "id": "arXiv:2106.05642",
    "title": "U2++: Unified Two-pass Bidirectional End-to-end Model for Speech  Recognition",
    "abstract": "The unified streaming and non-streaming two-pass (U2) end-to-end model for\nspeech recognition has shown great performance in terms of streaming\ncapability, accuracy, real-time factor (RTF), and latency. In this paper, we\npresent U2++, an enhanced version of U2 to further improve the accuracy. The\ncore idea of U2++ is to use the forward and the backward information of the\nlabeling sequences at the same time at training to learn richer information,\nand combine the forward and backward prediction at decoding to give more\naccurate recognition results. We also proposed a new data augmentation method\ncalled SpecSub to help the U2++ model to be more accurate and robust. Our\nexperiments show that, compared with U2, U2++ shows faster convergence at\ntraining, better robustness to the decoding method, as well as consistent 5\\% -\n8\\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we\nachieve a 4.63\\% character error rate (CER) with a non-streaming setup and\n5.05\\% with a streaming setup with 320ms latency by U2++. To the best of our\nknowledge, 5.05\\% is the best-published streaming result on the AISHELL-1 test\nset.",
    "descriptor": "",
    "authors": [
      "Di Wu",
      "Binbin Zhang",
      "Chao Yang",
      "Zhendong Peng",
      "Wenjing Xia",
      "Xiaoyu Chen",
      "Xin Lei"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05642"
  },
  {
    "id": "arXiv:2106.05648",
    "title": "Unsupervised Behaviour Discovery with Quality-Diversity Optimisation",
    "abstract": "Quality-Diversity algorithms refer to a class of evolutionary algorithms\ndesigned to find a collection of diverse and high-performing solutions to a\ngiven problem. In robotics, such algorithms can be used for generating a\ncollection of controllers covering most of the possible behaviours of a robot.\nTo do so, these algorithms associate a behavioural descriptor to each of these\nbehaviours. Each behavioural descriptor is used for estimating the novelty of\none behaviour compared to the others. In most existing algorithms, the\nbehavioural descriptor needs to be hand-coded, thus requiring prior knowledge\nabout the task to solve. In this paper, we introduce: Autonomous Robots\nRealising their Abilities, an algorithm that uses a dimensionality reduction\ntechnique to automatically learn behavioural descriptors based on raw sensory\ndata. The performance of this algorithm is assessed on three robotic tasks in\nsimulation. The experimental results show that it performs similarly to\ntraditional hand-coded approaches without the requirement to provide any\nhand-coded behavioural descriptor. In the collection of diverse and\nhigh-performing solutions, it also manages to find behaviours that are novel\nwith respect to more features than its hand-coded baselines. Finally, we\nintroduce a variant of the algorithm which is robust to the dimensionality of\nthe behavioural descriptor space.",
    "descriptor": "",
    "authors": [
      "Luca Grillotti",
      "Antoine Cully"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05648"
  },
  {
    "id": "arXiv:2106.05652",
    "title": "Latency and Information Freshness in Multipath Communications for  Virtual Reality",
    "abstract": "Wireless Virtual Reality (VR) and Augmented Reality (AR) will contribute to\npeople increasingly working and socializing remotely. However, the VR/AR\nexperience is very susceptible to various delays and timing discrepancies,\nwhich can lead to motion sickness and discomfort. This paper models and\nexploits the existence of multiple paths and redundancy to improve the timing\nperformance of wireless VR communications. We consider Multiple Description\nCoding (MDC), a scheme where the video stream is encoded in Q streams (Q = 2 in\nthis paper) known as descriptors and delivered independently over multiple\npaths. We also consider an alternating scheme, that simply switches between the\npaths. We analyze the full distribution of two relevant metrics: the packet\ndelay and the Peak Age of Information (PAoI), which measures the freshness of\nthe information at the receiver. The results show interesting trade-offs\nbetween picture quality, frame rate, and latency: full duplication results in\nfewer lost frames, but a higher latency than schemes with less redundancy. Even\nthe simple alternating scheme can outperform duplication in terms of PAoI, but\nMDC can exploit the independent decodability of the descriptors to deliver a\nbasic version of the frames faster, while still getting the full-quality frames\nwith a slightly higher delay.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Communications\n",
    "authors": [
      "Federico Chiariotti",
      "Beatriz Soret",
      "Petar Popovski"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05652"
  },
  {
    "id": "arXiv:2106.05654",
    "title": "Visual scoping operations for physical assembly",
    "abstract": "Planning is hard. The use of subgoals can make planning more tractable, but\nselecting these subgoals is computationally costly. What algorithms might\nenable us to reap the benefits of planning using subgoals while minimizing the\ncomputational overhead of selecting them? We propose visual scoping, a strategy\nthat interleaves planning and acting by alternately defining a spatial region\nas the next subgoal and selecting actions to achieve it. We evaluated our\nvisual scoping algorithm on a variety of physical assembly problems against two\nbaselines: planning all subgoals in advance and planning without subgoals. We\nfound that visual scoping achieves comparable task performance to the subgoal\nplanner while requiring only a fraction of the total computational cost.\nTogether, these results contribute to our understanding of how humans might\nmake efficient use of cognitive resources to solve complex planning problems.",
    "descriptor": "",
    "authors": [
      "Felix J Binder",
      "Marcelo M Mattar",
      "David Kirsh",
      "Judith E Fan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05654"
  },
  {
    "id": "arXiv:2106.05656",
    "title": "MST: Masked Self-Supervised Transformer for Visual Representation",
    "abstract": "Transformer has been widely used for self-supervised pre-training in Natural\nLanguage Processing (NLP) and achieved great success. However, it has not been\nfully explored in visual self-supervised learning. Meanwhile, previous methods\nonly consider the high-level feature and learning representation from a global\nperspective, which may fail to transfer to the downstream dense prediction\ntasks focusing on local features. In this paper, we present a novel Masked\nSelf-supervised Transformer approach named MST, which can explicitly capture\nthe local context of an image while preserving the global semantic information.\nSpecifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose\na masked token strategy based on the multi-head self-attention map, which\ndynamically masks some tokens of local patches without damaging the crucial\nstructure for self-supervised learning. More importantly, the masked tokens\ntogether with the remaining tokens are further recovered by a global image\ndecoder, which preserves the spatial information of the image and is more\nfriendly to the downstream dense prediction tasks. The experiments on multiple\ndatasets demonstrate the effectiveness and generality of the proposed method.\nFor instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using\n300-epoch pre-training by linear evaluation, which outperforms supervised\nmethods with the same epoch by 0.4% and its comparable variant DINO by 1.0\\%.\nFor dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object\ndetection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch\npre-training.",
    "descriptor": "",
    "authors": [
      "Zhaowen Li",
      "Zhiyang Chen",
      "Fan Yang",
      "Wei Li",
      "Yousong Zhu",
      "Chaoyang Zhao",
      "Rui Deng",
      "Liwei Wu",
      "Rui Zhao",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05656"
  },
  {
    "id": "arXiv:2106.05657",
    "title": "Deep neural network loses attention to adversarial images",
    "abstract": "Adversarial algorithms have shown to be effective against neural networks for\na variety of tasks. Some adversarial algorithms perturb all the pixels in the\nimage minimally for the image classification task in image classification. In\ncontrast, some algorithms perturb few pixels strongly. However, very little\ninformation is available regarding why these adversarial samples so diverse\nfrom each other exist. Recently, Vargas et al. showed that the existence of\nthese adversarial samples might be due to conflicting saliency within the\nneural network. We test this hypothesis of conflicting saliency by analysing\nthe Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM)\nof original and few different types of adversarial samples. We also analyse how\ndifferent adversarial samples distort the attention of the neural network\ncompared to original samples. We show that in the case of Pixel Attack,\nperturbed pixels either calls the network attention to themselves or divert the\nattention from them. Simultaneously, the Projected Gradient Descent Attack\nperturbs pixels so that intermediate layers inside the neural network lose\nattention for the correct class. We also show that both attacks affect the\nsaliency map and activation maps differently. Thus, shedding light on why some\ndefences successful against some attacks remain vulnerable against other\nattacks. We hope that this analysis will improve understanding of the existence\nand the effect of adversarial samples and enable the community to develop more\nrobust neural networks.",
    "descriptor": "\nComments: Accepted in Workshop on Artificial Intelligence Safety (AISafety 2021), IJCAI-2021\n",
    "authors": [
      "Shashank Kotyan",
      "Danilo Vasconcellos Vargas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05657"
  },
  {
    "id": "arXiv:2106.05659",
    "title": "Next-Gen Machine Learning Supported Diagnostic Systems for Spacecraft",
    "abstract": "Future short or long-term space missions require a new generation of\nmonitoring and diagnostic systems due to communication impasses as well as\nlimitations in specialized crew and equipment. Machine learning supported\ndiagnostic systems present a viable solution for medical and technical\napplications. We discuss challenges and applicability of such systems in light\nof upcoming missions and outline an example use case for a next-generation\nmedical diagnostic system for future space operations. Additionally, we present\napproach recommendations and constraints for the successful generation and use\nof machine learning models aboard a spacecraft.",
    "descriptor": "\nComments: Accepted in the AI for Spacecraft Longevity Workshop at IJCAI2021\n",
    "authors": [
      "Athanasios Vlontzos",
      "Gabriel Sutherland",
      "Siddha Ganju",
      "Frank Soboczenski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05659"
  },
  {
    "id": "arXiv:2106.05662",
    "title": "To The Point: Correspondence-driven monocular 3D category reconstruction",
    "abstract": "We present To The Point (TTP), a method for reconstructing 3D objects from a\nsingle image using 2D to 3D correspondences learned from weak supervision. We\nrecover a 3D shape from a 2D image by first regressing the 2D positions\ncorresponding to the 3D template vertices and then jointly estimating a rigid\ncamera transform and non-rigid template deformation that optimally explain the\n2D positions through the 3D shape projection. By relying on 3D-2D\ncorrespondences we use a simple per-sample optimization problem to replace\nCNN-based regression of camera pose and non-rigid deformation and thereby\nobtain substantially more accurate 3D reconstructions. We treat this\noptimization as a differentiable layer and train the whole system in an\nend-to-end manner. We report systematic quantitative improvements on multiple\ncategories and provide qualitative results comprising diverse shape, pose and\ntexture prediction examples. Project website:\nhttps://fkokkinos.github.io/to_the_point/.",
    "descriptor": "",
    "authors": [
      "Filippos Kokkinos",
      "Iasonas Kokkinos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05662"
  },
  {
    "id": "arXiv:2106.05664",
    "title": "Ruddit: Norms of Offensiveness for English Reddit Comments",
    "abstract": "On social media platforms, hateful and offensive language negatively impact\nthe mental well-being of users and the participation of people from diverse\nbackgrounds. Automatic methods to detect offensive language have largely relied\non datasets with categorical labels. However, comments can vary in their degree\nof offensiveness. We create the first dataset of English language Reddit\ncomments that has \\textit{fine-grained, real-valued scores} between -1\n(maximally supportive) and 1 (maximally offensive). The dataset was annotated\nusing \\emph{Best--Worst Scaling}, a form of comparative annotation that has\nbeen shown to alleviate known biases of using rating scales. We show that the\nmethod produces highly reliable offensiveness scores. Finally, we evaluate the\nability of widely-used neural models to predict offensiveness scores on this\nnew dataset.",
    "descriptor": "\nComments: Camera-ready version in ACL 2021\n",
    "authors": [
      "Rishav Hada",
      "Sohi Sudhir",
      "Pushkar Mishra",
      "Helen Yannakoudakis",
      "Saif M. Mohammad",
      "Ekaterina Shutova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05664"
  },
  {
    "id": "arXiv:2106.05665",
    "title": "Adaptive Streaming Perception using Deep Reinforcement Learning",
    "abstract": "Executing computer vision models on streaming visual data, or streaming\nperception is an emerging problem, with applications in self-driving, embodied\nagents, and augmented/virtual reality. The development of such systems is\nlargely governed by the accuracy and latency of the processing pipeline. While\npast work has proposed numerous approximate execution frameworks, their\ndecision functions solely focus on optimizing latency, accuracy, or energy,\netc. This results in sub-optimum decisions, affecting the overall system\nperformance. We argue that the streaming perception systems should holistically\nmaximize the overall system performance (i.e., considering both accuracy and\nlatency simultaneously). To this end, we describe a new approach based on deep\nreinforcement learning to learn these tradeoffs at runtime for streaming\nperception. This tradeoff optimization is formulated as a novel deep contextual\nbandit problem and we design a new reward function that holistically integrates\nlatency and accuracy into a single metric. We show that our agent can learn a\ncompetitive policy across multiple decision dimensions, which outperforms\nstate-of-the-art policies on public datasets.",
    "descriptor": "\nComments: 19 pages, 17 figures\n",
    "authors": [
      "Anurag Ghosh",
      "Akshay Nambi",
      "Aditya Singh",
      "Harish YVS",
      "Tanuja Ganu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05665"
  },
  {
    "id": "arXiv:2106.05667",
    "title": "GraphiT: Encoding Graph Structure in Transformers",
    "abstract": "We show that viewing graphs as sets of node features and incorporating\nstructural and positional information into a transformer architecture is able\nto outperform representations learned with classical graph neural networks\n(GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative\npositional encoding strategies in self-attention scores based on positive\ndefinite kernels on graphs, and (ii) enumerating and encoding local\nsub-structures such as paths of short length. We thoroughly evaluate these two\nideas on many classification and regression tasks, demonstrating the\neffectiveness of each of them independently, as well as their combination. In\naddition to performing well on standard benchmarks, our model also admits\nnatural visualization mechanisms for interpreting graph motifs explaining the\npredictions, making it a potentially strong candidate for scientific\napplications where interpretation is important. Code available at\nhttps://github.com/inria-thoth/GraphiT.",
    "descriptor": "",
    "authors": [
      "Gr\u00e9goire Mialon",
      "Dexiong Chen",
      "Margot Selosse",
      "Julien Mairal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05667"
  },
  {
    "id": "arXiv:2106.05671",
    "title": "Outage Performance of $3$D Mobile UAV Caching for Hybrid  Satellite-Terrestrial Networks",
    "abstract": "In this paper, we consider a hybrid satellite-terrestrial network (HSTN)\nwhere a multiantenna satellite communicates with a ground user equipment (UE)\nwith the help of multiple cache-enabled amplify-and-forward (AF)\nthree-dimensional ($3$D) mobile unmanned aerial vehicle (UAV) relays. Herein,\nwe employ the two fundamental most popular content (MPC) and uniform content\n(UC) caching schemes for two types of mobile UAV relays, namely fully $3$D and\nfixed height. Taking into account the multiantenna satellite links and the\nrandom $3$D distances between UAV relays and UE, we analyze the outage\nprobability (OP) of considered system with MPC and UC caching schemes. We\nfurther carry out the corresponding asymptotic OP analysis to present the\ninsights on achievable performance gains of two schemes for both types of $3$D\nmobile UAV relaying. Specifically, we show the following: (a) MPC caching\ndominates the UC and no caching schemes; (b) fully $3$D mobile UAV relaying\noutperforms its fixed height counterpart. We finally corroborate the theoretic\nanalysis by simulations.",
    "descriptor": "\nComments: 17 pages, 3 figures, Submitted to IEEE for possible publication\n",
    "authors": [
      "Pankaj K. Sharma",
      "Deepika Gupta",
      "Dong In Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.05671"
  },
  {
    "id": "arXiv:2106.05673",
    "title": "Blockchain and 6G: The Future of Secure and Ubiquitous Communication",
    "abstract": "The future communication will be characterized by ubiquitous connectivity and\nsecurity. These features will be essential requirements for the efficient\nfunctioning of the futuristic applications. In this paper, in order to\nhighlight the impact of blockchain and 6G on the future communication systems,\nwe categorize these application requirements into two broad groups. In the\nfirst category, called Requirement Group I \\mbox{(RG-I)}, we include the\nperformance-related needs on data rates, latency, reliability and massive\nconnectivity, while in the second category, called Requirement Group II\n\\mbox{(RG-II)}, we include the security-related needs on data integrity,\nnon-repudiability, and auditability. With blockchain and 6G, the network\ndecentralization and resource sharing would minimize resource under-utilization\nthereby facilitating RG-I targets. Furthermore, through appropriate selection\nof blockchain type and consensus algorithms, RG-II needs of 6G applications can\nalso be readily addressed. Through this study, the combination of blockchain\nand 6G emerges as an elegant solution for secure and ubiquitous future\ncommunication.",
    "descriptor": "",
    "authors": [
      "Ali Hussain Khan",
      "Naveed UL Hassan",
      "Chau Yuen",
      "Jun Zhao",
      "Dusit Niyato",
      "Yan Zhang",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05673"
  },
  {
    "id": "arXiv:2106.05677",
    "title": "DT-grams: Structured Dependency Grammar Stylometry for Cross-Language  Authorship Attribution",
    "abstract": "Cross-language authorship attribution problems rely on either translation to\nenable the use of single-language features, or language-independent feature\nextraction methods. Until recently, the lack of datasets for this problem\nhindered the development of the latter, and single-language solutions were\nperformed on machine-translated corpora. In this paper, we present a novel\nlanguage-independent feature for authorship analysis based on dependency graphs\nand universal part of speech tags, called DT-grams (dependency tree grams),\nwhich are constructed by selecting specific sub-parts of the dependency graph\nof sentences. We evaluate DT-grams by performing cross-language authorship\nattribution on untranslated datasets of bilingual authors, showing that, on\naverage, they achieve a macro-averaged F1 score of 0.081 higher than previous\nmethods across five different language pairs. Additionally, by providing\nresults for a diverse set of features for comparison, we provide a baseline on\nthe previously undocumented task of untranslated cross-language authorship\nattribution.",
    "descriptor": "\nComments: To be published in: \"32. GI-Workshop Grundlagen von Datenbanken\"\n",
    "authors": [
      "Benjamin Murauer",
      "G\u00fcnther Specht"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05677"
  },
  {
    "id": "arXiv:2106.05680",
    "title": "A multi-objective perspective on jointly tuning hardware and  hyperparameters",
    "abstract": "In addition to the best model architecture and hyperparameters, a full AutoML\nsolution requires selecting appropriate hardware automatically. This can be\nframed as a multi-objective optimization problem: there is not a single best\nhardware configuration but a set of optimal ones achieving different trade-offs\nbetween cost and runtime. In practice, some choices may be overly costly or\ntake days to train. To lift this burden, we adopt a multi-objective approach\nthat selects and adapts the hardware configuration automatically alongside\nneural architectures and their hyperparameters. Our method builds on Hyperband\nand extends it in two ways. First, we replace the stopping rule used in\nHyperband by a non-dominated sorting rule to preemptively stop unpromising\nconfigurations. Second, we leverage hyperparameter evaluations from related\ntasks via transfer learning by building a probabilistic estimate of the Pareto\nfront that finds promising configurations more efficiently than random search.\nWe show in extensive NAS and HPO experiments that both ingredients bring\nsignificant speed-ups and cost savings, with little to no impact on accuracy.\nIn three benchmarks where hardware is selected in addition to hyperparameters,\nwe obtain runtime and cost reductions of at least 5.8x and 8.8x, respectively.\nFurthermore, when applying our multi-objective method to the tuning of\nhyperparameters only, we obtain a 10\\% improvement in runtime while maintaining\nthe same accuracy on two popular NAS benchmarks.",
    "descriptor": "",
    "authors": [
      "David Salinas",
      "Valerio Perrone",
      "Olivier Cruchant",
      "Cedric Archambeau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05680"
  },
  {
    "id": "arXiv:2106.05681",
    "title": "A Dataset And Benchmark Of Underwater Object Detection For Robot Picking",
    "abstract": "Underwater object detection for robot picking has attracted a lot of\ninterest. However, it is still an unsolved problem due to several challenges.\nWe take steps towards making it more realistic by addressing the following\nchallenges. Firstly, the currently available datasets basically lack the test\nset annotations, causing researchers must compare their method with other SOTAs\non a self-divided test set (from the training set). Training other methods lead\nto an increase in workload and different researchers divide different datasets,\nresulting there is no unified benchmark to compare the performance of different\nalgorithms. Secondly, these datasets also have other shortcomings, e.g., too\nmany similar images or incomplete labels. Towards these challenges we introduce\na dataset, Detecting Underwater Objects (DUO), and a corresponding benchmark,\nbased on the collection and re-annotation of all relevant datasets. DUO\ncontains a collection of diverse underwater images with more rational\nannotations. The corresponding benchmark provides indicators of both efficiency\nand accuracy of SOTAs (under the MMDtection framework) for academic research\nand industrial applications, where JETSON AGX XAVIER is used to assess detector\nspeed to simulate the robot-embedded environment.",
    "descriptor": "",
    "authors": [
      "Chongwei Liu",
      "Haojie Li",
      "Shuchang Wang",
      "Ming Zhu",
      "Dong Wang",
      "Xin Fan",
      "Zhihui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05681"
  },
  {
    "id": "arXiv:2106.05682",
    "title": "Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced  Semi-Supervised Learning",
    "abstract": "The capability of the traditional semi-supervised learning (SSL) methods is\nfar from real-world application since they do not consider (1) class imbalance\nand (2) class distribution mismatch between labeled and unlabeled data. This\npaper addresses such a relatively under-explored problem, imbalanced\nsemi-supervised learning, where heavily biased pseudo-labels can harm the model\nperformance. Interestingly, we find that the semantic pseudo-labels from a\nsimilarity-based classifier in feature space and the traditional pseudo-labels\nfrom the linear classifier show the complementary property. To this end, we\npropose a general pseudo-labeling framework to address the bias motivated by\nthis observation. The key idea is to class-adaptively blend the semantic\npseudo-label to the linear one, depending on the current pseudo-label\ndistribution. Thereby, the increased semantic pseudo-label component suppresses\nthe false positives in the majority classes and vice versa. We term the novel\npseudo-labeling framework for imbalanced SSL as Distribution-Aware\nSemantics-Oriented (DASO) Pseudo-label. Extensive evaluation on CIFAR10/100-LT\nand STL10-LT shows that DASO consistently outperforms both recently proposed\nre-balancing methods for label and pseudo-label. Moreover, we demonstrate that\ntypical SSL algorithms can effectively benefit from unlabeled data with DASO,\nespecially when (1) class imbalance and (2) class distribution mismatch exist\nand even on recent real-world Semi-Aves benchmark.",
    "descriptor": "\nComments: \"Code: this https URL\"\n",
    "authors": [
      "Youngtaek Oh",
      "Dong-Jin Kim",
      "In So Kweon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05682"
  },
  {
    "id": "arXiv:2106.05686",
    "title": "Spatiotemporal Spike-Pattern Selectivity in Single Mixed-Signal Neurons  with Balanced Synapses",
    "abstract": "Realizing the potential of mixed-signal neuromorphic processors for\nultra-low-power inference and learning requires efficient use of their\ninhomogeneous analog circuitry as well as sparse, time-based information\nencoding and processing. Here, we investigate spike-timing-based spatiotemporal\nreceptive fields of output-neurons in the Spatiotemporal Correlator (STC)\nnetwork, for which we used excitatory-inhibitory balanced disynaptic inputs\ninstead of dedicated axonal or neuronal delays. We present hardware-in-the-loop\nexperiments with a mixed-signal DYNAP-SE neuromorphic processor, in which\nfive-dimensional receptive fields of hardware neurons were mapped by randomly\nsampling input spike-patterns from a uniform distribution. We find that, when\nthe balanced disynaptic elements are randomly programmed, some of the neurons\ndisplay distinct receptive fields. Furthermore, we demonstrate how a neuron was\ntuned to detect a particular spatiotemporal feature, to which it initially was\nnon-selective, by activating a different subset of the inhomogeneous analog\nsynaptic circuits. The energy dissipation of the balanced synaptic elements is\none order of magnitude lower per lateral connection (0.65 nJ vs 9.3 nJ per\nspike) than former delay-based neuromorphic hardware implementations. Thus, we\nshow how the inhomogeneous synaptic circuits could be utilized for\nresource-efficient implementation of STC network layers, in a way that enables\nsynapse-address reprogramming as a discrete mechanism for feature tuning.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Mattias Nilsson",
      "Foteini Liwicki",
      "Fredrik Sandin"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05686"
  },
  {
    "id": "arXiv:2106.05688",
    "title": "AI-enabled Automation for Completeness Checking of Privacy Policies",
    "abstract": "Technological advances in information sharing have raised concerns about data\nprotection. Privacy policies contain privacy-related requirements about how the\npersonal data of individuals will be handled by an organization or a software\nsystem (e.g., a web service or an app). In Europe, privacy policies are subject\nto compliance with the General Data Protection Regulation (GDPR). A\nprerequisite for GDPR compliance checking is to verify whether the content of a\nprivacy policy is complete according to the provisions of GDPR. Incomplete\nprivacy policies might result in large fines on violating organization as well\nas incomplete privacy-related software specifications. Manual completeness\nchecking is both time-consuming and error-prone. In this paper, we propose\nAI-based automation for the completeness checking of privacy policies. Through\nsystematic qualitative methods, we first build two artifacts to characterize\nthe privacy-related provisions of GDPR, namely a conceptual model and a set of\ncompleteness criteria. Then, we develop an automated solution on top of these\nartifacts by leveraging a combination of natural language processing and\nsupervised machine learning. Specifically, we identify the GDPR-relevant\ninformation content in privacy policies and subsequently check them against the\ncompleteness criteria. To evaluate our approach, we collected 234 real privacy\npolicies from the fund industry. Over a set of 48 unseen privacy policies, our\napproach detected 300 of the total of 334 violations of some completeness\ncriteria correctly, while producing 23 false positives. The approach thus has a\nprecision of 92.9% and recall of 89.8%. Compared to a baseline that applies\nkeyword search only, our approach results in an improvement of 24.5% in\nprecision and 38% in recall.",
    "descriptor": "",
    "authors": [
      "Orlando Amaral",
      "Sallam Abualhaija",
      "Damiano Torre",
      "Mehrdad Sabetzadeh",
      "Lionel C. Briand"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.05688"
  },
  {
    "id": "arXiv:2106.05691",
    "title": "Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT  Knowledge Distillation",
    "abstract": "Recently, knowledge distillation (KD) has shown great success in BERT\ncompression. Instead of only learning from the teacher's soft label as in\nconventional KD, researchers find that the rich information contained in the\nhidden layers of BERT is conducive to the student's performance. To better\nexploit the hidden knowledge, a common practice is to force the student to\ndeeply mimic the teacher's hidden states of all the tokens in a layer-wise\nmanner. In this paper, however, we observe that although distilling the\nteacher's hidden state knowledge (HSK) is helpful, the performance gain\n(marginal utility) diminishes quickly as more HSK is distilled. To understand\nthis effect, we conduct a series of analysis. Specifically, we divide the HSK\nof BERT into three dimensions, namely depth, length and width. We first\ninvestigate a variety of strategies to extract crucial knowledge for each\nsingle dimension and then jointly compress the three dimensions. In this way,\nwe show that 1) the student's performance can be improved by extracting and\ndistilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the\nsame performance as extensive HSK distillation. Based on the second finding, we\nfurther propose an efficient KD paradigm to compress BERT, which does not\nrequire loading the teacher during the training of student. For two kinds of\nstudent models and computing devices, the proposed KD paradigm gives rise to\ntraining speedup of 2.7x ~ 3.4x.",
    "descriptor": "\nComments: Accepted by ACL 2021\n",
    "authors": [
      "Yuanxin Liu",
      "Fandong Meng",
      "Zheng Lin",
      "Weiping Wang",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05691"
  },
  {
    "id": "arXiv:2106.05700",
    "title": "A Wearable Virtual Touch System for Cars",
    "abstract": "In automotive domain, operation of secondary tasks like accessing\ninfotainment system, adjusting air conditioning vents, and side mirrors\ndistract drivers from driving. Though existing modalities like gesture and\nspeech recognition systems facilitate undertaking secondary tasks by reducing\nduration of eyes off the road, those often require remembering a set of\ngestures or screen sequences. In this paper, we have proposed two different\nmodalities for drivers to virtually touch the dashboard display using a laser\ntracker with a mechanical switch and an eye gaze switch. We compared\nperformances of our proposed modalities against conventional touch modality in\nautomotive environment by comparing pointing and selection times of\nrepresentative secondary task and also analysed effect on driving performance\nin terms of deviation from lane, average speed, variation in perceived workload\nand system usability. We did not find significant difference in driving and\npointing performance between laser tracking system and existing touchscreen\nsystem. Our result also showed that the driving and pointing performance of the\nvirtual touch system with eye gaze switch was significantly better than the\nsame with mechanical switch. We evaluated the efficacy of the proposed virtual\ntouch system with eye gaze switch inside a real car and investigated acceptance\nof the system by professional drivers using qualitative research. The\nquantitative and qualitative studies indicated importance of using multimodal\nsystem inside car and highlighted several criteria for acceptance of new\nautomotive user interface.",
    "descriptor": "\nComments: Journal on Multimodal User Interface 2021\n",
    "authors": [
      "Gowdham Prabhakar",
      "Priyam Rajkhowa",
      "Pradipta Biswas"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05700"
  },
  {
    "id": "arXiv:2106.05701",
    "title": "Learnable Hypergraph Laplacian for Hypergraph Learning",
    "abstract": "HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their\npotential in modeling high-order relations preserved in graph structured data.\nHowever, most existing convolution filters are localized and determined by the\npre-defined initial hypergraph topology, neglecting to explore implicit and\nlong-ange relations in real-world data. In this paper, we propose the first\nlearning-based method tailored for constructing adaptive hypergraph structure,\ntermed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic\nplug-in-play module for improving the representational power of HGCNNs.\nSpecifically, HERALD adaptively optimizes the adjacency relationship between\nhypernodes and hyperedges in an end-to-end manner and thus the task-aware\nhypergraph is learned. Furthermore, HERALD employs the self-attention mechanism\nto capture the non-local paired-nodes relation. Extensive experiments on\nvarious popular hypergraph datasets for node classification and graph\nclassification tasks demonstrate that our approach obtains consistent and\nconsiderable performance enhancement, proving its effectiveness and\ngeneralization ability.",
    "descriptor": "",
    "authors": [
      "Jiying Zhang",
      "Yuzhao Chen",
      "Xi Xiao",
      "Runiu Lu",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05701"
  },
  {
    "id": "arXiv:2106.05702",
    "title": "Myths and Misconceptions about Attackers and Attacks",
    "abstract": "This paper is based on a three year project during which we studied\nattackers' behavior, reading military planning literature, and thinking on how\nwould we do the same things they do, and what problems would we, as attackers,\nface. This research is still ongoing, but while participating in applications\nfor other projects and talking to cyber security experts we constantly face the\nsame issues, namely attackers' behavior is not well understood, and\nconsequently, there are a number of misconceptions floating around that are\nsimply not true, or are only partially true. This is actually expected as\nsomeone who casually follows news about incidents easily gets impression that\nattackers and attacks are everywhere and every one is under attack. Our goal in\nthis paper is to debunk these myths, to show what attackers really can and can\nnot, what dilemmas they face, what we don't know about attackers and attacks,\netc. The conclusion is that, while attackers do have upper hand, they don't\nhave absolute advantage, i.e. they also operate in an uncertain environment.\nKnowing this, means that defenses could be well established.",
    "descriptor": "\nComments: 8 pages, 27 reference. This paper is work in progress and as such may contain inaccuracies, missing or unfinished sentences and paragraphs\n",
    "authors": [
      "Stjepan Gro\u0161"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05702"
  },
  {
    "id": "arXiv:2106.05706",
    "title": "Outlier-Robust Filtering For Nonlinear Systems With Selective  Observations Rejection",
    "abstract": "This letter presents a novel outlier-robust filter for nonlinear dynamical\nsystems. We consider a common case where measurements are obtained from\nindependent sensors. The proposed method is devised by modifying the\nmeasurement model and subsequently using the theory of Variational Bayes and\ngeneral Gaussian filtering. We treat the measurement outliers independently for\nindependent observations leading to selective rejection of the corrupted\nobservations during inference. By carrying out simulations for variable number\nof sensors we verify that an implementation of the proposed filter is\ncomputationally more efficient as compared to similar baseline methods.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Aamir Hussain Chughtai",
      "Muhammad Tahir",
      "Momin Uppal"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05706"
  },
  {
    "id": "arXiv:2106.05707",
    "title": "FEVEROUS: Fact Extraction and VERification Over Unstructured and  Structured information",
    "abstract": "Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.",
    "descriptor": "",
    "authors": [
      "Rami Aly",
      "Zhijiang Guo",
      "Michael Schlichtkrull",
      "James Thorne",
      "Andreas Vlachos",
      "Christos Christodoulopoulos",
      "Oana Cocarascu",
      "Arpit Mittal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05707"
  },
  {
    "id": "arXiv:2106.05714",
    "title": "A shape preserving quasi-interpolation operator based on a new  transcendental RBF",
    "abstract": "It is well-known that the univariate Multiquadric quasi-interpolation\noperator is constructed based on the piecewise linear interpolation by |x|. In\nthis paper, we first introduce a new transcendental RBF based on the hyperbolic\ntangent function as a smooth approximant to f(r)=r with higher accuracy and\nbetter convergence properties than the multiquadric. Then Wu-Schaback's\nquasi-interpolation formula is rewritten using the proposed RBF. It preserves\nconvexity and monotonicity. We prove that the proposed scheme converges with a\nrate of O(h^2). So it has a higher degree of smoothness. Some numerical\nexperiments are given in order to demonstrate the efficiency and accuracy of\nthe method.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Mohammad Heidari",
      "Maryam Mohammadi",
      "Stefano De Marchi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05714"
  },
  {
    "id": "arXiv:2106.05723",
    "title": "Automatic Construction of Context-Aware Sentiment Lexicon in the  Financial Domain Using Direction-Dependent Words",
    "abstract": "Increasing attention has been drawn to the sentiment analysis of financial\ndocuments. The most popular examples of such documents include analyst reports\nand economic news, the analysis of which is frequently used to capture the\ntrends in market sentiments. On the other hand, the significance of the role\nsentiment analysis plays in the financial domain has given rise to the efforts\nto construct a financial domain-specific sentiment lexicon. Sentiment lexicons\nlend a hand for solving various text mining tasks, such as unsupervised\nclassification of text data, while alleviating the arduous human labor required\nfor manual labeling. One of the challenges in the construction of an effective\nsentiment lexicon is that the semantic orientation of a word may change\ndepending on the context in which it appears. For instance, the word ``profit\"\nusually conveys positive sentiments; however, when the word is juxtaposed with\nanother word ``decrease,\" the sentiment associated with the phrase ``profit\ndecreases\" now becomes negative. Hence, the sentiment of a given word may shift\nas one begins to consider the context surrounding the word. In this paper, we\naddress this issue by incorporating context when building sentiment lexicon\nfrom a given corpus. Specifically, we construct a lexicon named Senti-DD for\nthe Sentiment lexicon composed of Direction-Dependent words, which expresses\neach term a pair of a directional word and a direction-dependent word.\nExperiment results show that higher classification performance is achieved with\nSenti-DD, proving the effectiveness of our method for automatically\nconstructing a context-aware sentiment lexicon in the financial domain.",
    "descriptor": "",
    "authors": [
      "Jihye Park",
      "Hye Jin Lee",
      "Sungzoon Cho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05723"
  },
  {
    "id": "arXiv:2106.05725",
    "title": "Academics evaluating academics: a methodology to inform the review  process on top of open citations",
    "abstract": "In the past, several works have investigated ways for combining quantitative\nand qualitative methods in research assessment exercises. In this work, we aim\nat introducing a methodology to explore whether citation-based metrics,\ncalculated only considering open bibliographic and citation data, can yield\ninsights on how human peer-review of research assessment exercises is\nconducted. To understand if and what metrics provide relevant information, we\npropose to use a series of machine learning models to replicate the decisions\nof the committees of the research assessment exercises.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2103.07942\n",
    "authors": [
      "Federica Bologna",
      "Angelo Di Iorio",
      "Silvio Peroni",
      "Francesco Poggi"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05725"
  },
  {
    "id": "arXiv:2106.05727",
    "title": "Fairness for Cooperative Multi-Agent Learning with Equivariant Policies",
    "abstract": "We study fairness through the lens of cooperative multi-agent learning. Our\nwork is motivated by empirical evidence that naive maximization of team reward\nyields unfair outcomes for individual team members. To address fairness in\nmulti-agent contexts, we introduce team fairness, a group-based fairness\nmeasure for multi-agent learning. We then incorporate team fairness into policy\noptimization -- introducing Fairness through Equivariance (Fair-E), a novel\nlearning strategy that achieves provably fair reward distributions. We then\nintroduce Fairness through Equivariance Regularization (Fair-ER) as a\nsoft-constraint version of Fair-E and show that Fair-ER reaches higher levels\nof utility than Fair-E and fairer outcomes than policies with no equivariance.\nFinally, we investigate the fairness-utility trade-off in multi-agent settings.",
    "descriptor": "\nComments: 15 pages, 4 figures\n",
    "authors": [
      "Niko A. Grupen",
      "Bart Selman",
      "Daniel D. Lee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.05727"
  },
  {
    "id": "arXiv:2106.05728",
    "title": "Face mask detection using convolution neural network",
    "abstract": "In the recent times, the Coronaviruses that are a big family of different\nviruses have become very common, contagious and dangerous to the whole human\nkind. It spreads human to human by exhaling the infection breath, which leaves\ndroplets of the virus on different surface which is then inhaled by other\nperson and catches the infection too. So it has become very important to\nprotect ourselves and the people around us from this situation. We can take\nprecautions such as social distancing, washing hands every two hours, using\nsanitizer, maintaining social distance and the most important wearing a mask.\nPublic use of wearing a masks has become very common everywhere in the whole\nworld now. From that the most affected and devastating condition is of India\ndue to its extreme population in small area. This paper proposes a method to\ndetect the face mask is put on or not for offices, or any other work place with\na lot of people coming to work. We have used convolutional neural network for\nthe same. The model is trained on a real world dataset and tested with live\nvideo streaming with a good accuracy. Further the accuracy of the model with\ndifferent hyper parameters and multiple people at different distance and\nlocation of the frame is done.",
    "descriptor": "\nComments: 4 PAGES, 3 FIGURES, 1 TABLE\n",
    "authors": [
      "Riya Shah Rutva Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.05728"
  },
  {
    "id": "arXiv:2106.05729",
    "title": "GRASP: Graph Alignment through Spectral Signatures",
    "abstract": "What is the best way to match the nodes of two graphs? This graph alignment\nproblem generalizes graph isomorphism and arises in applications from social\nnetwork analysis to bioinformatics. Some solutions assume that auxiliary\ninformation on known matches or node or edge attributes is available, or\nutilize arbitrary graph features. Such methods fare poorly in the pure form of\nthe problem, in which only graph structures are given. Other proposals\ntranslate the problem to one of aligning node embeddings, yet, by doing so,\nprovide only a single-scale view of the graph.In this paper, we transfer the\nshape-analysis concept of functional maps from the continuous to the discrete\ncase, and treat the graph alignment problem as a special case of the problem of\nfinding a mapping between functions on graphs. We present GRASP, a method that\nfirst establishes a correspondence between functions derived from Laplacian\nmatrix eigenvectors, which capture multiscale structural characteristics,and\nthen exploits this correspondence to align nodes. Our experimental study,\nfeaturing noise levels higher than anything used in previous studies, shows\nthat GRASP outperforms state-of-the-art methods for graph alignment across\nnoise levels and graph types.",
    "descriptor": "\nComments: Accepted to APWeb-WAIM\n",
    "authors": [
      "Judith Hermanns",
      "Anton Tsitsulin",
      "Marina Munkhoeva",
      "Alex Bronstein",
      "Davide Mottin",
      "Panagiotis Karras"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05729"
  },
  {
    "id": "arXiv:2106.05731",
    "title": "Leveraged Weighted Loss for Partial Label Learning",
    "abstract": "As an important branch of weakly supervised learning, partial label learning\ndeals with data where each instance is assigned with a set of candidate labels,\nwhereas only one of them is true. Despite many methodology studies on learning\nfrom partial labels, there still lacks theoretical understandings of their risk\nconsistent properties under relatively weak assumptions, especially on the link\nbetween theoretical results and the empirical choice of parameters. In this\npaper, we propose a family of loss functions named \\textit{Leveraged Weighted}\n(LW) loss, which for the first time introduces the leverage parameter $\\beta$\nto consider the trade-off between losses on partial labels and non-partial\nones. From the theoretical side, we derive a generalized result of risk\nconsistency for the LW loss in learning from partial labels, based on which we\nprovide guidance to the choice of the leverage parameter $\\beta$. In\nexperiments, we verify the theoretical guidance, and show the high\neffectiveness of our proposed LW loss on both benchmark and real datasets\ncompared with other state-of-the-art partial label learning algorithms.",
    "descriptor": "\nComments: Accepted to ICML2021 as long talk\n",
    "authors": [
      "Hongwei Wen",
      "Jingyi Cui",
      "Hanyuan Hang",
      "Jiabin Liu",
      "Yisen Wang",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05731"
  },
  {
    "id": "arXiv:2106.05734",
    "title": "A Topology-Shape-Metrics Framework for Ortho-Radial Graph Drawing",
    "abstract": "Orthogonal drawings, i.e., embeddings of graphs into grids, are a classic\ntopic in Graph Drawing. Often the goal is to find a drawing that minimizes the\nnumber of bends on the edges. A key ingredient for bend minimization algorithms\nis the existence of an orthogonal representation that describes such drawings\ncombinatorially by only listing the angles between the edges around each vertex\nand the directions of bends on the edges, but neglecting any kind of geometric\ninformation such as vertex coordinates or edge lengths.\nWe generalize this idea to ortho-radial representations of ortho-radial\ndrawings, which are embeddings into an ortho-radial grid, whose gridlines are\nconcentric circles around the origin and straight-line spokes emanating from\nthe origin but excluding the origin itself. Unlike the orthogonal case, there\nexist ortho-radial representations that do not admit a corresponding drawing,\nfor example so-called strictly monotone cycles. An ortho-radial drawing is\ncalled valid if it does not contain a strictly monotone cycle. Our first result\nis that an ortho-radial representation admits a corresponding drawing if and\nonly if it is valid. Previously such a characterization was only known for\northo-radial drawings of paths, cycles, and theta graphs, and in the special\ncase of rectangular drawings of cubic graphs, where the contour of each face is\nrequired to be a rectangle. Further, we give a quadratic-time algorithm that\ntests for an ortho-radial representation whether it is valid, and we show how\nto draw a valid ortho-radial representation in the same running time.\nAltogether, this reduces the problem of computing a minimum-bend ortho-radial\ndrawing to the task of computing a valid ortho-radial representation with the\nminimum number of bends, and hence establishes an ortho-radial analogue of the\ntopology-shape-metrics framework for planar orthogonal drawings by Tamassia.",
    "descriptor": "\nComments: submitted to the journal Discrete & Computational Geometry. arXiv admin note: text overlap with arXiv:1903.05048\n",
    "authors": [
      "Lukas Barth",
      "Benjamin Niedermann",
      "Ignaz Rutter",
      "Matthias Wolf"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.05734"
  },
  {
    "id": "arXiv:2106.05735",
    "title": "The Medical Segmentation Decathlon",
    "abstract": "International challenges have become the de facto standard for comparative\nassessment of image analysis algorithms given a specific task. Segmentation is\nso far the most widely investigated medical image processing task, but the\nvarious segmentation challenges have typically been organized in isolation,\nsuch that algorithm development was driven by the need to tackle a single\nspecific clinical problem. We hypothesized that a method capable of performing\nwell on multiple tasks will generalize well to a previously unseen task and\npotentially outperform a custom-designed solution. To investigate the\nhypothesis, we organized the Medical Segmentation Decathlon (MSD) - a\nbiomedical image analysis challenge, in which algorithms compete in a multitude\nof both tasks and modalities. The underlying data set was designed to explore\nthe axis of difficulties typically encountered when dealing with medical\nimages, such as small data sets, unbalanced labels, multi-site data and small\nobjects. The MSD challenge confirmed that algorithms with a consistent good\nperformance on a set of tasks preserved their good average performance on a\ndifferent set of previously unseen tasks. Moreover, by monitoring the MSD\nwinner for two years, we found that this algorithm continued generalizing well\nto a wide range of other clinical problems, further confirming our hypothesis.\nThree main conclusions can be drawn from this study: (1) state-of-the-art image\nsegmentation algorithms are mature, accurate, and generalize well when\nretrained on unseen tasks; (2) consistent algorithmic performance across\nmultiple tasks is a strong surrogate of algorithmic generalizability; (3) the\ntraining of accurate AI segmentation models is now commoditized to non AI\nexperts.",
    "descriptor": "",
    "authors": [
      "Michela Antonelli",
      "Annika Reinke",
      "Spyridon Bakas",
      "Keyvan Farahani",
      "AnnetteKopp-Schneider",
      "Bennett A. Landman",
      "Geert Litjens",
      "Bjoern Menze",
      "Olaf Ronneberger",
      "Ronald M.Summers",
      "Bram van Ginneken",
      "Michel Bilello",
      "Patrick Bilic",
      "Patrick F. Christ",
      "Richard K. G. Do",
      "Marc J. Gollub",
      "Stephan H. Heckers",
      "Henkjan Huisman",
      "William R. Jarnagin",
      "Maureen K. McHugo",
      "Sandy Napel",
      "Jennifer S. Goli Pernicka",
      "Kawal Rhode",
      "Catalina Tobon-Gomez",
      "Eugene Vorontsov",
      "Henkjan Huisman",
      "James A. Meakin",
      "Sebastien Ourselin",
      "Manuel Wiesenfarth",
      "Pablo Arbelaez",
      "Byeonguk Bae",
      "Sihong Chen",
      "Laura Daza",
      "Jianjiang Feng",
      "Baochun He",
      "Fabian Isensee",
      "Yuanfeng Ji",
      "Fucang Jia",
      "Namkug Kim",
      "Ildoo Kim",
      "Dorit Merhof",
      "Akshay Pai",
      "Beomhee Park",
      "Mathias Perslev",
      "Ramin Rezaiifar",
      "Oliver Rippel",
      "Ignacio Sarasua",
      "Wei Shen",
      "Jaemin Son",
      "Christian Wachinger"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05735"
  },
  {
    "id": "arXiv:2106.05740",
    "title": "Adaptive Robust Data-driven Building Control via Bi-level Reformulation:  an Experimental Result",
    "abstract": "In the era of digitalization, utilization of data-driven control approaches\nto minimize energy consumption of residential/commercial building is of\nfar-reaching significance. Meanwhile, A number of recent approaches based on\nthe application of Willems' fundamental lemma for data-driven controller design\nfrom input/output measurements are very promising for deterministic LTI\nsystems. This paper addresses the key noise-free assumption, and extends these\ndata-driven control schemes to adaptive building control with measured process\nnoise and unknown measurement noise via a robust bilevel formulation, whose\nupper level ensures robustness and whose lower level guarantees prediction\nquality. Corresponding numerical improvements and an active excitation\nmechanism are proposed to enable a computationally efficient reliable\noperation. The efficacy of the proposed scheme is validated by a numerical\nexample and a real-world experiment on a lecture hall on EPFL campus.",
    "descriptor": "",
    "authors": [
      "Yingzhao Lian",
      "Jicheng Shi",
      "Manuel Pascal Koch",
      "Colin Neil Jones"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05740"
  },
  {
    "id": "arXiv:2106.05741",
    "title": "End-to-end lung nodule detection framework with model-based feature  projection block",
    "abstract": "This paper proposes novel end-to-end framework for detecting suspicious\npulmonary nodules in chest CT scans. The method core idea is a new nodule\nsegmentation architecture with a model-based feature projection block on\nthree-dimensional convolutions. This block acts as a preliminary feature\nextractor for a two-dimensional U-Net-like convolutional network. Using the\nproposed approach along with an axial, coronal, and sagittal projection\nanalysis makes it possible to abandon the widely used false positives reduction\nstep. The proposed method achieves SOTA on LUNA2016 with 0.959 average\nsensitivity, and 0.936 sensitivity if the false-positive level per scan is\n0.25. The paper describes the proposed approach and represents the experimental\nresults on LUNA2016 as well as ablation studies.",
    "descriptor": "",
    "authors": [
      "Ivan Drokin",
      "Elena Ericheva"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05741"
  },
  {
    "id": "arXiv:2106.05744",
    "title": "Pivotal Tuning for Latent-based Editing of Real Images",
    "abstract": "Recently, a surge of advanced facial editing techniques have been proposed\nthat leverage the generative power of a pre-trained StyleGAN. To successfully\nedit an image this way, one must first project (or invert) the image into the\npre-trained generator's domain. As it turns out, however, StyleGAN's latent\nspace induces an inherent tradeoff between distortion and editability, i.e.\nbetween maintaining the original appearance and convincingly altering some of\nits attributes. Practically, this means it is still challenging to apply\nID-preserving facial latent-space editing to faces which are out of the\ngenerator's domain. In this paper, we present an approach to bridge this gap.\nOur technique slightly alters the generator, so that an out-of-domain image is\nfaithfully mapped into an in-domain latent code. The key idea is pivotal tuning\n- a brief training process that preserves the editing quality of an in-domain\nlatent region, while changing its portrayed identity and appearance. In Pivotal\nTuning Inversion (PTI), an initial inverted latent code serves as a pivot,\naround which the generator is fined-tuned. At the same time, a regularization\nterm keeps nearby identities intact, to locally contain the effect. This\nsurgical training process ends up altering appearance features that represent\nmostly identity, without affecting editing capabilities. We validate our\ntechnique through inversion and editing metrics, and show preferable scores to\nstate-of-the-art methods. We further qualitatively demonstrate our technique by\napplying advanced edits (such as pose, age, or expression) to numerous images\nof well-known and recognizable identities. Finally, we demonstrate resilience\nto harder cases, including heavy make-up, elaborate hairstyles and/or headwear,\nwhich otherwise could not have been successfully inverted and edited by\nstate-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Daniel Roich",
      "Ron Mokady",
      "Amit H. Bermano",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05744"
  },
  {
    "id": "arXiv:2106.05746",
    "title": "The 2021 Hotel-ID to Combat Human Trafficking Competition Dataset",
    "abstract": "Hotel recognition is an important task for human trafficking investigations\nsince victims are often photographed in hotel rooms. Identifying these hotels\nis vital to trafficking investigations since they can help track down current\nand future victims who might be taken to the same places. Hotel recognition is\na challenging fine grained visual classification task as there can be little\nsimilarity between different rooms within the same hotel, and high similarity\nbetween rooms from different hotels (especially if they are from the same\nchain). Hotel recognition to combat human trafficking poses additional\nchallenges as investigative images are often low quality, contain uncommon\ncamera angles and are highly occluded. Here, we present the 2021 Hotel-ID\ndataset to help raise awareness for this problem and generate novel approaches.\nThe dataset consists of hotel room images that have been crowd-sourced and\nuploaded through the TraffickCam mobile application. The quality of these\nimages is similar to investigative images and hence models trained on these\nimages have good chances of accurately narrowing down on the correct hotel.",
    "descriptor": "\nComments: CVPR 2021 Workshop on Fine-Grained Visual Categorization (FGVC)\n",
    "authors": [
      "Rashmi Kamath",
      "Greg Rolwes",
      "Samuel Black",
      "Abby Stylianou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05746"
  },
  {
    "id": "arXiv:2106.05748",
    "title": "Multi-resolution Outlier Pooling for Sorghum Classification",
    "abstract": "Automated high throughput plant phenotyping involves leveraging sensors, such\nas RGB, thermal and hyperspectral cameras (among others), to make large scale\nand rapid measurements of the physical properties of plants for the purpose of\nbetter understanding the difference between crops and facilitating rapid plant\nbreeding programs. One of the most basic phenotyping tasks is to determine the\ncultivar, or species, in a particular sensor product. This simple phenotype can\nbe used to detect errors in planting and to learn the most differentiating\nfeatures between cultivars. It is also a challenging visual recognition task,\nas a large number of highly related crops are grown simultaneously, leading to\na classification problem with low inter-class variance. In this paper, we\nintroduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum\ncaptured by a state-of-the-art gantry system, a multi-resolution network\narchitecture that learns both global and fine-grained features on the crops,\nand a new global pooling strategy called Dynamic Outlier Pooling which\noutperforms standard global pooling strategies on this task.",
    "descriptor": "\nComments: CVPR 2021 Agriculture-Vision Workshop\n",
    "authors": [
      "Chao Ren",
      "Justin Dulay",
      "Gregory Rolwes",
      "Duke Pauli",
      "Nadia Shakoor",
      "Abby Stylianou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05748"
  },
  {
    "id": "arXiv:2106.05752",
    "title": "Parallel Deep Learning-Driven Sarcasm Detection from Pop Culture Text  and English Humor Literature",
    "abstract": "Sarcasm is a sophisticated way of wrapping any immanent truth, mes-sage, or\neven mockery within a hilarious manner. The advent of communications using\nsocial networks has mass-produced new avenues of socialization. It can be\nfurther said that humor, irony, sarcasm, and wit are the four chariots of being\nsocially funny in the modern days. In this paper, we manually extract the\nsarcastic word distribution features of a benchmark pop culture sarcasm corpus,\ncontaining sarcastic dialogues and monologues. We generate input sequences\nformed of the weighted vectors from such words. We further propose an\namalgamation of four parallel deep long-short term networks (pLSTM), each with\ndistinctive activation classifier. These modules are primarily aimed at\nsuccessfully detecting sarcasm from the text corpus. Our proposed model for\ndetecting sarcasm peaks a training accuracy of 98.95% when trained with the\ndiscussed dataset. Consecutively, it obtains the highest of 98.31% overall\nvalidation accuracy on two handpicked Project Gutenberg English humor\nliterature among all the test cases. Our approach transcends previous\nstate-of-the-art works on several sarcasm corpora and results in a new gold\nstandard performance for sarcasm detection.",
    "descriptor": "\nComments: 10 pages, 2 figures, 4 tables\n",
    "authors": [
      "Sourav Das",
      "Anup Kumar Kolya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.05752"
  },
  {
    "id": "arXiv:2106.05756",
    "title": "Lifting The Grey Curtain: A First Look at the Ecosystem of CULPRITWARE",
    "abstract": "Mobile apps are extensively involved in cyber-crimes. Some apps are malware\nwhich compromise users' devices, while some others may lead to privacy leakage.\nApart from them, there also exist apps which directly make profit from victims\nthrough deceiving, threatening or other criminal actions. We name these apps as\nCULPRITWARE. They have become emerging threats in recent years. However, the\ncharacteristics and the ecosystem of CULPRITWARE remain mysterious. This paper\ntakes the first step towards systematically studying CULPRITWARE and its\necosystem. Specifically, we first characterize CULPRITWARE by categorizing and\ncomparing them with benign apps and malware. The result shows that CULPRITWARE\nhave unique features, e.g., the usage of app generators (25.27%) deviates from\nthat of benign apps (5.08%) and malware (0.43%). Such a discrepancy can be used\nto distinguish CULPRITWARE from benign apps and malware. Then we understand the\nstructure of the ecosystem by revealing the four participating entities (i.e.,\ndeveloper, agent, operator and reaper) and the workflow. After that, we further\nreveal the characteristics of the ecosystem by studying the participating\nentities. Our investigation shows that the majority of CULPRITWARE (at least\n52.08%) are propagated through social media rather than the official app\nmarkets, and most CULPRITWARE (96%) indirectly rely on the covert fourth-party\npayment services to transfer the profits. Our findings shed light on the\necosystem, and can facilitate the community and law enforcement authorities to\nmitigate the threats. We will release the source code of our tools to engage\nthe community.",
    "descriptor": "\nComments: 13 pages, 10 figures\n",
    "authors": [
      "Zhuo Chen",
      "Lei Wu",
      "Jing Cheng",
      "Yubo Hu",
      "Yajin Zhou",
      "Zhushou Tang",
      "Yexuan Chen",
      "Jinku Li",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05756"
  },
  {
    "id": "arXiv:2106.05761",
    "title": "Valued Authorization Policy Existence Problem",
    "abstract": "Problems of satisfiability and resiliency in workflows have been widely\nstudied in the last decade. Recent work has shown that many such problems may\nbe viewed as special cases of the authorization policy existence problem\n(APEP), which returns an authorization policy if one exists and 'No' otherwise.\nA solution may not exist because of the restrictions imposed by the base\nauthorization relation and constraints that form part of the input to APEP.\nHowever, in many practical settings it would be more useful to obtain a 'least\nbad' policy than just a 'No', where 'least bad' is characterized by some\nnumerical value associated with the policy indicating the extent to which the\npolicy violates the base authorization relation and constraints. Accordingly,\nwe introduce the Valued APEP, which returns an authorization policy of minimum\nweight, where the (non-negative) weight is determined by the constraints\nviolated by the returned solution (and is 0 if all constraints are satisfied).\nWe then establish a number of results concerning the parameterized complexity\nof Valued APEP. We prove that the problem is fixed-parameter tractable if the\nset of constraints satisfies two restrictions, but is intractable if only one\nof these restrictions holds. (Most constraints known to be of practical use\nsatisfy these restrictions.) We introduce the notion of a user profile for a\nweighted constraint, which enables us to prove a powerful result, a corollary\nof which improves on known complexity results for APEP. Finally, we consider\nValued APEP when restricted to particular sub-classes of constraints and show\nthat instances of such problems can be reduced to the Valued WSP, enabling us\nto exploit known algorithms to solve these particular instances.",
    "descriptor": "\nComments: Accepted to SACMAT 2021\n",
    "authors": [
      "Jason Crampton",
      "Eduard Eiben",
      "Gregory Gutin",
      "Daniel Karapetyan",
      "Diptapriyo Majumdar"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05761"
  },
  {
    "id": "arXiv:2106.05762",
    "title": "Improving multi-speaker TTS prosody variance with a residual encoder and  normalizing flows",
    "abstract": "Text-to-speech systems recently achieved almost indistinguishable quality\nfrom human speech. However, the prosody of those systems is generally flatter\nthan natural speech, producing samples with low expressiveness. Disentanglement\nof speaker id and prosody is crucial in text-to-speech systems to improve on\nnaturalness and produce more variable syntheses. This paper proposes a new\nneural text-to-speech model that approaches the disentanglement problem by\nconditioning a Tacotron2-like architecture on flow-normalized speaker\nembeddings, and by substituting the reference encoder with a new learned latent\ndistribution responsible for modeling the intra-sentence variability due to the\nprosody. By removing the reference encoder dependency, the speaker-leakage\nproblem typically happening in this kind of systems disappears, producing more\ndistinctive syntheses at inference time. The new model achieves significantly\nhigher prosody variance than the baseline in a set of quantitative prosody\nfeatures, as well as higher speaker distinctiveness, without decreasing the\nspeaker intelligibility. Finally, we observe that the normalized speaker\nembeddings enable much richer speaker interpolations, substantially improving\nthe distinctiveness of the new interpolated speakers.",
    "descriptor": "\nComments: in Proceedings of Interspeech 2021 conference\n",
    "authors": [
      "Iv\u00e1n Vall\u00e9s-P\u00e9rez",
      "Julian Roth",
      "Grzegorz Beringer",
      "Roberto Barra-Chicote",
      "Jasha Droppo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05762"
  },
  {
    "id": "arXiv:2106.05763",
    "title": "A Deep Variational Approach to Clustering Survival Data",
    "abstract": "Survival analysis has gained significant attention in the medical domain and\nhas many far-reaching applications. Although a variety of machine learning\nmethods have been introduced for tackling time-to-event prediction in\nunstructured data with complex dependencies, clustering of survival data\nremains an under-explored problem. The latter is particularly helpful in\ndiscovering patient subpopulations whose survival is regulated by different\ngenerative mechanisms, a critical problem in precision medicine. To this end,\nwe introduce a novel probabilistic approach to cluster survival data in a\nvariational deep clustering setting. Our proposed method employs a deep\ngenerative model to uncover the underlying distribution of both the explanatory\nvariables and the potentially censored survival times. We compare our model to\nthe related work on survival clustering in comprehensive experiments on a range\nof synthetic, semi-synthetic, and real-world datasets. Our proposed method\nperforms better at identifying clusters and is competitive at predicting\nsurvival times in terms of the concordance index and relative absolute error.\nTo further demonstrate the usefulness of our approach, we show that our method\nidentifies meaningful clusters from an observational cohort of hemodialysis\npatients that are consistent with previous clinical findings.",
    "descriptor": "",
    "authors": [
      "Laura Manduchi",
      "Ri\u010dards Marcinkevi\u010ds",
      "Michela C. Massi",
      "Verena Gotta",
      "Timothy M\u00fcller",
      "Flavio Vasella",
      "Marian C. Neidert",
      "Marc Pfister",
      "Julia E. Vogt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05763"
  },
  {
    "id": "arXiv:2106.05764",
    "title": "Analyzing Non-Textual Content Elements to Detect Academic Plagiarism",
    "abstract": "Identifying academic plagiarism is a pressing problem, among others, for\nresearch institutions, publishers, and funding organizations. Detection\napproaches proposed so far analyze lexical, syntactical, and semantic text\nsimilarity. These approaches find copied, moderately reworded, and literally\ntranslated text. However, reliably detecting disguised plagiarism, such as\nstrong paraphrases, sense-for-sense translations, and the reuse of non-textual\ncontent and ideas, is an open research problem.\nThe thesis addresses this problem by proposing plagiarism detection\napproaches that implement a different concept: analyzing non-textual content in\nacademic documents, specifically citations, images, and mathematical content.\nTo validate the effectiveness of the proposed detection approaches, the\nthesis presents five evaluations that use real cases of academic plagiarism and\nexploratory searches for unknown cases.\nThe evaluation results show that non-textual content elements contain a high\ndegree of semantic information, are language-independent, and largely immutable\nto the alterations that authors typically perform to conceal plagiarism.\nAnalyzing non-textual content complements text-based detection approaches and\nincreases the detection effectiveness, particularly for disguised forms of\nacademic plagiarism.\nTo demonstrate the benefit of combining non-textual and text-based detection\nmethods, the thesis describes the first plagiarism detection system that\nintegrates the analysis of citation-based, image-based, math-based, and\ntext-based document similarity. The system's user interface employs\nvisualizations that significantly reduce the effort and time users must invest\nin examining content similarity.",
    "descriptor": "\nComments: Ph.D. Thesis, University of Konstanz\n",
    "authors": [
      "Norman Meuschke"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.05764"
  },
  {
    "id": "arXiv:2106.05768",
    "title": "Linguistically Informed Masking for Representation Learning in the  Patent Domain",
    "abstract": "Domain-specific contextualized language models have demonstrated substantial\neffectiveness gains for domain-specific downstream tasks, like similarity\nmatching, entity recognition or information retrieval. However successfully\napplying such models in highly specific language domains requires domain\nadaptation of the pre-trained models. In this paper we propose the empirically\nmotivated Linguistically Informed Masking (LIM) method to focus\ndomain-adaptative pre-training on the linguistic patterns of patents, which use\na highly technical sublanguage. We quantify the relevant differences between\npatent, scientific and general-purpose language and demonstrate for two\ndifferent language models (BERT and SciBERT) that domain adaptation with LIM\nleads to systematically improved representations by evaluating the performance\nof the domain-adapted representations of patent language on two independent\ndownstream tasks, the IPC classification and similarity matching. We\ndemonstrate the impact of balancing the learning from different information\nsources during domain adaptation for the patent domain. We make the source code\nas well as the domain-adaptive pre-trained patent language models publicly\navailable at https://github.com/sophiaalthammer/patent-lim.",
    "descriptor": "\nComments: Published at SIGIR 2021 PatentSemTech workshop\n",
    "authors": [
      "Sophia Althammer",
      "Mark Buckley",
      "Sebastian Hofst\u00e4tter",
      "Allan Hanbury"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05768"
  },
  {
    "id": "arXiv:2106.05769",
    "title": "End-to-End Transmission Analysis of Simultaneous Wireless Information  and Power Transfer using Resonant Beam",
    "abstract": "Integrating the wireless power transfer (WPT) technology into the wireless\ncommunication system has been important for operational cost saving and\npower-hungry problem solving of electronic devices. In this paper, we propose a\nresonant beam simultaneous wireless information and power transfer (RB-SWIPT)\nsystem, which utilizes a gain medium and two retro-reflecting surfaces to\nenhance and retro-reflect energy, and allows devices to recharge their\nbatteries and exchange information from the resonant beam wirelessly. To reveal\nthe SWIPT mechanism and evaluate the SWIPT performance, we establish an\nanalytical end-to-end (E2E) transmission model based on a modular approach and\nthe electromagnetic field propagation. Then, the intra-cavity power intensity\ndistribution, transmission loss, output power, and E2E efficiency can be\nobtained. The numerical evaluation illustrates that the exemplary RB-SWIPT\nsystem can provide about 4.20W electric power and 12.41bps/Hz spectral\nefficiency, and shorter transmission distance or larger retro-reflecting\nsurface size can lead to higher E2E efficiency. The RB-SWIPT presents a new way\nfor high-power, long-range WPT, and high-rate communication.",
    "descriptor": "",
    "authors": [
      "Wen Fang",
      "Hao Deng",
      "Qingwen Liu",
      "Mingqing Liu",
      "Mengyuan Xu",
      "Liuqing Yang",
      "Georgios B. Giannakis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05769"
  },
  {
    "id": "arXiv:2106.05779",
    "title": "Deep Implicit Surface Point Prediction Networks",
    "abstract": "Deep neural representations of 3D shapes as implicit functions have been\nshown to produce high fidelity models surpassing the resolution-memory\ntrade-off faced by the explicit representations using meshes and point clouds.\nHowever, most such approaches focus on representing closed shapes. Unsigned\ndistance function (UDF) based approaches have been proposed recently as a\npromising alternative to represent both open and closed shapes. However, since\nthe gradients of UDFs vanish on the surface, it is challenging to estimate\nlocal (differential) geometric properties like the normals and tangent planes\nwhich are needed for many downstream applications in vision and graphics. There\nare additional challenges in computing these properties efficiently with a\nlow-memory footprint. This paper presents a novel approach that models such\nsurfaces using a new class of implicit representations called the closest\nsurface-point (CSP) representation. We show that CSP allows us to represent\ncomplex surfaces of any topology (open or closed) with high fidelity. It also\nallows for accurate and efficient computation of local geometric properties. We\nfurther demonstrate that it leads to efficient implementation of downstream\nalgorithms like sphere-tracing for rendering the 3D surface as well as to\ncreate explicit mesh-based representations. Extensive experimental evaluation\non the ShapeNet dataset validate the above contributions with results\nsurpassing the state-of-the-art.",
    "descriptor": "\nComments: 22 pages, 17 figures. Under review at ICCV 2021\n",
    "authors": [
      "Rahul Venkatesh",
      "Tejan Karmali",
      "Sarthak Sharma",
      "Aurobrata Ghosh",
      "L\u00e1szl\u00f3 A. Jeni",
      "R. Venkatesh Babu",
      "Maneesh Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.05779"
  },
  {
    "id": "arXiv:2106.05784",
    "title": "Programming Puzzles",
    "abstract": "We introduce a new type of programming challenge called programming puzzles,\nas an objective and comprehensive evaluation of program synthesis, and release\nan open-source dataset of Python Programming Puzzles (P3). Each puzzle is\ndefined by a short Python program $f$, and the goal is to find an input $x$\nwhich makes $f$ output \"True\". The puzzles are objective in that each one is\nspecified entirely by the source code of its verifier $f$, so evaluating $f(x)$\nis all that is needed to test a candidate solution $x$. They do not require an\nanswer key or input/output examples, nor do they depend on natural language\nunderstanding. The dataset is comprehensive in that it spans problems of a\nrange of difficulties and domains, ranging from trivial string manipulation\nproblems that are immediately obvious to human programmers (but not necessarily\nto AI), to classic programming puzzles (e.g., Towers of Hanoi), to\ninterview/competitive-programming problems (e.g., dynamic programming), to\nlongstanding open problems in algorithms and mathematics (e.g., factoring). The\nobjective nature of P3 readily supports self-supervised bootstrapping. We\ndevelop baseline enumerative program synthesis and GPT-3 solvers that are\ncapable of solving easy puzzles -- even without access to any reference\nsolutions -- by learning from their own past solutions. Based on a small user\nstudy, we find puzzle difficulty to correlate between human programmers and the\nbaseline AI solvers.",
    "descriptor": "\nComments: The puzzles repo: this https URL\n",
    "authors": [
      "Tal Schuster",
      "Ashwin Kalyan",
      "Oleksandr Polozov",
      "Adam Tauman Kalai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.05784"
  },
  {
    "id": "arXiv:2106.05785",
    "title": "Efficient Recovery of a Shared Secret via Cooperation: Applications to  SDMM and PIR",
    "abstract": "This work considers the problem of privately outsourcing the computation of a\nmatrix product over a finite field $\\mathbb{F}_q$ to $N$ helper servers. These\nservers are considered to be honest but curious, i.e., they behave according to\nthe protocol but will try to deduce information about the user's data.\nFurthermore, any set of up to $X$ servers is allowed to share their data.\nPrevious works considered this collusion a hindrance and the download cost of\nthe schemes increases with growing $X$. We propose to utilize such linkage\nbetween servers to the user's advantage by allowing servers to cooperate in the\ncomputational task. This leads to a significant gain in the download cost for\nthe proposed schemes. The gain naturally comes at the cost of increased\ncommunication load between the servers. Hence, the proposed cooperative scheme\ncan be understood as outsourcing both computational cost and communication\ncost.\nWhile the present work exemplifies the proposed server cooperation in the\ncase of a specific secure distributed matrix multiplication (SDMM) scheme, the\nsame idea applies to many other use cases as well. For instance, other SDMM\nschemes as well as linear private information retrieval (PIR) as a special case\nof SDMM are instantly covered.",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Jie Li",
      "Camilla Hollanti",
      "Oliver Gnilke"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05785"
  },
  {
    "id": "arXiv:2106.05786",
    "title": "CAT: Cross Attention in Vision Transformer",
    "abstract": "Since Transformer has found widespread use in NLP, the potential of\nTransformer in CV has been realized and has inspired many new approaches.\nHowever, the computation required for replacing word tokens with image patches\nfor Transformer after the tokenization of the image is vast(e.g., ViT), which\nbottlenecks model training and inference. In this paper, we propose a new\nattention mechanism in Transformer termed Cross Attention, which alternates\nattention inner the image patch instead of the whole image to capture local\ninformation and apply attention between image patches which are divided from\nsingle-channel feature maps capture global information. Both operations have\nless computation than standard self-attention in Transformer. By alternately\napplying attention inner patch and between patches, we implement cross\nattention to maintain the performance with lower computational cost and build a\nhierarchical network called Cross Attention Transformer(CAT) for other vision\ntasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves\nthe performance of other methods on COCO and ADE20K, illustrating that our\nnetwork has the potential to serve as general backbones. The code and models\nare available at \\url{https://github.com/linhezheng19/CAT}.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05786"
  },
  {
    "id": "arXiv:2106.05795",
    "title": "Transformed CNNs: recasting pre-trained convolutional layers with  self-attention",
    "abstract": "Vision Transformers (ViT) have recently emerged as a powerful alternative to\nconvolutional networks (CNNs). Although hybrid models attempt to bridge the gap\nbetween these two architectures, the self-attention layers they rely on induce\na strong computational bottleneck, especially at large spatial resolutions. In\nthis work, we explore the idea of reducing the time spent training these layers\nby initializing them as convolutional layers. This enables us to transition\nsmoothly from any pre-trained CNN to its functionally identical hybrid model,\ncalled Transformed CNN (T-CNN). With only 50 epochs of fine-tuning, the\nresulting T-CNNs demonstrate significant performance gains over the CNN (+2.2%\ntop-1 on ImageNet-1k for a ResNet50-RS) as well as substantially improved\nrobustness (+11% top-1 on ImageNet-C). We analyze the representations learnt by\nthe T-CNN, providing deeper insights into the fruitful interplay between\nconvolutions and self-attention. Finally, we experiment initializing the T-CNN\nfrom a partially trained CNN, and find that it reaches better performance than\nthe corresponding hybrid model trained from scratch, while reducing training\ntime.",
    "descriptor": "",
    "authors": [
      "St\u00e9phane d'Ascoli",
      "Levent Sagun",
      "Giulio Biroli",
      "Ari Morcos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05795"
  },
  {
    "id": "arXiv:2106.05799",
    "title": "Hybrid Machine Learning Forecasts for the UEFA EURO 2020",
    "abstract": "Three state-of-the-art statistical ranking methods for forecasting football\nmatches are combined with several other predictors in a hybrid machine learning\nmodel. Namely an ability estimate for every team based on historic matches; an\nability estimate for every team based on bookmaker consensus; average\nplus-minus player ratings based on their individual performances in their home\nclubs and national teams; and further team covariates (e.g., market value, team\nstructure) and country-specific socio-economic factors (population, GDP). The\nproposed combined approach is used for learning the number of goals scored in\nthe matches from the four previous UEFA EUROs 2004-2016 and then applied to\ncurrent information to forecast the upcoming UEFA EURO 2020. Based on the\nresulting estimates, the tournament is simulated repeatedly and winning\nprobabilities are obtained for all teams. A random forest model favors the\ncurrent World Champion France with a winning probability of 14.8% before\nEngland (13.5%) and Spain (12.3%). Additionally, we provide survival\nprobabilities for all teams and at all tournament stages.",
    "descriptor": "\nComments: Keywords: UEFA EURO 2020, Football, Machine Learning, Team abilities, Sports tournaments. arXiv admin note: substantial text overlap with arXiv:1906.01131, arXiv:1806.03208\n",
    "authors": [
      "Andreas Groll",
      "Lars Magnus Hvattum",
      "Christophe Ley",
      "Franziska Popp",
      "Gunther Schauberger",
      "Hans Van Eetvelde",
      "Achim Zeileis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.05799"
  },
  {
    "id": "arXiv:2106.05802",
    "title": "Informative Policy Representations in Multi-Agent Reinforcement Learning  via Joint-Action Distributions",
    "abstract": "In multi-agent reinforcement learning, the inherent non-stationarity of the\nenvironment caused by other agents' actions posed significant difficulties for\nan agent to learn a good policy independently. One way to deal with\nnon-stationarity is agent modeling, by which the agent takes into consideration\nthe influence of other agents' policies. Most existing work relies on\npredicting other agents' actions or goals, or discriminating between their\npolicies. However, such modeling fails to capture the similarities and\ndifferences between policies simultaneously and thus cannot provide useful\ninformation when generalizing to unseen policies. To address this, we propose a\ngeneral method to learn representations of other agents' policies via the\njoint-action distributions sampled in interactions. The similarities and\ndifferences between policies are naturally captured by the policy distance\ninferred from the joint-action distributions and deliberately reflected in the\nlearned representations. Agents conditioned on the policy representations can\nwell generalize to unseen agents. We empirically demonstrate that our method\noutperforms existing work in multi-agent tasks when facing unseen agents.",
    "descriptor": "",
    "authors": [
      "Yifan Yu",
      "Haobin Jiang",
      "Zongqing Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.05802"
  },
  {
    "id": "arXiv:2106.05809",
    "title": "Simple Graph Convolutional Networks",
    "abstract": "Many neural networks for graphs are based on the graph convolution operator,\nproposed more than a decade ago. Since then, many alternative definitions have\nbeen proposed, that tend to add complexity (and non-linearity) to the model. In\nthis paper, we follow the opposite direction by proposing simple graph\nconvolution operators, that can be implemented in single-layer graph\nconvolutional networks. We show that our convolution operators are more\ntheoretically grounded than many proposals in literature, and exhibit\nstate-of-the-art predictive performance on the considered benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Luca Pasa",
      "Nicol\u00f2 Navarin",
      "Wolfgang Erb",
      "Alessandro Sperduti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05809"
  },
  {
    "id": "arXiv:2106.05810",
    "title": "On the overlooked issue of defining explanation objectives for  local-surrogate explainers",
    "abstract": "Local surrogate approaches for explaining machine learning model predictions\nhave appealing properties, such as being model-agnostic and flexible in their\nmodelling. Several methods exist that fit this description and share this goal.\nHowever, despite their shared overall procedure, they set out different\nobjectives, extract different information from the black-box, and consequently\nproduce diverse explanations, that are -- in general -- incomparable. In this\nwork we review the similarities and differences amongst multiple methods, with\na particular focus on what information they extract from the model, as this has\nlarge impact on the output: the explanation. We discuss the implications of the\nlack of agreement, and clarity, amongst the methods' objectives on the research\nand practice of explainability.",
    "descriptor": "",
    "authors": [
      "Rafael Poyiadzi",
      "Xavier Renard",
      "Thibault Laugel",
      "Raul Santos-Rodriguez",
      "Marcin Detyniecki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05810"
  },
  {
    "id": "arXiv:2106.05814",
    "title": "A concise method for feature selection via normalized frequencies",
    "abstract": "Feature selection is an important part of building a machine learning model.\nBy eliminating redundant or misleading features from data, the machine learning\nmodel can achieve better performance while reducing the demand on com-puting\nresources. Metaheuristic algorithms are mostly used to implement feature\nselection such as swarm intelligence algorithms and evolutionary algorithms.\nHowever, they suffer from the disadvantage of relative complexity and slowness.\nIn this paper, a concise method is proposed for universal feature selection.\nThe proposed method uses a fusion of the filter method and the wrapper method,\nrather than a combination of them. In the method, one-hoting encoding is used\nto preprocess the dataset, and random forest is utilized as the classifier. The\nproposed method uses normalized frequencies to assign a value to each feature,\nwhich will be used to find the optimal feature subset. Furthermore, we propose\na novel approach to exploit the outputs of mutual information, which allows for\na better starting point for the experiments. Two real-world dataset in the\nfield of intrusion detection were used to evaluate the proposed method. The\nevaluation results show that the proposed method outperformed several\nstate-of-the-art related works in terms of accuracy, precision, recall, F-score\nand AUC.",
    "descriptor": "\nComments: 15 pages, 6 figures\n",
    "authors": [
      "Song Tan",
      "Xia He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.05814"
  },
  {
    "id": "arXiv:2106.05815",
    "title": "Italian Twitter semantic network during the Covid-19 epidemic",
    "abstract": "The Covid-19 pandemic has had a deep impact on the lives of the entire world\npopulation, inducing a participated societal debate. As in other contexts, the\ndebate has been the subject of several d/misinformation campaigns; in a quite\nunprecedented fashion, however, the presence of false information has seriously\nput at risk the public health. In this sense, detecting the presence of\nmalicious narratives and identifying the kinds of users that are more prone to\nspread them represent the first step to limit the persistence of the former\nones. In the present paper we analyse the semantic network observed on Twitter\nduring the first Italian lockdown (induced by the hashtags contained in\napproximately 1.5 millions tweets published between the 23rd of March 2020 and\nthe 23rd of April 2020) and study the extent to which various discursive\ncommunities are exposed to d/misinformation arguments. As observed in other\nstudies, the recovered discursive communities largely overlap with traditional\npolitical parties, even if the debated topics concern different facets of the\nmanagement of the pandemic. Although the themes directly related to\nd/misinformation are a minority of those discussed within our semantic\nnetworks, their popularity is unevenly distributed among the various discursive\ncommunities.",
    "descriptor": "\nComments: 29 pages, 11 figures\n",
    "authors": [
      "Mattia Mattei",
      "Guido Caldarelli",
      "Tiziano Squartini",
      "Fabio Saracco"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2106.05815"
  },
  {
    "id": "arXiv:2106.05819",
    "title": "Adversarial Graph Augmentation to Improve Graph Contrastive Learning",
    "abstract": "Self-supervised learning of graph neural networks (GNN) is in great need\nbecause of the widespread label scarcity issue in real-world graph/network\ndata. Graph contrastive learning (GCL), by training GNNs to maximize the\ncorrespondence between the representations of the same graph in its different\naugmented forms, may yield robust and transferable GNNs even without using\nlabels. However, GNNs trained by traditional GCL often risk capturing redundant\ngraph features and thus may be brittle and provide sub-par performance in\ndownstream tasks. Here, we propose a novel principle, termed adversarial-GCL\n(AD-GCL), which enables GNNs to avoid capturing redundant information during\nthe training by optimizing adversarial graph augmentation strategies used in\nGCL. We pair AD-GCL with theoretical explanations and design a practical\ninstantiation based on trainable edge-dropping graph augmentation. We\nexperimentally validate AD-GCL by comparing with the state-of-the-art GCL\nmethods and achieve performance gains of up-to $14\\%$ in unsupervised, $6\\%$ in\ntransfer, and $3\\%$ in semi-supervised learning settings overall with 18\ndifferent benchmark datasets for the tasks of molecule property regression and\nclassification, and social network classification.",
    "descriptor": "",
    "authors": [
      "Susheel Suresh",
      "Pan Li",
      "Cong Hao",
      "Jennifer Neville"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05819"
  },
  {
    "id": "arXiv:2106.05822",
    "title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped  Structures",
    "abstract": "Attention based language models have become a critical component in\nstate-of-the-art natural language processing systems. However, these models\nhave significant computational requirements, due to long training times, dense\noperations and large parameter count. In this work we demonstrate a set of\nmodifications to the structure of a Transformer layer, producing a more\nefficient architecture. First, we add a convolutional module to complement the\nself-attention module, decoupling the learning of local and global\ninteractions. Secondly, we rely on grouped transformations to reduce the\ncomputational cost of dense feed-forward layers and convolutions, while\npreserving the expressivity of the model. We apply the resulting architecture\nto language representation learning and demonstrate its superior performance\ncompared to BERT models of different scales. We further highlight its improved\nefficiency, both in terms of floating-point operations (FLOPs) and\ntime-to-train.",
    "descriptor": "",
    "authors": [
      "Ivan Chelombiev",
      "Daniel Justus",
      "Douglas Orr",
      "Anastasia Dietrich",
      "Frithjof Gressmann",
      "Alexandros Koliousis",
      "Carlo Luschi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05822"
  },
  {
    "id": "arXiv:2106.05823",
    "title": "Neural Text Classification and StackedHeterogeneous Embeddings for Named  Entity Recognition in SMM4H 2021",
    "abstract": "This paper presents our findings from participating in the SMM4H Shared Task\n2021. We addressed Named Entity Recognition (NER) and Text Classification. To\naddress NER we explored BiLSTM-CRF with Stacked Heterogeneous Embeddings and\nlinguistic features. We investigated various machine learning algorithms\n(logistic regression, Support Vector Machine (SVM) and Neural Networks) to\naddress text classification. Our proposed approaches can be generalized to\ndifferent languages and we have shown its effectiveness for English and\nSpanish. Our text classification submissions (team:MIC-NLP) have achieved\ncompetitive performance with F1-score of $0.46$ and $0.90$ on ADE\nClassification (Task 1a) and Profession Classification (Task 7a) respectively.\nIn the case of NER, our submissions scored F1-score of $0.50$ and $0.82$ on ADE\nSpan Detection (Task 1b) and Profession Span detection (Task 7b) respectively.",
    "descriptor": "\nComments: NAACL 2021\n",
    "authors": [
      "Usama Yaseen",
      "Stefan Langer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05823"
  },
  {
    "id": "arXiv:2106.05824",
    "title": "Rare event estimation using stochastic spectral embedding",
    "abstract": "Estimating the probability of rare failure events is an essential step in the\nreliability assessment of engineering systems. Computing this failure\nprobability for complex non-linear systems is challenging, and has recently\nspurred the development of active-learning reliability methods. These methods\napproximate the limit-state function (LSF) using surrogate models trained with\na sequentially enriched set of model evaluations. A recently proposed method\ncalled stochastic spectral embedding (SSE) aims to improve the local\napproximation accuracy of global, spectral surrogate modelling techniques by\nsequentially embedding local residual expansions in subdomains of the input\nspace. In this work we apply SSE to the LSF, giving rise to a stochastic\nspectral embedding-based reliability (SSER) method. The resulting partition of\nthe input space decomposes the failure probability into a set of\neasy-to-compute domain-wise failure probabilities. We propose a set of\nmodifications that tailor the algorithm to efficiently solve rare event\nestimation problems. These modifications include specialized refinement domain\nselection, partitioning and enrichment strategies. We showcase the algorithm\nperformance on four benchmark problems of various dimensionality and complexity\nin the LSF.",
    "descriptor": "",
    "authors": [
      "P.-R. Wagner",
      "S. Marelli",
      "I. Papaioannou",
      "D. Straub",
      "B. Sudret"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05824"
  },
  {
    "id": "arXiv:2106.05825",
    "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against  Adversarial Machine Learning Attacks",
    "abstract": "DNNs are known to be vulnerable to so-called adversarial attacks, in which\ninputs are carefully manipulated to induce misclassification. Existing defenses\nare mostly software-based and come with high overheads or other limitations.\nThis paper presents HASI, a hardware-accelerated defense that uses a process we\ncall stochastic inference to detect adversarial inputs. HASI carefully injects\nnoise into the model at inference time and used the model's response to\ndifferentiate adversarial inputs from benign ones. We show an adversarial\ndetection rate of average 87% which exceeds the detection rate of the\nstate-of-the-art approaches, with a much lower overhead. We demonstrate a\nsoftware/hardware-accelerated co-design, which reduces the performance impact\nof stochastic inference to 1.58X-2X relative to the unprotected baseline,\ncompared to 14X-20X overhead for a software-only GPU implementation.",
    "descriptor": "",
    "authors": [
      "Mohammad Hossein Samavatian",
      "Saikat Majumdar",
      "Kristin Barber",
      "Radu Teodorescu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05825"
  },
  {
    "id": "arXiv:2106.05830",
    "title": "A Template-guided Hybrid Pointer Network for  Knowledge-basedTask-oriented Dialogue Systems",
    "abstract": "Most existing neural network based task-oriented dialogue systems follow\nencoder-decoder paradigm, where the decoder purely depends on the source texts\nto generate a sequence of words, usually suffering from instability and poor\nreadability. Inspired by the traditional template-based generation approaches,\nwe propose a template-guided hybrid pointer network for the knowledge-based\ntask-oriented dialogue system, which retrieves several potentially relevant\nanswers from a pre-constructed domain-specific conversational repository as\nguidance answers, and incorporates the guidance answers into both the encoding\nand decoding processes. Specifically, we design a memory pointer network model\nwith a gating mechanism to fully exploit the semantic correlation between the\nretrieved answers and the ground-truth response. We evaluate our model on four\nwidely used task-oriented datasets, including one simulated and three manually\ncreated datasets. The experimental results demonstrate that the proposed model\nachieves significantly better performance than the state-of-the-art methods\nover different automatic evaluation metrics.",
    "descriptor": "\nComments: DialDoc workshop@ACL-IJCNLP-2021\n",
    "authors": [
      "Dingmin Wang",
      "Ziyao Chen",
      "Wanwei He",
      "Li Zhong",
      "Yunzhe Tao",
      "Min Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05830"
  },
  {
    "id": "arXiv:2106.05831",
    "title": "Algorithm Auditing at a Large-Scale: Insights from Search Engine Audits",
    "abstract": "Algorithm audits have increased in recent years due to a growing need to\nindependently assess the performance of automatically curated services that\nprocess, filter and rank the large and dynamic amount of information available\non the internet. Among several methodologies to perform such audits, virtual\nagents stand out because they offer the possibility of performing systematic\nexperiments simulating human behaviour without the associated costs of\nrecruiting participants. Motivated by the importance of research transparency\nand replicability of results, this paper focuses on the challenges of such an\napproach, and it provides methodological details, recommendations, lessons\nlearned and limitations that researchers should take into consideration when\nsetting up experiments with virtual agents. We demonstrate the successful\nperformance of our research infrastructure in multiple data collections with\ndiverse experimental designs, and point to different changes and strategies\nthat improved the quality of the method. We conclude that virtual agents are a\npromising venue for monitoring the performance of algorithms during longer\nperiods of time, and we hope that this paper serves as a base to widen the\nresearch in this direction.",
    "descriptor": "",
    "authors": [
      "Roberto Ulloa",
      "Mykola Makhortykh",
      "Aleksandra Urman"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05831"
  },
  {
    "id": "arXiv:2106.05832",
    "title": "Construction of Differential-Cascaded Structures for Control of Robot  Manipulators",
    "abstract": "This paper focuses on the construction of differential-cascaded structures\nfor control of nonlinear robot manipulators subjected to disturbances and\nunavailability of partial information of the desired trajectory. The proposed\ndifferential-cascaded structures rely on infinite differential series to handle\nthe robustness with respect to time-varying disturbances and the partial\nknowledge of the desired trajectories for nonlinear robot manipulators. The\nlong-standing problem of reliable adaptation in the presence of sustaining\ndisturbances is solved by the proposed forwardstepping control with\nforwardstepping adaptation, and stacked reference dynamics yielding adaptive\ndifferential-cascaded structures have been proposed to facilitate the\nforwardstepping adaptation to both the uncertainty of robot dynamics and that\nof the frequencies of disturbances. A distinctive point of the proposed\ndifferential-cascaded approach is that the reference dynamics for design and\nanalysis involve high-order quantities, but via degree-reduction implementation\nof the reference dynamics, the control typically involves only the low-order\nquantities, thus facilitating its applications to control of most physical\nsystems. Our result relies on neither the explicit estimation of the\ndisturbances or derivative and second derivative of the desired position nor\nthe solutions to linear/nonlinear regulator equations, and the employed\nessential element is a differential-cascaded structure governing robot\ndynamics.",
    "descriptor": "",
    "authors": [
      "Hanlei Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05832"
  },
  {
    "id": "arXiv:2106.05833",
    "title": "Incremental space-filling design based on coverings and spacings:  improving upon low discrepancy sequences",
    "abstract": "The paper addresses the problem of defining families of ordered sequences\n$\\{x_i\\}_{i\\in N}$ of elements of a compact subset $X$ of $R^d$ whose prefixes\n$X_n=\\{x_i\\}_{i=1}^{n}$, for all orders $n$, have good space-filling properties\nas measured by the dispersion (covering radius) criterion. Our ultimate aim is\nthe definition of incremental algorithms that generate sequences $X_n$ with\nsmall optimality gap, i.e., with a small increase in the maximum distance\nbetween points of $X$ and the elements of $X_n$ with respect to the optimal\nsolution $X_n^\\star$. The paper is a first step in this direction, presenting\nincremental design algorithms with proven optimality bound for one-parameter\nfamilies of criteria based on coverings and spacings that both converge to\ndispersion for large values of their parameter. The examples presented show\nthat the covering-based method outperforms state-of-the-art competitors,\nincluding coffee-house, suggesting that it inherits from its guaranteed 50\\%\noptimality gap.",
    "descriptor": "\nComments: 28 pages, 13 figures\n",
    "authors": [
      "Amaya Nogales G\u00f3mez",
      "Luc Pronzato",
      "Maria-Jo\u00e3o Rendas"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05833"
  },
  {
    "id": "arXiv:2106.05834",
    "title": "Online Bayesian inference for multiple changepoints and risk assessment",
    "abstract": "The aim of the present study is to detect abrupt trend changes in the mean of\na multidimensional sequential signal. Directly inspired by papers of Fernhead\nand Liu ([4] and [5]), this work describes the signal in a hierarchical manner\n: the change dates of a time segmentation process trigger the renewal of a\npiece-wise constant emission law. Bayesian posterior information on the change\ndates and emission parameters is obtained. These estimations can be revised\nonline, i.e. as new data arrive. This paper proposes explicit formulations\ncorresponding to various emission laws, as well as a generalization to the case\nwhere only partially observed data are available. Practical applications\ninclude the returns of partially observed multi-asset investment strategies,\nwhen only scant prior knowledge of the movers of the returns is at hand,\nlimited to some statistical assumptions. This situation is different from the\nstudy of trend changes in the returns of individual assets, where fundamental\nexogenous information (news, earnings announcements, controversies, etc.) can\nbe used.",
    "descriptor": "",
    "authors": [
      "Olivier Sorba",
      "C Geissler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05834"
  },
  {
    "id": "arXiv:2106.05836",
    "title": "EventDrop: data augmentation for event-based learning",
    "abstract": "The advantages of event-sensing over conventional sensors (e.g., higher\ndynamic range, lower time latency, and lower power consumption) have spurred\nresearch into machine learning for event data. Unsurprisingly, deep learning\nhas emerged as a competitive methodology for learning with event sensors; in\ntypical setups, discrete and asynchronous events are first converted into\nframe-like tensors on which standard deep networks can be applied. However,\nover-fitting remains a challenge, particularly since event datasets remain\nsmall relative to conventional datasets (e.g., ImageNet). In this paper, we\nintroduce EventDrop, a new method for augmenting asynchronous event data to\nimprove the generalization of deep models. By dropping events selected with\nvarious strategies, we are able to increase the diversity of training data\n(e.g., to simulate various levels of occlusion). From a practical perspective,\nEventDrop is simple to implement and computationally low-cost. Experiments on\ntwo event datasets (N-Caltech101 and N-Cars) demonstrate that EventDrop can\nsignificantly improve the generalization performance across a variety of deep\nnetworks.",
    "descriptor": "\nComments: IJCAI 2021\n",
    "authors": [
      "Fuqiang Gu",
      "Weicong Sng",
      "Xuke Hu",
      "Fangwen Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05836"
  },
  {
    "id": "arXiv:2106.05840",
    "title": "A Bagging and Boosting Based Convexly Combined Optimum Mixture  Probabilistic Model",
    "abstract": "Unlike previous studies on mixture distributions, a bagging and boosting\nbased convexly combined mixture probabilistic model has been suggested. This\nmodel is a result of iteratively searching for obtaining the optimum\nprobabilistic model that provides the maximum p value.",
    "descriptor": "",
    "authors": [
      "Mian Arif Shams Adnan",
      "H. M. Miraz Mahmud"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05840"
  },
  {
    "id": "arXiv:2106.05841",
    "title": "Hybrid gene selection approach using XGBoost and multi-objective genetic  algorithm for cancer classification",
    "abstract": "Microarray gene expression data are often accompanied by a large number of\ngenes and a small number of samples. However, only a few of these genes are\nrelevant to cancer, resulting in signigicant gene selection challenges. Hence,\nwe propose a two-stage gene selection approach by combining extreme gradient\nboosting (XGBoost) and a multi-objective optimization genetic algorithm\n(XGBoost-MOGA) for cancer classification in microarray datasets. In the first\nstage, the genes are ranked use an ensemble-based feature selection using\nXGBoost. This stage can effectively remove irrelevant genes and yield a group\ncomprising the most relevant genes related to the class. In the second stage,\nXGBoost-MOGA searches for an optimal gene subset based on the most relevant\ngenes's group using a multi-objective optimization genetic algorithm. We\nperformed comprehensive experiments to compare XGBoost-MOGA with other\nstate-of-the-art feature selection methods using two well-known learning\nclassifiers on 13 publicly available microarray expression datasets. The\nexperimental results show that XGBoost-MOGA yields significantly better results\nthan previous state-of-the-art algorithms in terms of various evaluation\ncriteria, such as accuracy, F-score, precision, and recall.",
    "descriptor": "",
    "authors": [
      "Xiongshi Deng",
      "Min Li",
      "Shaobo Deng",
      "Lei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05841"
  },
  {
    "id": "arXiv:2106.05842",
    "title": "Causality in Neural Networks -- An Extended Abstract",
    "abstract": "Causal reasoning is the main learning and explanation tool used by humans. AI\nsystems should possess causal reasoning capabilities to be deployed in the real\nworld with trust and reliability. Introducing the ideas of causality to machine\nlearning helps in providing better learning and explainable models.\nExplainability, causal disentanglement are some important aspects of any\nmachine learning model. Causal explanations are required to believe in a\nmodel's decision and causal disentanglement learning is important for transfer\nlearning applications. We exploit the ideas of causality to be used in deep\nlearning models to achieve better and causally explainable models that are\nuseful in fairness, disentangled representation, etc.",
    "descriptor": "",
    "authors": [
      "Abbavaram Gowtham Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05842"
  },
  {
    "id": "arXiv:2106.05843",
    "title": "Enforcing Morphological Information in Fully Convolutional Networks to  Improve Cell Instance Segmentation in Fluorescence Microscopy Images",
    "abstract": "Cell instance segmentation in fluorescence microscopy images is becoming\nessential for cancer dynamics and prognosis. Data extracted from cancer\ndynamics allows to understand and accurately model different metabolic\nprocesses such as proliferation. This enables customized and more precise\ncancer treatments. However, accurate cell instance segmentation, necessary for\nfurther cell tracking and behavior analysis, is still challenging in scenarios\nwith high cell concentration and overlapping edges. Within this framework, we\npropose a novel cell instance segmentation approach based on the well-known\nU-Net architecture. To enforce the learning of morphological information per\npixel, a deep distance transformer (DDT) acts as a back-bone model. The DDT\noutput is subsequently used to train a top-model. The following top-models are\nconsidered: a three-class (\\emph{e.g.,} foreground, background and cell border)\nU-net, and a watershed transform. The obtained results suggest a performance\nboost over traditional U-Net architectures. This opens an interesting research\nline around the idea of injecting morphological information into a fully\nconvolutional model.",
    "descriptor": "\nComments: Accepted at the IWANN 2021 (International Work-Conference on Artificial and Natural Neural Networks)\n",
    "authors": [
      "Willard Zamora-Cardenas",
      "Mauro Mendez",
      "Saul Calderon-Ramirez",
      "Martin Vargas",
      "Gerardo Monge",
      "Steve Quiros",
      "David Elizondo",
      "David Elizondo",
      "Miguel A. Molina-Cabello"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05843"
  },
  {
    "id": "arXiv:2106.05844",
    "title": "SemSegLoss: A python package of loss functions for semantic segmentation",
    "abstract": "Image Segmentation has been an active field of research as it has a wide\nrange of applications, ranging from automated disease detection to self-driving\ncars. In recent years, various research papers proposed different loss\nfunctions used in case of biased data, sparse segmentation, and unbalanced\ndataset. In this paper, we introduce SemSegLoss, a python package consisting of\nsome of the well-known loss functions widely used for image segmentation. It is\ndeveloped with the intent to help researchers in the development of novel loss\nfunctions and perform an extensive set of experiments on model architectures\nfor various applications. The ease-of-use and flexibility of the presented\npackage have allowed reducing the development time and increased evaluation\nstrategies of machine learning models for semantic segmentation. Furthermore,\ndifferent applications that use image segmentation can use SemSegLoss because\nof the generality of its functions. This wide range of applications will lead\nto the development and growth of AI across all industries.",
    "descriptor": "\nComments: 8 pages, 2 tables\n",
    "authors": [
      "Shruti Jadon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.05844"
  },
  {
    "id": "arXiv:2106.05846",
    "title": "Latent Space Arc Therapy Optimization",
    "abstract": "Volumetric modulated arc therapy planning is a challenging problem in\nhigh-dimensional, non-convex optimization. Traditionally, heuristics such as\nfluence-map-optimization-informed segment initialization use locally optimal\nsolutions to begin the search of the full arc therapy plan space from a\nreasonable starting point. These routines facilitate arc therapy optimization\nsuch that clinically satisfactory radiation treatment plans can be created in\nabout 10 minutes. However, current optimization algorithms favor solutions near\ntheir initialization point and are slower than necessary due to plan\noverparameterization. In this work, arc therapy overparameterization is\naddressed by reducing the effective dimension of treatment plans with\nunsupervised deep learning. An optimization engine is then built based on\nlow-dimensional arc representations which facilitates faster planning times.",
    "descriptor": "",
    "authors": [
      "Noah Bice",
      "Mohamad Fakhreddine",
      "Ruiqi Li",
      "Dan Nguyen",
      "Christopher Kabat",
      "Pamela Myers",
      "Niko Papanikolaou",
      "Neil Kirby"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05846"
  },
  {
    "id": "arXiv:2106.05848",
    "title": "Deep Probabilistic Time Series Forecasting using Augmented Recurrent  Input for Dynamic Systems",
    "abstract": "The demand of probabilistic time series forecasting has been recently raised\nin various dynamic system scenarios, for example, system identification and\nprognostic and health management of machines. To this end, we combine the\nadvances in both deep generative models and state space model (SSM) to come up\nwith a novel, data-driven deep probabilistic sequence model. Specially, we\nfollow the popular encoder-decoder generative structure to build the recurrent\nneural networks (RNN) assisted variational sequence model on an augmented\nrecurrent input space, which could induce rich stochastic sequence dependency.\nBesides, in order to alleviate the issue of inconsistency between training and\npredicting as well as improving the mining of dynamic patterns, we (i) propose\nusing a hybrid output as input at next time step, which brings training and\npredicting into alignment; and (ii) further devise a generalized\nauto-regressive strategy that encodes all the historical dependencies at\ncurrent time step. Thereafter, we first investigate the methodological\ncharacteristics of the proposed deep probabilistic sequence model on toy cases,\nand then comprehensively demonstrate the superiority of our model against\nexisting deep probabilistic SSM models through extensive numerical experiments\non eight system identification benchmarks from various dynamic systems.\nFinally, we apply our sequence model to a real-world centrifugal compressor\nsensor data forecasting problem, and again verify its outstanding performance\nby quantifying the time series predictive distribution.",
    "descriptor": "\nComments: 25 pages, 7 figures, 4 tables, preprint under review\n",
    "authors": [
      "Haitao Liu",
      "Changjun Liu",
      "Xiaomo Jiang",
      "Xudong Chen",
      "Shuhua Yang",
      "Xiaofang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05848"
  },
  {
    "id": "arXiv:2106.05854",
    "title": "Investigating the Multidisciplinary Perspective of Perceived Safety in  Human-Robot Interaction",
    "abstract": "Safety is a key issue in human-robot interaction, but perceived safety has\nnot been well-studied in comparison to physical safety. In this paper, we\naddress the multidisciplinary perspective of perceived safety in human-robot\ninteraction. To investigate how the comfort of the user, sense of control of\nthe user, unpredictable robot behaviors, and trust impact the safety perception\nof the user, we designed a randomized controlled within-subject experiment. We\ndevised five different experimental conditions in which we investigated the\nrelationships between perceived safety and comfort, sense of control, and\ntrust. In each condition, we modified one factor. To extend our previous\nfindings, the participants were asked to answer questionnaires that measure\ncomfort, sense of control, trust, and perceived safety. The questionnaire\nresults show a strong correlation between these factors and the perceived\nsafety. Since these factors are the main factors that influence perceived\nsafety, they should be considered in human-robot interaction design decisions.\nThe effect of individual characteristics such as personality and gender on\nperceived safety was also discussed. Moreover, we analyzed the facial affect\nand physiological signals of the participants for predicting perceived safety\nfrom objective measures. The data from objective measures revealed that\nphysiological signals give better prediction of perceived safety rather than\nfacial affect data. We believe this article can play an important role in the\ngoal of better understanding perceived safety in human-robot interaction.",
    "descriptor": "",
    "authors": [
      "Neziha Akalin",
      "Annica Kristoffersson",
      "Amy Loutfi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05854"
  },
  {
    "id": "arXiv:2106.05855",
    "title": "Empirical observations on the effects of data transformation in machine  learning classification of geological domains",
    "abstract": "In the literature, a large body of work advocates the use of log-ratio\ntransformation for multivariate statistical analysis of compositional data. In\ncontrast, few studies have looked at how data transformation changes the\nefficacy of machine learning classifiers within geoscience. This letter\npresents experiment results and empirical observations to further explore this\nissue. The objective is to study the effects of data transformation on geozone\nclassification performance when machine learning (ML) classifiers/estimators\nare trained using geochemical data. The training input consists of exploration\nhole assay samples obtained from a Pilbara iron-ore deposit in Western\nAustralia, and geozone labels assigned based on stratigraphic units, the\nabsence or presence and type of mineralization. The ML techniques considered\nare multinomial logistic regression, Gaussian na\\\"{i}ve Bayes, kNN, linear\nsupport vector classifier, RBF-SVM, gradient boosting and extreme GB, random\nforest (RF) and multi-layer perceptron (MLP). The transformations examined\ninclude isometric log-ratio (ILR), center log-ratio (CLR) coupled with\nprincipal component analysis (PCA) or independent component analysis (ICA), and\na manifold learning approach based on local linear embedding (LLE). The results\nreveal that different ML classifiers exhibit varying sensitivity to these\ntransformations, with some clearly more advantageous or deleterious than\nothers. Overall, the best performing candidate is ILR which is unsurprising\nconsidering the compositional nature of the data. The performance of pairwise\nlog-ratio (PWLR) transformation is better than ILR for ensemble and tree-based\nlearners such as boosting and RF; but worse for MLP, SVM and other classifiers.",
    "descriptor": "\nComments: Keywords: Compositional data, supervised learning, geological domain, likelihood estimation, classification performance, effects of data transformation. 10 page article, 2 figures, 7 tables\n",
    "authors": [
      "Raymond Leung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05855"
  },
  {
    "id": "arXiv:2106.05858",
    "title": "Characterizing Residential Load Patterns by Household Demographic and  Socioeconomic Factors",
    "abstract": "The wide adoption of smart meters makes residential load data available and\nthus improves the understanding of the energy consumption behavior. Many\nexisting studies have focused on smart-meter data analysis, but the drivers of\nenergy consumption behaviors are not well understood. This paper aims to\ncharacterize and estimate users' load patterns based on their demographic and\nsocioeconomic information. We adopt the symbolic aggregate approximation (SAX)\nmethod to process the load data and use the K-Means method to extract key load\npatterns. We develop a deep neural network (DNN) to analyze the relationship\nbetween users' load patterns and their demographic and socioeconomic features.\nUsing real-world load data, we validate our framework and demonstrate the\nconnections between load patterns and household demographic and socioeconomic\nfeatures. We also take two regression models as benchmarks for comparisons.",
    "descriptor": "\nComments: ACM International Conference on Future Energy Systems (ACM e-Energy) 2021\n",
    "authors": [
      "Zhuo Wei",
      "Hao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05858"
  },
  {
    "id": "arXiv:2106.05859",
    "title": "A Meta Learning Approach to Discerning Causal Graph Structure",
    "abstract": "We explore the usage of meta-learning to derive the causal direction between\nvariables by optimizing over a measure of distribution simplicity. We\nincorporate a stochastic graph representation which includes latent variables\nand allows for more generalizability and graph structure expression. Our model\nis able to learn causal direction indicators for complex graph structures\ndespite effects of latent confounders. Further, we explore robustness of our\nmethod with respect to violations of our distributional assumptions and data\nscarcity. Our model is particularly robust to modest data scarcity, but is less\nrobust to distributional changes. By interpreting the model predictions as\nstochastic events, we propose a simple ensemble method classifier to reduce the\noutcome variability as an average of biased events. This methodology\ndemonstrates ability to infer the existence as well as the direction of a\ncausal relationship between data distributions.",
    "descriptor": "",
    "authors": [
      "Justin Wong",
      "Dominik Damjakob"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05859"
  },
  {
    "id": "arXiv:2106.05860",
    "title": "DMIDAS: Deep Mixed Data Sampling Regression for Long Multi-Horizon Time  Series Forecasting",
    "abstract": "Neural forecasting has shown significant improvements in the accuracy of\nlarge-scale systems, yet predicting extremely long horizons remains a\nchallenging task. Two common problems are the volatility of the predictions and\ntheir computational complexity; we addressed them by incorporating smoothness\nregularization and mixed data sampling techniques to a well-performing\nmulti-layer perceptron based architecture (NBEATS). We validate our proposed\nmethod, DMIDAS, on high-frequency healthcare and electricity price data with\nlong forecasting horizons (~1000 timestamps) where we improve the prediction\naccuracy by 5% over state-of-the-art models, reducing the number of parameters\nof NBEATS by nearly 70%.",
    "descriptor": "",
    "authors": [
      "Cristian Challu",
      "Kin G. Olivares",
      "Gus Welter",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05860"
  },
  {
    "id": "arXiv:2106.05863",
    "title": "Learning Functional Priors and Posteriors from Data and Physics",
    "abstract": "We develop a new Bayesian framework based on deep neural networks to be able\nto extrapolate in space-time using historical data and to quantify\nuncertainties arising from both noisy and gappy data in physical problems.\nSpecifically, the proposed approach has two stages: (1) prior learning and (2)\nposterior estimation. At the first stage, we employ the physics-informed\nGenerative Adversarial Networks (PI-GAN) to learn a functional prior either\nfrom a prescribed function distribution, e.g., Gaussian process, or from\nhistorical data and physics. At the second stage, we employ the Hamiltonian\nMonte Carlo (HMC) method to estimate the posterior in the latent space of\nPI-GANs. In addition, we use two different approaches to encode the physics:\n(1) automatic differentiation, used in the physics-informed neural networks\n(PINNs) for scenarios with explicitly known partial differential equations\n(PDEs), and (2) operator regression using the deep operator network (DeepONet)\nfor PDE-agnostic scenarios. We then test the proposed method for (1)\nmeta-learning for one-dimensional regression, and forward/inverse PDE problems\n(combined with PINNs); (2) PDE-agnostic physical problems (combined with\nDeepONet), e.g., fractional diffusion as well as saturated stochastic\n(100-dimensional) flows in heterogeneous porous media; and (3) spatial-temporal\nregression problems, i.e., inference of a marine riser displacement field. The\nresults demonstrate that the proposed approach can provide accurate predictions\nas well as uncertainty quantification given very limited scattered and noisy\ndata, since historical data could be available to provide informative priors.\nIn summary, the proposed method is capable of learning flexible functional\npriors, and can be extended to big data problems using stochastic HMC or\nnormalizing flows since the latent space is generally characterized as low\ndimensional.",
    "descriptor": "",
    "authors": [
      "Xuhui Meng",
      "Liu Yang",
      "Zhiping Mao",
      "Jose del Aguila Ferrandis",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05863"
  },
  {
    "id": "arXiv:2106.05864",
    "title": "Verifiable and Compositional Reinforcement Learning Systems",
    "abstract": "We propose a novel framework for verifiable and compositional reinforcement\nlearning (RL) in which a collection of RL sub-systems, each of which learns to\naccomplish a separate sub-task, are composed to achieve an overall task. The\nframework consists of a high-level model, represented as a parametric Markov\ndecision process (pMDP) which is used to plan and to analyze compositions of\nsub-systems, and of the collection of low-level sub-systems themselves. By\ndefining interfaces between the sub-systems, the framework enables automatic\ndecompositons of task specifications, e.g., reach a target set of states with a\nprobability of at least 0.95, into individual sub-task specifications, i.e.\nachieve the sub-system's exit conditions with at least some minimum\nprobability, given that its entry conditions are met. This in turn allows for\nthe independent training and testing of the sub-systems; if they each learn a\npolicy satisfying the appropriate sub-task specification, then their\ncomposition is guaranteed to satisfy the overall task specification.\nConversely, if the sub-task specifications cannot all be satisfied by the\nlearned policies, we present a method, formulated as the problem of finding an\noptimal set of parameters in the pMDP, to automatically update the sub-task\nspecifications to account for the observed shortcomings. The result is an\niterative procedure for defining sub-task specifications, and for training the\nsub-systems to meet them. As an additional benefit, this procedure allows for\nparticularly challenging or important components of an overall task to be\ndetermined automatically, and focused on, during training. Experimental results\ndemonstrate the presented framework's novel capabilities.",
    "descriptor": "",
    "authors": [
      "Cyrus Neary",
      "Christos Verginis",
      "Murat Cubuktepe",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05864"
  },
  {
    "id": "arXiv:2106.05870",
    "title": "Investigation of Uncertainty of Deep Learning-based Object  Classification on Radar Spectra",
    "abstract": "Deep learning (DL) has recently attracted increasing interest to improve\nobject type classification for automotive radar.In addition to high accuracy,\nit is crucial for decision making in autonomous vehicles to evaluate the\nreliability of the predictions; however, decisions of DL networks are\nnon-transparent. Current DL research has investigated how uncertainties of\npredictions can be quantified, and in this article, we evaluate the potential\nof these methods for safe, automotive radar perception. In particular we\nevaluate how uncertainty quantification can support radar perception under (1)\ndomain shift, (2) corruptions of input signals, and (3) in the presence of\nunknown objects. We find that in agreement with phenomena observed in the\nliterature,deep radar classifiers are overly confident, even in their wrong\npredictions. This raises concerns about the use of the confidence values for\ndecision making under uncertainty, as the model fails to notify when it cannot\nhandle an unknown situation. Accurate confidence values would allow optimal\nintegration of multiple information sources, e.g. via sensor fusion. We show\nthat by applying state-of-the-art post-hoc uncertainty calibration, the quality\nof confidence measures can be significantly improved,thereby partially\nresolving the over-confidence problem. Our investigation shows that further\nresearch into training and calibrating DL networks is necessary and offers\ngreat potential for safe automotive object classification with radar sensors.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Kanil Patel",
      "William Beluch",
      "Kilian Rambach",
      "Adriana-Eliza Cozma",
      "Michael Pfeiffer",
      "Bin Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05870"
  },
  {
    "id": "arXiv:2106.05872",
    "title": "Knowing when we do not know: Bayesian continual learning for  sensing-based analysis tasks",
    "abstract": "Despite much research targeted at enabling conventional machine learning\nmodels to continually learn tasks and data distributions sequentially without\nforgetting the knowledge acquired, little effort has been devoted to account\nfor more realistic situations where learning some tasks accurately might be\nmore critical than forgetting previous ones. In this paper we propose a\nBayesian inference based framework to continually learn a set of real-world,\nsensing-based analysis tasks that can be tuned to prioritize the remembering of\npreviously learned tasks or the learning of new ones. Our experiments prove the\nrobustness and reliability of the learned models to adapt to the changing\nsensing environment, and show the suitability of using uncertainty of the\npredictions to assess their reliability.",
    "descriptor": "",
    "authors": [
      "Sandra Servia-Rodriguez",
      "Cecilia Mascolo",
      "Young D. Kwon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05872"
  },
  {
    "id": "arXiv:2106.05874",
    "title": "DREAMS: Drilling and Extraction Automated System",
    "abstract": "Drilling and Extraction Automated System (DREAMS) is a fully automated\nprototype-drilling rig that can drill, extract water and assess subsurface\ndensity profiles from simulated lunar and Martian subsurface ice. DREAMS system\nis developed by the Texas A&M drilling automation team and composed of four\nmain components: 1- tensegrity rig structure, 2- drilling system, 3- water\nextracting and heating system, and 4- electronic hardware, controls, and\nmachine algorithm. The vertical and rotational movements are controlled by\nusing an Acme rod, stepper, and rotary motor. DREAMS is a unique system and\ndifferent from other systems presented before in the NASA Rascal-Al competition\nbecause 1- It uses the tensegrity structure concept to decrease the system\nweight, improve mobility, and easier installation in space. 2- It cuts rock\nlayers by using a short bit length connected to drill pipes. This drilling\nmethodology is expected to drill hundreds and thousands of meters below the\nmoon and Martian surfaces without any anticipated problems (not only 1 m.). 3-\nDrilling, heating, and extraction systems are integrated into one system that\ncan work simultaneously or individually to save time and cost.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Mohamed Khaled",
      "Srivignesh Srinivasan",
      "Alkassoum Toure",
      "Muhao Chen",
      "Emily Kincaid",
      "Thomas Lopaz",
      "Luis Rodriguez",
      "Jessica Ezemba",
      "Ayodeji Adeniran",
      "Teresa Valdez",
      "Uthej Vattipalli",
      "Le linh",
      "Ahmed Madi",
      "Eduardo Gildin",
      "Robert Skelton",
      "Sam Noynaert",
      "George Moridis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.05874"
  },
  {
    "id": "arXiv:2106.05875",
    "title": "Interferometric Graph Transform for Community Labeling",
    "abstract": "We present a new approach for learning unsupervised node representations in\ncommunity graphs. We significantly extend the Interferometric Graph Transform\n(IGT) to community labeling: this non-linear operator iteratively extracts\nfeatures that take advantage of the graph topology through demodulation\noperations. An unsupervised feature extraction step cascades modulus\nnon-linearity with linear operators that aim at building relevant invariants\nfor community labeling. Via a simplified model, we show that the IGT\nconcentrates around the E-IGT: those two representations are related through\nsome ergodicity properties. Experiments on community labeling tasks show that\nthis unsupervised representation achieves performances at the level of the\nstate of the art on the standard and challenging datasets Cora, Citeseer,\nPubmed and WikiCS.",
    "descriptor": "",
    "authors": [
      "Nathan Grinsztajn",
      "Louis Leconte",
      "Philippe Preux",
      "Edouard Oyallon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05875"
  },
  {
    "id": "arXiv:2106.05876",
    "title": "Data Fusion for Deep Learning on Transport Mode Detection: A Case Study",
    "abstract": "In Transport Mode Detection, a great diversity of methodologies exist\naccording to the choice made on sensors, preprocessing, model used, etc. In\nthis domain, the comparisons between each option are not always complete.\nExperiments on a public, real-life dataset are led here to evaluate carefully\neach of the choices that were made, with a specific emphasis on data fusion\nmethods. Our most surprising finding is that none of the methods we implemented\nfrom the literature is better than a simple late fusion. Two important\ndecisions are the choice of a sensor and the choice of a representation for the\ndata: we found that using 2D convolutions on spectrograms with a logarithmic\naxis for the frequencies was better than 1-dimensional temporal\nrepresentations.",
    "descriptor": "\nComments: 12 pages, 2 figures, 4 tables Code avaible at this https URL\n",
    "authors": [
      "Hugues Moreau",
      "Andr\u00e9a Vassilev",
      "Liming Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05876"
  },
  {
    "id": "arXiv:2106.05885",
    "title": "KARI: KAnari/QCRI's End-to-End systems for the INTERSPEECH 2021 Indian  Languages Code-Switching Challenge",
    "abstract": "In this paper, we present the Kanari/QCRI (KARI) system and the modeling\nstrategies used to participate in the Interspeech 2021 Code-switching (CS)\nchallenge for low-resource Indian languages. The subtask involved developing a\nspeech recognition system for two CS datasets: Hindi-English and\nBengali-English, collected in a real-life scenario. To tackle the CS\nchallenges, we use transfer learning for incorporating the publicly available\nmonolingual Hindi, Bengali, and English speech data. In this work, we study the\neffectiveness of two steps transfer learning protocol for low-resourced CS\ndata: monolingual pretraining, followed by fine-tuning. For acoustic modeling,\nwe develop an end-to-end convolution-augmented transformer (Conformer). We show\nthat selecting the percentage of each monolingual data affects model biases\ntowards using one language character set over the other in a CS scenario. The\nmodels pretrained on well-aligned and accurate monolingual data showed\nrobustness against misalignment between the segments and the transcription.\nFinally, we develop word-level n-gram language models (LM) to rescore ASR\nrecognition.",
    "descriptor": "",
    "authors": [
      "Amir Hussein",
      "Shammur Chowdhury",
      "Ahmed Ali"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05885"
  },
  {
    "id": "arXiv:2106.05886",
    "title": "Group Equivariant Subsampling",
    "abstract": "Subsampling is used in convolutional neural networks (CNNs) in the form of\npooling or strided convolutions, to reduce the spatial dimensions of feature\nmaps and to allow the receptive fields to grow exponentially with depth.\nHowever, it is known that such subsampling operations are not translation\nequivariant, unlike convolutions that are translation equivariant. Here, we\nfirst introduce translation equivariant subsampling/upsampling layers that can\nbe used to construct exact translation equivariant CNNs. We then generalise\nthese layers beyond translations to general groups, thus proposing group\nequivariant subsampling/upsampling. We use these layers to construct group\nequivariant autoencoders (GAEs) that allow us to learn low-dimensional\nequivariant representations. We empirically verify on images that the\nrepresentations are indeed equivariant to input translations and rotations, and\nthus generalise well to unseen positions and orientations. We further use GAEs\nin models that learn object-centric representations on multi-object datasets,\nand show improved data efficiency and decomposition compared to non-equivariant\nbaselines.",
    "descriptor": "",
    "authors": [
      "Jin Xu",
      "Hyunjik Kim",
      "Tom Rainforth",
      "Yee Whye Teh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05886"
  },
  {
    "id": "arXiv:2106.05891",
    "title": "Temporal and Object Quantification Networks",
    "abstract": "We present Temporal and Object Quantification Networks (TOQ-Nets), a new\nclass of neuro-symbolic networks with a structural bias that enables them to\nlearn to recognize complex relational-temporal events. This is done by\nincluding reasoning layers that implement finite-domain quantification over\nobjects and time. The structure allows them to generalize directly to input\ninstances with varying numbers of objects in temporal sequences of varying\nlengths. We evaluate TOQ-Nets on input domains that require recognizing\nevent-types in terms of complex temporal relational patterns. We demonstrate\nthat TOQ-Nets can generalize from small amounts of data to scenarios containing\nmore objects than were present during training and to temporal warpings of\ninput sequences.",
    "descriptor": "\nComments: IJCAI 2021. First two authors contributed equally. Project page: this http URL\n",
    "authors": [
      "Jiayuan Mao",
      "Zhezheng Luo",
      "Chuang Gan",
      "Joshua B. Tenenbaum",
      "Jiajun Wu",
      "Leslie Pack Kaelbling",
      "Tomer D. Ullman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05891"
  },
  {
    "id": "arXiv:2106.05894",
    "title": "Synthesizing Adversarial Negative Responses for Robust Response Ranking  and Evaluation",
    "abstract": "Open-domain neural dialogue models have achieved high performance in response\nranking and evaluation tasks. These tasks are formulated as a binary\nclassification of responses given in a dialogue context, and models generally\nlearn to make predictions based on context-response content similarity.\nHowever, over-reliance on content similarity makes the models less sensitive to\nthe presence of inconsistencies, incorrect time expressions and other factors\nimportant for response appropriateness and coherence. We propose approaches for\nautomatically creating adversarial negative training data to help ranking and\nevaluation models learn features beyond content similarity. We propose\nmask-and-fill and keyword-guided approaches that generate negative examples for\ntraining more robust dialogue systems. These generated adversarial responses\nhave high content similarity with the contexts but are either incoherent,\ninappropriate or not fluent. Our approaches are fully data-driven and can be\neasily incorporated in existing models and datasets. Experiments on\nclassification, ranking and evaluation tasks across multiple datasets\ndemonstrate that our approaches outperform strong baselines in providing\ninformative negative examples for training dialogue systems.",
    "descriptor": "\nComments: Accepted to Findings of ACL 2021\n",
    "authors": [
      "Prakhar Gupta",
      "Yulia Tsvetkov",
      "Jeffrey P. Bigham"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05894"
  },
  {
    "id": "arXiv:2106.05895",
    "title": "A Novel HOC-Immersed Interface Approach For Elliptic Problems",
    "abstract": "We present a new higher-order accurate finite difference explicit jump\nImmersed Interface Method (EJIIM) for solving two-dimensional elliptic problems\nwith singular source and discontinuous coefficients in the irregular region on\na compact Cartesian mesh. We propose a new strategy for discretizing the\nsolution at irregular points on a nine point compact stencil such that the high\norder compactness is maintained throughout the whole computational domain. The\nscheme is employed to solve four problems embedded with circular and star\nshaped interfaces in a rectangular region having analytical solutions and\nvaried discontinuities across the interface in source and the coefficient\nterms. In the process, we show the superiority of the proposed strategy over\nthe EJIIM and other existing IIM methods. We also simulate the steady-state\nflow past a circular cylinder governed by the Navier-Stokes equations. In all\nthe cases our computed results extremely close to the available numerical and\nexperimental results.",
    "descriptor": "",
    "authors": [
      "Raghav Singhal",
      "Jiten C Kalita"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2106.05895"
  },
  {
    "id": "arXiv:2106.05897",
    "title": "Unsupervised Co-part Segmentation through Assembly",
    "abstract": "Co-part segmentation is an important problem in computer vision for its rich\napplications. We propose an unsupervised learning approach for co-part\nsegmentation from images. For the training stage, we leverage motion\ninformation embedded in videos and explicitly extract latent representations to\nsegment meaningful object parts. More importantly, we introduce a dual\nprocedure of part-assembly to form a closed loop with part-segmentation,\nenabling an effective self-supervision. We demonstrate the effectiveness of our\napproach with a host of extensive experiments, ranging from human bodies,\nhands, quadruped, and robot arms. We show that our approach can achieve\nmeaningful and compact part segmentation, outperforming state-of-the-art\napproaches on diverse benchmarks.",
    "descriptor": "",
    "authors": [
      "Qingzhe Gao",
      "Bin Wang",
      "Libin Liu",
      "Baoquan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05897"
  },
  {
    "id": "arXiv:2106.05903",
    "title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for  Multimodal Hate",
    "abstract": "Accurate detection and classification of online hate is a difficult task.\nImplicit hate is particularly challenging as such content tends to have unusual\nsyntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This\nproblem is heightened with multimodal content, such as memes (combinations of\ntext and images), as they are often harder to decipher than unimodal content\n(e.g., text alone). This paper evaluates the role of semantic and multimodal\ncontext for detecting implicit and explicit hate. We show that both text- and\nvisual- enrichment improves model performance, with the multimodal model\n(0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While\nthe unimodal-text context-aware (transformer) model was the most accurate on\nthe subtask of implicit hate detection, the multimodal model outperformed it\noverall because of a lower propensity towards false positives. We find that all\nmodels perform better on content with full annotator agreement and that\nmultimodal models are best at classifying the content where annotators\ndisagree. To conduct these investigations, we undertook high-quality annotation\nof a sample of 5,000 multimodal entries. Tweets were annotated for primary\ncategory, modality, and strategy. We make this corpus, along with the codebook,\ncode, and final model, freely available.",
    "descriptor": "\nComments: Please note the paper contains examples of hateful content\n",
    "authors": [
      "Austin Botelho",
      "Bertie Vidgen",
      "Scott A. Hale"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05903"
  },
  {
    "id": "arXiv:2106.05905",
    "title": "Multiple Dynamic Pricing for Demand Response with Adaptive  Clustering-based Customer Segmentation in Smart Grids",
    "abstract": "In this paper, we propose a realistic multiple dynamic pricing approach to\ndemand response in the retail market. First, an adaptive clustering-based\ncustomer segmentation framework is proposed to categorize customers into\ndifferent groups to enable the effective identification of usage patterns.\nSecond, customized demand models with important market constraints which\ncapture the price-demand relationship explicitly, are developed for each group\nof customers to improve the model accuracy and enable meaningful pricing.\nThird, the multiple pricing based demand response is formulated as a profit\nmaximization problem subject to realistic market constraints. The overall aim\nof the proposed scalable and practical method aims to achieve 'right' prices\nfor 'right' customers so as to benefit various stakeholders in the system such\nas grid operators, customers and retailers. The proposed multiple pricing\nframework is evaluated via simulations based on real-world datasets.",
    "descriptor": "",
    "authors": [
      "Fanlin Meng",
      "Qian Ma",
      "Zixu Liu",
      "Xiao-Jun Zeng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05905"
  },
  {
    "id": "arXiv:2106.05907",
    "title": "Disentangled Attention as Intrinsic Regularization for Bimanual  Multi-Object Manipulation",
    "abstract": "We address the problem of solving complex bimanual robot manipulation tasks\non multiple objects with sparse rewards. Such complex tasks can be decomposed\ninto sub-tasks that are accomplishable by different robots concurrently or\nsequentially for better efficiency. While previous reinforcement learning\napproaches primarily focus on modeling the compositionality of sub-tasks, two\nfundamental issues are largely ignored particularly when learning cooperative\nstrategies for two robots: (i) domination, i.e., one robot may try to solve a\ntask by itself and leaves the other idle; (ii) conflict, i.e., one robot can\neasily interrupt another's workspace when executing different sub-tasks\nsimultaneously. To tackle these two issues, we propose a novel technique called\ndisentangled attention, which provides an intrinsic regularization for two\nrobots to focus on separate sub-tasks and objects. We evaluate our method on\nfour bimanual manipulation tasks. Experimental results show that our proposed\nintrinsic regularization successfully avoids domination and reduces conflicts\nfor the policies, which leads to significantly more effective cooperative\nstrategies than all the baselines. Our project page with videos is at\nhttps://mehooz.github.io/bimanual-attention.",
    "descriptor": "\nComments: Webpage: this https URL\n",
    "authors": [
      "Minghao Zhang",
      "Pingcheng Jian",
      "Yi Wu",
      "Huazhe Xu",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05907"
  },
  {
    "id": "arXiv:2106.05915",
    "title": "Anatomy X-Net: A Semi-Supervised Anatomy Aware Convolutional Neural  Network for Thoracic Disease Classification",
    "abstract": "Thoracic disease detection from chest radiographs using deep learning methods\nhas been an active area of research in the last decade. Most previous methods\nattempt to focus on the diseased organs of the image by identifying spatial\nregions responsible for significant contributions to the model's prediction. In\ncontrast, expert radiologists first locate the prominent anatomical structures\nbefore determining if those regions are anomalous. Therefore, integrating\nanatomical knowledge within deep learning models could bring substantial\nimprovement in automatic disease classification. This work proposes an\nanatomy-aware attention-based architecture named Anatomy X-Net, that\nprioritizes the spatial features guided by the pre-identified anatomy regions.\nWe leverage a semi-supervised learning method using the JSRT dataset containing\norgan-level annotation to obtain the anatomical segmentation masks (for lungs\nand heart) for the NIH and CheXpert datasets. The proposed Anatomy X-Net uses\nthe pre-trained DenseNet-121 as the backbone network with two corresponding\nstructured modules, the Anatomy Aware Attention (AAA) and Probabilistic\nWeighted Average Pooling (PWAP), in a cohesive framework for anatomical\nattention learning. Our proposed method sets new state-of-the-art performance\non the official NIH test set with an AUC score of 0.8439, proving the efficacy\nof utilizing the anatomy segmentation knowledge to improve the thoracic disease\nclassification. Furthermore, the Anatomy X-Net yields an averaged AUC of 0.9020\non the Stanford CheXpert dataset, improving on existing methods that\ndemonstrate the generalizability of the proposed framework.",
    "descriptor": "",
    "authors": [
      "Uday Kamal",
      "Mohammad Zunaed",
      "Nusrat Binta Nizam",
      "Taufiq Hasan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05915"
  },
  {
    "id": "arXiv:2106.05917",
    "title": "Examining the Examiners: Students' Privacy and Security Perceptions of  Online Proctoring Services",
    "abstract": "In response to the Covid-19 pandemic, educational institutions quickly\ntransitioned to remote learning. The problem of how to perform student\nassessment in an online environment has become increasingly relevant, leading\nmany institutions and educators to turn to online proctoring services to\nadminister remote exams. These services employ various student monitoring\nmethods to curb cheating, including restricted (\"lockdown\") browser modes,\nvideo/screen monitoring, local network traffic analysis, and eye tracking. In\nthis paper, we explore the security and privacy perceptions of the student\ntest-takers being proctored. We analyze user reviews of proctoring services'\nbrowser extensions and subsequently perform an online survey (n=102). Our\nfindings indicate that participants are concerned about both the amount and the\npersonal nature of the information shared with the exam proctoring companies.\nHowever, many participants also recognize a trade-off between pandemic safety\nconcerns and the arguably invasive means by which proctoring services ensure\nexam integrity. Our findings also suggest that institutional power dynamics and\nstudents' trust in their institutions may dissuade students' opposition to\nremote proctoring.",
    "descriptor": "",
    "authors": [
      "David G. Balash",
      "Dongkun Kim",
      "Darikia Shaibekova",
      "Rahel A. Fainchtein",
      "Micah Sherr",
      "Adam J. Aviv"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05917"
  },
  {
    "id": "arXiv:2106.05920",
    "title": "Implicit Feature Alignment: Learn to Convert Text Recognizer to Text  Spotter",
    "abstract": "Text recognition is a popular research subject with many associated\nchallenges. Despite the considerable progress made in recent years, the text\nrecognition task itself is still constrained to solve the problem of reading\ncropped line text images and serves as a subtask of optical character\nrecognition (OCR) systems. As a result, the final text recognition result is\nlimited by the performance of the text detector. In this paper, we propose a\nsimple, elegant and effective paradigm called Implicit Feature Alignment (IFA),\nwhich can be easily integrated into current text recognizers, resulting in a\nnovel inference mechanism called IFAinference. This enables an ordinary text\nrecognizer to process multi-line text such that text detection can be\ncompletely freed. Specifically, we integrate IFA into the two most prevailing\ntext recognition streams (attention-based and CTC-based) and propose\nattention-guided dense prediction (ADP) and Extended CTC (ExCTC). Furthermore,\nthe Wasserstein-based Hollow Aggregation Cross-Entropy (WH-ACE) is proposed to\nsuppress negative predictions to assist in training ADP and ExCTC. We\nexperimentally demonstrate that IFA achieves state-of-the-art performance on\nend-to-end document recognition tasks while maintaining the fastest speed, and\nADP and ExCTC complement each other on the perspective of different application\nscenarios. Code will be available at\nhttps://github.com/WangTianwei/Implicit-feature-alignment.",
    "descriptor": "\nComments: Accepted to CVPR 2021\n",
    "authors": [
      "Tianwei Wang",
      "Yuanzhi Zhu",
      "Lianwen Jin",
      "Dezhi Peng",
      "Zhe Li",
      "Mengchao He",
      "Yongpan Wang",
      "Canjie Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05920"
  },
  {
    "id": "arXiv:2106.05923",
    "title": "FetReg: Placental Vessel Segmentation and Registration in Fetoscopy  Challenge Dataset",
    "abstract": "Fetoscopy laser photocoagulation is a widely used procedure for the treatment\nof Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic\nmultiple pregnancies due to placental vascular anastomoses. This procedure is\nparticularly challenging due to limited field of view, poor manoeuvrability of\nthe fetoscope, poor visibility due to fluid turbidity, variability in light\nsource, and unusual position of the placenta. This may lead to increased\nprocedural time and incomplete ablation, resulting in persistent TTTS.\nComputer-assisted intervention may help overcome these challenges by expanding\nthe fetoscopic field of view through video mosaicking and providing better\nvisualization of the vessel network. However, the research and development in\nthis domain remain limited due to unavailability of high-quality data to encode\nthe intra- and inter-procedure variability. Through the Fetoscopic Placental\nVessel Segmentation and Registration (FetReg) challenge, we present a\nlarge-scale multi-centre dataset for the development of generalized and robust\nsemantic segmentation and video mosaicking algorithms for the fetal environment\nwith a focus on creating drift-free mosaics from long duration fetoscopy\nvideos. In this paper, we provide an overview of the FetReg dataset, challenge\ntasks, evaluation metrics and baseline methods for both segmentation and\nregistration. Baseline methods results on the FetReg dataset shows that our\ndataset poses interesting challenges, which can be modelled and competed for\nthrough our crowd-sourcing initiative of the FetReg challenge.",
    "descriptor": "",
    "authors": [
      "Sophia Bano",
      "Alessandro Casella",
      "Francisco Vasconcelos",
      "Sara Moccia",
      "George Attilakos",
      "Ruwan Wimalasundera",
      "Anna L. David",
      "Dario Paladini",
      "Jan Deprest",
      "Leonardo S. Mattos",
      "Danail Stoyanov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.05923"
  },
  {
    "id": "arXiv:2106.05929",
    "title": "Domain Specific Transporter Framework to Detect Fractures in Ultrasound",
    "abstract": "Ultrasound examination for detecting fractures is ideally suited for\nEmergency Departments (ED) as it is relatively fast, safe (from ionizing\nradiation), has dynamic imaging capability and is easily portable. High\ninterobserver variability in manual assessment of ultrasound scans has piqued\nresearch interest in automatic assessment techniques using Deep Learning (DL).\nMost DL techniques are supervised and are trained on large numbers of labeled\ndata which is expensive and requires many hours of careful annotation by\nexperts. In this paper, we propose an unsupervised, domain specific transporter\nframework to identify relevant keypoints from wrist ultrasound scans. Our\nframework provides a concise geometric representation highlighting regions with\nhigh structural variation in a 3D ultrasound (3DUS) sequence. We also\nincorporate domain specific information represented by instantaneous local\nphase (LP) which detects bone features from 3DUS. We validate the technique on\n3DUS videos obtained from 30 subjects. Each ultrasound scan was independently\nassessed by three readers to identify fractures along with the corresponding\nx-ray. Saliency of keypoints detected in the image\\ are compared against manual\nassessment based on distance from relevant features.The transporter neural\nnetwork was able to accurately detect 180 out of 250 bone regions sampled from\nwrist ultrasound videos. We expect this technique to increase the applicability\nof ultrasound in fracture detection.",
    "descriptor": "\nComments: 10 pages,3 figures\n",
    "authors": [
      "Arpan Tripathi",
      "Abhilash Rakkunedeth",
      "Mahesh Raveendranatha Panicker",
      "Jack Zhang",
      "Naveenjyote Boora",
      "Jacob Jaremko"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05929"
  },
  {
    "id": "arXiv:2106.05932",
    "title": "Early-stopped neural networks are consistent",
    "abstract": "This work studies the behavior of neural networks trained with the logistic\nloss via gradient descent on binary classification data where the underlying\ndata distribution is general, and the (optimal) Bayes risk is not necessarily\nzero. In this setting, it is shown that gradient descent with early stopping\nachieves population risk arbitrarily close to optimal in terms of not just\nlogistic and misclassification losses, but also in terms of calibration,\nmeaning the sigmoid mapping of its outputs approximates the true underlying\nconditional distribution arbitrarily finely. Moreover, the necessary iteration,\nsample, and architectural complexities of this analysis all scale naturally\nwith a certain complexity measure of the true conditional model. Lastly, while\nit is not shown that early stopping is necessary, it is shown that any\nunivariate classifier satisfying a local interpolation property is necessarily\ninconsistent.",
    "descriptor": "",
    "authors": [
      "Ziwei Ji",
      "Justin D. Li",
      "Matus Telgarsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05932"
  },
  {
    "id": "arXiv:2106.05933",
    "title": "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition",
    "abstract": "Recent work on speech self-supervised learning (speech SSL) demonstrated the\nbenefits of scale in learning rich and transferable representations for\nAutomatic Speech Recognition (ASR) with limited parallel data. It is then\nnatural to investigate the existence of sparse and transferrable subnetworks in\npre-trained speech SSL models that can achieve even better low-resource ASR\nperformance. However, directly applying widely adopted pruning methods such as\nthe Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost\nneeded. Moreover, contrary to what LTH predicts, the discovered subnetworks\nyield minimal performance gain compared to the original dense network. In this\nwork, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes\nsubnetworks for much better ASR performance, while only requiring a single\ndownstream finetuning run. PARP is inspired by our surprising observation that\nsubnetworks pruned for pre-training tasks only needed to be slightly adjusted\nto achieve a sizeable performance boost in downstream ASR tasks. Extensive\nexperiments on low-resource English and multi-lingual ASR show (1) sparse\nsubnetworks exist in pre-trained speech SSL, and (2) the computational\nadvantage and performance gain of PARP over baseline pruning methods. On the\n10min Librispeech split without LM decoding, PARP discovers subnetworks from\nwav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full\nmodel. We demonstrate PARP mitigates performance degradation in cross-lingual\nmask transfer, and investigate the possibility of discovering a single\nsubnetwork for 10 spoken languages in one run.",
    "descriptor": "",
    "authors": [
      "Cheng-I Jeff Lai",
      "Yang Zhang",
      "Alexander H. Liu",
      "Shiyu Chang",
      "Yi-Lun Liao",
      "Yung-Sung Chuang",
      "Kaizhi Qian",
      "Sameer Khurana",
      "David Cox",
      "James Glass"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.05933"
  },
  {
    "id": "arXiv:2106.05937",
    "title": "Fair Normalizing Flows",
    "abstract": "Fair representation learning is an attractive approach that promises fairness\nof downstream predictors by encoding sensitive data. Unfortunately, recent work\nhas shown that strong adversarial predictors can still exhibit unfairness by\nrecovering sensitive attributes from these representations. In this work, we\npresent Fair Normalizing Flows (FNF), a new approach offering more rigorous\nfairness guarantees for learned representations. Specifically, we consider a\npractical setting where we can estimate the probability density for sensitive\ngroups. The key idea is to model the encoder as a normalizing flow trained to\nminimize the statistical distance between the latent representations of\ndifferent groups. The main advantage of FNF is that its exact likelihood\ncomputation allows us to obtain guarantees on the maximum unfairness of any\npotentially adversarial downstream predictor. We experimentally demonstrate the\neffectiveness of FNF in enforcing various group fairness notions, as well as\nother attractive properties such as interpretability and transfer learning, on\na variety of challenging real-world datasets.",
    "descriptor": "",
    "authors": [
      "Mislav Balunovi\u0107",
      "Anian Ruoss",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05937"
  },
  {
    "id": "arXiv:2106.05939",
    "title": "Graph Balancing with Orientation Costs",
    "abstract": "Motivated by the classic Generalized Assignment Problem, we consider the\nGraph Balancing problem in the presence of orientation costs: given an\nundirected multi-graph G = (V,E) equipped with edge weights and orientation\ncosts on the edges, the goal is to find an orientation of the edges that\nminimizes both the maximum weight of edges oriented toward any vertex\n(makespan) and total orientation cost. We present a general framework for\nminimizing makespan in the presence of costs that allows us to: (1) achieve\nbicriteria approximations for the Graph Balancing problem that capture known\nprevious results (Shmoys-Tardos [Math. Progrm. 93], Ebenlendr-Krc\\'al- Sgall\n[Algorithmica 14], and Wang-Sitters [Inf. Process. Lett. 16]); and (2) achieve\nbicriteria approximations for extensions of the Graph Balancing problem that\nadmit hyperedges and unrelated weights. Our framework is based on a remarkably\nsimple rounding of a strengthened linear relaxation. We complement the above by\npresenting bicriteria lower bounds with respect to the linear programming\nrelaxations we use that show that a loss in the total orientation cost is\nrequired if one aims for an approximation better than 2 in the makespan.",
    "descriptor": "",
    "authors": [
      "Roy Schwartz",
      "Ran Yeheskel"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05939"
  },
  {
    "id": "arXiv:2106.05944",
    "title": "An Optimal Algorithm for Strict Circular Seriation",
    "abstract": "We study the problem of circular seriation, where we are given a matrix of\npairwise dissimilarities between $n$ objects, and the goal is to find a {\\em\ncircular order} of the objects in a manner that is consistent with their\ndissimilarity. This problem is a generalization of the classical {\\em linear\nseriation} problem where the goal is to find a {\\em linear order}, and for\nwhich optimal ${\\cal O}(n^2)$ algorithms are known. Our contributions can be\nsummarized as follows. First, we introduce {\\em circular Robinson matrices} as\nthe natural class of dissimilarity matrices for the circular seriation problem.\nSecond, for the case of {\\em strict circular Robinson dissimilarity matrices}\nwe provide an optimal ${\\cal O}(n^2)$ algorithm for the circular seriation\nproblem. Finally, we propose a statistical model to analyze the well-posedness\nof the circular seriation problem for large $n$. In particular, we establish\n${\\cal O}(\\log(n)/n)$ rates on the distance between any circular ordering found\nby solving the circular seriation problem to the underlying order of the model,\nin the Kendall-tau metric.",
    "descriptor": "\nComments: 27 pages, 5 figures\n",
    "authors": [
      "Santiago Armstrong",
      "Crist\u00f3bal Guzm\u00e1n",
      "Carlos A. Sing-Long"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.05944"
  },
  {
    "id": "arXiv:2106.05945",
    "title": "Does Knowledge Distillation Really Work?",
    "abstract": "Knowledge distillation is a popular technique for training a small student\nnetwork to emulate a larger teacher model, such as an ensemble of networks. We\nshow that while knowledge distillation can improve student generalization, it\ndoes not typically work as it is commonly understood: there often remains a\nsurprisingly large discrepancy between the predictive distributions of the\nteacher and the student, even in cases when the student has the capacity to\nperfectly match the teacher. We identify difficulties in optimization as a key\nreason for why the student is unable to match the teacher. We also show how the\ndetails of the dataset used for distillation play a role in how closely the\nstudent matches the teacher -- and that more closely matching the teacher\nparadoxically does not always lead to better student generalization.",
    "descriptor": "",
    "authors": [
      "Samuel Stanton",
      "Pavel Izmailov",
      "Polina Kirichenko",
      "Alexander A. Alemi",
      "Andrew Gordon Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05945"
  },
  {
    "id": "arXiv:2106.05946",
    "title": "Curiously Effective Features for Image Quality Prediction",
    "abstract": "The performance of visual quality prediction models is commonly assumed to be\nclosely tied to their ability to capture perceptually relevant image aspects.\nModels are thus either based on sophisticated feature extractors carefully\ndesigned from extensive domain knowledge or optimized through feature learning.\nIn contrast to this, we find feature extractors constructed from random noise\nto be sufficient to learn a linear regression model whose quality predictions\nreach high correlations with human visual quality ratings, on par with a model\nwith learned features. We analyze this curious result and show that besides the\nquality of feature extractors also their quantity plays a crucial role - with\ntop performances only being achieved in highly overparameterized models.",
    "descriptor": "\nComments: To be published at ICIP 2021\n",
    "authors": [
      "S\u00f6ren Becker",
      "Thomas Wiegand",
      "Sebastian Bosse"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05946"
  },
  {
    "id": "arXiv:2106.05953",
    "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via  Contrastive Learning",
    "abstract": "Acquiring accurate 3D annotated data for hand pose estimation is a\nnotoriously difficult problem. This typically requires complex multi-camera\nsetups and controlled conditions, which in turn creates a domain gap that is\nhard to bridge to fully unconstrained settings. Encouraged by the success of\ncontrastive learning on image classification tasks, we propose a new\nself-supervised method for the structured regression task of 3D hand pose\nestimation. Contrastive learning makes use of unlabeled data for the purpose of\nrepresentation learning via a loss formulation that encourages the learned\nfeature representations to be invariant under any image transformation. For 3D\nhand pose estimation, it too is desirable to have invariance to appearance\ntransformation such as color jitter. However, the task requires equivariance\nunder affine transformations, such as rotation and translation. To address this\nissue, we propose an equivariant contrastive objective and demonstrate its\neffectiveness in the context of 3D hand pose estimation. We experimentally\ninvestigate the impact of invariant and equivariant contrastive objectives and\nshow that learning equivariant features leads to better representations for the\ntask of 3D hand pose estimation. Furthermore, we show that a standard\nResNet-152, trained on additional unlabeled data, attains an improvement of\n$7.6\\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance\nwithout any task specific, specialized architectures.",
    "descriptor": "",
    "authors": [
      "Adrian Spurr",
      "Aneesh Dahiya",
      "Xucong Zhang",
      "Xi Wang",
      "Otmar Hilliges"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05953"
  },
  {
    "id": "arXiv:2106.05954",
    "title": "Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation",
    "abstract": "Hand pose estimation is difficult due to different environmental conditions,\nobject- and self-occlusion as well as diversity in hand shape and appearance.\nExhaustively covering this wide range of factors in fully annotated datasets\nhas remained impractical, posing significant challenges for generalization of\nsupervised methods. Embracing this challenge, we propose to combine ideas from\nadversarial training and motion modelling to tap into unlabeled videos. To this\nend we propose what to the best of our knowledge is the first motion model for\nhands and show that an adversarial formulation leads to better generalization\nproperties of the hand pose estimator via semi-supervised training on unlabeled\nvideo sequences. In this setting, the pose predictor must produce a valid\nsequence of hand poses, as determined by a discriminative adversary. This\nadversary reasons both on the structural as well as temporal domain,\neffectively exploiting the spatio-temporal structure in the task. The main\nadvantage of our approach is that we can make use of unpaired videos and joint\nsequence data both of which are much easier to attain than paired training\ndata. We perform extensive evaluation, investigating essential components\nneeded for the proposed framework and empirically demonstrate in two\nchallenging settings that the proposed approach leads to significant\nimprovements in pose estimation accuracy. In the lowest label setting, we\nattain an improvement of $40\\%$ in absolute mean joint error.",
    "descriptor": "",
    "authors": [
      "Adrian Spurr",
      "Pavlo Molchanov",
      "Umar Iqbal",
      "Jan Kautz",
      "Otmar Hilliges"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05954"
  },
  {
    "id": "arXiv:2106.05956",
    "title": "Beyond BatchNorm: Towards a General Understanding of Normalization in  Deep Learning",
    "abstract": "Inspired by BatchNorm, there has been an explosion of normalization layers in\ndeep learning. Recent works have identified a multitude of beneficial\nproperties in BatchNorm to explain its success. However, given the pursuit of\nalternative normalization techniques, these properties need to be generalized\nso that any given layer's success/failure can be accurately predicted. In this\nwork, we take a first step towards this goal by extending known properties of\nBatchNorm in randomly initialized deep neural networks (DNNs) to nine recently\nproposed normalization layers. Our primary findings follow: (i) Similar to\nBatchNorm, activations-based normalization layers can avoid exploding\nactivations in ResNets; (ii) Use of GroupNorm ensures rank of activations is at\nleast $\\Omega(\\sqrt{\\frac{\\text{width}}{\\text{Group Size}}})$, thus explaining\nwhy LayerNorm witnesses slow optimization speed; (iii) Small group sizes result\nin large gradient norm in earlier layers, hence justifying training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals several general mechanisms that\nexplain the success of normalization techniques in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.",
    "descriptor": "",
    "authors": [
      "Ekdeep Singh Lubana",
      "Robert P. Dick",
      "Hidenori Tanaka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05956"
  },
  {
    "id": "arXiv:2106.05961",
    "title": "What Does Rotation Prediction Tell Us about Classifier Accuracy under  Varying Testing Environments?",
    "abstract": "Understanding classifier decision under novel environments is central to the\ncommunity, and a common practice is evaluating it on labeled test sets.\nHowever, in real-world testing, image annotations are difficult and expensive\nto obtain, especially when the test environment is changing. A natural question\nthen arises: given a trained classifier, can we evaluate its accuracy on\nvarying unlabeled test sets? In this work, we train semantic classification and\nrotation prediction in a multi-task way. On a series of datasets, we report an\ninteresting finding, i.e., the semantic classification accuracy exhibits a\nstrong linear relationship with the accuracy of the rotation prediction task\n(Pearson's Correlation r > 0.88). This finding allows us to utilize linear\nregression to estimate classifier performance from the accuracy of rotation\nprediction which can be obtained on the test set through the freely generated\nrotation labels.",
    "descriptor": "\nComments: ICML 2021 camera ready\n",
    "authors": [
      "Weijian Deng",
      "Stephen Gould",
      "Liang Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05961"
  },
  {
    "id": "arXiv:2106.05963",
    "title": "Learning to See by Looking at Noise",
    "abstract": "Current vision systems are trained on huge datasets, and these datasets come\nwith costs: curation is expensive, they inherit human biases, and there are\nconcerns over privacy and usage rights. To counter these costs, interest has\nsurged in learning from cheaper data sources, such as unlabeled images. In this\npaper we go a step further and ask if we can do away with real image datasets\nentirely, instead learning from noise processes. We investigate a suite of\nimage generation models that produce images from simple random processes. These\nare then used as training data for a visual representation learner with a\ncontrastive loss. We study two types of noise processes, statistical image\nmodels and deep generative models under different random initializations. Our\nfindings show that it is important for the noise to capture certain structural\nproperties of real data but that good performance can be achieved even with\nprocesses that are far from realistic. We also find that diversity is a key\nproperty to learn good representations. Datasets, models, and code are\navailable at https://mbaradad.github.io/learning_with_noise.",
    "descriptor": "",
    "authors": [
      "Manel Baradad",
      "Jonas Wulff",
      "Tongzhou Wang",
      "Phillip Isola",
      "Antonio Torralba"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05963"
  },
  {
    "id": "arXiv:2106.05964",
    "title": "Fair Classification with Adversarial Perturbations",
    "abstract": "We study fair classification in the presence of an omniscient adversary that,\ngiven an $\\eta$, is allowed to choose an arbitrary $\\eta$-fraction of the\ntraining samples and arbitrarily perturb their protected attributes. The\nmotivation comes from settings in which protected attributes can be incorrect\ndue to strategic misreporting, malicious actors, or errors in imputation; and\nprior approaches that make stochastic or independence assumptions on errors may\nnot satisfy their guarantees in this adversarial setting. Our main contribution\nis an optimization framework to learn fair classifiers in this adversarial\nsetting that comes with provable guarantees on accuracy and fairness. Our\nframework works with multiple and non-binary protected attributes, is designed\nfor the large class of linear-fractional fairness metrics, and can also handle\nperturbations besides protected attributes. We prove near-tightness of our\nframework's guarantees for natural hypothesis classes: no algorithm can have\nsignificantly better accuracy and any algorithm with better fairness must have\nlower accuracy. Empirically, we evaluate the classifiers produced by our\nframework for statistical rate on real-world and synthetic datasets for a\nfamily of adversaries.",
    "descriptor": "",
    "authors": [
      "L. Elisa Celis",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05964"
  },
  {
    "id": "arXiv:2106.05965",
    "title": "Implicit-PDF: Non-Parametric Representation of Probability Distributions  on the Rotation Manifold",
    "abstract": "Single image pose estimation is a fundamental problem in many vision and\nrobotics tasks, and existing deep learning approaches suffer by not completely\nmodeling and handling: i) uncertainty about the predictions, and ii) symmetric\nobjects with multiple (sometimes infinite) correct poses. To this end, we\nintroduce a method to estimate arbitrary, non-parametric distributions on\nSO(3). Our key idea is to represent the distributions implicitly, with a neural\nnetwork that estimates the probability given the input image and a candidate\npose. Grid sampling or gradient ascent can be used to find the most likely\npose, but it is also possible to evaluate the probability at any pose, enabling\nreasoning about symmetries and uncertainty. This is the most general way of\nrepresenting distributions on manifolds, and to showcase the rich expressive\npower, we introduce a dataset of challenging symmetric and nearly-symmetric\nobjects. We require no supervision on pose uncertainty -- the model trains only\nwith a single pose per example. Nonetheless, our implicit model is highly\nexpressive to handle complex distributions over 3D poses, while still obtaining\naccurate pose estimation on standard non-ambiguous environments, achieving\nstate-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.",
    "descriptor": "",
    "authors": [
      "Kieran Murphy",
      "Carlos Esteves",
      "Varun Jampani",
      "Srikumar Ramalingam",
      "Ameesh Makadia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05965"
  },
  {
    "id": "arXiv:2106.05966",
    "title": "Learning by Watching",
    "abstract": "When in a new situation or geographical location, human drivers have an\nextraordinary ability to watch others and learn maneuvers that they themselves\nmay have never performed. In contrast, existing techniques for learning to\ndrive preclude such a possibility as they assume direct access to an\ninstrumented ego-vehicle with fully known observations and expert driver\nactions. However, such measurements cannot be directly accessed for the non-ego\nvehicles when learning by watching others. Therefore, in an application where\ndata is regarded as a highly valuable asset, current approaches completely\ndiscard the vast portion of the training data that can be potentially obtained\nthrough indirect observation of surrounding vehicles. Motivated by this key\ninsight, we propose the Learning by Watching (LbW) framework which enables\nlearning a driving policy without requiring full knowledge of neither the state\nnor expert actions. To increase its data, i.e., with new perspectives and\nmaneuvers, LbW makes use of the demonstrations of other vehicles in a given\nscene by (1) transforming the ego-vehicle's observations to their points of\nview, and (2) inferring their expert actions. Our LbW agent learns more robust\ndriving policies while enabling data-efficient learning, including quick\nadaptation of the policy to rare and novel scenarios. In particular, LbW drives\nrobustly even with a fraction of available driving data required by existing\nmethods, achieving an average success rate of 92% on the original CARLA\nbenchmark with only 30 minutes of total driving data and 82% with only 10\nminutes.",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Jimuyang Zhang",
      "Eshed Ohn-Bar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05966"
  },
  {
    "id": "arXiv:2106.05967",
    "title": "Revisiting Contrastive Methods for Unsupervised Learning of Visual  Representations",
    "abstract": "Contrastive self-supervised learning has outperformed supervised pretraining\non many downstream tasks like segmentation and object detection. However,\ncurrent methods are still primarily applied to curated datasets like ImageNet.\nIn this paper, we first study how biases in the dataset affect existing\nmethods. Our results show that current contrastive approaches work surprisingly\nwell across: (i) object- versus scene-centric, (ii) uniform versus long-tailed\nand (iii) general versus domain-specific datasets. Second, given the generality\nof the approach, we try to realize further gains with minor modifications. We\nshow that learning additional invariances -- through the use of multi-scale\ncropping, stronger augmentations and nearest neighbors -- improves the\nrepresentations. Finally, we observe that MoCo learns spatially structured\nrepresentations when trained with a multi-crop strategy. The representations\ncan be used for semantic segment retrieval and video instance segmentation\nwithout finetuning. Moreover, the results are on par with specialized models.\nWe hope this work will serve as a useful study for other researchers. The code\nand models will be available at\nhttps://github.com/wvangansbeke/Revisiting-Contrastive-SSL.",
    "descriptor": "\nComments: Paper and supplementary (20 pages). Code: this https URL\n",
    "authors": [
      "Wouter Van Gansbeke",
      "Simon Vandenhende",
      "Stamatios Georgoulis",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05967"
  },
  {
    "id": "arXiv:2106.05968",
    "title": "Space-time Mixing Attention for Video Transformer",
    "abstract": "This paper is on video recognition using Transformers. Very recent attempts\nin this area have demonstrated promising results in terms of recognition\naccuracy, yet they have been also shown to induce, in many cases, significant\ncomputational overheads due to the additional modelling of the temporal\ninformation. In this work, we propose a Video Transformer model the complexity\nof which scales linearly with the number of frames in the video sequence and\nhence induces \\textit{no overhead} compared to an image-based Transformer\nmodel. To achieve this, our model makes two approximations to the full\nspace-time attention used in Video Transformers: (a) It restricts time\nattention to a local temporal window and capitalizes on the Transformer's depth\nto obtain full temporal coverage of the video sequence. (b) It uses efficient\nspace-time mixing to attend \\textit{jointly} spatial and temporal locations\nwithout inducing any additional cost on top of a spatial-only attention model.\nWe also show how to integrate 2 very lightweight mechanisms for global\ntemporal-only attention which provide additional accuracy improvements at\nminimal computational cost. We demonstrate that our model produces very high\nrecognition accuracy on the most popular video recognition datasets while at\nthe same time being significantly more efficient than other Video Transformer\nmodels. Code will be made available.",
    "descriptor": "",
    "authors": [
      "Adrian Bulat",
      "Juan-Manuel Perez-Rua",
      "Swathikiran Sudhakaran",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05968"
  },
  {
    "id": "arXiv:2106.05969",
    "title": "Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation",
    "abstract": "We propose a method for object-aware 3D egocentric pose estimation that\ntightly integrates kinematics modeling, dynamics modeling, and scene object\ninformation. Unlike prior kinematics or dynamics-based approaches where the two\ncomponents are used disjointly, we synergize the two approaches via\ndynamics-regulated training. At each timestep, a kinematic model is used to\nprovide a target pose using video evidence and simulation state. Then, a\nprelearned dynamics model attempts to mimic the kinematic pose in a physics\nsimulator. By comparing the pose instructed by the kinematic model against the\npose generated by the dynamics model, we can use their misalignment to further\nimprove the kinematic model. By factoring in the 6DoF pose of objects (e.g.,\nchairs, boxes) in the scene, we demonstrate for the first time, the ability to\nestimate physically-plausible 3D human-object interactions using a single\nwearable camera. We evaluate our egocentric pose estimation method in both\ncontrolled laboratory settings and real-world scenarios.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Zhengyi Luo",
      "Ryo Hachiuma",
      "Ye Yuan",
      "Kris Kitani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05969"
  },
  {
    "id": "arXiv:2106.05970",
    "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural  Language Generation",
    "abstract": "Automatic evaluations for natural language generation (NLG) conventionally\nrely on token-level or embedding-level comparisons with the text references.\nThis is different from human language processing, for which visual imaginations\noften improve comprehension. In this work, we propose ImaginE, an\nimagination-based automatic evaluation metric for natural language generation.\nWith the help of CLIP and DALL-E, two cross-modal models pre-trained on\nlarge-scale image-text pairs, we automatically generate an image as the\nembodied imagination for the text snippet and compute the imagination\nsimilarity using contextual embeddings. Experiments spanning several text\ngeneration tasks demonstrate that adding imagination with our ImaginE displays\ngreat potential in introducing multi-modal information into NLG evaluation, and\nimproves existing automatic metrics' correlations with human similarity\njudgments in many circumstances.",
    "descriptor": "",
    "authors": [
      "Wanrong Zhu",
      "Xin Eric Wang",
      "An Yan",
      "Miguel Eckstein",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05970"
  },
  {
    "id": "arXiv:2106.03686",
    "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing",
    "abstract": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-output (MIMO)\nwireless radar. Here, the strong clutter due to the reflection of the layered\nstructure's surface often makes the detection of the defects challenging. Thus,\nsophisticated signal separation methods are required for improved defect\ndetection. In many scenarios, the number of defects that we are interested in\nis limited and the signaling response of the layered structure can be modeled\nas a low-rank structure. Therefore, we propose joint rank and sparsity\nminimization for defect detection. In particular, we propose a non-convex\napproach based on the iteratively reweighted nuclear and $\\ell_1-$norm (a\ndouble-reweighted approach) to obtain a higher accuracy compared to the\nconventional nuclear norm and $\\ell_1-$norm minimization. To this end, an\niterative algorithm is designed to estimate the low-rank and sparse\ncontributions. Further, we propose deep learning to learn the parameters of the\nalgorithm (i.e., algorithm unfolding) to improve the accuracy and the speed of\nconvergence of the algorithm. Our numerical results show that the proposed\napproach outperforms the conventional approaches in terms of mean square errors\nof the recovered low-rank and sparse components and the speed of convergence.",
    "descriptor": "",
    "authors": [
      "Udaya S.K.P. Miriya Thanthrige",
      "Peter Jung",
      "Aydin Sezgin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03686"
  },
  {
    "id": "arXiv:2106.05275",
    "title": "Tractable Density Estimation on Learned Manifolds with Conformal  Embedding Flows",
    "abstract": "Normalizing flows are generative models that provide tractable density\nestimation by transforming a simple base distribution into a complex target\ndistribution. However, this technique cannot directly model data supported on\nan unknown low-dimensional manifold, a common occurrence in real-world domains\nsuch as image data. Recent attempts to remedy this limitation have introduced\ngeometric complications that defeat a central benefit of normalizing flows:\nexact density estimation. We recover this benefit with Conformal Embedding\nFlows, a framework for designing flows that learn manifolds with tractable\ndensities. We argue that composing a standard flow with a trainable conformal\nembedding is the most natural way to model manifold-supported data. To this\nend, we present a series of conformal building blocks and apply them in\nexperiments with real-world and synthetic data to demonstrate that flows can\nmodel manifold-supported distributions without sacrificing tractable\nlikelihoods.",
    "descriptor": "",
    "authors": [
      "Brendan Leigh Ross",
      "Jesse C. Cresswell"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05275"
  },
  {
    "id": "arXiv:2106.05285",
    "title": "CaloFlow: Fast and Accurate Generation of Calorimeter Showers with  Normalizing Flows",
    "abstract": "We introduce CaloFlow, a fast detector simulation framework based on\nnormalizing flows. For the first time, we demonstrate that normalizing flows\ncan reproduce many-channel calorimeter showers with extremely high fidelity,\nproviding a fresh alternative to computationally expensive GEANT4 simulations,\nas well as other state-of-the-art fast simulation frameworks based on GANs and\nVAEs. Besides the usual histograms of physical features and images of\ncalorimeter showers, we introduce a new metric for judging the quality of\ngenerative modeling: the performance of a classifier trained to differentiate\nreal from generated images. We show that GAN-generated images can be identified\nby the classifier with 100% accuracy, while images generated from CaloFlow are\nable to fool the classifier much of the time. More broadly, normalizing flows\noffer several advantages compared to other state-of-the-art approaches (GANs\nand VAEs), including: tractable likelihoods; stable and convergent training;\nand principled model selection. Normalizing flows also provide a bijective\nmapping between data and the latent space, which could have other applications\nbeyond simulation, for example, to detector unfolding.",
    "descriptor": "\nComments: 42 pages, 18 figures, 5 tables\n",
    "authors": [
      "Claudius Krause",
      "David Shih"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Phenomenology (hep-ph)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2106.05285"
  },
  {
    "id": "arXiv:2106.05299",
    "title": "Grover's Algorithm for Question Answering",
    "abstract": "Grover's algorithm, a well-know quantum search algorithm, allows one to find\nthe correct item in a database, with quadratic speedup. In this paper we adapt\nGrover's algorithm to the problem of finding a correct answer to a natural\nlanguage question in English, thus contributing to the growing field of Quantum\nNatural Language Processing. Using a grammar that can be interpreted as tensor\ncontractions, each word is represented as a quantum state that serves as input\nto the quantum circuit. We here introduce a quantum measurement to contract the\nrepresentations of words, resulting in the representation of larger text\nfragments. Using this framework, a representation for the question is found\nthat contains all the possible answers in equal quantum superposition, and\nallows for the building of an oracle that can detect a correct answer, being\nagnostic to the specific question. Furthermore, we show that our construction\ncan deal with certain types of ambiguous phrases by keeping the various\ndifferent meanings in quantum superposition.",
    "descriptor": "\nComments: 13 pages, 9 figures. Presented at SemSpace21\n",
    "authors": [
      "A. D. Correia",
      "M. Moortgat",
      "H. T. C. Stoof"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05299"
  },
  {
    "id": "arXiv:2106.05312",
    "title": "B1-EPG representations using block-cutpoint trees",
    "abstract": "In this paper, we are interested in the edge intersection graphs of paths of\na grid where each path has at most one bend, called B1-EPG graphs and first\nintroduced by Golumbic et al (2009). We also consider a proper subclass of\nB1-EPG, the L-EPG graphs, which allows paths only in ``L'' shape. We show that\ntwo superclasses of trees are B1-EPG (one of them being the cactus graphs). On\nthe other hand, we show that the block graphs are L-EPG and provide a linear\ntime algorithm to produce L-EPG representations of generalization of trees.\nThese proofs employed a new technique from previous results in the area based\non block-cutpoint trees of the respective graphs.",
    "descriptor": "\nComments: 9 pages, 13 figures\n",
    "authors": [
      "V. T. F. Luca",
      "F. S. Oliveira",
      "J. L. Szwarcfiter"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.05312"
  },
  {
    "id": "arXiv:2106.05320",
    "title": "Differentiator for Noisy Sampled Signals with Best Worst-Case Accuracy",
    "abstract": "This paper proposes a differentiator for sampled signals with bounded noise\nand bounded second derivative. It is based on a linear program derived from the\navailable sample information and requires no further tuning beyond the noise\nand derivative bounds. A tight bound on the worst-case accuracy, i.e., the\nworst-case differentiation error, is derived, which is the best among all\ncausal differentiators and is moreover shown to be obtained after a fixed\nnumber of sampling steps. Comparisons with the accuracy of existing high-gain\nand sliding-mode differentiators illustrate the obtained results.",
    "descriptor": "\nComments: Please cite the publisher's version. For the publisher's version and full citation details see: this https URL\n",
    "authors": [
      "Hernan Haimovich",
      "Richard Seeber",
      "Rodrigo Aldana-L\u00f3pez",
      "David G\u00f3mez-Guti\u00e9rrez"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05320"
  },
  {
    "id": "arXiv:2106.05344",
    "title": "Uniform intersecting families with large covering number",
    "abstract": "A family $\\mathcal F$ has covering number $\\tau$ if the size of the smallest\nset intersecting all sets from $\\mathcal F$ is equal to $s$. Let $m(n,k,\\tau)$\nstand for the size of the largest intersecting family $\\mathcal F$ of\n$k$-element subsets of $\\{1,\\ldots,n\\}$ with covering number $\\tau$. It is a\nclassical result of Erd\\H os and Lov\\'asz that $m(n,k,k)\\le k^k$ for any $n$.\nIn this short note, we explore the behaviour of $m(n,k,\\tau)$ for $n<k^2$ and\nlarge $\\tau$. The results are quite surprising: For example, we show that\n$m(k^{3/2},k,\\tau) = (1-o(1)){n-1\\choose k-1}$ for $\\tau\\le k-k^{3/4+o(1)}$. At\nthe same time, $m(k^{3/2},k,\\tau)<e^{-ck^{1/2}}{n\\choose k}$ if $\\tau>k-\\frac\n12k^{1/2}$.",
    "descriptor": "",
    "authors": [
      "Peter Frankl",
      "Andrey Kupavskii"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.05344"
  },
  {
    "id": "arXiv:2106.05355",
    "title": "Best possible bounds on the number of distinct differences in  intersecting families",
    "abstract": "For a family $\\mathcal F$, let $\\mathcal D(\\mathcal F)$ stand for the family\nof all sets that can be expressed as $F\\setminus G$, where $F,G\\in \\mathcal F$.\nA family $\\mathcal F$ is intersecting if any two sets from the family have\nnon-empty intersection. In this paper, we study the following question: what is\nthe maximum of $|\\mathcal D(\\mathcal F)|$ for an intersecting family of\n$k$-element sets? Frankl conjectured that the maximum is attained when\n$\\mathcal F$ is the family of all sets containing a fixed element. We show that\nthis holds if $n>50k\\log k$ and $k>50$. At the same time, we provide a\ncounterexample for $n< 4k.$",
    "descriptor": "",
    "authors": [
      "Peter Frankl",
      "Sergei Kiselev",
      "Andrey Kupavskii"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.05355"
  },
  {
    "id": "arXiv:2106.05359",
    "title": "Public Transit for Special Events: Ridership Prediction and Train  Optimization",
    "abstract": "Many special events, including sport games and concerts, often cause surges\nin demand and congestion for transit systems. Therefore, it is important for\ntransit providers to understand their impact on disruptions, delays, and fare\nrevenues. This paper proposes a suite of data-driven techniques that exploit\nAutomated Fare Collection (AFC) data for evaluating, anticipating, and managing\nthe performance of transit systems during recurring congestion peaks due to\nspecial events. This includes an extensive analysis of ridership of the two\nmajor stadiums in downtown Atlanta using rail data from the Metropolitan\nAtlanta Rapid Transit Authority (MARTA). The paper first highlights the\nridership predictability at the aggregate level for each station on both event\nand non-event days. It then presents an unsupervised machine-learning model to\ncluster passengers and identify which train they are boarding. The model makes\nit possible to evaluate system performance in terms of fundamental metrics such\nas the passenger load per train and the wait times of riders. The paper also\npresents linear regression and random forest models for predicting ridership\nthat are used in combination with historical throughput analysis to forecast\ndemand. Finally, simulations are performed that showcase the potential\nimprovements to wait times and demand matching by leveraging proposed\ntechniques to optimize train frequencies based on forecasted demand.",
    "descriptor": "\nComments: 13 pages, 18 figures, 8 tables\n",
    "authors": [
      "Tejas Santanam",
      "Anthony Trasatti",
      "Pascal Van Hentenryck",
      "Hanyu Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05359"
  },
  {
    "id": "arXiv:2106.05397",
    "title": "From inexact optimization to learning via gradient concentration",
    "abstract": "Optimization was recently shown to control the inductive bias in a learning\nprocess, a property referred to as implicit, or iterative regularization. The\nestimator obtained iteratively minimizing the training error can generalise\nwell with no need of further penalties or constraints. In this paper, we\ninvestigate this phenomenon in the context of linear models with smooth loss\nfunctions. In particular, we investigate and propose a proof technique\ncombining ideas from inexact optimization and probability theory, specifically\ngradient concentration. The proof is easy to follow and allows to obtain sharp\nlearning bounds. More generally, it highlights a way to develop optimization\nresults into learning guarantees.",
    "descriptor": "",
    "authors": [
      "Bernhard Stankewitz",
      "Nicole M\u00fccke",
      "Lorenzo Rosasco"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05397"
  },
  {
    "id": "arXiv:2106.05400",
    "title": "Fine-Grained System Identification of Nonlinear Neural Circuits",
    "abstract": "We study the problem of sparse nonlinear model recovery of high dimensional\ncompositional functions. Our study is motivated by emerging opportunities in\nneuroscience to recover fine-grained models of biological neural circuits using\ncollected measurement data. Guided by available domain knowledge in\nneuroscience, we explore conditions under which one can recover the underlying\nbiological circuit that generated the training data. Our results suggest\ninsights of both theoretical and practical interests. Most notably, we find\nthat a sign constraint on the weights is a necessary condition for system\nrecovery, which we establish both theoretically with an identifiability\nguarantee and empirically on simulated biological circuits. We conclude with a\ncase study on retinal ganglion cell circuits using data collected from mouse\nretina, showcasing the practical potential of this approach.",
    "descriptor": "\nComments: Preprint. 11 pages, 8 figures. Accepted Research Track paper to appear at KDD '21, August 14-18, Virtual Event, Singapore\n",
    "authors": [
      "Dawna Bagherian",
      "James Gornet",
      "Jeremy Bernstein",
      "Yu-Li Ni",
      "Yisong Yue",
      "Markus Meister"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05400"
  },
  {
    "id": "arXiv:2106.05408",
    "title": "Audiovisual transfer learning for audio tagging and sound event  detection",
    "abstract": "We study the merit of transfer learning for two sound recognition problems,\ni.e., audio tagging and sound event detection. Employing feature fusion, we\nadapt a baseline system utilizing only spectral acoustic inputs to also make\nuse of pretrained auditory and visual features, extracted from networks built\nfor different tasks and trained with external data. We perform experiments with\nthese modified models on an audiovisual multi-label data set, of which the\ntraining partition contains a large number of unlabeled samples and a smaller\namount of clips with weak annotations, indicating the clip-level presence of 10\nsound categories without specifying the temporal boundaries of the active\nauditory events. For clip-based audio tagging, this transfer learning method\ngrants marked improvements. Addition of the visual modality on top of audio\nalso proves to be advantageous in this context. When it comes to generating\ntranscriptions of audio recordings, the benefit of pretrained features depends\non the requested temporal resolution: for coarse-grained sound event detection,\ntheir utility remains notable. But when more fine-grained predictions are\nrequired, performance gains are strongly reduced due to a mismatch between the\nproblem at hand and the goals of the models from which the pretrained vectors\nwere obtained.",
    "descriptor": "\nComments: submitted to INTERSPEECH 2021\n",
    "authors": [
      "Wim Boes",
      "Hugo Van hamme"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.05408"
  },
  {
    "id": "arXiv:2106.05445",
    "title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive  Sample Size Approach",
    "abstract": "In this paper, we study the application of quasi-Newton methods for solving\nempirical risk minimization (ERM) problems defined over a large dataset.\nTraditional deterministic and stochastic quasi-Newton methods can be executed\nto solve such problems; however, it is known that their global convergence rate\nmay not be better than first-order methods, and their local superlinear\nconvergence only appears towards the end of the learning process. In this\npaper, we use an adaptive sample size scheme that exploits the superlinear\nconvergence of quasi-Newton methods globally and throughout the entire learning\nprocess. The main idea of the proposed adaptive sample size algorithms is to\nstart with a small subset of data points and solve their corresponding ERM\nproblem within its statistical accuracy, and then enlarge the sample size\ngeometrically and use the optimal solution of the problem corresponding to the\nsmaller set as an initial point for solving the subsequent ERM problem with\nmore samples. We show that if the initial sample size is sufficiently large and\nwe use quasi-Newton methods to solve each subproblem, the subproblems can be\nsolved superlinearly fast (after at most three iterations), as we guarantee\nthat the iterates always stay within a neighborhood that quasi-Newton methods\nconverge superlinearly. Numerical experiments on various datasets confirm our\ntheoretical results and demonstrate the computational advantages of our method.",
    "descriptor": "",
    "authors": [
      "Qiujiang Jin",
      "Aryan Mokhtari"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05445"
  },
  {
    "id": "arXiv:2106.05466",
    "title": "Adaptive machine learning for protein engineering",
    "abstract": "Machine-learning models that learn from data to predict how protein sequence\nencodes function are emerging as a useful protein engineering tool. However,\nwhen using these models to suggest new protein designs, one must deal with the\nvast combinatorial complexity of protein sequences. Here, we review how to use\na sequence-to-function machine-learning surrogate model to select sequences for\nexperimental measurement. First, we discuss how to select sequences through a\nsingle round of machine-learning optimization. Then, we discuss sequential\noptimization, where the goal is to discover optimized sequences and improve the\nmodel across multiple rounds of training, optimization, and experimental\nmeasurement.",
    "descriptor": "\nComments: 9 pages, 2 figures\n",
    "authors": [
      "Brian L. Hie",
      "Kevin K. Yang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2106.05466"
  },
  {
    "id": "arXiv:2106.05472",
    "title": "A Central Limit Theorem, Loss Aversion and Multi-Armed Bandits",
    "abstract": "This paper establishes a central limit theorem under the assumption that\nconditional variances can vary in a largely unstructured history-dependent way\nacross experiments subject only to the restriction that they lie in a fixed\ninterval. Limits take a novel and tractable form, and are expressed in terms of\noscillating Brownian motion. A second contribution is application of this\nresult to a class of multi-armed bandit problems where the decision-maker is\nloss averse.",
    "descriptor": "",
    "authors": [
      "Zengjing Chen",
      "Larry G. Epstein",
      "Guodong Zhang"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05472"
  },
  {
    "id": "arXiv:2106.05490",
    "title": "SignalNet: A Low Resolution Sinusoid Decomposition and Estimation  Network",
    "abstract": "The detection and estimation of sinusoids is a fundamental signal processing\ntask for many applications related to sensing and communications. While\nalgorithms have been proposed for this setting, quantization is a critical, but\noften ignored modeling effect. In wireless communications, estimation with low\nresolution data converters is relevant for reduced power consumption in\nwideband receivers. Similarly, low resolution sampling in imaging and spectrum\nsensing allows for efficient data collection. In this work, we propose\nSignalNet, a neural network architecture that detects the number of sinusoids\nand estimates their parameters from quantized in-phase and quadrature samples.\nWe incorporate signal reconstruction internally as domain knowledge within the\nnetwork to enhance learning and surpass traditional algorithms in mean squared\nerror and Chamfer error. We introduce a worst-case learning threshold for\ncomparing the results of our network relative to the underlying data\ndistributions. This threshold provides insight into why neural networks tend to\noutperform traditional methods and into the learned relationships between the\ninput and output distributions. In simulation, we find that our algorithm is\nalways able to surpass the threshold for three-bit data but often cannot exceed\nthe threshold for one-bit data. We use the learning threshold to explain, in\nthe one-bit case, how our estimators learn to minimize the distributional loss,\nrather than learn features from the data.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Signal Processing\n",
    "authors": [
      "Ryan Dreifuerst",
      "Robert W. Heath Jr"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05490"
  },
  {
    "id": "arXiv:2106.05494",
    "title": "Super-Scalable Molecular Dynamics Algorithm",
    "abstract": "Coulomb interaction, following an inverse-square force-law, quantifies the\namount of force between two stationary, electrically charged particles. The\nlong-range nature of Coulomb interactions poses a major challenge to molecular\ndynamics simulations which are major tools for problems at the nano-/micro-\nscale. Various algorithms aim to speed up the pairwise Coulomb interactions to\na linear scaling but the poor scalability limits the size of simulated systems.\nHere, we conduct an efficient molecular dynamics algorithm with the random\nbatch Ewald method on all-atom systems where the complete Fourier components in\nthe Coulomb interaction are replaced by randomly selected mini batches. By\nsimulating the N-body systems up to 100 million particles using 10 thousand CPU\ncores, we show that this algorithm furnishes O(N) complexity, almost perfect\nscalability and an order of magnitude faster computational speed when compared\nto the existing state-of-the-art algorithms. Further examinations of our\nalgorithm on distinct systems, including pure water, micro-phase-separated\nelectrolyte and protein solution demonstrate that the spatiotemporal\ninformation on all time and length scales investigated and thermodynamic\nquantities derived from our algorithm are in perfect agreement with those\nobtained from the existing algorithms. Therefore, our algorithm provides a\nbreakthrough solution on scalability of computing the Coulomb interaction. It\nis particularly useful and cost-effective to simulate ultra-large systems,\nwhich was either impossible or very costing to conduct using existing\nalgorithms, thus would benefit the broad community of sciences.",
    "descriptor": "\nComments: 20 pages, including 4 figures, 4 extended data figures and supporting information\n",
    "authors": [
      "Jiuyang Liang",
      "Pan Tan",
      "Yue Zhao",
      "Lei Li",
      "Shi Jin",
      "Liang Hong",
      "Zhenli Xu"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.05494"
  },
  {
    "id": "arXiv:2106.05531",
    "title": "CALTeC: Content-Adaptive Linear Tensor Completion for Collaborative  Intelligence",
    "abstract": "In collaborative intelligence, an artificial intelligence (AI) model is\ntypically split between an edge device and the cloud. Feature tensors produced\nby the edge sub-model are sent to the cloud via an imperfect communication\nchannel. At the cloud side, parts of the feature tensor may be missing due to\npacket loss. In this paper we propose a method called Content-Adaptive Linear\nTensor Completion (CALTeC) to recover the missing feature data. The proposed\nmethod is fast, data-adaptive, does not require pre-training, and produces\nbetter results than existing methods for tensor data recovery in collaborative\nintelligence.",
    "descriptor": "\nComments: 5 pages, 4 figures, accepted for presentation at IEEE ICIP 2021\n",
    "authors": [
      "Ashiv Dhondea",
      "Robert A. Cohen",
      "Ivan V. Baji\u0107"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05531"
  },
  {
    "id": "arXiv:2106.05533",
    "title": "Perturbation Theory for Quantum Information",
    "abstract": "We report lowest-order series expansions for primary matrix functions of\nquantum states based on a perturbation theory for functions of linear\noperators. Our theory enables efficient computation of functions of perturbed\nquantum states that assume only knowledge of the eigenspectrum of the zeroth\norder state and the density matrix elements of a zero-trace, Hermitian\nperturbation operator, not requiring analysis of the full state or the\nperturbation term. We develop theories for two classes of quantum state\nperturbations, perturbations that preserve the vector support of the original\nstate and perturbations that extend the support beyond the support of the\noriginal state. We highlight relevant features of the two situations, in\nparticular the fact that functions and measures of perturbed quantum states\nwith preserved support can be elegantly and efficiently represented using\nFr\\'echet derivatives. We apply our perturbation theories to find simple\nexpressions for four of the most important quantities in quantum information\ntheory that are commonly computed from density matrices: the Von Neumann\nentropy, the quantum relative entropy, the quantum Chernoff bound, and the\nquantum fidelity.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Michael R Grace",
      "Saikat Guha"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05533"
  },
  {
    "id": "arXiv:2106.05536",
    "title": "An Interpretable Neural Network for Parameter Inference",
    "abstract": "Adoption of deep neural networks in fields such as economics or finance has\nbeen constrained by the lack of interpretability of model outcomes. This paper\nproposes a generative neural network architecture - the parameter encoder\nneural network (PENN) - capable of estimating local posterior distributions for\nthe parameters of a regression model. The parameters fully explain predictions\nin terms of the inputs and permit visualization, interpretation and inference\nin the presence of complex heterogeneous effects and feature dependencies. The\nuse of Bayesian inference techniques offers an intuitive mechanism to\nregularize local parameter estimates towards a stable solution, and to reduce\nnoise-fitting in settings of limited data availability. The proposed neural\nnetwork is particularly well-suited to applications in economics and finance,\nwhere parameter inference plays an important role. An application to an asset\npricing problem demonstrates how the PENN can be used to explore nonlinear risk\ndynamics in financial markets, and to compare empirical nonlinear effects to\nbehavior posited by financial theory.",
    "descriptor": "",
    "authors": [
      "Johann Pfitzinger"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2106.05536"
  },
  {
    "id": "arXiv:2106.05565",
    "title": "Identifiability of interaction kernels in mean-field equations of  interacting particles",
    "abstract": "We study the identifiability of the interaction kernels in mean-field\nequations for intreacting particle systems. The key is to identify function\nspaces on which a probabilistic loss functional has a unique minimizer. We\nprove that identifiability holds on any subspace of two reproducing kernel\nHilbert spaces (RKHS), whose reproducing kernels are intrinsic to the system\nand are data-adaptive. Furthermore, identifiability holds on two ambient L2\nspaces if and only if the integral operators associated with the reproducing\nkernels are strictly positive. Thus, the inverse problem is ill-posed in\ngeneral. We also discuss the implications of identifiability in computational\npractice.",
    "descriptor": "",
    "authors": [
      "Quanjun Lang",
      "Fei Lu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.05565"
  },
  {
    "id": "arXiv:2106.05573",
    "title": "Some modal and temporal translations of generalized basic logic",
    "abstract": "We introduce a family of modal expansions of {\\L}ukasiewicz logic that are\ndesigned to accommodate modal translations of generalized basic logic (as\nformulated with exchange, weakening, and falsum). We further exhibit algebraic\nsemantics for each logic in this family, in particular showing that all of them\nare algebraizable in the sense of Blok and Pigozzi. Using this algebraization\nresult and an analysis of congruences in the pertinent varieties, we establish\nthat each of the introduced modal {\\L}ukasiewicz logics has a local\ndeduction-detachment theorem. By applying Jipsen and Montagna's poset product\nconstruction, we give two translations of generalized basic logic with\nexchange, weakening, and falsum in the style of the celebrated\nG\\\"odel-McKinsey-Tarski translation. The first of these interprets generalized\nbasic logic in a modal {\\L}ukasiewicz logic in the spirit of the classical\nmodal logic S4, whereas the second interprets generalized basic logic in a\ntemporal variant of the latter.",
    "descriptor": "",
    "authors": [
      "Wesley Fussner",
      "William Zuluaga Botero"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05573"
  },
  {
    "id": "arXiv:2106.05582",
    "title": "Learning Nonparametric Volterra Kernels with Gaussian Processes",
    "abstract": "This paper introduces a method for the nonparametric Bayesian learning of\nnonlinear operators, through the use of the Volterra series with kernels\nrepresented using Gaussian processes (GPs), which we term the nonparametric\nVolterra kernels model (NVKM). When the input function to the operator is\nunobserved and has a GP prior, the NVKM constitutes a powerful method for both\nsingle and multiple output regression, and can be viewed as a nonlinear and\nnonparametric latent force model. When the input function is observed, the NVKM\ncan be used to perform Bayesian system identification. We use recent advances\nin efficient sampling of explicit functions from GPs to map process\nrealisations through the Volterra series without resorting to numerical\nintegration, allowing scalability through doubly stochastic variational\ninference, and avoiding the need for Gaussian approximations of the output\nprocesses. We demonstrate the performance of the model for both multiple output\nregression and system identification using standard benchmarks.",
    "descriptor": "\nComments: 17 pages, 5 figures\n",
    "authors": [
      "Magnus Ross",
      "Michael T. Smith",
      "Mauricio A. \u00c1lvarez"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05582"
  },
  {
    "id": "arXiv:2106.05586",
    "title": "Data augmentation in Bayesian neural networks and the cold posterior  effect",
    "abstract": "Data augmentation is a highly effective approach for improving performance in\ndeep neural networks. The standard view is that it creates an enlarged dataset\nby adding synthetic data, which raises a problem when combining it with\nBayesian inference: how much data are we really conditioning on? This question\nis particularly relevant to recent observations linking data augmentation to\nthe cold posterior effect. We investigate various principled ways of finding a\nlog-likelihood for augmented datasets. Our approach prescribes augmenting the\nsame underlying image multiple times, both at test and train-time, and\naveraging either the logits or the predictive probabilities. Empirically, we\nobserve the best performance with averaging probabilities. While there are\ninteractions with the cold posterior effect, neither averaging logits or\naveraging probabilities eliminates it.",
    "descriptor": "",
    "authors": [
      "Seth Nabarro",
      "Stoil Ganev",
      "Adri\u00e0 Garriga-Alonso",
      "Vincent Fortuin",
      "Mark van der Wilk",
      "Laurence Aitchison"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05586"
  },
  {
    "id": "arXiv:2106.05605",
    "title": "Statistical behaviour of interfaces subjected to curvature flow and  torque effects applied to microstructural evolutions",
    "abstract": "The movement of grain boundaries in pure metals and alloys with a low\nconcentration of dislocations has been historically proved to follow curvature\nflow behavior. This mechanism is typically known as grain growth (GG). However,\nrecent 3D in-situ experimental results tend to question this global picture\nconcerning the influence of the curvature on the kinetics of interface\nmigration. This article explains, thanks to 2D anisotropic full-field\nsimulations, how the torque effects can complexify these discussions. It is\nthen illustrated that neglecting torque effects in full-field formulations\nremains potentially a strong hypothesis. The apparent mobility can be much more\ncomplex than expected without necessarily questioning the influence of the\ncurvature on the local kinetic equation.",
    "descriptor": "",
    "authors": [
      "Sebastian Florez",
      "Karen Alvarado",
      "Marc Bernacki"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.05605"
  },
  {
    "id": "arXiv:2106.05639",
    "title": "C-GLISp: Preference-Based Global Optimization under Unknown Constraints  with Applications to Controller Calibration",
    "abstract": "Preference-based global optimization algorithms minimize an unknown objective\nfunction only based on whether the function is better, worse, or similar for\ngiven pairs of candidate optimization vectors. Such optimization problems arise\nin many real-life examples, such as finding the optimal calibration of the\nparameters of a control law. The calibrator can judge whether a particular\ncombination of parameters leads to a better, worse, or similar closed-loop\nperformance. Often, the search for the optimal parameters is also subject to\nunknown constraints. For example, the vector of calibration parameters must not\nlead to closed-loop instability. This paper extends an active preference\nlearning algorithm introduced recently by the authors to handle unknown\nconstraints. The proposed method, called C-GLISp, looks for an optimizer of the\nproblem only based on preferences expressed on pairs of candidate vectors, and\non whether a given vector is reported feasible and/or satisfactory. C-GLISp\nlearns a surrogate of the underlying objective function based on the expressed\npreferences, and a surrogate of the probability that a sample is feasible\nand/or satisfactory based on whether each of the tested vectors was judged as\nsuch. The surrogate functions are used to propose a new candidate vector for\ntesting and assessment iteratively. Numerical benchmarks and a semi-automated\ncontrol calibration task demonstrate the effectiveness of C-GLISp, showing that\nit can reach near-optimal solutions within a small number of iterations.",
    "descriptor": "\nComments: A MATLAB and a Python implementation of C-GLISp is available at this http URL Note: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Mengjia Zhu",
      "Dario Piga",
      "Alberto Bemporad"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05639"
  },
  {
    "id": "arXiv:2106.05647",
    "title": "On the Use of Data from Multiple Mobile Network Operators in Europe to  fight COVID-19",
    "abstract": "The rapid spread of COVID-19 infections on a global level has highlighted the\nneed for accurate, transparent and timely information regarding collective\nmobility patterns to inform de-escalation strategies as well as to provide\nforecasting capacity for re-escalation policies aiming at addressing further\nwaves of the virus. Such information can be extracted using aggregate\nanonymised data from innovative sources such as mobile positioning data. This\npaper presents lessons learnt and results of a unique Business-to-Government\n(B2G) initiative between several Mobile Network Operators in Europe and the\nEuropean Commission. Mobile positioning data have supported policy makers and\npractitioners with evidence and data-driven knowledge to understand and predict\nthe spread of the disease, the effectiveness of the containment measures, their\nsocio-economic impacts while feeding scenarios at EU scale and in a comparable\nway across countries. The challenges of this data sharing initiative are not\nlimited to data quality, harmonisation, and comparability across countries,\nhowever important they are. Equally essential aspects that need to be addressed\nfrom the onset are related to data privacy, security, fundamental rights and\ncommercial sensitivity.",
    "descriptor": "",
    "authors": [
      "Michele Vespe",
      "Stefano Maria Iacus",
      "Carlos Santamaria",
      "Francesco Sermi",
      "Spyridon Spyratos"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05647"
  },
  {
    "id": "arXiv:2106.05658",
    "title": "Quantized Conditional COT-GAN for Video Prediction",
    "abstract": "Causal Optimal Transport (COT) results from imposing a temporal causality\nconstraint on classic optimal transport problems, which naturally generates a\nnew concept of distances between distributions on path spaces. The first\napplication of the COT theory for sequential learning was given in Xu et al.\n(2020), where COT-GAN was introduced as an adversarial algorithm to train\nimplicit generative models optimized for producing sequential data. Relying on\nXu et al. (2020), the contribution of the present paper is twofold. First, we\ndevelop a conditional version of COT-GAN suitable for sequence prediction. This\nmeans that the dataset is now used in order to learn how a sequence will evolve\ngiven the observation of its past evolution. Second, we improve on the\nconvergence results by working with modifications of the empirical measures via\na specific type of quantization due to Backhoff et al. (2020). The resulting\nquantized conditional COT-GAN algorithm is illustrated with an application for\nvideo prediction.",
    "descriptor": "",
    "authors": [
      "Tianlin Xu",
      "Beatrice Acciaio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05658"
  },
  {
    "id": "arXiv:2106.05669",
    "title": "Information Geometry of Reversible Markov Chains",
    "abstract": "We analyze the information geometric structure of time reversibility for\nparametric families of irreducible transition kernels of Markov chains. We\ndefine and characterize reversible exponential families of Markov kernels, and\nshow that irreducible and reversible Markov kernels form both a mixture family\nand, perhaps surprisingly, an exponential family in the set of all stochastic\nkernels. We propose a parametrization of the entire manifold of reversible\nkernels, and inspect reversible geodesics. We define information projections\nonto the reversible manifold, and derive closed-form expressions for the\ne-projection and m-projection, along with Pythagorean identities with respect\nto information divergence, leading to some new notion of reversiblization of\nMarkov kernels. We show the family of edge measures pertaining to irreducible\nand reversible kernels also forms an exponential family among distributions\nover pairs. We further explore geometric properties of the reversible family,\nby comparing them with other remarkable families of stochastic matrices.\nFinally, we show that reversible kernels are, in a sense we define, the minimal\nexponential family generated by the m-family of symmetric kernels, and the\nsmallest mixture family that comprises the e-family of memoryless kernels.",
    "descriptor": "",
    "authors": [
      "Geoffrey Wolfer",
      "Shun Watanabe"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05669"
  },
  {
    "id": "arXiv:2106.05710",
    "title": "DNN-Based Topology Optimisation: Spatial Invariance and Neural Tangent  Kernel",
    "abstract": "We study the SIMP method with a density field generated by a fully-connected\nneural network, taking the coordinates as inputs. In the large width limit, we\nshow that the use of DNNs leads to a filtering effect similar to traditional\nfiltering techniques for SIMP, with a filter described by the Neural Tangent\nKernel (NTK). This filter is however not invariant under translation, leading\nto visual artifacts and non-optimal shapes. We propose two embeddings of the\ninput coordinates, which lead to (approximate) spatial invariance of the NTK\nand of the filter. We empirically confirm our theoretical observations and\nstudy how the filter size is affected by the architecture of the network. Our\nsolution can easily be applied to any other coordinates-based generation\nmethod.",
    "descriptor": "",
    "authors": [
      "Benjamin Dupuis",
      "Arthur Jacot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05710"
  },
  {
    "id": "arXiv:2106.05716",
    "title": "Fastening the Initial Access in 5G NR Sidelink for 6G V2X Networks",
    "abstract": "The ever-increasing demand for intelligent, automated, and connected mobility\nsolutions pushes for the development of an innovative sixth Generation (6G) of\ncellular networks. A radical transformation on the physical layer of vehicular\ncommunications is planned, with a paradigm shift towards beam-based millimeter\nWaves or sub-Terahertz communications, which require precise beam pointing for\nguaranteeing the communication link, especially in high mobility. A key design\naspect is a fast and proactive Initial Access (IA) algorithm to select the\noptimal beam to be used. In this work, we investigate alternative IA techniques\nto fasten the current fifth-generation (5G) standard, targeting an efficient 6G\ndesign. First, we discuss cooperative position-based schemes that rely on the\nposition information. Then, motivated by the intuition of a non-uniform\ndistribution of the communication directions due to road topology constraints,\nwe design two Probabilistic Codebook (PCB) techniques of prioritized beams. In\nthe first one, the PCBs are built leveraging past collected traffic\ninformation, while in the second one, we use the Hough Transform over the\ndigital map to extract dominant road directions. We also show that the\ninformation coming from the angular probability distribution allows designing\nnon-uniform codebook quantization, reducing the degradation of the performances\ncompared to uniform one. Numerical simulation on realistic scenarios shows that\nPCBs-based beam selection outperforms the 5G standard in terms of the number of\nIA trials, with a performance comparable to position-based methods, without\nrequiring the signaling of sensitive information.",
    "descriptor": "",
    "authors": [
      "Marouan Mizmizi",
      "Francesco Linsalata",
      "Mattia Brambilla",
      "Filippo Morandi",
      "Kai Dong",
      "Maurizio Magarini",
      "Monica Nicoli",
      "Majid Nasiri Khormuji",
      "Peng Wang",
      "Renaud Alexandre Pitaval",
      "Umberto Spagnolini"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05716"
  },
  {
    "id": "arXiv:2106.05720",
    "title": "Mismatching as a tool to enhance algorithmic performances of Monte Carlo  methods for the planted clique model",
    "abstract": "Over-parametrization was a crucial ingredient for recent developments in\ninference and machine-learning fields. However a good theory explaining this\nsuccess is still lacking. In this paper we study a very simple case of\nmismatched over-parametrized algorithm applied to one of the most studied\ninference problem: the planted clique problem. We analyze a Monte Carlo (MC)\nalgorithm in the same class of the famous Jerrum algorithm. We show how this MC\nalgorithm is in general suboptimal for the recovery of the planted clique. We\nshow however how to enhance its performances by adding a (mismatched)\nparameter: the temperature; we numerically find that this over-parametrized\nversion of the algorithm can reach the supposed algorithmic threshold for the\nplanted clique problem.",
    "descriptor": "",
    "authors": [
      "Maria Chiara Angelini",
      "Paolo Fachin",
      "Simone de Feo"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Data Structures and Algorithms (cs.DS)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.05720"
  },
  {
    "id": "arXiv:2106.05722",
    "title": "Real-time simulation of parameter-dependent fluid flows through deep  learning-based reduced order models",
    "abstract": "Simulating fluid flows in different virtual scenarios is of key importance in\nengineering applications. However, high-fidelity, full-order models relying,\ne.g., on the finite element method, are unaffordable whenever fluid flows must\nbe simulated in almost real-time. Reduced order models (ROMs) relying, e.g., on\nproper orthogonal decomposition (POD) provide reliable approximations to\nparameter-dependent fluid dynamics problems in rapid times. However, they might\nrequire expensive hyper-reduction strategies for handling parameterized\nnonlinear terms, and enriched reduced spaces (or Petrov-Galerkin projections)\nif a mixed velocity-pressure formulation is considered, possibly hampering the\nevaluation of reliable solutions in real-time. Dealing with fluid-structure\ninteractions entails even higher difficulties. The proposed deep learning\n(DL)-based ROMs overcome all these limitations by learning in a non-intrusive\nway both the nonlinear trial manifold and the reduced dynamics. To do so, they\nrely on deep neural networks, after performing a former dimensionality\nreduction through POD enhancing their training times substantially. The\nresulting POD-DL-ROMs are shown to provide accurate results in almost real-time\nfor the flow around a cylinder benchmark, the fluid-structure interaction\nbetween an elastic beam attached to a fixed, rigid block and a laminar\nincompressible flow, and the blood flow in a cerebral aneurysm.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Stefania Fresca",
      "Andrea Manzoni"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05722"
  },
  {
    "id": "arXiv:2106.05724",
    "title": "Distributionally Robust Prescriptive Analytics with Wasserstein Distance",
    "abstract": "In prescriptive analytics, the decision-maker observes historical samples of\n$(X, Y)$, where $Y$ is the uncertain problem parameter and $X$ is the\nconcurrent covariate, without knowing the joint distribution. Given an\nadditional covariate observation $x$, the goal is to choose a decision $z$\nconditional on this observation to minimize the cost $\\mathbb{E}[c(z,Y)|X=x]$.\nThis paper proposes a new distributionally robust approach under Wasserstein\nambiguity sets, in which the nominal distribution of $Y|X=x$ is constructed\nbased on the Nadaraya-Watson kernel estimator concerning the historical data.\nWe show that the nominal distribution converges to the actual conditional\ndistribution under the Wasserstein distance. We establish the out-of-sample\nguarantees and the computational tractability of the framework. Through\nsynthetic and empirical experiments about the newsvendor problem and portfolio\noptimization, we demonstrate the strong performance and practical value of the\nproposed framework.",
    "descriptor": "",
    "authors": [
      "Tianyu Wang",
      "Ningyuan Chen",
      "Chun Wang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05724"
  },
  {
    "id": "arXiv:2106.05737",
    "title": "dFDA-VeD: A Dynamic Future Demand Aware Vehicle Dispatching System",
    "abstract": "With the rising demand of smart mobility, ride-hailing service is getting\npopular in the urban regions. These services maintain a system for serving the\nincoming trip requests by dispatching available vehicles to the pickup points.\nAs the process should be socially and economically profitable, the task of\nvehicle dispatching is highly challenging, specially due to the time-varying\ntravel demands and traffic conditions. Due to the uneven distribution of travel\ndemands, many idle vehicles could be generated during the operation in\ndifferent subareas. Most of the existing works on vehicle dispatching system,\ndesigned static relocation centers to relocate idle vehicles. However, as\ntraffic conditions and demand distribution dynamically change over time, the\nstatic solution can not fit the evolving situations. In this paper, we propose\na dynamic future demand aware vehicle dispatching system. It can dynamically\nsearch the relocation centers considering both travel demand and traffic\nconditions. We evaluate the system on real-world dataset, and compare with the\nexisting state-of-the-art methods in our experiments in terms of several\nstandard evaluation metrics and operation time. Through our experiments, we\ndemonstrate that the proposed system significantly improves the serving ratio\nand with a very small increase in operation cost.",
    "descriptor": "\nComments: Accepted by EAI MobiQuitous 2020\n",
    "authors": [
      "Yang Guo",
      "Tarique Anwar",
      "Jian Yang",
      "Jia Wu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05737"
  },
  {
    "id": "arXiv:2106.05738",
    "title": "GBHT: Gradient Boosting Histogram Transform for Density Estimation",
    "abstract": "In this paper, we propose a density estimation algorithm called\n\\textit{Gradient Boosting Histogram Transform} (GBHT), where we adopt the\n\\textit{Negative Log Likelihood} as the loss function to make the boosting\nprocedure available for the unsupervised tasks. From a learning theory\nviewpoint, we first prove fast convergence rates for GBHT with the smoothness\nassumption that the underlying density function lies in the space\n$C^{0,\\alpha}$. Then when the target density function lies in spaces\n$C^{1,\\alpha}$, we present an upper bound for GBHT which is smaller than the\nlower bound of its corresponding base learner, in the sense of convergence\nrates. To the best of our knowledge, we make the first attempt to theoretically\nexplain why boosting can enhance the performance of its base learners for\ndensity estimation problems. In experiments, we not only conduct performance\ncomparisons with the widely used KDE, but also apply GBHT to anomaly detection\nto showcase a further application of GBHT.",
    "descriptor": "\nComments: Accepted to ICML2021. arXiv admin note: text overlap with arXiv:2106.01986\n",
    "authors": [
      "Jingyi Cui",
      "Hanyuan Hang",
      "Yisen Wang",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05738"
  },
  {
    "id": "arXiv:2106.05739",
    "title": "Separation Results between Fixed-Kernel and Feature-Learning Probability  Metrics",
    "abstract": "Several works in implicit and explicit generative modeling empirically\nobserved that feature-learning discriminators outperform fixed-kernel\ndiscriminators in terms of the sample quality of the models. We provide\nseparation results between probability metrics with fixed-kernel and\nfeature-learning discriminators using the function classes $\\mathcal{F}_2$ and\n$\\mathcal{F}_1$ respectively, which were developed to study overparametrized\ntwo-layer neural networks. In particular, we construct pairs of distributions\nover hyper-spheres that can not be discriminated by fixed kernel\n$(\\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)\nin high dimensions, but that can be discriminated by their feature learning\n($\\mathcal{F}_1$) counterparts. To further study the separation we provide\nlinks between the $\\mathcal{F}_1$ and $\\mathcal{F}_2$ IPMs with sliced\nWasserstein distances. Our work suggests that fixed-kernel discriminators\nperform worse than their feature learning counterparts because their\ncorresponding metrics are weaker.",
    "descriptor": "\nComments: 32 pages, 5 figures\n",
    "authors": [
      "Carles Domingo-Enrich",
      "Youssef Mroueh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.05739"
  },
  {
    "id": "arXiv:2106.05767",
    "title": "Meta-Learning for Symbolic Hyperparameter Defaults",
    "abstract": "Hyperparameter optimization in machine learning (ML) deals with the problem\nof empirically learning an optimal algorithm configuration from data, usually\nformulated as a black-box optimization problem. In this work, we propose a\nzero-shot method to meta-learn symbolic default hyperparameter configurations\nthat are expressed in terms of the properties of the dataset. This enables a\nmuch faster, but still data-dependent, configuration of the ML algorithm,\ncompared to standard hyperparameter optimization approaches. In the past,\nsymbolic and static default values have usually been obtained as hand-crafted\nheuristics. We propose an approach of learning such symbolic configurations as\nformulas of dataset properties from a large set of prior evaluations on\nmultiple datasets by optimizing over a grammar of expressions using an\nevolutionary algorithm. We evaluate our method on surrogate empirical\nperformance models as well as on real data across 6 ML algorithms on more than\n100 datasets and demonstrate that our method indeed finds viable symbolic\ndefaults.",
    "descriptor": "\nComments: Pieter Gijsbers and Florian Pfisterer contributed equally to the paper. Two page GECCO poster paper accepted at GECCO 2021\n",
    "authors": [
      "Pieter Gijsbers",
      "Florian Pfisterer",
      "Jan N. van Rijn",
      "Bernd Bischl",
      "Joaquin Vanschoren"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05767"
  },
  {
    "id": "arXiv:2106.05797",
    "title": "Linear Classifiers Under Infinite Imbalance",
    "abstract": "We study the behavior of linear discriminant functions for binary\nclassification in the infinite-imbalance limit, where the sample size of one\nclass grows without bound while the sample size of the other remains fixed. The\ncoefficients of the classifier minimize an expected loss specified through a\nweight function. We show that for a broad class of weight functions, the\nintercept diverges but the rest of the coefficient vector has a finite limit\nunder infinite imbalance, extending prior work on logistic regression. The\nlimit depends on the left tail of the weight function, for which we distinguish\nthree cases: bounded, asymptotically polynomial, and asymptotically\nexponential. The limiting coefficient vectors reflect robustness or\nconservatism properties in the sense that they optimize against certain\nworst-case alternatives. In the bounded and polynomial cases, the limit is\nequivalent to an implicit choice of upsampling distribution for the minority\nclass. We apply these ideas in a credit risk setting, with particular emphasis\non performance in the high-sensitivity and high-specificity regions.",
    "descriptor": "",
    "authors": [
      "Paul Glasserman",
      "Mike Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Risk Management (q-fin.RM)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05797"
  },
  {
    "id": "arXiv:2106.05808",
    "title": "Edge Domination Number and the Number of Minimum Edge Dominating Sets in  Pseudofractal Scale-Free Web and Sierpi\u0144ski Gasket",
    "abstract": "As a fundamental research object, the minimum edge dominating set (MEDS)\nproblem is of both theoretical and practical interest. However, determining the\nsize of a MEDS and the number of all MEDSs in a general graph is NP-hard, and\nit thus makes sense to find special graphs for which the MEDS problem can be\nexactly solved. In this paper, we study analytically the MEDS problem in the\npseudofractal scale-free web and the Sierpi\\'nski gasket with the same number\nof vertices and edges. For both graphs, we obtain exact expressions for the\nedge domination number, as well as recursive solutions to the number of\ndistinct MEDSs. In the pseudofractal scale-free web, the edge domination number\nis one-ninth of the number of edges, which is three-fifths of the edge\ndomination number of the Sierpi\\'nski gasket. Moreover, the number of all MEDSs\nin the pseudofractal scale-free web is also less than that corresponding to the\nSierpi\\'nski gasket. We argue that the difference of the size and number of\nMEDSs between the two studied graphs lies in the scale-free topology.",
    "descriptor": "\nComments: 23 pages, 20 figures\n",
    "authors": [
      "Xiaotian Zhou",
      "Zhongzhi Zhang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2106.05808"
  },
  {
    "id": "arXiv:2106.05811",
    "title": "Existence of Strong Solution for the Complexified Non-linear Poisson  Boltzmann Equation",
    "abstract": "We prove the existence and uniqueness of the complexified Nonlinear\nPoisson-Boltzmann Equation (nPBE) in a bounded domain in $\\mathbb{R}^3$. The\nnPBE is a model equation in nonlinear electrostatics. The standard convex\noptimization argument to the complexified nPBE no longer applies, but instead,\na contraction mapping argument is developed. Furthermore, we show that\nuniqueness can be lost if the hypotheses given are not satisfied. The\ncomplixified nPBE is highly relevant to regularity analysis of the solution of\nthe real nPBE with respect to the dielectric (diffusion) and Debye-H\\\"uckel\ncoefficients. This approach is also well-suited to investigate the existence\nand uniqueness problem for a wide class of semi-linear elliptic Partial\nDifferential Equations (PDEs).",
    "descriptor": "",
    "authors": [
      "Brian Choi",
      "Jie Xu",
      "Trevor Norton",
      "Mark Kon",
      "Julio E. Castrillon-Candas"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.05811"
  },
  {
    "id": "arXiv:2106.05837",
    "title": "Niche to normality -- an interdisciplinary review of Vehicle-to-Grid",
    "abstract": "Vehicle-to-Grid (V2G) capabilities, which enable electric vehicles to\ndischarge power from their batteries for external uses, epitomise the coupling\nof the electricity and transport sectors. To thrive at the nexus of these large\nand well-established sectors V2G services must deliver technical, economic and\nsocial values to many stakeholders. In this Review we present a holistic and\ninterdisciplinary examination of V2G services, highlighting the wide range of\npotential benefits as well as the challenges slowing the technology's evolution\nfrom niche trials to mainstream adoption. We find that benefits tend to be\nsiloed by value proposition and stakeholder while the challenges tend to stem\nfrom stacking multiple values and connecting multiple stakeholders.\nConsequently, we identify key areas for future research, industry and policy\nactivities that will accelerate and smoothen the realisation of V2G's potential\nas an essential pillar of clean transport-electricity systems.",
    "descriptor": "",
    "authors": [
      "Bjorn C. P. Sturmberg",
      "Laura Jones",
      "Kathryn Lucas-Healey",
      "Monirul Islam",
      "Hugo Temby"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05837"
  },
  {
    "id": "arXiv:2106.05838",
    "title": "Large-scale optimal transport map estimation using projection pursuit",
    "abstract": "This paper studies the estimation of large-scale optimal transport maps\n(OTM), which is a well-known challenging problem owing to the curse of\ndimensionality. Existing literature approximates the large-scale OTM by a\nseries of one-dimensional OTM problems through iterative random projection.\nSuch methods, however, suffer from slow or none convergence in practice due to\nthe nature of randomly selected projection directions. Instead, we propose an\nestimation method of large-scale OTM by combining the idea of projection\npursuit regression and sufficient dimension reduction. The proposed method,\nnamed projection pursuit Monge map (PPMM), adaptively selects the most\n``informative'' projection direction in each iteration. We theoretically show\nthe proposed dimension reduction method can consistently estimate the most\n``informative'' projection direction in each iteration. Furthermore, the PPMM\nalgorithm weakly convergences to the target large-scale OTM in a reasonable\nnumber of steps. Empirically, PPMM is computationally easy and converges fast.\nWe assess its finite sample performance through the applications of Wasserstein\ndistance estimation and generative models.",
    "descriptor": "",
    "authors": [
      "Cheng Meng",
      "Yuan Ke",
      "Jingyi Zhang",
      "Mengrui Zhang",
      "Wenxuan Zhong",
      "Ping Ma"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05838"
  },
  {
    "id": "arXiv:2106.05850",
    "title": "Matrix Completion with Model-free Weighting",
    "abstract": "In this paper, we propose a novel method for matrix completion under general\nnon-uniform missing structures. By controlling an upper bound of a novel\nbalancing error, we construct weights that can actively adjust for the\nnon-uniformity in the empirical risk without explicitly modeling the\nobservation probabilities, and can be computed efficiently via convex\noptimization. The recovered matrix based on the proposed weighted empirical\nrisk enjoys appealing theoretical guarantees. In particular, the proposed\nmethod achieves a stronger guarantee than existing work in terms of the scaling\nwith respect to the observation probabilities, under asymptotically\nheterogeneous missing settings (where entry-wise observation probabilities can\nbe of different orders). These settings can be regarded as a better theoretical\nmodel of missing patterns with highly varying probabilities. We also provide a\nnew minimax lower bound under a class of heterogeneous settings. Numerical\nexperiments are also provided to demonstrate the effectiveness of the proposed\nmethod.",
    "descriptor": "\nComments: Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021\n",
    "authors": [
      "Jiayi Wang",
      "Raymond K. W. Wong",
      "Xiaojun Mao",
      "Kwun Chuen Gary Chan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05850"
  },
  {
    "id": "arXiv:2106.05852",
    "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and  Modelling Insights",
    "abstract": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.",
    "descriptor": "\nComments: Accepted paper at the 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021 Findings)\n",
    "authors": [
      "Devaraja Adiga",
      "Rishabh Kumar",
      "Amrith Krishna",
      "Preethi Jyothi",
      "Ganesh Ramakrishnan",
      "Pawan Goyal"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.05852"
  },
  {
    "id": "arXiv:2106.05856",
    "title": "MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation",
    "abstract": "We propose a hierarchical normalizing flow model for generating molecular\ngraphs. The model produces new molecular structures from a single-node graph by\nrecursively splitting every node into two. All operations are invertible and\ncan be used as plug-and-play modules. The hierarchical nature of the latent\ncodes allows for precise changes in the resulting graph: perturbations in the\ntop layer cause global structural changes, while perturbations in the\nconsequent layers change the resulting molecule marginally. The proposed model\noutperforms existing generative graph models on the distribution learning task.\nWe also show successful experiments on global and constrained optimization of\nchemical properties using latent codes of the model.",
    "descriptor": "",
    "authors": [
      "Maksim Kuznetsov",
      "Daniil Polykovskiy"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05856"
  },
  {
    "id": "arXiv:2106.05861",
    "title": "CoviLearn: A Machine Learning Integrated Smart X-Ray Device in  Healthcare Cyber-Physical System for Automatic Initial Screening of COVID-19",
    "abstract": "The pandemic of novel Coronavirus Disease 2019 (COVID-19) is widespread all\nover the world causing serious health problems as well as serious impact on the\nglobal economy. Reliable and fast testing of the COVID-19 has been a challenge\nfor researchers and healthcare practitioners. In this work we present a novel\nmachine learning (ML) integrated X-ray device in Healthcare Cyber-Physical\nSystem (H-CPS) or smart healthcare framework (called CoviLearn) to allow\nhealthcare practitioners to perform automatic initial screening of COVID-19\npatients. We propose convolutional neural network (CNN) models of X-ray images\nintegrated into an X-ray device for automatic COVID-19 detection. The proposed\nCoviLearn device will be useful in detecting if a person is COVID-19 positive\nor negative by considering the chest X-ray image of individuals. CoviLearn will\nbe useful tool doctors to detect potential COVID-19 infections instantaneously\nwithout taking more intrusive healthcare data samples, such as saliva and\nblood. COVID-19 attacks the endothelium tissues that support respiratory tract,\nX-rays images can be used to analyze the health of a patient lungs. As all\nhealthcare centers have X-ray machines, it could be possible to use proposed\nCoviLearn X-rays to test for COVID-19 without the especial test kits. Our\nproposed automated analysis system CoviLearn which has 99% accuracy will be\nable to save valuable time of medical professionals as the X-ray machines come\nwith a drawback as it needed a radiology expert.",
    "descriptor": "",
    "authors": [
      "Debanjan Das",
      "Chirag Samal",
      "Deewanshu Ukey",
      "Gourav Chowdhary",
      "Saraju P. Mohanty"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05861"
  },
  {
    "id": "arXiv:2106.05900",
    "title": "Classical algorithms and quantum limitations for maximum cut on  high-girth graphs",
    "abstract": "We study the performance of local quantum algorithms such as the Quantum\nApproximate Optimization Algorithm (QAOA) for the maximum cut problem, and\ntheir relationship to that of classical algorithms.\n(1) We prove that every (quantum or classical) one-local algorithm achieves\non $D$-regular graphs of girth $> 5$ a maximum cut of at most $1/2 +\nC/\\sqrt{D}$ for $C=1/\\sqrt{2} \\approx 0.7071$. This is the first such result\nshowing that one-local algorithms achieve a value bounded away from the true\noptimum for random graphs, which is $1/2 + P_*/\\sqrt{D} + o(1/\\sqrt{D})$ for\n$P_* \\approx 0.7632$. (2) We show that there is a classical $k$-local algorithm\nthat achieves a value of $1/2 + C/\\sqrt{D} - O(1/\\sqrt{k})$ for $D$-regular\ngraphs of girth $> 2k+1$, where $C = 2/\\pi \\approx 0.6366$. This is an\nalgorithmic version of the existential bound of Lyons and is related to the\nalgorithm of Aizenman, Lebowitz, and Ruelle (ALR) for the\nSherrington-Kirkpatrick model. This bound is better than that achieved by the\none-local and two-local versions of QAOA on high-girth graphs. (3) Through\ncomputational experiments, we give evidence that the ALR algorithm achieves\nbetter performance than constant-locality QAOA for random $D$-regular graphs,\nas well as other natural instances, including graphs that do have short cycles.\nOur experimental work suggests that it could be possible to extend beyond our\ntheoretical constraints. This points at the tantalizing possibility that\n$O(1)$-local quantum maximum-cut algorithms might be *pointwise dominated* by\npolynomial-time classical algorithms, in the sense that there is a classical\nalgorithm outputting cuts of equal or better quality *on every possible\ninstance*. This is in contrast to the evidence that polynomial-time algorithms\ncannot simulate the probability distributions induced by local quantum\nalgorithms.",
    "descriptor": "\nComments: 1+20 pages, 2 figures, code online at this https URL\n",
    "authors": [
      "Boaz Barak",
      "Kunal Marwaha"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.05900"
  },
  {
    "id": "arXiv:2106.05931",
    "title": "Score-based Generative Modeling in Latent Space",
    "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive\nresults in terms of both sample quality and distribution coverage. However,\nthey are usually applied directly in data space and often require thousands of\nnetwork evaluations for sampling. Here, we propose the Latent Score-based\nGenerative Model (LSGM), a novel approach that trains SGMs in a latent space,\nrelying on the variational autoencoder framework. Moving from data to latent\nspace allows us to train more expressive generative models, apply SGMs to\nnon-continuous data, and learn smoother SGMs in a smaller space, resulting in\nfewer network evaluations and faster sampling. To enable training LSGMs\nend-to-end in a scalable and stable manner, we (i) introduce a new\nscore-matching objective suitable to the LSGM setting, (ii) propose a novel\nparameterization of the score function that allows SGM to focus on the mismatch\nof the target distribution with respect to a simple Normal one, and (iii)\nanalytically derive multiple techniques for variance reduction of the training\nobjective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10,\noutperforming all existing generative results on this dataset. On\nCelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while\noutperforming them in sampling time by two orders of magnitude. In modeling\nbinary images, LSGM achieves state-of-the-art likelihood on the binarized\nOMNIGLOT dataset.",
    "descriptor": "",
    "authors": [
      "Arash Vahdat",
      "Karsten Kreis",
      "Jan Kautz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05931"
  },
  {
    "id": "arXiv:2106.05934",
    "title": "Flow-based sampling for fermionic lattice field theories",
    "abstract": "Algorithms based on normalizing flows are emerging as promising machine\nlearning approaches to sampling complicated probability distributions in a way\nthat can be made asymptotically exact. In the context of lattice field theory,\nproof-of-principle studies have demonstrated the effectiveness of this approach\nfor scalar theories, gauge theories, and statistical systems. This work\ndevelops approaches that enable flow-based sampling of theories with dynamical\nfermions, which is necessary for the technique to be applied to lattice field\ntheory studies of the Standard Model of particle physics and many condensed\nmatter systems. As a practical demonstration, these methods are applied to the\nsampling of field configurations for a two-dimensional theory of massless\nstaggered fermions coupled to a scalar field via a Yukawa interaction.",
    "descriptor": "\nComments: 26 pages, 5 figures\n",
    "authors": [
      "Michael S. Albergo",
      "Gurtej Kanwar",
      "S\u00e9bastien Racani\u00e8re",
      "Danilo J. Rezende",
      "Julian M. Urban",
      "Denis Boyda",
      "Kyle Cranmer",
      "Daniel C. Hackett",
      "Phiala E. Shanahan"
    ],
    "subjectives": [
      "High Energy Physics - Lattice (hep-lat)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05934"
  },
  {
    "id": "arXiv:2106.05947",
    "title": "Integer programs with bounded subdeterminants and two nonzeros per row",
    "abstract": "We give a strongly polynomial-time algorithm for integer linear programs\ndefined by integer coefficient matrices whose subdeterminants are bounded by a\nconstant and that contain at most two nonzero entries in each row. The core of\nour approach is the first polynomial-time algorithm for the weighted stable set\nproblem on graphs that do not contain more than $k$ vertex-disjoint odd cycles,\nwhere $k$ is any constant. Previously, polynomial-time algorithms were only\nknown for $k=0$ (bipartite graphs) and for $k=1$.\nWe observe that integer linear programs defined by coefficient matrices with\nbounded subdeterminants and two nonzeros per column can be also solved in\nstrongly polynomial-time, using a reduction to $b$-matching.",
    "descriptor": "",
    "authors": [
      "Samuel Fiorini",
      "Gwena\u00ebl Joret",
      "Stefan Weltge",
      "Yelena Yuditsky"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.05947"
  },
  {
    "id": "arXiv:2106.05951",
    "title": "Support Recovery of Sparse Signals from a Mixture of Linear Measurements",
    "abstract": "Recovery of support of a sparse vector from simple measurements is a widely\nstudied problem, considered under the frameworks of compressed sensing, 1-bit\ncompressed sensing, and more general single index models. We consider\ngeneralizations of this problem: mixtures of linear regressions, and mixtures\nof linear classifiers, where the goal is to recover supports of multiple sparse\nvectors using only a small number of possibly noisy linear, and 1-bit\nmeasurements respectively. The key challenge is that the measurements from\ndifferent vectors are randomly mixed. Both of these problems were also\nextensively studied recently. In mixtures of linear classifiers, the\nobservations correspond to the side of queried hyperplane a random unknown\nvector lies in, whereas in mixtures of linear regressions we observe the\nprojection of a random unknown vector on the queried hyperplane. The primary\nstep in recovering the unknown vectors from the mixture is to first identify\nthe support of all the individual component vectors. In this work, we study the\nnumber of measurements sufficient for recovering the supports of all the\ncomponent vectors in a mixture in both these models. We provide algorithms that\nuse a number of measurements polynomial in $k, \\log n$ and quasi-polynomial in\n$\\ell$, to recover the support of all the $\\ell$ unknown vectors in the mixture\nwith high probability when each individual component is a $k$-sparse\n$n$-dimensional vector.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Venkata Gandikota",
      "Arya Mazumdar",
      "Soumyabrata Pal"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05951"
  },
  {
    "id": "arXiv:2106.05958",
    "title": "Near-Optimal High Probability Complexity Bounds for Non-Smooth  Stochastic Optimization with Heavy-Tailed Noise",
    "abstract": "Thanks to their practical efficiency and random nature of the data,\nstochastic first-order methods are standard for training large-scale machine\nlearning models. Random behavior may cause a particular run of an algorithm to\nresult in a highly suboptimal objective value, whereas theoretical guarantees\nare usually proved for the expectation of the objective value. Thus, it is\nessential to theoretically guarantee that algorithms provide small objective\nresidual with high probability. Existing methods for non-smooth stochastic\nconvex optimization have complexity bounds with the dependence on the\nconfidence level that is either negative-power or logarithmic but under an\nadditional assumption of sub-Gaussian (light-tailed) noise distribution that\nmay not hold in practice, e.g., in several NLP tasks. In our paper, we resolve\nthis issue and derive the first high-probability convergence results with\nlogarithmic dependence on the confidence level for non-smooth convex stochastic\noptimization problems with non-sub-Gaussian (heavy-tailed) noise. To derive our\nresults, we propose novel stepsize rules for two stochastic methods with\ngradient clipping. Moreover, our analysis works for generalized smooth\nobjectives with H\\\"older-continuous gradients, and for both methods, we provide\nan extension for strongly convex problems. Finally, our results imply that the\nfirst (accelerated) method we consider also has optimal iteration and oracle\ncomplexity in all the regimes, and the second one is optimal in the non-smooth\nsetting.",
    "descriptor": "\nComments: 53 pages, 5 figures. arXiv admin note: text overlap with arXiv:2005.10785\n",
    "authors": [
      "Eduard Gorbunov",
      "Marina Danilova",
      "Innokentiy Shibaev",
      "Pavel Dvurechensky",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05958"
  },
  {
    "id": "arXiv:2106.05960",
    "title": "Compositional Modeling of Nonlinear Dynamical Systems with ODE-based  Random Features",
    "abstract": "Effectively modeling phenomena present in highly nonlinear dynamical systems\nwhilst also accurately quantifying uncertainty is a challenging task, which\noften requires problem-specific techniques. We present a novel, domain-agnostic\napproach to tackling this problem, using compositions of physics-informed\nrandom features, derived from ordinary differential equations. The architecture\nof our model leverages recent advances in approximate inference for deep\nGaussian processes, such as layer-wise weight-space approximations which allow\nus to incorporate random Fourier features, and stochastic variational inference\nfor approximate Bayesian inference. We provide evidence that our model is\ncapable of capturing highly nonlinear behaviour in real-world multivariate time\nseries data. In addition, we find that our approach achieves comparable\nperformance to a number of other probabilistic models on benchmark regression\ntasks.",
    "descriptor": "\nComments: 14 pages, 6 figures\n",
    "authors": [
      "Thomas M. McDonald",
      "Mauricio A. \u00c1lvarez"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05960"
  },
  {
    "id": "arXiv:1512.03473",
    "title": "Sensitivity Analysis for Binary Sampling Systems via Quantitative Fisher  Information Lower Bounds",
    "abstract": "Sensitivity Analysis for Binary Sampling Systems via Quantitative Fisher  Information Lower Bounds",
    "descriptor": "",
    "authors": [
      "Manuel S. Stein"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/1512.03473"
  },
  {
    "id": "arXiv:1608.02389",
    "title": "On $H$-Topological Intersection Graphs",
    "abstract": "On $H$-Topological Intersection Graphs",
    "descriptor": "",
    "authors": [
      "Steven Chaplick",
      "Martin T\u00f6pfer",
      "Jan Voborn\u00edk",
      "Peter Zeman"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1608.02389"
  },
  {
    "id": "arXiv:1708.06578",
    "title": "Cascade and Parallel Convolutional Recurrent Neural Networks on  EEG-based Intention Recognition for Brain Computer Interface",
    "abstract": "Comments: 8 pages, 5 figures",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Dalin Zhang",
      "Lina Yao",
      "Xiang Zhang",
      "Sen Wang",
      "Weitong Chen",
      "Robert Boots"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/1708.06578"
  },
  {
    "id": "arXiv:1811.11606",
    "title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering",
    "abstract": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering",
    "descriptor": "",
    "authors": [
      "Philipp Henzler",
      "Niloy Mitra",
      "Tobias Ritschel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/1811.11606"
  },
  {
    "id": "arXiv:1812.00263",
    "title": "PowerCut and Obfuscator: An Exploration of the Design Space for  Privacy-Preserving Interventions for Voice Assistants",
    "abstract": "PowerCut and Obfuscator: An Exploration of the Design Space for  Privacy-Preserving Interventions for Voice Assistants",
    "descriptor": "",
    "authors": [
      "Varun Chandrasekaran",
      "Suman Banerjee",
      "Bilge Mutlu",
      "Kassem Fawaz"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/1812.00263"
  },
  {
    "id": "arXiv:1901.11351",
    "title": "Semi-Supervised Ordinal Regression Based on Empirical Risk Minimization",
    "abstract": "Comments: 38 pages, 9 figures",
    "descriptor": "\nComments: 38 pages, 9 figures\n",
    "authors": [
      "Taira Tsuchiya",
      "Nontawat Charoenphakdee",
      "Issei Sato",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1901.11351"
  },
  {
    "id": "arXiv:1904.10258",
    "title": "Compression is Comprehension, and the Unreasonable Effectiveness of  Digital Computation in the Natural World",
    "abstract": "Comments: 30 pages. Invited contribution to Chaitin's festschrift based on an invited talk delivered at the Workshop on 'Patterns in the World', Department of Philosophy, University of Barcelona on December 14, 2018",
    "descriptor": "\nComments: 30 pages. Invited contribution to Chaitin's festschrift based on an invited talk delivered at the Workshop on 'Patterns in the World', Department of Philosophy, University of Barcelona on December 14, 2018\n",
    "authors": [
      "Hector Zenil"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1904.10258"
  },
  {
    "id": "arXiv:1906.10091",
    "title": "Fixed-time Control under Spatiotemporal and Input Constraints: A  Quadratic Program Based Approach",
    "abstract": "Comments: 14 pages, 13 figures",
    "descriptor": "\nComments: 14 pages, 13 figures\n",
    "authors": [
      "Kunal Garg",
      "Ehsan Arabi",
      "Dimitra Panagou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/1906.10091"
  },
  {
    "id": "arXiv:1908.01947",
    "title": "New Design Paradigm of Distortion Cost Function for Efficient JPEG  Steganography",
    "abstract": "New Design Paradigm of Distortion Cost Function for Efficient JPEG  Steganography",
    "descriptor": "",
    "authors": [
      "Wenkang Su",
      "Jiangqun Ni",
      "Xianglei Hu",
      "Jiwu Huang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/1908.01947"
  },
  {
    "id": "arXiv:1908.06401",
    "title": "On the Robustness of Human Pose Estimation",
    "abstract": "Comments: Accepted in CVPR-W",
    "descriptor": "\nComments: Accepted in CVPR-W\n",
    "authors": [
      "Sahil Shah",
      "Naman Jain",
      "Abhishek Sharma",
      "Arjun Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1908.06401"
  },
  {
    "id": "arXiv:1909.01904",
    "title": "VoIPLoc: Passive VoIP call provenance via acoustic side-channels",
    "abstract": "Comments: 12 pages, 8 figures, 5 tables",
    "descriptor": "\nComments: 12 pages, 8 figures, 5 tables\n",
    "authors": [
      "Shishir Nagaraja",
      "Ryan Shah"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/1909.01904"
  },
  {
    "id": "arXiv:1909.12425",
    "title": "Dynamic Search -- Optimizing the Game of Information Seeking",
    "abstract": "Dynamic Search -- Optimizing the Game of Information Seeking",
    "descriptor": "",
    "authors": [
      "Zhiwen Tang",
      "Grace Hui Yang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/1909.12425"
  },
  {
    "id": "arXiv:1911.01626",
    "title": "Faster Parallel Algorithm for Approximate Shortest Path",
    "abstract": "Comments: 53 pages, STOC 2020",
    "descriptor": "\nComments: 53 pages, STOC 2020\n",
    "authors": [
      "Jason Li"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/1911.01626"
  },
  {
    "id": "arXiv:2001.05535",
    "title": "The Bhargava greedoid as a Gaussian elimination greedoid",
    "abstract": "Comments: 67 pages. Pages 1-36 are the core of the paper; the rest proves the optimality of the bound (under certain conditions) and fills in some very basic details. See this http URL for a streamlined survey. Comments are welcome! v3 adds a brief discussion of phylogenetic applications (p. 3) and more references",
    "descriptor": "\nComments: 67 pages. Pages 1-36 are the core of the paper; the rest proves the optimality of the bound (under certain conditions) and fills in some very basic details. See this http URL for a streamlined survey. Comments are welcome! v3 adds a brief discussion of phylogenetic applications (p. 3) and more references\n",
    "authors": [
      "Darij Grinberg"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2001.05535"
  },
  {
    "id": "arXiv:2002.05273",
    "title": "A Second look at Exponential and Cosine Step Sizes: Simplicity,  Adaptivity, and Performance",
    "abstract": "A Second look at Exponential and Cosine Step Sizes: Simplicity,  Adaptivity, and Performance",
    "descriptor": "",
    "authors": [
      "Xiaoyu Li",
      "Zhenxun Zhuang",
      "Francesco Orabona"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2002.05273"
  },
  {
    "id": "arXiv:2002.08405",
    "title": "On Under-exploration in Bandits with Mean Bounds from Confounded Data",
    "abstract": "On Under-exploration in Bandits with Mean Bounds from Confounded Data",
    "descriptor": "",
    "authors": [
      "Nihal Sharma",
      "Soumya Basu",
      "Karthikeyan Shanmugam",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.08405"
  },
  {
    "id": "arXiv:2002.12301",
    "title": "An On-Device Federated Learning Approach for Cooperative Anomaly  Detection",
    "abstract": "An On-Device Federated Learning Approach for Cooperative Anomaly  Detection",
    "descriptor": "",
    "authors": [
      "Rei Ito",
      "Mineto Tsukada",
      "Hiroki Matsutani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.12301"
  },
  {
    "id": "arXiv:2003.02974",
    "title": "A flow disturbance estimation and rejection strategy for multirotors  with round-trip trajectories",
    "abstract": "Comments: Experimental validation video can be found here: this https URL",
    "descriptor": "\nComments: Experimental validation video can be found here: this https URL\n",
    "authors": [
      "Jaeseung Byun",
      "Simo A. M\u00e4kiharju",
      "Mark W. Mueller"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2003.02974"
  },
  {
    "id": "arXiv:2003.05554",
    "title": "Linear-time inference for Gaussian Processes on one dimension",
    "abstract": "Comments: New experiments about importance of rank, Q",
    "descriptor": "\nComments: New experiments about importance of rank, Q\n",
    "authors": [
      "Jackson Loper",
      "David Blei",
      "John P. Cunningham",
      "Liam Paninski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2003.05554"
  },
  {
    "id": "arXiv:2003.09671",
    "title": "On Information Plane Analyses of Neural Network Classifiers -- A Review",
    "abstract": "Comments: 12 pages, 3 figures; accepted for publication in IEEE Transactions on Neural Networks and Learning Systems. (c) 2021 IEEE",
    "descriptor": "\nComments: 12 pages, 3 figures; accepted for publication in IEEE Transactions on Neural Networks and Learning Systems. (c) 2021 IEEE\n",
    "authors": [
      "Bernhard C. Geiger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.09671"
  },
  {
    "id": "arXiv:2003.13827",
    "title": "Co-occurrence of deep convolutional features for image search",
    "abstract": "Co-occurrence of deep convolutional features for image search",
    "descriptor": "",
    "authors": [
      "J.I.Forcen",
      "Miguel Pagola",
      "Edurne Barrenechea",
      "Humberto Bustince"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2003.13827"
  },
  {
    "id": "arXiv:2004.00858",
    "title": "Projection Neural Network for a Class of Sparse Regression Problems with  Cardinality Penalty",
    "abstract": "Projection Neural Network for a Class of Sparse Regression Problems with  Cardinality Penalty",
    "descriptor": "",
    "authors": [
      "Wenjing Li",
      "Wei Bian"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2004.00858"
  },
  {
    "id": "arXiv:2004.03725",
    "title": "Fully-Heterogeneous Containment Control of a Network of Leader-Follower  Systems",
    "abstract": "Fully-Heterogeneous Containment Control of a Network of Leader-Follower  Systems",
    "descriptor": "",
    "authors": [
      "Majid Mazouchi",
      "Farzaneh Tatari",
      "Bahare Kiumarsi",
      "Hamidreza Modares"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2004.03725"
  },
  {
    "id": "arXiv:2006.04404",
    "title": "Gradient Flow Approach to the Calculation of Stationary States on  Nonlinear Quantum Graphs",
    "abstract": "Gradient Flow Approach to the Calculation of Stationary States on  Nonlinear Quantum Graphs",
    "descriptor": "",
    "authors": [
      "Christophe Besse",
      "Romain Duboscq",
      "Stefan Le Coz"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2006.04404"
  },
  {
    "id": "arXiv:2006.06721",
    "title": "Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural  Networks",
    "abstract": "Comments: 17 pages, 7 figures, under submission, (major revision from previous version)",
    "descriptor": "\nComments: 17 pages, 7 figures, under submission, (major revision from previous version)\n",
    "authors": [
      "Kathrin Grosse",
      "Taesung Lee",
      "Battista Biggio",
      "Youngja Park",
      "Michael Backes",
      "Ian Molloy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.06721"
  },
  {
    "id": "arXiv:2006.07593",
    "title": "Optimal Transport Kernels for Sequential and Parallel Neural  Architecture Search",
    "abstract": "Comments: 23 pages, camera ready ICML2021",
    "descriptor": "\nComments: 23 pages, camera ready ICML2021\n",
    "authors": [
      "Vu Nguyen",
      "Tam Le",
      "Makoto Yamada",
      "Michael A Osborne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.07593"
  },
  {
    "id": "arXiv:2006.09668",
    "title": "Analysis and Design of Thompson Sampling for Stochastic Partial  Monitoring",
    "abstract": "Comments: Published version in NeurIPS 2020 (this https URL), 39 pages, 4 figures",
    "descriptor": "\nComments: Published version in NeurIPS 2020 (this https URL), 39 pages, 4 figures\n",
    "authors": [
      "Taira Tsuchiya",
      "Junya Honda",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.09668"
  },
  {
    "id": "arXiv:2006.11652",
    "title": "A numerical framework for elastic surface matching, comparison, and  interpolation",
    "abstract": "Comments: 21 pages, 11 figures, 1 table, 3 algorithms. Forthcoming in the International Journal of Computer Vision",
    "descriptor": "\nComments: 21 pages, 11 figures, 1 table, 3 algorithms. Forthcoming in the International Journal of Computer Vision\n",
    "authors": [
      "Martin Bauer",
      "Nicolas Charon",
      "Philipp Harms",
      "Hsi-Wei Hsieh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2006.11652"
  },
  {
    "id": "arXiv:2006.16993",
    "title": "Feature Extraction for Novelty Detection in Network Traffic",
    "abstract": "Feature Extraction for Novelty Detection in Network Traffic",
    "descriptor": "",
    "authors": [
      "Kun Yang",
      "Samory Kpotufe",
      "Nick Feamster"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.16993"
  },
  {
    "id": "arXiv:2007.00903",
    "title": "Coordinate-wise Median: Not Bad, Not Bad, Pretty Good",
    "abstract": "Comments: 24 pages",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Sumit Goel",
      "Wade Hann-Caruthers"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2007.00903"
  },
  {
    "id": "arXiv:2007.01243",
    "title": "Learning ordered pooling weights in image classification",
    "abstract": "Learning ordered pooling weights in image classification",
    "descriptor": "",
    "authors": [
      "J.I.Forcen",
      "Miguel Pagola",
      "Edurne Barrenechea",
      "Humberto Bustince"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.01243"
  },
  {
    "id": "arXiv:2007.01903",
    "title": "Model Distillation for Revenue Optimization: Interpretable Personalized  Pricing",
    "abstract": "Model Distillation for Revenue Optimization: Interpretable Personalized  Pricing",
    "descriptor": "",
    "authors": [
      "Max Biggs",
      "Wei Sun",
      "Markus Ettl"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2007.01903"
  },
  {
    "id": "arXiv:2007.04584",
    "title": "VisImages: a Corpus of Visualizations in the Images of Visualization  Publications",
    "abstract": "VisImages: a Corpus of Visualizations in the Images of Visualization  Publications",
    "descriptor": "",
    "authors": [
      "Dazhen Deng",
      "Yihong Wu",
      "Xinhuan Shu",
      "Jiang Wu",
      "Mengye Xu",
      "Siwei Fu",
      "Weiwei Cui",
      "Yingcai Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.04584"
  },
  {
    "id": "arXiv:2007.04938",
    "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep  Reinforcement Learning",
    "abstract": "Comments: ICML 2021 camera ready",
    "descriptor": "\nComments: ICML 2021 camera ready\n",
    "authors": [
      "Kimin Lee",
      "Michael Laskin",
      "Aravind Srinivas",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.04938"
  },
  {
    "id": "arXiv:2007.10704",
    "title": "A Greedy Algorithm for the Social Golfer and the Oberwolfach Problem",
    "abstract": "Comments: 24 pages, 4 figures",
    "descriptor": "\nComments: 24 pages, 4 figures\n",
    "authors": [
      "Daniel Schmand",
      "Marc Schr\u00f6der",
      "Laura Vargas Koch"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2007.10704"
  },
  {
    "id": "arXiv:2007.13040",
    "title": "Improving Generalization in Meta-learning via Task Augmentation",
    "abstract": "Comments: Accepted by ICML 2021",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Huaxiu Yao",
      "Longkai Huang",
      "Linjun Zhang",
      "Ying Wei",
      "Li Tian",
      "James Zou",
      "Junzhou Huang",
      "Zhenhui Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.13040"
  },
  {
    "id": "arXiv:2007.15966",
    "title": "LSOS: Line-search Second-Order Stochastic optimization methods for  nonconvex finite sums",
    "abstract": "Comments: 22 pages, 4 figures",
    "descriptor": "\nComments: 22 pages, 4 figures\n",
    "authors": [
      "Daniela di Serafino",
      "Nata\u0161a Kreji\u0107",
      "Nata\u0161a Krklec Jerinki\u0107",
      "Marco Viola"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2007.15966"
  },
  {
    "id": "arXiv:2008.03718",
    "title": "1-Point RANSAC-Based Method for Ground Object Pose Estimation",
    "abstract": "Comments: Accepted in the workshop on Autonomous Driving: Perception, Prediction and Planning in conjunction with CVPR 2021",
    "descriptor": "\nComments: Accepted in the workshop on Autonomous Driving: Perception, Prediction and Planning in conjunction with CVPR 2021\n",
    "authors": [
      "Jeong-Kyun Lee",
      "Young-Ki Baik",
      "Hankyu Cho",
      "Kang Kim",
      "Duck Hoon Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2008.03718"
  },
  {
    "id": "arXiv:2008.05742",
    "title": "SkeletonNet: A Topology-Preserving Solution for Learning Mesh  Reconstruction of Object Surfaces from RGB Images",
    "abstract": "Comments: 17 pages, 13 figures; TPAMI 2021",
    "descriptor": "\nComments: 17 pages, 13 figures; TPAMI 2021\n",
    "authors": [
      "Jiapeng Tang",
      "Xiaoguang Han",
      "Mingkui Tan",
      "Xin Tong",
      "Kui Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2008.05742"
  },
  {
    "id": "arXiv:2008.09093",
    "title": "PARADE: Passage Representation Aggregation for Document Reranking",
    "abstract": "PARADE: Passage Representation Aggregation for Document Reranking",
    "descriptor": "",
    "authors": [
      "Canjia Li",
      "Andrew Yates",
      "Sean MacAvaney",
      "Ben He",
      "Yingfei Sun"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2008.09093"
  },
  {
    "id": "arXiv:2008.09753",
    "title": "Unsupervised Hyperspectral Mixed Noise Removal Via Spatial-Spectral  Constrained Deep Image Prior",
    "abstract": "Unsupervised Hyperspectral Mixed Noise Removal Via Spatial-Spectral  Constrained Deep Image Prior",
    "descriptor": "",
    "authors": [
      "Yi-Si Luo",
      "Xi-Le Zhao",
      "Tai-Xiang Jiang",
      "Yu-Bang Zheng",
      "Yi Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2008.09753"
  },
  {
    "id": "arXiv:2009.06847",
    "title": "Toward Deep Supervised Anomaly Detection: Reinforcement Learning from  Partially Labeled Anomaly Data",
    "abstract": "Comments: Accepted to KDD 2021",
    "descriptor": "\nComments: Accepted to KDD 2021\n",
    "authors": [
      "Guansong Pang",
      "Anton van den Hengel",
      "Chunhua Shen",
      "Longbing Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.06847"
  },
  {
    "id": "arXiv:2009.07601",
    "title": "Efficient Quantum State Sample Tomography with Basis-dependent  Neural-networks",
    "abstract": "Efficient Quantum State Sample Tomography with Basis-dependent  Neural-networks",
    "descriptor": "",
    "authors": [
      "Alistair W. R. Smith",
      "Johnnie Gray",
      "M. S. Kim"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.07601"
  },
  {
    "id": "arXiv:2009.08435",
    "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness",
    "abstract": "Comments: 15 pages, 4 figures; v5: corrected typo",
    "descriptor": "\nComments: 15 pages, 4 figures; v5: corrected typo\n",
    "authors": [
      "Youwei Liang",
      "Dong Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.08435"
  },
  {
    "id": "arXiv:2009.10513",
    "title": "Local Post-Hoc Explanations for Predictive Process Monitoring in  Manufacturing",
    "abstract": "Comments: Accepted for publication in ECIS-2021 Proceedings (initial submission November 18, 2020). This version is an extension of the previous arXiv version",
    "descriptor": "\nComments: Accepted for publication in ECIS-2021 Proceedings (initial submission November 18, 2020). This version is an extension of the previous arXiv version\n",
    "authors": [
      "Nijat Mehdiyev",
      "Peter Fettke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.10513"
  },
  {
    "id": "arXiv:2009.14061",
    "title": "GraphITE: Estimating Individual Effects of Graph-structured Treatments",
    "abstract": "GraphITE: Estimating Individual Effects of Graph-structured Treatments",
    "descriptor": "",
    "authors": [
      "Shonosuke Harada",
      "Hisashi Kashima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.14061"
  },
  {
    "id": "arXiv:2010.01062",
    "title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement  Learning",
    "abstract": "Comments: Published at the International Conference on Machine Learning (ICML) 2021",
    "descriptor": "\nComments: Published at the International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Luisa Zintgraf",
      "Leo Feng",
      "Cong Lu",
      "Maximilian Igl",
      "Kristian Hartikainen",
      "Katja Hofmann",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.01062"
  },
  {
    "id": "arXiv:2010.02974",
    "title": "UneVEn: Universal Value Exploration for Multi-Agent Reinforcement  Learning",
    "abstract": "Comments: Published at ICML 2021",
    "descriptor": "\nComments: Published at ICML 2021\n",
    "authors": [
      "Tarun Gupta",
      "Anuj Mahajan",
      "Bei Peng",
      "Wendelin B\u00f6hmer",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2010.02974"
  },
  {
    "id": "arXiv:2010.10223",
    "title": "Generators and bases for algebras over a monad",
    "abstract": "Generators and bases for algebras over a monad",
    "descriptor": "",
    "authors": [
      "Stefan Zetzsche",
      "Alexandra Silva",
      "Matteo Sammartino"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2010.10223"
  },
  {
    "id": "arXiv:2010.13105",
    "title": "Two-stage Textual Knowledge Distillation for End-to-End Spoken Language  Understanding",
    "abstract": "Comments: ICASSP 2021; 5 pages, 1 figure",
    "descriptor": "\nComments: ICASSP 2021; 5 pages, 1 figure\n",
    "authors": [
      "Seongbin Kim",
      "Gyuwan Kim",
      "Seongjin Shin",
      "Sangmin Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2010.13105"
  },
  {
    "id": "arXiv:2010.14535",
    "title": "Neural Architecture Search of SPD Manifold Networks",
    "abstract": "Comments: This paper is accepted for publication at IJCAI 2021",
    "descriptor": "\nComments: This paper is accepted for publication at IJCAI 2021\n",
    "authors": [
      "Rhea Sanjay Sukthanker",
      "Zhiwu Huang",
      "Suryansh Kumar",
      "Erik Goron Endsjo",
      "Yan Wu",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2010.14535"
  },
  {
    "id": "arXiv:2010.15479",
    "title": "Learned infinite elements",
    "abstract": "Comments: 33 pages, 14 figures",
    "descriptor": "\nComments: 33 pages, 14 figures\n",
    "authors": [
      "Thorsten Hohage",
      "Christoph Lehrenfeld",
      "Janosch Preuss"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2010.15479"
  },
  {
    "id": "arXiv:2011.00355",
    "title": "Linear Classifiers that Encourage Constructive Adaptation",
    "abstract": "Linear Classifiers that Encourage Constructive Adaptation",
    "descriptor": "",
    "authors": [
      "Yatong Chen",
      "Jialu Wang",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.00355"
  },
  {
    "id": "arXiv:2011.01385",
    "title": "Dual Attention on Pyramid Feature Maps for Image Captioning",
    "abstract": "Comments: in IEEE Transactions on Multimedia, 2021",
    "descriptor": "\nComments: in IEEE Transactions on Multimedia, 2021\n",
    "authors": [
      "Litao Yu",
      "Jian Zhang",
      "Qiang Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.01385"
  },
  {
    "id": "arXiv:2011.01408",
    "title": "Hybrid Visual Servoing Tracking Control of Uncalibrated Robotic Systems  for Dynamic Dwarf Culture Orchards Harvest",
    "abstract": "Comments: 6 pages,15 figures, accepted by IEEE ICDL2021",
    "descriptor": "\nComments: 6 pages,15 figures, accepted by IEEE ICDL2021\n",
    "authors": [
      "Tao Li",
      "Quan Qiu",
      "Chunjiang Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.01408"
  },
  {
    "id": "arXiv:2011.04167",
    "title": "Distributed Optimal Conservation Voltage Reduction in Integrated  Primary-Secondary Distribution Systems",
    "abstract": "Comments: Accepted by IEEE Transactions on Smart Grid",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Smart Grid\n",
    "authors": [
      "Qianzhi Zhang",
      "Yifei Guo",
      "Zhaoyu Wang",
      "Fankun Bu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.04167"
  },
  {
    "id": "arXiv:2011.04609",
    "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Jacob Peplinski",
      "Joel Shor",
      "Sachin Joglekar",
      "Jake Garrison",
      "Shwetak Patel"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2011.04609"
  },
  {
    "id": "arXiv:2011.04611",
    "title": "On the hardness of code equivalence problems in rank metric",
    "abstract": "On the hardness of code equivalence problems in rank metric",
    "descriptor": "",
    "authors": [
      "Alain Couvreur",
      "Thomas Debris-Alazard",
      "Philippe Gaborit"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computational Geometry (cs.CG)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2011.04611"
  },
  {
    "id": "arXiv:2011.05530",
    "title": "On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU  Networks",
    "abstract": "On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU  Networks",
    "descriptor": "",
    "authors": [
      "Ramy E. Ali",
      "Jinhyun So",
      "A. Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2011.05530"
  },
  {
    "id": "arXiv:2011.05884",
    "title": "Efficient List-Decoding with Constant Alphabet and List Sizes",
    "abstract": "Efficient List-Decoding with Constant Alphabet and List Sizes",
    "descriptor": "",
    "authors": [
      "Zeyu Guo",
      "Noga Ron-Zewi"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2011.05884"
  },
  {
    "id": "arXiv:2011.07018",
    "title": "Synthetic Data -- Anonymisation Groundhog Day",
    "abstract": "Synthetic Data -- Anonymisation Groundhog Day",
    "descriptor": "",
    "authors": [
      "Theresa Stadler",
      "Bristena Oprisanu",
      "Carmela Troncoso"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2011.07018"
  },
  {
    "id": "arXiv:2011.09625",
    "title": "Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal  Clinical NLP",
    "abstract": "Comments: Best paper award at 3rd Clinical Natural Language Processing Workshop at EMNLP 2020",
    "descriptor": "\nComments: Best paper award at 3rd Clinical Natural Language Processing Workshop at EMNLP 2020\n",
    "authors": [
      "John Chen",
      "Ian Berlot-Attwell",
      "Safwan Hossain",
      "Xindi Wang",
      "Frank Rudzicz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.09625"
  },
  {
    "id": "arXiv:2011.12428",
    "title": "Align, then memorise: the dynamics of learning with feedback alignment",
    "abstract": "Comments: The accompanying code for this paper is available at this https URL",
    "descriptor": "\nComments: The accompanying code for this paper is available at this https URL\n",
    "authors": [
      "Maria Refinetti",
      "St\u00e9phane d'Ascoli",
      "Ruben Ohana",
      "Sebastian Goldt"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2011.12428"
  },
  {
    "id": "arXiv:2012.01276",
    "title": "Leveraging Unknown Structure in Quantum Query Algorithms",
    "abstract": "Comments: 19 pages, v2: organization improved, typos fixed, function evaluation error bound improved",
    "descriptor": "\nComments: 19 pages, v2: organization improved, typos fixed, function evaluation error bound improved\n",
    "authors": [
      "Noel T. Anderson",
      "Jay-U Chung",
      "Shelby Kimmel"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2012.01276"
  },
  {
    "id": "arXiv:2012.01744",
    "title": "Sample-Efficient L0-L2 Constrained Structure Learning of Sparse Ising  Models",
    "abstract": "Sample-Efficient L0-L2 Constrained Structure Learning of Sparse Ising  Models",
    "descriptor": "",
    "authors": [
      "Antoine Dedieu",
      "Miguel L\u00e1zaro-Gredilla",
      "Dileep George"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01744"
  },
  {
    "id": "arXiv:2012.02177",
    "title": "DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal  Fusion",
    "abstract": "Comments: CVPR 2021",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Arda D\u00fcz\u00e7eker",
      "Silvano Galliani",
      "Christoph Vogel",
      "Pablo Speciale",
      "Mihai Dusmanu",
      "Marc Pollefeys"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.02177"
  },
  {
    "id": "arXiv:2012.04035",
    "title": "ATOM3D: Tasks On Molecules in Three Dimensions",
    "abstract": "ATOM3D: Tasks On Molecules in Three Dimensions",
    "descriptor": "",
    "authors": [
      "Raphael J.L. Townshend",
      "Martin V\u00f6gele",
      "Patricia Suriana",
      "Alexander Derry",
      "Alexander Powers",
      "Yianni Laloudakis",
      "Sidhika Balachandar",
      "Bowen Jing",
      "Brandon Anderson",
      "Stephan Eismann",
      "Risi Kondor",
      "Russ B. Altman",
      "Ron O. Dror"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biological Physics (physics.bio-ph)",
      "Computational Physics (physics.comp-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2012.04035"
  },
  {
    "id": "arXiv:2012.05329",
    "title": "Know Your Limits: Uncertainty Estimation with ReLU Classifiers Fails at  Reliable OOD Detection",
    "abstract": "Know Your Limits: Uncertainty Estimation with ReLU Classifiers Fails at  Reliable OOD Detection",
    "descriptor": "",
    "authors": [
      "Dennis Ulmer",
      "Giovanni Cin\u00e0"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.05329"
  },
  {
    "id": "arXiv:2012.08681",
    "title": "Manufactured Solutions for the Method-of-Moments Implementation of the  Electric-Field Integral Equation",
    "abstract": "Manufactured Solutions for the Method-of-Moments Implementation of the  Electric-Field Integral Equation",
    "descriptor": "",
    "authors": [
      "Brian A. Freno",
      "Neil R. Matula",
      "William A. Johnson"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2012.08681"
  },
  {
    "id": "arXiv:2012.10018",
    "title": "NeurST: Neural Speech Translation Toolkit",
    "abstract": "Comments: Accepted by ACL 2021 (system demonstration)",
    "descriptor": "\nComments: Accepted by ACL 2021 (system demonstration)\n",
    "authors": [
      "Chengqi Zhao",
      "Mingxuan Wang",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2012.10018"
  },
  {
    "id": "arXiv:2012.12102",
    "title": "Using Persistent Homology Topological Features to Characterize Medical  Images: Case Studies on Lung and Brain Cancers",
    "abstract": "Using Persistent Homology Topological Features to Characterize Medical  Images: Case Studies on Lung and Brain Cancers",
    "descriptor": "",
    "authors": [
      "Chul Moon",
      "Qiwei Li",
      "Guanghua Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2012.12102"
  },
  {
    "id": "arXiv:2012.13169",
    "title": "SCC: an efficient deep reinforcement learning agent mastering the game  of StarCraft II",
    "abstract": "Comments: ICML 2021 camera ready",
    "descriptor": "\nComments: ICML 2021 camera ready\n",
    "authors": [
      "Xiangjun Wang",
      "Junxiao Song",
      "Penghui Qi",
      "Peng Peng",
      "Zhenkun Tang",
      "Wei Zhang",
      "Weimin Li",
      "Xiongjun Pi",
      "Jujie He",
      "Chao Gao",
      "Haitao Long",
      "Quan Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.13169"
  },
  {
    "id": "arXiv:2012.14313",
    "title": "How to Train Your Differentiable Filter",
    "abstract": "Comments: Autonomous Robots (2021)",
    "descriptor": "\nComments: Autonomous Robots (2021)\n",
    "authors": [
      "Alina Kloss",
      "Georg Martius",
      "Jeannette Bohg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2012.14313"
  },
  {
    "id": "arXiv:2101.00259",
    "title": "Code Generation from Natural Language with Less Prior and More  Monolingual Data",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Sajad Norouzi",
      "Keyi Tang",
      "Yanshuai Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.00259"
  },
  {
    "id": "arXiv:2101.03654",
    "title": "Disentangled Self-Attentive Neural Networks for Click-Through Rate  Prediction",
    "abstract": "Comments: 5 pages, work in progress",
    "descriptor": "\nComments: 5 pages, work in progress\n",
    "authors": [
      "Yichen Xu",
      "Yanqiao Zhu",
      "Feng Yu",
      "Qiang Liu",
      "Shu Wu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2101.03654"
  },
  {
    "id": "arXiv:2101.04459",
    "title": "The Hidden Cost of Using Amazon Mechanical Turk for Research",
    "abstract": "Comments: 19 pages",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Antonios Saravanos",
      "Stavros Zervoudakis",
      "Dongnanzi Zheng",
      "Neil Stott",
      "Bohdan Hawryluk",
      "Donatella Delfino"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2101.04459"
  },
  {
    "id": "arXiv:2101.05186",
    "title": "MC-LSTM: Mass-Conserving LSTM",
    "abstract": "Comments: 13 pages (8.5 without references) + 17 pages appendix",
    "descriptor": "\nComments: 13 pages (8.5 without references) + 17 pages appendix\n",
    "authors": [
      "Pieter-Jan Hoedt",
      "Frederik Kratzert",
      "Daniel Klotz",
      "Christina Halmich",
      "Markus Holzleitner",
      "Grey Nearing",
      "Sepp Hochreiter",
      "G\u00fcnter Klambauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.05186"
  },
  {
    "id": "arXiv:2101.06512",
    "title": "A Two-Level Simulation-Assisted Sequential Distribution System  Restoration Model With Frequency Dynamics Constraints",
    "abstract": "Comments: Accepted by IEEE Transactions on Smart Grid",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Smart Grid\n",
    "authors": [
      "Qianzhi Zhang",
      "Zixiao Ma",
      "Yongli Zhu",
      "Zhaoyu Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2101.06512"
  },
  {
    "id": "arXiv:2101.07233",
    "title": "Fully Dynamic Electrical Flows: Sparse Maxflow Faster Than Goldberg-Rao",
    "abstract": "Comments: 78 pages, v2. Fixes an issue relating to handling of adaptivity and randomness -- we thank Aaron Sidford for discussions during which this error was pointed out",
    "descriptor": "\nComments: 78 pages, v2. Fixes an issue relating to handling of adaptivity and randomness -- we thank Aaron Sidford for discussions during which this error was pointed out\n",
    "authors": [
      "Yu Gao",
      "Yang P. Liu",
      "Richard Peng"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2101.07233"
  },
  {
    "id": "arXiv:2101.07597",
    "title": "UniSpeech: Unified Speech Representation Learning with Labeled and  Unlabeled Data",
    "abstract": "Comments: accepted by ICML2021",
    "descriptor": "\nComments: accepted by ICML2021\n",
    "authors": [
      "Chengyi Wang",
      "Yu Wu",
      "Yao Qian",
      "Kenichi Kumatani",
      "Shujie Liu",
      "Furu Wei",
      "Michael Zeng",
      "Xuedong Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2101.07597"
  },
  {
    "id": "arXiv:2101.08734",
    "title": "Clairvoyant Prefetching for Distributed Machine Learning I/O",
    "abstract": "Comments: 13 pages, 16 figures; major revisions",
    "descriptor": "\nComments: 13 pages, 16 figures; major revisions\n",
    "authors": [
      "Nikoli Dryden",
      "Roman B\u00f6hringer",
      "Tal Ben-Nun",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.08734"
  },
  {
    "id": "arXiv:2101.10803",
    "title": "Automatic Curation of Large-Scale Datasets for Audio-Visual  Representation Learning",
    "abstract": "Automatic Curation of Large-Scale Datasets for Audio-Visual  Representation Learning",
    "descriptor": "",
    "authors": [
      "Sangho Lee",
      "Jiwan Chung",
      "Youngjae Yu",
      "Gunhee Kim",
      "Thomas Breuel",
      "Gal Chechik",
      "Yale Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.10803"
  },
  {
    "id": "arXiv:2102.01644",
    "title": "Zero-cost meta-programmed stateful functors in F*",
    "abstract": "Zero-cost meta-programmed stateful functors in F*",
    "descriptor": "",
    "authors": [
      "Jonathan Protzenko",
      "Son Ho"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2102.01644"
  },
  {
    "id": "arXiv:2102.01738",
    "title": "Noise and the frontier of quantum supremacy",
    "abstract": "Comments: 43 pages, 2 figures, presented at QIP 2021",
    "descriptor": "\nComments: 43 pages, 2 figures, presented at QIP 2021\n",
    "authors": [
      "Adam Bouland",
      "Bill Fefferman",
      "Zeph Landau",
      "Yunchao Liu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2102.01738"
  },
  {
    "id": "arXiv:2102.02649",
    "title": "A step towards a reinforcement learning de novo genome assembler",
    "abstract": "A step towards a reinforcement learning de novo genome assembler",
    "descriptor": "",
    "authors": [
      "Kleber Padovani",
      "Roberto Xavier",
      "Andre Carvalho",
      "Anna Reali",
      "Annie Chateau",
      "Ronnie Alves"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.02649"
  },
  {
    "id": "arXiv:2102.03310",
    "title": "Improving state estimation through projection post-processing for  activity recognition in football",
    "abstract": "Comments: 22 pages, 6 figures",
    "descriptor": "\nComments: 22 pages, 6 figures\n",
    "authors": [
      "Micha\u0142 Ciszewski",
      "Jakob S\u00f6hl",
      "Geurt Jongbloed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.03310"
  },
  {
    "id": "arXiv:2102.03334",
    "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region  Supervision",
    "abstract": "Comments: ICML 2021 Long Presentation",
    "descriptor": "\nComments: ICML 2021 Long Presentation\n",
    "authors": [
      "Wonjae Kim",
      "Bokyung Son",
      "Ildoo Kim"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.03334"
  },
  {
    "id": "arXiv:2102.03785",
    "title": "Robust Explanations for Private Support Vector Machines",
    "abstract": "Comments: 13 pages, 9 figures, 1 table",
    "descriptor": "\nComments: 13 pages, 9 figures, 1 table\n",
    "authors": [
      "Rami Mochaourab",
      "Sugandh Sinha",
      "Stanley Greenstein",
      "Panagiotis Papapetrou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.03785"
  },
  {
    "id": "arXiv:2102.05020",
    "title": "Predictive Factors of Kinematics in Traumatic Brain Injury from Head  Impacts Based on Statistical Interpretation",
    "abstract": "Predictive Factors of Kinematics in Traumatic Brain Injury from Head  Impacts Based on Statistical Interpretation",
    "descriptor": "",
    "authors": [
      "Xianghao Zhan",
      "Yiheng Li",
      "Yuzhe Liu",
      "August G. Domel",
      "Hossein Vahid Alizadeh",
      "Zhou Zhou",
      "Nicholas J. Cecchi",
      "Samuel J. Raymond",
      "Stephen Tiernan",
      "Jesse Ruan",
      "Saeed Barbat",
      "Olivier Gevaert",
      "Michael M. Zeineh",
      "Gerald A. Grant",
      "David B. Camarillo"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2102.05020"
  },
  {
    "id": "arXiv:2102.05749",
    "title": "Self-Supervised VQ-VAE for One-Shot Music Style Transfer",
    "abstract": "Comments: ICASSP 2021. Website: this https URL",
    "descriptor": "\nComments: ICASSP 2021. Website: this https URL\n",
    "authors": [
      "Ond\u0159ej C\u00edfka",
      "Alexey Ozerov",
      "Umut \u015eim\u015fekli",
      "Ga\u00ebl Richard"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.05749"
  },
  {
    "id": "arXiv:2102.05855",
    "title": "Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient  Descent",
    "abstract": "Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient  Descent",
    "descriptor": "",
    "authors": [
      "Rishav Chourasia",
      "Jiayuan Ye",
      "Reza Shokri"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05855"
  },
  {
    "id": "arXiv:2102.06356",
    "title": "A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers  Suffice Across Batch Sizes",
    "abstract": "A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers  Suffice Across Batch Sizes",
    "descriptor": "",
    "authors": [
      "Zachary Nado",
      "Justin M. Gilmer",
      "Christopher J. Shallue",
      "Rohan Anil",
      "George E. Dahl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.06356"
  },
  {
    "id": "arXiv:2102.06489",
    "title": "Stability and Convergence of Stochastic Gradient Clipping: Beyond  Lipschitz Continuity and Smoothness",
    "abstract": "Comments: ICML-2021",
    "descriptor": "\nComments: ICML-2021\n",
    "authors": [
      "Vien V. Mai",
      "Mikael Johansson"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06489"
  },
  {
    "id": "arXiv:2102.06645",
    "title": "Bayesian Quadrature on Riemannian Data Manifolds",
    "abstract": "Bayesian Quadrature on Riemannian Data Manifolds",
    "descriptor": "",
    "authors": [
      "Christian Fr\u00f6hlich",
      "Alexandra Gessner",
      "Philipp Hennig",
      "Bernhard Sch\u00f6lkopf",
      "Georgios Arvanitidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.06645"
  },
  {
    "id": "arXiv:2102.06700",
    "title": "Certified Defenses: Why Tighter Relaxations May Hurt Training",
    "abstract": "Certified Defenses: Why Tighter Relaxations May Hurt Training",
    "descriptor": "",
    "authors": [
      "Nikola Jovanovi\u0107",
      "Mislav Balunovi\u0107",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.06700"
  },
  {
    "id": "arXiv:2102.07188",
    "title": "Think Global and Act Local: Bayesian Optimisation over High-Dimensional  Categorical and Mixed Search Spaces",
    "abstract": "Comments: ICML 2021. 9 page, 6 figures (26 pages, 16 figures, 2 tables including references and appendices)",
    "descriptor": "\nComments: ICML 2021. 9 page, 6 figures (26 pages, 16 figures, 2 tables including references and appendices)\n",
    "authors": [
      "Xingchen Wan",
      "Vu Nguyen",
      "Huong Ha",
      "Binxin Ru",
      "Cong Lu",
      "Michael A. Osborne"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07188"
  },
  {
    "id": "arXiv:2102.07945",
    "title": "Local Hyper-Flow Diffusion",
    "abstract": "Comments: 47 pages, 10 figures, 12 tables",
    "descriptor": "\nComments: 47 pages, 10 figures, 12 tables\n",
    "authors": [
      "Kimon Fountoulakis",
      "Pan Li",
      "Shenghao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.07945"
  },
  {
    "id": "arXiv:2102.07954",
    "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence",
    "abstract": "Comments: International Conference on Machine Learning (ICML) 2021",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Dilin Wang",
      "Chengyue Gong",
      "Meng Li",
      "Qiang Liu",
      "Vikas Chandra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.07954"
  },
  {
    "id": "arXiv:2102.08208",
    "title": "Conditional Distributional Treatment Effect with Kernel Conditional Mean  Embeddings and U-Statistic Regression",
    "abstract": "Conditional Distributional Treatment Effect with Kernel Conditional Mean  Embeddings and U-Statistic Regression",
    "descriptor": "",
    "authors": [
      "Junhyung Park",
      "Uri Shalit",
      "Bernhard Sch\u00f6lkopf",
      "Krikamol Muandet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.08208"
  },
  {
    "id": "arXiv:2102.08248",
    "title": "Hierarchical VAEs Know What They Don't Know",
    "abstract": "Comments: Appeared in Proceedings of the 38th International Conference on Machine Learning (ICML 2021). 18 pages, source code available at this https URL, this https URL and this https URL",
    "descriptor": "\nComments: Appeared in Proceedings of the 38th International Conference on Machine Learning (ICML 2021). 18 pages, source code available at this https URL, this https URL and this https URL\n",
    "authors": [
      "Jakob D. Havtorn",
      "Jes Frellsen",
      "S\u00f8ren Hauberg",
      "Lars Maal\u00f8e"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.08248"
  },
  {
    "id": "arXiv:2102.08259",
    "title": "Dataset Condensation with Differentiable Siamese Augmentation",
    "abstract": "Dataset Condensation with Differentiable Siamese Augmentation",
    "descriptor": "",
    "authors": [
      "Bo Zhao",
      "Hakan Bilen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.08259"
  },
  {
    "id": "arXiv:2102.09299",
    "title": "Theory meets Practice at the Median: a worst case comparison of relative  error quantile algorithms",
    "abstract": "Comments: Updated experiments, improved presentation. To appear in KDD 2021",
    "descriptor": "\nComments: Updated experiments, improved presentation. To appear in KDD 2021\n",
    "authors": [
      "Graham Cormode",
      "Abhinav Mishra",
      "Joseph Ross",
      "Pavel Vesel\u00fd"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2102.09299"
  },
  {
    "id": "arXiv:2102.09430",
    "title": "State Entropy Maximization with Random Encoders for Efficient  Exploration",
    "abstract": "Comments: ICML 2021. First two authors contributed equally. Website: this https URL Code: this https URL",
    "descriptor": "\nComments: ICML 2021. First two authors contributed equally. Website: this https URL Code: this https URL\n",
    "authors": [
      "Younggyo Seo",
      "Lili Chen",
      "Jinwoo Shin",
      "Honglak Lee",
      "Pieter Abbeel",
      "Kimin Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09430"
  },
  {
    "id": "arXiv:2102.10274",
    "title": "Concealed Object Detection",
    "abstract": "Comments: 17 pages, 27 figures, Code: this https URL",
    "descriptor": "\nComments: 17 pages, 27 figures, Code: this https URL\n",
    "authors": [
      "Deng-Ping Fan",
      "Ge-Peng Ji",
      "Ming-Ming Cheng",
      "Ling Shao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.10274"
  },
  {
    "id": "arXiv:2102.10424",
    "title": "GIST: Distributed Training for Large-Scale Graph Convolutional Networks",
    "abstract": "Comments: 18 pages, 4 figures, pre-print under review",
    "descriptor": "\nComments: 18 pages, 4 figures, pre-print under review\n",
    "authors": [
      "Cameron R. Wolfe",
      "Jingkang Yang",
      "Arindam Chowdhury",
      "Chen Dun",
      "Artun Bayer",
      "Santiago Segarra",
      "Anastasios Kyrillidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.10424"
  },
  {
    "id": "arXiv:2102.10898",
    "title": "Adaptive Video Configuration and Bitrate Allocation for Teleoperated  Vehicles",
    "abstract": "Comments: Accepted at workshop for Road Vehicle Teleoperation (WS09) at the 2021 IEEE Intelligent Vehicles Symposium (IV21)",
    "descriptor": "\nComments: Accepted at workshop for Road Vehicle Teleoperation (WS09) at the 2021 IEEE Intelligent Vehicles Symposium (IV21)\n",
    "authors": [
      "Andreas Schimpe",
      "Simon Hoffmann",
      "Frank Diermeyer"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2102.10898"
  },
  {
    "id": "arXiv:2102.10992",
    "title": "Word frequency-rank relationship in tagged texts",
    "abstract": "Word frequency-rank relationship in tagged texts",
    "descriptor": "",
    "authors": [
      "A. Chacoma",
      "D. H. Zanette"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.10992"
  },
  {
    "id": "arXiv:2102.11375",
    "title": "Remote Renewable Hubs For Carbon-Neutral Synthetic Fuel Production",
    "abstract": "Remote Renewable Hubs For Carbon-Neutral Synthetic Fuel Production",
    "descriptor": "",
    "authors": [
      "Mathias Berger",
      "David Radu",
      "Ghislain Detienne",
      "Thierry Deschuyteneer",
      "Aurore Richel",
      "Damien Ernst"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2102.11375"
  },
  {
    "id": "arXiv:2102.11533",
    "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling",
    "abstract": "Comments: ICLR 2021",
    "descriptor": "\nComments: ICLR 2021\n",
    "authors": [
      "Jinheon Baek",
      "Minki Kang",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11533"
  },
  {
    "id": "arXiv:2102.11636",
    "title": "Using a deep neural network to predict the motion of under-resolved  triangular rigid bodies in an incompressible flow",
    "abstract": "Using a deep neural network to predict the motion of under-resolved  triangular rigid bodies in an incompressible flow",
    "descriptor": "",
    "authors": [
      "Henry von Wahl",
      "Thomas Richter"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2102.11636"
  },
  {
    "id": "arXiv:2102.11742",
    "title": "Classifying high-dimensional Gaussian mixtures: Where kernel methods  fail and neural networks succeed",
    "abstract": "Comments: The accompanying code for this paper is available at this https URL",
    "descriptor": "\nComments: The accompanying code for this paper is available at this https URL\n",
    "authors": [
      "Maria Refinetti",
      "Sebastian Goldt",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11742"
  },
  {
    "id": "arXiv:2102.12560",
    "title": "PsiPhi-Learning: Reinforcement Learning with Demonstrations using  Successor Features and Inverse Temporal Difference Learning",
    "abstract": "Comments: The last two authors contributed equally. Accepted at ICML 2021",
    "descriptor": "\nComments: The last two authors contributed equally. Accepted at ICML 2021\n",
    "authors": [
      "Angelos Filos",
      "Clare Lyle",
      "Yarin Gal",
      "Sergey Levine",
      "Natasha Jaques",
      "Gregory Farquhar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.12560"
  },
  {
    "id": "arXiv:2103.00349",
    "title": "High-Dimensional Bayesian Optimization with Sparse Axis-Aligned  Subspaces",
    "abstract": "Comments: To appear in UAI 2021",
    "descriptor": "\nComments: To appear in UAI 2021\n",
    "authors": [
      "David Eriksson",
      "Martin Jankowiak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.00349"
  },
  {
    "id": "arXiv:2103.02357",
    "title": "Reinforcement Learning for Orientation Estimation Using Inertial Sensors  with Performance Guarantee",
    "abstract": "Comments: This paper has been accepted by ICRA 2021",
    "descriptor": "\nComments: This paper has been accepted by ICRA 2021\n",
    "authors": [
      "Liang Hu",
      "Yujie Tang",
      "Zhipeng Zhou",
      "Wei Pan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.02357"
  },
  {
    "id": "arXiv:2103.02695",
    "title": "Shift Invariance Can Reduce Adversarial Robustness",
    "abstract": "Shift Invariance Can Reduce Adversarial Robustness",
    "descriptor": "",
    "authors": [
      "Songwei Ge",
      "Vasu Singla",
      "Ronen Basri",
      "David Jacobs"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.02695"
  },
  {
    "id": "arXiv:2103.03240",
    "title": "Learn your ABCs: Approximate Bijective Correspondence for isolating  factors of variation",
    "abstract": "Learn your ABCs: Approximate Bijective Correspondence for isolating  factors of variation",
    "descriptor": "",
    "authors": [
      "Kieran A. Murphy",
      "Varun Jampani",
      "Srikumar Ramalingam",
      "Ameesh Makadia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.03240"
  },
  {
    "id": "arXiv:2103.03545",
    "title": "A modified discrepancy principle to attain optimal convergence rates  under unknown noise",
    "abstract": "A modified discrepancy principle to attain optimal convergence rates  under unknown noise",
    "descriptor": "",
    "authors": [
      "Tim Jahn"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.03545"
  },
  {
    "id": "arXiv:2103.03931",
    "title": "Attention-Enhanced Cross-Task Network for Analysing Multiple Attributes  of Lung Nodules in CT",
    "abstract": "Attention-Enhanced Cross-Task Network for Analysing Multiple Attributes  of Lung Nodules in CT",
    "descriptor": "",
    "authors": [
      "Xiaohang Fu",
      "Lei Bi",
      "Ashnil Kumar",
      "Michael Fulham",
      "Jinman Kim"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.03931"
  },
  {
    "id": "arXiv:2103.04847",
    "title": "Adversarial Reinforcement Learning for Procedural Content Generation",
    "abstract": "Comments: 8 pages, 6 figures (11 subfigures)",
    "descriptor": "\nComments: 8 pages, 6 figures (11 subfigures)\n",
    "authors": [
      "Linus Gissl\u00e9n",
      "Andy Eakins",
      "Camilo Gordillo",
      "Joakim Bergdahl",
      "Konrad Tollmar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.04847"
  },
  {
    "id": "arXiv:2103.05487",
    "title": "UnICORNN: A recurrent model for learning very long time dependencies",
    "abstract": "UnICORNN: A recurrent model for learning very long time dependencies",
    "descriptor": "",
    "authors": [
      "T. Konstantin Rusch",
      "Siddhartha Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.05487"
  },
  {
    "id": "arXiv:2103.07877",
    "title": "R-GSN: The Relation-based Graph Similar Network for Heterogeneous Graph",
    "abstract": "R-GSN: The Relation-based Graph Similar Network for Heterogeneous Graph",
    "descriptor": "",
    "authors": [
      "Xinliang Wu",
      "Mengying Jiang",
      "Guizhong Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.07877"
  },
  {
    "id": "arXiv:2103.07927",
    "title": "Modelling Behavioural Diversity for Learning in Open-Ended Games",
    "abstract": "Comments: corresponds to &lt;yaodong.yang@cs.ucl.ac.uk&gt;",
    "descriptor": "\nComments: corresponds to &lt;yaodong.yang@cs.ucl.ac.uk&gt;\n",
    "authors": [
      "Nicolas Perez Nieves",
      "Yaodong Yang",
      "Oliver Slumbers",
      "David Henry Mguni",
      "Ying Wen",
      "Jun Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2103.07927"
  },
  {
    "id": "arXiv:2103.08233",
    "title": "Robust MAML: Prioritization task buffer with adaptive learning process  for model-agnostic meta-learning",
    "abstract": "Robust MAML: Prioritization task buffer with adaptive learning process  for model-agnostic meta-learning",
    "descriptor": "",
    "authors": [
      "Thanh Nguyen",
      "Tung Luu",
      "Trung Pham",
      "Sanzhar Rakhimkul",
      "Chang D. Yoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.08233"
  },
  {
    "id": "arXiv:2103.10697",
    "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive  Biases",
    "abstract": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive  Biases",
    "descriptor": "",
    "authors": [
      "St\u00e9phane d'Ascoli",
      "Hugo Touvron",
      "Matthew Leavitt",
      "Ari Morcos",
      "Giulio Biroli",
      "Levent Sagun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.10697"
  },
  {
    "id": "arXiv:2103.11379",
    "title": "A variational interpretation of Restricted Additive Schwarz with  impedance transmission condition for the Helmholtz problem",
    "abstract": "A variational interpretation of Restricted Additive Schwarz with  impedance transmission condition for the Helmholtz problem",
    "descriptor": "",
    "authors": [
      "Shihua Gong",
      "Martin J. Gander",
      "Ivan G. Graham",
      "Euan A. Spence"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.11379"
  },
  {
    "id": "arXiv:2103.12692",
    "title": "Benign Overfitting of Constant-Stepsize SGD for Linear Regression",
    "abstract": "Comments: 53 pages. This version provides improved upper bound results",
    "descriptor": "\nComments: 53 pages. This version provides improved upper bound results\n",
    "authors": [
      "Difan Zou",
      "Jingfeng Wu",
      "Vladimir Braverman",
      "Quanquan Gu",
      "Sham M. Kakade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.12692"
  },
  {
    "id": "arXiv:2103.14651",
    "title": "Local Explanations via Necessity and Sufficiency: Unifying Theory and  Practice",
    "abstract": "Local Explanations via Necessity and Sufficiency: Unifying Theory and  Practice",
    "descriptor": "",
    "authors": [
      "David Watson",
      "Limor Gultchin",
      "Ankur Taly",
      "Luciano Floridi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.14651"
  },
  {
    "id": "arXiv:2104.01393",
    "title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR",
    "abstract": "Comments: Accepted at INTERSPEECH 2021",
    "descriptor": "\nComments: Accepted at INTERSPEECH 2021\n",
    "authors": [
      "Tsz Kin Lam",
      "Mayumi Ohta",
      "Shigehiko Schamoni",
      "Stefan Riezler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.01393"
  },
  {
    "id": "arXiv:2104.03044",
    "title": "A First Look into the Structural Properties and Resilience of Blockchain  Overlays",
    "abstract": "Comments: 23 pages, 8 figures, 6 tables, submitted to ACM IMC 2021",
    "descriptor": "\nComments: 23 pages, 8 figures, 6 tables, submitted to ACM IMC 2021\n",
    "authors": [
      "Aristodemos Paphitis",
      "Nicolas Kourtellis",
      "Michael Sirivianos"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2104.03044"
  },
  {
    "id": "arXiv:2104.03469",
    "title": "Gi and Pal Scores: Deep Neural Network Generalization Statistics",
    "abstract": "Comments: Accepted to RobustML Workshop at ICLR 2021",
    "descriptor": "\nComments: Accepted to RobustML Workshop at ICLR 2021\n",
    "authors": [
      "Yair Schiff",
      "Brian Quanz",
      "Payel Das",
      "Pin-Yu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.03469"
  },
  {
    "id": "arXiv:2104.03841",
    "title": "ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition",
    "abstract": "ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition",
    "descriptor": "",
    "authors": [
      "Daniela Massiceti",
      "Luisa Zintgraf",
      "John Bronskill",
      "Lida Theodorou",
      "Matthew Tobias Harris",
      "Edward Cutrell",
      "Cecily Morrison",
      "Katja Hofmann",
      "Simone Stumpf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.03841"
  },
  {
    "id": "arXiv:2104.04045",
    "title": "End-to-end speaker segmentation for overlap-aware resegmentation",
    "abstract": "Comments: Camera-ready version for Interspeech 2021 with significantly better voice activity detection, overlapped speech detection, and speaker diarization results. The code used for results reported in v1 contained a small bug that has now been fixed",
    "descriptor": "\nComments: Camera-ready version for Interspeech 2021 with significantly better voice activity detection, overlapped speech detection, and speaker diarization results. The code used for results reported in v1 contained a small bug that has now been fixed\n",
    "authors": [
      "Herv\u00e9 Bredin",
      "Antoine Laurent"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.04045"
  },
  {
    "id": "arXiv:2104.04298",
    "title": "Feature Replacement and Combination for Hybrid ASR Systems",
    "abstract": "Feature Replacement and Combination for Hybrid ASR Systems",
    "descriptor": "",
    "authors": [
      "Peter Vieting",
      "Christoph L\u00fcscher",
      "Wilfried Michel",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.04298"
  },
  {
    "id": "arXiv:2104.05379",
    "title": "Comparing the Benefit of Synthetic Training Data for Various Automatic  Speech Recognition Architectures",
    "abstract": "Comparing the Benefit of Synthetic Training Data for Various Automatic  Speech Recognition Architectures",
    "descriptor": "",
    "authors": [
      "Nick Rossenbach",
      "Mohammad Zeineldeen",
      "Benedikt Hilmes",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.05379"
  },
  {
    "id": "arXiv:2104.09082",
    "title": "Classification of head impacts based on the spectral density of  measurable kinematics",
    "abstract": "Comments: 16 pages, 5 figures",
    "descriptor": "\nComments: 16 pages, 5 figures\n",
    "authors": [
      "Xianghao Zhan",
      "Yiheng Li",
      "Yuzhe Liu",
      "Nicholas J. Cecchi",
      "Samuel J. Raymond",
      "Zhou Zhou",
      "Hossein Vahid Alizadeh",
      "Jesse Ruan",
      "Saeed Barbat",
      "Stephen Tiernan",
      "Olivier Gevaert",
      "Michael M. Zeineh",
      "Gerald A. Grant",
      "David B. Camarillo"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biological Physics (physics.bio-ph)"
    ],
    "url": "https://arxiv.org/abs/2104.09082"
  },
  {
    "id": "arXiv:2104.10729",
    "title": "Low-Light Image and Video Enhancement Using Deep Learning: A Survey",
    "abstract": "Low-Light Image and Video Enhancement Using Deep Learning: A Survey",
    "descriptor": "",
    "authors": [
      "Chongyi Li",
      "Chunle Guo",
      "Linghao Han",
      "Jun Jiang",
      "Ming-Ming Cheng",
      "Jinwei Gu",
      "Chen Change Loy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.10729"
  },
  {
    "id": "arXiv:2104.11014",
    "title": "Network Space Search for Pareto-Efficient Spaces",
    "abstract": "Comments: CVPR2021 Workshop (Efficient Deep Learning for Computer Vision). Website: this https URL",
    "descriptor": "\nComments: CVPR2021 Workshop (Efficient Deep Learning for Computer Vision). Website: this https URL\n",
    "authors": [
      "Min-Fong Hong",
      "Hao-Yun Chen",
      "Min-Hung Chen",
      "Yu-Syuan Xu",
      "Hsien-Kai Kuo",
      "Yi-Min Tsai",
      "Hung-Jen Chen",
      "Kevin Jou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.11014"
  },
  {
    "id": "arXiv:2104.11353",
    "title": "Optimal Cost Design for Model Predictive Control",
    "abstract": "Comments: In proceedings of 3rd Annual Learning for Dynamics & Control Conference (L4DC) 2021",
    "descriptor": "\nComments: In proceedings of 3rd Annual Learning for Dynamics & Control Conference (L4DC) 2021\n",
    "authors": [
      "Avik Jain",
      "Lawrence Chan",
      "Daniel S. Brown",
      "Anca D. Dragan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.11353"
  },
  {
    "id": "arXiv:2104.11462",
    "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised  Representation Learning from Speech",
    "abstract": "Comments: Will be presented at Interspeech 2021",
    "descriptor": "\nComments: Will be presented at Interspeech 2021\n",
    "authors": [
      "Solene Evain",
      "Ha Nguyen",
      "Hang Le",
      "Marcely Zanon Boito",
      "Salima Mdhaffar",
      "Sina Alisamir",
      "Ziyi Tong",
      "Natalia Tomashenko",
      "Marco Dinarelli",
      "Titouan Parcollet",
      "Alexandre Allauzen",
      "Yannick Esteve",
      "Benjamin Lecouteux",
      "Francois Portet",
      "Solange Rossato",
      "Fabien Ringeval",
      "Didier Schwab",
      "Laurent Besacier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.11462"
  },
  {
    "id": "arXiv:2104.11463",
    "title": "Decision Tree Learning in CEGIS-Based Termination Analysis",
    "abstract": "Comments: camera ready for CAV 2021",
    "descriptor": "\nComments: camera ready for CAV 2021\n",
    "authors": [
      "Satoshi Kura",
      "Hiroshi Unno",
      "Ichiro Hasuo"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.11463"
  },
  {
    "id": "arXiv:2104.12753",
    "title": "Vision Transformers with Patch Diversification",
    "abstract": "Comments: preprint",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Chengyue Gong",
      "Dilin Wang",
      "Meng Li",
      "Vikas Chandra",
      "Qiang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.12753"
  },
  {
    "id": "arXiv:2104.13421",
    "title": "Canonical automata via distributive law homomorphisms",
    "abstract": "Canonical automata via distributive law homomorphisms",
    "descriptor": "",
    "authors": [
      "Stefan Zetzsche",
      "Gerco van Heerdt",
      "Alexandra Silva",
      "Matteo Sammartino"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2104.13421"
  },
  {
    "id": "arXiv:2104.13621",
    "title": "MLDemon: Deployment Monitoring for Machine Learning Systems",
    "abstract": "MLDemon: Deployment Monitoring for Machine Learning Systems",
    "descriptor": "",
    "authors": [
      "Antonio Ginart",
      "Martin Zhang",
      "James Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.13621"
  },
  {
    "id": "arXiv:2104.13774",
    "title": "Scouting the Path to a Million-Client Server",
    "abstract": "Scouting the Path to a Million-Client Server",
    "descriptor": "",
    "authors": [
      "Yimeng Zhao",
      "Ahmed Saeed",
      "Mostafa Ammar",
      "Ellen Zegura"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2104.13774"
  },
  {
    "id": "arXiv:2104.13866",
    "title": "Reachability in Vector Addition Systems is Ackermann-complete",
    "abstract": "Reachability in Vector Addition Systems is Ackermann-complete",
    "descriptor": "",
    "authors": [
      "Wojciech Czerwi\u0144ski",
      "\u0141ukasz Orlikowski"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.13866"
  },
  {
    "id": "arXiv:2105.00171",
    "title": "AlloST: Low-resource Speech Translation without Source Transcription",
    "abstract": "Comments: Accepted by Interspeech2021",
    "descriptor": "\nComments: Accepted by Interspeech2021\n",
    "authors": [
      "Yao-Fei Cheng",
      "Hung-Shin Lee",
      "Hsin-Min Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.00171"
  },
  {
    "id": "arXiv:2105.00620",
    "title": "COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction",
    "abstract": "COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction",
    "descriptor": "",
    "authors": [
      "Siawpeng Er",
      "Shihao Yang",
      "Tuo Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2105.00620"
  },
  {
    "id": "arXiv:2105.01601",
    "title": "MLP-Mixer: An all-MLP Architecture for Vision",
    "abstract": "Comments: Fixed parameter counts in Table 1 Added results on JFT-3B in Figure 2(right) Added Section 3.4 on the input permutations",
    "descriptor": "\nComments: Fixed parameter counts in Table 1 Added results on JFT-3B in Figure 2(right) Added Section 3.4 on the input permutations\n",
    "authors": [
      "Ilya Tolstikhin",
      "Neil Houlsby",
      "Alexander Kolesnikov",
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Thomas Unterthiner",
      "Jessica Yung",
      "Andreas Steiner",
      "Daniel Keysers",
      "Jakob Uszkoreit",
      "Mario Lucic",
      "Alexey Dosovitskiy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.01601"
  },
  {
    "id": "arXiv:2105.02428",
    "title": "Faster Algorithms for Bounded Tree Edit Distance",
    "abstract": "Comments: To appear in ICALP 2021. Updated funding information and references",
    "descriptor": "\nComments: To appear in ICALP 2021. Updated funding information and references\n",
    "authors": [
      "Shyan Akmal",
      "Ce Jin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2105.02428"
  },
  {
    "id": "arXiv:2105.02692",
    "title": "Learning to Perturb Word Embeddings for Out-of-distribution QA",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Seanie Lee",
      "Minki Kang",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.02692"
  },
  {
    "id": "arXiv:2105.03404",
    "title": "ResMLP: Feedforward networks for image classification with  data-efficient training",
    "abstract": "ResMLP: Feedforward networks for image classification with  data-efficient training",
    "descriptor": "",
    "authors": [
      "Hugo Touvron",
      "Piotr Bojanowski",
      "Mathilde Caron",
      "Matthieu Cord",
      "Alaaeldin El-Nouby",
      "Edouard Grave",
      "Gautier Izacard",
      "Armand Joulin",
      "Gabriel Synnaeve",
      "Jakob Verbeek",
      "Herv\u00e9 J\u00e9gou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.03404"
  },
  {
    "id": "arXiv:2105.03835",
    "title": "Segmenting Hybrid Trajectories using Latent ODEs",
    "abstract": "Segmenting Hybrid Trajectories using Latent ODEs",
    "descriptor": "",
    "authors": [
      "Ruian Shi",
      "Quaid Morris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03835"
  },
  {
    "id": "arXiv:2105.04045",
    "title": "Swarm Differential Privacy for Purpose Driven  Data-Information-Knowledge-Wisdom Architecture",
    "abstract": "Swarm Differential Privacy for Purpose Driven  Data-Information-Knowledge-Wisdom Architecture",
    "descriptor": "",
    "authors": [
      "Yingbo Li",
      "Yucong Duan",
      "Zakaria Maama",
      "Haoyang Che",
      "Anamaria-Beatrice Spulber",
      "Stelios Fuentes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2105.04045"
  },
  {
    "id": "arXiv:2105.04742",
    "title": "Incremental Graph Computation: Anchored Vertex Tracking in Dynamic  Social Networks",
    "abstract": "Comments: The manuscript (12 pages) is currently under review",
    "descriptor": "\nComments: The manuscript (12 pages) is currently under review\n",
    "authors": [
      "Taotao Cai",
      "Shuiqiao Yang",
      "Jianxin Li",
      "Quan Z. Sheng",
      "Jian Yang",
      "Xin Wang",
      "Wei Emma Zhang",
      "Longxiang Gao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2105.04742"
  },
  {
    "id": "arXiv:2105.05003",
    "title": "CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional  Convolution",
    "abstract": "CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional  Convolution",
    "descriptor": "",
    "authors": [
      "Lizhe Liu",
      "Xiaohao Chen",
      "Siyu Zhu",
      "Ping Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.05003"
  },
  {
    "id": "arXiv:2105.05403",
    "title": "Structure Guided Lane Detection",
    "abstract": "Comments: Accepted by IJCAI 2021",
    "descriptor": "\nComments: Accepted by IJCAI 2021\n",
    "authors": [
      "Jinming Su",
      "Chao Chen",
      "Ke Zhang",
      "Junfeng Luo",
      "Xiaoming Wei",
      "Xiaolin Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.05403"
  },
  {
    "id": "arXiv:2105.06340",
    "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video  Sequences using Temporal Oriented Reference Frame",
    "abstract": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video  Sequences using Temporal Oriented Reference Frame",
    "descriptor": "",
    "authors": [
      "Chuin Hong Yap",
      "Moi Hoon Yap",
      "Adrian K. Davison",
      "Ryan Cunningham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.06340"
  },
  {
    "id": "arXiv:2105.06411",
    "title": "Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single  Demonstration",
    "abstract": "Comments: Published at ICRA 2021. Webpage and video: this https URL",
    "descriptor": "\nComments: Published at ICRA 2021. Webpage and video: this https URL\n",
    "authors": [
      "Edward Johns"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.06411"
  },
  {
    "id": "arXiv:2105.07144",
    "title": "A Cognitive Regularizer for Language Modeling",
    "abstract": "Comments: ACL 2021 Camera-ready (fixed ordering of affiliation emojis)",
    "descriptor": "\nComments: ACL 2021 Camera-ready (fixed ordering of affiliation emojis)\n",
    "authors": [
      "Jason Wei",
      "Clara Meister",
      "Ryan Cotterell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.07144"
  },
  {
    "id": "arXiv:2105.08399",
    "title": "Relative Positional Encoding for Transformers with Linear Complexity",
    "abstract": "Comments: ICML 2021 (long talk) camera-ready. 24 pages",
    "descriptor": "\nComments: ICML 2021 (long talk) camera-ready. 24 pages\n",
    "authors": [
      "Antoine Liutkus",
      "Ond\u0159ej C\u00edfka",
      "Shih-Lun Wu",
      "Umut \u015eim\u015fekli",
      "Yi-Hsuan Yang",
      "Ga\u00ebl Richard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.08399"
  },
  {
    "id": "arXiv:2105.10056",
    "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning",
    "abstract": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning",
    "descriptor": "",
    "authors": [
      "Zhuangdi Zhu",
      "Junyuan Hong",
      "Jiayu Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2105.10056"
  },
  {
    "id": "arXiv:2105.12245",
    "title": "Scaling Properties of Deep Residual Networks",
    "abstract": "Comments: Published at ICML 2021",
    "descriptor": "\nComments: Published at ICML 2021\n",
    "authors": [
      "Alain-Sam Cohen",
      "Rama Cont",
      "Alain Rossier",
      "Renyuan Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.12245"
  },
  {
    "id": "arXiv:2105.12348",
    "title": "Composition and Application of Current Advanced Driving Assistance  System: A Review",
    "abstract": "Composition and Application of Current Advanced Driving Assistance  System: A Review",
    "descriptor": "",
    "authors": [
      "Xinran Li",
      "Kuo-Yi Lin",
      "Min Meng",
      "Xiuxian Li",
      "Li Li",
      "Yiguang Hong",
      "Jie Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2105.12348"
  },
  {
    "id": "arXiv:2105.13203",
    "title": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point  Solving",
    "abstract": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point  Solving",
    "descriptor": "",
    "authors": [
      "Julien Grand-Cl\u00e9ment",
      "Christian Kroer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2105.13203"
  },
  {
    "id": "arXiv:2105.13553",
    "title": "Autonomous Optimization of Fluid Systems at Varying Length Scales",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Alexander E. Siemenn",
      "Evyatar Shaulsky",
      "Matthew Beveridge",
      "Tonio Buonassisi",
      "Sara M. Hashmi",
      "Iddo Drori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2105.13553"
  },
  {
    "id": "arXiv:2105.14163",
    "title": "The query complexity of sampling from strongly log-concave distributions  in one dimension",
    "abstract": "Comments: 19 pages, 4 figures",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "Sinho Chewi",
      "Patrik Gerber",
      "Chen Lu",
      "Thibaut Le Gouic",
      "Philippe Rigollet"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.14163"
  },
  {
    "id": "arXiv:2105.14167",
    "title": "NeuralLog: Natural Language Inference with Joint Neural and Logical  Reasoning",
    "abstract": "Comments: 8 pages, 4 figures, The 10th Joint Conference on Lexical and Computational Semantics (*SEM2021) @ ACL2021",
    "descriptor": "\nComments: 8 pages, 4 figures, The 10th Joint Conference on Lexical and Computational Semantics (*SEM2021) @ ACL2021\n",
    "authors": [
      "Zeming Chen",
      "Qiyue Gao",
      "Lawrence S. Moss"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.14167"
  },
  {
    "id": "arXiv:2105.14202",
    "title": "Universal Adder Neural Networks",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:1912.13200",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1912.13200\n",
    "authors": [
      "Hanting Chen",
      "Yunhe Wang",
      "Chang Xu",
      "Chao Xu",
      "Chunjing Xu",
      "Tong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.14202"
  },
  {
    "id": "arXiv:2105.14711",
    "title": "CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in  Computed Tomography",
    "abstract": "CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in  Computed Tomography",
    "descriptor": "",
    "authors": [
      "Yang Deng",
      "Ce Wang",
      "Yuan Hui",
      "Qian Li",
      "Jun Li",
      "Shiwei Luo",
      "Mengke Sun",
      "Quan Quan",
      "Shuxin Yang",
      "You Hao",
      "Pengbo Liu",
      "Honghu Xiao",
      "Chunpeng Zhao",
      "Xinbao Wu",
      "S. Kevin Zhou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.14711"
  },
  {
    "id": "arXiv:2105.15079",
    "title": "SA2SL: From Aspect-Based Sentiment Analysis to Social Listening System  for Business Intelligence",
    "abstract": "SA2SL: From Aspect-Based Sentiment Analysis to Social Listening System  for Business Intelligence",
    "descriptor": "",
    "authors": [
      "Luong Luc Phan",
      "Phuc Huynh Pham",
      "Kim Thi-Thanh Nguyen",
      "Tham Thi Nguyen",
      "Sieu Khai Huynh",
      "Luan Thanh Nguyen",
      "Tin Van Huynh",
      "Kiet Van Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.15079"
  },
  {
    "id": "arXiv:2105.15111",
    "title": "An Epidemiological Model for contact tracing with the Dutch CoronaMelder  App",
    "abstract": "Comments: This is a first and preliminary draft. Future updates are expected",
    "descriptor": "\nComments: This is a first and preliminary draft. Future updates are expected\n",
    "authors": [
      "Peter Boncz"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2105.15111"
  },
  {
    "id": "arXiv:2106.01093",
    "title": "LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and  Non-Local Relations",
    "abstract": "Comments: 15 pages, 8 figures, accepted to ACL 2021 main conference",
    "descriptor": "\nComments: 15 pages, 8 figures, accepted to ACL 2021 main conference\n",
    "authors": [
      "Ruisheng Cao",
      "Lu Chen",
      "Zhi Chen",
      "Yanbin Zhao",
      "Su Zhu",
      "Kai Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.01093"
  },
  {
    "id": "arXiv:2106.01300",
    "title": "PP-Rec: News Recommendation with Personalized User Interest and  Time-aware News Popularity",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Yongfeng Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.01300"
  },
  {
    "id": "arXiv:2106.01721",
    "title": "Curiosity-based Robot Navigation under Uncertainty in Crowded  Environments",
    "abstract": "Comments: IEEE International Conference on Robotics and Automation Workshop on Social Intelligence in Humans and Robots (Xi'an, China, 2021)",
    "descriptor": "\nComments: IEEE International Conference on Robotics and Automation Workshop on Social Intelligence in Humans and Robots (Xi'an, China, 2021)\n",
    "authors": [
      "Kuanqi Cai",
      "Weinan Chen",
      "Chaoqun Wang",
      "Shuang Song",
      "Max Q.-H. Meng"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.01721"
  },
  {
    "id": "arXiv:2106.02029",
    "title": "Off-Policy Evaluation via Adaptive Weighting with Data from Contextual  Bandits",
    "abstract": "Off-Policy Evaluation via Adaptive Weighting with Data from Contextual  Bandits",
    "descriptor": "",
    "authors": [
      "Ruohan Zhan",
      "Vitor Hadad",
      "David A. Hirshberg",
      "Susan Athey"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.02029"
  },
  {
    "id": "arXiv:2106.02473",
    "title": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for  Classification Algorithm Test: from Linear Regression to Visual Transformer",
    "abstract": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for  Classification Algorithm Test: from Linear Regression to Visual Transformer",
    "descriptor": "",
    "authors": [
      "Weiming Hu",
      "Chen Li",
      "Xiaoyan Li",
      "Md Mamunur Rahaman",
      "Haoyuan Chen",
      "Wanli Liu",
      "Changhao Sun",
      "Yudong Yao",
      "Marcin Grzegorzek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.02473"
  },
  {
    "id": "arXiv:2106.02636",
    "title": "MERLOT: Multimodal Neural Script Knowledge Models",
    "abstract": "Comments: project page at this https URL",
    "descriptor": "\nComments: project page at this https URL\n",
    "authors": [
      "Rowan Zellers",
      "Ximing Lu",
      "Jack Hessel",
      "Youngjae Yu",
      "Jae Sung Park",
      "Jize Cao",
      "Ali Farhadi",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02636"
  },
  {
    "id": "arXiv:2106.02697",
    "title": "Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees",
    "abstract": "Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees",
    "descriptor": "",
    "authors": [
      "Philip A. Etter",
      "Kai Zhong",
      "Hsiang-Fu Yu",
      "Lexing Ying",
      "Inderjit Dhillon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02697"
  },
  {
    "id": "arXiv:2106.02812",
    "title": "Optimizing Ansatz Design in QAOA for Max-cut",
    "abstract": "Comments: 14 pages; single column (without reference)",
    "descriptor": "\nComments: 14 pages; single column (without reference)\n",
    "authors": [
      "Ritajit Majumdar",
      "Dhiraj Madan",
      "Debasmita Bhoumik",
      "Dhinakaran Vinayagamurthy",
      "Shesha Raghunathan",
      "Susmita Sur-Kolay"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.02812"
  },
  {
    "id": "arXiv:2106.03084",
    "title": "Combining Static Word Embeddings and Contextual Representations for  Bilingual Lexicon Induction",
    "abstract": "Comments: Accepted to Findings of ACL2021",
    "descriptor": "\nComments: Accepted to Findings of ACL2021\n",
    "authors": [
      "Jinpeng Zhang",
      "Baijun Ji",
      "Nini Xiao",
      "Xiangyu Duan",
      "Min Zhang",
      "Yangbin Shi",
      "Weihua Luo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03084"
  },
  {
    "id": "arXiv:2106.03142",
    "title": "A Physics-Informed Deep Learning Paradigm for Traffic State Estimation  and Fundamental Diagram Discovery",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2101.06580",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2101.06580\n",
    "authors": [
      "Rongye Shi",
      "Zhaobin Mo",
      "Kuang Huang",
      "Xuan Di",
      "Qiang Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03142"
  },
  {
    "id": "arXiv:2106.03161",
    "title": "Identifying Populist Paragraphs in Text: A machine-learning approach",
    "abstract": "Comments: 18 pages, 2 Figures, 3 Tables in main text, 2 tables in Annexes",
    "descriptor": "\nComments: 18 pages, 2 Figures, 3 Tables in main text, 2 tables in Annexes\n",
    "authors": [
      "Jogil\u0117 Ulinskait\u0117",
      "Lukas Pukelis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.03161"
  },
  {
    "id": "arXiv:2106.03673",
    "title": "Algorithms and Decision-Making in the Public Sector",
    "abstract": "Algorithms and Decision-Making in the Public Sector",
    "descriptor": "",
    "authors": [
      "Karen Levy",
      "Kyla Chasalow",
      "Sarah Riley"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.03673"
  },
  {
    "id": "arXiv:2106.03753",
    "title": "Energy-Efficient Naming in Beeping Networks",
    "abstract": "Energy-Efficient Naming in Beeping Networks",
    "descriptor": "",
    "authors": [
      "Ny Aina Andriambolamalala",
      "Vlady Ravelomanana"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.03753"
  },
  {
    "id": "arXiv:2106.04119",
    "title": "LaserShark: Establishing Fast, Bidirectional Communication into  Air-Gapped Systems",
    "abstract": "LaserShark: Establishing Fast, Bidirectional Communication into  Air-Gapped Systems",
    "descriptor": "",
    "authors": [
      "Niclas K\u00fchnapfel",
      "Stefan Preu\u00dfler",
      "Maximilian Noppel",
      "Thomas Schneider",
      "Konrad Rieck",
      "Christian Wressnegger"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.04119"
  },
  {
    "id": "arXiv:2106.04292",
    "title": "Principled Hyperedge Prediction with Structural Spectral Features and  Neural Networks",
    "abstract": "Principled Hyperedge Prediction with Structural Spectral Features and  Neural Networks",
    "descriptor": "",
    "authors": [
      "Changlin Wan",
      "Muhan Zhang",
      "Wei Hao",
      "Sha Cao",
      "Pan Li",
      "Chi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04292"
  },
  {
    "id": "arXiv:2106.04387",
    "title": "Multi-frame sequence generator of 4D human body motion",
    "abstract": "Multi-frame sequence generator of 4D human body motion",
    "descriptor": "",
    "authors": [
      "Marsot Mathieu",
      "Wuhrer Stefanie",
      "Franco Jean-Sebastien",
      "Durocher Stephane"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04387"
  },
  {
    "id": "arXiv:2106.04411",
    "title": "Fair Feature Distillation for Visual Recognition",
    "abstract": "Fair Feature Distillation for Visual Recognition",
    "descriptor": "",
    "authors": [
      "Sangwon Jung",
      "Donggyu Lee",
      "Taeeon Park",
      "Taesup Moon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04411"
  },
  {
    "id": "arXiv:2106.04515",
    "title": "Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in  North Carolina",
    "abstract": "Comments: 12 pages, 6 figures, 7 tables, to be published in ACM-BCB 2021, corrected misspelled author",
    "descriptor": "\nComments: 12 pages, 6 figures, 7 tables, to be published in ACM-BCB 2021, corrected misspelled author\n",
    "authors": [
      "Christopher Whitfield",
      "Yang Liu",
      "Mohd Anwar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04515"
  },
  {
    "id": "arXiv:2106.04521",
    "title": "An App for Visual Exploration, Discovery, and Sharing of Poncelet  3-Periodic Phenomena",
    "abstract": "Comments: 19 pages, 20 figures",
    "descriptor": "\nComments: 19 pages, 20 figures\n",
    "authors": [
      "Iverton Darlan",
      "Dan Reznik"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computational Geometry (cs.CG)",
      "Dynamical Systems (math.DS)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2106.04521"
  },
  {
    "id": "arXiv:2106.04533",
    "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration",
    "abstract": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration",
    "descriptor": "",
    "authors": [
      "Tianlong Chen",
      "Yu Cheng",
      "Zhe Gan",
      "Lu Yuan",
      "Lei Zhang",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04533"
  },
  {
    "id": "arXiv:2106.04615",
    "title": "Vector Quantized Models for Planning",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Sherjil Ozair",
      "Yazhe Li",
      "Ali Razavi",
      "Ioannis Antonoglou",
      "A\u00e4ron van den Oord",
      "Oriol Vinyals"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04615"
  },
  {
    "id": "arXiv:2106.04707",
    "title": "Job Dispatching Policies for Queueing Systems with Unknown Service Rates",
    "abstract": "Job Dispatching Policies for Queueing Systems with Unknown Service Rates",
    "descriptor": "",
    "authors": [
      "Tuhinangshu Choudhury",
      "Gauri Joshi",
      "Weina Wang",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04707"
  },
  {
    "id": "arXiv:2106.04763",
    "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A  Static-Adaptive Algorithm",
    "abstract": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A  Static-Adaptive Algorithm",
    "descriptor": "",
    "authors": [
      "MohammadJavad Azizi",
      "Branislav Kveton",
      "Mohammad Ghavamzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04763"
  },
  {
    "id": "arXiv:2106.04958",
    "title": "Unifying Behavioral and Response Diversity for Open-ended Learning in  Zero-sum Games",
    "abstract": "Comments: We investigate a new perspective on unifying diversity measures for open-ended learning in zero-sum games, which shapes an auto-curriculum to induce diverse yet effective behaviors",
    "descriptor": "\nComments: We investigate a new perspective on unifying diversity measures for open-ended learning in zero-sum games, which shapes an auto-curriculum to induce diverse yet effective behaviors\n",
    "authors": [
      "Xiangyu Liu",
      "Hangtian Jia",
      "Ying Wen",
      "Yaodong Yang",
      "Yujing Hu",
      "Yingfeng Chen",
      "Changjie Fan",
      "Zhipeng Hu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04958"
  },
  {
    "id": "arXiv:2106.05010",
    "title": "Loss function based second-order Jensen inequality and its application  to particle variational inference",
    "abstract": "Loss function based second-order Jensen inequality and its application  to particle variational inference",
    "descriptor": "",
    "authors": [
      "Futoshi Futami",
      "Tomoharu Iwata",
      "Naonori Ueda",
      "Issei Sato",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05010"
  },
  {
    "id": "arXiv:2106.05012",
    "title": "Bayesian Bellman Operators",
    "abstract": "Bayesian Bellman Operators",
    "descriptor": "",
    "authors": [
      "Matthew Fellows",
      "Kristian Hartikainen",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05012"
  },
  {
    "id": "arXiv:2106.05047",
    "title": "Salient Object Ranking with Position-Preserved Attention",
    "abstract": "Salient Object Ranking with Position-Preserved Attention",
    "descriptor": "",
    "authors": [
      "Hao Fang",
      "Daoxin Zhang",
      "Yi Zhang",
      "Minghao Chen",
      "Jiawei Li",
      "Yao Hu",
      "Deng Cai",
      "Xiaofei He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05047"
  },
  {
    "id": "arXiv:2106.05050",
    "title": "IChannels: Exploiting Current Management Mechanisms to Create Covert  Channels in Modern Processors",
    "abstract": "Comments: To appear in ISCA 2021",
    "descriptor": "\nComments: To appear in ISCA 2021\n",
    "authors": [
      "Jawad Haj-Yahya",
      "Jeremie S. Kim",
      "A. Giray Yaglikci",
      "Ivan Puddu",
      "Lois Orosa",
      "Juan G\u00f3mez Luna",
      "Mohammed Alser",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05050"
  },
  {
    "id": "arXiv:2106.05074",
    "title": "Operationalizing Complex Causes: A Pragmatic View of Mediation",
    "abstract": "Operationalizing Complex Causes: A Pragmatic View of Mediation",
    "descriptor": "",
    "authors": [
      "Limor Gultchin",
      "David S. Watson",
      "Matt J. Kusner",
      "Ricardo Silva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.05074"
  },
  {
    "id": "arXiv:2106.05152",
    "title": "Rethink Transfer Learning in Medical Image Classification",
    "abstract": "Comments: Under review for MICCAI 2021",
    "descriptor": "\nComments: Under review for MICCAI 2021\n",
    "authors": [
      "Le Peng",
      "Hengyue Liang",
      "Taihui Li",
      "Ju Sun"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05152"
  },
  {
    "id": "arXiv:2106.05220",
    "title": "Single-Server Private Linear Transformation: The Joint Privacy Case",
    "abstract": "Comments: 12 pages, 1 figure. This work is a long version of arXiv:2102.01665",
    "descriptor": "\nComments: 12 pages, 1 figure. This work is a long version of arXiv:2102.01665\n",
    "authors": [
      "Anoosheh Heidarzadeh",
      "Nahid Esmati",
      "Alex Sprintson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05220"
  },
  {
    "id": "arXiv:2106.05222",
    "title": "Single-Server Private Linear Transformation: The Individual Privacy Case",
    "abstract": "Comments: 14 pages, 1 figure. This work is a long version of arXiv:2102.01662",
    "descriptor": "\nComments: 14 pages, 1 figure. This work is a long version of arXiv:2102.01662\n",
    "authors": [
      "Anoosheh Heidarzadeh",
      "Nahid Esmati",
      "Alex Sprintson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05222"
  },
  {
    "id": "arXiv:2106.05261",
    "title": "We Can Always Catch You: Detecting Adversarial Patched Objects WITH or  WITHOUT Signature",
    "abstract": "We Can Always Catch You: Detecting Adversarial Patched Objects WITH or  WITHOUT Signature",
    "descriptor": "",
    "authors": [
      "Bin Liang",
      "Jiachun Li",
      "Jianjun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05261"
  }
]