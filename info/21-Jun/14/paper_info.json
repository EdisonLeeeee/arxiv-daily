[
  {
    "id": "arXiv:2106.05974",
    "title": "Scaling Vision with Sparse Mixture of Experts",
    "abstract": "Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent\nscalability in Natural Language Processing. In Computer Vision, however, almost\nall performant networks are \"dense\", that is, every input is processed by every\nparameter. We present a Vision MoE (V-MoE), a sparse version of the Vision\nTransformer, that is scalable and competitive with the largest dense networks.\nWhen applied to image recognition, V-MoE matches the performance of\nstate-of-the-art networks, while requiring as little as half of the compute at\ninference time. Further, we propose an extension to the routing algorithm that\ncan prioritize subsets of each input across the entire batch, leading to\nadaptive per-image compute. This allows V-MoE to trade-off performance and\ncompute smoothly at test-time. Finally, we demonstrate the potential of V-MoE\nto scale vision models, and train a 15B parameter model that attains 90.35% on\nImageNet.",
    "descriptor": "\nComments: 44 pages, 38 figures\n",
    "authors": [
      "Carlos Riquelme",
      "Joan Puigcerver",
      "Basil Mustafa",
      "Maxim Neumann",
      "Rodolphe Jenatton",
      "Andr\u00e9 Susano Pinto",
      "Daniel Keysers",
      "Neil Houlsby"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05974"
  },
  {
    "id": "arXiv:2106.05987",
    "title": "Hybrid Systems Verification with Isabelle/HOL: Simpler Syntax, Better  Models, Faster Proofs",
    "abstract": "We extend a semantic verification framework for hybrid systems with the\nIsabelle/HOL proof assistant by an algebraic model for hybrid program stores, a\nshallow expression model for hybrid programs and their correctness\nspecifications, and domain-specific deductive and calculational support. The\nnew store model yields clean separations and dynamic local views of variables,\ne.g. discrete/continuous, mutable/immutable, program/logical, and enhanced ways\nof manipulating them using combinators, projections and framing. This leads to\nmore local inference rules, procedures and tactics for reasoning with invariant\nsets, certifying solutions of hybrid specifications or calculating derivatives\nwith increased proof automation and scalability. The new expression model\nprovides more user-friendly syntax, better control of name spaces and\ninterfaces connecting the framework with real-world modelling languages.",
    "descriptor": "\nComments: 18 pages, submitted to FM 2021\n",
    "authors": [
      "Simon Foster",
      "Jonathan Juli\u00e1n Huerta y Munive",
      "Mario Gleirscher",
      "Georg Struth"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.05987"
  },
  {
    "id": "arXiv:2106.05992",
    "title": "Scalable Variational Gaussian Processes via Harmonic Kernel  Decomposition",
    "abstract": "We introduce a new scalable variational Gaussian process approximation which\nprovides a high fidelity approximation while retaining general applicability.\nWe propose the harmonic kernel decomposition (HKD), which uses Fourier series\nto decompose a kernel as a sum of orthogonal kernels. Our variational\napproximation exploits this orthogonality to enable a large number of inducing\npoints at a low computational cost. We demonstrate that, on a range of\nregression and classification problems, our approach can exploit input space\nsymmetries such as translations and reflections, and it significantly\noutperforms standard variational methods in scalability and accuracy. Notably,\nour approach achieves state-of-the-art results on CIFAR-10 among pure GP\nmodels.",
    "descriptor": "\nComments: ICML2021, 21 pages\n",
    "authors": [
      "Shengyang Sun",
      "Jiaxin Shi",
      "Andrew Gordon Wilson",
      "Roger Grosse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05992"
  },
  {
    "id": "arXiv:2106.05996",
    "title": "An Ensemble Approach Towards Adversarial Robustness",
    "abstract": "It is a known phenomenon that adversarial robustness comes at a cost to\nnatural accuracy. To improve this trade-off, this paper proposes an ensemble\napproach that divides a complex robust-classification task into simpler\nsubtasks. Specifically, fractal divide derives multiple training sets from the\ntraining data, and fractal aggregation combines inference outputs from multiple\nclassifiers that are trained on those sets. The resulting ensemble classifiers\nhave a unique property that ensures robustness for an input if certain\ndon't-care conditions are met. The new techniques are evaluated on MNIST and\nFashion-MNIST, with no adversarial training. The MNIST classifier has 99%\nnatural accuracy, 70% measured robustness and 36.9% provable robustness, within\nL2 distance of 2. The Fashion-MNIST classifier has 90% natural accuracy, 54.5%\nmeasured robustness and 28.2% provable robustness, within L2 distance of 1.5.\nBoth results are new state of the art, and we also present new state-of-the-art\nbinary results on challenging label-pairs.",
    "descriptor": "",
    "authors": [
      "Haifeng Qian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05996"
  },
  {
    "id": "arXiv:2106.05997",
    "title": "Verifying Quantized Neural Networks using SMT-Based Model Checking",
    "abstract": "Artificial Neural Networks (ANNs) are being deployed on an increasing number\nof safety-critical applications, including autonomous cars and medical\ndiagnosis. However, concerns about their reliability have been raised due to\ntheir black-box nature and apparent fragility to adversarial attacks. Here, we\ndevelop and evaluate a symbolic verification framework using incremental model\nchecking (IMC) and satisfiability modulo theories (SMT) to check for\nvulnerabilities in ANNs. More specifically, we propose several ANN-related\noptimizations for IMC, including invariant inference via interval analysis and\nthe discretization of non-linear activation functions. With this, we can\nprovide guarantees on the safe behavior of ANNs implemented both in\nfloating-point and fixed-point (quantized) arithmetic. In this regard, our\nverification approach was able to verify and produce adversarial examples for\n52 test cases spanning image classification and general machine learning\napplications. For small- to medium-sized ANN, our approach completes most of\nits verification runs in minutes. Moreover, in contrast to most\nstate-of-the-art methods, our approach is not restricted to specific choices of\nactivation functions or non-quantized representations.",
    "descriptor": "",
    "authors": [
      "Luiz Sena",
      "Xidan Song",
      "Erickson Alves",
      "Iury Bessa",
      "Edoardo Manino",
      "Lucas Cordeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2106.05997"
  },
  {
    "id": "arXiv:2106.06000",
    "title": "Use of a non-peer reviewed sources in cyber-security scientific research",
    "abstract": "Most publicly available data on cyber incidents comes from private companies\nand non-academic sources. Common sources of information include various\nsecurity bulletins, white papers, reports, court cases, and blog posts\ndescribing specific events, often from a single point of view, followed by\noccasional academic sources, usually conference proceedings. The main\ncharacteristics of the available data sources are: lack of peer review and\nunavailability of confidential data. In this paper, we use an indirect approach\nto identify trusted sources used in scientific work. We analyze how top-rated\npeer reviewed literature relies on the use of non-peer reviewed sources on\ncybersecurity incidents. To identify current non-peer reviewed sources on\ncybersecurity we analyze references in top rated peer reviewed computer\nsecurity conferences. We also analyze how non-peer reviewed sources are used,\nto motivate or support research. We examined 808 articles from top conferences\nin field of computer security. The result of this work are list of the most\ncommonly used non-peer reviewed data sources and information about the context\nin which this data is used. Since these sources are accepted in top\nconferences, other researchers can consider them in their future research. To\nthe best of our knowledge, analysis on how non-peer reviewed sources are used\nin cyber-security scientific research has not been done before.",
    "descriptor": "\nComments: 9 pages, 6 tables\n",
    "authors": [
      "Dalibor Gernhardt",
      "Stjepan Gro\u0161"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06000"
  },
  {
    "id": "arXiv:2106.06001",
    "title": "TIRA: An OpenAPI Extension and Toolbox for GDPR Transparency in RESTful  Architectures",
    "abstract": "Transparency - the provision of information about what personal data is\ncollected for which purposes, how long it is stored, or to which parties it is\ntransferred - is one of the core privacy principles underlying regulations such\nas the GDPR. Technical approaches for implementing transparency in practice\nare, however, only rarely considered. In this paper, we present a novel\napproach for doing so in current, RESTful application architectures and in line\nwith prevailing agile and DevOps-driven practices. For this purpose, we\nintroduce 1) a transparency-focused extension of OpenAPI specifications that\nallows individual service descriptions to be enriched with transparency-related\nannotations in a bottom-up fashion and 2) a set of higher-order tools for\naggregating respective information across multiple, interdependent services and\nfor coherently integrating our approach into automated CI/CD-pipelines.\nTogether, these building blocks pave the way for providing transparency\ninformation that is more specific and at the same time better reflects the\nactual implementation givens within complex service architectures than current,\noverly broad privacy statements.",
    "descriptor": "\nComments: Accepted for publication at the 2021 International Workshop on Privacy Engineering (IWPE'21). This is a preprint manuscript (authors' own version before final copy-editing)\n",
    "authors": [
      "Elias Gr\u00fcnewald",
      "Paul Wille",
      "Frank Pallas",
      "Maria C. Borges",
      "Max-R. Ulbricht"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.06001"
  },
  {
    "id": "arXiv:2106.06002",
    "title": "Probabilistic, Structure-Aware Algorithms for Improved Variety,  Accuracy, and Coverage of AMR Alignments",
    "abstract": "We present algorithms for aligning components of Abstract Meaning\nRepresentation (AMR) graphs to spans in English sentences. We leverage\nunsupervised learning in combination with heuristics, taking the best of both\nworlds from previous AMR aligners. Our unsupervised models, however, are more\nsensitive to graph substructures, without requiring a separate syntactic parse.\nOur approach covers a wider variety of AMR substructures than previously\nconsidered, achieves higher coverage of nodes and edges, and does so with\nhigher accuracy. We will release our LEAMR datasets and aligner for use in\nresearch on AMR parsing, generation, and evaluation.",
    "descriptor": "\nComments: ACL 2021 Camera-ready\n",
    "authors": [
      "Austin Blodgett",
      "Nathan Schneider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06002"
  },
  {
    "id": "arXiv:2106.06004",
    "title": "CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing",
    "abstract": "The NLP community has witnessed steep progress in a variety of tasks across\nthe realms of monolingual and multilingual language processing recently. These\nsuccesses, in conjunction with the proliferating mixed language interactions on\nsocial media have boosted interest in modeling code-mixed texts. In this work,\nwe present CodemixedNLP, an open-source library with the goals of bringing\ntogether the advances in code-mixed NLP and opening it up to a wider machine\nlearning community. The library consists of tools to develop and benchmark\nversatile model architectures that are tailored for mixed texts, methods to\nexpand training sets, techniques to quantify mixing styles, and fine-tuned\nstate-of-the-art models for 7 tasks in Hinglish. We believe this work has a\npotential to foster a distributed yet collaborative and sustainable ecosystem\nin an otherwise dispersed space of code-mixing research. The toolkit is\ndesigned to be simple, easily extensible, and resourceful to both researchers\nas well as practitioners.",
    "descriptor": "\nComments: Accepted at the Fifth Workshop on Computational Approaches to Linguistic Code-Switching-CALCS 2021\n",
    "authors": [
      "Sai Muralidhar Jayanthi",
      "Kavya Nerella",
      "Khyathi Raghavi Chandu",
      "Alan W Black"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06004"
  },
  {
    "id": "arXiv:2106.06007",
    "title": "Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG  by Synthetic Augmentation",
    "abstract": "Camera-based remote photoplethysmography (rPPG) provides a non-contact way to\nmeasure physiological signals (e.g., heart rate) using facial videos. Recent\ndeep learning architectures have improved the accuracy of such physiological\nmeasurement significantly, yet they are restricted by the diversity of the\nannotated videos. The existing datasets MMSE-HR, AFRL, and UBFC-RPPG contain\nroughly 10%, 0%, and 5% of dark-skinned subjects respectively. The unbalanced\ntraining sets result in a poor generalization capability to unseen subjects and\nlead to unwanted bias toward different demographic groups. In Western academia,\nit is regrettably difficult in a university setting to collect data on these\ndark-skinned subjects. Here we show a first attempt to overcome the lack of\ndark-skinned subjects by synthetic augmentation. A joint optimization framework\nis utilized to translate real videos from light-skinned subjects to dark skin\ntones while retaining their pulsatile signals. In the experiment, our method\nexhibits around 31% reduction in mean absolute error for the dark-skinned group\nand 46% improvement on bias mitigation for all the groups, as compared with the\nprevious work trained with just real samples.",
    "descriptor": "",
    "authors": [
      "Yunhao Ba",
      "Zhen Wang",
      "Kerim Doruk Karinca",
      "Oyku Deniz Bozkurt",
      "Achuta Kadambi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06007"
  },
  {
    "id": "arXiv:2106.06008",
    "title": "On the Bound of Energy Consumption in Cellular IoT Networks",
    "abstract": "Billions of sensors are expected to be connected to the Internet through the\nemerging Internet of Things (IoT) technologies. Many of these sensors will\nprimarily be connected using wireless technologies powered using batteries as\ntheir sole energy source which makes it paramount to optimize their energy\nconsumption. In this paper, we provide an analytic framework of the\nenergy-consumption profile and its lower bound for an IoT end device formulated\nbased on Shannon capacity. We extend the study to model the average\nenergy-consumption performance based on the random geometric distribution of\nIoT gateways by utilizing tools from stochastic geometry and real measurements\nof interference in the ISM-band. Experimental data, interference measurements\nand Monte-Carlo simulations are presented to validate the plausibility of the\nproposed analytic framework, where results demonstrate that the current network\ninfrastructures performance is bounded between two extreme geometric models.\nThis study considers interference seen by a gateway regardless of its source.",
    "descriptor": "",
    "authors": [
      "Bassel Al Homssi",
      "Akram Al-Hourani",
      "Sathyanarayanan Chandrasekharan",
      "Karina Mabell Gomez",
      "Sithamparanathan Kandeepan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.06008"
  },
  {
    "id": "arXiv:2106.06009",
    "title": "Synthesising Reinforcement Learning Policies through Set-Valued  Inductive Rule Learning",
    "abstract": "Today's advanced Reinforcement Learning algorithms produce black-box\npolicies, that are often difficult to interpret and trust for a person. We\nintroduce a policy distilling algorithm, building on the CN2 rule mining\nalgorithm, that distills the policy into a rule-based decision system. At the\ncore of our approach is the fact that an RL process does not just learn a\npolicy, a mapping from states to actions, but also produces extra\nmeta-information, such as action values indicating the quality of alternative\nactions. This meta-information can indicate whether more than one action is\nnear-optimal for a certain state. We extend CN2 to make it able to leverage\nknowledge about equally-good actions to distill the policy into fewer rules,\nincreasing its interpretability by a person. Then, to ensure that the rules\nexplain a valid, non-degenerate policy, we introduce a refinement algorithm\nthat fine-tunes the rules to obtain good performance when executed in the\nenvironment. We demonstrate the applicability of our algorithm on the Mario AI\nbenchmark, a complex task that requires modern reinforcement learning\nalgorithms including neural networks. The explanations we produce capture the\nlearned policy in only a few rules, that allow a person to understand what the\nblack-box agent learned. Source code:\nhttps://gitlab.ai.vub.ac.be/yocoppen/svcn2",
    "descriptor": "\nComments: 17 pages, 4 figures. The final authenticated publication is available online at this https URL\n",
    "authors": [
      "Youri Coppens",
      "Denis Steckelmacher",
      "Catholijn M. Jonker",
      "Ann Now\u00e9"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06009"
  },
  {
    "id": "arXiv:2106.06010",
    "title": "Machine Learning Framework for Sensing and Modeling Interference in IoT  Frequency Bands",
    "abstract": "Spectrum scarcity has surfaced as a prominent concern in wireless radio\ncommunications with the emergence of new technologies over the past few years.\nAs a result, there is growing need for better understanding of the spectrum\noccupancy with newly emerging access technologies supporting the Internet of\nThings. In this paper, we present a framework to capture and model the traffic\nbehavior of short-time spectrum occupancy for IoT applications in the shared\nbands to determine the existing interference. The proposed capturing method\nutilizes a software defined radio to monitor the short bursts of IoT\ntransmissions by capturing the time series data which is converted to power\nspectral density to extract the observed occupancy. Furthermore, we propose the\nuse of an unsupervised machine learning technique to enhance conventionally\nimplemented energy detection methods. Our experimental results show that the\ntemporal and frequency behavior of the spectrum can be well-captured using the\ncombination of two models, namely, semi-Markov chains and a\nPoisson-distribution arrival rate. We conduct an extensive measurement campaign\nin different urban environments and incorporate the spatial effect on the IoT\nshared spectrum.",
    "descriptor": "",
    "authors": [
      "Bassel Al Homssi",
      "Akram Al-Hourani",
      "Zarko Krusevac",
      "Wayne S T Rowe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06010"
  },
  {
    "id": "arXiv:2106.06011",
    "title": "A self-adapting super-resolution structures framework for automatic  design of GAN",
    "abstract": "With the development of deep learning, the single super-resolution image\nreconstruction network models are becoming more and more complex. Small changes\nin hyperparameters of the models have a greater impact on model performance. In\nthe existing works, experts have gradually explored a set of optimal model\nparameters based on empirical values or performing brute-force search. In this\npaper, we introduce a new super-resolution image reconstruction generative\nadversarial network framework, and a Bayesian optimization method used to\noptimizing the hyperparameters of the generator and discriminator. The\ngenerator is made by self-calibrated convolution, and discriminator is made by\nconvolution lays. We have defined the hyperparameters such as the number of\nnetwork layers and the number of neurons. Our method adopts Bayesian\noptimization as a optimization policy of GAN in our model. Not only can find\nthe optimal hyperparameter solution automatically, but also can construct a\nsuper-resolution image reconstruction network, reducing the manual workload.\nExperiments show that Bayesian optimization can search the optimal solution\nearlier than the other two optimization algorithms.",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Yibo Guo",
      "Haidi Wang",
      "Yiming Fan",
      "Shunyao Li",
      "Mingliang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.06011"
  },
  {
    "id": "arXiv:2106.06012",
    "title": "Within-layer Diversity Reduces Generalization Gap",
    "abstract": "Neural networks are composed of multiple layers arranged in a hierarchical\nstructure jointly trained with a gradient-based optimization, where the errors\nare back-propagated from the last layer back to the first one. At each\noptimization step, neurons at a given layer receive feedback from neurons\nbelonging to higher layers of the hierarchy. In this paper, we propose to\ncomplement this traditional 'between-layer' feedback with additional\n'within-layer' feedback to encourage diversity of the activations within the\nsame layer. To this end, we measure the pairwise similarity between the outputs\nof the neurons and use it to model the layer's overall diversity. By penalizing\nsimilarities and promoting diversity, we encourage each neuron to learn a\ndistinctive representation and, thus, to enrich the data representation learned\nwithin the layer and to increase the total capacity of the model. We\ntheoretically study how the within-layer activation diversity affects the\ngeneralization performance of a neural network and prove that increasing the\ndiversity of hidden activations reduces the estimation error. In addition to\nthe theoretical guarantees, we present an empirical study on three datasets\nconfirming that the proposed approach enhances the performance of\nstate-of-the-art neural network models and decreases the generalization gap.",
    "descriptor": "\nComments: 18 pages, 1 figure, 3 Tables\n",
    "authors": [
      "Firas Laakom",
      "Jenni Raitoharju",
      "Alexandros Iosifidis",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06012"
  },
  {
    "id": "arXiv:2106.06017",
    "title": "Cross-lingual Emotion Detection",
    "abstract": "Emotion detection is of great importance for understanding humans.\nConstructing annotated datasets to train automated models can be expensive. We\nexplore the efficacy of cross-lingual approaches that would use data from a\nsource language to build models for emotion detection in a target language. We\ncompare three approaches, namely: i) using inherently multilingual models; ii)\ntranslating training data into the target language; and iii) using an\nautomatically tagged parallel corpus. In our study, we consider English as the\nsource language with Arabic and Spanish as target languages. We study the\neffectiveness of different classification models such as BERT and SVMs trained\nwith different features. Our BERT-based monolingual models that are trained on\ntarget language data surpass state-of-the-art (SOTA) by 4% and 5% absolute\nJaccard score for Arabic and Spanish respectively. Next, we show that using\ncross-lingual approaches with English data alone, we can achieve more than 90%\nand 80% relative effectiveness of the Arabic and Spanish BERT models\nrespectively. Lastly, we use LIME to interpret the differences between models.",
    "descriptor": "",
    "authors": [
      "Sabit Hassan",
      "Shaden Shaar",
      "Kareem Darwish"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06017"
  },
  {
    "id": "arXiv:2106.06019",
    "title": "Analysis of Evolved Response Thresholds for Decentralized Dynamic Task  Allocation",
    "abstract": "We investigate the application of a multi-objective genetic algorithm to the\nproblem of task allocation in a self-organizing, decentralized, threshold-based\nswarm. Each agent in our system is capable of performing four tasks with a\nresponse threshold for each, and we seek to assign response threshold values to\nall of the agents a swarm such that the collective behavior of the swarm is\noptimized. Random assignment of threshold values according to a uniform\ndistribution is known to be effective; however, this method does not consider\nfeatures of particular problem instances. Dynamic response thresholds have some\nflexibility to address problem specific features through real-time adaptivity,\noften improving swarm performance.\nIn this work, we use a multi-objective genetic algorithm to evolve response\nthresholds for a simulated swarm engaged in a dynamic task allocation problem:\ntwo-dimensional collective tracking. We show that evolved thresholds not only\noutperform uniformly distributed thresholds and dynamic thresholds but achieve\nnearly optimal performance on a variety of tracking problem instances (target\npaths). More importantly, we demonstrate that thresholds evolved for one of\nseveral problem instances generalize to all other problem instances eliminating\nthe need to evolve new thresholds for each problem to be solved. We analyze the\nproperties that allow these paths to serve as universal training instances and\nshow that they are quite natural.",
    "descriptor": "\nComments: 22 pages, 12 figures\n",
    "authors": [
      "H. David Mathias",
      "Annie S. Wu",
      "Daniel Dang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.06019"
  },
  {
    "id": "arXiv:2106.06020",
    "title": "Coordinate Independent Convolutional Networks -- Isometry and Gauge  Equivariant Convolutions on Riemannian Manifolds",
    "abstract": "Motivated by the vast success of deep convolutional networks, there is a\ngreat interest in generalizing convolutions to non-Euclidean manifolds. A major\ncomplication in comparison to flat spaces is that it is unclear in which\nalignment a convolution kernel should be applied on a manifold. The underlying\nreason for this ambiguity is that general manifolds do not come with a\ncanonical choice of reference frames (gauge). Kernels and features therefore\nhave to be expressed relative to arbitrary coordinates. We argue that the\nparticular choice of coordinatization should not affect a network's inference\n-- it should be coordinate independent. A simultaneous demand for coordinate\nindependence and weight sharing is shown to result in a requirement on the\nnetwork to be equivariant under local gauge transformations (changes of local\nreference frames). The ambiguity of reference frames depends thereby on the\nG-structure of the manifold, such that the necessary level of gauge\nequivariance is prescribed by the corresponding structure group G. Coordinate\nindependent convolutions are proven to be equivariant w.r.t. those isometries\nthat are symmetries of the G-structure. The resulting theory is formulated in a\ncoordinate free fashion in terms of fiber bundles. To exemplify the design of\ncoordinate independent convolutions, we implement a convolutional network on\nthe M\\\"obius strip. The generality of our differential geometric formulation of\nconvolutional networks is demonstrated by an extensive literature review which\nexplains a large number of Euclidean CNNs, spherical CNNs and CNNs on general\nsurfaces as specific instances of coordinate independent convolutions.",
    "descriptor": "\nComments: The implementation of orientation independent M\\\"obius convolutions is publicly available at this https URL\n",
    "authors": [
      "Maurice Weiler",
      "Patrick Forr\u00e9",
      "Erik Verlinde",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06020"
  },
  {
    "id": "arXiv:2106.06022",
    "title": "IoT Virtualization with ML-based Information Extraction",
    "abstract": "For IoT to reach its full potential, the sharing and reuse of information in\ndifferent applications and across verticals is of paramount importance.\nHowever, there are a plethora of IoT platforms using different representations,\nprotocols and interaction patterns. To address this issue, the Fed4IoT project\nhas developed an IoT virtualization platform that, on the one hand, integrates\ninformation from many different source platforms and, on the other hand, makes\nthe information required by the respective users available in the target\nplatform of choice. To enable this, information is translated into a common,\nneutral exchange format. The format of choice is NGSI-LD, which is being\nstandardized by the ETSI Industry Specification Group on Context Information\nManagement (ETSI ISG CIM). Thing Visors are the components that translate the\nsource information to NGSI-LD, which is then delivered to the target platform\nand translated into the target format. ThingVisors can be implemented by hand,\nbut this requires significant human effort, especially considering the\nheterogeneity of low level information produced by a multitude of sensors.\nThus, supporting the human developer and, ideally, fully automating the process\nof extracting and enriching data and translating it to NGSI-LD is a crucial\nstep. Machine learning is a promising approach for this, but it typically\nrequires large amounts of hand-labelled data for training, an effort that makes\nit unrealistic in many IoT scenarios. A programmatic labelling approach called\nknowledge infusion that encodes expert knowledge is used for matching a schema\nor ontology extracted from the data with a target schema or ontology, providing\nthe basis for annotating the data and facilitating the translation to NGSI-LD.",
    "descriptor": "",
    "authors": [
      "Martin Bauer"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.06022"
  },
  {
    "id": "arXiv:2106.06026",
    "title": "Hardness of Approximate Diameter: Now for Undirected Graphs",
    "abstract": "Approximating the graph diameter is a basic task of both theoretical and\npractical interest. A simple folklore algorithm can output a 2-approximation to\nthe diameter in linear time by running BFS from an arbitrary vertex. It has\nbeen open whether a better approximation is possible in near-linear time. A\nseries of papers on fine-grained complexity have led to strong hardness results\nfor diameter in directed graphs, culminating in a recent tradeoff curve\nindependently discovered by [Li, STOC'21] and [Dalirrooyfard and Wein,\nSTOC'21], showing that under the Strong Exponential Time Hypothesis (SETH), for\nany integer $k\\ge 2$ and $\\delta>0$, a $2-\\frac{1}{k}-\\delta$ approximation for\ndiameter in directed $m$-edge graphs requires $mn^{1+1/(k-1)-o(1)}$ time. In\nparticular, the simple linear time $2$-approximation algorithm is optimal for\ndirected graphs.\nIn this paper we prove that the same tradeoff lower bound curve is possible\nfor undirected graphs as well, extending results of [Roditty and Vassilevska\nW., STOC'13], [Li'20] and [Bonnet, ICALP'21] who proved the first few cases of\nthe curve, $k=2,3$ and $4$, respectively. Our result shows in particular that\nthe simple linear time $2$-approximation algorithm is also optimal for\nundirected graphs. To obtain our result we develop new tools for fine-grained\nreductions that could be useful for proving SETH-based hardness for other\nproblems in undirected graphs related to distance computation.",
    "descriptor": "",
    "authors": [
      "Mina Dalirrooyfard",
      "Ray Li",
      "Virginia Vassilevska Williams"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.06026"
  },
  {
    "id": "arXiv:2106.06027",
    "title": "Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm",
    "abstract": "Sparse adversarial attacks can fool deep neural networks (DNNs) by only\nperturbing a few pixels (regularized by l_0 norm). Recent efforts combine it\nwith another l_infty imperceptible on the perturbation magnitudes. The\nresultant sparse and imperceptible attacks are practically relevant, and\nindicate an even higher vulnerability of DNNs that we usually imagined.\nHowever, such attacks are more challenging to generate due to the optimization\ndifficulty by coupling the l_0 regularizer and box constraints with a\nnon-convex objective. In this paper, we address this challenge by proposing a\nhomotopy algorithm, to jointly tackle the sparsity and the perturbation bound\nin one unified framework. Each iteration, the main step of our algorithm is to\noptimize an l_0-regularized adversarial loss, by leveraging the nonmonotone\nAccelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is\nfollowed by an l_0 change control step, and an optional post-attack step\ndesigned to escape bad local minima. We also extend the algorithm to handling\nthe structural sparsity regularizer. We extensively examine the effectiveness\nof our proposed homotopy attack for both targeted and non-targeted attack\nscenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art\nmethods, our homotopy attack leads to significantly fewer perturbations, e.g.,\nreducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted\nattack), at similar maximal perturbation magnitudes, when still achieving 100%\nattack success rates. Our codes are available at:\nhttps://github.com/VITA-Group/SparseADV_Homotopy.",
    "descriptor": "",
    "authors": [
      "Mingkang Zhu",
      "Tianlong Chen",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06027"
  },
  {
    "id": "arXiv:2106.06033",
    "title": "Deep Probabilistic Koopman: Long-term time-series forecasting under  periodic uncertainties",
    "abstract": "Probabilistic forecasting of complex phenomena is paramount to various\nscientific disciplines and applications. Despite the generality and importance\nof the problem, general mathematical techniques that allow for stable long-term\nforecasts with calibrated uncertainty measures are lacking. For most time\nseries models, the difficulty of obtaining accurate probabilistic future time\nstep predictions increases with the prediction horizon. In this paper, we\nintroduce a surprisingly simple approach that characterizes time-varying\ndistributions and enables reasonably accurate predictions thousands of\ntimesteps into the future. This technique, which we call Deep Probabilistic\nKoopman (DPK), is based on recent advances in linear Koopman operator theory,\nand does not require time stepping for future time predictions. Koopman models\nalso tend to have a small parameter footprint (often less than 10,000\nparameters). We demonstrate the long-term forecasting performance of these\nmodels on a diversity of domains, including electricity demand forecasting,\natmospheric chemistry, and neuroscience. For electricity demand modeling, our\ndomain-agnostic technique outperforms all of 177 domain-specific competitors in\nthe most recent Global Energy Forecasting Competition.",
    "descriptor": "\nComments: 16 pages, 10 figures, submitted to NeurIPS 2021\n",
    "authors": [
      "Alex Mallen",
      "Henning Lange",
      "J. Nathan Kutz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06033"
  },
  {
    "id": "arXiv:2106.06037",
    "title": "Small space and streaming pattern matching with k edits",
    "abstract": "In this work, we revisit the fundamental and well-studied problem of\napproximate pattern matching under edit distance. Given an integer $k$, a\npattern $P$ of length $m$, and a text $T$ of length $n \\ge m$, the task is to\nfind substrings of $T$ that are within edit distance $k$ from $P$. Our main\nresult is a streaming algorithm that solves the problem in $\\tilde{O}(k^5)$\nspace and $\\tilde{O}(k^8)$ amortised time per character of the text, providing\nanswers correct with high probability. (Hereafter, $\\tilde{O}(\\cdot)$ hides a\n$\\mathrm{poly}(\\log n)$ factor.) This answers a decade-old question: since the\ndiscovery of a $\\mathrm{poly}(k\\log n)$-space streaming algorithm for pattern\nmatching under Hamming distance by Porat and Porat [FOCS 2009], the existence\nof an analogous result for edit distance remained open. Up to this work, no\n$\\mathrm{poly}(k\\log n)$-space algorithm was known even in the simpler\nsemi-streaming model, where $T$ comes as a stream but $P$ is available for\nread-only access. In this model, we give a deterministic algorithm that\nachieves slightly better complexity.\nIn order to develop the fully streaming algorithm, we introduce a new edit\ndistance sketch parametrised by integers $n\\ge k$. For any string of length at\nmost $n$, the sketch is of size $\\tilde{O}(k^2)$ and it can be computed with an\n$\\tilde{O}(k^2)$-space streaming algorithm. Given the sketches of two strings,\nin $\\tilde{O}(k^3)$ time we can compute their edit distance or certify that it\nis larger than $k$. This result improves upon $\\tilde{O}(k^8)$-size sketches of\nBelazzougui and Zhu [FOCS 2016] and very recent $\\tilde{O}(k^3)$-size sketches\nof Jin, Nelson, and Wu [STACS 2021].",
    "descriptor": "",
    "authors": [
      "Tomasz Kociumaka",
      "Ely Porat",
      "Tatiana Starikovskaya"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.06037"
  },
  {
    "id": "arXiv:2106.06038",
    "title": "Modeling Hierarchical Structures with Continuous Recursive Neural  Networks",
    "abstract": "Recursive Neural Networks (RvNNs), which compose sequences according to their\nunderlying hierarchical syntactic structure, have performed well in several\nnatural language processing tasks compared to similar models without structural\nbiases. However, traditional RvNNs are incapable of inducing the latent\nstructure in a plain text sequence on their own. Several extensions have been\nproposed to overcome this limitation. Nevertheless, these extensions tend to\nrely on surrogate gradients or reinforcement learning at the cost of higher\nbias or variance. In this work, we propose Continuous Recursive Neural Network\n(CRvNN) as a backpropagation-friendly alternative to address the aforementioned\nlimitations. This is done by incorporating a continuous relaxation to the\ninduced structure. We demonstrate that CRvNN achieves strong performance in\nchallenging synthetic tasks such as logical inference and ListOps. We also show\nthat CRvNN performs comparably or better than prior latent structure models on\nreal-world tasks such as sentiment analysis and natural language inference.",
    "descriptor": "\nComments: Accepted in ICML 2021 (long talk)\n",
    "authors": [
      "Jishnu Ray Chowdhury",
      "Cornelia Caragea"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06038"
  },
  {
    "id": "arXiv:2106.06039",
    "title": "Neural Higher-order Pattern (Motif) Prediction in Temporal Networks",
    "abstract": "Dynamic systems that consist of a set of interacting elements can be\nabstracted as temporal networks. Recently, higher-order patterns that involve\nmultiple interacting nodes have been found crucial to indicate domain-specific\nlaws of different temporal networks. This posts us the challenge of designing\nmore sophisticated hypergraph models for these higher-order patterns and the\nassociated new learning algorithms. Here, we propose the first model, named\nHIT, for higher-order pattern prediction in temporal hypergraphs. Particularly,\nwe focus on predicting three types of common but important interaction patterns\ninvolving three interacting elements in temporal networks, which could be\nextended to even higher-order patterns. HIT extracts the structural\nrepresentation of a node triplet of interest on the temporal hypergraph and\nuses it to tell what type of, when, and why the interaction expansion could\nhappen in this triplet. HIT could achieve significant improvement(averaged 20%\nAUC gain to identify the interaction type, uniformly more accurate time\nestimation) compared to both heuristic and other neural-network-based baselines\non 5 real-world large temporal hypergraphs. Moreover, HIT provides a certain\ndegree of interpretability by identifying the most discriminatory structural\nfeatures on the temporal hypergraphs for predicting different higher-order\npatterns.",
    "descriptor": "",
    "authors": [
      "Yunyu Liu",
      "Jianzhu Ma",
      "Pan Li"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06039"
  },
  {
    "id": "arXiv:2106.06041",
    "title": "Adversarial purification with Score-based generative models",
    "abstract": "While adversarial training is considered as a standard defense method against\nadversarial attacks for image classifiers, adversarial purification, which\npurifies attacked images into clean images with a standalone purification\nmodel, has shown promises as an alternative defense method. Recently, an\nEnergy-Based Model (EBM) trained with Markov-Chain Monte-Carlo (MCMC) has been\nhighlighted as a purification model, where an attacked image is purified by\nrunning a long Markov-chain using the gradients of the EBM. Yet, the\npracticality of the adversarial purification using an EBM remains questionable\nbecause the number of MCMC steps required for such purification is too large.\nIn this paper, we propose a novel adversarial purification method based on an\nEBM trained with Denoising Score-Matching (DSM). We show that an EBM trained\nwith DSM can quickly purify attacked images within a few steps. We further\nintroduce a simple yet effective randomized purification scheme that injects\nrandom noises into images before purification. This process screens the\nadversarial perturbations imposed on images by the random noises and brings the\nimages to the regime where the EBM can denoise well. We show that our\npurification method is robust against various attacks and demonstrate its\nstate-of-the-art performances.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Jongmin Yoon",
      "Sung Ju Hwang",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06041"
  },
  {
    "id": "arXiv:2106.06042",
    "title": "FedBABU: Towards Enhanced Representation for Federated Image  Classification",
    "abstract": "Federated learning has evolved to improve a single global model under data\nheterogeneity (as a curse) or to develop multiple personalized models using\ndata heterogeneity (as a blessing). However, there has been little research\nconsidering both directions simultaneously. In this paper, we first investigate\nthe relationship between them by analyzing Federated Averaging at the client\nlevel and determine that a better federated global model performance does not\nconstantly improve personalization. To elucidate the cause of this\npersonalization performance degradation problem, we decompose the entire\nnetwork into the body (i.e., extractor), related to universality, and the head\n(i.e., classifier), related to personalization. We then point out that this\nproblem stems from training the head. Based on this observation, we propose a\nnovel federated learning algorithm, coined as FedBABU, which updates only the\nbody of the model during federated training (i.e., the head is randomly\ninitialized and never updated), and the head is fine-tuned for personalization\nduring the evaluation process. Extensive experiments show consistent\nperformance improvements and an efficient personalization of FedBABU.",
    "descriptor": "\nComments: 21 pages, 9 figures, 20 tables (NeurIPS 2021 under review)\n",
    "authors": [
      "Jaehoon Oh",
      "Sangmook Kim",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06042"
  },
  {
    "id": "arXiv:2106.06046",
    "title": "Information Theoretic Evaluation of Privacy-Leakage, Interpretability,  and Transferability for a Novel Trustworthy AI Framework",
    "abstract": "Guidelines and principles of trustworthy AI should be adhered to in practice\nduring the development of AI systems. This work suggests a novel information\ntheoretic trustworthy AI framework based on the hypothesis that information\ntheory enables taking into account the ethical AI principles during the\ndevelopment of machine learning and deep learning models via providing a way to\nstudy and optimize the inherent tradeoffs between trustworthy AI principles. A\nunified approach to \"privacy-preserving interpretable and transferable\nlearning\" is presented via introducing the information theoretic measures for\nprivacy-leakage, interpretability, and transferability. A technique based on\nvariational optimization, employing conditionally deep autoencoders, is\ndeveloped for practically calculating the defined information theoretic\nmeasures for privacy-leakage, interpretability, and transferability.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2105.04615, arXiv:2104.07060\n",
    "authors": [
      "Mohit Kumar",
      "Bernhard A. Moser",
      "Lukas Fischer",
      "Bernhard Freudenthaler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06046"
  },
  {
    "id": "arXiv:2106.06047",
    "title": "Rethinking Architecture Design for Tackling Data Heterogeneity in  Federated Learning",
    "abstract": "Federated learning is an emerging research paradigm enabling collaborative\ntraining of machine learning models among different organizations while keeping\ndata private at each institution. Despite recent progress, there remain\nfundamental challenges such as lack of convergence and potential for\ncatastrophic forgetting in federated learning across real-world heterogeneous\ndevices. In this paper, we demonstrate that attention-based architectures\n(e.g., Transformers) are fairly robust to distribution shifts and hence improve\nfederated learning over heterogeneous data. Concretely, we conduct the first\nrigorous empirical investigation of different neural architectures across a\nrange of federated algorithms, real-world benchmarks, and heterogeneous data\nsplits. Our experiments show that simply replacing convolutional networks with\nTransformers can greatly reduce catastrophic forgetting of previous devices,\naccelerate convergence, and reach a better global model, especially when\ndealing with heterogeneous data. We will release our code and pretrained models\nat https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in\nrobust architectures as an alternative to current research efforts on the\noptimization front.",
    "descriptor": "",
    "authors": [
      "Liangqiong Qu",
      "Yuyin Zhou",
      "Paul Pu Liang",
      "Yingda Xia",
      "Feifei Wang",
      "Li Fei-Fei",
      "Ehsan Adeli",
      "Daniel Rubin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06047"
  },
  {
    "id": "arXiv:2106.06048",
    "title": "High-Performance FPGA-based Accelerator for Bayesian Recurrent Neural  Networks",
    "abstract": "Neural networks have demonstrated their great performance in a wide range of\ntasks. Especially in time-series analysis, recurrent architectures based on\nlong-short term memory (LSTM) cells have manifested excellent capability to\nmodel time dependencies in real-world data. However, standard recurrent\narchitectures cannot estimate their uncertainty which is essential for\nsafety-critical applications such as in medicine. In contrast, Bayesian\nrecurrent neural networks (RNNs) are able to provide uncertainty estimation\nwith improved accuracy. Nonetheless, Bayesian RNNs are computationally and\nmemory demanding, which limits their practicality despite their advantages. To\naddress this issue, we propose an FPGA-based hardware design to accelerate\nBayesian LSTM-based RNNs. To further improve the overall algorithmic-hardware\nperformance, a co-design framework is proposed to explore the most optimal\nalgorithmic-hardware configurations for Bayesian RNNs. We conduct extensive\nexperiments on health-related tasks to demonstrate the improvement of our\ndesign and the effectiveness of our framework. Compared with GPU\nimplementation, our FPGA-based design can achieve up to 10 times speedup with\nnearly 106 times higher energy efficiency. To the best of our knowledge, this\nis the first work targeting the acceleration of Bayesian RNNs on FPGAs.",
    "descriptor": "\nComments: 9 pages. Martin Ferianc and Zhiqiang Que share an equal contribution\n",
    "authors": [
      "Martin Ferianc",
      "Zhiqiang Que",
      "Hongxiang Fan",
      "Wayne Luk",
      "Miguel Rodrigues"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06048"
  },
  {
    "id": "arXiv:2106.06049",
    "title": "FiSH: Fair Spatial Hotspots",
    "abstract": "Pervasiveness of tracking devices and enhanced availability of spatially\nlocated data has deepened interest in using them for various policy\ninterventions, through computational data analysis tasks such as spatial hot\nspot detection. In this paper, we consider, for the first time to our best\nknowledge, fairness in detecting spatial hot spots. We motivate the need for\nensuring fairness through statistical parity over the collective population\ncovered across chosen hot spots. We then characterize the task of identifying a\ndiverse set of solutions in the noteworthiness-fairness trade-off spectrum, to\nempower the user to choose a trade-off justified by the policy domain. Being a\nnovel task formulation, we also develop a suite of evaluation metrics for fair\nhot spots, motivated by the need to evaluate pertinent aspects of the task. We\nillustrate the computational infeasibility of identifying fair hot spots using\nnaive and/or direct approaches and devise a method, codenamed {\\it FiSH}, for\nefficiently identifying high-quality, fair and diverse sets of spatial hot\nspots. FiSH traverses the tree-structured search space using heuristics that\nguide it towards identifying effective and fair sets of spatial hot spots.\nThrough an extensive empirical analysis over a real-world dataset from the\ndomain of human development, we illustrate that FiSH generates high-quality\nsolutions at fast response times.",
    "descriptor": "",
    "authors": [
      "Deepak P",
      "Sowmya S Sundaram"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06049"
  },
  {
    "id": "arXiv:2106.06051",
    "title": "Well-Balanced Allocation on General Graphs",
    "abstract": "We study the graphical generalization of the 2-choice balls-into-bins\nprocess, where rather than choosing any two random bins, the bins correspond to\nvertices of an underlying graph, and only the bins connected by an edge can be\nchosen.\nFor any $k(n)$ edge-connected, $d(n)$-regular graph on $n$ vertices and any\nnumber of balls, we give an allocation strategy which guarantees that the\nmaximum gap between the bin loads is $O((d/k) \\log^4n \\log \\log n)$, with high\nprobability. We further show that the dependence on $k$ is tight and give an\n$\\Omega((d/k) + \\log n)$ lower bound on the gap achievable by any allocation\nstrategy, for any graph $G$. In particular, our result gives polylogarithmic\nbounds for natural graphs such as cycles and tori, where the classical greedy\nallocation appears to result in a polynomial gap. Previously such a bound was\nknown only for graphs with good expansion.\nThe construction is based on defining certain orthogonal flows on cut-based\nR\\\"{a}cke decomposition of graphs. The allocation algorithm itself, however, is\nsimple to implement and takes only $O(\\log(n))$ time per allocation, and can be\nviewed as a global version of the greedy strategy that compares average load on\nsets of vertices, rather than on individual vertices.",
    "descriptor": "",
    "authors": [
      "Nikhil Bansal",
      "Ohad Feldheim"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.06051"
  },
  {
    "id": "arXiv:2106.06052",
    "title": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic  Next-Generation Benchmarking",
    "abstract": "We introduce Dynaboard, an evaluation-as-a-service framework for hosting\nbenchmarks and conducting holistic model comparison, integrated with the\nDynabench platform. Our platform evaluates NLP models directly instead of\nrelying on self-reported metrics or predictions on a single dataset. Under this\nparadigm, models are submitted to be evaluated in the cloud, circumventing the\nissues of reproducibility, accessibility, and backwards compatibility that\noften hinder benchmarking in NLP. This allows users to interact with uploaded\nmodels in real time to assess their quality, and permits the collection of\nadditional metrics such as memory use, throughput, and robustness, which --\ndespite their importance to practitioners -- have traditionally been absent\nfrom leaderboards. On each task, models are ranked according to the Dynascore,\na novel utility-based aggregation of these statistics, which users can\ncustomize to better reflect their preferences, placing more/less weight on a\nparticular axis of evaluation or dataset. As state-of-the-art NLP models push\nthe limits of traditional benchmarks, Dynaboard offers a standardized solution\nfor a more diverse and comprehensive evaluation of model quality.",
    "descriptor": "",
    "authors": [
      "Zhiyi Ma",
      "Kawin Ethayarajh",
      "Tristan Thrush",
      "Somya Jain",
      "Ledell Wu",
      "Robin Jia",
      "Christopher Potts",
      "Adina Williams",
      "Douwe Kiela"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06052"
  },
  {
    "id": "arXiv:2106.06053",
    "title": "Identifying and Supporting Financially Vulnerable Consumers in a  Privacy-Preserving Manner: A Use Case Using Decentralised Identifiers and  Verifiable Credentials",
    "abstract": "Vulnerable individuals have a limited ability to make reasonable financial\ndecisions and choices and, thus, the level of care that is appropriate to be\nprovided to them by financial institutions may be different from that required\nfor other consumers. Therefore, identifying vulnerability is of central\nimportance for the design and effective provision of financial services and\nproducts. However, validating the information that customers share and\nrespecting their privacy are both particularly important in finance and this\nposes a challenge for identifying and caring for vulnerable populations. This\nposition paper examines the potential of the combination of two emerging\ntechnologies, Decentralized Identifiers (DIDs) and Verifiable Credentials\n(VCs), for the identification of vulnerable consumers in finance in an\nefficient and privacy-preserving manner.",
    "descriptor": "\nComments: Published in the ACM CHI 2021 workshop on Designing for New Forms of Vulnerability\n",
    "authors": [
      "Tasos Spiliotopoulos",
      "Dave Horsfall",
      "Magdalene Ng",
      "Kovila Coopamootoo",
      "Aad van Moorsel",
      "Karen Elliott"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.06053"
  },
  {
    "id": "arXiv:2106.06054",
    "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data  Transformers in Machine Learning Pipeline",
    "abstract": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
    "descriptor": "\nComments: ESEC/FSE'2021: The 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021\n",
    "authors": [
      "Sumon Biswas",
      "Hridesh Rajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06054"
  },
  {
    "id": "arXiv:2106.06056",
    "title": "Progressive-Scale Boundary Blackbox Attack via Projective Gradient  Estimation",
    "abstract": "Boundary based blackbox attack has been recognized as practical and\neffective, given that an attacker only needs to access the final model\nprediction. However, the query efficiency of it is in general high especially\nfor high dimensional image data. In this paper, we show that such efficiency\nhighly depends on the scale at which the attack is applied, and attacking at\nthe optimal scale significantly improves the efficiency. In particular, we\npropose a theoretical framework to analyze and show three key characteristics\nto improve the query efficiency. We prove that there exists an optimal scale\nfor projective gradient estimation. Our framework also explains the\nsatisfactory performance achieved by existing boundary black-box attacks. Based\non our theoretical framework, we propose Progressive-Scale enabled projective\nBoundary Attack (PSBA) to improve the query efficiency via progressive scaling\ntechniques. In particular, we employ Progressive-GAN to optimize the scale of\nprojections, which we call PSBA-PGAN. We evaluate our approach on both spatial\nand frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and\nImageNet against different models including a real-world face recognition API\nshow that PSBA-PGAN significantly outperforms existing baseline attacks in\nterms of query efficiency and attack success rate. We also observe relatively\nstable optimal scales for different models and datasets. The code is publicly\navailable at https://github.com/AI-secure/PSBA.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Jiawei Zhang",
      "Linyi Li",
      "Huichen Li",
      "Xiaolu Zhang",
      "Shuang Yang",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06056"
  },
  {
    "id": "arXiv:2106.06057",
    "title": "Domain Transformer: Predicting Samples of Unseen, Future Domains",
    "abstract": "The data distribution commonly evolves over time leading to problems such as\nconcept drift that often decrease classifier performance. We seek to predict\nunseen data (and their labels) allowing us to tackle challenges due to a\nnon-constant data distribution in a \\emph{proactive} manner rather than\ndetecting and reacting to already existing changes that might already have led\nto errors. To this end, we learn a domain transformer in an unsupervised manner\nthat allows generating data of unseen domains. Our approach first matches\nindependently learned latent representations of two given domains obtained from\nan auto-encoder using a Cycle-GAN. In turn, a transformation of the original\nsamples can be learned that can be applied iteratively to extrapolate to unseen\ndomains. Our evaluation on CNNs on image data confirms the usefulness of the\napproach. It also achieves very good results on the well-known problem of\nunsupervised domain adaption, where labels but not samples have to be\npredicted.",
    "descriptor": "",
    "authors": [
      "Johannes Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06057"
  },
  {
    "id": "arXiv:2106.06059",
    "title": "Predicting Next Local Appearance for Video Anomaly Detection",
    "abstract": "We present a local anomaly detection method in videos. As opposed to most\nexisting methods that are computationally expensive and are not very\ngeneralizable across different video scenes, we propose an adversarial\nframework that learns the temporal local appearance variations by predicting\nthe appearance of a normally behaving object in the next frame of a scene by\nonly relying on its current and past appearances. In the presence of an\nabnormally behaving object, the reconstruction error between the real and the\npredicted next appearance of that object indicates the likelihood of an\nanomaly. Our method is competitive with the existing state-of-the-art while\nbeing significantly faster for both training and inference and being better at\ngeneralizing to unseen video scenes.",
    "descriptor": "\nComments: Accepted as an oral presentation for MVA'2021\n",
    "authors": [
      "Pankaj Raj Roy",
      "Guillaume-Alexandre Bilodeau",
      "Lama Seoud"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06059"
  },
  {
    "id": "arXiv:2106.06060",
    "title": "Achieving Diverse Objectives with AI-driven Prices in Deep Reinforcement  Learning Multi-agent Markets",
    "abstract": "We propose a practical approach to computing market prices and allocations\nvia a deep reinforcement learning policymaker agent, operating in an\nenvironment of other learning agents. Compared to the idealized market\nequilibrium outcome -- which we use as a benchmark -- our policymaker is much\nmore flexible, allowing us to tune the prices with regard to diverse objectives\nsuch as sustainability and resource wastefulness, fairness, buyers' and\nsellers' welfare, etc. To evaluate our approach, we design a realistic market\nwith multiple and diverse buyers and sellers. Additionally, the sellers, which\nare deep learning agents themselves, compete for resources in a common-pool\nappropriation environment based on bio-economic models of commercial fisheries.\nWe demonstrate that: (a) The introduced policymaker is able to achieve\ncomparable performance to the market equilibrium, showcasing the potential of\nsuch approaches in markets where the equilibrium prices can not be efficiently\ncomputed. (b) Our policymaker can notably outperform the equilibrium solution\non certain metrics, while at the same time maintaining comparable performance\nfor the remaining ones. (c) As a highlight of our findings, our policymaker is\nsignificantly more successful in maintaining resource sustainability, compared\nto the market outcome, in scarce resource environments.",
    "descriptor": "",
    "authors": [
      "Panayiotis Danassis",
      "Aris Filos-Ratsikas",
      "Boi Faltings"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.06060"
  },
  {
    "id": "arXiv:2106.06061",
    "title": "Data-driven battery operation for energy arbitrage using rainbow deep  reinforcement learning",
    "abstract": "As the world seeks to become more sustainable, intelligent solutions are\nneeded to increase the penetration of renewable energy. In this paper, the\nmodel-free deep reinforcement learning algorithm Rainbow Deep Q-Networks is\nused to control a battery in a small microgrid to perform energy arbitrage and\nmore efficiently utilise solar and wind energy sources. The grid operates with\nits own demand and renewable generation based on a dataset collected at Keele\nUniversity, as well as using dynamic energy pricing from a real wholesale\nenergy market. Four scenarios are tested including using demand and price\nforecasting produced with local weather data. The algorithm and its\nsubcomponents are evaluated against two continuous control benchmarks with\nRainbow able to outperform all other method. This research shows the importance\nof using the distributional approach for reinforcement learning when working\nwith complex environments and reward functions, as well as how it can be used\nto visualise and contextualise the agent's behaviour for real-world\napplications.",
    "descriptor": "\nComments: 13 pages, 9 figures (17 counting each subfigure)\n",
    "authors": [
      "Daniel J. B. Harrold",
      "Jun Cao",
      "Zhong Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06061"
  },
  {
    "id": "arXiv:2106.06065",
    "title": "Windows Kernel Hijacking Is Not an Option: MemoryRanger Comes to the  Rescue Again",
    "abstract": "The security of a computer system depends on OS kernel protection. It is\ncrucial to reveal and inspect new attacks on kernel data, as these are used by\nhackers. The purpose of this paper is to continue research into attacks on\ndynamically allocated data in the Windows OS kernel and demonstrate the\ncapacity of MemoryRanger to prevent these attacks. This paper discusses three\nnew hijacking attacks on kernel data, which are based on bypassing OS security\nmechanisms. The first two hijacking attacks result in illegal access to files\nopen in exclusive access. The third attack escalates process privileges,\nwithout applying token swapping. Although Windows security experts have issued\nnew protection features, access attempts to the dynamically allocated data in\nthe kernel are not fully controlled. MemoryRanger hypervisor is designed to\nfill this security gap. The updated MemoryRanger prevents these new attacks as\nwell as supporting the Windows 10 1903 x64.",
    "descriptor": "\nComments: 29 pages, 7 figures. Korkin, I. (2021, June 10). Windows Kernel Hijacking Is Not an Option: MemoryRanger Comes to the Rescue Again. Journal of Digital Forensics, Security and Law, Vol 16, No.1, Article 4. Available at: this https URL\n",
    "authors": [
      "Igor Korkin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2106.06065"
  },
  {
    "id": "arXiv:2106.06067",
    "title": "Bayesian Optimisation with Formal Guarantees",
    "abstract": "Application domains of Bayesian optimization include optimizing black-box\nfunctions or very complex functions. The functions we are interested in\ndescribe\ncomplex real-world systems applied in industrial settings. Even though\nthey do have explicit representations, standard optimization\ntechniques fail to provide validated solutions and correctness\nguarantees for them.\nIn this paper we present a combination of Bayesian optimisation and SMT-based\nconstraint solving to achieve safe and stable solutions with optimality\nguarantees.",
    "descriptor": "\nComments: FMCAD-2021\n",
    "authors": [
      "Franz Brau\u00dfe",
      "Zurab Khasidashvili",
      "Konstantin Korovin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.06067"
  },
  {
    "id": "arXiv:2106.06068",
    "title": "Subgame solving without common knowledge",
    "abstract": "In imperfect-information games, subgame solving is significantly more\nchallenging than in perfect-information games, but in the last few years, such\ntechniques have been developed. They were the key ingredient to the milestone\nof superhuman play in no-limit Texas hold'em poker. Current subgame-solving\ntechniques analyze the entire common-knowledge closure of the player's current\ninformation set, that is, the smallest set of nodes within which it is common\nknowledge that the current node lies. However, this set is too large to handle\nin many games. We introduce an approach that overcomes this obstacle, by\ninstead working with only low-order knowledge. Our approach allows an agent,\nupon arriving at an infoset, to basically prune any node that is no longer\nreachable, thereby massively reducing the game tree size relative to the\ncommon-knowledge subgame. We prove that, as is, our approach can increase\nexploitability compared to the blueprint strategy. However, we develop three\navenues by which safety can be guaranteed. First, safety is guaranteed if the\nresults of subgame solves are incorporated back into the blueprint. Second, we\nprovide a method where safety is achieved by limiting the infosets at which\nsubgame solving is performed. Third, we prove that our approach, when applied\nat every infoset reached during play, achieves a weaker notion of equilibrium,\nwhich we coin affine equilibrium, and which may be of independent interest. We\nshow that affine equilibria cannot be exploited by any Nash strategy of the\nopponent, so an opponent who wishes to exploit must open herself to\ncounter-exploitation. Even without the safety-guaranteeing additions,\nexperiments on medium-sized games show that our approach always reduced\nexploitability even when applied at every infoset, and a depth-limited version\nof it led to--to our knowledge--the first strong AI for the massive challenge\nproblem dark chess.",
    "descriptor": "",
    "authors": [
      "Brian Hu Zhang",
      "Tuomas Sandholm"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.06068"
  },
  {
    "id": "arXiv:2106.06069",
    "title": "Concurrent multi-parameter learning demonstrated on the  Kuramoto-Sivashinsky equation",
    "abstract": "We develop an algorithm for the concurrent (on-the-fly) estimation of\nparameters for a system of evolutionary dissipative partial differential\nequations in which the state is partially observed. The intuitive nature of the\nalgorithm makes its extension to several different systems immediate, and it\nallows for recovery of multiple parameters simultaneously. We test this\nalgorithm on the Kuramoto-Sivashinsky equation in one dimension and demonstrate\nits efficacy in this context.",
    "descriptor": "",
    "authors": [
      "Benjamin Pachev",
      "Jared P. Whitehead",
      "Shane A. McQuarrie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2106.06069"
  },
  {
    "id": "arXiv:2106.06072",
    "title": "Gaussian Bounding Boxes and Probabilistic Intersection-over-Union for  Object Detection",
    "abstract": "Most object detection methods use bounding boxes to encode and represent the\nobject shape and location. In this work, we explore a fuzzy representation of\nobject regions using Gaussian distributions, which provides an implicit binary\nrepresentation as (potentially rotated) ellipses. We also present a similarity\nmeasure for the Gaussian distributions based on the Hellinger Distance, which\ncan be viewed as a Probabilistic Intersection-over-Union (ProbIoU). Our\nexperimental results show that the proposed Gaussian representations are closer\nto annotated segmentation masks in publicly available datasets, and that loss\nfunctions based on ProbIoU can be successfully used to regress the parameters\nof the Gaussian representation. Furthermore, we present a simple mapping scheme\nfrom traditional (or rotated) bounding boxes to Gaussian representations,\nallowing the proposed ProbIoU-based losses to be seamlessly integrated into any\nobject detector.",
    "descriptor": "",
    "authors": [
      "Jeffri M. Llerena",
      "Luis Felipe Zeni",
      "Lucas N. Kristen",
      "Claudio Jung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06072"
  },
  {
    "id": "arXiv:2106.06073",
    "title": "A modular framework for object-based saccadic decisions in dynamic  scenes",
    "abstract": "Visually exploring the world around us is not a passive process. Instead, we\nactively explore the world and acquire visual information over time. Here, we\npresent a new model for simulating human eye-movement behavior in dynamic\nreal-world scenes. We model this active scene exploration as a sequential\ndecision making process. We adapt the popular drift-diffusion model (DDM) for\nperceptual decision making and extend it towards multiple options, defined by\nobjects present in the scene. For each possible choice, the model integrates\nevidence over time and a decision (saccadic eye movement) is triggered as soon\nas evidence crosses a decision threshold. Drawing this explicit connection\nbetween decision making and object-based scene perception is highly relevant in\nthe context of active viewing, where decisions are made continuously while\ninteracting with an external environment. We validate our model with a\ncarefully designed ablation study and explore influences of our model\nparameters. A comparison on the VidCom dataset supports the plausibility of the\nproposed approach.",
    "descriptor": "\nComments: Accepted for presentation at EPIC@CVPR2021 workshop, 4 pages, 2 figures\n",
    "authors": [
      "Nicolas Roth",
      "Pia Bideau",
      "Olaf Hellwich",
      "Martin Rolfs",
      "Klaus Obermayer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06073"
  },
  {
    "id": "arXiv:2106.06079",
    "title": "A Nonmyopic Approach to Cost-Constrained Bayesian Optimization",
    "abstract": "Bayesian optimization (BO) is a popular method for optimizing\nexpensive-to-evaluate black-box functions. BO budgets are typically given in\niterations, which implicitly assumes each evaluation has the same cost. In\nfact, in many BO applications, evaluation costs vary significantly in different\nregions of the search space. In hyperparameter optimization, the time spent on\nneural network training increases with layer size; in clinical trials, the\nmonetary cost of drug compounds vary; and in optimal control, control actions\nhave differing complexities. Cost-constrained BO measures convergence with\nalternative cost metrics such as time, money, or energy, for which the sample\nefficiency of standard BO methods is ill-suited. For cost-constrained BO, cost\nefficiency is far more important than sample efficiency. In this paper, we\nformulate cost-constrained BO as a constrained Markov decision process (CMDP),\nand develop an efficient rollout approximation to the optimal CMDP policy that\ntakes both the cost and future iterations into account. We validate our method\non a collection of hyperparameter optimization problems as well as a sensor set\nselection application.",
    "descriptor": "\nComments: To appear in UAI 2021\n",
    "authors": [
      "Eric Hans Lee",
      "David Eriksson",
      "Valerio Perrone",
      "Matthias Seeger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06079"
  },
  {
    "id": "arXiv:2106.06080",
    "title": "Gradual Domain Adaptation in the Wild:When Intermediate Distributions  are Absent",
    "abstract": "We focus on the problem of domain adaptation when the goal is shifting the\nmodel towards the target distribution, rather than learning domain invariant\nrepresentations. It has been shown that under the following two assumptions:\n(a) access to samples from intermediate distributions, and (b) samples being\nannotated with the amount of change from the source distribution, self-training\ncan be successfully applied on gradually shifted samples to adapt the model\ntoward the target distribution. We hypothesize having (a) is enough to enable\niterative self-training to slowly adapt the model to the target distribution,\nby making use of an implicit curriculum. In the case where (a) does not hold,\nwe observe that iterative self-training falls short. We propose GIFT, a method\nthat creates virtual samples from intermediate distributions by interpolating\nrepresentations of examples from source and target domains. We evaluate an\niterative-self-training method on datasets with natural distribution shifts,\nand show that when applied on top of other domain adaptation methods, it\nimproves the performance of the model on the target dataset. We run an analysis\non a synthetic dataset to show that in the presence of (a)\niterative-self-training naturally forms a curriculum of samples. Furthermore,\nwe show that when (a) does not hold, GIFT performs better than iterative\nself-training.",
    "descriptor": "",
    "authors": [
      "Samira Abnar",
      "Rianne van den Berg",
      "Golnaz Ghiasi",
      "Mostafa Dehghani",
      "Nal Kalchbrenner",
      "Hanie Sedghi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06080"
  },
  {
    "id": "arXiv:2106.06082",
    "title": "One Sense Per Translation",
    "abstract": "The idea of using lexical translations to define sense inventories has a long\nhistory in lexical semantics. We propose a theoretical framework which allows\nus to answer the question of why this apparently reasonable idea failed to\nproduce useful results. We formally prove several propositions on how the\ntranslations of a word relate to its senses, as well as on the relationship\nbetween synonymy and polysemy. We empirically validate our theoretical findings\non BabelNet, and demonstrate how they could be used to perform unsupervised\nword sense disambiguation of a substantial fraction of the lexicon.",
    "descriptor": "",
    "authors": [
      "Bradley Hauer",
      "Grzegorz Kondrak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06082"
  },
  {
    "id": "arXiv:2106.06083",
    "title": "Analyzing Neural Jacobian Methods in Applications of Visual Servoing and  Kinematic Control",
    "abstract": "Designing adaptable control laws that can transfer between different robots\nis a challenge because of kinematic and dynamic differences, as well as in\nscenarios where external sensors are used. In this work, we empirically\ninvestigate a neural networks ability to approximate the Jacobian matrix for an\napplication in Cartesian control schemes. Specifically, we are interested in\napproximating the kinematic Jacobian, which arises from kinematic equations\nmapping a manipulator's joint angles to the end-effector's location. We propose\ntwo different approaches to learn the kinematic Jacobian. The first method\narises from visual servoing where we learn the kinematic Jacobian as an\napproximate linear system of equations from the k-nearest neighbors for a\ndesired joint configuration. The second, motivated by forward models in machine\nlearning, learns the kinematic behavior directly and calculates the Jacobian by\ndifferentiating the learned neural kinematics model. Simulation experimental\nresults show that both methods achieve better performance than alternative\ndata-driven methods for control, provide closer approximations to the proper\nkinematics Jacobian matrix, and on average produce better-conditioned Jacobian\nmatrices. Real-world experiments were conducted on a Kinova Gen-3 lightweight\nrobotic manipulator, which includes an uncalibrated visual servoing experiment,\na practical application of our methods, as well as a 7-DOF point-to-point task\nhighlighting that our methods are applicable on real robotic manipulators.",
    "descriptor": "\nComments: 8 pages, 6 Figures, this https URL\n",
    "authors": [
      "Michael Przystupa",
      "Masood Dehghan",
      "Martin Jagersand",
      "A. Rupam Mahmood"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.06083"
  },
  {
    "id": "arXiv:2106.06085",
    "title": "Problem-solving benefits of down-sampled lexicase selection",
    "abstract": "In genetic programming, an evolutionary method for producing computer\nprograms that solve specified computational problems, parent selection is\nordinarily based on aggregate measures of performance across an entire training\nset. Lexicase selection, by contrast, selects on the basis of performance on\nrandom sequences of training cases; this has been shown to enhance\nproblem-solving power in many circumstances. Lexicase selection can also be\nseen as better reflecting biological evolution, by modeling sequences of\nchallenges that organisms face over their lifetimes. Recent work has\ndemonstrated that the advantages of lexicase selection can be amplified by\ndown-sampling, meaning that only a random subsample of the training cases is\nused each generation. This can be seen as modeling the fact that individual\norganisms encounter only subsets of the possible environments, and that\nenvironments change over time. Here we provide the most extensive benchmarking\nof down-sampled lexicase selection to date, showing that its benefits hold up\nto increased scrutiny. The reasons that down-sampling helps, however, are not\nyet fully understood. Hypotheses include that down-sampling allows for more\ngenerations to be processed with the same budget of program evaluations; that\nthe variation of training data across generations acts as a changing\nenvironment, encouraging adaptation; or that it reduces overfitting, leading to\nmore general solutions. We systematically evaluate these hypotheses, finding\nevidence against all three, and instead draw the conclusion that down-sampled\nlexicase selection's main benefit stems from the fact that it allows the\nevolutionary process to examine more individuals within the same computational\nbudget, even though each individual is examined less completely.",
    "descriptor": "\nComments: to be published in Artificial Life Journal\n",
    "authors": [
      "Thomas Helmuth",
      "Lee Spector"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06085"
  },
  {
    "id": "arXiv:2106.06086",
    "title": "PSB2: The Second Program Synthesis Benchmark Suite",
    "abstract": "For the past six years, researchers in genetic programming and other program\nsynthesis disciplines have used the General Program Synthesis Benchmark Suite\nto benchmark many aspects of automatic program synthesis systems. These\nproblems have been used to make notable progress toward the goal of general\nprogram synthesis: automatically creating the types of software that human\nprogrammers code. Many of the systems that have attempted the problems in the\noriginal benchmark suite have used it to demonstrate performance improvements\ngranted through new techniques. Over time, the suite has gradually become\noutdated, hindering the accurate measurement of further improvements. The field\nneeds a new set of more difficult benchmark problems to move beyond what was\npreviously possible.\nIn this paper, we describe the 25 new general program synthesis benchmark\nproblems that make up PSB2, a new benchmark suite. These problems are curated\nfrom a variety of sources, including programming katas and college courses. We\nselected these problems to be more difficult than those in the original suite,\nand give results using PushGP showing this increase in difficulty. These new\nproblems give plenty of room for improvement, pointing the way for the next six\nor more years of general program synthesis research.",
    "descriptor": "\nComments: To be published in GECCO 2021\n",
    "authors": [
      "Thomas Helmuth",
      "Peter Kelly"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06086"
  },
  {
    "id": "arXiv:2106.06087",
    "title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language  Models",
    "abstract": "Targeted syntactic evaluations have demonstrated the ability of language\nmodels to perform subject-verb agreement given difficult contexts. To elucidate\nthe mechanisms by which the models accomplish this behavior, this study applies\ncausal mediation analysis to pre-trained neural language models. We investigate\nthe magnitude of models' preferences for grammatical inflections, as well as\nwhether neurons process subject-verb agreement similarly across sentences with\ndifferent syntactic structures. We uncover similarities and differences across\narchitectures and model sizes -- notably, that larger models do not necessarily\nlearn stronger preferences. We also observe two distinct mechanisms for\nproducing subject-verb agreement depending on the syntactic structure of the\ninput sentence. Finally, we find that language models rely on similar sets of\nneurons when given sentences with similar syntactic structure.",
    "descriptor": "\nComments: Code can be found at this https URL\n",
    "authors": [
      "Matthew Finlayson",
      "Aaron Mueller",
      "Stuart Shieber",
      "Sebastian Gehrmann",
      "Tal Linzen",
      "Yonatan Belinkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06087"
  },
  {
    "id": "arXiv:2106.06089",
    "title": "Gradient Disaggregation: Breaking Privacy in Federated Learning by  Reconstructing the User Participant Matrix",
    "abstract": "We show that aggregated model updates in federated learning may be insecure.\nAn untrusted central server may disaggregate user updates from sums of updates\nacross participants given repeated observations, enabling the server to recover\nprivileged information about individual users' private training data via\ntraditional gradient inference attacks. Our method revolves around\nreconstructing participant information (e.g: which rounds of training users\nparticipated in) from aggregated model updates by leveraging summary\ninformation from device analytics commonly used to monitor, debug, and manage\nfederated learning systems. Our attack is parallelizable and we successfully\ndisaggregate user updates on settings with up to thousands of participants. We\nquantitatively and qualitatively demonstrate significant improvements in the\ncapability of various inference attacks on the disaggregated updates. Our\nattack enables the attribution of learned properties to individual users,\nviolating anonymity, and shows that a determined central server may undermine\nthe secure aggregation protocol to break individual users' data privacy in\nfederated learning.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Maximilian Lam",
      "Gu-Yeon Wei",
      "David Brooks",
      "Vijay Janapa Reddi",
      "Michael Mitzenmacher"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06089"
  },
  {
    "id": "arXiv:2106.06090",
    "title": "Graph Neural Networks for Natural Language Processing: A Survey",
    "abstract": "Deep learning has become the dominant approach in coping with various tasks\nin Natural LanguageProcessing (NLP). Although text inputs are typically\nrepresented as a sequence of tokens, there isa rich variety of NLP problems\nthat can be best expressed with a graph structure. As a result, thereis a surge\nof interests in developing new deep learning techniques on graphs for a large\nnumberof NLP tasks. In this survey, we present a comprehensive overview onGraph\nNeural Networks(GNNs) for Natural Language Processing. We propose a new\ntaxonomy of GNNs for NLP, whichsystematically organizes existing research of\nGNNs for NLP along three axes: graph construction,graph representation\nlearning, and graph based encoder-decoder models. We further introducea large\nnumber of NLP applications that are exploiting the power of GNNs and summarize\nthecorresponding benchmark datasets, evaluation metrics, and open-source codes.\nFinally, we discussvarious outstanding challenges for making the full use of\nGNNs for NLP as well as future researchdirections. To the best of our\nknowledge, this is the first comprehensive overview of Graph NeuralNetworks for\nNatural Language Processing.",
    "descriptor": "\nComments: 127 pages\n",
    "authors": [
      "Lingfei Wu",
      "Yu Chen",
      "Kai Shen",
      "Xiaojie Guo",
      "Hanning Gao",
      "Shucheng Li",
      "Jian Pei",
      "Bo Long"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06090"
  },
  {
    "id": "arXiv:2106.06091",
    "title": "DECORE: Deep Compression with Reinforcement Learning",
    "abstract": "Deep learning has become an increasingly popular and powerful option for\nmodern pattern recognition systems. However, many deep neural networks have\nmillions to billions of parameters, making them untenable for real-world\napplications with constraints on memory or latency. As a result, powerful\nnetwork compression techniques are a must for the widespread adoption of deep\nlearning. We present DECORE, a reinforcement learning approach to automate the\nnetwork compression process. Using a simple policy gradient method to learn\nwhich neurons or channels to keep or remove, we are able to achieve compression\nrates 3x to 5x greater than contemporary approaches. In contrast with other\narchitecture search methods, DECORE is simple and quick to train, requiring\nonly a few hours of training on 1 GPU. When applied to standard network\narchitectures on different datasets, our approach achieves 11x to 103x\ncompression on different architectures while maintaining accuracies similar to\nthose of the original, large networks.",
    "descriptor": "",
    "authors": [
      "Manoj Alwani",
      "Vashisht Madhavan",
      "Yang Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06091"
  },
  {
    "id": "arXiv:2106.06092",
    "title": "Collaborative Multidisciplinary Design Optimization with Neural Networks",
    "abstract": "The design of complex engineering systems leads to solving very large\noptimization problems involving different disciplines. Strategies allowing\ndisciplines to optimize in parallel by providing sub-objectives and splitting\nthe problem into smaller parts, such as Collaborative Optimization, are\npromising solutions.However, most of them have slow convergence which reduces\ntheir practical use. Earlier efforts to fasten convergence by learning\nsurrogate models have not yet succeeded at sufficiently improving the\ncompetitiveness of these strategies.This paper shows that, in the case of\nCollaborative Optimization, faster and more reliable convergence can be\nobtained by solving an interesting instance of binary classification: on top of\nthe target label, the training data of one of the two classes contains the\ndistance to the decision boundary and its derivative. Leveraging this\ninformation, we propose to train a neural network with an asymmetric loss\nfunction, a structure that guarantees Lipshitz continuity, and a regularization\ntowards respecting basic distance function properties. The approach is\ndemonstrated on a toy learning example, and then applied to a multidisciplinary\naircraft design problem.",
    "descriptor": "\nComments: Proceedings of Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020\n",
    "authors": [
      "Jean de Becdelievre",
      "Ilan Kroo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06092"
  },
  {
    "id": "arXiv:2106.06095",
    "title": "Sparse Bayesian Learning via Stepwise Regression",
    "abstract": "Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity\nin probabilistic models. Herein, we propose a coordinate ascent algorithm for\nSBL termed Relevance Matching Pursuit (RMP) and show that, as its noise\nvariance parameter goes to zero, RMP exhibits a surprising connection to\nStepwise Regression. Further, we derive novel guarantees for Stepwise\nRegression algorithms, which also shed light on RMP. Our guarantees for Forward\nRegression improve on deterministic and probabilistic results for Orthogonal\nMatching Pursuit with noise. Our analysis of Backward Regression on determined\nsystems culminates in a bound on the residual of the optimal solution to the\nsubset selection problem that, if satisfied, guarantees the optimality of the\nresult. To our knowledge, this bound is the first that can be computed in\npolynomial time and depends chiefly on the smallest singular value of the\nmatrix. We report numerical experiments using a variety of feature selection\nalgorithms. Notably, RMP and its limiting variant are both efficient and\nmaintain strong performance with correlated features.",
    "descriptor": "",
    "authors": [
      "Sebastian Ament",
      "Carla Gomes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06095"
  },
  {
    "id": "arXiv:2106.06098",
    "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms",
    "abstract": "We present an online multi-task learning approach for adaptive nonlinear\ncontrol, which we call Online Meta-Adaptive Control (OMAC). The goal is to\ncontrol a nonlinear system subject to adversarial disturbance and unknown\n$\\textit{environment-dependent}$ nonlinear dynamics, under the assumption that\nthe environment-dependent dynamics can be well captured with some shared\nrepresentation. Our approach is motivated by robot control, where a robotic\nsystem encounters a sequence of new environmental conditions that it must\nquickly adapt to. A key emphasis is to integrate online representation learning\nwith established methods from control theory, in order to arrive at a unified\nframework that yields both control-theoretic and learning-theoretic guarantees.\nWe provide instantiations of our approach under varying conditions, leading to\nthe first non-asymptotic end-to-end convergence guarantee for multi-task\nadaptive nonlinear control. OMAC can also be integrated with deep\nrepresentation learning. Experiments show that OMAC significantly outperforms\nconventional adaptive control approaches which do not learn the shared\nrepresentation.",
    "descriptor": "",
    "authors": [
      "Guanya Shi",
      "Kamyar Azizzadenesheli",
      "Soon-Jo Chung",
      "Yisong Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06098"
  },
  {
    "id": "arXiv:2106.06103",
    "title": "Conditional Variational Autoencoder with Adversarial Learning for  End-to-End Text-to-Speech",
    "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage\ntraining and parallel sampling have been proposed, but their sample quality\ndoes not match that of two-stage TTS systems. In this work, we present a\nparallel end-to-end TTS method that generates more natural sounding audio than\ncurrent two-stage models. Our method adopts variational inference augmented\nwith normalizing flows and an adversarial training process, which improves the\nexpressive power of generative modeling. We also propose a stochastic duration\npredictor to synthesize speech with diverse rhythms from input text. With the\nuncertainty modeling over latent variables and the stochastic duration\npredictor, our method expresses the natural one-to-many relationship in which a\ntext input can be spoken in multiple ways with different pitches and rhythms. A\nsubjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a\nsingle speaker dataset, shows that our method outperforms the best publicly\navailable TTS systems and achieves a MOS comparable to ground truth.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Jaehyeon Kim",
      "Jungil Kong",
      "Juhee Son"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06103"
  },
  {
    "id": "arXiv:2106.06106",
    "title": "Wireless Communication with Extremely Large-Scale Intelligent Reflecting  Surface",
    "abstract": "Intelligent reflecting surface (IRS) is a promising technology for wireless\ncommunications, thanks to its potential capability to engineer the radio\nenvironment. However, in practice, such an envisaged benefit is attainable only\nwhen the passive IRS is of a sufficiently large size, for which the\nconventional uniform plane wave (UPW)-based channel model may become\ninaccurate. In this paper, we pursue a new channel modelling and performance\nanalysis for wireless communications with extremely large-scale IRS (XL-IRS).\nBy taking into account the variations in signal's amplitude and projected\naperture across different reflecting elements, we derive both lower- and\nupper-bounds of the received signal-to-noise ratio (SNR) for the general\nuniform planar array (UPA)-based XL-IRS. Our results reveal that, instead of\nscaling quadratically with the increased number of reflecting elements M as in\nthe conventional UPW model, the SNR under the more practically applicable\nnon-UPW model increases with M only with a diminishing return and gets\nsaturated eventually. To gain more insights, we further study the special case\nof uniform linear array (ULA)-based XL-IRS, for which a closed-form SNR\nexpression in terms of the IRS size and transmitter/receiver location is\nderived. This result shows that the SNR mainly depends on the two geometric\nangles formed by the transmitter/receiver locations with the IRS, as well as\nthe boundary points of the IRS. Numerical results validate our analysis and\ndemonstrate the importance of proper channel modelling for wireless\ncommunications aided by XL-IRS.",
    "descriptor": "\nComments: 6 pages, 5 figures, conference\n",
    "authors": [
      "Chao Feng",
      "Haiquan Lu",
      "Yong Zeng",
      "Shi Jin",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.06106"
  },
  {
    "id": "arXiv:2106.06110",
    "title": "Assessing the Effectiveness of Syntactic Structure to Learn Code Edit  Representations",
    "abstract": "In recent times, it has been shown that one can use code as data to aid\nvarious applications such as automatic commit message generation, automatic\ngeneration of pull request descriptions and automatic program repair. Take for\ninstance the problem of commit message generation. Treating source code as a\nsequence of tokens, state of the art techniques generate commit messages using\nneural machine translation models. However, they tend to ignore the syntactic\nstructure of programming languages.\nPrevious work, i.e., code2seq has used structural information from Abstract\nSyntax Tree (AST) to represent source code and they use it to automatically\ngenerate method names. In this paper, we elaborate upon this state of the art\napproach and modify it to represent source code edits. We determine the effect\nof using such syntactic structure for the problem of classifying code edits.\nInspired by the code2seq approach, we evaluate how using structural information\nfrom AST, i.e., paths between AST leaf nodes can help with the task of code\nedit classification on two datasets of fine-grained syntactic edits.\nOur experiments shows that attempts of adding syntactic structure does not\nresult in any improvements over less sophisticated methods. The results suggest\nthat techniques such as code2seq, while promising, have a long way to go before\nthey can be generically applied to learning code edit representations. We hope\nthat these results will benefit other researchers and inspire them to work\nfurther on this problem.",
    "descriptor": "",
    "authors": [
      "Syed Arbaaz Qureshi",
      "Sonu Mehta",
      "Ranjita Bhagwan",
      "Rahul Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06110"
  },
  {
    "id": "arXiv:2106.06112",
    "title": "Spectral Unsupervised Domain Adaptation for Visual Recognition",
    "abstract": "Unsupervised domain adaptation (UDA) aims to learn a well-performed model in\nan unlabeled target domain by leveraging labeled data from one or multiple\nrelated source domains. It remains a great challenge due to 1) the lack of\nannotations in the target domain and 2) the rich discrepancy between the\ndistributions of source and target data. We propose Spectral UDA (SUDA), an\nefficient yet effective UDA technique that works in the spectral space and is\ngeneric across different visual recognition tasks in detection, classification\nand segmentation. SUDA addresses UDA challenges from two perspectives. First,\nit mitigates inter-domain discrepancies by a spectrum transformer (ST) that\nmaps source and target images into spectral space and learns to enhance\ndomain-invariant spectra while suppressing domain-variant spectra\nsimultaneously. To this end, we design novel adversarial multi-head spectrum\nattention that leverages contextual information to identify domain-variant and\ndomain-invariant spectra effectively. Second, it mitigates the lack of\nannotations in target domain by introducing multi-view spectral learning which\naims to learn comprehensive yet confident target representations by maximizing\nthe mutual information among multiple ST augmentations capturing different\nspectral views of each target sample. Extensive experiments over different\nvisual tasks (e.g., detection, classification and segmentation) show that SUDA\nachieves superior accuracy and it is also complementary with state-of-the-art\nUDA methods with consistent performance boosts but little extra computation.",
    "descriptor": "",
    "authors": [
      "Jingyi Zhang",
      "Jiaxing Huang",
      "Shijian Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06112"
  },
  {
    "id": "arXiv:2106.06114",
    "title": "Interpreting Expert Annotation Differences in Animal Behavior",
    "abstract": "Hand-annotated data can vary due to factors such as subjective differences,\nintra-rater variability, and differing annotator expertise. We study\nannotations from different experts who labelled the same behavior classes on a\nset of animal behavior videos, and observe a variation in annotation styles. We\npropose a new method using program synthesis to help interpret annotation\ndifferences for behavior analysis. Our model selects relevant trajectory\nfeatures and learns a temporal filter as part of a program, which corresponds\nto estimated importance an annotator places on that feature at each timestamp.\nOur experiments on a dataset from behavioral neuroscience demonstrate that\ncompared to baseline approaches, our method is more accurate at capturing\nannotator labels and learns interpretable temporal filters. We believe that our\nmethod can lead to greater reproducibility of behavior annotations used in\nscientific studies. We plan to release our code.",
    "descriptor": "\nComments: 4 pages, 5 figures, presented as a poster at CV4Animals workshop @ CVPR21\n",
    "authors": [
      "Megan Tjandrasuwita",
      "Jennifer J. Sun",
      "Ann Kennedy",
      "Swarat Chaudhuri",
      "Yisong Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06114"
  },
  {
    "id": "arXiv:2106.06115",
    "title": "Self-Trained One-class Classification for Unsupervised Anomaly Detection",
    "abstract": "Anomaly detection (AD), separating anomalies from normal data, has various\napplications across domains, from manufacturing to healthcare. While most\nprevious works have shown to be effective for cases with fully or partially\nlabeled data, they are less practical for AD applications due to tedious data\nlabeling processes. In this work, we focus on unsupervised AD problems whose\nentire training data are unlabeled and may contain both normal and anomalous\nsamples. To tackle this problem, we build a robust one-class classification\nframework via data refinement. To refine the data accurately, we propose an\nensemble of one-class classifiers, each of which is trained on a disjoint\nsubset of training data. Moreover, we propose a self-training of deep\nrepresentation one-class classifiers (STOC) that iteratively refines the data\nand deep representations. In experiments, we show the efficacy of our method\nfor unsupervised anomaly detection on benchmarks from image and tabular data\ndomains. For example, with a 10% anomaly ratio on CIFAR-10 data, the proposed\nmethod outperforms state-of-the-art one-class classification method by 6.3 AUC\nand 12.5 average precision.",
    "descriptor": "",
    "authors": [
      "Jinsung Yoon",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Sercan O. Arik",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06115"
  },
  {
    "id": "arXiv:2106.06124",
    "title": "Twin Neural Network Regression is a Semi-Supervised Regression Algorithm",
    "abstract": "Twin neural network regression (TNNR) is a semi-supervised regression\nalgorithm, it can be trained on unlabelled data points as long as other,\nlabelled anchor data points, are present. TNNR is trained to predict\ndifferences between the target values of two different data points rather than\nthe targets themselves. By ensembling predicted differences between the targets\nof an unseen data point and all training data points, it is possible to obtain\na very accurate prediction for the original regression problem. Since any loop\nof predicted differences should sum to zero, loops can be supplied to the\ntraining data, even if the data points themselves within loops are unlabelled.\nSemi-supervised training improves TNNR performance, which is already state of\nthe art, significantly.",
    "descriptor": "",
    "authors": [
      "Sebastian J. Wetzel",
      "Roger G. Melko",
      "Isaac Tamblyn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06124"
  },
  {
    "id": "arXiv:2106.06125",
    "title": "Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language  Generation",
    "abstract": "A well-known limitation in pretrain-finetune paradigm lies in its\ninflexibility caused by the one-size-fits-all vocabulary. This potentially\nweakens the effect when applying pretrained models into natural language\ngeneration (NLG) tasks, especially for the subword distributions between\nupstream and downstream tasks with significant discrepancy. Towards approaching\nthis problem, we extend the vanilla pretrain-finetune pipeline with an extra\nembedding transfer step. Specifically, a plug-and-play embedding generator is\nintroduced to produce the representation of any input token, according to\npre-trained embeddings of its morphologically similar ones. Thus, embeddings of\nmismatch tokens in downstream tasks can also be efficiently initialized. We\nconduct experiments on a variety of NLG tasks under the pretrain-finetune\nfashion. Experimental results and extensive analyses show that the proposed\nstrategy offers us opportunities to feel free to transfer the vocabulary,\nleading to more efficient and better performed downstream NLG models.",
    "descriptor": "\nComments: Accepted by ACL2021\n",
    "authors": [
      "Xin Liu",
      "Baosong Yang",
      "Dayiheng Liu",
      "Haibo Zhang",
      "Weihua Luo",
      "Min Zhang",
      "Haiying Zhang",
      "Jinsong Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06125"
  },
  {
    "id": "arXiv:2106.06126",
    "title": "Exploiting Large-scale Teacher-Student Training for On-device Acoustic  Models",
    "abstract": "We present results from Alexa speech teams on semi-supervised learning (SSL)\nof acoustic models (AM) with experiments spanning over 3000 hours of GPU time,\nmaking our study one of the largest of its kind. We discuss SSL for AMs in a\nsmall footprint setting, showing that a smaller capacity model trained with 1\nmillion hours of unsupervised data can outperform a baseline supervised system\nby 14.3% word error rate reduction (WERR). When increasing the supervised data\nto seven-fold, our gains diminish to 7.1% WERR; to improve SSL efficiency at\nlarger supervised data regimes, we employ a step-wise distillation into a\nsmaller model, obtaining a WERR of 14.4%. We then switch to SSL using larger\nstudent models in low data regimes; while learning efficiency with unsupervised\ndata is higher, student models may outperform teacher models in such a setting.\nWe develop a theoretical sketch to explain this behavior.",
    "descriptor": "\nComments: TSD2021\n",
    "authors": [
      "Jing Liu",
      "Rupak Vignesh Swaminathan",
      "Sree Hari Krishnan Parthasarathi",
      "Chunchuan Lyu",
      "Athanasios Mouchtaris",
      "Siegfried Kunzmann"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06126"
  },
  {
    "id": "arXiv:2106.06127",
    "title": "Differentially Private Federated Learning via Inexact ADMM",
    "abstract": "Differential privacy (DP) techniques can be applied to the federated learning\nmodel to protect data privacy against inference attacks to communication among\nthe learning agents. The DP techniques, however, hinder achieving a greater\nlearning performance while ensuring strong data privacy. In this paper we\ndevelop a DP inexact alternating direction method of multipliers algorithm that\nsolves a sequence of trust-region subproblems with the objective perturbation\nby random noises generated from a Laplace distribution. We show that our\nalgorithm provides $\\bar{\\epsilon}$-DP for every iteration and\n$\\mathcal{O}(1/T)$ rate of convergence in expectation, where $T$ is the number\nof iterations. Using MNIST and FEMNIST datasets for the image classification,\nwe demonstrate that our algorithm reduces the testing error by at most $22\\%$\ncompared with the existing DP algorithm, while achieving the same level of data\nprivacy. The numerical experiment also shows that our algorithm converges\nfaster than the existing algorithm.",
    "descriptor": "\nComments: 21 pages, 6 figures\n",
    "authors": [
      "Minseok Ryu",
      "Kibaek Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06127"
  },
  {
    "id": "arXiv:2106.06128",
    "title": "Maximizing Influence of Leaders in Social Networks",
    "abstract": "The operation of adding edges has been frequently used to the study of\nopinion dynamics in social networks for various purposes. In this paper, we\nconsider the edge addition problem for the DeGroot model of opinion dynamics in\na social network with $n$ nodes and $m$ edges, in the presence of a small\nnumber $s \\ll n$ of competing leaders with binary opposing opinions 0 or 1.\nConcretely, we pose and investigate the problem of maximizing the equilibrium\noverall opinion by creating $k$ new edges in a candidate edge set, where each\nedge is incident to a 1-valued leader and a follower node. We show that the\nobjective function is monotone and submodular. We then propose a simple greedy\nalgorithm with an approximation factor $(1-\\frac{1}{e})$ that approximately\nsolves the problem in $O(n^3)$ time. Moreover, we provide a fast algorithm with\na $(1-\\frac{1}{e}-\\epsilon)$ approximation ratio and\n$\\tilde{O}(mk\\epsilon^{-2})$ time complexity for any $\\epsilon>0$, where\n$\\tilde{O}(\\cdot)$ notation suppresses the ${\\rm poly} (\\log n)$ factors.\nExtensive experiments demonstrate that our second approximate algorithm is\nefficient and effective, which scales to large networks with more than a\nmillion nodes.",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Xiaotian Zhou",
      "Zhongzhi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.06128"
  },
  {
    "id": "arXiv:2106.06129",
    "title": "Instance-Level Task Parameters: A Robust Multi-task Weighting Framework",
    "abstract": "Recent works have shown that deep neural networks benefit from multi-task\nlearning by learning a shared representation across several related tasks.\nHowever, performance of such systems depend on relative weighting between\nvarious losses involved during training. Prior works on loss weighting schemes\nassume that instances are equally easy or hard for all tasks. In order to break\nthis assumption, we let the training process dictate the optimal weighting of\ntasks for every instance in the dataset. More specifically, we equip every\ninstance in the dataset with a set of learnable parameters (instance-level task\nparameters) where the cardinality is equal to the number of tasks learned by\nthe model. These parameters model the weighting of each task for an instance.\nThey are updated by gradient descent and do not require hand-crafted rules. We\nconduct extensive experiments on SURREAL and CityScapes datasets, for human\nshape and pose estimation, depth estimation and semantic segmentation tasks. In\nthese tasks, our approach outperforms recent dynamic loss weighting approaches,\ne.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to\ndatasets where one or more tasks can have noisy annotations, the proposed\nmethod learns to prioritize learning from clean labels for a given task, e.g.\nreducing surface estimation errors by up to 60%. We also show that we can\nreliably detect corrupt labels for a given task as a by-product from learned\ninstance-level task parameters.",
    "descriptor": "",
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Shreyas Saxena",
      "Oncel Tuzel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06129"
  },
  {
    "id": "arXiv:2106.06130",
    "title": "ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for  Property Prediction",
    "abstract": "Effective molecular representation learning is of great importance to\nfacilitate molecular property prediction, which is a fundamental task for the\ndrug and material industry. Recent advances in graph neural networks (GNNs)\nhave shown great promise in applying GNNs for molecular representation\nlearning. Moreover, a few recent studies have also demonstrated successful\napplications of self-supervised learning methods to pre-train the GNNs to\novercome the problem of insufficient labeled molecules. However, existing GNNs\nand pre-training strategies usually treat molecules as topological graph data\nwithout fully utilizing the molecular geometry information. Whereas, the\nthree-dimensional (3D) spatial structure of a molecule, a.k.a molecular\ngeometry, is one of the most critical factors for determining molecular\nphysical, chemical, and biological properties. To this end, we propose a novel\nGeometry Enhanced Molecular representation learning method (GEM) for Chemical\nRepresentation Learning (ChemRL). At first, we design a geometry-based GNN\narchitecture that simultaneously models atoms, bonds, and bond angles in a\nmolecule. To be specific, we devised double graphs for a molecule: The first\none encodes the atom-bond relations; The second one encodes bond-angle\nrelations. Moreover, on top of the devised GNN architecture, we propose several\nnovel geometry-level self-supervised learning strategies to learn spatial\nknowledge by utilizing the local and global molecular 3D structures. We compare\nChemRL-GEM with various state-of-the-art (SOTA) baselines on different\nmolecular benchmarks and exhibit that ChemRL-GEM can significantly outperform\nall baselines in both regression and classification tasks. For example, the\nexperimental results show an overall improvement of $8.8\\%$ on average compared\nto SOTA baselines on the regression tasks, demonstrating the superiority of the\nproposed method.",
    "descriptor": "",
    "authors": [
      "Xiaomin Fang",
      "Lihang Liu",
      "Jieqiong Lei",
      "Donglong He",
      "Shanzhuo Zhang",
      "Jingbo Zhou",
      "Fan Wang",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Molecular Networks (q-bio.MN)"
    ],
    "url": "https://arxiv.org/abs/2106.06130"
  },
  {
    "id": "arXiv:2106.06132",
    "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives",
    "abstract": "Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.",
    "descriptor": "\nComments: Accepted to Findings of ACL, 2021 Data available at this http URL\n",
    "authors": [
      "Yash Kumar Lal",
      "Nathanael Chambers",
      "Raymond Mooney",
      "Niranjan Balasubramanian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06132"
  },
  {
    "id": "arXiv:2106.06133",
    "title": "Refining Pseudo Labels with Clustering Consensus over Generations for  Unsupervised Object Re-identification",
    "abstract": "Unsupervised object re-identification targets at learning discriminative\nrepresentations for object retrieval without any annotations. Clustering-based\nmethods conduct training with the generated pseudo labels and currently\ndominate this research direction. However, they still suffer from the issue of\npseudo label noise. To tackle the challenge, we propose to properly estimate\npseudo label similarities between consecutive training generations with\nclustering consensus and refine pseudo labels with temporally propagated and\nensembled pseudo labels. To the best of our knowledge, this is the first\nattempt to leverage the spirit of temporal ensembling to improve classification\nwith dynamically changing classes over generations. The proposed pseudo label\nrefinery strategy is simple yet effective and can be seamlessly integrated into\nexisting clustering-based unsupervised re-identification methods. With our\nproposed approach, state-of-the-art method can be further boosted with up to\n8.8% mAP improvements on the challenging MSMT17 dataset.",
    "descriptor": "\nComments: Accepted by CVPR2021\n",
    "authors": [
      "Xiao Zhang",
      "Yixiao Ge",
      "Yu Qiao",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06133"
  },
  {
    "id": "arXiv:2106.06134",
    "title": "Is Homophily a Necessity for Graph Neural Networks?",
    "abstract": "Graph neural networks (GNNs) have shown great prowess in learning\nrepresentations suitable for numerous graph-based machine learning tasks. When\napplied to semi-supervised node classification, GNNs are widely believed to\nwork well due to the homophily assumption (``like attracts like''), and fail to\ngeneralize to heterophilous graphs where dissimilar nodes connect. Recent works\ndesign new architectures to overcome such heterophily-related limitations,\nciting poor baseline performance and new architecture improvements on a few\nheterophilous graph benchmark datasets as evidence for this notion. In our\nexperiments, we empirically find that standard graph convolutional networks\n(GCNs) can actually achieve better performance than such carefully designed\nmethods on some commonly used heterophilous graphs. This motivates us to\nreconsider whether homophily is truly necessary for good GNN performance. We\nfind that this claim is not quite true, and in fact, GCNs can achieve strong\nperformance on heterophilous graphs under certain conditions. Our work\ncarefully characterizes these conditions, and provides supporting theoretical\nunderstanding and empirical observations. Finally, we examine existing\nheterophilous graphs benchmarks and reconcile how the GCN (under)performs on\nthem based on this understanding.",
    "descriptor": "",
    "authors": [
      "Yao Ma",
      "Xiaorui Liu",
      "Neil Shah",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06134"
  },
  {
    "id": "arXiv:2106.06135",
    "title": "DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning",
    "abstract": "Games are abstractions of the real world, where artificial agents learn to\ncompete and cooperate with other agents. While significant achievements have\nbeen made in various perfect- and imperfect-information games, DouDizhu (a.k.a.\nFighting the Landlord), a three-player card game, is still unsolved. DouDizhu\nis a very challenging domain with competition, collaboration, imperfect\ninformation, large state space, and particularly a massive set of possible\nactions where the legal actions vary significantly from turn to turn.\nUnfortunately, modern reinforcement learning algorithms mainly focus on simple\nand small action spaces, and not surprisingly, are shown not to make\nsatisfactory progress in DouDizhu. In this work, we propose a conceptually\nsimple yet effective DouDizhu AI system, namely DouZero, which enhances\ntraditional Monte-Carlo methods with deep neural networks, action encoding, and\nparallel actors. Starting from scratch in a single server with four GPUs,\nDouZero outperformed all the existing DouDizhu AI programs in days of training\nand was ranked the first in the Botzone leaderboard among 344 AI agents.\nThrough building DouZero, we show that classic Monte-Carlo methods can be made\nto deliver strong results in a hard domain with a complex action space. The\ncode and an online demo are released at https://github.com/kwai/DouZero with\nthe hope that this insight could motivate future work.",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Daochen Zha",
      "Jingru Xie",
      "Wenye Ma",
      "Sheng Zhang",
      "Xiangru Lian",
      "Xia Hu",
      "Ji Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06135"
  },
  {
    "id": "arXiv:2106.06136",
    "title": "Security and Privacy for Healthcare Blockchains",
    "abstract": "Healthcare blockchains provide an innovative way to store healthcare\ninformation, execute healthcare transactions, and build trust for healthcare\ndata sharing and data integration in a decentralized open healthcare network\nenvironment. Although the healthcare blockchain technology has attracted broad\ninterests and attention in industry, government and academia, the security and\nprivacy concerns remain the focus of debate when deploying blockchains for\ninformation sharing in the healthcare sector from business operation to\nresearch collaboration. This paper focuses on the security and privacy\nrequirements for medical data sharing using blockchain, and provides a\ncomprehensive analysis of the security and privacy risks and requirements,\naccompanied by technical solution techniques and strategies. First, we discuss\nthe security and privacy requirements and attributes required for electronic\nmedical data sharing by deploying the healthcare blockchain. Second, we\ncategorize existing efforts into three reference blockchain usage scenarios for\nelectronic medical data sharing, and discuss the technologies for implementing\nthese security and privacy properties in the three categories of usage\nscenarios for healthcare blockchain, such as anonymous signatures,\nattribute-based encryption, zero-knowledge proofs, verification techniques for\nsmart contract security. Finally, we discuss other potential blockchain\napplication scenarios in healthcare sector. We conjecture that this survey will\nhelp healthcare professionals, decision makers, and healthcare service\ndevelopers to gain technical and intuitive insights into the security and\nprivacy of healthcare blockchains in terms of concepts, risks, requirements,\ndevelopment and deployment technologies and systems.",
    "descriptor": "",
    "authors": [
      "Rui Zhang",
      "Rui Xue",
      "Ling Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06136"
  },
  {
    "id": "arXiv:2106.06138",
    "title": "Team RUC_AIM3 Technical Report at ActivityNet 2021: Entities Object  Localization",
    "abstract": "Entities Object Localization (EOL) aims to evaluate how grounded or faithful\na description is, which consists of caption generation and object grounding.\nPrevious works tackle this problem by jointly training the two modules in a\nframework, which limits the complexity of each module. Therefore, in this work,\nwe propose to divide these two modules into two stages and improve them\nrespectively to boost the whole system performance. For the caption generation,\nwe propose a Unified Multi-modal Pre-training Model (UMPM) to generate event\ndescriptions with rich objects for better localization. For the object\ngrounding, we fine-tune the state-of-the-art detection model MDETR and design a\npost processing method to make the grounding results more faithful. Our overall\nsystem achieves the state-of-the-art performances on both sub-tasks in Entities\nObject Localization challenge at Activitynet 2021, with 72.57 localization\naccuracy on the testing set of sub-task I and 0.2477 F1_all_per_sent on the\nhidden testing set of sub-task II.",
    "descriptor": "\nComments: 6 pages, 4 figures\n",
    "authors": [
      "Ludan Ruan",
      "Jieting Chen",
      "Yuqing Song",
      "Shizhe Chen",
      "Qin Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06138"
  },
  {
    "id": "arXiv:2106.06139",
    "title": "A comprehensive solution to retrieval-based chatbot construction",
    "abstract": "In this paper we present the results of our experiments in training and\ndeploying a self-supervised retrieval-based chatbot trained with contrastive\nlearning for assisting customer support agents. In contrast to most existing\nresearch papers in this area where the focus is on solving just one component\nof a deployable chatbot, we present an end-to-end set of solutions to take the\nreader from an unlabelled chatlogs to a deployed chatbot. This set of solutions\nincludes creating a self-supervised dataset and a weakly labelled dataset from\nchatlogs, as well as a systematic approach to selecting a fixed list of canned\nresponses. We present a hierarchical-based RNN architecture for the response\nselection model, chosen for its ability to cache intermediate utterance\nembeddings, which helped to meet deployment inference speed requirements. We\ncompare the performance of this architecture across 3 different learning\nobjectives: self-supervised contrastive learning, binary classification, and\nmulti-class classification. We find that using a self-supervised contrastive\nlearning model outperforms training the binary and multi-class classification\nmodels on a weakly labelled dataset. Our results validate that the\nself-supervised contrastive learning approach can be effectively used for a\nreal-world chatbot scenario.",
    "descriptor": "",
    "authors": [
      "Kristen Moore",
      "Shenjun Zhong",
      "Zhen He",
      "Torsten Rudolf",
      "Nils Fisher",
      "Brandon Victor",
      "Neha Jindal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06139"
  },
  {
    "id": "arXiv:2106.06141",
    "title": "A Taxonomy of Data Quality Challenges in Empirical Software Engineering",
    "abstract": "Reliable empirical models such as those used in software effort estimation or\ndefect prediction are inherently dependent on the data from which they are\nbuilt. As demands for process and product improvement continue to grow, the\nquality of the data used in measurement and prediction systems warrants\nincreasingly close scrutiny. In this paper we propose a taxonomy of data\nquality challenges in empirical software engineering, based on an extensive\nreview of prior research. We consider current assessment techniques for each\nquality issue and proposed mechanisms to address these issues, where available.\nOur taxonomy classifies data quality issues into three broad areas: first,\ncharacteristics of data that mean they are not fit for modeling; second, data\nset characteristics that lead to concerns about the suitability of applying a\ngiven model to another data set; and third, factors that prevent or limit data\naccessibility and trust. We identify this latter area as of particular need in\nterms of further research.",
    "descriptor": "\nComments: Conference paper, 12 pages, 6 figures\n",
    "authors": [
      "Michael Franklin Bosu",
      "Stephen G. MacDonell"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.06141"
  },
  {
    "id": "arXiv:2106.06142",
    "title": "DORO: Distributional and Outlier Robust Optimization",
    "abstract": "Many machine learning tasks involve subpopulation shift where the testing\ndata distribution is a subpopulation of the training distribution. For such\nsettings, a line of recent work has proposed the use of a variant of empirical\nrisk minimization(ERM) known as distributionally robust optimization (DRO). In\nthis work, we apply DRO to real, large-scale tasks with subpopulation shift,\nand observe that DRO performs relatively poorly, and moreover has severe\ninstability. We identify one direct cause of this phenomenon: sensitivity of\nDRO to outliers in the datasets. To resolve this issue, we propose the\nframework of DORO, for Distributional and Outlier Robust Optimization. At the\ncore of this approach is a refined risk function which prevents DRO from\noverfitting to potential outliers. We instantiate DORO for the Cressie-Read\nfamily of R\\'enyi divergence, and delve into two specific instances of this\nfamily: CVaR and $\\chi^2$-DRO. We theoretically prove the effectiveness of the\nproposed method, and empirically show that DORO improves the performance and\nstability of DRO with experiments on large modern datasets, thereby positively\naddressing the open question raised by Hashimoto et al., 2018.",
    "descriptor": "\nComments: ICML 2021. Codes: this https URL\n",
    "authors": [
      "Runtian Zhai",
      "Chen Dan",
      "J. Zico Kolter",
      "Pradeep Ravikumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06142"
  },
  {
    "id": "arXiv:2106.06147",
    "title": "NAAQA: A Neural Architecture for Acoustic Question Answering",
    "abstract": "The goal of the Acoustic Question Answering (AQA) task is to answer a\nfree-form text question about the content of an acoustic scene. It was inspired\nby the Visual Question Answering (VQA) task. In this paper, based on the\npreviously introduced CLEAR dataset, we propose a new benchmark for AQA that\nemphasizes the specific challenges of acoustic inputs, e.g. variable duration\nscenes. We also introduce NAAQA, a neural architecture that leverages specific\nproperties of acoustic inputs. The usage of time and frequency 1D convolutions\nto process 2D spectro-temporal representations of acoustic content shows\npromising results and enables reductions in model complexity. NAAQA achieves\n91.6% of accuracy on the AQA task with about 7 times fewer parameters than the\npreviously explored VQA model. We provide a detailed analysis of the results\nfor the different question types. The effectiveness of coordinate maps in this\nacoustic context was also studied and we show that time coordinate maps augment\ntemporal localization capabilities which enhance performance of the network by\nabout 17 percentage points.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) in April 2021\n",
    "authors": [
      "Jerome Abdelnour",
      "Jean Rouat",
      "Giampiero Salvi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06147"
  },
  {
    "id": "arXiv:2106.06148",
    "title": "Cell-Free Symbiotic Radio: Channel Estimation Method and Achievable Rate  Analysis",
    "abstract": "Cell-free massive MIMO and symbiotic radio are promising beyond 5G (B5G)\nnetworking architecture and transmission technology, respectively. This paper\nstudies cell-free symbiotic radio systems, where a number of distributed access\npoints (APs) cooperatively send primary information to a receiver, and\nsimultaneously support the backscattering communication of the secondary\nbackscatter device (BD). An efficient two-phase uplink-training based channel\nestimation method is proposed to estimate the direct-link channel and cascaded\nbackscatter channel, and the achievable primary and secondary communication\nrates taking into account the channel estimation errors are derived.\nFurthermore, to achieve a flexible trade-off between the primary and secondary\ncommunication rates, we propose a low-complexity weighted-maximal-ratio\ntransmission (weighted-MRT) beamforming scheme, which only requires local\nprocessing at each AP without having to exchange the estimated channel state\ninformation. Simulation results are provided to show the impact of the channel\ntraining lengths on the performance of the cell-free symbiotic radio systems.",
    "descriptor": "\nComments: 6 pages, 3 figures, conference\n",
    "authors": [
      "Zhuoyin Dai",
      "Ruoguang Li",
      "Jingran Xu",
      "Yong Zeng",
      "Shi Jin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.06148"
  },
  {
    "id": "arXiv:2106.06150",
    "title": "Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs",
    "abstract": "Graph neural networks (GNNs) are powerful tools for learning from graph data\nand are widely used in various applications such as social network\nrecommendation, fraud detection, and graph search. The graphs in these\napplications are typically large, usually containing hundreds of millions of\nnodes. Training GNN models on such large graphs efficiently remains a big\nchallenge. Despite a number of sampling-based methods have been proposed to\nenable mini-batch training on large graphs, these methods have not been proved\nto work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU\ntraining. The state-of-the-art sampling-based methods are usually not optimized\nfor these real-world hardware setups, in which data movement between CPUs and\nGPUs is a bottleneck. To address this issue, we propose Global Neighborhood\nSampling that aims at training GNNs on giant graphs specifically for\nmixed-CPU-GPU training. The algorithm samples a global cache of nodes\nperiodically for all mini-batches and stores them in GPUs. This global cache\nallows in-GPU importance sampling of mini-batches, which drastically reduces\nthe number of nodes in a mini-batch, especially in the input layer, to reduce\ndata copy between CPU and GPU and mini-batch computation without compromising\nthe training convergence rate or model accuracy. We provide a highly efficient\nimplementation of this method and show that our implementation outperforms an\nefficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant\ngraphs. It outperforms an efficient implementation of LADIES with small layers\nby a factor of 2X-14X while achieving much higher accuracy than LADIES.We also\ntheoretically analyze the proposed algorithm and show that with cached node\ndata of a proper size, it enjoys a comparable convergence rate as the\nunderlying node-wise sampling method.",
    "descriptor": "\nComments: The paper is published in KDD 2021\n",
    "authors": [
      "Jialin Dong",
      "Da Zheng",
      "Lin F. Yang",
      "Geroge Karypis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06150"
  },
  {
    "id": "arXiv:2106.06151",
    "title": "Anomalous Sound Detection Using a Binary Classification Model and Class  Centroids",
    "abstract": "An anomalous sound detection system to detect unknown anomalous sounds\nusually needs to be built using only normal sound data. Moreover, it is\ndesirable to improve the system by effectively using a small amount of\nanomalous sound data, which will be accumulated through the system's operation.\nAs one of the methods to meet these requirements, we focus on a binary\nclassification model that is developed by using not only normal data but also\noutlier data in the other domains as pseudo-anomalous sound data, which can be\neasily updated by using anomalous data. In this paper, we implement a new loss\nfunction based on metric learning to learn the distance relationship from each\nclass centroid in feature space for the binary classification model. The\nproposed multi-task learning of the binary classification and the metric\nlearning makes it possible to build the feature space where the within-class\nvariance is minimized and the between-class variance is maximized while keeping\nnormal and anomalous classes linearly separable. We also investigate the\neffectiveness of additionally using anomalous sound data for further improving\nthe binary classification model. Our results showed that multi-task learning\nusing binary classification and metric learning to consider the distance from\neach class centroid in the feature space is effective, and performance can be\nsignificantly improved by using even a small amount of anomalous data during\ntraining.",
    "descriptor": "\nComments: 6 pages, 2 figures, 2 tables, EUSIPCO2021\n",
    "authors": [
      "Ibuki Kuroyanagi",
      "Tomoki Hayashi",
      "Kazuya Takeda",
      "Tomoki Toda"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06151"
  },
  {
    "id": "arXiv:2106.06152",
    "title": "On the Robustness of Average Losses for Partial-Label Learning",
    "abstract": "Partial-label (PL) learning is a typical weakly supervised classification\nproblem, where a PL of an instance is a set of candidate labels such that a\nfixed but unknown candidate is the true label. For PL learning, there are two\nlines of research: (a) the identification-based strategy (IBS) purifies each\nlabel set and extracts the true label; (b) the average-based strategy (ABS)\ntreats all candidates equally for training. In the past two decades, IBS was a\nmuch hotter topic than ABS, since it was believed that IBS is more promising.\nIn this paper, we theoretically analyze ABS and find it also promising in the\nsense of the robustness of its loss functions. Specifically, we consider five\nproblem settings for the generation of clean or noisy PLs, and we prove that\naverage PL losses with bounded multi-class losses are always robust under mild\nassumptions on the domination of true labels, while average PL losses with\nunbounded multi-class losses (e.g., the cross-entropy loss) may not be robust.\nWe also conduct experiments to validate our theoretical findings. Note that IBS\nis heuristic, and we cannot prove its robustness by a similar proof technique;\nhence, ABS is more advantageous from a theoretical point of view, and it is\nworth paying attention to the design of more advanced PL learning methods\nfollowing ABS.",
    "descriptor": "",
    "authors": [
      "Jiaqi Lv",
      "Lei Feng",
      "Miao Xu",
      "Bo An",
      "Gang Niu",
      "Xin Geng",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06152"
  },
  {
    "id": "arXiv:2106.06153",
    "title": "Towards Understanding Generalization via Decomposing Excess Risk  Dynamics",
    "abstract": "Generalization is one of the critical issues in machine learning. However,\ntraditional methods like uniform convergence are not powerful enough to fully\nexplain generalization because they may yield vacuous bounds even in\noverparameterized linear regression regimes. An alternative solution is to\nanalyze the generalization dynamics to derive algorithm-dependent bounds, e.g.,\nstability. Unfortunately, the stability-based bound is still far from\nexplaining the remarkable generalization ability of neural networks due to the\ncoarse-grained analysis of the signal and noise. Inspired by the observation\nthat neural networks show a slow convergence rate when fitting noise, we\npropose decomposing the excess risk dynamics and applying stability-based bound\nonly on the variance part (which measures how the model performs on pure\nnoise). We provide two applications for the framework, including a linear case\n(overparameterized linear regression with gradient descent) and a non-linear\ncase (matrix recovery with gradient flow). Under the decomposition framework,\nthe new bound accords better with the theoretical and empirical evidence\ncompared to the stability-based bound and uniform convergence bound.",
    "descriptor": "",
    "authors": [
      "Jiaye Teng",
      "Jianhao Ma",
      "Yang Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06153"
  },
  {
    "id": "arXiv:2106.06154",
    "title": "Self-organising Roles in Agile Globally Distributed Teams",
    "abstract": "The ability to self-organise is posited to be a fundamental requirement for\nsuccessful agile teams. In particular, self-organising teams are said to be\ncrucial in agile globally distributed software development (AGSD) settings,\nwhere distance exacerbates team issues. We used contextual analysis to study\nthe specific interaction behaviours and enacted roles of practitioners working\nin multiple AGSD teams. Our results show that the teams studied were extremely\ntask focussed, and those who occupied team lead or programmer roles were\ncentral to their teams' self-organisation. These findings have implications for\nAGSD teams, and particularly for instances when programmers - or those\noccupying similar non-leadership positions - may not be willing to accept such\nresponsibilities. We discuss the implications of our findings for information\nsystem development (ISD) practice.",
    "descriptor": "\nComments: Conference paper, 10 pages, 5 tables, 3 figures. Link: this https URL\n",
    "authors": [
      "Sherlock A. Licorish",
      "Stephen G. MacDonell"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.06154"
  },
  {
    "id": "arXiv:2106.06157",
    "title": "Assessing Political Prudence of Open-domain Chatbots",
    "abstract": "Politically sensitive topics are still a challenge for open-domain chatbots.\nHowever, dealing with politically sensitive content in a responsible,\nnon-partisan, and safe behavior way is integral for these chatbots. Currently,\nthe main approach to handling political sensitivity is by simply changing such\na topic when it is detected. This is safe but evasive and results in a chatbot\nthat is less engaging. In this work, as a first step towards a politically safe\nchatbot, we propose a group of metrics for assessing their political prudence.\nWe then conduct political prudence analysis of various chatbots and discuss\ntheir behavior from multiple angles through our automatic metric and human\nevaluation metrics. The testsets and codebase are released to promote research\nin this area.",
    "descriptor": "\nComments: SIGDIAL 2021 - Safety for E2E Conversational AI (Camera-ready Version)\n",
    "authors": [
      "Yejin Bang",
      "Nayeon Lee",
      "Etsuko Ishii",
      "Andrea Madotto",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06157"
  },
  {
    "id": "arXiv:2106.06158",
    "title": "PyGAD: An Intuitive Genetic Algorithm Python Library",
    "abstract": "This paper introduces PyGAD, an open-source easy-to-use Python library for\nbuilding the genetic algorithm. PyGAD supports a wide range of parameters to\ngive the user control over everything in its life cycle. This includes, but is\nnot limited to, population, gene value range, gene data type, parent selection,\ncrossover, and mutation. PyGAD is designed as a general-purpose optimization\nlibrary that allows the user to customize the fitness function. Its usage\nconsists of 3 main steps: build the fitness function, create an instance of the\npygad.GA class, and calling the pygad.GA.run() method. The library supports\ntraining deep learning models created either with PyGAD itself or with\nframeworks like Keras and PyTorch. Given its stable state, PyGAD is also in\nactive development to respond to the user's requested features and enhancement\nreceived on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD\ncomes with documentation https://pygad.readthedocs.io for further details and\nexamples.",
    "descriptor": "",
    "authors": [
      "Ahmed Fawzy Gad"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.06158"
  },
  {
    "id": "arXiv:2106.06159",
    "title": "Learning the Precise Feature for Cluster Assignment",
    "abstract": "Clustering is one of the fundamental tasks in computer vision and pattern\nrecognition. Recently, deep clustering methods (algorithms based on deep\nlearning) have attracted wide attention with their impressive performance. Most\nof these algorithms combine deep unsupervised representation learning and\nstandard clustering together. However, the separation of representation\nlearning and clustering will lead to suboptimal solutions because the two-stage\nstrategy prevents representation learning from adapting to subsequent tasks\n(e.g., clustering according to specific cues). To overcome this issue, efforts\nhave been made in the dynamic adaption of representation and cluster\nassignment, whereas current state-of-the-art methods suffer from heuristically\nconstructed objectives with representation and cluster assignment alternatively\noptimized. To further standardize the clustering problem, we audaciously\nformulate the objective of clustering as finding a precise feature as the cue\nfor cluster assignment. Based on this, we propose a general-purpose deep\nclustering framework which radically integrates representation learning and\nclustering into a single pipeline for the first time. The proposed framework\nexploits the powerful ability of recently developed generative models for\nlearning intrinsic features, and imposes an entropy minimization on the\ndistribution of the cluster assignment by a dedicated variational algorithm.\nExperimental results show that the performance of the proposed method is\nsuperior, or at least comparable to, the state-of-the-art methods on the\nhandwritten digit recognition, fashion recognition, face recognition and object\nrecognition benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Yanhai Gan",
      "Xinghui Dong",
      "Huiyu Zhou",
      "Feng Gao",
      "Junyu Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06159"
  },
  {
    "id": "arXiv:2106.06160",
    "title": "Spoken Term Detection Methods for Sparse Transcription in Very  Low-resource Settings",
    "abstract": "We investigate the efficiency of two very different spoken term detection\napproaches for transcription when the available data is insufficient to train a\nrobust ASR system. This work is grounded in very low-resource language\ndocumentation scenario where only few minutes of recording have been\ntranscribed for a given language so far.Experiments on two oral languages show\nthat a pretrained universal phone recognizer, fine-tuned with only a few\nminutes of target language speech, can be used for spoken term detection with a\nbetter overall performance than a dynamic time warping approach. In addition,\nwe show that representing phoneme recognition ambiguity in a graph structure\ncan further boost the recall while maintaining high precision in the low\nresource spoken term detection task.",
    "descriptor": "",
    "authors": [
      "\u00c9ric Le Ferrand",
      "Steven Bird",
      "Laurent Besacier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06160"
  },
  {
    "id": "arXiv:2106.06161",
    "title": "Bandwidth-Optimal Random Shuffling for GPUs",
    "abstract": "Linear-time algorithms that are traditionally used to shuffle data on CPUs,\nsuch as the method of Fisher-Yates, are not well suited to implementation on\nGPUs due to inherent sequential dependencies. Moreover, existing parallel\nshuffling algorithms show unsatisfactory performance on GPU architectures\nbecause they incur a large number of read/write operations to high latency\nglobal memory. To address this, we provide a method of generating pseudo-random\npermutations in parallel by fusing suitable pseudo-random bijective functions\nwith stream compaction operations. Our algorithm, termed `bijective shuffle'\ntrades increased per-thread arithmetic operations for reduced global memory\ntransactions. It is work-efficient, deterministic, and only requires a single\nglobal memory read and write per shuffle input, thus maximising use of global\nmemory bandwidth. To empirically demonstrate the correctness of the algorithm,\nwe develop a consistent, linear time, statistical test for the quality of\npseudo-random permutations based on kernel space embeddings. Empirical results\nshow that the bijective shuffle algorithm outperforms competing algorithms on\nmulticore CPUs and GPUs, showing improvements of between one and two orders of\nmagnitude and approaching peak device bandwidth.",
    "descriptor": "",
    "authors": [
      "Rory Mitchell",
      "Daniel Stokes",
      "Eibe Frank",
      "Geoffrey Holmes"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06161"
  },
  {
    "id": "arXiv:2106.06162",
    "title": "Hybrid Generative-Contrastive Representation Learning",
    "abstract": "Unsupervised representation learning has recently received lots of interest\ndue to its powerful generalizability through effectively leveraging large-scale\nunlabeled data. There are two prevalent approaches for this, contrastive\nlearning and generative pre-training, where the former learns representations\nfrom instance-wise discrimination tasks and the latter learns them from\nestimating the likelihood. These seemingly orthogonal approaches have their own\nstrengths and weaknesses. Contrastive learning tends to extract semantic\ninformation and discards details irrelevant for classifying objects, making the\nrepresentations effective for discriminative tasks while degrading robustness\nto out-of-distribution data. On the other hand, the generative pre-training\ndirectly estimates the data distribution, so the representations tend to be\nrobust but not optimal for discriminative tasks. In this paper, we show that we\ncould achieve the best of both worlds by a hybrid training scheme.\nSpecifically, we demonstrated that a transformer-based encoder-decoder\narchitecture trained with both contrastive and generative losses can learn\nhighly discriminative and robust representations without hurting the generative\nperformance. We extensively validate our approach on various tasks.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Saehoon Kim",
      "Sungwoong Kim",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06162"
  },
  {
    "id": "arXiv:2106.06163",
    "title": "Deception Detection in Group Video Conversations using Dynamic  Interaction Networks",
    "abstract": "Detecting groups of people who are jointly deceptive in video conversations\nis crucial in settings such as meetings, sales pitches, and negotiations. Past\nwork on deception in videos focuses on detecting a single deceiver and uses\nfacial or visual features only. In this paper, we propose the concept of\nFace-to-Face Dynamic Interaction Networks (FFDINs) to model the interpersonal\ninteractions within a group of people. The use of FFDINs enables us to leverage\nnetwork relations in detecting group deception in video conversations for the\nfirst time. We use a dataset of 185 videos from a deception-based game called\nResistance. We first characterize the behavior of individual, pairs, and groups\nof deceptive participants and compare them to non-deceptive participants. Our\nanalysis reveals that pairs of deceivers tend to avoid mutual interaction and\nfocus their attention on non-deceivers. In contrast, non-deceivers interact\nwith everyone equally. We propose Negative Dynamic Interaction Networks to\ncapture the notion of missing interactions. We create the DeceptionRank\nalgorithm to detect deceivers from NDINs extracted from videos that are just\none minute long. We show that our method outperforms recent state-of-the-art\ncomputer vision, graph embedding, and ensemble methods by at least 20.9% AUROC\nin identifying deception from videos.",
    "descriptor": "\nComments: The paper is published at ICWSM 2021. Dataset link: this https URL\n",
    "authors": [
      "Srijan Kumar",
      "Chongyang Bai",
      "V.S. Subrahmanian",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.06163"
  },
  {
    "id": "arXiv:2106.06165",
    "title": "Modeling Sequences as Distributions with Uncertainty for Sequential  Recommendation",
    "abstract": "The sequential patterns within the user interactions are pivotal for\nrepresenting the user's preference and capturing latent relationships among\nitems. The recent advancements of sequence modeling by Transformers advocate\nthe community to devise more effective encoders for the sequential\nrecommendation. Most existing sequential methods assume users are\ndeterministic. However, item-item transitions might fluctuate significantly in\nseveral item aspects and exhibit randomness of user interests. This\n\\textit{stochastic characteristics} brings up a solid demand to include\nuncertainties in representing sequences and items. Additionally, modeling\nsequences and items with uncertainties expands users' and items' interaction\nspaces, thus further alleviating cold-start problems.\nIn this work, we propose a Distribution-based Transformer for Sequential\nRecommendation (DT4SR), which injects uncertainties into sequential modeling.\nWe use Elliptical Gaussian distributions to describe items and sequences with\nuncertainty. We describe the uncertainty in items and sequences as Elliptical\nGaussian distribution. And we adopt Wasserstein distance to measure the\nsimilarity between distributions. We devise two novel Trans-formers for\nmodeling mean and covariance, which guarantees the positive-definite property\nof distributions. The proposed method significantly outperforms the\nstate-of-the-art methods. The experiments on three benchmark datasets also\ndemonstrate its effectiveness in alleviating cold-start issues. The code is\navailable inhttps://github.com/DyGRec/DT4SR.",
    "descriptor": "",
    "authors": [
      "Ziwei Fan",
      "Zhiwei Liu",
      "Lei Zheng",
      "Shen Wang",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06165"
  },
  {
    "id": "arXiv:2106.06167",
    "title": "HIFI: Anomaly Detection for Multivariate Time Series with High-order  Feature Interactions",
    "abstract": "Monitoring complex systems results in massive multivariate time series data,\nand anomaly detection of these data is very important to maintain the normal\noperation of the systems. Despite the recent emergence of a large number of\nanomaly detection algorithms for multivariate time series, most of them ignore\nthe correlation modeling among multivariate, which can often lead to poor\nanomaly detection results. In this work, we propose a novel anomaly detection\nmodel for multivariate time series with \\underline{HI}gh-order\n\\underline{F}eature \\underline{I}nteractions (HIFI). More specifically, HIFI\nbuilds multivariate feature interaction graph automatically and uses the graph\nconvolutional neural network to achieve high-order feature interactions, in\nwhich the long-term temporal dependencies are modeled by attention mechanisms\nand a variational encoding technique is utilized to improve the model\nperformance and robustness. Extensive experiments on three publicly available\ndatasets demonstrate the superiority of our framework compared with\nstate-of-the-art approaches.",
    "descriptor": "\nComments: 8 pages, 1 figures\n",
    "authors": [
      "Liwei Deng",
      "Xuanhao Chen",
      "Yan Zhao",
      "Kai Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06167"
  },
  {
    "id": "arXiv:2106.06168",
    "title": "Generate, Annotate, and Learn: Generative Models Advance Self-Training  and Knowledge Distillation",
    "abstract": "Semi-Supervised Learning (SSL) has seen success in many application domains,\nbut this success often hinges on the availability of task-specific unlabeled\ndata. Knowledge distillation (KD) has enabled compressing deep networks and\nensembles, achieving the best results when distilling knowledge on fresh\ntask-specific unlabeled examples. However, task-specific unlabeled data can be\nchallenging to find. We present a general framework called \"generate, annotate,\nand learn (GAL)\" that uses unconditional generative models to synthesize\nin-domain unlabeled data, helping advance SSL and KD on different tasks. To\nobtain strong task-specific generative models, we adopt generic generative\nmodels, pretrained on open-domain data, and fine-tune them on inputs from\nspecific tasks. Then, we use existing classifiers to annotate generated\nunlabeled examples with soft pseudo labels, which are used for additional\ntraining. When self-training is combined with samples generated from\nGPT2-large, fine-tuned on the inputs of each GLUE task, we outperform a strong\nRoBERTa-large baseline on the GLUE benchmark. Moreover, KD on GPT-2 samples\nyields a new state-of-the-art for 6-layer transformers on the GLUE leaderboard.\nFinally, self-training with GAL offers significant gains on image\nclassification on CIFAR-10 and four tabular tasks from the UCI repository",
    "descriptor": "\nComments: 26 pages, 3 figures\n",
    "authors": [
      "Xuanli He",
      "Islam Nassar",
      "Jamie Kiros",
      "Gholamreza Haffari",
      "Mohammad Norouzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06168"
  },
  {
    "id": "arXiv:2106.06169",
    "title": "BoB: BERT Over BERT for Training Persona-based Dialogue Models from  Limited Personalized Data",
    "abstract": "Maintaining consistent personas is essential for dialogue agents. Although\ntremendous advancements have been brought, the limited-scale of annotated\npersona-dense data are still barriers towards training robust and consistent\npersona-based dialogue models. In this work, we show how the challenges can be\naddressed by disentangling persona-based dialogue generation into two sub-tasks\nwith a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a\nBERT-based encoder and two BERT-based decoders, where one decoder is for\nresponse generation, and another is for consistency understanding. In\nparticular, to learn the ability of consistency understanding from large-scale\nnon-dialogue inference data, we train the second decoder in an unlikelihood\nmanner. Under different limited data settings, both automatic and human\nevaluations demonstrate that the proposed model outperforms strong baselines in\nresponse quality and persona consistency.",
    "descriptor": "\nComments: publised in ACL 2021\n",
    "authors": [
      "Haoyu Song",
      "Yan Wang",
      "Kaiyan Zhang",
      "Wei-Nan Zhang",
      "Ting Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06169"
  },
  {
    "id": "arXiv:2106.06170",
    "title": "Taylor Expansion of Discount Factors",
    "abstract": "In practical reinforcement learning (RL), the discount factor used for\nestimating value functions often differs from that used for defining the\nevaluation objective. In this work, we study the effect that this discrepancy\nof discount factors has during learning, and discover a family of objectives\nthat interpolate value functions of two distinct discount factors. Our analysis\nsuggests new ways for estimating value functions and performing policy\noptimization updates, which demonstrate empirical performance gains. This\nframework also leads to new insights on commonly-used deep RL heuristic\nmodifications to policy optimization algorithms.",
    "descriptor": "\nComments: Accepted at International Conference of Machine Learning (ICML), 2021\n",
    "authors": [
      "Yunhao Tang",
      "Mark Rowland",
      "R\u00e9mi Munos",
      "Michal Valko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06170"
  },
  {
    "id": "arXiv:2106.06171",
    "title": "Inter-domain Multi-relational Link Prediction",
    "abstract": "Multi-relational graph is a ubiquitous and important data structure, allowing\nflexible representation of multiple types of interactions and relations between\nentities. Similar to other graph-structured data, link prediction is one of the\nmost important tasks on multi-relational graphs and is often used for knowledge\ncompletion. When related graphs coexist, it is of great benefit to build a\nlarger graph via integrating the smaller ones. The integration requires\npredicting hidden relational connections between entities belonged to different\ngraphs (inter-domain link prediction). However, this poses a real challenge to\nexisting methods that are exclusively designed for link prediction between\nentities of the same graph only (intra-domain link prediction). In this study,\nwe propose a new approach to tackle the inter-domain link prediction problem by\nsoftly aligning the entity distributions between different domains with optimal\ntransport and maximum mean discrepancy regularizers. Experiments on real-world\ndatasets show that optimal transport regularizer is beneficial and considerably\nimproves the performance of baseline methods.",
    "descriptor": "\nComments: 16 pages, 2 figures, 5 tables\n",
    "authors": [
      "Luu Huu Phuc",
      "Koh Takeuchi",
      "Seiji Okajima",
      "Arseny Tolmachev",
      "Tomoyoshi Takebayashi",
      "Koji Maruhashi",
      "Hisashi Kashima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06171"
  },
  {
    "id": "arXiv:2106.06174",
    "title": "Generalized Moving Peaks Benchmark",
    "abstract": "This document describes the Generalized Moving Peaks Benchmark (GMPB) that\ngenerates continuous dynamic optimization problem instances. The landscapes\ngenerated by GMPB are constructed by assembling several components with a\nvariety of controllable characteristics ranging from unimodal to highly\nmultimodal, symmetric to highly asymmetric, smooth to highly irregular, and\nvarious degrees of variable interaction and ill-conditioning. In this document,\nwe explain how these characteristics can be generated by different parameter\nsettings of GMPB. The MATLAB source code of GMPB is also explained. This\ndocument forms the basis for a range of competitions on Evolutionary Continuous\nDynamic Optimization in the upcoming well-known conferences.",
    "descriptor": "\nComments: This document forms the basis for a range of competitions on Evolutionary Continuous Dynamic Optimization in the upcoming well-known conferences\n",
    "authors": [
      "Danial Yazdani",
      "Juergen Branke",
      "Mohammad Nabi Omidvar",
      "Changhe Li",
      "Michalis Mavrovouniotis",
      "Trung Thanh Nguyen",
      "Shengxiang Yang",
      "Xin Yao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06174"
  },
  {
    "id": "arXiv:2106.06178",
    "title": "AI Empowered Resource Management for Future Wireless Networks",
    "abstract": "Resource management plays a pivotal role in wireless networks, which,\nunfortunately, leads to challenging NP-hard problems. Artificial Intelligence\n(AI), especially deep learning techniques, has recently emerged as a disruptive\ntechnology to solve such challenging problems in a real-time manner. However,\nalthough promising results have been reported, practical design guidelines and\nperformance guarantees of AI-based approaches are still missing. In this paper,\nwe endeavor to address two fundamental questions: 1) What are the main\nadvantages of AI-based methods compared with classical techniques; and 2) Which\nneural network should we choose for a given resource management task. For the\nfirst question, four advantages are identified and discussed. For the second\nquestion, \\emph{optimality gap}, i.e., the gap to the optimal performance, is\nproposed as a measure for selecting model architectures, as well as, for\nenabling a theoretical comparison between different AI-based approaches.\nSpecifically, for $K$-user interference management problem, we theoretically\nshow that graph neural networks (GNNs) are superior to multi-layer perceptrons\n(MLPs), and the performance gap between these two methods grows with\n$\\sqrt{K}$.",
    "descriptor": "\nComments: Meditcom 2021\n",
    "authors": [
      "Yifei Shen",
      "Jun Zhang",
      "S.H. Song",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.06178"
  },
  {
    "id": "arXiv:2106.06181",
    "title": "Calibration and Auto-Refinement for Light Field Cameras",
    "abstract": "The ability to create an accurate three-dimensional reconstruction of a\ncaptured scene draws attention to the principles of light fields. This paper\npresents an approach for light field camera calibration and rectification,\nbased on pairwise pattern-based parameters extraction. It is followed by a\ncorrespondence-based algorithm for camera parameters refinement from arbitrary\nscenes using the triangulation filter and nonlinear optimization. The\neffectiveness of our approach is validated on both real and synthetic data.",
    "descriptor": "\nComments: Presented on 29. International Conference on Computer Graphics, Visualization and Computer Vision 2021 (WSCG 2021)\n",
    "authors": [
      "Yuriy Anisimov",
      "Gerd Reis",
      "Didier Stricker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06181"
  },
  {
    "id": "arXiv:2106.06184",
    "title": "A Single-Layer Dual-Mesh Boundary Element Method for Multiscale  Electromagnetic Modeling of Penetrable Objects in Layered Media",
    "abstract": "A surface integral representation of Maxwell's equations allows the efficient\nelectromagnetic (EM) modeling of three-dimensional structures with a\ntwo-dimensional discretization, via the boundary element method (BEM). However,\nexisting BEM formulations either lead to a poorly conditioned system matrix for\nmultiscale problems, or are computationally expensive for objects embedded in\nlayered substrates. This article presents a new BEM formulation which leverages\nthe surface equivalence principle and Buffa-Christiansen basis functions\ndefined on a dual mesh, to obtain a well-conditioned system matrix suitable for\nmultiscale EM modeling. Unlike existing methods involving dual meshes, the\nproposed formulation avoids the double-layer potential operator for the\nsurrounding medium, which may be a stratified substrate requiring the use of an\nadvanced Green's function. This feature greatly alleviates the computational\nexpense associated with the use of Buffa-Christiansen functions. Numerical\nexamples drawn from several applications, including remote sensing, chip-level\nEM analysis, and metasurface modeling, demonstrate speed-ups ranging from 3x to\n7x compared to state-of-the-art formulations.",
    "descriptor": "\nComments: 10 pages, 11 figures, submitted to the IEEE Journal on Multiscale and Multiphysics Computational Techniques\n",
    "authors": [
      "Shashwat Sharma",
      "Piero Triverio"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.06184"
  },
  {
    "id": "arXiv:2106.06187",
    "title": "Non-orthogonal Multiple Access for Multi-cell Indoor VLC",
    "abstract": "In this letter, we propose a non-orthogonal multiple access (NOMA) based\nscheme for multi-cell indoor visible light communications (VLC), where\ncell-edge user is jointly served by multiple cells. Unlike the typical designs\nin the existing literature, the proposed scheme doesn't involve complex domain\nand direct current bias addition, which makes the design simple and feasible\nfor real time implementation in indoor VLC. The symbol error rate (SER) of the\nproposed NOMA scheme is analysed using analytical and simulation results, and\nis compared with orthogonal multiple access (OMA). It is observed that the\naverage SER of the users with the proposed NOMA is marginally degraded as\ncompared to OMA. However, the SER of cell-edge user with the proposed NOMA is\nsignificantly improved with joint maximum likelihood decoding, and outperforms\nsuccessive interference cancellation based decoding and OMA, with trade-off on\ncomputational complexity.",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "T. Uday",
      "Abhinav Kumar",
      "L. Natarajan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.06187"
  },
  {
    "id": "arXiv:2106.06191",
    "title": "Structural evolution and on-demand growth of artificial synapses via  field-directed polymerization",
    "abstract": "Interconnectivity, fault tolerance, and dynamic evolution of the circuitry\nare long sought-after objectives of bio-inspired engineering. Here, we propose\ndendritic transistors composed of organic semiconductors as building blocks for\nneuromorphic computing. These devices, owning to their voltage-triggered growth\nand resemblance to neural structures, respond to action potentials to achieve\ncomplex brain-like features, such as Pavlovian learning, pattern recognition,\nand spike-timing-dependent plasticity. The dynamic formation of the connections\nis reminiscent of a biological learning mechanism known as synaptogenesis, and\nit is carried out by an electrochemical reaction that we name field-directed\npolymerization. We employ it to dendritic connections and, by modulating the\ngrowth parameters, control material properties such as the resistance and the\ntime constants relevant for plasticity. We believe these results will inspire\nfurther research towards the complex integration of polymerized synapses for\nbrain-inspired computing.",
    "descriptor": "",
    "authors": [
      "Matteo Cucchi",
      "Hans Kleemann",
      "Hsin Tseng",
      "Alexander Lee",
      "Karl Leo"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Applied Physics (physics.app-ph)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.06191"
  },
  {
    "id": "arXiv:2106.06195",
    "title": "MlTr: Multi-label Classification with Transformer",
    "abstract": "The task of multi-label image classification is to recognize all the object\nlabels presented in an image. Though advancing for years, small objects,\nsimilar objects and objects with high conditional probability are still the\nmain bottlenecks of previous convolutional neural network(CNN) based models,\nlimited by convolutional kernels' representational capacity. Recent vision\ntransformer networks utilize the self-attention mechanism to extract the\nfeature of pixel granularity, which expresses richer local semantic\ninformation, while is insufficient for mining global spatial dependence. In\nthis paper, we point out the three crucial problems that CNN-based methods\nencounter and explore the possibility of conducting specific transformer\nmodules to settle them. We put forward a Multi-label Transformer\narchitecture(MlTr) constructed with windows partitioning, in-window pixel\nattention, cross-window attention, particularly improving the performance of\nmulti-label image classification tasks. The proposed MlTr shows\nstate-of-the-art results on various prevalent multi-label datasets such as\nMS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively.\nThe code will be available soon at https://github.com/starmemda/MlTr/",
    "descriptor": "",
    "authors": [
      "Xing Cheng",
      "Hezheng Lin",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Nian Shi",
      "Honglin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06195"
  },
  {
    "id": "arXiv:2106.06196",
    "title": "Adversarial Robustness through the Lens of Causality",
    "abstract": "The adversarial vulnerability of deep neural networks has attracted\nsignificant attention in machine learning. From a causal viewpoint, adversarial\nattacks can be considered as a specific type of distribution change on natural\ndata. As causal reasoning has an instinct for modeling distribution change, we\npropose to incorporate causality into mitigating adversarial vulnerability.\nHowever, causal formulations of the intuition of adversarial attack and the\ndevelopment of robust DNNs are still lacking in the literature. To bridge this\ngap, we construct a causal graph to model the generation process of adversarial\nexamples and define the adversarial distribution to formalize the intuition of\nadversarial attacks. From a causal perspective, we find that the label is\nspuriously correlated with the style (content-independent) information when an\ninstance is given. The spurious correlation implies that the adversarial\ndistribution is constructed via making the statistical conditional association\nbetween style information and labels drastically different from that in natural\ndistribution. Thus, DNNs that fit the spurious correlation are vulnerable to\nthe adversarial distribution. Inspired by the observation, we propose the\nadversarial distribution alignment method to eliminate the difference between\nthe natural distribution and the adversarial distribution. Extensive\nexperiments demonstrate the efficacy of the proposed method. Our method can be\nseen as the first attempt to leverage causality for mitigating adversarial\nvulnerability.",
    "descriptor": "",
    "authors": [
      "Yonggang Zhang",
      "Mingming Gong",
      "Tongliang Liu",
      "Gang Niu",
      "Xinmei Tian",
      "Bo Han",
      "Bernhard Sch\u00f6lkopf",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06196"
  },
  {
    "id": "arXiv:2106.06198",
    "title": "Dynamic Event-Triggered Consensus of Multi-agent Systems on  Matrix-weighted Networks",
    "abstract": "This paper examines event-triggered consensus of multi-agent systems on\nmatrix-weighted networks, where the interdependencies among higher-dimensional\nstates of neighboring agents are characterized by matrix-weighted edges in the\nnetwork. Specifically, a distributed dynamic event-triggered coordination\nstrategy is proposed for this category of generalized networks, in which an\nauxiliary system is employed for each agent to dynamically adjust the trigger\nthreshold, which plays an essential role in guaranteeing that the triggering\ntime sequence does not exhibit Zeno behavior. Distributed event-triggered\ncontrol protocols are proposed to guarantee leaderless and leader-follower\nconsensus for multi-agent systems on matrix-weighted networks, respectively. It\nis shown that that the spectral properties of matrix-valued weights are crucial\nin event-triggered mechanism design for matrix-weighted networks. Finally,\nsimulation examples are provided to demonstrate the theoretical results.",
    "descriptor": "",
    "authors": [
      "Lulu Pan",
      "Haibin Shao",
      "Dewei Li",
      "Yugeng Xi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.06198"
  },
  {
    "id": "arXiv:2106.06199",
    "title": "LocoProp: Enhancing BackProp via Local Loss Optimization",
    "abstract": "We study a local loss construction approach for optimizing neural networks.\nWe start by motivating the problem as minimizing a squared loss between the\npre-activations of each layer and a local target, plus a regularizer term on\nthe weights. The targets are chosen so that the first gradient descent step on\nthe local objectives recovers vanilla BackProp, while the exact solution to\neach problem results in a preconditioned gradient update. We improve the local\nloss construction by forming a Bregman divergence in each layer tailored to the\ntransfer function which keeps the local problem convex w.r.t. the weights. The\ngeneralized local problem is again solved iteratively by taking small gradient\ndescent steps on the weights, for which the first step recovers BackProp. We\nrun several ablations and show that our construction consistently improves\nconvergence, reducing the gap between first-order and second-order methods.",
    "descriptor": "",
    "authors": [
      "Ehsan Amid",
      "Rohan Anil",
      "Manfred K. Warmuth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06199"
  },
  {
    "id": "arXiv:2106.06200",
    "title": "Towards User-Driven Neural Machine Translation",
    "abstract": "A good translation should not only translate the original content\nsemantically, but also incarnate personal traits of the original text. For a\nreal-world neural machine translation (NMT) system, these user traits (e.g.,\ntopic preference, stylistic characteristics and expression habits) can be\npreserved in user behavior (e.g., historical inputs). However, current NMT\nsystems marginally consider the user behavior due to: 1) the difficulty of\nmodeling user portraits in zero-shot scenarios, and 2) the lack of\nuser-behavior annotated parallel dataset. To fill this gap, we introduce a\nnovel framework called user-driven NMT. Specifically, a cache-based module and\na user-driven contrastive learning method are proposed to offer NMT the ability\nto capture potential user traits from their historical inputs under a zero-shot\nlearning fashion. Furthermore, we contribute the first Chinese-English parallel\ncorpus annotated with user behavior called UDT-Corpus. Experimental results\nconfirm that the proposed user-driven NMT can generate user-specific\ntranslations.",
    "descriptor": "",
    "authors": [
      "Huan Lin",
      "Liang Yao",
      "Baosong Yang",
      "Dayiheng Liu",
      "Haibo Zhang",
      "Weihua Luo",
      "Degen Huang",
      "Jinsong Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06200"
  },
  {
    "id": "arXiv:2106.06201",
    "title": "Distributed Urban Freeway Traffic Optimization Considering Congestion  Propagation",
    "abstract": "Effective traffic optimization strategies can improve the performance of\ntransportation networks significantly. Most exiting works develop traffic\noptimization strategies depending on the local traffic states of congested road\nsegments, where the congestion propagation is neglected. This paper proposes a\nnovel distributed traffic optimization method for urban freeways considering\nthe potential congested road segments, which are called\npotential-homogeneous-area. The proposed approach is based on the intuition\nthat the evolution of congestion may affect the neighbor segments due to the\nmobility of traffic flow. We identify potential-homogeneous-area by applying\nour proposed temporal-spatial lambda-connectedness method using historical\ntraffic data. Further, global dynamic capacity constraint of this area is\nintegrated with cell transmission model (CTM) in the traffic optimization\nproblem. To reduce computational complexity and improve scalability, we propose\na fully distributed algorithm to solve the problem, which is based on the\npartial augmented Lagrangian and dual-consensus alternating direction method of\nmultipliers (ADMM). By this means, distributed coordination of ramp metering\nand variable speed limit control is achieved. We prove that the proposed\nalgorithm converges to the optimal solution so long as the traffic optimization\nobjective is convex. The performance of the proposed method is evaluated by\nmacroscopic simulation using real data of Shanghai, China.",
    "descriptor": "",
    "authors": [
      "Fengkun Gao",
      "Bo Yang",
      "Cailian Chen",
      "Xinping Guan",
      "Yang Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06201"
  },
  {
    "id": "arXiv:2106.06203",
    "title": "Inter-Plane Inter-Satellite Connectivity in LEO Constellations: Beam  Switching vs. Beam Steering",
    "abstract": "Low Earth orbit (LEO) satellite constellations rely on inter-satellite links\n(ISLs) to provide global connectivity. However, one significant challenge is to\nestablish and maintain inter-plane ISLs, which support communication between\ndifferent orbital planes. This is due to the fast movement of the\ninfrastructure and to the limited computation and communication capabilities on\nthe satellites. In this paper, we make use of antenna arrays with either Butler\nmatrix beam switching networks or digital beam steering to establish the\ninter-plane ISLs in a LEO satellite constellation. Furthermore, we present a\ngreedy matching algorithm to establish inter-plane ISLs with the objective of\nmaximizing the sum of rates. This is achieved by sequentially selecting the\npairs, switching or pointing the beams and, finally, setting the data rates.\nOur results show that, by selecting an update period of 30 seconds for the\nmatching, reliable communication can be achieved throughout the constellation,\nwhere the impact of interference in the rates is less than 0.7 % when compared\nto orthogonal links, even for relatively small antenna arrays. Furthermore,\ndoubling the number of antenna elements increases the rates by around one order\nof magnitude.",
    "descriptor": "\nComments: Submitted to IEEE GLOBECOM 2021\n",
    "authors": [
      "Israel Leyva-Mayorga",
      "Maik R\u00f6per",
      "Bho Matthiesen",
      "Armin Dekorsy",
      "Petar Popovski",
      "Beatriz Soret"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.06203"
  },
  {
    "id": "arXiv:2106.06210",
    "title": "Learning to Pool in Graph Neural Networks for Extrapolation",
    "abstract": "Graph neural networks (GNNs) are one of the most popular approaches to using\ndeep learning on graph-structured data, and they have shown state-of-the-art\nperformances on a variety of tasks. However, according to a recent study, a\ncareful choice of pooling functions, which are used for the aggregation or\nreadout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without\nthe ideal combination of pooling functions, which varies across tasks, GNNs\ncompletely fail to generalize to out-of-distribution data, while the number of\npossible combinations grows exponentially with the number of layers. In this\npaper, we present GNP, a $L^p$ norm-like pooling function that is trainable\nend-to-end for any given task. Notably, GNP generalizes most of the widely-used\npooling functions. We verify experimentally that simply replacing all pooling\nfunctions with GNP enables GNNs to extrapolate well on many node-level,\ngraph-level, and set-related tasks; and GNP sometimes performs even better than\noptimal combinations of existing pooling functions.",
    "descriptor": "",
    "authors": [
      "Jihoon Ko",
      "Taehyung Kwon",
      "Kijung Shin",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06210"
  },
  {
    "id": "arXiv:2106.06213",
    "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity  understanding and detection",
    "abstract": "Traditional toxicity detection models have focused on the single utterance\nlevel without deeper understanding of context. We introduce CONDA, a new\ndataset for in-game toxic language detection enabling joint intent\nclassification and slot filling analysis, which is the core task of Natural\nLanguage Understanding (NLU). The dataset consists of 45K utterances from 12K\nconversations from the chat logs of 1.9K completed Dota 2 matches. We propose a\nrobust dual semantic-level toxicity framework, which handles utterance and\ntoken-level patterns, and rich contextual chatting history. Accompanying the\ndataset is a thorough in-game toxicity analysis, which provides comprehensive\nunderstanding of context at utterance, token, and dual levels. Inspired by NLU,\nwe also apply its metrics to the toxicity detection tasks for assessing\ntoxicity and game-specific aspects. We evaluate strong NLU models on CONDA,\nproviding fine-grained results for different intent classes and slot classes.\nFurthermore, we examine the coverage of toxicity nature in our dataset by\ncomparing it with other toxicity datasets.",
    "descriptor": "",
    "authors": [
      "Henry Weld",
      "Guanghao Huang",
      "Jean Lee",
      "Tongshu Zhang",
      "Kunze Wang",
      "Xinghong Guo",
      "Siqu Long",
      "Josiah Poon",
      "Soyeon Caren Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06213"
  },
  {
    "id": "arXiv:2106.06216",
    "title": "Nested and Balanced Entity Recognition using Multi-Task Learning",
    "abstract": "Entity Recognition (ER) within a text is a fundamental exercise in Natural\nLanguage Processing, enabling further depending tasks such as Knowledge\nExtraction, Text Summarisation, or Keyphrase Extraction. An entity consists of\nsingle words or of a consecutive sequence of terms, constituting the basic\nbuilding blocks for communication. Mainstream ER approaches are mainly limited\nto flat structures, concentrating on the outermost entities while ignoring the\ninner ones. This paper introduces a partly-layered network architecture that\ndeals with the complexity of overlapping and nested cases. The proposed\narchitecture consists of two parts: (1) a shared Sequence Layer and (2) a\nstacked component with multiple Tagging Layers. The adoption of such an\narchitecture has the advantage of preventing overfit to a specific word-length,\nthus maintaining performance for longer entities despite their lower frequency.\nTo verify the proposed architecture's effectiveness, we train and evaluate this\narchitecture to recognise two kinds of entities - Concepts (CR) and Named\nEntities (NER). Our approach achieves state-of-the-art NER performances, while\nit outperforms previous CR approaches. Considering these promising results, we\nsee the possibility to evolve the architecture for other cases such as the\nextraction of events or the detection of argumentative components.",
    "descriptor": "",
    "authors": [
      "Andreas Waldis",
      "Luca Mazzola"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06216"
  },
  {
    "id": "arXiv:2106.06218",
    "title": "Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs",
    "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields due\nto their powerful representations of graph-structured data. Despite the success\nof GNNs, most existing GNNs are designed to learn node representations on the\nfixed and homogeneous graphs. The limitations especially become problematic\nwhen learning representations on a misspecified graph or a heterogeneous graph\nthat consists of various types of nodes and edges. To address this limitations,\nwe propose Graph Transformer Networks (GTNs) that are capable of generating new\ngraph structures, which preclude noisy connections and include useful\nconnections (e.g., meta-paths) for tasks, while learning effective node\nrepresentations on the new graphs in an end-to-end fashion. We further propose\nenhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that\nimprove scalability of graph transformations. Compared to GTNs, FastGTNs are\n230x faster and use 100x less memory while allowing the identical graph\ntransformations as GTNs. In addition, we extend graph transformations to the\nsemantic proximity of nodes allowing non-local operations beyond meta-paths.\nExtensive experiments on both homogeneous graphs and heterogeneous graphs show\nthat GTNs and FastGTNs with non-local operations achieve the state-of-the-art\nperformance for node classification tasks. The code is available:\nhttps://github.com/seongjunyun/Graph_Transformer_Networks",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1911.06455\n",
    "authors": [
      "Seongjun Yun",
      "Minbyul Jeong",
      "Sungdong Yoo",
      "Seunghun Lee",
      "Sean S. Yi",
      "Raehyun Kim",
      "Jaewoo Kang",
      "Hyunwoo J. Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.06218"
  },
  {
    "id": "arXiv:2106.06219",
    "title": "Robin Pre-Training for the Deep Ritz Method",
    "abstract": "We compare different training strategies for the Deep Ritz Method for\nelliptic equations with Dirichlet boundary conditions and highlight the\nproblems arising from the boundary values. We distinguish between an exact\nresolution of the boundary values by introducing a distance function and the\napproximation through a Robin Boundary Value problem. However, distance\nfunctions are difficult to obtain for complex domains. Therefore, it is more\nfeasible to solve a Robin Boundary Value problem which approximates the\nsolution to the Dirichlet Boundary Value problem, yet the na\\\"ive approach to\nthis problem becomes unstable for large penalizations. A novel method to\ncompensate this problem is proposed using a small penalization strength to\npre-train the model before the main training on the target penalization\nstrength is conducted. We present numerical and theoretical evidence that the\nproposed method is beneficial.",
    "descriptor": "",
    "authors": [
      "Luca Courte",
      "Marius Zeinhofer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06219"
  },
  {
    "id": "arXiv:2106.06220",
    "title": "On the efficiency of decentralized epidemic management and application  to Covid-19",
    "abstract": "In this paper, we introduce a game that allows one to assess the potential\nloss of efficiency induced by a decentralized control or local management of a\nglobal epidemic. Each player typically represents a region or a country which\nis assumed to choose its control action to implement a tradeoff between\nsocioeconomic aspects and the health aspect. We conduct the Nash equilibrium\nanalysis of this game. Since the analysis is not trivial in general, sufficient\nconditions for existence and uniqueness are provided. Then we quantify through\nnumerical results the loss induced by decentralization, measured in terms of\nprice of anarchy (PoA) and price of connectedness (PoC). These results allow\none to clearly identify scenarios where decentralization is acceptable or not\nregarding to the retained global efficiency measures.",
    "descriptor": "\nComments: Accepted for publication in LCSS\n",
    "authors": [
      "Olivier Lindamulage De Silva",
      "Samson Lasaulce",
      "Irinel-Constantin Mor\u0103rescu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06220"
  },
  {
    "id": "arXiv:2106.06224",
    "title": "A Cooperative-Competitive Multi-Agent Framework for Auto-bidding in  Online Advertising",
    "abstract": "In online advertising, auto-bidding has become an essential tool for\nadvertisers to optimize their preferred ad performance metrics by simply\nexpressing the high-level campaign objectives and constraints. Previous works\nconsider the design of auto-bidding agents from the single-agent view without\nmodeling the mutual influence between agents. In this paper, we instead\nconsider this problem from the perspective of a distributed multi-agent system,\nand propose a general Multi-Agent reinforcement learning framework for\nAuto-Bidding, namely MAAB, to learn the auto-bidding strategies. First, we\ninvestigate the competition and cooperation relation among auto-bidding agents,\nand propose temperature-regularized credit assignment for establishing a mixed\ncooperative-competitive paradigm. By carefully making a competition and\ncooperation trade-off among the agents, we can reach an equilibrium state that\nguarantees not only individual advertiser's utility but also the system\nperformance (social welfare). Second, due to the observed collusion behaviors\nof bidding low prices underlying the cooperation, we further propose bar agents\nto set a personalized bidding bar for each agent, and then to alleviate the\ndegradation of revenue. Third, to deploy MAAB to the large-scale advertising\nsystem with millions of advertisers, we propose a mean-field approach. By\ngrouping advertisers with the same objective as a mean auto-bidding agent, the\ninteractions among advertisers are greatly simplified, making it practical to\ntrain MAAB efficiently. Extensive experiments on the offline industrial dataset\nand Alibaba advertising platform demonstrate that our approach outperforms\nseveral baseline methods in terms of social welfare and guarantees the ad\nplatform's revenue.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Chao Wen",
      "Miao Xu",
      "Zhilin Zhang",
      "Zhenzhe Zheng",
      "Yuhui Wang",
      "Xiangyu Liu",
      "Yu Rong",
      "Dong Xie",
      "Xiaoyang Tan",
      "Chuan Yu",
      "Jian Xu",
      "Fan Wu",
      "Guihai Chen",
      "Xiaoqiang Zhu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.06224"
  },
  {
    "id": "arXiv:2106.06228",
    "title": "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via  Synchronous Semantic Decoding",
    "abstract": "Semantic parsing is challenging due to the structure gap and the semantic gap\nbetween utterances and logical forms. In this paper, we propose an unsupervised\nsemantic parsing method - Synchronous Semantic Decoding (SSD), which can\nsimultaneously resolve the semantic gap and the structure gap by jointly\nleveraging paraphrasing and grammar constrained decoding. Specifically, we\nreformulate semantic parsing as a constrained paraphrasing problem: given an\nutterance, our model synchronously generates its canonical utterance and\nmeaning representation. During synchronous decoding: the utterance paraphrasing\nis constrained by the structure of the logical form, therefore the canonical\nutterance can be paraphrased controlledly; the semantic decoding is guided by\nthe semantics of the canonical utterance, therefore its logical form can be\ngenerated unsupervisedly. Experimental results show that SSD is a promising\napproach and can achieve competitive unsupervised semantic parsing performance\non multiple datasets.",
    "descriptor": "\nComments: Accepted by ACL 2021\n",
    "authors": [
      "Shan Wu",
      "Bo Chen",
      "Chunlei Xin",
      "Xianpei Han",
      "Le Sun",
      "Weipeng Zhang",
      "Jiansong Chen",
      "Fan Yang",
      "Xunliang Cai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06228"
  },
  {
    "id": "arXiv:2106.06230",
    "title": "Sprachsynthese -- State-of-the-Art in englischer und deutscher Sprache",
    "abstract": "Reading text aloud is an important feature for modern computer applications.\nIt not only facilitates access to information for visually impaired people, but\nis also a pleasant convenience for non-impaired users. In this article, the\nstate of the art of speech synthesis is presented separately for\nmel-spectrogram generation and vocoders. It concludes with an overview of\navailable data sets for English and German with a discussion of the\ntransferability of the good speech synthesis results from English to German\nlanguage.",
    "descriptor": "\nComments: in German\n",
    "authors": [
      "Ren\u00e9 Peinl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06230"
  },
  {
    "id": "arXiv:2106.06232",
    "title": "GDI: Rethinking What Makes Reinforcement Learning Different From  Supervised Learning",
    "abstract": "Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200 training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.",
    "descriptor": "",
    "authors": [
      "Jiajun Fan",
      "Changnan Xiao",
      "Yue Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06232"
  },
  {
    "id": "arXiv:2106.06233",
    "title": "Spoken Style Learning with Multi-modal Hierarchical Context Encoding for  Conversational Text-to-Speech Synthesis",
    "abstract": "For conversational text-to-speech (TTS) systems, it is vital that the systems\ncan adjust the spoken styles of synthesized speech according to different\ncontent and spoken styles in historical conversations. However, the study about\nlearning spoken styles from historical conversations is still in its infancy.\nOnly the transcripts of the historical conversations are considered, which\nneglects the spoken styles in historical speeches. Moreover, only the\ninteractions of the global aspect between speakers are modeled, missing the\nparty aspect self interactions inside each speaker. In this paper, to achieve\nbetter spoken style learning for conversational TTS, we propose a spoken style\nlearning approach with multi-modal hierarchical context encoding. The textual\ninformation and spoken styles in the historical conversations are processed\nthrough multiple hierarchical recurrent neural networks to learn the spoken\nstyle related features in global and party aspects. The attention mechanism is\nfurther employed to summarize these features into a conversational context\nencoding. Experimental results demonstrate the effectiveness of our proposed\napproach, which outperform a baseline method using context encoding learnt only\nfrom the transcripts in global aspects, with MOS score on the naturalness of\nsynthesized speech increasing from 3.138 to 3.408 and ABX preference rate\nexceeding the baseline method by 36.45%.",
    "descriptor": "",
    "authors": [
      "Jingbei Li",
      "Yi Meng",
      "Chenyi Li",
      "Zhiyong Wu",
      "Helen Meng",
      "Chao Weng",
      "Dan Su"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06233"
  },
  {
    "id": "arXiv:2106.06234",
    "title": "A deep learning approach to clustering visual arts",
    "abstract": "Clustering artworks is difficult for several reasons. On the one hand,\nrecognizing meaningful patterns based on domain knowledge and visual perception\nis extremely hard. On the other hand, applying traditional clustering and\nfeature reduction techniques to the highly dimensional pixel space can be\nineffective. To address these issues, in this paper we propose DELIUS: a DEep\nlearning approach to cLustering vIsUal artS. The method uses a pre-trained\nconvolutional network to extract features and then feeds these features into a\ndeep embedded clustering model, where the task of mapping the raw input data to\na latent space is jointly optimized with the task of finding a set of cluster\ncentroids in this latent space. Quantitative and qualitative experimental\nresults show the effectiveness of the proposed method. DELIUS can be useful for\nseveral tasks related to art analysis, in particular visual link retrieval and\nhistorical knowledge discovery in painting datasets.",
    "descriptor": "\nComments: Submitted to IJCV\n",
    "authors": [
      "Giovanna Castellano",
      "Gennaro Vessio"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06234"
  },
  {
    "id": "arXiv:2106.06235",
    "title": "Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial  Attacks",
    "abstract": "Despite the great successes achieved by deep neural networks (DNNs), recent\nstudies show that they are vulnerable against adversarial examples, which aim\nto mislead DNNs by adding small adversarial perturbations. Several defenses\nhave been proposed against such attacks, while many of them have been\nadaptively attacked. In this work, we aim to enhance the ML robustness from a\ndifferent perspective by leveraging domain knowledge: We propose a Knowledge\nEnhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e.,\nlogic relationships among different predictions) into a probabilistic graphical\nmodel via first-order logic rules. In particular, we develop KEMLP by\nintegrating a diverse set of weak auxiliary models based on their logical\nrelationships to the main DNN model that performs the target task.\nTheoretically, we provide convergence results and prove that, under mild\nconditions, the prediction of KEMLP is more robust than that of the main DNN\nmodel. Empirically, we take road sign recognition as an example and leverage\nthe relationships between road signs and their shapes and contents as domain\nknowledge. We show that compared with adversarial training and other baselines,\nKEMLP achieves higher robustness against physical attacks, $\\mathcal{L}_p$\nbounded attacks, unforeseen attacks, and natural corruptions under both\nwhitebox and blackbox settings, while still maintaining high clean accuracy.",
    "descriptor": "\nComments: International Conference on Machine Learning 2021, 37 pages, 8 figures, 9 tables\n",
    "authors": [
      "Nezihe Merve G\u00fcrel",
      "Xiangyu Qi",
      "Luka Rimanic",
      "Ce Zhang",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06235"
  },
  {
    "id": "arXiv:2106.06238",
    "title": "Approximation error method for imaging the human head by electrical  impedance tomography",
    "abstract": "This work considers electrical impedance tomography imaging of the human\nhead, with the ultimate goal of locating and classifying a stroke in emergency\ncare. One of the main difficulties in the envisioned application is that the\nelectrode locations and the shape of the head are not precisely known, leading\nto significant imaging artifacts due to impedance tomography being sensitive to\nmodeling errors. In this study, the natural variations in the geometry of the\nhead and skull are modeled based on a library of head anatomies. The effect of\nthese variations, as well as that of misplaced electrodes, on (absolute)\nimpedance tomography measurements is in turn modeled by the approximation error\nmethod. This enables reliably reconstructing the conductivity perturbation\ncaused by the stroke in an average head model, instead of the actual head,\nrelative to its average conductivity levels. The functionality of a certain\nedge-preferring reconstruction algorithm for locating the stroke is\ndemonstrated via numerical experiments based on simulated three-dimensional\ndata.",
    "descriptor": "\nComments: 24 pages, 5 figures\n",
    "authors": [
      "Valentina Candiani",
      "Nuutti Hyv\u00f6nen",
      "Jari P. Kaipio",
      "Ville Kolehmainen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06238"
  },
  {
    "id": "arXiv:2106.06239",
    "title": "Safe Reinforcement Learning with Linear Function Approximation",
    "abstract": "Safety in reinforcement learning has become increasingly important in recent\nyears. Yet, existing solutions either fail to strictly avoid choosing unsafe\nactions, which may lead to catastrophic results in safety-critical systems, or\nfail to provide regret guarantees for settings where safety constraints need to\nbe learned. In this paper, we address both problems by first modeling safety as\nan unknown linear cost function of states and actions, which must always fall\nbelow a certain threshold. We then present algorithms, termed SLUCB-QVI and\nRSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function\napproximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \\emph{no\nsafety violation}, achieve a\n$\\tilde{\\mathcal{O}}\\left(\\kappa\\sqrt{d^3H^3T}\\right)$ regret, nearly matching\nthat of state-of-the-art unsafe algorithms, where $H$ is the duration of each\nepisode, $d$ is the dimension of the feature mapping, $\\kappa$ is a constant\ncharacterizing the safety constraints, and $T$ is the total number of action\nplays. We further present numerical simulations that corroborate our\ntheoretical findings.",
    "descriptor": "",
    "authors": [
      "Sanae Amani",
      "Christos Thrampoulidis",
      "Lin F. Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06239"
  },
  {
    "id": "arXiv:2106.06242",
    "title": "Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2  Instance",
    "abstract": "Video streaming became an undivided part of the Internet. To efficiently\nutilize the limited network bandwidth it is essential to encode the video\ncontent. However, encoding is a computationally intensive task, involving\nhigh-performance resources provided by private infrastructures or public\nclouds. Public clouds, such as Amazon EC2, provide a large portfolio of\nservices and instances optimized for specific purposes and budgets. The\nmajority of Amazon instances use x86 processors, such as Intel Xeon or AMD\nEPYC. However, following the recent trends in computer architecture, Amazon\nintroduced Arm-based instances that promise up to 40% better cost-performance\nratio than comparable x86 instances for specific workloads. We evaluate in this\npaper the video encoding performance of x86 and Arm instances of four instance\nfamilies using the latest FFmpeg version and two video codecs. We examine the\nimpact of the encoding parameters, such as different presets and bitrates, on\nthe time and cost for encoding. Our experiments reveal that Arm instances show\nhigh time and cost-saving potential of up to 33.63% for specific bitrates and\npresets, especially for the x264 codec. However, the x86 instances are more\ngeneral and achieve low encoding times, regardless of the codec.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Roland Math\u00e1",
      "Dragi Kimovski",
      "Anatoliy Zabrovskiy",
      "Christian Timmerer",
      "Radu Prodan"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2106.06242"
  },
  {
    "id": "arXiv:2106.06244",
    "title": "Predicting Knowledge Gain during Web Search based on Multimedia Resource  Consumption",
    "abstract": "In informal learning scenarios the popularity of multimedia content, such as\nvideo tutorials or lectures, has significantly increased. Yet, the users'\ninteractions, navigation behavior, and consequently learning outcome, have not\nbeen researched extensively. Related work in this field, also called search as\nlearning, has focused on behavioral or text resource features to predict\nlearning outcome and knowledge gain. In this paper, we investigate whether we\ncan exploit features representing multimedia resource consumption to predict of\nknowledge gain (KG) during Web search from in-session data, that is without\nprior knowledge about the learner. For this purpose, we suggest a set of\nmultimedia features related to image and video consumption. Our feature\nextraction is evaluated in a lab study with 113 participants where we collected\ndata for a given search as learning task on the formation of thunderstorms and\nlightning. We automatically analyze the monitored log data and utilize\nstate-of-the-art computer vision methods to extract features about the seen\nmultimedia resources. Experimental results demonstrate that multimedia features\ncan improve KG prediction. Finally, we provide an analysis on feature\nimportance (text and multimedia) for KG prediction.",
    "descriptor": "\nComments: 13 pages, 2 figures, 2 tables\n",
    "authors": [
      "Christian Otto",
      "Ran Yu",
      "Georg Pardi",
      "Johannes von Hoyer",
      "Markus Rokicki",
      "Anett Hoppe",
      "Peter Holtz",
      "Yvonne Kammerer",
      "Stefan Dietze",
      "Ralph Ewerth"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.06244"
  },
  {
    "id": "arXiv:2106.06247",
    "title": "FedNLP: An interpretable NLP System to Decode Federal Reserve  Communications",
    "abstract": "The Federal Reserve System (the Fed) plays a significant role in affecting\nmonetary policy and financial conditions worldwide. Although it is important to\nanalyse the Fed's communications to extract useful information, it is generally\nlong-form and complex due to the ambiguous and esoteric nature of content. In\nthis paper, we present FedNLP, an interpretable multi-component Natural\nLanguage Processing system to decode Federal Reserve communications. This\nsystem is designed for end-users to explore how NLP techniques can assist their\nholistic understanding of the Fed's communications with NO coding. Behind the\nscenes, FedNLP uses multiple NLP models from traditional machine learning\nalgorithms to deep neural network architectures in each downstream task. The\ndemonstration shows multiple results at once including sentiment analysis,\nsummary of the document, prediction of the Federal Funds Rate movement and\nvisualization for interpreting the prediction model's result.",
    "descriptor": "\nComments: Accepted by SIGIR 2021\n",
    "authors": [
      "Jean Lee",
      "Hoyoul Luis Youn",
      "Nicholas Stevens",
      "Josiah Poon",
      "Soyeon Caren Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06247"
  },
  {
    "id": "arXiv:2106.06249",
    "title": "Matching Patterns with Variables under Hamming Distance",
    "abstract": "A pattern $\\alpha$ is a string of variables and terminal letters. We say that\n$\\alpha$ matches a word $w$, consisting only of terminal letters, if $w$ can be\nobtained by replacing the variables of $\\alpha$ by terminal words. The matching\nproblem, i.e., deciding whether a given pattern matches a given word, was\nheavily investigated: it is NP-complete in general, but can be solved\nefficiently for classes of patterns with restricted structure. In this paper,\nwe approach this problem in a generalized setting, by considering approximate\npattern matching under Hamming distance. More precisely, we are interested in\nwhat is the minimum Hamming distance between $w$ and any word $u$ obtained by\nreplacing the variables of $\\alpha$ by terminal words. Firstly, we address the\nclass of regular patterns (in which no variable occurs twice) and propose\nefficient algorithms for this problem, as well as matching conditional lower\nbounds. We show that the problem can still be solved efficiently if we allow\nrepeated variables, but restrict the way the different variables can be\ninterleaved according to a locality parameter. However, as soon as we allow a\nvariable to occur more than once and its occurrences can be interleaved\narbitrarily with those of other variables, even if none of them occurs more\nthan once, the problem becomes intractable.",
    "descriptor": "",
    "authors": [
      "Pawe\u0142 Gawrychowski",
      "Florin Manea",
      "Stefan Siemer"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.06249"
  },
  {
    "id": "arXiv:2106.06250",
    "title": "AugNet: End-to-End Unsupervised Visual Representation Learning with  Image Augmentation",
    "abstract": "Most of the achievements in artificial intelligence so far were accomplished\nby supervised learning which requires numerous annotated training data and thus\ncosts innumerable manpower for labeling. Unsupervised learning is one of the\neffective solutions to overcome such difficulties. In our work, we propose\nAugNet, a new deep learning training paradigm to learn image features from a\ncollection of unlabeled pictures. We develop a method to construct the\nsimilarities between pictures as distance metrics in the embedding space by\nleveraging the inter-correlation between augmented versions of samples. Our\nexperiments demonstrate that the method is able to represent the image in low\ndimensional space and performs competitively in downstream tasks such as image\nclassification and image similarity comparison. Specifically, we achieved over\n60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised\nclustering, respectively. Moreover, unlike many deep-learning-based image\nretrieval algorithms, our approach does not require access to external\nannotated datasets to train the feature extractor, but still shows comparable\nor even better feature representation ability and easy-to-use characteristics.\nIn our evaluations, the method outperforms all the state-of-the-art image\nretrieval algorithms on some out-of-domain image datasets. The code for the\nmodel implementation is available at\nhttps://github.com/chenmingxiang110/AugNet.",
    "descriptor": "",
    "authors": [
      "Mingxiang Chen",
      "Zhanguo Chang",
      "Haonan Lu",
      "Bitao Yang",
      "Zhuang Li",
      "Liufang Guo",
      "Zhecheng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06250"
  },
  {
    "id": "arXiv:2106.06255",
    "title": "Improving Take-over Situation by Active Communication",
    "abstract": "In this short paper an idea is sketched, how to support drivers of an\nautonomous vehicle in taking back control of the vehicle after a longer section\nof autonomous cruising. The hypothesis is that a clear communication about the\nlocation and behavior of relevant objects in the environment will help the\ndriver to quickly grasp the situational context and thus support drivers in\nsafely handling the ongoing driving situation manually after take-over. Based\non this hypothesis, a research concept is sketched, which entails the necessary\ncomponents as well as the disciplines involved.",
    "descriptor": "",
    "authors": [
      "Monika Sester",
      "Mark Vollrath",
      "Hao Cheng"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.06255"
  },
  {
    "id": "arXiv:2106.06257",
    "title": "HPO-B: A Large-Scale Reproducible Benchmark for Black-Box HPO based on  OpenML",
    "abstract": "Hyperparameter optimization (HPO) is a core problem for the machine learning\ncommunity and remains largely unsolved due to the significant computational\nresources required to evaluate hyperparameter configurations. As a result, a\nseries of recent related works have focused on the direction of transfer\nlearning for quickly fine-tuning hyperparameters on a dataset. Unfortunately,\nthe community does not have a common large-scale benchmark for comparing HPO\nalgorithms. Instead, the de facto practice consists of empirical protocols on\narbitrary small-scale meta-datasets that vary inconsistently across\npublications, making reproducibility a challenge. To resolve this major\nbottleneck and enable a fair and fast comparison of black-box HPO methods on a\nlevel playing field, we propose HPO-B, a new large-scale benchmark in the form\nof a collection of meta-datasets. Our benchmark is assembled and preprocessed\nfrom the OpenML repository and consists of 176 search spaces (algorithms)\nevaluated sparsely on 196 datasets with a total of 6.4 million hyperparameter\nevaluations. For ensuring reproducibility on our benchmark, we detail explicit\nexperimental protocols, splits, and evaluation measures for comparing methods\nfor both non-transfer, as well as, transfer learning HPO.",
    "descriptor": "",
    "authors": [
      "Sebastian Pineda Arango",
      "Hadi S. Jomaa",
      "Martin Wistuba",
      "Josif Grabocka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06257"
  },
  {
    "id": "arXiv:2106.06258",
    "title": "DebiasGAN: Eliminating Position Bias in News Recommendation with  Adversarial Learning",
    "abstract": "News recommendation is important for improving news reading experience of\nusers. Users' news click behaviors are widely used for inferring user interests\nand predicting future clicks. However, click behaviors are heavily affected by\nthe biases brought by the positions of news displayed on the webpage. It is\nimportant to eliminate the effect of position biases on the recommendation\nmodel to accurately target user interests. In this paper, we propose a news\nrecommendation method named DebiasGAN that can effectively eliminate the effect\nof position biases via adversarial learning. We use a bias-aware click model to\ncapture the influence of position bias on click behaviors, and we use a\nbias-invariant click model with random candidate news positions to estimate the\nideally unbiased click scores. We apply adversarial learning techniques to the\nhidden representations learned by the two models to help the bias-invariant\nclick model capture the bias-independent interest of users on news.\nExperimental results on two real-world datasets show that DebiasGAN can\neffectively improve the accuracy of news recommendation by eliminating position\nbiases.",
    "descriptor": "",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Yongfeng Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.06258"
  },
  {
    "id": "arXiv:2106.06261",
    "title": "States of confusion: Eye and Head tracking reveal surgeons' confusion  during arthroscopic surgery",
    "abstract": "During arthroscopic surgeries, surgeons are faced with challenges like\ncognitive re-projection of the 2D screen output into the 3D operating site or\nnavigation through highly similar tissue. Training of these cognitive processes\ntakes much time and effort for young surgeons, but is necessary and crucial for\ntheir education. In this study we want to show how to recognize states of\nconfusion of young surgeons during an arthroscopic surgery, by looking at their\neye and head movements and feeding them to a machine learning model. With an\naccuracy of over 94\\% and detection speed of 0.039 seconds, our model is a step\ntowards online diagnostic and training systems for the perceptual-cognitive\nprocesses of surgeons during arthroscopic surgeries.",
    "descriptor": "",
    "authors": [
      "Benedikt Hosp",
      "Myat Su Yin",
      "peter Haddawy",
      "Ratthapoom Watcharporas",
      "paphon Sa-ngasoonsong",
      "Enkelejda Kasneci"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06261"
  },
  {
    "id": "arXiv:2106.06271",
    "title": "Polynomial propagation of moments in stochastic differential equations",
    "abstract": "We address the problem of approximating the moments of the solution,\n$\\boldsymbol{X}(t)$, of an It\\^o stochastic differential equation (SDE) with\ndrift and a diffusion terms over a time-grid $t_0, t_1, \\ldots, t_n$. In\nparticular, we assume an explicit numerical scheme for the generation of sample\npaths $\\hat{\\boldsymbol{X}}(t_0), \\ldots, \\hat{\\boldsymbol{X}}(t_n), \\ldots$\nand then obtain recursive equations that yield any desired non-central moment\nof $\\hat{\\boldsymbol{X}}(t_n)$ as a function of the initial condition\n$\\boldsymbol{X}_0$. The core of the methodology is the decomposition of the\nnumerical solution into a \"central part\" and an \"effective noise\" term. The\ncentral term is computed deterministically from the ordinary differential\nequation (ODE) that results from eliminating the diffusion term in the SDE,\nwhile the effective noise accounts for the stochastic deviation from the\nnumerical solution of the ODE. For simplicity, we describe algorithms based on\nan Euler-Maruyama integrator, but other explicit numerical schemes can be\nexploited in the same way. We also apply the moment approximations to construct\nestimates of the 1-dimensional marginal probability density functions of\n$\\hat{\\boldsymbol{X}}(t_n)$ based on a Gram-Charlier expansion. Both for the\napproximation of moments and 1-dimensional densities, we describe how to handle\nthe cases in which the initial condition is fixed (i.e., $\\boldsymbol{X}_0 =\n\\boldsymbol{x}_0$ for some known $\\boldsymbol{x_0}$) or random. In the latter\ncase, we resort to polynomial chaos expansion (PCE) schemes to approximate the\ntarget moments. The methodology has been inspired by the PCE and differential\nalgebra (DA) methods used for uncertainty propagation in astrodynamics\nproblems. Hence, we illustrate its application for the quantification of\nuncertainty in a 2-dimensional Keplerian orbit perturbed by a Wiener noise\nprocess.",
    "descriptor": "",
    "authors": [
      "Albert L\u00f3pez-Yela",
      "Joaquin Miguez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.06271"
  },
  {
    "id": "arXiv:2106.06272",
    "title": "Model-based Safety and Security Co-analysis: a Survey",
    "abstract": "We survey the state-of-the-art on model-based formalisms for safety and\nsecurity analysis, where safety refers to the absence of unintended failures,\nand security absence of malicious attacks. We consider ten model-based\nformalisms, comparing their modeling principles, the interaction between safety\nand security, and analysis methods. In each formalism, we model the classical\nLocked Door Example where possible. Our key finding is that the exact nature of\nsafety-security interaction is still ill-understood. Existing formalisms merge\nprevious safety and security formalisms, without introducing specific\nconstructs to model safety-security interactions, or metrics to analyze trade\noffs.",
    "descriptor": "",
    "authors": [
      "Christina Kolb",
      "Stefano M. Nicoletti",
      "Marijn Peppelman",
      "Mari\u00eblle Stoelinga"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06272"
  },
  {
    "id": "arXiv:2106.06273",
    "title": "TrafficStream: A Streaming Traffic Flow Forecasting Framework Based on  Graph Neural Networks and Continual Learning",
    "abstract": "With the rapid growth of traffic sensors deployed, a massive amount of\ntraffic flow data are collected, revealing the long-term evolution of traffic\nflows and the gradual expansion of traffic networks. How to accurately\nforecasting these traffic flow attracts the attention of researchers as it is\nof great significance for improving the efficiency of transportation systems.\nHowever, existing methods mainly focus on the spatial-temporal correlation of\nstatic networks, leaving the problem of efficiently learning models on networks\nwith expansion and evolving patterns less studied. To tackle this problem, we\npropose a Streaming Traffic Flow Forecasting Framework, TrafficStream, based on\nGraph Neural Networks (GNNs) and Continual Learning (CL), achieving accurate\npredictions and high efficiency. Firstly, we design a traffic pattern fusion\nmethod, cleverly integrating the new patterns that emerged during the long-term\nperiod into the model. A JS-divergence-based algorithm is proposed to mine new\ntraffic patterns. Secondly, we introduce CL to consolidate the knowledge\nlearned previously and transfer them to the current model. Specifically, we\nadopt two strategies: historical data replay and parameter smoothing. We\nconstruct a streaming traffic dataset to verify the efficiency and\neffectiveness of our model. Extensive experiments demonstrate its excellent\npotential to extract traffic patterns with high efficiency on long-term\nstreaming network scene. The source code is available at\nhttps://github.com/AprLie/TrafficStream.",
    "descriptor": "\nComments: IJCAI-2021\n",
    "authors": [
      "Xu Chen",
      "Junshan Wang",
      "Kunqing Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06273"
  },
  {
    "id": "arXiv:2106.06278",
    "title": "Union and intersection contracts are hard, actually",
    "abstract": "Union and intersection types are a staple of gradually typed language such as\nTypeScript. While it's long been recognized that union and intersection types\nare difficult to verify statically, it may appear at first that the dynamic\npart of gradual typing is actually pretty simple.\nIt turns out however, that in presence of higher-order contracts union and\nintersection are deceptively difficult. The literature on higher-order\ncontracts with union and intersection, while keenly aware of the fact, doesn't\nreally explain why. We point and illustrate the problems and trade-offs\ninherent to union and intersection contracts, via example and a survey of the\nliterature.",
    "descriptor": "",
    "authors": [
      "Teodoro Freund",
      "Yann Hamdaoui",
      "Arnaud Spiwack"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.06278"
  },
  {
    "id": "arXiv:2106.06280",
    "title": "A compact subcell WENO limiting strategy using immediate neighbors for  Runge-Kutta discontinuous Galerkin methods for unstructured meshes",
    "abstract": "In this paper, we generalize the compact subcell weighted essentially non\noscillatory (CSWENO) limiting strategy for Runge-Kutta discontinuous Galerkin\nmethod developed recently by us in 2021 for structured meshes to unstructured\ntriangular meshes. The main idea of the limiting strategy is to divide the\nimmediate neighbors of a given cell into the required stencil and to use a WENO\nreconstruction for limiting. This strategy can be applied for any type of WENO\nreconstruction. We have used the WENO reconstruction proposed by Zhu and Shu in\n2019 and provided accuracy tests and results for two-dimensional Burgers'\nequation and two dimensional Euler equations to illustrate the performance of\nthis limiting strategy.",
    "descriptor": "\nComments: 23 pages, 16 figures, 4 tables. arXiv admin note: text overlap with arXiv:1904.11147\n",
    "authors": [
      "S R Siva Prasad Kochi",
      "M Ramakrishna"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06280"
  },
  {
    "id": "arXiv:2106.06285",
    "title": "A discontinuous Galerkin overset scheme using WENO reconstruction and  subcells for two-dimensional problems",
    "abstract": "A new scheme for communication between overset grids using subcells and\nWeighted Essentially Non Oscillatory (WENO) reconstruction for two-dimensional\nproblems has been proposed. The effectiveness of this procedure is demonstrated\nusing the discontinuous Galerkin method (DGM). This scheme uses WENO\nreconstruction using cell averages by dividing the immediate neighbors into\nsubcells to find the degrees of freedom in cells near the overset interface.\nThis also has the added advantage that it also works as a limiter if a\ndiscontinuity passes through the overset interface. Accuracy tests to\ndemonstrate the maintenance of higher order are provided. Results containing\nshocks are also provided to demonstrate the limiter aspect of the data\ncommunication procedure.",
    "descriptor": "\nComments: 20 pages, 17 figures, 3 tables\n",
    "authors": [
      "S R Siva Prasad Kochi",
      "M Ramakrishna"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06285"
  },
  {
    "id": "arXiv:2106.06291",
    "title": "DRLD-SP: A Deep Reinforcement Learning-based Dynamic Service Placement  in Edge-Enabled Internet of Vehicles",
    "abstract": "The growth of 5G and edge computing has enabled the emergence of Internet of\nVehicles. It supports different types of services with different resource and\nservice requirements. However, limited resources at the edge, high mobility of\nvehicles, increasing demand, and dynamicity in service request-types have made\nservice placement a challenging task. A typical static placement solution is\nnot effective as it does not consider the traffic mobility and service\ndynamics. Handling dynamics in IoV for service placement is an important and\nchallenging problem which is the primary focus of our work in this paper. We\npropose a Deep Reinforcement Learning-based Dynamic Service Placement (DRLD-SP)\nframework with the objective of minimizing the maximum edge resource usage and\nservice delay while considering the vehicle's mobility, varying demand, and\ndynamics in the requests for different types of services. We use SUMO and\nMATLAB to carry out simulation experiments. The experimental results show that\nthe proposed DRLD-SP approach is effective and outperforms other static and\ndynamic placement approaches.",
    "descriptor": "\nComments: Submitted to IEEE Internet of Things Journal\n",
    "authors": [
      "Anum Talpur",
      "Mohan Gurusamy"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06291"
  },
  {
    "id": "arXiv:2106.06292",
    "title": "A Discussion on Building Practical NLP Leaderboards: The Case of Machine  Translation",
    "abstract": "Recent advances in AI and ML applications have benefited from rapid progress\nin NLP research. Leaderboards have emerged as a popular mechanism to track and\naccelerate progress in NLP through competitive model development. While this\nhas increased interest and participation, the over-reliance on single, and\naccuracy-based metrics have shifted focus from other important metrics that\nmight be equally pertinent to consider in real-world contexts. In this paper,\nwe offer a preliminary discussion of the risks associated with focusing\nexclusively on accuracy metrics and draw on recent discussions to highlight\nprescriptive suggestions on how to develop more practical and effective\nleaderboards that can better reflect the real-world utility of models.",
    "descriptor": "\nComments: pre-print: comments and suggestions welcome\n",
    "authors": [
      "Sebastin Santy",
      "Prasanta Bhattacharya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06292"
  },
  {
    "id": "arXiv:2106.06293",
    "title": "Acceleration-as-a-\u03bcService: A Cloud-native Monte-Carlo Option  Pricing Engine on CPUs, GPUs and Disaggregated FPGAs",
    "abstract": "The evolution of cloud applications into loosely-coupled microservices opens\nnew opportunities for hardware accelerators to improve workload performance.\nExisting accelerator techniques for cloud sacrifice the consolidation benefits\nof microservices. This paper presents CloudiFi, a framework to deploy and\ncompare accelerators as a cloud service. We evaluate our framework in the\ncontext of a financial workload and present early results indicating up to 485x\ngains in microservice response time.",
    "descriptor": "\nComments: 3 pages, 6 figures\n",
    "authors": [
      "Dionysios Diamantopoulos",
      "Raphael Polig",
      "Burkhard Ringlein",
      "Mitra Purandare",
      "Beat Weiss",
      "Christoph Hagleitner",
      "Mark Lantz",
      "Francois Abel"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06293"
  },
  {
    "id": "arXiv:2106.06295",
    "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
    "abstract": "Transformers with linearised attention (\"linear Transformers\") have\ndemonstrated the practical scalability and effectiveness of outer product-based\nFast Weight Programmers (FWPs) from the '90s. However, the original FWP\nformulation is more general than the one of linear Transformers: a slow neural\nnetwork (NN) continually reprograms the weights of a fast NN with arbitrary NN\narchitectures. In existing linear Transformers, both NNs are feedforward and\nconsist of a single layer. Here we explore new variations by adding recurrence\nto the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two\nsynthetic algorithmic tasks (code execution and sequential ListOps),\nWikitext-103 language models, and on the Atari 2600 2D game environment. Our\nmodels exhibit properties of Transformers and RNNs. In the reinforcement\nlearning setting, we report large improvements over LSTM in several Atari\ngames. Our code is public.",
    "descriptor": "",
    "authors": [
      "Kazuki Irie",
      "Imanol Schlag",
      "R\u00f3bert Csord\u00e1s",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06295"
  },
  {
    "id": "arXiv:2106.06297",
    "title": "Dynamic Language Models for Continuously Evolving Content",
    "abstract": "The content on the web is in a constant state of flux. New entities, issues,\nand ideas continuously emerge, while the semantics of the existing conversation\ntopics gradually shift. In recent years, pre-trained language models like BERT\ngreatly improved the state-of-the-art for a large spectrum of content\nunderstanding tasks. Therefore, in this paper, we aim to study how these\nlanguage models can be adapted to better handle continuously evolving web\ncontent. In our study, we first analyze the evolution of 2013 - 2019 Twitter\ndata, and unequivocally confirm that a BERT model trained on past tweets would\nheavily deteriorate when directly applied to data from later years. Then, we\ninvestigate two possible sources of the deterioration: the semantic shift of\nexisting tokens and the sub-optimal or failed understanding of new tokens. To\nthis end, we both explore two different vocabulary composition methods, as well\nas propose three sampling methods which help in efficient incremental training\nfor BERT-like models. Compared to a new model trained from scratch offline, our\nincremental training (a) reduces the training costs, (b) achieves better\nperformance on evolving content, and (c) is suitable for online deployment. The\nsuperiority of our methods is validated using two downstream tasks. We\ndemonstrate significant improvements when incrementally evolving the model from\na particular base year, on the task of Country Hashtag Prediction, as well as\non the OffensEval 2019 task.",
    "descriptor": "",
    "authors": [
      "Spurthi Amba Hombaiah",
      "Tao Chen",
      "Mingyang Zhang",
      "Michael Bendersky",
      "Marc Najork"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06297"
  },
  {
    "id": "arXiv:2106.06298",
    "title": "A Novel Approach to Lifelong Learning: The Plastic Support Structure",
    "abstract": "We propose a novel approach to lifelong learning, introducing a compact\nencapsulated support structure which endows a network with the capability to\nexpand its capacity as needed to learn new tasks while preventing the loss of\nlearned tasks. This is achieved by splitting neurons with high semantic drift\nand constructing an adjacent network to encode the new tasks at hand. We call\nthis the Plastic Support Structure (PSS), it is a compact structure to learn\nnew tasks that cannot be efficiently encoded in the existing structure of the\nnetwork. We validate the PSS on public datasets against existing lifelong\nlearning architectures, showing it performs similarly to them but without prior\nknowledge of the task and in some cases with fewer parameters and in a more\nunderstandable fashion where the PSS is an encapsulated container for specific\nfeatures related to specific tasks, thus making it an ideal \"add-on\" solution\nfor endowing a network to learn more tasks.",
    "descriptor": "",
    "authors": [
      "Georges Kanaan",
      "Kai Wen Zheng",
      "Lucas Fenaux"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06298"
  },
  {
    "id": "arXiv:2106.06304",
    "title": "Automated Configuration of Genetic Algorithms by Tuning for Anytime  Performance",
    "abstract": "Finding the best configuration of algorithms' hyperparameters for a given\noptimization problem is an important task in evolutionary computation. We\ncompare in this work the results of four different hyperparameter tuning\napproaches for a family of genetic algorithms on 25 diverse pseudo-Boolean\noptimization problems. More precisely, we compare previously obtained results\nfrom a grid search with those obtained from three automated configuration\ntechniques: iterated racing, mixed-integer parallel efficient global\noptimization, and mixed-integer evolutionary strategies.\nUsing two different cost metrics, expected running time and the area under\nthe empirical cumulative distribution function curve, we find that in several\ncases the best configurations with respect to expected running time are\nobtained when using the area under the empirical cumulative distribution\nfunction curve as the cost metric during the configuration process. Our results\nsuggest that even when interested in expected running time performance, it\nmight be preferable to use anytime performance measures for the configuration\ntask. We also observe that tuning for expected running time is much more\nsensitive with respect to the budget that is allocated to the target\nalgorithms.",
    "descriptor": "",
    "authors": [
      "Furong Ye",
      "Carola Doerr",
      "Hao Wang",
      "Thomas B\u00e4ck"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06304"
  },
  {
    "id": "arXiv:2106.06306",
    "title": "A survey on Functional Encryption",
    "abstract": "Functional Encryption (FE) expands traditional public-key encryption in two\ndifferent ways: it supports fine-grained access control and it allows to learn\na function of the encrypted data. In this paper, we review all FE classes,\ndescribing their functionalities and main characteristics. In particular, for\neach class we mention several schemes, providing their security assumptions and\ncomparing their properties. To our knowledge, this is the first survey that\nencompasses the entire FE family.",
    "descriptor": "",
    "authors": [
      "Carla Mascia",
      "Massimiliano Sala",
      "Irene Villa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06306"
  },
  {
    "id": "arXiv:2106.06307",
    "title": "Survey of Image Based Graph Neural Networks",
    "abstract": "In this survey paper, we analyze image based graph neural networks and\npropose a three-step classification approach. We first convert the image into\nsuperpixels using the Quickshift algorithm so as to reduce 30% of the input\ndata. The superpixels are subsequently used to generate a region adjacency\ngraph. Finally, the graph is passed through a state-of-art graph convolutional\nneural network to get classification scores. We also analyze the spatial and\nspectral convolution filtering techniques in graph neural networks.\nSpectral-based models perform better than spatial-based models and classical\nCNN with lesser compute cost.",
    "descriptor": "",
    "authors": [
      "Usman Nazir",
      "He Wang",
      "Murtaza Taj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06307"
  },
  {
    "id": "arXiv:2106.06308",
    "title": "The Complexity of Sparse Tensor PCA",
    "abstract": "We study the problem of sparse tensor principal component analysis: given a\ntensor $\\pmb Y = \\pmb W + \\lambda x^{\\otimes p}$ with $\\pmb W \\in\n\\otimes^p\\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover\nthe $k$-sparse unit vector $x \\in \\mathbb{R}^n$. The model captures both sparse\nPCA (in its Wigner form) and tensor PCA.\nFor the highly sparse regime of $k \\leq \\sqrt{n}$, we present a family of\nalgorithms that smoothly interpolates between a simple polynomial-time\nalgorithm and the exponential-time exhaustive search algorithm. For any $1 \\leq\nt \\leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio\n$\\lambda \\geq \\tilde{\\mathcal{O}} (\\sqrt{t} \\cdot (k/t)^{p/2})$ in time\n$\\tilde{\\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for\nthe matrix settings (in both the polynomial-time and sub-exponential time\nregimes).\nOur results naturally extend to the case of $r$ distinct $k$-sparse signals\nwith disjoint supports, with guarantees that are independent of the number of\nspikes. Even in the restricted case of sparse PCA, known algorithms only\nrecover the sparse vectors for $\\lambda \\geq \\tilde{\\mathcal{O}}(k \\cdot r)$\nwhile our algorithms require $\\lambda \\geq \\tilde{\\mathcal{O}}(k)$.\nFinally, by analyzing the low-degree likelihood ratio, we complement these\nalgorithmic results with rigorous evidence illustrating the trade-offs between\nsignal-to-noise ratio and running time. This lower bound captures the known\nlower bounds for both sparse PCA and tensor PCA. In this general model, we\nobserve a more intricate three-way trade-off between the number of samples $n$,\nthe sparsity $k$, and the tensor power $p$.",
    "descriptor": "",
    "authors": [
      "Davin Choo",
      "Tommaso d'Orsi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06308"
  },
  {
    "id": "arXiv:2106.06309",
    "title": "HUI-Audio-Corpus-German: A high quality TTS dataset",
    "abstract": "The increasing availability of audio data on the internet lead to a multitude\nof datasets for development and training of text to speech applications, based\non neural networks. Highly differing quality of voice, low sampling rates, lack\nof text normalization and disadvantageous alignment of audio samples to\ncorresponding transcript sentences still limit the performance of deep neural\nnetworks trained on this task. Additionally, data resources in languages like\nGerman are still very limited. We introduce the \"HUI-Audio-Corpus-German\", a\nlarge, open-source dataset for TTS engines, created with a processing pipeline,\nwhich produces high quality audio to transcription alignments and decreases\nmanual effort needed for creation.",
    "descriptor": "",
    "authors": [
      "Pascal Puchtler",
      "Johannes Wirth",
      "Ren\u00e9 Peinl"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06309"
  },
  {
    "id": "arXiv:2106.06312",
    "title": "Exploiting Record Similarity for Practical Vertical Federated Learning",
    "abstract": "As the privacy of machine learning has drawn increasing attention, federated\nlearning is introduced to enable collaborative learning without revealing raw\ndata. Notably, \\textit{vertical federated learning} (VFL), where parties share\nthe same set of samples but only hold partial features, has a wide range of\nreal-world applications. However, existing studies in VFL rarely study the\n``record linkage'' process. They either design algorithms assuming the data\nfrom different parties have been linked or use simple linkage methods like\nexact-linkage or top1-linkage. These approaches are unsuitable for many\napplications, such as the GPS location and noisy titles requiring fuzzy\nmatching. In this paper, we design a novel similarity-based VFL framework,\nFedSim, which is suitable for more real-world applications and achieves higher\nperformance on traditional VFL tasks. Moreover, we theoretically analyze the\nprivacy risk caused by sharing similarities. Our experiments on three synthetic\ndatasets and five real-world datasets with various similarity metrics show that\nFedSim consistently outperforms other state-of-the-art baselines.",
    "descriptor": "",
    "authors": [
      "Zhaomin Wu",
      "Qinbin Li",
      "Bingsheng He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06312"
  },
  {
    "id": "arXiv:2106.06313",
    "title": "Bridge the Gap Between Model-based and Model-free Human Reconstruction",
    "abstract": "It is challenging to directly estimate the geometry of human from a single\nimage due to the high diversity and complexity of body shapes with the various\nclothing styles. Most of model-based approaches are limited to predict the\nshape and pose of a minimally clothed body with over-smoothing surface.\nAlthough capturing the fine detailed geometries, the model-free methods are\nlack of the fixed mesh topology. To address these issues, we propose a novel\ntopology-preserved human reconstruction approach by bridging the gap between\nmodel-based and model-free human reconstruction. We present an end-to-end\nneural network that simultaneously predicts the pixel-aligned implicit surface\nand the explicit mesh model built by graph convolutional neural network.\nMoreover, an extra graph convolutional neural network is employed to estimate\nthe vertex offsets between the implicit surface and parametric mesh model.\nFinally, we suggest an efficient implicit registration method to refine the\nneural network output in implicit space. Experiments on DeepHuman dataset\nshowed that our approach is effective.",
    "descriptor": "",
    "authors": [
      "Lixiang Lin",
      "Jianke Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06313"
  },
  {
    "id": "arXiv:2106.06314",
    "title": "A Virtual Element Method for the wave equation on curved edges in two  dimensions",
    "abstract": "In this work we present an extension of the Virtual Element Method with\ncurved edges for the numerical approximation of the second order wave equation\nin a bidimensional setting. Curved elements are used to describe the domain\nboundary, as well as internal interfaces corresponding to the change of some\nmechanical parameters. As opposite to the classic and isoparametric Finite\nElement approaches, where the geometry of the domain is approximated\nrespectively by piecewise straight lines and by higher order polynomial maps,\nin the proposed method the geometry is exactly represented, thus ensuring a\nhighly accurate numerical solution. Indeed, if in the former approach the\ngeometrical error might deteriorate the quality of the numerical solution, in\nthe latter approach the curved interfaces/boundaries are approximated exactly\nguaranteeing the expected order of convergence for the numerical scheme.\nTheoretical results and numerical findings confirm the validity of the proposed\napproach.",
    "descriptor": "",
    "authors": [
      "Franco Dassi",
      "Alessio Fumagalli",
      "Ilario Mazzieri",
      "Anna Scotti",
      "Giuseppe Vacca"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06314"
  },
  {
    "id": "arXiv:2106.06317",
    "title": "Automatic Risk Adaptation in Distributional Reinforcement Learning",
    "abstract": "The use of Reinforcement Learning (RL) agents in practical applications\nrequires the consideration of suboptimal outcomes, depending on the familiarity\nof the agent with its environment. This is especially important in\nsafety-critical environments, where errors can lead to high costs or damage. In\ndistributional RL, the risk-sensitivity can be controlled via different\ndistortion measures of the estimated return distribution. However, these\ndistortion functions require an estimate of the risk level, which is difficult\nto obtain and depends on the current state. In this work, we demonstrate the\nsuboptimality of a static risk level estimation and propose a method to\ndynamically select risk levels at each environment step. Our method ARA\n(Automatic Risk Adaptation) estimates the appropriate risk level in both known\nand unknown environments using a Random Network Distillation error. We show\nreduced failure rates by up to a factor of 7 and improved generalization\nperformance by up to 14% compared to both risk-aware and risk-agnostic agents\nin several locomotion environments.",
    "descriptor": "",
    "authors": [
      "Frederik Schubert",
      "Theresa Eimer",
      "Bodo Rosenhahn",
      "Marius Lindauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06317"
  },
  {
    "id": "arXiv:2106.06321",
    "title": "ViT-Inception-GAN for Image Colourising",
    "abstract": "Studies involving colourising images has been garnering researchers' keen\nattention over time, assisted by significant advances in various Machine\nLearning techniques and compute power availability. Traditionally, colourising\nimages have been an intricate task that gave a substantial degree of freedom\nduring the assignment of chromatic information. In our proposed method, we\nattempt to colourise images using Vision Transformer - Inception - Generative\nAdversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in\nthe generator. For a stable and robust network, we have used Vision Transformer\n(ViT) as the discriminator. We trained the model on the Unsplash and the COCO\ndataset for demonstrating the improvement made by the Inception-v3 embedding.\nWe have compared the results between ViT-GANs with and without Inception-v3\nembedding.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Tejas Bana",
      "Jatan Loya",
      "Siddhant Kulkarni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06321"
  },
  {
    "id": "arXiv:2106.06326",
    "title": "TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation",
    "abstract": "In few-shot domain adaptation (FDA), classifiers for the target domain are\ntrained with accessible labeled data in the source domain (SD) and few labeled\ndata in the target domain (TD). However, data usually contain private\ninformation in the current era, e.g., data distributed on personal phones.\nThus, the private information will be leaked if we directly access data in SD\nto train a target-domain classifier (required by FDA methods). In this paper,\nto thoroughly prevent the privacy leakage in SD, we consider a very challenging\nproblem setting, where the classifier for the TD has to be trained using few\nlabeled target data and a well-trained SD classifier, named few-shot hypothesis\nadaptation (FHA). In FHA, we cannot access data in SD, as a result, the private\ninformation in SD will be protected well. To this end, we propose a target\norientated hypothesis adaptation network (TOHAN) to solve the FHA problem,\nwhere we generate highly-compatible unlabeled data (i.e., an intermediate\ndomain) to help train a target-domain classifier. TOHAN maintains two deep\nnetworks simultaneously, where one focuses on learning an intermediate domain\nand the other takes care of the intermediate-to-target distributional\nadaptation and the target-risk minimization. Experimental results show that\nTOHAN outperforms competitive baselines significantly.",
    "descriptor": "",
    "authors": [
      "Haoang Chi",
      "Feng Liu",
      "Wenjing Yang",
      "Long Lan",
      "Tongliang Liu",
      "Bo Han",
      "William K. Cheung",
      "James T. Kwok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06326"
  },
  {
    "id": "arXiv:2106.06330",
    "title": "Safety of Dynamical Systems with Multiple Non-Convex Unsafe Sets Using  Control Barrier Functions",
    "abstract": "This paper presents an approach to deal with safety of dynamical systems in\npresence of multiple non-convex unsafe sets. While optimal control and model\npredictive control strategies can be employed in these scenarios, they suffer\nfrom high computational complexity in case of general nonlinear systems.\nLeveraging control barrier functions, on the other hand, results in\ncomputationally efficient control algorithms. Nevertheless, when safety\nguarantees have to be enforced alongside stability objectives, undesired\nasymptotically stable equilibrium points have been shown to arise. We propose a\ncomputationally efficient optimization-based approach which allows us to ensure\nsafety of dynamical systems without introducing undesired equilibria even in\npresence of multiple non-convex unsafe sets. The developed control algorithm is\nshowcased in simulation and in a real robot navigation application.",
    "descriptor": "",
    "authors": [
      "Gennaro Notomista",
      "Matteo Saveriano"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.06330"
  },
  {
    "id": "arXiv:2106.06332",
    "title": "Smart textiles that teach: Fabric-based haptic device improves the rate  of motor learning",
    "abstract": "People learn motor activities best when they are conscious of their errors\nand make a concerted effort to correct them. While haptic interfaces can\nfacilitate motor training, existing interfaces are often bulky and do not\nalways ensure post-training skill retention. Here, we describe a programmable\nhaptic sleeve composed of textile-based electroadhesive clutches for skill\nacquisition and retention. We show its functionality in a motor learning study\nwhere users control a drone's movement using elbow joint rotation. Haptic\nfeedback is used to restrain elbow motion and make users aware of their errors.\nThis helps users consciously learn to avoid errors from occurring. While all\nsubjects exhibited similar performance during the baseline phase of motor\nlearning, those subjects who received haptic feedback from the haptic sleeve\ncommitted 23.5% fewer errors than subjects in the control group during the\nevaluation phase. The results show that the sleeve helps users retain and\ntransfer motor skills better than visual feedback alone. This work shows the\npotential for fabric-based haptic interfaces as a training aid for motor tasks\nin the fields of rehabilitation and teleoperation.",
    "descriptor": "",
    "authors": [
      "Vivek Ramachandran",
      "Fabian Schilling",
      "Amy Wu",
      "Dario Floreano"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.06332"
  },
  {
    "id": "arXiv:2106.06333",
    "title": "Invariant Information Bottleneck for Domain Generalization",
    "abstract": "The main challenge for domain generalization (DG) is to overcome the\npotential distributional shift between multiple training domains and unseen\ntest domains. One popular class of DG algorithms aims to learn representations\nthat have an invariant causal relation across the training domains. However,\ncertain features, called \\emph{pseudo-invariant features}, may be invariant in\nthe training domain but not the test domain and can substantially decreases the\nperformance of existing algorithms. To address this issue, we propose a novel\nalgorithm, called Invariant Information Bottleneck (IIB), that learns a\nminimally sufficient representation that is invariant across training and\ntesting domains. By minimizing the mutual information between the\nrepresentation and inputs, IIB alleviates its reliance on pseudo-invariant\nfeatures, which is desirable for DG. To verify the effectiveness of the IIB\nprinciple, we conduct extensive experiments on large-scale DG benchmarks. The\nresults show that IIB outperforms invariant learning baseline (e.g. IRM) by an\naverage of 2.8\\% and 3.8\\% accuracy over two evaluation metrics.",
    "descriptor": "",
    "authors": [
      "Bo Li",
      "Yifei Shen",
      "Yezhen Wang",
      "Wenzhen Zhu",
      "Colorado J. Reed",
      "Tong Che",
      "Jun Zhang",
      "Dongsheng Li",
      "Kurt Keutzer",
      "Han Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06333"
  },
  {
    "id": "arXiv:2106.06334",
    "title": "CommAID: Visual Analytics for Communication Analysis through Interactive  Dynamics Modeling",
    "abstract": "Communication consists of both meta-information as well as content.\nCurrently, the automated analysis of such data often focuses either on the\nnetwork aspects via social network analysis or on the content, utilizing\nmethods from text-mining. However, the first category of approaches does not\nleverage the rich content information, while the latter ignores the\nconversation environment and the temporal evolution, as evident in the\nmeta-information. In contradiction to communication research, which stresses\nthe importance of a holistic approach, both aspects are rarely applied\nsimultaneously, and consequently, their combination has not yet received enough\nattention in automated analysis systems. In this work, we aim to address this\nchallenge by discussing the difficulties and design decisions of such a path as\nwell as contribute CommAID, a blueprint for a holistic strategy to\ncommunication analysis. It features an integrated visual analytics design to\nanalyze communication networks through dynamics modeling, semantic pattern\nretrieval, and a user-adaptable and problem-specific machine learning-based\nretrieval system. An interactive multi-level matrix-based visualization\nfacilitates a focused analysis of both network and content using inline visuals\nsupporting cross-checks and reducing context switches. We evaluate our approach\nin both a case study and through formative evaluation with eight law\nenforcement experts using a real-world communication corpus. Results show that\nour solution surpasses existing techniques in terms of integration level and\napplicability. With this contribution, we aim to pave the path for a more\nholistic approach to communication analysis.",
    "descriptor": "\nComments: 12 pages, 7 figures, Computer Graphics Forum 2021 (pre-peer reviewed version)\n",
    "authors": [
      "Maximilian T. Fischer",
      "Daniel Seebacher",
      "Rita Sevastjanova",
      "Daniel A. Keim",
      "Mennatallah El-Assady"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.06334"
  },
  {
    "id": "arXiv:2106.06335",
    "title": "ERRANT: Realistic Emulation of Radio Access Networks",
    "abstract": "Mobile networks have become ubiquitous, but running experiments on them is\nexpensive and hard, given their complexity and diversity. Emulation can be the\nsolution, and, with ERRANT, we offer a realistic emulator of mobile networks\nbased on a large measurement campaign on European Mobile Network Operators. It\nimproves the current situation, where tools and emulators only implement\npre-defined profiles, with built-in parameters, which are not supported with\nreal measurements.",
    "descriptor": "",
    "authors": [
      "Martino Trevisan",
      "Ali Safari Khatouni",
      "Danilo Giordano"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.06335"
  },
  {
    "id": "arXiv:2106.06338",
    "title": "Dictionary and prior learning with unrolled algorithms for unsupervised  inverse problems",
    "abstract": "Inverse problems consist in recovering a signal given noisy observations. One\nclassical resolution approach is to leverage sparsity and integrate prior\nknowledge of the signal to the reconstruction algorithm to get a plausible\nsolution. Still, this prior might not be sufficiently adapted to the data. In\nthis work, we study Dictionary and Prior learning from degraded measurements as\na bi-level problem, and we take advantage of unrolled algorithms to solve\napproximate formulations of Synthesis and Analysis. We provide an empirical and\ntheoretical analysis of automatic differentiation for Dictionary Learning to\nunderstand better the pros and cons of unrolling in this context. We find that\nunrolled algorithms speed up the recovery process for a small number of\niterations by improving the gradient estimation. Then we compare Analysis and\nSynthesis by evaluating the performance of unrolled algorithms for inverse\nproblems, without access to any ground truth data for several classes of\ndictionaries and priors. While Analysis can achieve good results,Synthesis is\nmore robust and performs better. Finally, we illustrate our method on pattern\nand structure learning tasks from degraded measurements.",
    "descriptor": "",
    "authors": [
      "Beno\u00eet Mal\u00e9zieux",
      "Thomas Moreau",
      "Matthieu Kowalski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06338"
  },
  {
    "id": "arXiv:2106.06340",
    "title": "SimSwap: An Efficient Framework For High Fidelity Face Swapping",
    "abstract": "We propose an efficient framework, called Simple Swap (SimSwap), aiming for\ngeneralized and high fidelity face swapping. In contrast to previous approaches\nthat either lack the ability to generalize to arbitrary identity or fail to\npreserve attributes like facial expression and gaze direction, our framework is\ncapable of transferring the identity of an arbitrary source face into an\narbitrary target face while preserving the attributes of the target face. We\novercome the above defects in the following two ways. First, we present the ID\nInjection Module (IIM) which transfers the identity information of the source\nface into the target face at feature level. By using this module, we extend the\narchitecture of an identity-specific face swapping algorithm to a framework for\narbitrary face swapping. Second, we propose the Weak Feature Matching Loss\nwhich efficiently helps our framework to preserve the facial attributes in an\nimplicit way. Extensive experiments on wild faces demonstrate that our SimSwap\nis able to achieve competitive identity performance while preserving attributes\nbetter than previous state-of-the-art methods. The code is already available on\ngithub: https://github.com/neuralchen/SimSwap.",
    "descriptor": "\nComments: Accepted by ACMMM 2020\n",
    "authors": [
      "Renwang Chen",
      "Xuanhong Chen",
      "Bingbing Ni",
      "Yanhao Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06340"
  },
  {
    "id": "arXiv:2106.06343",
    "title": "Multiscale modeling of cancellous bone considering full coupling of  mechanical, electrical and magnetic effects",
    "abstract": "Modeling of cancellous bone has important applications in the detection and\ntreatment of fatigue fractures and diseases like osteoporosis. In this paper,\nwe present a fully coupled multiscale approach considering mechanical,\nelectrical and magnetic effects by using the multiscale finite element method\nand a two-phase material model on the microscale. We show numerical results for\nboth scales, including calculations for a femur bone, comparing a healthy bone\nto ones affected by different stages of osteoporosis. Here, the magnetic field\nstrength resulting from a small mechanical impact decreases drastically for\nlater stages of the disease, confirming experimental research.",
    "descriptor": "",
    "authors": [
      "Mischa Blaszczyk",
      "Klaus Hackl"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.06343"
  },
  {
    "id": "arXiv:2106.06345",
    "title": "JKOnet: Proximal Optimal Transport Modeling of Population Dynamics",
    "abstract": "Consider a heterogeneous population of points evolving with time. While the\npopulation evolves, both in size and nature, we can observe it periodically,\nthrough snapshots taken at different timestamps. Each of these snapshots is\nformed by sampling points from the population at that time, and then creating\nfeatures to recover point clouds. While these snapshots describe the\npopulation's evolution on aggregate, they do not provide directly insights on\nindividual trajectories. This scenario is encountered in several applications,\nnotably single-cell genomics experiments, tracking of particles, or when\nstudying crowd motion. In this paper, we propose to model that dynamic as\nresulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.\nThe JKO scheme posits that the configuration taken by a population at time $t$\nis one that trades off a decrease w.r.t. an energy (the model we seek to learn)\npenalized by an optimal transport distance w.r.t. the previous configuration.\nTo that end, we propose JKOnet, a neural architecture that combines an energy\nmodel on measures, with (small) optimal displacements solved with input convex\nneural networks (ICNN). We demonstrate the applicability of our model to\nexplain and predict population dynamics.",
    "descriptor": "",
    "authors": [
      "Charlotte Bunne",
      "Laetitia Meng-Papaxanthos",
      "Andreas Krause",
      "Marco Cuturi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06345"
  },
  {
    "id": "arXiv:2106.06351",
    "title": "Part-aware Panoptic Segmentation",
    "abstract": "In this work, we introduce the new scene understanding task of Part-aware\nPanoptic Segmentation (PPS), which aims to understand a scene at multiple\nlevels of abstraction, and unifies the tasks of scene parsing and part parsing.\nFor this novel task, we provide consistent annotations on two commonly used\ndatasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to\nevaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task,\nusing the metric and annotations, we set multiple baselines by merging results\nof existing state-of-the-art methods for panoptic segmentation and part\nsegmentation. Finally, we conduct several experiments that evaluate the\nimportance of the different levels of abstraction in this single task.",
    "descriptor": "\nComments: CVPR 2021. Code and data: this https URL\n",
    "authors": [
      "Daan de Geus",
      "Panagiotis Meletis",
      "Chenyang Lu",
      "Xiaoxiao Wen",
      "Gijs Dubbelman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06351"
  },
  {
    "id": "arXiv:2106.06356",
    "title": "Nonmyopic Multifidelity Active Search",
    "abstract": "Active search is a learning paradigm where we seek to identify as many\nmembers of a rare, valuable class as possible given a labeling budget. Previous\nwork on active search has assumed access to a faithful (and expensive) oracle\nreporting experimental results. However, some settings offer access to cheaper\nsurrogates such as computational simulation that may aid in the search. We\npropose a model of multifidelity active search, as well as a novel,\ncomputationally efficient policy for this setting that is motivated by\nstate-of-the-art classical policies. Our policy is nonmyopic and budget aware,\nallowing for a dynamic tradeoff between exploration and exploitation. We\nevaluate the performance of our solution on real-world datasets and demonstrate\nsignificantly better performance than natural benchmarks.",
    "descriptor": "\nComments: To appear in ICML 2021\n",
    "authors": [
      "Quan Nguyen",
      "Arghavan Modiri",
      "Roman Garnett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06356"
  },
  {
    "id": "arXiv:2106.06358",
    "title": "Projection-based resolved interface mixed-dimension method for embedded  tubular network systems",
    "abstract": "We present a flexible discretization technique for computational models of\nthin tubular networks embedded in a bulk domain, for example a porous medium.\nThese systems occur in the simulation of fluid flow in vascularized biological\ntissue, root water and nutrient uptake in soil, hydrological or petroleum wells\nin rock formations, or heat transport in micro-cooling devices. The key\nprocesses, such as heat and mass transfer, are usually dominated by the\nexchange between the network system and the embedding domain. By explicitly\nresolving the interface between these domains with the computational mesh, we\ncan accurately describe these processes. The network is efficiently described\nby a network of line segments. Coupling terms are evaluated by projection of\nthe interface variables. The new method is naturally applicable for nonlinear\nand time-dependent problems and can therefore be used as a reference method in\nthe development of novel implicit interface 1D-3D methods and in the design of\nverification benchmarks for embedded tubular network methods. Implicit\ninterface, not resolving the bulk-network interface explicitly have proven to\nbe very efficient but have only been mathematically analyzed for linear\nelliptic problems so far. Using two application scenarios, fluid perfusion of\nvascularized tissue and root water uptake from soil, we investigate the effect\nof some common modeling assumptions of implicit interface methods numerically.",
    "descriptor": "\nComments: 33 pages, 15 figures\n",
    "authors": [
      "Timo Koch"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.06358"
  },
  {
    "id": "arXiv:2106.06360",
    "title": "Conterfactual Generative Zero-Shot Semantic Segmentation",
    "abstract": "zero-shot learning is an essential part of computer vision. As a classical\ndownstream task, zero-shot semantic segmentation has been studied because of\nits applicant value. One of the popular zero-shot semantic segmentation methods\nis based on the generative model Most new proposed works added structures on\nthe same architecture to enhance this model. However, we found that, from the\nview of causal inference, the result of the original model has been influenced\nby spurious statistical relationships. Thus the performance of the prediction\nshows severe bias. In this work, we consider counterfactual methods to avoid\nthe confounder in the original model. Based on this method, we proposed a new\nframework for zero-shot semantic segmentation. Our model is compared with\nbaseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The\nexperiment results show proposed models can surpass previous confounded models\nand can still make use of additional structures to improve the performance. We\nalso design a simple structure based on Graph Convolutional Networks (GCN) in\nthis work.",
    "descriptor": "\nComments: 11 pages, 8 figures\n",
    "authors": [
      "Feihong Shen",
      "Jun Liu",
      "Ping Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06360"
  },
  {
    "id": "arXiv:2106.06361",
    "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word  Substitution",
    "abstract": "Recent studies show that neural natural language processing (NLP) models are\nvulnerable to backdoor attacks. Injected with backdoors, models perform\nnormally on benign examples but produce attacker-specified predictions when the\nbackdoor is activated, presenting serious security threats to real-world\napplications. Since existing textual backdoor attacks pay little attention to\nthe invisibility of backdoors, they can be easily detected and blocked. In this\nwork, we present invisible backdoors that are activated by a learnable\ncombination of word substitution. We show that NLP models can be injected with\nbackdoors that lead to a nearly 100% attack success rate, whereas being highly\ninvisible to existing defense strategies and even human inspections. The\nresults raise a serious alarm to the security of NLP models, which requires\nfurther research to be resolved. All the data and code of this paper are\nreleased at https://github.com/thunlp/BkdAtk-LWS.",
    "descriptor": "\nComments: Accepted by the main conference of ACL-IJCNLP as a long paper. Camera-ready version\n",
    "authors": [
      "Fanchao Qi",
      "Yuan Yao",
      "Sophia Xu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06361"
  },
  {
    "id": "arXiv:2106.06362",
    "title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker  Verification and Voice Anti-Spoofing",
    "abstract": "Whether it be for results summarization, or the analysis of classifier\nfusion, some means to compare different classifiers can often provide\nilluminating insight into their behaviour, (dis)similarity or complementarity.\nWe propose a simple method to derive 2D representation from detection scores\nproduced by an arbitrary set of binary classifiers in response to a common\ndataset. Based upon rank correlations, our method facilitates a visual\ncomparison of classifiers with arbitrary scores and with close relation to\nreceiver operating characteristic (ROC) and detection error trade-off (DET)\nanalyses. While the approach is fully versatile and can be applied to any\ndetection task, we demonstrate the method using scores produced by automatic\nspeaker verification and voice anti-spoofing systems. The former are produced\nby a Gaussian mixture model system trained with VoxCeleb data whereas the\nlatter stem from submissions to the ASVspoof 2019 challenge.",
    "descriptor": "\nComments: Accepted to Interspeech 2021. Example code available at this https URL\n",
    "authors": [
      "Tomi Kinnunen",
      "Andreas Nautsch",
      "Md Sahidullah",
      "Nicholas Evans",
      "Xin Wang",
      "Massimiliano Todisco",
      "H\u00e9ctor Delgado",
      "Junichi Yamagishi",
      "Kong Aik Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.06362"
  },
  {
    "id": "arXiv:2106.06363",
    "title": "To Beam Or Not To Beam: That is a Question of Cooperation for Language  GANs",
    "abstract": "Due to the discrete nature of words, language GANs require to be optimized\nfrom rewards provided by discriminator networks, via reinforcement learning\nmethods. This is a much harder setting than for continuous tasks, which enjoy\ngradient flows from discriminators to generators, usually leading to dramatic\nlearning instabilities. However, we claim that this can be solved by making\ndiscriminator and generator networks cooperate to produce output sequences\nduring training. These cooperative outputs, inherently built to obtain higher\ndiscrimination scores, not only provide denser rewards for training, but also\nform a more compact artificial set for discriminator training, hence improving\nits accuracy and stability. In this paper, we show that our SelfGAN framework,\nbuilt on this cooperative principle, outperforms Teacher Forcing and obtains\nstate-of-the-art results on two challenging tasks, Summarization and Question\nGeneration.",
    "descriptor": "",
    "authors": [
      "Thomas Scialom",
      "Paul-Alexis Dray",
      "Sylvain Lamprier",
      "Benjamin Piwowarski",
      "Jacopo Staiano"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06363"
  },
  {
    "id": "arXiv:2106.06369",
    "title": "Courteous Behavior of Automated Vehicles at Unsignalized Intersections  via Reinforcement Learning",
    "abstract": "The transition from today's mostly human-driven traffic to a purely automated\none will be a gradual evolution, with the effect that we will likely experience\nmixed traffic in the near future. Connected and automated vehicles can benefit\nhuman-driven ones and the whole traffic system in different ways, for example\nby improving collision avoidance and reducing traffic waves. Many studies have\nbeen carried out to improve intersection management, a significant bottleneck\nin traffic, with intelligent traffic signals or exclusively automated vehicles.\nHowever, the problem of how to improve mixed traffic at unsignalized\nintersections has received less attention. In this paper, we propose a novel\napproach to optimizing traffic flow at intersections in mixed traffic\nsituations using deep reinforcement learning. Our reinforcement learning agent\nlearns a policy for a centralized controller to let connected autonomous\nvehicles at unsignalized intersections give up their right of way and yield to\nother vehicles to optimize traffic flow. We implemented our approach and tested\nit in the traffic simulator SUMO based on simulated and real traffic data. The\nexperimental evaluation demonstrates that our method significantly improves\ntraffic flow through unsignalized intersections in mixed traffic settings and\nalso provides better performance on a wide range of traffic situations compared\nto the state-of-the-art traffic signal controller for the corresponding\nsignalized intersection.",
    "descriptor": "",
    "authors": [
      "Shengchao Yan",
      "Tim Welschehold",
      "Daniel B\u00fcscher",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.06369"
  },
  {
    "id": "arXiv:2106.06380",
    "title": "Finite volume schemes and Lax-Wendroff consistency",
    "abstract": "We present a (partial) historical summary of the mathematical analysis of\nfinite differences and finite volumes methods, paying a special attention to\nthe Lax-Richtmyer and Lax-Wendroff theorems. We then state a Lax-Wendroff\nconsistency result for convection operators on staggered grids (often used in\nfluid flow simulations), which illustrates a recent generalization of the flux\nconsistency notion designed to cope with general discrete functions.",
    "descriptor": "",
    "authors": [
      "R Eymard",
      "T Gallou\u00ebt",
      "R Herbin",
      "J.-C Latch\u00e9"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06380"
  },
  {
    "id": "arXiv:2106.06381",
    "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word  Alignment",
    "abstract": "The cross-lingual language models are typically pretrained with masked\nlanguage modeling on multilingual text or parallel sentences. In this paper, we\nintroduce denoising word alignment as a new cross-lingual pre-training task.\nSpecifically, the model first self-labels word alignments for parallel\nsentences. Then we randomly mask tokens in a bitext pair. Given a masked token,\nthe model uses a pointer network to predict the aligned token in the other\nlanguage. We alternately perform the above two steps in an\nexpectation-maximization manner. Experimental results show that our method\nimproves cross-lingual transferability on various datasets, especially on the\ntoken-level tasks, such as question answering, and structured prediction.\nMoreover, the model can serve as a pretrained word aligner, which achieves\nreasonably low error rates on the alignment benchmarks. The code and pretrained\nparameters are available at https://github.com/CZWin32768/XLM-Align.",
    "descriptor": "\nComments: ACL-2021\n",
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Bo Zheng",
      "Shaohan Huang",
      "Xian-Ling Mao",
      "Heyan Huang",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06381"
  },
  {
    "id": "arXiv:2106.06385",
    "title": "Deep Conditional Gaussian Mixture Model for Constrained Clustering",
    "abstract": "Constrained clustering has gained significant attention in the field of\nmachine learning as it can leverage prior information on a growing amount of\nonly partially labeled data. Following recent advances in deep generative\nmodels, we propose a novel framework for constrained clustering that is\nintuitive, interpretable, and can be trained efficiently in the framework of\nstochastic gradient variational inference. By explicitly integrating domain\nknowledge in the form of probabilistic relations, our proposed model (DC-GMM)\nuncovers the underlying distribution of data conditioned on prior clustering\npreferences, expressed as pairwise constraints. These constraints guide the\nclustering process towards a desirable partition of the data by indicating\nwhich samples should or should not belong to the same cluster. We provide\nextensive experiments to demonstrate that DC-GMM shows superior clustering\nperformances and robustness compared to state-of-the-art deep constrained\nclustering methods on a wide range of data sets. We further demonstrate the\nusefulness of our approach on two challenging real-world applications.",
    "descriptor": "",
    "authors": [
      "Laura Manduchi",
      "Kieran Chin-Cheong",
      "Holger Michel",
      "Sven Wellmann",
      "Julia E. Vogt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06385"
  },
  {
    "id": "arXiv:2106.06401",
    "title": "Decoupled Greedy Learning of CNNs for Synchronous and Asynchronous  Distributed Learning",
    "abstract": "A commonly cited inefficiency of neural network training using\nback-propagation is the update locking problem: each layer must wait for the\nsignal to propagate through the full network before updating. Several\nalternatives that can alleviate this issue have been proposed. In this context,\nwe consider a simple alternative based on minimal feedback, which we call\nDecoupled Greedy Learning (DGL). It is based on a classic greedy relaxation of\nthe joint training objective, recently shown to be effective in the context of\nConvolutional Neural Networks (CNNs) on large-scale image classification. We\nconsider an optimization of this objective that permits us to decouple the\nlayer training, allowing for layers or modules in networks to be trained with a\npotentially linear parallelization. With the use of a replay buffer we show\nthat this approach can be extended to asynchronous settings, where modules can\noperate and continue to update with possibly large communication delays. To\naddress bandwidth and memory issues we propose an approach based on online\nvector quantization. This allows to drastically reduce the communication\nbandwidth between modules and required memory for replay buffers. We show\ntheoretically and empirically that this approach converges and compare it to\nthe sequential solvers. We demonstrate the effectiveness of DGL against\nalternative approaches on the CIFAR-10 dataset and on the large-scale ImageNet\ndataset.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1901.08164\n",
    "authors": [
      "Eugene Belilovsky",
      "Louis Leconte",
      "Lucas Caccia",
      "Michael Eickenberg",
      "Edouard Oyallon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06401"
  },
  {
    "id": "arXiv:2106.06403",
    "title": "Small Object Detection for Near Real-Time Egocentric Perception in a  Manual Assembly Scenario",
    "abstract": "Detecting small objects in video streams of head-worn augmented reality\ndevices in near real-time is a huge challenge: training data is typically\nscarce, the input video stream can be of limited quality, and small objects are\nnotoriously hard to detect. In industrial scenarios, however, it is often\npossible to leverage contextual knowledge for the detection of small objects.\nFurthermore, CAD data of objects are typically available and can be used to\ngenerate synthetic training data. We describe a near real-time small object\ndetection pipeline for egocentric perception in a manual assembly scenario: We\ngenerate a training data set based on CAD data and realistic backgrounds in\nUnity. We then train a YOLOv4 model for a two-stage detection process: First,\nthe context is recognized, then the small object of interest is detected. We\nevaluate our pipeline on the augmented reality device Microsoft Hololens 2.",
    "descriptor": "\nComments: Accepted for presentation at EPIC@CVPR2021 workshop\n",
    "authors": [
      "Hooman Tavakoli",
      "Snehal Walunj",
      "Parsha Pahlevannejad",
      "Christiane Plociennik",
      "Martin Ruskowski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.06403"
  },
  {
    "id": "arXiv:2106.06404",
    "title": "Multilevel Spectral Domain Decomposition",
    "abstract": "Highly heterogeneous, anisotropic coefficients, e.g. in the simulation of\ncarbon-fibre composite components, can lead to extremely challenging finite\nelement systems. Direct solvers for the resulting large and sparse linear\nsystems suffer from severe memory requirements and limited parallel\nscalability, while iterative solvers in general lack robustness. Two-level\nspectral domain decomposition methods can provide such robustness for symmetric\npositive definite linear systems, by using coarse spaces based on independent\ngeneralized eigenproblems in the subdomains. Rigorous condition number bounds\nare independent of mesh size, number of subdomains, as well as coefficient\ncontrast. However, their parallel scalability is still limited by the fact that\n(in order to guarantee robustness) the coarse problem is solved via a direct\nmethod. In this paper, we introduce a multilevel variant in the context of\nsubspace correction methods and provide a general convergence theory for its\nrobust convergence for abstract, elliptic variational problems. Assumptions of\nthe theory are verified for conforming, as well as for discontinuous Galerkin\nmethods applied to a scalar diffusion problem. Numerical results illustrate the\nperformance of the method for two- and three-dimensional problems and for\nvarious discretization schemes, in the context of scalar diffusion and linear\nelasticity.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Peter Bastian",
      "Robert S. Scheichl",
      "Linus Seelinger",
      "Arne Strehlow"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06404"
  },
  {
    "id": "arXiv:2106.06410",
    "title": "What Can Knowledge Bring to Machine Learning? -- A Survey of Low-shot  Learning for Structured Data",
    "abstract": "Supervised machine learning has several drawbacks that make it difficult to\nuse in many situations. Drawbacks include: heavy reliance on massive training\ndata, limited generalizability and poor expressiveness of high-level semantics.\nLow-shot Learning attempts to address these drawbacks. Low-shot learning allows\nthe model to obtain good predictive power with very little or no training data,\nwhere structured knowledge plays a key role as a high-level semantic\nrepresentation of human. This article will review the fundamental factors of\nlow-shot learning technologies, with a focus on the operation of structured\nknowledge under different low-shot conditions. We also introduce other\ntechniques relevant to low-shot learning. Finally, we point out the limitations\nof low-shot learning, the prospects and gaps of industrial applications, and\nfuture research directions.",
    "descriptor": "\nComments: 41 pages, 280 references\n",
    "authors": [
      "Yang Hu",
      "Adriane Chapman",
      "Guihua Wen",
      "Dame Wendy Hall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06410"
  },
  {
    "id": "arXiv:2106.06411",
    "title": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers",
    "abstract": "Controlling neural network-based models for natural language generation (NLG)\nhas broad applications in numerous areas such as machine translation, document\nsummarization, and dialog systems. Approaches that enable such control in a\nzero-shot manner would be of great importance as, among other reasons, they\nremove the need for additional annotated data and training. In this work, we\npropose novel approaches for controlling encoder-decoder transformer-based NLG\nmodels in a zero-shot manner. This is done by introducing three control knobs;\nnamely, attention biasing, decoder mixing, and context augmentation, that are\napplied to these models at generation time. These knobs control the generation\nprocess by directly manipulating trained NLG models (e.g., biasing\ncross-attention layers) to realize the desired attributes in the generated\noutputs. We show that not only are these NLG models robust to such\nmanipulations, but also their behavior could be controlled without an impact on\ntheir generation performance. These results, to the best of our knowledge, are\nthe first of their kind. Through these control knobs, we also investigate the\nrole of transformer decoder's self-attention module and show strong evidence\nthat its primary role is maintaining fluency of sentences generated by these\nmodels. Based on this hypothesis, we show that alternative architectures for\ntransformer decoders could be viable options. We also study how this hypothesis\ncould lead to more efficient ways for training encoder-decoder transformer\nmodels.",
    "descriptor": "",
    "authors": [
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Dilek Hakkani-T\u00fcr"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06411"
  },
  {
    "id": "arXiv:2106.06415",
    "title": "Attention-based Partial Face Recognition",
    "abstract": "Photos of faces captured in unconstrained environments, such as large crowds,\nstill constitute challenges for current face recognition approaches as often\nfaces are occluded by objects or people in the foreground. However, few studies\nhave addressed the task of recognizing partial faces. In this paper, we propose\na novel approach to partial face recognition capable of recognizing faces with\ndifferent occluded areas. We achieve this by combining attentional pooling of a\nResNet's intermediate feature maps with a separate aggregation module. We\nfurther adapt common losses to partial faces in order to ensure that the\nattention maps are diverse and handle occluded parts. Our thorough analysis\ndemonstrates that we outperform all baselines under multiple benchmark\nprotocols, including naturally and synthetically occluded partial faces. This\nsuggests that our method successfully focuses on the relevant parts of the\noccluded face.",
    "descriptor": "\nComments: To be published in IEEE ICIP 2021\n",
    "authors": [
      "Stefan H\u00f6rmann",
      "Zeyuan Zhang",
      "Martin Knoche",
      "Torben Teepe",
      "Gerhard Rigoll"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06415"
  },
  {
    "id": "arXiv:2106.06418",
    "title": "Scale-invariant scale-channel networks: Deep networks that generalise to  previously unseen scales",
    "abstract": "The ability to handle large scale variations is crucial for many real world\nvisual tasks. A straightforward approach for handling scale in a deep network\nis to process an image at several scales simultaneously in a set of scale\nchannels. Scale invariance can then, in principle, be achieved by using weight\nsharing between the scale channels together with max or average pooling over\nthe outputs from the scale channels. The ability of such scale channel networks\nto generalise to scales not present in the training set over significant scale\nranges has, however, not previously been explored.\nIn this paper, we present a systematic study of this methodology by\nimplementing different types of scale channel networks and evaluating their\nability to generalise to previously unseen scales. We develop a formalism for\nanalysing the covariance and invariance properties of scale channel networks,\nand explore how different design choices, unique to scaling transformations,\naffect the overall performance of scale channel networks. We first show that\ntwo previously proposed scale channel network designs do not generalise well to\nscales not present in the training set. We explain theoretically and\ndemonstrate experimentally why generalisation fails in these cases.\nWe then propose a new type of foveated scale channel architecture}, where the\nscale channels process increasingly larger parts of the image with decreasing\nresolution. This new type of scale channel network is shown to generalise\nextremely well, provided sufficient image resolution and the absence of\nboundary effects. Our proposed FovMax and FovAvg networks perform almost\nidentically over a scale range of 8, also when training on single scale\ntraining data, and do also give improved performance when learning from\ndatasets with large scale variations in the small sample regime.",
    "descriptor": "\nComments: 29 pages, 14 figures, 6 tables. arXiv admin note: substantial text overlap with arXiv:2004.01536\n",
    "authors": [
      "Ylva Jansson",
      "Tony Lindeberg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06418"
  },
  {
    "id": "arXiv:2106.06420",
    "title": "A Framework to Enhance Generalization of Deep Metric Learning methods  using General Discriminative Feature Learning and Class Adversarial Neural  Networks",
    "abstract": "Metric learning algorithms aim to learn a distance function that brings the\nsemantically similar data items together and keeps dissimilar ones at a\ndistance. The traditional Mahalanobis distance learning is equivalent to find a\nlinear projection. In contrast, Deep Metric Learning (DML) methods are proposed\nthat automatically extract features from data and learn a non-linear\ntransformation from input space to a semantically embedding space. Recently,\nmany DML methods are proposed focused to enhance the discrimination power of\nthe learned metric by providing novel sampling strategies or loss functions.\nThis approach is very helpful when both the training and test examples are\ncoming from the same set of categories. However, it is less effective in many\napplications of DML such as image retrieval and person-reidentification. Here,\nthe DML should learn general semantic concepts from observed classes and employ\nthem to rank or identify objects from unseen categories. Neglecting the\ngeneralization ability of the learned representation and just emphasizing to\nlearn a more discriminative embedding on the observed classes may lead to the\noverfitting problem. To address this limitation, we propose a framework to\nenhance the generalization power of existing DML methods in a Zero-Shot\nLearning (ZSL) setting by general yet discriminative representation learning\nand employing a class adversarial neural network. To learn a more general\nrepresentation, we propose to employ feature maps of intermediate layers in a\ndeep neural network and enhance their discrimination power through an attention\nmechanism. Besides, a class adversarial network is utilized to enforce the deep\nmodel to seek class invariant features for the DML task. We evaluate our work\non widely used machine vision datasets in a ZSL setting.",
    "descriptor": "\nComments: Includes: 31 Pages, 5 Tables, 15 Figures\n",
    "authors": [
      "Karrar Al-Kaabi",
      "Reza Monsefi",
      "Davood Zabihzadeh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06420"
  },
  {
    "id": "arXiv:2106.06422",
    "title": "From Blackboard to the Office: A Look Into How Practitioners Perceive  Software Testing Education",
    "abstract": "The teaching-learning process may require specific pedagogical approaches to\nestablish a relationship with industry practices. Recently, some studies\ninvestigated the educators' perspectives and the undergraduate courses\ncurriculum to identify potential weaknesses and solutions for the software\ntesting teaching process. However, it is still unclear how the practitioners\nevaluate the acquisition of knowledge about software testing in undergraduate\ncourses. This study carried out an expert survey with 68 newly graduated\npractitioners to determine what the industry expects from them and what they\nlearned in academia. The yielded results indicated that those practitioners\nlearned at a similar rate as others with a long industry experience. Also, they\nstudied less than half of the 35 software testing topics collected in the\nsurvey and took industry-backed extracurricular courses to complement their\nlearning. Additionally, our findings point out a set of implications for future\nresearch, as the respondents' learning difficulties (e.g., lack of learning\nsources) and the gap between academic education and industry expectations\n(e.g., certifications).",
    "descriptor": "\nComments: Preprint of the manuscript accepted for publication at EASE 2021\n",
    "authors": [
      "Luana Martins",
      "Vinicius Brito",
      "Daniela Feitosa",
      "Larissa Rocha",
      "Heitor Costa",
      "Ivan Machado"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.06422"
  },
  {
    "id": "arXiv:2106.06426",
    "title": "Catch-A-Waveform: Learning to Generate Audio from a Single Short Example",
    "abstract": "Models for audio generation are typically trained on hours of recordings.\nHere, we illustrate that capturing the essence of an audio source is typically\npossible from as little as a few tens of seconds from a single training signal.\nSpecifically, we present a GAN-based generative model that can be trained on\none short audio signal from any domain (e.g. speech, music, etc.) and does not\nrequire pre-training or any other form of external supervision. Once trained,\nour model can generate random samples of arbitrary duration that maintain\nsemantic similarity to the training waveform, yet exhibit new compositions of\nits audio primitives. This enables a long line of interesting applications,\nincluding generating new jazz improvisations or new a-cappella rap variants\nbased on a single short example, producing coherent modifications to famous\nsongs (e.g. adding a new verse to a Beatles song based solely on the original\nrecording), filling-in of missing parts (inpainting), extending the bandwidth\nof a speech signal (super-resolution), and enhancing old recordings without\naccess to any clean training example. We show that in all cases, no more than\n20 seconds of training audio commonly suffice for our model to achieve\nstate-of-the-art results. This is despite its complete lack of prior knowledge\nabout the nature of audio signals in general.",
    "descriptor": "",
    "authors": [
      "Gal Greshler",
      "Tamar Rott Shaham",
      "Tomer Michaeli"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06426"
  },
  {
    "id": "arXiv:2106.06427",
    "title": "Neural Symbolic Regression that Scales",
    "abstract": "Symbolic equations are at the core of scientific discovery. The task of\ndiscovering the underlying equation from a set of input-output pairs is called\nsymbolic regression. Traditionally, symbolic regression methods use\nhand-designed strategies that do not improve with experience. In this paper, we\nintroduce the first symbolic regression method that leverages large scale\npre-training. We procedurally generate an unbounded set of equations, and\nsimultaneously pre-train a Transformer to predict the symbolic equation from a\ncorresponding set of input-output-pairs. At test time, we query the model on a\nnew set of points and use its output to guide the search for the equation. We\nshow empirically that this approach can re-discover a set of well-known\nphysical equations, and that it improves over time with more data and compute.",
    "descriptor": "\nComments: Accepted at the 38th International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Luca Biggio",
      "Tommaso Bendinelli",
      "Alexander Neitz",
      "Aurelien Lucchi",
      "Giambattista Parascandolo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06427"
  },
  {
    "id": "arXiv:2106.06431",
    "title": "Offline Reinforcement Learning as Anti-Exploration",
    "abstract": "Offline Reinforcement Learning (RL) aims at learning an optimal control from\na fixed dataset, without interactions with the system. An agent in this setting\nshould avoid selecting actions whose consequences cannot be predicted from the\ndata. This is the converse of exploration in RL, which favors such actions. We\nthus take inspiration from the literature on bonus-based exploration to design\na new offline RL agent. The core idea is to subtract a prediction-based\nexploration bonus from the reward, instead of adding it for exploration. This\nallows the policy to stay close to the support of the dataset. We connect this\napproach to a more common regularization of the learned policy towards the\ndata. Instantiated with a bonus based on the prediction error of a variational\nautoencoder, we show that our agent is competitive with the state of the art on\na set of continuous control locomotion and manipulation tasks.",
    "descriptor": "",
    "authors": [
      "Shideh Rezaeifar",
      "Robert Dadashi",
      "Nino Vieillard",
      "L\u00e9onard Hussenot",
      "Olivier Bachem",
      "Olivier Pietquin",
      "Matthieu Geist"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06431"
  },
  {
    "id": "arXiv:2106.06433",
    "title": "FPGA-based Near-Memory Acceleration of Modern Data-Intensive  Applications",
    "abstract": "Modern data-intensive applications demand high computation capabilities with\nstrict power constraints. Unfortunately, such applications suffer from a\nsignificant waste of both execution cycles and energy in current computing\nsystems due to the costly data movement between the computation units and the\nmemory units. Genome analysis and weather prediction are two examples of such\napplications. Recent FPGAs couple a reconfigurable fabric with high-bandwidth\nmemory (HBM) to enable more efficient data movement and improve overall\nperformance and energy efficiency. This trend is an example of a paradigm shift\nto near-memory computing. We leverage such an FPGA with high-bandwidth memory\n(HBM) for improving the pre-alignment filtering step of genome analysis and\nrepresentative kernels from a weather prediction model. Our evaluation\ndemonstrates large speedups and energy savings over a high-end IBM POWER9\nsystem and a conventional FPGA board with DDR4 memory. We conclude that\nFPGA-based near-memory computing has the potential to alleviate the data\nmovement bottleneck for modern data-intensive applications.",
    "descriptor": "\nComments: This is an extended version of a paper accepted to IEEE Micro\n",
    "authors": [
      "Gagandeep Singh",
      "Mohammed Alser",
      "Damla Senol Cali",
      "Dionysios Diamantopoulos",
      "Juan G\u00f3mez-Luna",
      "Henk Corporaal",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06433"
  },
  {
    "id": "arXiv:2106.06437",
    "title": "Feature Selection Tutorial with Python Examples",
    "abstract": "In Machine Learning, feature selection entails selecting a subset of the\navailable features in a dataset to use for model development. There are many\nmotivations for feature selection, it may result in better models, it may\nprovide insight into the data and it may deliver economies in data gathering or\ndata processing. For these reasons feature selection has received a lot of\nattention in data analytics research. In this paper we provide an overview of\nthe main methods and present practical examples with Python implementations.\nWhile the main focus is on supervised feature selection techniques, we also\ncover some feature transformation methods.",
    "descriptor": "\nComments: 20 pages, 19 figures\n",
    "authors": [
      "Padraig Cunningham",
      "Bahavathy Kathirgamanathan",
      "Sarah Jane Delany"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06437"
  },
  {
    "id": "arXiv:2106.06438",
    "title": "Encoding of probability distributions for Asymmetric Numeral Systems",
    "abstract": "Many data compressors regularly encode probability distributions for entropy\ncoding - requiring minimal description length type of optimizations. Canonical\nprefix/Huffman coding usually just writes lengths of bit sequences, this way\napproximating probabilities with powers-of-2. Operating on more accurate\nprobabilities usually allows for better compression ratios, and is possible\ne.g. using arithmetic coding and Asymmetric Numeral Systems family. Especially\nthe tabled variant of the latter (tANS) often replaces Huffman coding due to\nbetter compression at similar computational cost - e.g. in Facebook Zstandard\nand Apple LZFSE popular compressors. There is discussed encoding of probability\ndistributions for this kind of applications, especially using Pyramid Vector\nQuantizer(PVQ)-based approach with deformation, also tuned symbol spread for\ntANS.",
    "descriptor": "\nComments: 4 pages, 4 figures\n",
    "authors": [
      "Jarek Duda"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.06438"
  },
  {
    "id": "arXiv:2106.06439",
    "title": "An Image Forensic Technique Based on JPEG Ghosts",
    "abstract": "The unprecedented growth in the easy availability of photo-editing tools has\nendangered the power of digital images.An image was supposed to be worth more\nthan a thousand words,but now this can be said only if it can be authenticated\northe integrity of the image can be proved to be intact. In thispaper, we\npropose a digital image forensic technique for JPEG images. It can detect any\nforgery in the image if the forged portion called a ghost image is having a\ncompression quality different from that of the cover image. It is based on\nresaving the JPEG image at different JPEG qualities, and the detection of the\nforged portion is maximum when it is saved at the same JPEG quality as the\ncover image. Also, we can precisely predictthe JPEG quality of the cover image\nby analyzing the similarity using Structural Similarity Index Measure (SSIM) or\nthe energyof the images. The first maxima in SSIM or the first minima inenergy\ncorrespond to the cover image JPEG quality. We created adataset for varying\nJPEG compression qualities of the ghost and the cover images and validated the\nscalability of the experimental results.We also, experimented with varied\nattack scenarios, e.g. high-quality ghost image embedded in low quality of\ncover image,low-quality ghost image embedded in high-quality of cover image,and\nghost image and cover image both at the same quality.The proposed method is\nable to localize the tampered portions accurately even for forgeries as small\nas 10x10 sized pixel blocks.Our technique is also robust against other attack\nscenarios like copy-move forgery, inserting text into image, rescaling\n(zoom-out/zoom-in) ghost image and then pasting on cover image.",
    "descriptor": "\nComments: 8 pages, 10 figures, 2 tables\n",
    "authors": [
      "Divakar Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06439"
  },
  {
    "id": "arXiv:2106.06440",
    "title": "Learning Compositional Shape Priors for Few-Shot 3D Reconstruction",
    "abstract": "The impressive performance of deep convolutional neural networks in\nsingle-view 3D reconstruction suggests that these models perform non-trivial\nreasoning about the 3D structure of the output space. Recent work has\nchallenged this belief, showing that, on standard benchmarks, complex\nencoder-decoder architectures perform similarly to nearest-neighbor baselines\nor simple linear decoder models that exploit large amounts of per-category\ndata. However, building large collections of 3D shapes for supervised training\nis a laborious process; a more realistic and less constraining task is\ninferring 3D shapes for categories with few available training examples,\ncalling for a model that can successfully generalize to novel object classes.\nIn this work we experimentally demonstrate that naive baselines fail in this\nfew-shot learning setting, in which the network must learn informative shape\npriors for inference of new categories. We propose three ways to learn a\nclass-specific global shape prior, directly from data. Using these techniques,\nwe are able to capture multi-scale information about the 3D shape, and account\nfor intra-class variability by virtue of an implicit compositional structure.\nExperiments on the popular ShapeNet dataset show that our method outperforms a\nzero-shot baseline by over 40%, and the current state-of-the-art by over 10%,\nin terms of relative performance, in the few-shot setting.12",
    "descriptor": "\nComments: 12 pages, 12 figures. arXiv admin note: substantial text overlap with arXiv:2004.06302\n",
    "authors": [
      "Mateusz Michalkiewicz",
      "Stavros Tsogkas",
      "Sarah Parisot",
      "Mahsa Baktashmotlagh",
      "Anders Eriksson",
      "Eugene Belilovsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06440"
  },
  {
    "id": "arXiv:2106.06442",
    "title": "K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets",
    "abstract": "In one-shot weight sharing for NAS, the weights of each operation (at each\nlayer) are supposed to be identical for all architectures (paths) in the\nsupernet. However, this rules out the possibility of adjusting operation\nweights to cater for different paths, which limits the reliability of the\nevaluation results. In this paper, instead of counting on a single supernet, we\nintroduce $K$-shot supernets and take their weights for each operation as a\ndictionary. The operation weight for each path is represented as a convex\ncombination of items in a dictionary with a simplex code. This enables a matrix\napproximation of the stand-alone weight matrix with a higher rank ($K>1$). A\n\\textit{simplex-net} is introduced to produce architecture-customized code for\neach path. As a result, all paths can adaptively learn how to share weights in\nthe $K$-shot supernets and acquire corresponding weights for better evaluation.\n$K$-shot supernets and simplex-net can be iteratively trained, and we further\nextend the search to the channel dimension. Extensive experiments on benchmark\ndatasets validate that K-shot NAS significantly improves the evaluation\naccuracy of paths and thus brings in impressive performance improvements.",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Xiu Su",
      "Shan You",
      "Mingkai Zheng",
      "Fei Wang",
      "Chen Qian",
      "Changshui Zhang",
      "Chang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06442"
  },
  {
    "id": "arXiv:2106.06444",
    "title": "Autonomous Fire Fighting with a UAV-UGV Team at MBZIRC 2020",
    "abstract": "Every day, burning buildings threaten the lives of occupants and first\nresponders trying to save them. Quick action is of essence, but some areas\nmight not be accessible or too dangerous to enter. Robotic systems have become\na promising addition to firefighting, but at this stage, they are mostly\nmanually controlled, which is error-prone and requires specially trained\npersonal.\nWe present two systems for autonomous firefighting from air and ground we\ndeveloped for the Mohamed Bin Zayed International Robotics Challenge (MBZIRC)\n2020. The systems use LiDAR for reliable localization within narrow,\npotentially GNSS-restricted environments while maneuvering close to obstacles.\nMeasurements from LiDAR and thermal cameras are fused to track fires, while\nrelative navigation ensures successful extinguishing.\nWe analyze and discuss our successful participation during the MBZIRC 2020,\npresent further experiments, and provide insights into our lessons learned from\nthe competition.",
    "descriptor": "\nComments: 8 pages, accepted for ICUAS 2021\n",
    "authors": [
      "Jan Quenzel",
      "Malte Splietker",
      "Dmytro Pavlichenko",
      "Daniel Schleich",
      "Christian Lenz",
      "Max Schwarz",
      "Michael Schreiber",
      "Marius Beul",
      "Sven Behnke"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.06444"
  },
  {
    "id": "arXiv:2106.06445",
    "title": "Coded-InvNet for Resilient Prediction Serving Systems",
    "abstract": "Inspired by a new coded computation algorithm for invertible functions, we\npropose Coded-InvNet a new approach to design resilient prediction serving\nsystems that can gracefully handle stragglers or node failures. Coded-InvNet\nleverages recent findings in the deep learning literature such as invertible\nneural networks, Manifold Mixup, and domain translation algorithms, identifying\ninteresting research directions that span across machine learning and systems.\nOur experimental results show that Coded-InvNet can outperform existing\napproaches, especially when the compute resource overhead is as low as 10%. For\ninstance, without knowing which of the ten workers is going to fail, our\nalgorithm can design a backup task so that it can correctly recover the missing\nprediction result with an accuracy of 85.9%, significantly outperforming the\nprevious SOTA by 32.5%.",
    "descriptor": "",
    "authors": [
      "Tuan Dinh",
      "Kangwook Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06445"
  },
  {
    "id": "arXiv:2106.06452",
    "title": "Keyframe-Focused Visual Imitation Learning",
    "abstract": "Imitation learning trains control policies by mimicking pre-recorded expert\ndemonstrations. In partially observable settings, imitation policies must rely\non observation histories, but many seemingly paradoxical results show better\nperformance for policies that only access the most recent observation. Recent\nsolutions ranging from causal graph learning to deep information bottlenecks\nhave shown promising results, but failed to scale to realistic settings such as\nvisual imitation. We propose a solution that outperforms these prior approaches\nby upweighting demonstration keyframes corresponding to expert action\nchangepoints. This simple approach easily scales to complex visual imitation\nsettings. Our experimental results demonstrate consistent performance\nimprovements over all baselines on image-based Gym MuJoCo continuous control\ntasks. Finally, on the CARLA photorealistic vision-based urban driving\nsimulator, we resolve a long-standing issue in behavioral cloning for driving\nby demonstrating effective imitation from observation histories. Supplementary\nmaterials and code at: \\url{https://tinyurl.com/imitation-keyframes}.",
    "descriptor": "\nComments: 14 pages, 7 figures, ICML2021\n",
    "authors": [
      "Chuan Wen",
      "Jierui Lin",
      "Jianing Qian",
      "Yang Gao",
      "Dinesh Jayaraman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.06452"
  },
  {
    "id": "arXiv:2106.06453",
    "title": "Property-Preserving Hash Functions from Standard Assumptions",
    "abstract": "Property-preserving hash functions allow for compressing long inputs $x_0$\nand $x_1$ into short hashes $h(x_0)$ and $h(x_1)$ in a manner that allows for\ncomputing a predicate $P(x_0, x_1)$ given only the two hash values without\nhaving access to the original data. Such hash functions are said to be\nadversarially robust if an adversary that gets to pick $x_0$ and $x_1$ after\nthe hash function has been sampled, cannot find inputs for which the predicate\nevaluated on the hash values outputs the incorrect result.\nIn this work we construct robust property-preserving hash functions for the\nhamming-distance predicate which distinguishes inputs with a hamming distance\nat least some threshold $t$ from those with distance less than $t$. The\nsecurity of the construction is based on standard lattice hardness assumptions.\nOur construction has several advantages over the best known previous\nconstruction by Fleischhacker and Simkin. Our construction relies on a single\nwell-studied hardness assumption from lattice cryptography whereas the previous\nwork relied on a newly introduced family of computational hardness assumptions.\nIn terms of computational effort, our construction only requires a small number\nof modular additions per input bit, whereas previously several exponentiations\nper bit as well as the interpolation and evaluation of high-degree polynomials\nover large fields were required. An additional benefit of our construction is\nthat the description of the hash function can be compressed to $\\lambda$ bits\nassuming a random oracle. Previous work has descriptions of length\n$\\mathcal{O}(\\ell \\lambda)$ bits for input bit-length $\\ell$, which has a\nsecret structure and thus cannot be compressed.\nWe prove a lower bound on the output size of any property-preserving hash\nfunction for the hamming distance predicate. The bound shows that the size of\nour hash value is not far from optimal.",
    "descriptor": "",
    "authors": [
      "Nils Fleischhacker",
      "Kasper Green Larsen",
      "and Mark Simkin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.06453"
  },
  {
    "id": "arXiv:2106.06455",
    "title": "Certifying the LTL Formula p Until q in Hybrid Systems",
    "abstract": "In this paper, we propose sufficient conditions to guarantee that a linear\ntemporal logic (LTL) formula of the form p Until q, denoted by $p \\mathcal{U}\nq$, is satisfied for a hybrid system. Roughly speaking, the formula $p\n\\mathcal{U} q$ is satisfied means that the solutions, initially satisfying\nproposition p, keep satisfying this proposition until proposition q is\nsatisfied. To certify such a formula, connections to invariance notions such as\nconditional invariance (CI) and eventual conditional invariance (ECI), as well\nas finite-time attractivity (FTA) are established. As a result, sufficient\nconditions involving the data of the hybrid system and an appropriate choice of\nLyapunov-like functions, such as barrier functions, are derived. The considered\nhybrid system is given in terms of differential and difference inclusions,\nwhich capture the continuous and the discrete dynamics present in the same\nsystem, respectively. Examples illustrate the results throughout the paper.",
    "descriptor": "\nComments: 19 pages. The technical report accompanying \"Certifying the LTL Formula p Until q in Hybrid Systems\" submitted to IEEE Transactions on Automatic Control, 2021\n",
    "authors": [
      "Hyejin Han",
      "Mohamed Maghenem",
      "Ricardo G. Sanfelice"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06455"
  },
  {
    "id": "arXiv:2106.06457",
    "title": "A New Upper Bound on Cache Hit Probability for Non-anticipative Caching  Policies",
    "abstract": "Caching systems have long been crucial for improving the performance of a\nwide variety of network and web based online applications. In such systems,\nend-to-end application performance heavily depends on the fraction of objects\ntransferred from the cache, also known as the cache hit probability. Many\ncaching policies have been proposed and implemented to improve the hit\nprobability. In this work, we propose a new method to compute an upper bound on\nhit probability for all non-anticipative caching policies, i.e., for policies\nthat have no knowledge of future requests. Our key insight is to order the\nobjects according to the ratio of their Hazard Rate (HR) function values to\ntheir sizes and place in the cache the objects with the largest ratios till the\ncache capacity is exhausted. Under some statistical assumptions, we prove that\nour proposed HR to size ratio based ordering model computes the maximum\nachievable hit probability and serves as an upper bound for all\nnon-anticipative caching policies. We derive closed form expressions for the\nupper bound under some specific object request arrival processes. We also\nprovide simulation results to validate its correctness and to compare it to the\nstate-of-the-art upper bounds. We find it to be tighter than state-of-the-art\nupper bounds for a variety of object request arrival processes.",
    "descriptor": "\nComments: IFIP WG 7.3 Performance\n",
    "authors": [
      "Nitish K. Panigrahy",
      "Philippe Nain",
      "Giovanni Neglia",
      "Don Towsley"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2106.06457"
  },
  {
    "id": "arXiv:2106.06458",
    "title": "SolcTrans: Towards machine translation of Solidity smart contract source  code",
    "abstract": "Context: Decentralized applications on blockchain platforms are realized\nthrough smart contracts. However, participants who lack programming knowledge\noften have difficulties reading the smart contract source codes, which leads to\npotential security risks and barriers to participation. Objective: Our\nobjective is to translate the smart contract source codes into natural language\ndescriptions to help people better understand, operate, and learn smart\ncontracts. Method: This paper proposes an automated translation tool for\nSolidity smart contracts, termed SolcTrans, based on an abstract syntax tree\nand formal grammar. We have investigated 3,000 smart contracts and determined\nthe part of speeches of corresponding blockchain terms. Among them, we further\nfiltered out contract snippets without detailed comments and left 811 snippets\nto evaluate the translation quality of SolcTrans. Results: Experimental results\nshow that even with a small corpus, SolcTrans can achieve similar performance\nto the state-of-the-art code comments generation models for other programming\nlanguages. In addition, SolcTrans has consistent performance when dealing with\ncode snippets with different lengths and gas consumption. Conclusion: SolcTrans\ncan correctly interpret Solidity codes and automatically convert them into\ncomprehensible English text. We will release our tool and dataset for\nsupporting reproduction and further studies in related fields.",
    "descriptor": "",
    "authors": [
      "Chaochen Shi",
      "Yong Xiang",
      "Jiangshan Yu",
      "Keshav Sood",
      "Longxiang Gao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.06458"
  },
  {
    "id": "arXiv:2106.06462",
    "title": "Semi-Supervised and Unsupervised Sense Annotation via Translations",
    "abstract": "Acquisition of multilingual training data continues to be a challenge in word\nsense disambiguation (WSD). To address this problem, unsupervised approaches\nhave been developed in recent years that automatically generate sense\nannotations suitable for training supervised WSD systems. We present three new\nmethods to creating sense-annotated corpora, which leverage translations,\nparallel corpora, lexical resources, and contextual and synset embeddings. Our\nsemi-supervised method applies machine translation to transfer existing sense\nannotations to other languages. Our two unsupervised methods use a\nknowledge-based WSD system to annotate a parallel corpus, and refine the\nresulting sense annotations by identifying lexical translations. We obtain\nstate-of-the-art results on standard WSD benchmarks.",
    "descriptor": "",
    "authors": [
      "Bradley Hauer",
      "Grzegorz Kondrak",
      "Yixing Luan",
      "Arnob Mallik",
      "Lili Mou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06462"
  },
  {
    "id": "arXiv:2106.06465",
    "title": "Stochastic modelling of blockchain consensus",
    "abstract": "Blockchain and general purpose distributed ledgers are foundational\ntechnologies which bring significant innovation in the infrastructures and\nother underpinnings of our socio-economic systems. These P2P technologies are\nable to securely diffuse information within and across networks, without need\nfor trustees or central authorities to enforce consensus. In this contribution,\nwe propose a minimalistic stochastic model to understand the dynamics of\nblockchain-based consensus. By leveraging on random-walk theory, we model block\npropagation delay on different network topologies and provide a classification\nof blockchain systems in terms of two emergent properties. Firstly, we identify\ntwo performing regimes: a functional regime corresponding to an optimal system\nfunction; and a non-functional regime characterised by a congested or branched\nstate of sub-optimal blockchains. Secondly, we discover a phase transition\nduring the emergence of consensus and numerically investigate the corresponding\ncritical point. Our results provide important insights into the consensus\nmechanism and sub-optimal states in decentralised systems.",
    "descriptor": "",
    "authors": [
      "Claudio J. Tessone",
      "Paolo Tasca",
      "Flavio Iannelli"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.06465"
  },
  {
    "id": "arXiv:2106.06467",
    "title": "A Large-Scale Rich Context Query and Recommendation Dataset in Online  Knowledge-Sharing",
    "abstract": "Data plays a vital role in machine learning studies. In the research of\nrecommendation, both user behaviors and side information are helpful to model\nusers. So, large-scale real scenario datasets with abundant user behaviors will\ncontribute a lot. However, it is not easy to get such datasets as most of them\nare only hold and protected by companies. In this paper, a new large-scale\ndataset collected from a knowledge-sharing platform is presented, which is\ncomposed of around 100M interactions collected within 10 days, 798K users, 165K\nquestions, 554K answers, 240K authors, 70K topics, and more than 501K user\nquery keywords. There are also descriptions of users, answers, questions,\nauthors, and topics, which are anonymous. Note that each user's latest query\nkeywords have not been included in previous open datasets, which reveal users'\nexplicit information needs.\nWe characterize the dataset and demonstrate its potential applications for\nrecommendation study. Multiple experiments show the dataset can be used to\nevaluate algorithms in general top-N recommendation, sequential recommendation,\nand context-aware recommendation. This dataset can also be used to integrate\nsearch and recommendation and recommendation with negative feedback. Besides,\ntasks beyond recommendation, such as user gender prediction, most valuable\nanswerer identification, and high-quality answer recognition, can also use this\ndataset. To the best of our knowledge, this is the largest real-world\ninteraction dataset for personalized recommendation.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Bin Hao",
      "Min Zhang",
      "Weizhi Ma",
      "Shaoyun Shi",
      "Xinxing Yu",
      "Houzhi Shan",
      "Yiqun Liu",
      "Shaoping Ma"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.06467"
  },
  {
    "id": "arXiv:2106.06468",
    "title": "Locally Sparse Networks for Interpretable Predictions",
    "abstract": "Despite the enormous success of neural networks, they are still hard to\ninterpret and often overfit when applied to low-sample-size (LSS) datasets. To\ntackle these obstacles, we propose a framework for training locally sparse\nneural networks where the local sparsity is learned via a sample-specific\ngating mechanism that identifies the subset of most relevant features for each\nmeasurement. The sample-specific sparsity is predicted via a \\textit{gating}\nnetwork, which is trained in tandem with the \\textit{prediction} network. By\nlearning these subsets and weights of a prediction model, we obtain an\ninterpretable neural network that can handle LSS data and can remove nuisance\nvariables, which are irrelevant for the supervised learning task. Using both\nsynthetic and real-world datasets, we demonstrate that our method outperforms\nstate-of-the-art models when predicting the target function with far fewer\nfeatures per instance.",
    "descriptor": "",
    "authors": [
      "Junchen Yang",
      "Ofir Lindenbaum",
      "Yuval Kluger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06468"
  },
  {
    "id": "arXiv:2106.06469",
    "title": "Topological Detection of Trojaned Neural Networks",
    "abstract": "Deep neural networks are known to have security issues. One particular threat\nis the Trojan attack. It occurs when the attackers stealthily manipulate the\nmodel's behavior through Trojaned training samples, which can later be\nexploited.\nGuided by basic neuroscientific principles we discover subtle -- yet critical\n-- structural deviation characterizing Trojaned models. In our analysis we use\ntopological tools. They allow us to model high-order dependencies in the\nnetworks, robustly compare different networks, and localize structural\nabnormalities. One interesting observation is that Trojaned models develop\nshort-cuts from input to output layers.\nInspired by these observations, we devise a strategy for robust detection of\nTrojaned models. Compared to standard baselines it displays better performance\non multiple benchmarks.",
    "descriptor": "",
    "authors": [
      "Songzhu Zheng",
      "Yikai Zhang",
      "Hubert Wagner",
      "Mayank Goswami",
      "Chao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.06469"
  },
  {
    "id": "arXiv:2106.06471",
    "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report  Generation",
    "abstract": "Medical report generation is one of the most challenging tasks in medical\nimage analysis. Although existing approaches have achieved promising results,\nthey either require a predefined template database in order to retrieve\nsentences or ignore the hierarchical nature of medical report generation. To\naddress these issues, we propose MedWriter that incorporates a novel\nhierarchical retrieval mechanism to automatically extract both report and\nsentence-level templates for clinically accurate report generation. MedWriter\nfirst employs the Visual-Language Retrieval~(VLR) module to retrieve the most\nrelevant reports for the given images. To guarantee the logical coherence\nbetween sentences, the Language-Language Retrieval~(LLR) module is introduced\nto retrieve relevant sentences based on the previous generated description. At\nlast, a language decoder fuses image features and features from retrieved\nreports and sentences to generate meaningful medical reports. We verified the\neffectiveness of our model by automatic evaluation and human evaluation on two\ndatasets, i.e., Open-I and MIMIC-CXR.",
    "descriptor": "\nComments: Accepted by ACL 2021, Camera-ready version\n",
    "authors": [
      "Xingyi Yang",
      "Muchao Ye",
      "Quanzeng You",
      "Fenglong Ma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.06471"
  },
  {
    "id": "arXiv:2106.06475",
    "title": "Analyzing the Travel and Charging Behavior of Electric Vehicles -- A  Data-driven Approach",
    "abstract": "The increasing market penetration of electric vehicles (EVs) may pose\nsignificant electricity demand on power systems. This electricity demand is\naffected by the inherent uncertainties of EVs' travel behavior that makes\nforecasting the daily charging demand (CD) very challenging. In this project,\nwe use the National House Hold Survey (NHTS) data to form sequences of trips,\nand develop machine learning models to predict the parameters of the next trip\nof the drivers, including trip start time, end time, and distance. These\nparameters are later used to model the temporal charging behavior of EVs. The\nsimulation results show that the proposed modeling can effectively estimate the\ndaily CD pattern based on travel behavior of EVs, and simple machine learning\ntechniques can forecast the travel parameters with acceptable accuracy.",
    "descriptor": "\nComments: Accepted in IEEE Kansas Power and Energy Conference 2021\n",
    "authors": [
      "Sina Baghali",
      "Samiul Hasan",
      "Zhaomiao Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06475"
  },
  {
    "id": "arXiv:2106.06478",
    "title": "Data-Driven Multiscale Design of Cellular Composites with Multiclass  Microstructures for Natural Frequency Maximization",
    "abstract": "For natural frequency optimization of engineering structures, cellular\ncomposites have been shown to possess an edge over solid. However, existing\nmultiscale design methods for cellular composites are either computationally\nexhaustive or confined to a single class of microstructures. In this paper, we\npropose a data-driven topology optimization (TO) approach to enable the\nmultiscale design of cellular structures with various choices of microstructure\nclasses. The key component is a newly proposed latent-variable Gaussian process\n(LVGP) model through which different classes of microstructures are mapped into\na low-dimensional continuous latent space. It provides an interpretable\ndistance metric between classes and captures their effects on the homogenized\nstiffness tensors. By introducing latent vectors as design variables, a\ndifferentiable transition of stiffness matrix between classes can be easily\nachieved with an analytical gradient. After integrating LVGP with the\ndensity-based TO, an efficient data-driven cellular composite optimization\nprocess is developed to enable concurrent exploration of microstructure\nconcepts and the associated volume fractions for natural frequency\noptimization. Examples reveal that the proposed cellular designs with\nmulticlass microstructures achieve higher natural frequencies than both\nsingle-scale and single-class designs. This framework can be easily extended to\nother multi-scale TO problems, such as thermal compliance and dynamic response\noptimization.",
    "descriptor": "\nComments: Preprint submitted to Composite Structures\n",
    "authors": [
      "Liwei Wang",
      "Anton van Beek",
      "Daicong Da",
      "Yu-Chin Chan",
      "Ping Zhu",
      "Wei Chen"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06478"
  },
  {
    "id": "arXiv:2106.06479",
    "title": "Numerical Solution of the $L^1$-Optimal Transport Problem on Surfaces",
    "abstract": "In this article we study the numerical solution of the $L^1$-Optimal\nTransport Problem on 2D surfaces embedded in $R^3$, via the DMK formulation\nintroduced in [FaccaCardinPutti:2018]. We extend from the Euclidean into the\nRiemannian setting the DMK model and conjecture the equivalence with the\nsolution Monge-Kantorovich equations, a PDE-based formulation of the\n$L^1$-Optimal Transport Problem.\nWe generalize the numerical method proposed in\n[FaccaCardinPutti:2018,FaccaDaneriCardinPutti:2020] to 2D surfaces embedded in\n$\\REAL^3$ using the Surface Finite Element Model approach to approximate the\nLaplace-Beltrami equation arising from the model. We test the accuracy and\nefficiency of the proposed numerical scheme, comparing our approximate solution\nwith respect to an exact solution on a 2D sphere. The results show that the\nnumerical scheme is efficient, robust, and more accurate with respect to other\nnumerical schemes presented in the literature for the solution of\nls$L^1$-Optimal Transport Problem on 2D surfaces.",
    "descriptor": "",
    "authors": [
      "Luca Berti",
      "Enrico Facca",
      "Mario Putti"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.06479"
  },
  {
    "id": "arXiv:2106.06480",
    "title": "Multi-Receiver Online Bayesian Persuasion",
    "abstract": "Bayesian persuasion studies how an informed sender should partially disclose\ninformation to influence the behavior of a self-interested receiver. Classical\nmodels make the stringent assumption that the sender knows the receiver's\nutility. This can be relaxed by considering an online learning framework in\nwhich the sender repeatedly faces a receiver of an unknown, adversarially\nselected type. We study, for the first time, an online Bayesian persuasion\nsetting with multiple receivers. We focus on the case with no externalities and\nbinary actions, as customary in offline models. Our goal is to design no-regret\nalgorithms for the sender with polynomial per-iteration running time. First, we\nprove a negative result: for any $0 < \\alpha \\leq 1$, there is no\npolynomial-time no-$\\alpha$-regret algorithm when the sender's utility function\nis supermodular or anonymous. Then, we focus on the case of submodular sender's\nutility functions and we show that, in this case, it is possible to design a\npolynomial-time no-$(1 - \\frac{1}{e})$-regret algorithm. To do so, we introduce\na general online gradient descent scheme to handle online learning problems\nwith a finite number of possible loss functions. This requires the existence of\nan approximate projection oracle. We show that, in our setting, there exists\none such projection oracle which can be implemented in polynomial time.",
    "descriptor": "",
    "authors": [
      "Matteo Castiglioni",
      "Alberto Marchesi",
      "Andrea Celli",
      "Nicola Gatti"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06480"
  },
  {
    "id": "arXiv:2106.06482",
    "title": "Neural Network Modeling of Probabilities for Coding the Octree  Representation of Point Clouds",
    "abstract": "This paper describes a novel lossless point cloud compression algorithm that\nuses a neural network for estimating the coding probabilities for the occupancy\nstatus of voxels, depending on wide three dimensional contexts around the voxel\nto be encoded. The point cloud is represented as an octree, with each\nresolution layer being sequentially encoded and decoded using arithmetic\ncoding, starting from the lowest resolution, until the final resolution is\nreached. The occupancy probability of each voxel of the splitting pattern at\neach node of the octree is modeled by a neural network, having at its input the\nalready encoded occupancy status of several octree nodes (belonging to the past\nand current resolutions), corresponding to a 3D context surrounding the node to\nbe encoded. The algorithm has a fast and a slow version, the fast version\nselecting differently several voxels of the context, which allows an increased\nparallelization by sending larger batches of templates to be estimated by the\nneural network, at both encoder and decoder. The proposed algorithms yield\nstate-of-the-art results on benchmark datasets. The implementation will be made\navailable at https://github.com/marmus12/nnctx",
    "descriptor": "\nComments: 6 pages, 3 figures, Submitted to MMSP 2021\n",
    "authors": [
      "Emre Can Kaya",
      "Ioan Tabus"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.06482"
  },
  {
    "id": "arXiv:2106.06483",
    "title": "Optimal Model Selection in Contextual Bandits with Many Classes via  Offline Oracles",
    "abstract": "We study the problem of model selection for contextual bandits, in which the\nalgorithm must balance the bias-variance trade-off for model estimation while\nalso balancing the exploration-exploitation trade-off. In this paper, we\npropose the first reduction of model selection in contextual bandits to offline\nmodel selection oracles, allowing for flexible general purpose algorithms with\ncomputational requirements no worse than those for model selection for\nregression. Our main result is a new model selection guarantee for stochastic\ncontextual bandits. When one of the classes in our set is realizable, up to a\nlogarithmic dependency on the number of classes, our algorithm attains optimal\nrealizability-based regret bounds for that class under one of two conditions:\nif the time-horizon is large enough, or if an assumption that helps with\ndetecting misspecification holds. Hence our algorithm adapts to the complexity\nof this unknown class. Even when this realizable class is known, we prove\nimproved regret guarantees in early rounds by relying on simpler model classes\nfor those rounds and hence further establish the importance of model selection\nin contextual bandits.",
    "descriptor": "",
    "authors": [
      "Sanath Kumar Krishnamurthy",
      "Susan Athey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06483"
  },
  {
    "id": "arXiv:2106.06485",
    "title": "Pedestrian Attribute Recognition in Video Surveillance Scenarios Based  on View-attribute Attention Localization",
    "abstract": "Pedestrian attribute recognition in surveillance scenarios is still a\nchallenging task due to inaccurate localization of specific attributes. In this\npaper, we propose a novel view-attribute localization method based on attention\n(VALA), which relies on the strong relevance between attributes and views to\ncapture specific view-attributes and to localize attribute-corresponding areas\nby attention mechanism. A specific view-attribute is composed by the extracted\nattribute feature and four view scores which are predicted by view predictor as\nthe confidences for attribute from different views. View-attribute is then\ndelivered back to shallow network layers for supervising deep feature\nextraction. To explore the location of a view-attribute, regional attention is\nintroduced to aggregate spatial information of the input attribute feature in\nheight and width direction for constraining the image into a narrow range.\nMoreover, the inter-channel dependency of view-feature is embedded in the above\ntwo spatial directions. An attention attribute-specific region is gained after\nfining the narrow range by balancing the ratio of channel dependencies between\nheight and width branches. The final view-attribute recognition outcome is\nobtained by combining the output of regional attention with the view scores\nfrom view predictor. Experiments on three wide datasets (RAP, RAPv2, PETA, and\nPA-100K) demonstrate the effectiveness of our approach compared with\nstate-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Weichen Chen",
      "Xinyi Yu",
      "Linlin Ou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06485"
  },
  {
    "id": "arXiv:2106.06487",
    "title": "A dataset of mentorship in science with semantic and demographic  estimations",
    "abstract": "Mentorship in science is crucial for topic choice, career decisions, and the\nsuccess of mentees and mentors. Typically, researchers who study mentorship use\narticle co-authorship and doctoral dissertation datasets. However, available\ndatasets of this type focus on narrow selections of fields and miss out on\nearly career and non-publication-related interactions. Here, we describe\nMENTORSHIP, a crowdsourced dataset of 743176 mentorship relationships among\n738989 scientists across 112 fields that avoids these shortcomings. We enrich\nthe scientists' profiles with publication data from the Microsoft Academic\nGraph and \"semantic\" representations of research using deep learning content\nanalysis. Because gender and race have become critical dimensions when\nanalyzing mentorship and disparities in science, we also provide estimations of\nthese factors. We perform extensive validations of the profile--publication\nmatching, semantic content, and demographic inferences. We anticipate this\ndataset will spur the study of mentorship in science and deepen our\nunderstanding of its role in scientists' career outcomes.",
    "descriptor": "\nComments: Data can be found at this https URL\n",
    "authors": [
      "Qing Ke",
      "Lizhen Liang",
      "Ying Ding",
      "Stephen V. David",
      "Daniel E. Acuna"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.06487"
  },
  {
    "id": "arXiv:2106.06489",
    "title": "Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression  Spotting from Long Videos",
    "abstract": "Facial expressions vary from the visible to the subtle. In recent years, the\nanalysis of micro-expressions $-$ a natural occurrence resulting from the\nsuppression of one's true emotions, has drawn the attention of researchers with\na broad range of potential applications. However, spotting microexpressions in\nlong videos becomes increasingly challenging when intertwined with normal or\nmacro-expressions. In this paper, we propose a shallow optical flow\nthree-stream CNN (SOFTNet) model to predict a score that captures the\nlikelihood of a frame being in an expression interval. By fashioning the\nspotting task as a regression problem, we introduce pseudo-labeling to\nfacilitate the learning process. We demonstrate the efficacy and efficiency of\nthe proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art\nperformance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM\nLong Videos.",
    "descriptor": "\nComments: Accepted for publication in ICIP2021. 9 pages, including 3 pages of supplemental notes\n",
    "authors": [
      "Gen-Bing Liong",
      "John See",
      "Lai-Kuan Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.06489"
  },
  {
    "id": "arXiv:2106.06498",
    "title": "An adaptive cognitive sensor node for ECG monitoring in the Internet of  Medical Things",
    "abstract": "The Internet of Medical Things (IoMT) paradigm is becoming mainstream in\nmultiple clinical trials and healthcare procedures. It relies on novel very\naccurate and compact sensing devices and communication infrastructures, opening\npreviously unmatched possibilities of implementing data collection and\ncontinuous patient monitoring. Nevertheless, to fully exploit the potential of\nthis technology, some steps forwards are needed. First, the edge-computing\nparadigm must be added to the picture. A certain level of near-sensor\nprocessing has to be enabled, to improve the scalability, portability,\nreliability, responsiveness of the IoMT nodes. Second, novel, increasingly\naccurate, data analysis algorithms, such as those based on artificial\nintelligence and Deep Learning, must be exploited. To reach these objectives,\ndesigners, programmers of IoMT nodes, have to face challenging optimization\ntasks, in order to execute fairly complex computing tasks on low-power wearable\nand portable processing systems, with tight power and battery lifetime budgets.\nIn this work, we explore the implementation of cognitive data analysis\nalgorithm on resource-constrained computing platforms. To minimize power\nconsumption, we add an adaptivity layer that dynamically manages the hardware\nand software configuration of the device to adapt it at runtime to the required\noperating mode. We have assessed our approach on a use-case using a\nconvolutional neural network to classify electrocardiogram (ECG) traces on a\nlow-power microcontroller. Our experimental results show that adapting the node\nsetup to the workload at runtime can save up to 50% power consumption and a\nquantized neural network reaches an accuracy value higher than 98% for\narrhythmia disorders detection on MIT-BIH Arrhythmia dataset.",
    "descriptor": "",
    "authors": [
      "Matteo Antonio Scrugli",
      "Daniela Loi",
      "Luigi Raffo",
      "Paolo Meloni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.06498"
  },
  {
    "id": "arXiv:2106.06499",
    "title": "Policy Gradient Bayesian Robust Optimization for Imitation Learning",
    "abstract": "The difficulty in specifying rewards for many real-world problems has led to\nan increased focus on learning rewards from human feedback, such as\ndemonstrations. However, there are often many different reward functions that\nexplain the human feedback, leaving agents with uncertainty over what the true\nreward function is. While most policy optimization approaches handle this\nuncertainty by optimizing for expected performance, many applications demand\nrisk-averse behavior. We derive a novel policy gradient-style robust\noptimization approach, PG-BROIL, that optimizes a soft-robust objective that\nbalances expected performance and risk. To the best of our knowledge, PG-BROIL\nis the first policy optimization algorithm robust to a distribution of reward\nhypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL\ncan produce a family of behaviors ranging from risk-neutral to risk-averse and\noutperforms state-of-the-art imitation learning algorithms when learning from\nambiguous demonstrations by hedging against uncertainty, rather than seeking to\nuniquely identify the demonstrator's reward function.",
    "descriptor": "\nComments: In proceedings International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Zaynah Javed",
      "Daniel S. Brown",
      "Satvik Sharma",
      "Jerry Zhu",
      "Ashwin Balakrishna",
      "Marek Petrik",
      "Anca D. Dragan",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06499"
  },
  {
    "id": "arXiv:2106.06500",
    "title": "A Benchmark of Dynamical Variational Autoencoders applied to Speech  Spectrogram Modeling",
    "abstract": "The Variational Autoencoder (VAE) is a powerful deep generative model that is\nnow extensively used to represent high-dimensional complex data via a\nlow-dimensional latent space learned in an unsupervised manner. In the original\nVAE model, input data vectors are processed independently. In recent years, a\nseries of papers have presented different extensions of the VAE to process\nsequential data, that not only model the latent space, but also model the\ntemporal dependencies within a sequence of data vectors and corresponding\nlatent vectors, relying on recurrent neural networks. We recently performed a\ncomprehensive review of those models and unified them into a general class\ncalled Dynamical Variational Autoencoders (DVAEs). In the present paper, we\npresent the results of an experimental benchmark comparing six of those DVAE\nmodels on the speech analysis-resynthesis task, as an illustration of the high\npotential of DVAEs for speech modeling.",
    "descriptor": "\nComments: Accepted to Interspeech 2021. arXiv admin note: text overlap with arXiv:2008.12595\n",
    "authors": [
      "Xiaoyu Bie",
      "Laurent Girin",
      "Simon Leglaive",
      "Thomas Hueber",
      "Xavier Alameda-Pineda"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06500"
  },
  {
    "id": "arXiv:2106.06503",
    "title": "High order integrators obtained by linear combinations of  symmetric-conjugate compositions",
    "abstract": "A new family of methods involving complex coefficients for the numerical\nintegration of differential equations is presented and analyzed. They are\nconstructed as linear combinations of symmetric-conjugate compositions obtained\nfrom a basic time-symmetric integrator of order 2n (n $\\ge$ 1). The new\nintegrators are of order 2(n + k), k = 1, 2, ..., and preserve time-symmetry up\nto order 4n + 3 when applied to differential equations with real vector fields.\nIf in addition the system is Hamiltonian and the basic scheme is symplectic,\nthen they also preserve symplecticity up to order 4n + 3. We show that these\nintegrators are well suited for a parallel implementation, thus improving their\nefficiency. Methods up to order 10 based on a 4th-order integrator are built\nand tested in comparison with other standard procedures to increase the order\nof a basic scheme.",
    "descriptor": "",
    "authors": [
      "Fernando Casas",
      "Alejandro Escorihuela-Tom\u00e0s"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06503"
  },
  {
    "id": "arXiv:2106.06504",
    "title": "How Should Agents Ask Questions For Situated Learning? An Annotated  Dialogue Corpus",
    "abstract": "Intelligent agents that are confronted with novel concepts in situated\nenvironments will need to ask their human teammates questions to learn about\nthe physical world. To better understand this problem, we need data about\nasking questions in situated task-based interactions. To this end, we present\nthe Human-Robot Dialogue Learning (HuRDL) Corpus - a novel dialogue corpus\ncollected in an online interactive virtual environment in which human\nparticipants play the role of a robot performing a collaborative\ntool-organization task. We describe the corpus data and a corresponding\nannotation scheme to offer insight into the form and content of questions that\nhumans ask to facilitate learning in a situated environment. We provide the\ncorpus as an empirically-grounded resource for improving question generation in\nsituated intelligent agents.",
    "descriptor": "\nComments: Corpus available at this https URL . To appear in proceedings of SIGDial 2021\n",
    "authors": [
      "Felix Gervits",
      "Antonio Roque",
      "Gordon Briggs",
      "Matthias Scheutz",
      "Matthew Marge"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06504"
  },
  {
    "id": "arXiv:2106.06505",
    "title": "Efficient Deep Learning Architectures for Fast Identification of  Bacterial Strains in Resource-Constrained Devices",
    "abstract": "This work presents twelve fine-tuned deep learning architectures to solve the\nbacterial classification problem over the Digital Image of Bacterial Species\nDataset. The base architectures were mainly published as mobile or efficient\nsolutions to the ImageNet challenge, and all experiments presented in this work\nconsisted of making several modifications to the original designs, in order to\nmake them able to solve the bacterial classification problem by using\nfine-tuning and transfer learning techniques. This work also proposes a novel\ndata augmentation technique for this dataset, which is based on the idea of\nartificial zooming, strongly increasing the performance of every tested\narchitecture, even doubling it in some cases. In order to get robust and\ncomplete evaluations, all experiments were performed with 10-fold\ncross-validation and evaluated with five different metrics: top-1 and top-5\naccuracy, precision, recall, and F1 score. This paper presents a complete\ncomparison of the twelve different architectures, cross-validated with the\noriginal and the augmented version of the dataset, the results are also\ncompared with several literature methods. Overall, eight of the eleven\narchitectures surpassed the 0.95 scores in top-1 accuracy with our data\naugmentation method, being 0.9738 the highest top-1 accuracy. The impact of the\ndata augmentation technique is reported with relative improvement scores.",
    "descriptor": "\nComments: 22 pages, 2 figures, 5 tables. Submitted to Multimedia Tools and Applications, issue 1218 - Engineering Tools and Applications in Medical Imaging (currently in reviewing process)\n",
    "authors": [
      "R. Gallardo Garc\u00eda",
      "S. Jarqu\u00edn Rodr\u00edguez",
      "B. Beltr\u00e1n Mart\u00ednez",
      "C. Hern\u00e1ndez Gracidas",
      "R. Mart\u00ednez Torres"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06505"
  },
  {
    "id": "arXiv:2106.06508",
    "title": "Preferential Temporal Difference Learning",
    "abstract": "Temporal-Difference (TD) learning is a general and very useful tool for\nestimating the value function of a given policy, which in turn is required to\nfind good policies. Generally speaking, TD learning updates states whenever\nthey are visited. When the agent lands in a state, its value can be used to\ncompute the TD-error, which is then propagated to other states. However, it may\nbe interesting, when computing updates, to take into account other information\nthan whether a state is visited or not. For example, some states might be more\nimportant than others (such as states which are frequently seen in a successful\ntrajectory). Or, some states might have unreliable value estimates (for\nexample, due to partial observability or lack of data), making their values\nless desirable as targets. We propose an approach to re-weighting states used\nin TD updates, both when they are the input and when they provide the target\nfor the update. We prove that our approach converges with linear function\napproximation and illustrate its desirable empirical behaviour compared to\nother TD-style methods.",
    "descriptor": "\nComments: Accepted at the 38th International Conference on Machine Learning (ICML, 2021)\n",
    "authors": [
      "Nishanth Anand",
      "Doina Precup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06508"
  },
  {
    "id": "arXiv:2106.06509",
    "title": "Step-Wise Hierarchical Alignment Network for Image-Text Matching",
    "abstract": "Image-text matching plays a central role in bridging the semantic gap between\nvision and language. The key point to achieve precise visual-semantic alignment\nlies in capturing the fine-grained cross-modal correspondence between image and\ntext. Most previous methods rely on single-step reasoning to discover the\nvisual-semantic interactions, which lacks the ability of exploiting the\nmulti-level information to locate the hierarchical fine-grained relevance.\nDifferent from them, in this work, we propose a step-wise hierarchical\nalignment network (SHAN) that decomposes image-text matching into multi-step\ncross-modal reasoning process. Specifically, we first achieve local-to-local\nalignment at fragment level, following by performing global-to-local and\nglobal-to-global alignment at context level sequentially. This progressive\nalignment strategy supplies our model with more complementary and sufficient\nsemantic clues to understand the hierarchical correlations between image and\ntext. The experimental results on two benchmark datasets demonstrate the\nsuperiority of our proposed method.",
    "descriptor": "\nComments: Accepted by IJCAI 2021\n",
    "authors": [
      "Zhong Ji",
      "Kexin Chen",
      "Haoran Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06509"
  },
  {
    "id": "arXiv:2106.06515",
    "title": "Probability Paths and the Structure of Predictions over Time",
    "abstract": "In settings ranging from weather forecasts to political prognostications to\nfinancial projections, probability estimates of future binary outcomes often\nevolve over time. For example, the estimated likelihood of rain on a specific\nday changes by the hour as new information becomes available. Given a\ncollection of such probability paths, we introduce a Bayesian framework --\nwhich we call the Gaussian latent information martingale, or GLIM -- for\nmodeling the structure of dynamic predictions over time. Suppose, for example,\nthat the likelihood of rain in a week is 50%, and consider two hypothetical\nscenarios. In the first, one expects the forecast is equally likely to become\neither 25% or 75% tomorrow; in the second, one expects the forecast to stay\nconstant for the next several days. A time-sensitive decision-maker might\nselect a course of action immediately in the latter scenario, but may postpone\ntheir decision in the former, knowing that new information is imminent. We\nmodel these trajectories by assuming predictions update according to a latent\nprocess of information flow, which is inferred from historical data. In\ncontrast to general methods for time series analysis, this approach preserves\nthe martingale structure of probability paths and better quantifies future\nuncertainties around probability paths. We show that GLIM outperforms three\npopular baseline methods, producing better estimated posterior probability path\ndistributions measured by three different metrics. By elucidating the dynamic\nstructure of predictions over time, we hope to help individuals make more\ninformed choices.",
    "descriptor": "",
    "authors": [
      "Zhiyuan Lin",
      "Hao Sheng",
      "Sharad Goel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06515"
  },
  {
    "id": "arXiv:2106.06519",
    "title": "N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR  Hypotheses",
    "abstract": "Spoken Language Understanding (SLU) systems parse speech into semantic\nstructures like dialog acts and slots. This involves the use of an Automatic\nSpeech Recognizer (ASR) to transcribe speech into multiple text alternatives\n(hypotheses). Transcription errors, common in ASRs, impact downstream SLU\nperformance negatively. Approaches to mitigate such errors involve using richer\ninformation from the ASR, either in form of N-best hypotheses or word-lattices.\nWe hypothesize that transformer models learn better with a simpler utterance\nrepresentation using the concatenation of the N-best ASR alternatives, where\neach alternative is separated by a special delimiter [SEP]. In our work, we\ntest our hypothesis by using concatenated N-best ASR alternatives as the input\nto transformer encoder models, namely BERT and XLM-RoBERTa, and achieve\nperformance equivalent to the prior state-of-the-art model on DSTC2 dataset. We\nalso show that our approach significantly outperforms the prior\nstate-of-the-art when subjected to the low data regime. Additionally, this\nmethodology is accessible to users of third-party ASR APIs which do not provide\nword-lattice information.",
    "descriptor": "\nComments: 6 pages, 3 figures, Accepted at ACL 2021 as a main conference paper\n",
    "authors": [
      "Karthik Ganesan",
      "Pakhi Bamdev",
      "Jaivarsan B",
      "Amresh Venugopal",
      "Abhinav Tushar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06519"
  },
  {
    "id": "arXiv:2106.06524",
    "title": "WAX-ML: A Python library for machine learning and feedback loops on  streaming data",
    "abstract": "Wax is what you put on a surfboard to avoid slipping. It is an essential tool\nto go surfing... We introduce WAX-ML a research-oriented Python library\nproviding tools to design powerful machine learning algorithms and feedback\nloops working on streaming data. It strives to complement JAX with tools\ndedicated to time series. WAX-ML makes JAX-based programs easy to use for\nend-users working with pandas and xarray for data manipulation. It provides a\nsimple mechanism for implementing feedback loops, allows the implementation of\nonline learning and reinforcement learning algorithms with functions, and makes\nthem easy to integrate by end-users working with the object-oriented\nreinforcement learning framework from the Gym library. It is released with an\nApache open-source license on GitHub at https://github.com/eserie/wax-ml.",
    "descriptor": "\nComments: 12 pages, 7 figures\n",
    "authors": [
      "Emmanuel S\u00e9ri\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06524"
  },
  {
    "id": "arXiv:2106.06525",
    "title": "Analysis of a new Cardinality Estimator -- ExtendedHyperLogLog",
    "abstract": "We discuss the problem of counting distinct elements in a stream. A stream is\nusually considered as a sequence of elements that come one at a time. An exact\nsolution to the problem requires memory space of the size of the stream. For\nmany applications this solution is infeasible due to very large streams. The\nsolution in that case, is to use a probabilistic data structure (also called\nsketch), from which we can estimate with high accuracy the cardinality of the\nstream. We present a new algorithm, ExtendedHyperLogLog (EHLL), which is based\non the state-of-the-art algorithm, HyperLogLog (HLL). In order to achieve the\nsame accuracy as HLL, EHLL uses 16% less memory. In recent years, a martingale\napproach has bean developed. In the martingale setting we receive better\naccuracy at the price of not being able to merge sketches. EHLL also works in\nthe martingale setting. Martingale EHLL achieves the same accuracy as\nMartingale HLL using 12% less memory.",
    "descriptor": "",
    "authors": [
      "Tal Ohayon",
      "Aryeh Kontorovich"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.06525"
  },
  {
    "id": "arXiv:2106.06526",
    "title": "Online Continual Adaptation with Active Self-Training",
    "abstract": "Models trained with offline data often suffer from continual distribution\nshifts and expensive labeling in changing environments. This calls for a new\nonline learning paradigm where the learner can continually adapt to changing\nenvironments with limited labels. In this paper, we propose a new online\nsetting -- Online Active Continual Adaptation, where the learner aims to\ncontinually adapt to changing distributions using both unlabeled samples and\nactive queries of limited labels. To this end, we propose Online Self-Adaptive\nMirror Descent (OSAMD), which adopts an online teacher-student structure to\nenable online self-training from unlabeled data, and a margin-based criterion\nthat decides whether to query the labels to track changing distributions.\nTheoretically, we show that, in the separable case, OSAMD has an $O({T}^{1/2})$\ndynamic regret bound under mild assumptions, which is even tighter than the\nlower bound $\\Omega(T^{2/3})$ of traditional online learning with full labels.\nIn the general case, we show a regret bound of $O({\\alpha^*}^{1/3} {T}^{2/3} +\n\\alpha^* T)$, where $\\alpha^*$ denotes the separability of domains and is\nusually small. Our theoretical results show that OSAMD can fast adapt to\nchanging environments with active queries. Empirically, we demonstrate that\nOSAMD achieves favorable regrets under changing environments with limited\nlabels on both simulated and real-world data, which corroborates our\ntheoretical findings.",
    "descriptor": "",
    "authors": [
      "Shiji Zhou",
      "Han Zhao",
      "Shanghang Zhang",
      "Lianzhe Wang",
      "Heng Chang",
      "Zhi Wang",
      "Wenwu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06526"
  },
  {
    "id": "arXiv:2106.06528",
    "title": "Local Explanation of Dialogue Response Generation",
    "abstract": "In comparison to the interpretation of classification models, the explanation\nof sequence generation models is also an important problem, however it has seen\nlittle attention. In this work, we study model-agnostic explanations of a\nrepresentative text generation task -- dialogue response generation. Dialog\nresponse generation is challenging with its open-ended sentences and multiple\nacceptable responses. To gain insights into the reasoning process of a\ngeneration model, we propose anew method, local explanation of response\ngeneration (LERG) that regards the explanations as the mutual interaction of\nsegments in input and output sentences. LERG views the sequence prediction as\nuncertainty estimation of a human response and then creates explanations by\nperturbing the input and calculating the certainty change over the human\nresponse. We show that LERG adheres to desired properties of explanations for\ntext generation including unbiased approximation, consistency and cause\nidentification. Empirically, our results show that our method consistently\nimproves other widely used methods on proposed automatic- and human- evaluation\nmetrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can\nextract both explicit and implicit relations between input and output segments.",
    "descriptor": "",
    "authors": [
      "Yi-Lin Tuan",
      "Connor Pryor",
      "Wenhu Chen",
      "Lise Getoor",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06528"
  },
  {
    "id": "arXiv:2106.06529",
    "title": "The Limitations of Large Width in Neural Networks: A Deep Gaussian  Process Perspective",
    "abstract": "Large width limits have been a recent focus of deep learning research: modulo\ncomputational practicalities, do wider networks outperform narrower ones?\nAnswering this question has been challenging, as conventional networks gain\nrepresentational power with width, potentially masking any negative effects.\nOur analysis in this paper decouples capacity and width via the generalization\nof neural networks to Deep Gaussian Processes (Deep GP), a class of\nhierarchical models that subsume neural nets. In doing so, we aim to understand\nhow width affects standard neural networks once they have sufficient capacity\nfor a given modeling task. Our theoretical and empirical results on Deep GP\nsuggest that large width is generally detrimental to hierarchical models.\nSurprisingly, we prove that even nonparametric Deep GP converge to Gaussian\nprocesses, effectively becoming shallower without any increase in\nrepresentational power. The posterior, which corresponds to a mixture of\ndata-adaptable basis functions, becomes less data-dependent with width. Our\ntail analysis demonstrates that width and depth have opposite effects: depth\naccentuates a model's non-Gaussianity, while width makes models increasingly\nGaussian. We find there is a \"sweet spot\" that maximizes test set performance\nbefore the limiting GP behavior prevents adaptability, occurring at width = 1\nor width = 2 for nonparametric Deep GP. These results make strong predictions\nabout the same phenomenon in conventional neural networks: we show empirically\nthat many neural network architectures need 10 - 500 hidden units for\nsufficient capacity - depending on the dataset - but further width degrades\ntest performance.",
    "descriptor": "",
    "authors": [
      "Geoff Pleiss",
      "John P. Cunningham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06529"
  },
  {
    "id": "arXiv:2106.06530",
    "title": "Label Noise SGD Provably Prefers Flat Global Minimizers",
    "abstract": "In overparametrized models, the noise in stochastic gradient descent (SGD)\nimplicitly regularizes the optimization trajectory and determines which local\nminimum SGD converges to. Motivated by empirical studies that demonstrate that\ntraining with noisy labels improves generalization, we study the implicit\nregularization effect of SGD with label noise. We show that SGD with label\nnoise converges to a stationary point of a regularized loss $L(\\theta) +\\lambda\nR(\\theta)$, where $L(\\theta)$ is the training loss, $\\lambda$ is an effective\nregularization parameter depending on the step size, strength of the label\nnoise, and the batch size, and $R(\\theta)$ is an explicit regularizer that\npenalizes sharp minimizers. Our analysis uncovers an additional regularization\neffect of large learning rates beyond the linear scaling rule that penalizes\nlarge eigenvalues of the Hessian more than small ones. We also prove extensions\nto classification with general loss functions, SGD with momentum, and SGD with\ngeneral noise covariance, significantly strengthening the prior work of Blanc\net al. to global convergence and large learning rates and of HaoChen et al. to\ngeneral models.",
    "descriptor": "\nComments: 53 pages, 4 figures, under review for NeurIPS 2021\n",
    "authors": [
      "Alex Damian",
      "Tengyu Ma",
      "Jason Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06530"
  },
  {
    "id": "arXiv:2106.06533",
    "title": "View Generalization for Single Image Textured 3D Models",
    "abstract": "Humans can easily infer the underlying 3D geometry and texture of an object\nonly from a single 2D image. Current computer vision methods can do this, too,\nbut suffer from view generalization problems - the models inferred tend to make\npoor predictions of appearance in novel views. As for generalization problems\nin machine learning, the difficulty is balancing single-view accuracy (cf.\ntraining error; bias) with novel view accuracy (cf. test error; variance). We\ndescribe a class of models whose geometric rigidity is easily controlled to\nmanage this tradeoff. We describe a cycle consistency loss that improves view\ngeneralization (roughly, a model from a generated view should predict the\noriginal view well). View generalization of textures requires that models share\ntexture information, so a car seen from the back still has headlights because\nother cars have headlights. We describe a cycle consistency loss that\nencourages model textures to be aligned, so as to encourage sharing. We compare\nour method against the state-of-the-art method and show both qualitative and\nquantitative improvements.",
    "descriptor": "\nComments: CVPR 2021. Project website: this https URL\n",
    "authors": [
      "Anand Bhattad",
      "Aysegul Dundar",
      "Guilin Liu",
      "Andrew Tao",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06533"
  },
  {
    "id": "arXiv:2106.06536",
    "title": "Unsupervised Neural Hidden Markov Models with a Continuous latent state  space",
    "abstract": "We introduce a new procedure to neuralize unsupervised Hidden Markov Models\nin the continuous case. This provides higher flexibility to solve problems with\nunderlying latent variables. This approach is evaluated on both synthetic and\nreal data. On top of generating likely model parameters with comparable\nperformances to off-the-shelf neural architecture (LSTMs, GRUs,..), the\nobtained results are easily interpretable.",
    "descriptor": "",
    "authors": [
      "Firas Jarboui",
      "Vianney Perchet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06536"
  },
  {
    "id": "arXiv:2106.05999",
    "title": "Electricity and Reserve Pricing in Chance-Constrained Electricity  Markets with Asymmetric Balancing Reserve Policies",
    "abstract": "Recently, chance-constrained stochastic electricity market designs have been\nproposed to address the shortcomings of scenario-based stochastic market\ndesigns. In particular, the use of chance-constrained market-clearing avoids\ntrading off in-expectation and per-scenario characteristics and yields unique\nenergy and reserves prices. However, current formulations rely on symmetric\ncontrol policies based on the aggregated system imbalance, which restricts\nbalancing reserve providers in their energy and reserve commitments. This paper\nextends existing chance-constrained market-clearing formulations by leveraging\nnode-to-node and asymmetric balancing reserve policies and deriving the\nresulting energy and reserve prices. The proposed node-to-node policy allows\nfor relating the remuneration of balancing reserve providers and payment of\nuncertain resources using a marginal cost-based approach. Further, we introduce\nasymmetric balancing reserve policies into the chance-constrained electricity\nmarket design and show how this additional degree of freedom affects market\noutcomes.",
    "descriptor": "",
    "authors": [
      "Alvaro Gonzalez-Castellanos",
      "Anton Hinneck",
      "Robert Mieth",
      "David Pozo",
      "Yury Dvorkin"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.05999"
  },
  {
    "id": "arXiv:2106.06016",
    "title": "Ensemble inversion for brain tumor growth models with mass effect",
    "abstract": "We propose a method for extracting physics-based biomarkers from a single\nmultiparametric Magnetic Resonance Imaging (mpMRI) scan bearing a glioma tumor.\nWe account for mass effect, the deformation of brain parenchyma due to the\ngrowing tumor, which on its own is an important radiographic feature but its\nautomatic quantification remains an open problem. In particular, we calibrate a\npartial differential equation (PDE) tumor growth model that captures mass\neffect, parameterized by a single scalar parameter, tumor proliferation,\nmigration, while localizing the tumor initiation site. The single-scan\ncalibration problem is severely ill-posed because the precancerous, healthy,\nbrain anatomy is unknown. To address the ill-posedness, we introduce an\nensemble inversion scheme that uses a number of normal subject brain templates\nas proxies for the healthy precancer subject anatomy. We verify our solver on a\nsynthetic dataset and perform a retrospective analysis on a clinical dataset of\n216 glioblastoma (GBM) patients. We analyze the reconstructions using our\ncalibrated biophysical model and demonstrate that our solver provides both\nglobal and local quantitative measures of tumor biophysics and mass effect. We\nfurther highlight the improved performance in model calibration through the\ninclusion of mass effect in tumor growth models -- including mass effect in the\nmodel leads to 10% increase in average dice coefficients for patients with\nsignificant mass effect. We further evaluate our model by introducing novel\nbiophysics-based features and using them for survival analysis. Our preliminary\nanalysis suggests that including such features can improve patient\nstratification and survival prediction.",
    "descriptor": "\nComments: 10 pages, 3 supplementary pages\n",
    "authors": [
      "Shashank Subramanian",
      "Klaudius Scheufele",
      "Naveen Himthani",
      "Christos Davatzikos",
      "George Biros"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2106.06016"
  },
  {
    "id": "arXiv:2106.06044",
    "title": "Convergence and Alignment of Gradient Descentwith Random Back  propagation Weights",
    "abstract": "Stochastic gradient descent with backpropagation is the workhorse of\nartificial neural networks. It has long been recognized that backpropagation\nfails to be a biologically plausible algorithm. Fundamentally, it is a\nnon-local procedure -- updating one neuron's synaptic weights requires\nknowledge of synaptic weights or receptive fields of downstream neurons. This\nlimits the use of artificial neural networks as a tool for understanding the\nbiological principles of information processing in the brain. Lillicrap et al.\n(2016) propose a more biologically plausible \"feedback alignment\" algorithm\nthat uses random and fixed backpropagation weights, and show promising\nsimulations. In this paper we study the mathematical properties of the feedback\nalignment procedure by analyzing convergence and alignment for two-layer\nnetworks under squared error loss. In the overparameterized setting, we prove\nthat the error converges to zero exponentially fast, and also that\nregularization is necessary in order for the parameters to become aligned with\nthe random backpropagation weights. Simulations are given that are consistent\nwith this analysis and suggest further generalizations. These results\ncontribute to our understanding of how biologically plausible algorithms might\ncarry out weight learning in a manner different from Hebbian learning, with\nperformance that is comparable with the full non-local backpropagation\nalgorithm.",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Ganlin Song",
      "Ruitu Xu",
      "John Lafferty"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06044"
  },
  {
    "id": "arXiv:2106.06064",
    "title": "RNN with Particle Flow for Probabilistic Spatio-temporal Forecasting",
    "abstract": "Spatio-temporal forecasting has numerous applications in analyzing wireless,\ntraffic, and financial networks. Many classical statistical models often fall\nshort in handling the complexity and high non-linearity present in time-series\ndata. Recent advances in deep learning allow for better modelling of spatial\nand temporal dependencies. While most of these models focus on obtaining\naccurate point forecasts, they do not characterize the prediction uncertainty.\nIn this work, we consider the time-series data as a random realization from a\nnonlinear state-space model and target Bayesian inference of the hidden states\nfor probabilistic forecasting. We use particle flow as the tool for\napproximating the posterior distribution of the states, as it is shown to be\nhighly effective in complex, high-dimensional settings. Thorough\nexperimentation on several real world time-series datasets demonstrates that\nour approach provides better characterization of uncertainty while maintaining\ncomparable accuracy to the state-of-the art point forecasting methods.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Soumyasundar Pal",
      "Liheng Ma",
      "Yingxue Zhang",
      "Mark Coates"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06064"
  },
  {
    "id": "arXiv:2106.06075",
    "title": "A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max  Optimization Problems",
    "abstract": "Min-max saddle point games have recently been intensely studied, due to their\nwide range of applications, including training Generative Adversarial\nNetworks~(GANs). However, most of the recent efforts for solving them are\nlimited to special regimes such as convex-concave games. Further, it is\ncustomarily assumed that the underlying optimization problem is solved either\nby a single machine or in the case of multiple machines connected in\ncentralized fashion, wherein each one communicates with a central node. The\nlatter approach becomes challenging, when the underlying communications network\nhas low bandwidth. In addition, privacy considerations may dictate that certain\nnodes can communicate with a subset of other nodes. Hence, it is of interest to\ndevelop methods that solve min-max games in a decentralized manner. To that\nend, we develop a decentralized adaptive momentum (ADAM)-type algorithm for\nsolving min-max optimization problem under the condition that the objective\nfunction satisfies a Minty Variational Inequality condition, which is a\ngeneralization to convex-concave case. The proposed method overcomes\nshortcomings of recent non-adaptive gradient-based decentralized algorithms for\nmin-max optimization problems that do not perform well in practice and require\ncareful tuning. In this paper, we obtain non-asymptotic rates of convergence of\nthe proposed algorithm (coined DADAM$^3$) for finding a (stochastic)\nfirst-order Nash equilibrium point and subsequently evaluate its performance on\ntraining GANs. The extensive empirical evaluation shows that DADAM$^3$\noutperforms recently developed methods, including decentralized optimistic\nstochastic gradient for solving such min-max problems.",
    "descriptor": "",
    "authors": [
      "Babak Barazandeh",
      "Tianjian Huang",
      "George Michailidis"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06075"
  },
  {
    "id": "arXiv:2106.06094",
    "title": "Indistinguishability Obfuscation of Null Quantum Circuits and  Applications",
    "abstract": "We study the notion of indistinguishability obfuscation for null quantum\ncircuits (quantum null-iO). We present a construction assuming: - The quantum\nhardness of learning with errors (LWE). - Post-quantum indistinguishability\nobfuscation for classical circuits. - A notion of ''dual-mode'' classical\nverification of quantum computation (CVQC).\nWe give evidence that our notion of dual-mode CVQC exists by proposing a\nscheme that is secure assuming LWE in the quantum random oracle model (QROM).\nThen we show how quantum null-iO enables a series of new cryptographic\nprimitives that, prior to our work, were unknown to exist even making heuristic\nassumptions. Among others, we obtain the first witness encryption scheme for\nQMA, the first publicly verifiable non-interactive zero-knowledge (NIZK) scheme\nfor QMA, and the first attribute-based encryption (ABE) scheme for BQP.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1912.04769 by other authors\n",
    "authors": [
      "James Bartusek",
      "Giulio Malavolta"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06094"
  },
  {
    "id": "arXiv:2106.06097",
    "title": "Neural Optimization Kernel: Towards Robust Deep Learning",
    "abstract": "Recent studies show a close connection between neural networks (NN) and\nkernel methods. However, most of these analyses (e.g., NTK) focus on the\ninfluence of (infinite) width instead of the depth of NN models. There remains\na gap between theory and practical network designs that benefit from the depth.\nThis paper first proposes a novel kernel family named Neural Optimization\nKernel (NOK). Our kernel is defined as the inner product between two $T$-step\nupdated functionals in RKHS w.r.t. a regularized optimization problem.\nTheoretically, we proved the monotonic descent property of our update rule for\nboth convex and non-convex problems, and a $O(1/T)$ convergence rate of our\nupdates for convex problems. Moreover, we propose a data-dependent structured\napproximation of our NOK, which builds the connection between training deep NNs\nand kernel methods associated with NOK. The resultant computational graph is a\nResNet-type finite width NN. Our structured approximation preserved the\nmonotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer\nNN performs $T$-step monotonic descent updates. Notably, we show our\n$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate\nw.r.t. a convex regularized problem, which explains the success of ReLU on\ntraining deep NN from a NN architecture optimization perspective. For the\nunsupervised learning and the shared parameter case, we show the equivalence of\ntraining structured NN with GD and performing functional gradient descent in\nRKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.\nFor finite NOKs, we prove generalization bounds. Remarkably, we show that\noverparameterized deep NN (NOK) can increase the expressive power to reduce\nempirical risk and reduce the generalization bound at the same time. Extensive\nexperiments verify the robustness of our structured NOK blocks.",
    "descriptor": "\nComments: Deep Learning, Kernel Methods, Deep Learning Theory, Kernel Approximation, Integral Approximation\n",
    "authors": [
      "Yueming Lyu",
      "Ivor Tsang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06097"
  },
  {
    "id": "arXiv:2106.06123",
    "title": "A Unified Framework for Constructing Nonconvex Regularizations",
    "abstract": "Over the past decades, many individual nonconvex methods have been proposed\nto achieve better sparse recovery performance in various scenarios. However,\nhow to construct a valid nonconvex regularization function remains open in\npractice. In this paper, we fill in this gap by presenting a unified framework\nfor constructing the nonconvex regularization based on the probability density\nfunction. Meanwhile, a new nonconvex sparse recovery method constructed via the\nWeibull distribution is studied.",
    "descriptor": "",
    "authors": [
      "Zhiyong Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06123"
  },
  {
    "id": "arXiv:2106.06143",
    "title": "Monotonic Neural Network: combining Deep Learning with Domain Knowledge  for Chiller Plants Energy Optimization",
    "abstract": "In this paper, we are interested in building a domain knowledge based deep\nlearning framework to solve the chiller plants energy optimization problems.\nCompared to the hotspot applications of deep learning (e.g. image\nclassification and NLP), it is difficult to collect enormous data for deep\nnetwork training in real-world physical systems. Most existing methods reduce\nthe complex systems into linear model to facilitate the training on small\nsamples. To tackle the small sample size problem, this paper considers domain\nknowledge in the structure and loss design of deep network to build a nonlinear\nmodel with lower redundancy function space. Specifically, the energy\nconsumption estimation of most chillers can be physically viewed as an\ninput-output monotonic problem. Thus, we can design a Neural Network with\nmonotonic constraints to mimic the physical behavior of the system. We verify\nthe proposed method in a cooling system of a data center, experimental results\nshow the superiority of our framework in energy optimization compared to the\nexisting ones.",
    "descriptor": "",
    "authors": [
      "Fanhe Ma",
      "Faen Zhang",
      "Shenglan Ben",
      "Shuxin Qin",
      "Pengcheng Zhou",
      "Changsheng Zhou",
      "Fengyi Xu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06143"
  },
  {
    "id": "arXiv:2106.06183",
    "title": "Improving RNN-T ASR Performance with Date-Time and Location Awareness",
    "abstract": "In this paper, we explore the benefits of incorporating context into a\nRecurrent Neural Network (RNN-T) based Automatic Speech Recognition (ASR) model\nto improve the speech recognition for virtual assistants. Specifically, we use\nmeta information extracted from the time at which the utterance is spoken and\nthe approximate location information to make ASR context aware. We show that\nthese contextual information, when used individually, improves overall\nperformance by as much as 3.48% relative to the baseline and when the contexts\nare combined, the model learns complementary features and the recognition\nimproves by 4.62%. On specific domains, these contextual signals show\nimprovements as high as 11.5%, without any significant degradation on others.\nWe ran experiments with models trained on data of sizes 30K hours and 10K\nhours. We show that the scale of improvement with the 10K hours dataset is much\nhigher than the one obtained with 30K hours dataset. Our results indicate that\nwith limited data to train the ASR model, contextual signals can improve the\nperformance significantly.",
    "descriptor": "\nComments: This paper has been accepted for publication in TSD 2021\n",
    "authors": [
      "Swayambhu Nath Ray",
      "Soumyajit Mitra",
      "Raghavendra Bilgi",
      "Sri Garimella"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.06183"
  },
  {
    "id": "arXiv:2106.06189",
    "title": "Order Matters: Probabilistic Modeling of Node Sequence for Graph  Generation",
    "abstract": "A graph generative model defines a distribution over graphs. One type of\ngenerative model is constructed by autoregressive neural networks, which\nsequentially add nodes and edges to generate a graph. However, the likelihood\nof a graph under the autoregressive model is intractable, as there are numerous\nsequences leading to the given graph; this makes maximum likelihood estimation\nchallenging. Instead, in this work we derive the exact joint probability over\nthe graph and the node ordering of the sequential process. From the joint, we\napproximately marginalize out the node orderings and compute a lower bound on\nthe log-likelihood using variational inference. We train graph generative\nmodels by maximizing this bound, without using the ad-hoc node orderings of\nprevious methods. Our experiments show that the log-likelihood bound is\nsignificantly tighter than the bound of previous schemes. Moreover, the models\nfitted with the proposed algorithm can generate high-quality graphs that match\nthe structures of target graphs not seen during training. We have made our code\npublicly available at\n\\hyperref[https://github.com/tufts-ml/graph-generation-vi]{https://github.com/tufts-ml/graph-generation-vi}.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Xiaohui Chen",
      "Xu Han",
      "Jiajing Hu",
      "Francisco J. R. Ruiz",
      "Liping Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.06189"
  },
  {
    "id": "arXiv:2106.06205",
    "title": "Time Warps, from Algebra to Algorithms",
    "abstract": "Graded modalities have been proposed in recent work on programming languages\nas a general framework for refining type systems with intensional properties.\nIn particular, continuous endomaps of the discrete time scale, or time warps,\ncan be used to quantify the growth of information in the course of program\nexecution. Time warps form a complete residuated lattice, with the residuals\nplaying an important role in potential programming applications. In this paper,\nwe study the algebraic structure of time warps, and prove that their equational\ntheory is decidable, a necessary condition for their use in real-world\ncompilers. We also describe how our universal-algebraic proof technique lends\nitself to a constraint-based implementation, establishing a new link between\nuniversal algebra and verification technology.",
    "descriptor": "\nComments: Submitted to a conference\n",
    "authors": [
      "Sam van Gool",
      "Adrien Guatto",
      "George Metcalfe",
      "Simon Santschi"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.06205"
  },
  {
    "id": "arXiv:2106.06231",
    "title": "Numerical Methods for Mean Field Games and Mean Field Type Control",
    "abstract": "Mean Field Games (MFG) have been introduced to tackle games with a large\nnumber of competing players. Considering the limit when the number of players\nis infinite, Nash equilibria are studied by considering the interaction of a\ntypical player with the population's distribution. The situation in which the\nplayers cooperate corresponds to Mean Field Control (MFC) problems, which can\nalso be viewed as optimal control problems driven by a McKean-Vlasov dynamics.\nThese two types of problems have found a wide range of potential applications,\nfor which numerical methods play a key role since most models do not have\nanalytical solutions. In these notes, we review several aspects of numerical\nmethods for MFG and MFC. We start by presenting some heuristics in a basic\nlinear-quadratic setting. We then discuss numerical schemes for\nforward-backward systems of partial differential equations (PDEs), optimization\ntechniques for variational problems driven by a Kolmogorov-Fokker-Planck PDE,\nan approach based on a monotone operator viewpoint, and stochastic methods\nrelying on machine learning tools.",
    "descriptor": "",
    "authors": [
      "Mathieu Lauriere"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06231"
  },
  {
    "id": "arXiv:2106.06237",
    "title": "KRADA: Known-region-aware Domain Alignment for Open World Semantic  Segmentation",
    "abstract": "In semantic segmentation, we aim to train a pixel-level classifier to assign\ncategory labels to all pixels in an image, where labeled training images and\nunlabeled test images are from the same distribution and share the same label\nset. However, in an open world, the unlabeled test images probably contain\nunknown categories and have different distributions from the labeled images.\nHence, in this paper, we consider a new, more realistic, and more challenging\nproblem setting where the pixel-level classifier has to be trained with labeled\nimages and unlabeled open-world images -- we name it open world semantic\nsegmentation (OSS). In OSS, the trained classifier is expected to identify\nunknown-class pixels and classify known-class pixels well. To solve OSS, we\nfirst investigate which distribution that unknown-class pixels obey. Then,\nmotivated by the goodness-of-fit test, we use statistical measurements to show\nhow a pixel fits the distribution of an unknown class and select highly-fitted\npixels to form the unknown region in each image. Eventually, we propose an\nend-to-end learning framework, known-region-aware domain alignment (KRADA), to\ndistinguish unknown classes while aligning distributions of known classes in\nlabeled and unlabeled open-world images. The effectiveness of KRADA has been\nverified on two synthetic tasks and one COVID-19 segmentation task.",
    "descriptor": "",
    "authors": [
      "Chenhong Zhou",
      "Feng Liu",
      "Chen Gong",
      "Tongliang Liu",
      "Bo Han",
      "William Cheung"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06237"
  },
  {
    "id": "arXiv:2106.06243",
    "title": "Unsupervised Anomaly Detection Ensembles using Item Response Theory",
    "abstract": "Constructing an ensemble from a heterogeneous set of unsupervised anomaly\ndetection methods is challenging because the class labels or the ground truth\nis unknown. Thus, traditional ensemble techniques that use the response\nvariable or the class labels cannot be used to construct an ensemble for\nunsupervised anomaly detection.\nWe use Item Response Theory (IRT) -- a class of models used in educational\npsychometrics to assess student and test question characteristics -- to\nconstruct an unsupervised anomaly detection ensemble. IRT's latent trait\ncomputation lends itself to anomaly detection because the latent trait can be\nused to uncover the hidden ground truth. Using a novel IRT mapping to the\nanomaly detection problem, we construct an ensemble that can downplay noisy,\nnon-discriminatory methods and accentuate sharper methods. We demonstrate the\neffectiveness of the IRT ensemble on an extensive data repository, by comparing\nits performance to other ensemble techniques.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Sevvandi Kandanaarachchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06243"
  },
  {
    "id": "arXiv:2106.06245",
    "title": "Model Selection for Bayesian Autoencoders",
    "abstract": "We develop a novel method for carrying out model selection for Bayesian\nautoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by\nthe common practice of type-II maximum likelihood optimization and its\nequivalence to Kullback-Leibler divergence minimization, we propose to optimize\nthe distributional sliced-Wasserstein distance (DSWD) between the output of the\nautoencoder and the empirical data distribution. The advantages of this\nformulation are that we can estimate the DSWD based on samples and handle\nhigh-dimensional problems. We carry out posterior estimation of the BAE\nparameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE\ninto a generative model by fitting a flexible Dirichlet mixture model in the\nlatent space. Consequently, we obtain a powerful alternative to variational\nautoencoders, which are the preferred choice in modern applications of\nautoencoders for representation learning with uncertainty. We evaluate our\napproach qualitatively and quantitatively using a vast experimental campaign on\na number of unsupervised learning tasks and show that, in small-data regimes\nwhere priors matter, our approach provides state-of-the-art results,\noutperforming multiple competitive baselines.",
    "descriptor": "",
    "authors": [
      "Ba-Hien Tran",
      "Simone Rossi",
      "Dimitrios Milios",
      "Pietro Michiardi",
      "Edwin V. Bonilla",
      "Maurizio Filippone"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06245"
  },
  {
    "id": "arXiv:2106.06251",
    "title": "On Learnability via Gradient Method for Two-Layer ReLU Neural Networks  in Teacher-Student Setting",
    "abstract": "Deep learning empirically achieves high performance in many applications, but\nits training dynamics has not been fully understood theoretically. In this\npaper, we explore theoretical analysis on training two-layer ReLU neural\nnetworks in a teacher-student regression model, in which a student network\nlearns an unknown teacher network through its outputs. We show that with a\nspecific regularization and sufficient over-parameterization, the student\nnetwork can identify the parameters of the teacher network with high\nprobability via gradient descent with a norm dependent stepsize even though the\nobjective function is highly non-convex. The key theoretical tool is the\nmeasure representation of the neural networks and a novel application of a dual\ncertificate argument for sparse estimation on a measure space. We analyze the\nglobal minima and global convergence property in the measure space.",
    "descriptor": "\nComments: 47 pages, 3 figures\n",
    "authors": [
      "Shunta Akiyama",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06251"
  },
  {
    "id": "arXiv:2106.06270",
    "title": "Measurement of onset of structural relaxation in melt-quenched phase  change materials",
    "abstract": "Chalcogenide phase change materials enable non-volatile, low-latency\nstorage-class memory. They are also being explored for new forms of computing\nsuch as neuromorphic and in-memory computing. A key challenge, however, is the\ntemporal drift in the electrical resistance of the amorphous states that encode\ndata. Drift, caused by the spontaneous structural relaxation of the newly\nrecreated melt-quenched amorphous phase, has consistently been observed to have\na logarithmic dependence in time. Here, we show that this observation is valid\nonly in a certain observable timescale. Using threshold-switching voltage as\nthe measured variable, based on temperature-dependent and short timescale\nelectrical characterization, we experimentally measure the onset of drift. This\nadditional feature of the structural relaxation dynamics serves as a new\nbenchmark to appraise the different classical models to explain drift.",
    "descriptor": "",
    "authors": [
      "Benedikt Kersting",
      "Syed Ghazi Sarwat",
      "Manuel Le Gallo",
      "Kevin Brew",
      "Sebastian Walfort",
      "Nicole Saulnier",
      "Martin Salinga",
      "Abu Sebastian"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.06270"
  },
  {
    "id": "arXiv:2106.06274",
    "title": "On Applying the Lackadaisical Quantum Walk Algorithm to Search for  Multiple Solutions on Grids",
    "abstract": "Quantum computing holds the promise of improving the information processing\npower to levels unreachable by classical computation. Quantum walks are heading\nthe development of quantum algorithms for searching information on graphs more\nefficiently than their classical counterparts. A quantum-walk-based algorithm\nthat is standing out in the literature is the lackadaisical quantum walk. The\nlackadaisical quantum walk is an algorithm developed to search two-dimensional\ngrids whose vertices have a self-loop of weight $l$. In this paper, we address\nseveral issues related to the application of the lackadaisical quantum walk to\nsuccessfully search for multiple solutions on grids. Firstly, we show that only\none of the two stopping conditions found in the literature is suitable for\nsimulations. We also demonstrate that the final success probability depends on\nthe space density of solutions and the relative distance between solutions.\nFurthermore, this work generalizes the lackadaisical quantum walk to search for\nmultiple solutions on grids of arbitrary dimensions. In addition, we propose an\noptimal adjustment of the self-loop weight $l$ for such scenarios of arbitrary\ndimensions. It turns out the other fits of $l$ found in the literature are\nparticular cases. Finally, we observe a two-to-one relation between the steps\nof the lackadaisical quantum walk and the ones of Grover's algorithm, which\nrequires modifications in the stopping condition. In conclusion, this work\ndeals with practical issues one should consider when applying the lackadaisical\nquantum walk, besides expanding the technique to a wider range of search\nproblems.",
    "descriptor": "\nComments: Extended version of the conference paper available at this https URL . 21 pages, 6 figures\n",
    "authors": [
      "Jonathan H. A. de Carvalho",
      "Luciano S. de Souza",
      "Fernando M. de Paula Neto",
      "Tiago A. E. Ferreira"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2106.06274"
  },
  {
    "id": "arXiv:2106.06279",
    "title": "Model-Free Learning for Two-Player Zero-Sum Partially Observable Markov  Games with Perfect Recall",
    "abstract": "We study the problem of learning a Nash equilibrium (NE) in an imperfect\ninformation game (IIG) through self-play. Precisely, we focus on two-player,\nzero-sum, episodic, tabular IIG under the perfect-recall assumption where the\nonly feedback is realizations of the game (bandit feedback). In particular, the\ndynamic of the IIG is not known -- we can only access it by sampling or\ninteracting with a game simulator. For this learning setting, we provide the\nImplicit Exploration Online Mirror Descent (IXOMD) algorithm. It is a\nmodel-free algorithm with a high-probability bound on the convergence rate to\nthe NE of order $1/\\sqrt{T}$ where $T$ is the number of played games. Moreover,\nIXOMD is computationally efficient as it needs to perform the updates only\nalong the sampled trajectory.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Tadashi Kozuno",
      "Pierre M\u00e9nard",
      "R\u00e9mi Munos",
      "Michal Valko"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06279"
  },
  {
    "id": "arXiv:2106.06300",
    "title": "DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm",
    "abstract": "Performing reliable Bayesian inference on a big data scale is becoming a\nkeystone in the modern era of machine learning. A workhorse class of methods to\nachieve this task are Markov chain Monte Carlo (MCMC) algorithms and their\ndesign to handle distributed datasets has been the subject of many works.\nHowever, existing methods are not completely either reliable or computationally\nefficient. In this paper, we propose to fill this gap in the case where the\ndataset is partitioned and stored on computing nodes within a cluster under a\nmaster/slaves architecture. We derive a user-friendly centralised distributed\nMCMC algorithm with provable scaling in high-dimensional settings. We\nillustrate the relevance of the proposed methodology on both synthetic and real\ndata experiments.",
    "descriptor": "\nComments: 77 pages. Accepted for publication at ICML 2021, to appear\n",
    "authors": [
      "Vincent Plassier",
      "Maxime Vono",
      "Alain Durmus",
      "Eric Moulines"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.06300"
  },
  {
    "id": "arXiv:2106.06347",
    "title": "Asymptotic Properties of Monte Carlo Methods in Elliptic PDE-Constrained  Optimization under Uncertainty",
    "abstract": "Monte Carlo approximations for random linear elliptic PDE constrained\noptimization problems are studied. We use empirical process theory to obtain\nbest possible mean convergence rates $O(n^{-\\frac{1}{2}})$ for optimal values\nand solutions, and a central limit theorem for optimal values. The latter\nallows to determine asymptotically consistent confidence intervals by using\nresampling techniques.",
    "descriptor": "",
    "authors": [
      "Werner R\u00f6misch",
      "Thomas M. Surowiec"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.06347"
  },
  {
    "id": "arXiv:2106.06389",
    "title": "An Empirical Study of DeFi Liquidations: Incentives, Risks, and  Instabilities",
    "abstract": "Financial speculators often seek to increase their potential gains with\nleverage. Debt is a popular form of leverage, and with over 39.88B USD of total\nvalue locked (TVL), the Decentralized Finance (DeFi) lending markets are\nthriving. Debts, however, entail the risks of liquidation, the process of\nselling the debt collateral at a discount to liquidators. Nevertheless, few\nquantitative insights are known about the existing liquidation mechanisms.\nIn this paper, to the best of our knowledge, we are the first to study the\nbreadth of the borrowing and lending markets of the Ethereum DeFi ecosystem. We\nfocus on Aave, Compound, MakerDAO, and dYdX, which collectively represent over\n85% of the lending market on Ethereum. Given extensive liquidation data\nmeasurements and insights, we systematize the prevalent liquidation mechanisms\nand are the first to provide a methodology to compare them objectively. We find\nthat the existing liquidation designs well incentivize liquidators but sell\nexcessive amounts of discounted collateral at the borrowers' expenses. We\nmeasure various risks that liquidation participants are exposed to and quantify\nthe instabilities of existing lending protocols. Moreover, we propose an\noptimal strategy that allows liquidators to increase their liquidation profit,\nwhich may aggravate the loss of borrowers.",
    "descriptor": "",
    "authors": [
      "Kaihua Qin",
      "Liyi Zhou",
      "Pablo Gamito",
      "Philipp Jovanovic",
      "Arthur Gervais"
    ],
    "subjectives": [
      "General Finance (q-fin.GN)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06389"
  },
  {
    "id": "arXiv:2106.06406",
    "title": "PriorGrad: Improving Conditional Denoising Diffusion Models with  Data-Driven Adaptive Prior",
    "abstract": "Denoising diffusion probabilistic models have been recently proposed to\ngenerate high-quality samples by estimating the gradient of the data density.\nThe framework assumes the prior noise as a standard Gaussian distribution,\nwhereas the corresponding data distribution may be more complicated than the\nstandard Gaussian distribution, which potentially introduces inefficiency in\ndenoising the prior noise into the data sample because of the discrepancy\nbetween the data and the prior. In this paper, we propose PriorGrad to improve\nthe efficiency of the conditional diffusion model (for example, a vocoder using\na mel-spectrogram as the condition) by applying an adaptive prior derived from\nthe data statistics based on the conditional information. We formulate the\ntraining and sampling procedures of PriorGrad and demonstrate the advantages of\nan adaptive prior through a theoretical analysis. Focusing on the audio domain,\nwe consider the recently proposed diffusion-based audio generative models based\non both the spectral and time domains and show that PriorGrad achieves a faster\nconvergence leading to data and parameter efficiency and improved quality, and\nthereby demonstrating the efficiency of a data-driven adaptive prior.",
    "descriptor": "\nComments: 16 pages, 5 figures, 7 tables. Audio samples: this https URL\n",
    "authors": [
      "Sang-gil Lee",
      "Heeseung Kim",
      "Chaehun Shin",
      "Xu Tan",
      "Chang Liu",
      "Qi Meng",
      "Tao Qin",
      "Wei Chen",
      "Sungroh Yoon",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06406"
  },
  {
    "id": "arXiv:2106.06409",
    "title": "Real-time thermoacoustic data assimilation",
    "abstract": "Low-order thermoacoustic models are qualitatively correct, but they are\ntypically quantitatively inaccurate. We propose a time-domain method to make\nqualitatively low-order models quantitatively (more) accurate. First, we\ndevelop a Bayesian data assimilation method for a low-order model to self-adapt\nand self-correct any time that reference data, for example from experiments,\nbecomes available. Second, we apply the methodology to infer the thermoacoustic\nstates, heat release parameters, and model errors on the fly without storing\ndata (real-time). Third, we analyse the performance of the data assimilation\nwith synthetic data and interpret the results physically. We apply the data\nassimilation algorithm to all nonlinear thermoacoustic regimes, from limit\ncycles to chaos, in which acoustic pressure measurements from microphones are\nassimilated. Fourth, we propose practical rules for thermoacoustic data\nassimilation based on physical observations on the dynamics. An increase,\nreject, inflate strategy is proposed to deal with the rich nonlinear behaviour,\nthe bifurcations of which are sensitive to small perturbations to the\nparameters. We show that (i) the correct acoustic pressure and parameters can\nbe accurately inferred; (ii) the learning is robust because it can tackle large\nuncertainties in the observations (up to 50% the mean values); (iii) the\nuncertainty of the prediction and parameters is naturally part of the output;\nand (iv) both the time-accurate solution and statistics can be successfully\ninferred. Physical time scales for assimilation are proposed in non-chaotic\nregimes (with the Nyquist-Shannon criterion) and in chaotic regimes (with the\nLyapunov time). Data assimilation opens up new possibility for real--time\nprediction of thermoacoustics by synergistically combining physical knowledge\nand data.",
    "descriptor": "\nComments: 42 pages, 21 figures\n",
    "authors": [
      "Andrea N\u00f3voa",
      "Luca Magri"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.06409"
  },
  {
    "id": "arXiv:2106.06429",
    "title": "Designing predefined-time differentiators with bounded time-varying  gains",
    "abstract": "There is an increasing interest in designing differentiators, which converge\nexactly before a prespecified time regardless of the initial conditions, i.e.,\nwhich are fixed-time convergent with a predefined Upper Bound of their Settling\nTime (UBST), due to their ability to solve estimation and control problems with\ntime constraints. However, for the class of signals with a known bound of their\n$(n+1)$-th time derivative, the existing design methodologies are either only\navailable for first-order differentiators, yielding a very conservative UBST,\nor result in gains that tend to infinity at the convergence time. Here, we\nintroduce a new methodology based on time-varying gains to design\narbitrary-order exact differentiators with a predefined UBST. This UBST is a\npriori set as one parameter of the algorithm. Our approach guarantees that the\nUBST can be set arbitrarily tight, and we also provide sufficient conditions to\nobtain exact convergence while maintaining bounded time-varying gains.\nAdditionally, we provide necessary and sufficient conditions such that our\napproach yields error dynamics with a uniformly Lyapunov stable equilibrium.\nOur results show how time-varying gains offer a general and flexible\nmethodology to design algorithms with a predefined UBST.",
    "descriptor": "",
    "authors": [
      "Rodrigo Aldana-L\u00f3pez",
      "Richard Seeber",
      "David G\u00f3mez-Guti\u00e9rrez",
      "Marco Tulio Angulo",
      "Michael Defoort"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06429"
  },
  {
    "id": "arXiv:2106.06430",
    "title": "Continuous Herded Gibbs Sampling",
    "abstract": "Herding is a technique to sequentially generate deterministic samples from a\nprobability distribution. In this work, we propose a continuous herded Gibbs\nsampler, that combines kernel herding on continuous densities with Gibbs\nsampling. Our algorithm allows for deterministically sampling from\nhigh-dimensional multivariate probability densities, without directly sampling\nfrom the joint density. Experiments with Gaussian mixture densities indicate\nthat the L2 error decreases similarly to kernel herding, while the computation\ntime is significantly lower, i.e., linear in the number of dimensions.",
    "descriptor": "\nComments: 6 pages, 7 figures\n",
    "authors": [
      "Laura M. Wolf",
      "Marcus Baum"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.06430"
  },
  {
    "id": "arXiv:2106.06510",
    "title": "Measuring the sensitivity of Gaussian processes to kernel choice",
    "abstract": "Gaussian processes (GPs) are used to make medical and scientific decisions,\nincluding in cardiac care and monitoring of carbon dioxide emissions. But the\nchoice of GP kernel is often somewhat arbitrary. In particular, uncountably\nmany kernels typically align with qualitative prior knowledge (e.g. function\nsmoothness or stationarity). But in practice, data analysts choose among a\nhandful of convenient standard kernels (e.g. squared exponential). In the\npresent work, we ask: Would decisions made with a GP differ under other,\nqualitatively interchangeable kernels? We show how to formulate this\nsensitivity analysis as a constrained optimization problem over a\nfinite-dimensional space. We can then use standard optimizers to identify\nsubstantive changes in relevant decisions made with a GP. We demonstrate in\nboth synthetic and real-world examples that decisions made with a GP can\nexhibit substantial sensitivity to kernel choice, even when prior draws are\nqualitatively interchangeable to a user.",
    "descriptor": "",
    "authors": [
      "William T. Stephenson",
      "Soumya Ghosh",
      "Tin D. Nguyen",
      "Mikhail Yurochkin",
      "Sameer K. Deshpande",
      "Tamara Broderick"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.06510"
  },
  {
    "id": "arXiv:2106.06513",
    "title": "Learning the optimal regularizer for inverse problems",
    "abstract": "In this work, we consider the linear inverse problem $y=Ax+\\epsilon$, where\n$A\\colon X\\to Y$ is a known linear operator between the separable Hilbert\nspaces $X$ and $Y$, $x$ is a random variable in $X$ and $\\epsilon$ is a\nzero-mean random process in $Y$. This setting covers several inverse problems\nin imaging including denoising, deblurring, and X-ray tomography. Within the\nclassical framework of regularization, we focus on the case where the\nregularization functional is not given a priori but learned from data. Our\nfirst result is a characterization of the optimal generalized Tikhonov\nregularizer, with respect to the mean squared error. We find that it is\ncompletely independent of the forward operator $A$ and depends only on the mean\nand covariance of $x$. Then, we consider the problem of learning the\nregularizer from a finite training set in two different frameworks: one\nsupervised, based on samples of both $x$ and $y$, and one unsupervised, based\nonly on samples of $x$. In both cases, we prove generalization bounds, under\nsome weak assumptions on the distribution of $x$ and $\\epsilon$, including the\ncase of sub-Gaussian variables. Our bounds hold in infinite-dimensional spaces,\nthereby showing that finer and finer discretizations do not make this learning\nproblem harder. The results are validated through numerical simulations.",
    "descriptor": "",
    "authors": [
      "Giovanni S. Alberti",
      "Ernesto De Vito",
      "Matti Lassas",
      "Luca Ratti",
      "Matteo Santacesaria"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.06513"
  },
  {
    "id": "arXiv:2106.06523",
    "title": "Recovery of Meteorites Using an Autonomous Drone and Machine Learning",
    "abstract": "The recovery of freshly fallen meteorites from tracked and triangulated\nmeteors is critical to determining their source asteroid families. However,\nlocating meteorite fragments in strewn fields remains a challenge with very few\nmeteorites being recovered from the meteors triangulated in past and ongoing\nmeteor camera networks. We examined if locating meteorites can be automated\nusing machine learning and an autonomous drone. Drones can be programmed to fly\na grid search pattern and take systematic pictures of the ground over a large\nsurvey area. Those images can be analyzed using a machine learning classifier\nto identify meteorites in the field among many other features. Here, we\ndescribe a proof-of-concept meteorite classifier that deploys off-line a\ncombination of different convolution neural networks to recognize meteorites\nfrom images taken by drones in the field. The system was implemented in a\nconceptual drone setup and tested in the suspected strewn field of a recent\nmeteorite fall near Walker Lake, Nevada.",
    "descriptor": "\nComments: 16 pages, 9 Figures\n",
    "authors": [
      "Robert I. Citron",
      "Peter Jenniskens",
      "Christopher Watkins",
      "Sravanthi Sinha",
      "Amar Shah",
      "Chedy Raissi",
      "Hadrien Devillepoix",
      "Jim Albers"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06523"
  },
  {
    "id": "arXiv:1806.06142",
    "title": "Possibility results for graph clustering: A novel consistency axiom",
    "abstract": "Comments: Minor changes, 'Related work' and 'Overview of results' sections added to the introduction, small rewrite of the conclusions",
    "descriptor": "\nComments: Minor changes, 'Related work' and 'Overview of results' sections added to the introduction, small rewrite of the conclusions\n",
    "authors": [
      "Fabio Strazzeri",
      "Rub\u00e9n J. S\u00e1nchez-Garc\u00eda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1806.06142"
  },
  {
    "id": "arXiv:1809.06933",
    "title": "Identifying the lights position in photometric stereo under unknown  lighting",
    "abstract": "Comments: new version",
    "descriptor": "\nComments: new version\n",
    "authors": [
      "A. Concas",
      "R. Dess\u00ec",
      "C. Fenu",
      "G. Rodriguez",
      "M. Vanzi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1809.06933"
  },
  {
    "id": "arXiv:1904.03116",
    "title": "Fast Weakly Supervised Action Segmentation Using Mutual Consistency",
    "abstract": "Comments: Accepted for publication at TPAMI (IEEE Transactions on Pattern Analysis and Machine Intelligence) in 2021. First two authors contributed equally",
    "descriptor": "\nComments: Accepted for publication at TPAMI (IEEE Transactions on Pattern Analysis and Machine Intelligence) in 2021. First two authors contributed equally\n",
    "authors": [
      "Yaser Souri",
      "Mohsen Fayyaz",
      "Luca Minciullo",
      "Gianpiero Francesca",
      "Juergen Gall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1904.03116"
  },
  {
    "id": "arXiv:1904.12767",
    "title": "Local non-Bayesian social learning with stubborn agents",
    "abstract": "Local non-Bayesian social learning with stubborn agents",
    "descriptor": "",
    "authors": [
      "Daniel Vial",
      "Vijay Subramanian"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/1904.12767"
  },
  {
    "id": "arXiv:1905.12346",
    "title": "Nystr\u00f6m landmark sampling and regularized Christoffel functions",
    "abstract": "Comments: Improved presentation. Typos corrected",
    "descriptor": "\nComments: Improved presentation. Typos corrected\n",
    "authors": [
      "Micha\u00ebl Fanuel",
      "Joachim Schreurs",
      "Johan A.K. Suykens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1905.12346"
  },
  {
    "id": "arXiv:1908.01961",
    "title": "Real-Time Global Illumination Decomposition of Videos",
    "abstract": "Real-Time Global Illumination Decomposition of Videos",
    "descriptor": "",
    "authors": [
      "Abhimitra Meka",
      "Mohammad Shafiei",
      "Michael Zollhoefer",
      "Christian Richardt",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1908.01961"
  },
  {
    "id": "arXiv:1909.04810",
    "title": "Antipodal Robotic Grasping using Generative Residual Convolutional  Neural Network",
    "abstract": "Comments: 8 pages, 5 figures, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020",
    "descriptor": "\nComments: 8 pages, 5 figures, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020\n",
    "authors": [
      "Sulabh Kumra",
      "Shirin Joshi",
      "Ferat Sahin"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1909.04810"
  },
  {
    "id": "arXiv:1909.11294",
    "title": "Hierarchical Probabilistic Model for Blind Source Separation via  Legendre Transformation",
    "abstract": "Comments: 13 pages, 7 figures, UAI2021",
    "descriptor": "\nComments: 13 pages, 7 figures, UAI2021\n",
    "authors": [
      "Simon Luo",
      "Lamiae Azizi",
      "Mahito Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1909.11294"
  },
  {
    "id": "arXiv:2001.02811",
    "title": "Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for  Addressing Value Estimation Errors",
    "abstract": "Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for  Addressing Value Estimation Errors",
    "descriptor": "",
    "authors": [
      "Jingliang Duan",
      "Yang Guan",
      "Shengbo Eben Li",
      "Yangang Ren",
      "Bo Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2001.02811"
  },
  {
    "id": "arXiv:2001.03040",
    "title": "Deep Network Approximation for Smooth Functions",
    "abstract": "Deep Network Approximation for Smooth Functions",
    "descriptor": "",
    "authors": [
      "Jianfeng Lu",
      "Zuowei Shen",
      "Haizhao Yang",
      "Shijun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2001.03040"
  },
  {
    "id": "arXiv:2002.03205",
    "title": "Asymptotically Optimal Control of a Centralized Dynamic Matching Market  with General Utilities",
    "abstract": "Comments: 81 pages",
    "descriptor": "\nComments: 81 pages\n",
    "authors": [
      "Jose H. Blanchet",
      "Martin I. Reiman",
      "Viragh Shah",
      "Lawrence M. Wein",
      "Linjia Wu"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Social and Information Networks (cs.SI)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2002.03205"
  },
  {
    "id": "arXiv:2002.04137",
    "title": "On Robust Mean Estimation under Coordinate-level Corruption",
    "abstract": "Comments: 21 pages, 5 figures",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Zifan Liu",
      "Jongho Park",
      "Theodoros Rekatsinas",
      "Christos Tzamos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.04137"
  },
  {
    "id": "arXiv:2002.06284",
    "title": "Leveraging Coupled BBR and Adaptive Packet Scheduling to Boost MPTCP",
    "abstract": "Leveraging Coupled BBR and Adaptive Packet Scheduling to Boost MPTCP",
    "descriptor": "",
    "authors": [
      "Jiangping Han",
      "Yitao Xing",
      "Kaiping Xue",
      "David S.L. Wei",
      "Guoliang Xue",
      "Peilin Hong"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2002.06284"
  },
  {
    "id": "arXiv:2002.09773",
    "title": "Revealing the Structure of Deep Neural Networks via Convex Duality",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Tolga Ergen",
      "Mert Pilanci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.09773"
  },
  {
    "id": "arXiv:2002.11318",
    "title": "Can we have it all? On the Trade-off between Spatial and Adversarial  Robustness of Neural Networks",
    "abstract": "Comments: Preliminary version consisting early experimental results was presented in ICML 2018 Workshop on \"Towards learning with limited labels: Equivariance, Invariance,and Beyond\" as \"Understanding Adversarial Robustness of Symmetric Networks\"",
    "descriptor": "\nComments: Preliminary version consisting early experimental results was presented in ICML 2018 Workshop on \"Towards learning with limited labels: Equivariance, Invariance,and Beyond\" as \"Understanding Adversarial Robustness of Symmetric Networks\"\n",
    "authors": [
      "Sandesh Kamath",
      "Amit Deshpande",
      "K V Subrahmanyam",
      "Vineeth N Balasubramanian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.11318"
  },
  {
    "id": "arXiv:2003.11679",
    "title": "An Accelerated Surface Integral Equation Method for the Electromagnetic  Modeling of Dielectric and Lossy Objects of Arbitrary Conductivity",
    "abstract": "Comments: IEEE Transactions on Antennas and Propagation",
    "descriptor": "\nComments: IEEE Transactions on Antennas and Propagation\n",
    "authors": [
      "Shashwat Sharma",
      "Piero Triverio"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2003.11679"
  },
  {
    "id": "arXiv:2005.04916",
    "title": "A Logical Characterization of Constant-Depth Circuits over the Reals",
    "abstract": "Comments: 24 pages, submitted to WoLLIC 2021",
    "descriptor": "\nComments: 24 pages, submitted to WoLLIC 2021\n",
    "authors": [
      "Timon Barlag",
      "Heribert Vollmer"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2005.04916"
  },
  {
    "id": "arXiv:2005.13300",
    "title": "Scalable Polyhedral Verification of Recurrent Neural Networks",
    "abstract": "Comments: Published in CAV 2021",
    "descriptor": "\nComments: Published in CAV 2021\n",
    "authors": [
      "Wonryong Ryou",
      "Jiayu Chen",
      "Mislav Balunovic",
      "Gagandeep Singh",
      "Andrei Dan",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.13300"
  },
  {
    "id": "arXiv:2006.01241",
    "title": "Constructing the Field of Values of Decomposable and General Square  Matrices",
    "abstract": "Constructing the Field of Values of Decomposable and General Square  Matrices",
    "descriptor": "",
    "authors": [
      "Frank Uhlig"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2006.01241"
  },
  {
    "id": "arXiv:2006.03204",
    "title": "Black-box Explanation of Object Detectors via Saliency Maps",
    "abstract": "Comments: CVPR 2021 (oral). Project page this https URL",
    "descriptor": "\nComments: CVPR 2021 (oral). Project page this https URL\n",
    "authors": [
      "Vitali Petsiuk",
      "Rajiv Jain",
      "Varun Manjunatha",
      "Vlad I. Morariu",
      "Ashutosh Mehra",
      "Vicente Ordonez",
      "Kate Saenko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.03204"
  },
  {
    "id": "arXiv:2006.03268",
    "title": "A Simple Approach to Increase the Maximum Allowable Transmission  Interval",
    "abstract": "A Simple Approach to Increase the Maximum Allowable Transmission  Interval",
    "descriptor": "",
    "authors": [
      "Michael Hertneck",
      "Frank Allg\u00f6wer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2006.03268"
  },
  {
    "id": "arXiv:2006.03573",
    "title": "Exploration-Exploitation Motivated Variational Auto-Encoder for  Recommender Systems",
    "abstract": "Comments: 9 pages, 6 figures",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Yizi Zhang",
      "Meimei Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.03573"
  },
  {
    "id": "arXiv:2006.04647",
    "title": "Neural Architecture Search without Training",
    "abstract": "Comments: Accepted at ICML 2021 for a long presentation",
    "descriptor": "\nComments: Accepted at ICML 2021 for a long presentation\n",
    "authors": [
      "Joseph Mellor",
      "Jack Turner",
      "Amos Storkey",
      "Elliot J. Crowley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.04647"
  },
  {
    "id": "arXiv:2006.04969",
    "title": "Scalability in Computing and Robotics",
    "abstract": "Comments: 33 pages, 8 figures",
    "descriptor": "\nComments: 33 pages, 8 figures\n",
    "authors": [
      "Heiko Hamann",
      "Andreagiovanni Reina"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Performance (cs.PF)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2006.04969"
  },
  {
    "id": "arXiv:2006.04988",
    "title": "Object Segmentation Without Labels with Large-Scale Generative Models",
    "abstract": "Object Segmentation Without Labels with Large-Scale Generative Models",
    "descriptor": "",
    "authors": [
      "Andrey Voynov",
      "Stanislav Morozov",
      "Artem Babenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.04988"
  },
  {
    "id": "arXiv:2006.06077",
    "title": "S-semantics -- an example",
    "abstract": "Comments: 15 pages, 1 figure. This version -- some errors corrected + small modifications",
    "descriptor": "\nComments: 15 pages, 1 figure. This version -- some errors corrected + small modifications\n",
    "authors": [
      "W\u0142odzimierz Drabent"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2006.06077"
  },
  {
    "id": "arXiv:2006.07869",
    "title": "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in  Cooperative Tasks",
    "abstract": "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in  Cooperative Tasks",
    "descriptor": "",
    "authors": [
      "Georgios Papoudakis",
      "Filippos Christianos",
      "Lukas Sch\u00e4fer",
      "Stefano V. Albrecht"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.07869"
  },
  {
    "id": "arXiv:2006.08085",
    "title": "Optimal Complexity in Decentralized Training",
    "abstract": "Optimal Complexity in Decentralized Training",
    "descriptor": "",
    "authors": [
      "Yucheng Lu",
      "Christopher De Sa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.08085"
  },
  {
    "id": "arXiv:2006.09500",
    "title": "Logic of Machine Learning",
    "abstract": "Logic of Machine Learning",
    "descriptor": "",
    "authors": [
      "Marina Sapir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.09500"
  },
  {
    "id": "arXiv:2006.10836",
    "title": "An Integer Linear Programming Framework for Mining Constraints from Data",
    "abstract": "Comments: 13 pages, published in ICML2021",
    "descriptor": "\nComments: 13 pages, published in ICML2021\n",
    "authors": [
      "Tao Meng",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.10836"
  },
  {
    "id": "arXiv:2006.12242",
    "title": "Exploiting topology awareness for routing in LEO satellite  constellations",
    "abstract": "Comments: Submitted for publication at IEEE GLOBECOM 2021",
    "descriptor": "\nComments: Submitted for publication at IEEE GLOBECOM 2021\n",
    "authors": [
      "Jonas W. Rabjerg",
      "Israel Leyva-Mayorga",
      "Beatriz Soret"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2006.12242"
  },
  {
    "id": "arXiv:2006.12297",
    "title": "Optimal Rates for Averaged Stochastic Gradient Descent under Neural  Tangent Kernel Regime",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Atsushi Nitanda",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.12297"
  },
  {
    "id": "arXiv:2006.14425",
    "title": "Strengthening the Baillie-PSW primality test",
    "abstract": "Comments: 25 pages",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Robert Baillie",
      "Andrew Fiori",
      "Samuel S. Wagstaff Jr."
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2006.14425"
  },
  {
    "id": "arXiv:2006.15327",
    "title": "Compositional Video Synthesis with Action Graphs",
    "abstract": "Comments: ICML 2021 Camera Ready",
    "descriptor": "\nComments: ICML 2021 Camera Ready\n",
    "authors": [
      "Amir Bar",
      "Roei Herzig",
      "Xiaolong Wang",
      "Anna Rohrbach",
      "Gal Chechik",
      "Trevor Darrell",
      "Amir Globerson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.15327"
  },
  {
    "id": "arXiv:2006.16495",
    "title": "Guarantees for Tuning the Step Size using a Learning-to-Learn Approach",
    "abstract": "Comments: ICML 2021. Added proof sketch",
    "descriptor": "\nComments: ICML 2021. Added proof sketch\n",
    "authors": [
      "Xiang Wang",
      "Shuai Yuan",
      "Chenwei Wu",
      "Rong Ge"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.16495"
  },
  {
    "id": "arXiv:2006.16617",
    "title": "Statistical Mechanical Analysis of Neural Network Pruning",
    "abstract": "Comments: Authors Ankani Chattoraj and Boyu Zhang made an equal contribution",
    "descriptor": "\nComments: Authors Ankani Chattoraj and Boyu Zhang made an equal contribution\n",
    "authors": [
      "Rupam Acharyya",
      "Ankani Chattoraj",
      "Boyu Zhang",
      "Shouman Das",
      "Daniel Stefankovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.16617"
  },
  {
    "id": "arXiv:2007.00903",
    "title": "Coordinate-wise Median: Not Bad, Not Bad, Pretty Good",
    "abstract": "Comments: 24 pages",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Sumit Goel",
      "Wade Hann-Caruthers"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2007.00903"
  },
  {
    "id": "arXiv:2007.03893",
    "title": "Multi-Resolution Beta-Divergence NMF for Blind Spectral Unmixing",
    "abstract": "Comments: v2: We have significantly modified Sections I and II to better introduce the problem, and clarify our contributions",
    "descriptor": "\nComments: v2: We have significantly modified Sections I and II to better introduce the problem, and clarify our contributions\n",
    "authors": [
      "Valentin Leplat",
      "Nicolas Gillis",
      "C\u00e9dric F\u00e9votte"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2007.03893"
  },
  {
    "id": "arXiv:2007.06192",
    "title": "Probabilistic bounds on neuron death in deep rectifier networks",
    "abstract": "Probabilistic bounds on neuron death in deep rectifier networks",
    "descriptor": "",
    "authors": [
      "Blaine Rister",
      "Daniel L. Rubin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.06192"
  },
  {
    "id": "arXiv:2007.07210",
    "title": "Simple and Efficient Hard Label Black-box Adversarial Attacks in Low  Query Budget Regimes",
    "abstract": "Comments: Accepted at KDD 2021. arXiv admin note: substantial text overlap with arXiv:1909.13857",
    "descriptor": "\nComments: Accepted at KDD 2021. arXiv admin note: substantial text overlap with arXiv:1909.13857\n",
    "authors": [
      "Satya Narayan Shukla",
      "Anit Kumar Sahu",
      "Devin Willmott",
      "J. Zico Kolter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.07210"
  },
  {
    "id": "arXiv:2007.07878",
    "title": "Quantifying and Reducing Bias in Maximum Likelihood Estimation of  Structured Anomalies",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Uthsav Chitra",
      "Kimberly Ding",
      "Jasper C.H. Lee",
      "Benjamin J. Raphael"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.07878"
  },
  {
    "id": "arXiv:2007.11041",
    "title": "Non-asymptotic moment bounds for random variables rounded to  non-uniformly spaced sets",
    "abstract": "Comments: This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1762114",
    "descriptor": "\nComments: This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1762114\n",
    "authors": [
      "Tyler Chen"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2007.11041"
  },
  {
    "id": "arXiv:2008.07648",
    "title": "Nonparametric Learning of Two-Layer ReLU Residual Units",
    "abstract": "Nonparametric Learning of Two-Layer ReLU Residual Units",
    "descriptor": "",
    "authors": [
      "Zhunxuan Wang",
      "Linyun He",
      "Chunchuan Lyu",
      "Shay B. Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.07648"
  },
  {
    "id": "arXiv:2008.09610",
    "title": "Implementing backjumping by throw/1 and catch/3 of Prolog",
    "abstract": "Comments: 7 pages. This version - an extension (Approach 1a)",
    "descriptor": "\nComments: 7 pages. This version - an extension (Approach 1a)\n",
    "authors": [
      "W\u0142odzimierz Drabent"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2008.09610"
  },
  {
    "id": "arXiv:2009.00100",
    "title": "Online Multi-Object Tracking and Segmentation with GMPHD Filter and  Mask-based Affinity Fusion",
    "abstract": "Online Multi-Object Tracking and Segmentation with GMPHD Filter and  Mask-based Affinity Fusion",
    "descriptor": "",
    "authors": [
      "Young-min Song",
      "Young-chul Yoon",
      "Kwangjin Yoon",
      "Moongu Jeon",
      "Seong-Whan Lee",
      "Witold Pedrycz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2009.00100"
  },
  {
    "id": "arXiv:2009.01133",
    "title": "Bound-preserving flux limiting for high-order explicit Runge-Kutta time  discretizations of hyperbolic conservation laws",
    "abstract": "Bound-preserving flux limiting for high-order explicit Runge-Kutta time  discretizations of hyperbolic conservation laws",
    "descriptor": "",
    "authors": [
      "Dmitri Kuzmin",
      "Manuel Quezada de Luna",
      "David I. Ketcheson",
      "Johanna Gr\u00fcll"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2009.01133"
  },
  {
    "id": "arXiv:2009.03294",
    "title": "GraphNorm: A Principled Approach to Accelerating Graph Neural Network  Training",
    "abstract": "Comments: ICML 2021, Code: this https URL",
    "descriptor": "\nComments: ICML 2021, Code: this https URL\n",
    "authors": [
      "Tianle Cai",
      "Shengjie Luo",
      "Keyulu Xu",
      "Di He",
      "Tie-Yan Liu",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.03294"
  },
  {
    "id": "arXiv:2009.03434",
    "title": "A Fast Parametric Ellipse Algorithm",
    "abstract": "Comments: 32 pages, 7 figures, C++ source code",
    "descriptor": "\nComments: 32 pages, 7 figures, C++ source code\n",
    "authors": [
      "Jerry R. Van Aken"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2009.03434"
  },
  {
    "id": "arXiv:2009.05839",
    "title": "On topology optimization of design-dependent pressure-loaded  three-dimensional structures and compliant mechanisms",
    "abstract": "Comments: 25 pages, 17 figures",
    "descriptor": "\nComments: 25 pages, 17 figures\n",
    "authors": [
      "Prabhat Kumar",
      "Matthijs Langelaar"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2009.05839"
  },
  {
    "id": "arXiv:2009.07643",
    "title": "Partial MDS Codes with Regeneration",
    "abstract": "Comments: Extended version of arXiv:2001.04711",
    "descriptor": "\nComments: Extended version of arXiv:2001.04711\n",
    "authors": [
      "Lukas Holzbaur",
      "Sven Puchinger",
      "Eitan Yaakobi",
      "Antonia Wachter-Zeh"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2009.07643"
  },
  {
    "id": "arXiv:2010.03485",
    "title": "SPPL: Probabilistic Programming with Fast Exact Symbolic Inference",
    "abstract": "SPPL: Probabilistic Programming with Fast Exact Symbolic Inference",
    "descriptor": "",
    "authors": [
      "Feras A. Saad",
      "Martin C. Rinard",
      "Vikash K. Mansinghka"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Machine Learning (cs.LG)",
      "Symbolic Computation (cs.SC)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.03485"
  },
  {
    "id": "arXiv:2010.04157",
    "title": "Online and Distribution-Free Robustness: Regression and Contextual  Bandits with Huber Contamination",
    "abstract": "Comments: 66 pages, 1 figure, v3: refined exposition and improved rates",
    "descriptor": "\nComments: 66 pages, 1 figure, v3: refined exposition and improved rates\n",
    "authors": [
      "Sitan Chen",
      "Frederic Koehler",
      "Ankur Moitra",
      "Morris Yau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.04157"
  },
  {
    "id": "arXiv:2010.05501",
    "title": "BiPointNet: Binary Neural Network for Point Clouds",
    "abstract": "BiPointNet: Binary Neural Network for Point Clouds",
    "descriptor": "",
    "authors": [
      "Haotong Qin",
      "Zhongang Cai",
      "Mingyuan Zhang",
      "Yifu Ding",
      "Haiyu Zhao",
      "Shuai Yi",
      "Xianglong Liu",
      "Hao Su"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.05501"
  },
  {
    "id": "arXiv:2010.05690",
    "title": "COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis",
    "abstract": "Comments: The paper is accepted in CMC (Computers, Materials and Continua)",
    "descriptor": "\nComments: The paper is accepted in CMC (Computers, Materials and Continua)\n",
    "authors": [
      "Lalith Bharadwaj B",
      "Rohit Boddeda",
      "Sai Vardhan K",
      "Madhu G"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2010.05690"
  },
  {
    "id": "arXiv:2010.07249",
    "title": "Environment Inference for Invariant Learning",
    "abstract": "Environment Inference for Invariant Learning",
    "descriptor": "",
    "authors": [
      "Elliot Creager",
      "J\u00f6rn-Henrik Jacobsen",
      "Richard Zemel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2010.07249"
  },
  {
    "id": "arXiv:2010.07615",
    "title": "Asynchronous \u03b5-Greedy Bayesian Optimisation",
    "abstract": "Comments: Accepted for the 37th conference on Uncertainty in Artificial Intelligence (UAI 2021). 11 pages (main paper) + 22 pages (supplementary material)",
    "descriptor": "\nComments: Accepted for the 37th conference on Uncertainty in Artificial Intelligence (UAI 2021). 11 pages (main paper) + 22 pages (supplementary material)\n",
    "authors": [
      "George De Ath",
      "Richard M. Everson",
      "Jonathan E. Fieldsend"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.07615"
  },
  {
    "id": "arXiv:2010.08844",
    "title": "Finding Physical Adversarial Examples for Autonomous Driving with Fast  and Differentiable Image Compositing",
    "abstract": "Finding Physical Adversarial Examples for Autonomous Driving with Fast  and Differentiable Image Compositing",
    "descriptor": "",
    "authors": [
      "Jinghan Yang",
      "Adith Boloor",
      "Ayan Chakrabarti",
      "Xuan Zhang",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.08844"
  },
  {
    "id": "arXiv:2010.09536",
    "title": "Represent Your Own Policies: Reinforcement Learning with Policy-extended  Value Function Approximator",
    "abstract": "Comments: Preprint version, 33 pages",
    "descriptor": "\nComments: Preprint version, 33 pages\n",
    "authors": [
      "Hongyao Tang",
      "Zhaopeng Meng",
      "Jianye Hao",
      "Chen Chen",
      "Daniel Graves",
      "Dong Li",
      "Hangyu Mao",
      "Wulong Liu",
      "Yaodong Yang",
      "Changmin Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2010.09536"
  },
  {
    "id": "arXiv:2010.09649",
    "title": "Hutch++: Optimal Stochastic Trace Estimation",
    "abstract": "Comments: SIAM Symposium on Simplicity in Algorithms (SOSA21)",
    "descriptor": "\nComments: SIAM Symposium on Simplicity in Algorithms (SOSA21)\n",
    "authors": [
      "Raphael A. Meyer",
      "Cameron Musco",
      "Christopher Musco",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2010.09649"
  },
  {
    "id": "arXiv:2010.11568",
    "title": "Quantile Bandits for Best Arms Identification",
    "abstract": "Comments: Proceedings of the 38th International Conference on Machine Learning, 2021",
    "descriptor": "\nComments: Proceedings of the 38th International Conference on Machine Learning, 2021\n",
    "authors": [
      "Mengyan Zhang",
      "Cheng Soon Ong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.11568"
  },
  {
    "id": "arXiv:2010.11875",
    "title": "Scene-Agnostic Multi-Microphone Speech Dereverberation",
    "abstract": "Scene-Agnostic Multi-Microphone Speech Dereverberation",
    "descriptor": "",
    "authors": [
      "Yochai Yemini",
      "Ethan Fetaya",
      "Haggai Maron",
      "Sharon Gannot"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2010.11875"
  },
  {
    "id": "arXiv:2010.14986",
    "title": "Evaluating Robustness of Predictive Uncertainty Estimation: Are  Dirichlet-based Models Reliable?",
    "abstract": "Comments: Published at ICML 2021",
    "descriptor": "\nComments: Published at ICML 2021\n",
    "authors": [
      "Anna-Kathrin Kopetzki",
      "Bertrand Charpentier",
      "Daniel Z\u00fcgner",
      "Sandhya Giri",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.14986"
  },
  {
    "id": "arXiv:2010.15560",
    "title": "Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel  Segmentation Using a Genetic Algorithm",
    "abstract": "Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel  Segmentation Using a Genetic Algorithm",
    "descriptor": "",
    "authors": [
      "Jiahong Wei",
      "Zhun Fan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.15560"
  },
  {
    "id": "arXiv:2010.15951",
    "title": "Active Sampling Count Sketch (ASCS) for Online Sparse Estimation of a  Trillion Scale Covariance Matrix",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Zhenwei Dai",
      "Aditya Desai",
      "Reinhard Heckel",
      "Anshumali Shrivastava"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2010.15951"
  },
  {
    "id": "arXiv:2011.00344",
    "title": "A Distribution-Dependent Analysis of Meta-Learning",
    "abstract": "Comments: 19 pages, 7 figures",
    "descriptor": "\nComments: 19 pages, 7 figures\n",
    "authors": [
      "Mikhail Konobeev",
      "Ilja Kuzborskij",
      "Csaba Szepesv\u00e1ri"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.00344"
  },
  {
    "id": "arXiv:2011.01668",
    "title": "Towards a Unified Quadrature Framework for Large-Scale Kernel Machines",
    "abstract": "Comments: 17 pages, 9 figures",
    "descriptor": "\nComments: 17 pages, 9 figures\n",
    "authors": [
      "Fanghui Liu",
      "Xiaolin Huang",
      "Yudong Chen",
      "Johan A.K. Suykens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.01668"
  },
  {
    "id": "arXiv:2011.06125",
    "title": "Hurricane Forecasting: A Novel Multimodal Machine Learning Framework",
    "abstract": "Comments: Under revision by the AMS' Weather and Forecasting journal",
    "descriptor": "\nComments: Under revision by the AMS' Weather and Forecasting journal\n",
    "authors": [
      "L\u00e9onard Boussioux",
      "Cynthia Zeng",
      "Th\u00e9o Gu\u00e9nais",
      "Dimitris Bertsimas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2011.06125"
  },
  {
    "id": "arXiv:2011.08097",
    "title": "Faster connectivity in low-rank hypergraphs via expander decomposition",
    "abstract": "Comments: Section 4.1 is new and contains an algorithm for finding the min-cut in a hypergraph with an imbalanced min-cut. When one side of a min-cut has at most $s$ vertices this algorithm finds a min-cut in time $\\tilde{O}_r(s^{6r}p)$. Using this new algorithm as a subroutine in our min-cut algorithm improves the overall runtime",
    "descriptor": "\nComments: Section 4.1 is new and contains an algorithm for finding the min-cut in a hypergraph with an imbalanced min-cut. When one side of a min-cut has at most $s$ vertices this algorithm finds a min-cut in time $\\tilde{O}_r(s^{6r}p)$. Using this new algorithm as a subroutine in our min-cut algorithm improves the overall runtime\n",
    "authors": [
      "Calvin Beideman",
      "Karthekeyan Chandrasekaran",
      "Sagnik Mukhopadhyay",
      "Danupon Nanongkai"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2011.08097"
  },
  {
    "id": "arXiv:2011.09065",
    "title": "Towards Online Monitoring and Data-driven Control: A Study of  Segmentation Algorithms for Laser Powder Bed Fusion Processes",
    "abstract": "Towards Online Monitoring and Data-driven Control: A Study of  Segmentation Algorithms for Laser Powder Bed Fusion Processes",
    "descriptor": "",
    "authors": [
      "Alexander Nettekoven",
      "Scott Fish",
      "Joseph Beaman",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.09065"
  },
  {
    "id": "arXiv:2011.09361",
    "title": "A Knowledge Distillation Ensemble Framework for Predicting Short and  Long-term Hospitalisation Outcomes from Electronic Health Records Data",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Zina M Ibrahim",
      "Daniel Bean",
      "Thomas Searle",
      "Honghan Wu",
      "Anthony Shek",
      "Zeljko Kraljevic",
      "James Galloway",
      "Sam Norton",
      "James T Teo",
      "Richard JB Dobson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2011.09361"
  },
  {
    "id": "arXiv:2011.12249",
    "title": "Generalizing Cross-Document Event Coreference Resolution Across Multiple  Corpora",
    "abstract": "Comments: Accepted at CL Journal",
    "descriptor": "\nComments: Accepted at CL Journal\n",
    "authors": [
      "Michael Bugert",
      "Nils Reimers",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2011.12249"
  },
  {
    "id": "arXiv:2011.13837",
    "title": "A theory of transaction parallelism in blockchains",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1905.04366",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1905.04366\n",
    "authors": [
      "Massimo Bartoletti",
      "Letterio Galletta",
      "Maurizio Murgia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2011.13837"
  },
  {
    "id": "arXiv:2012.01316",
    "title": "Improved Contrastive Divergence Training of Energy Based Models",
    "abstract": "Comments: ICML 2021, Project webpage at this https URL",
    "descriptor": "\nComments: ICML 2021, Project webpage at this https URL\n",
    "authors": [
      "Yilun Du",
      "Shuang Li",
      "Joshua Tenenbaum",
      "Igor Mordatch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01316"
  },
  {
    "id": "arXiv:2012.01557",
    "title": "Value Alignment Verification",
    "abstract": "Comments: In proceedings International Conference on Machine Learning (ICML) 2021",
    "descriptor": "\nComments: In proceedings International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Daniel S. Brown",
      "Jordan Schneider",
      "Anca D. Dragan",
      "Scott Niekum"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01557"
  },
  {
    "id": "arXiv:2012.03310",
    "title": "PAC-Learning for Strategic Classification",
    "abstract": "PAC-Learning for Strategic Classification",
    "descriptor": "",
    "authors": [
      "Ravi Sundaram",
      "Anil Vullikanti",
      "Haifeng Xu",
      "Fan Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.03310"
  },
  {
    "id": "arXiv:2012.03636",
    "title": "Noise and Fluctuation of Finite Learning Rate Stochastic Gradient  Descent",
    "abstract": "Comments: Camera-ready version for the Thirty-eighth International Conference on Machine Learning (ICML 2021). 12 + 14 pages, 6 + 3 figures, 1 + 0 table. *First two authors contributed equally",
    "descriptor": "\nComments: Camera-ready version for the Thirty-eighth International Conference on Machine Learning (ICML 2021). 12 + 14 pages, 6 + 3 figures, 1 + 0 table. *First two authors contributed equally\n",
    "authors": [
      "Kangqiao Liu",
      "Liu Ziyin",
      "Masahito Ueda"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.03636"
  },
  {
    "id": "arXiv:2012.04868",
    "title": "Counting Real Roots in Polynomial-Time for Systems Supported on Circuits",
    "abstract": "Comments: 29 pages, 1 figure, accepted for presentation at MEGA (Effective Methods in Algebraic Geometry) 2021. You can see a recording of my talk at MEGA 2021 (June 9, 2021) at this YouTube link: this https URL",
    "descriptor": "\nComments: 29 pages, 1 figure, accepted for presentation at MEGA (Effective Methods in Algebraic Geometry) 2021. You can see a recording of my talk at MEGA 2021 (June 9, 2021) at this YouTube link: this https URL\n",
    "authors": [
      "J. Maurice Rojas"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Computational Complexity (cs.CC)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2012.04868"
  },
  {
    "id": "arXiv:2012.07551",
    "title": "Towards unsupervised phone and word segmentation using self-supervised  vector-quantized neural networks",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Herman Kamper",
      "Benjamin van Niekerk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2012.07551"
  },
  {
    "id": "arXiv:2012.07641",
    "title": "Best Arm Identification in Graphical Bilinear Bandits",
    "abstract": "Best Arm Identification in Graphical Bilinear Bandits",
    "descriptor": "",
    "authors": [
      "Geovani Rizk",
      "Albert Thomas",
      "Igor Colin",
      "Rida Laraki",
      "Yann Chevaleyre"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.07641"
  },
  {
    "id": "arXiv:2012.08154",
    "title": "Inference of Causal Effects when Control Variables are Unknown",
    "abstract": "Inference of Causal Effects when Control Variables are Unknown",
    "descriptor": "",
    "authors": [
      "Ludvig Hult",
      "Dave Zachariah"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.08154"
  },
  {
    "id": "arXiv:2012.11654",
    "title": "Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for  Deep ReLU Networks",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Quynh Nguyen",
      "Marco Mondelli",
      "Guido Montufar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.11654"
  },
  {
    "id": "arXiv:2012.14193",
    "title": "Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts  Generalization",
    "abstract": "Comments: The last two authors contributed equally. Accepted to the International Conference on Machine Learning 2021",
    "descriptor": "\nComments: The last two authors contributed equally. Accepted to the International Conference on Machine Learning 2021\n",
    "authors": [
      "Stanislaw Jastrzebski",
      "Devansh Arpit",
      "Oliver Astrand",
      "Giancarlo Kerg",
      "Huan Wang",
      "Caiming Xiong",
      "Richard Socher",
      "Kyunghyun Cho",
      "Krzysztof Geras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.14193"
  },
  {
    "id": "arXiv:2012.15477",
    "title": "Particle Dual Averaging: Optimization of Mean Field Neural Networks with  Global Convergence Rate Analysis",
    "abstract": "Comments: 39 pages",
    "descriptor": "\nComments: 39 pages\n",
    "authors": [
      "Atsushi Nitanda",
      "Denny Wu",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.15477"
  },
  {
    "id": "arXiv:2012.15764",
    "title": "Divergence Regulated Encoder Network for Joint Dimensionality Reduction  and Classification",
    "abstract": "Comments: 5 pages, 3 figures",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Joshua Peeples",
      "Sarah Walker",
      "Connor McCurley",
      "Alina Zare",
      "James Keller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.15764"
  },
  {
    "id": "arXiv:2101.00010",
    "title": "UnNatural Language Inference",
    "abstract": "Comments: Accepted at ACL 2021 (Long Paper), 9 pages + Appendix",
    "descriptor": "\nComments: Accepted at ACL 2021 (Long Paper), 9 pages + Appendix\n",
    "authors": [
      "Koustuv Sinha",
      "Prasanna Parthasarathi",
      "Joelle Pineau",
      "Adina Williams"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.00010"
  },
  {
    "id": "arXiv:2101.01350",
    "title": "Towards an efficient approach for the nonconvex $\\ell_p$ ball  projection: algorithm and analysis",
    "abstract": "Comments: This work has been submitted and may be published. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted and may be published. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Xiangyu Yang",
      "Jiashan Wang",
      "Hao Wang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.01350"
  },
  {
    "id": "arXiv:2101.02074",
    "title": "Latency Analysis of ROS2 Multi-Node Systems",
    "abstract": "Latency Analysis of ROS2 Multi-Node Systems",
    "descriptor": "",
    "authors": [
      "Tobias Kronauer",
      "Joshwa Pohlmann",
      "Maximilian Matthe",
      "Till Smejkal",
      "Gerhard Fettweis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Performance (cs.PF)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2101.02074"
  },
  {
    "id": "arXiv:2101.02153",
    "title": "The Shapley Value of Classifiers in Ensemble Games",
    "abstract": "Comments: Source code is available here: this https URL",
    "descriptor": "\nComments: Source code is available here: this https URL\n",
    "authors": [
      "Benedek Rozemberczki",
      "Rik Sarkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Computer Science and Game Theory (cs.GT)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2101.02153"
  },
  {
    "id": "arXiv:2101.02966",
    "title": "Infinite-dimensional Folded-in-time Deep Neural Networks",
    "abstract": "Infinite-dimensional Folded-in-time Deep Neural Networks",
    "descriptor": "",
    "authors": [
      "Florian Stelzer",
      "Serhiy Yanchuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Dynamical Systems (math.DS)",
      "Functional Analysis (math.FA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2101.02966"
  },
  {
    "id": "arXiv:2101.04061",
    "title": "Towards Real-World Blind Face Restoration with Generative Facial Prior",
    "abstract": "Comments: CVPR 2021. Codes: this https URL",
    "descriptor": "\nComments: CVPR 2021. Codes: this https URL\n",
    "authors": [
      "Xintao Wang",
      "Yu Li",
      "Honglun Zhang",
      "Ying Shan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.04061"
  },
  {
    "id": "arXiv:2101.05510",
    "title": "Signal Processing on Higher-Order Networks: Livin' on the Edge ... and  Beyond",
    "abstract": "Comments: 41 pages; 8 figures",
    "descriptor": "\nComments: 41 pages; 8 figures\n",
    "authors": [
      "Michael T. Schaub",
      "Yu Zhu",
      "Jean-Baptiste Seby",
      "T. Mitchell Roddenberry",
      "Santiago Segarra"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.05510"
  },
  {
    "id": "arXiv:2101.06087",
    "title": "An Abstract Contract Theory for Programs with Procedures",
    "abstract": "Comments: 24 pages. This is the full version of the paper An Abstract Contract Theory for Programs with Procedures, published in Proceedings of the 24th International Conference on Fundamental Approaches to Software Engineering (FASE 2021), which includes the proofs of all theorems and additional examples. The conference version should always be cited",
    "descriptor": "\nComments: 24 pages. This is the full version of the paper An Abstract Contract Theory for Programs with Procedures, published in Proceedings of the 24th International Conference on Fundamental Approaches to Software Engineering (FASE 2021), which includes the proofs of all theorems and additional examples. The conference version should always be cited\n",
    "authors": [
      "Christian Lidstr\u00f6m",
      "Dilian Gurov"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2101.06087"
  },
  {
    "id": "arXiv:2101.08336",
    "title": "Location Management in IP-based Future LEO Satellite Networks: A Review",
    "abstract": "Comments: Submitted to the Proceedings of the IEEE",
    "descriptor": "\nComments: Submitted to the Proceedings of the IEEE\n",
    "authors": [
      "Tasneem Darwish",
      "Gunes Kurt",
      "Halim Yanikomeroglu",
      "Guillaume Lamontagne",
      "Michel Bellemare"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2101.08336"
  },
  {
    "id": "arXiv:2101.09612",
    "title": "On the Proof of Global Convergence of Gradient Descent for Deep ReLU  Networks with Linear Widths",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Quynh Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.09612"
  },
  {
    "id": "arXiv:2101.09855",
    "title": "Diffusion Asymptotics for Sequential Experiments",
    "abstract": "Diffusion Asymptotics for Sequential Experiments",
    "descriptor": "",
    "authors": [
      "Stefan Wager",
      "Kuang Xu"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.09855"
  },
  {
    "id": "arXiv:2101.11376",
    "title": "Learning Abstract Representations through Lossy Compression of  Multi-Modal Signals",
    "abstract": "Learning Abstract Representations through Lossy Compression of  Multi-Modal Signals",
    "descriptor": "",
    "authors": [
      "Charles Wilmot",
      "Jochen Triesch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2101.11376"
  },
  {
    "id": "arXiv:2101.12411",
    "title": "A note on synthesizing geodesic based contact curves",
    "abstract": "A note on synthesizing geodesic based contact curves",
    "descriptor": "",
    "authors": [
      "Rajesh Kumar",
      "Sudipto Mukherjee"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2101.12411"
  },
  {
    "id": "arXiv:2101.12690",
    "title": "Towards Generalising Neural Implicit Representations",
    "abstract": "Towards Generalising Neural Implicit Representations",
    "descriptor": "",
    "authors": [
      "Theo W. Costain",
      "Victor Adrian Prisacariu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.12690"
  },
  {
    "id": "arXiv:2102.00552",
    "title": "Conservation-Based Modeling and Boundary Control of Congestion with an  Application to Traffic Management in Center City Philadelphia",
    "abstract": "Conservation-Based Modeling and Boundary Control of Congestion with an  Application to Traffic Management in Center City Philadelphia",
    "descriptor": "",
    "authors": [
      "Xun Liu",
      "Hossein Rastgoftar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.00552"
  },
  {
    "id": "arXiv:2102.00678",
    "title": "Binary Classification from Multiple Unlabeled Datasets via Surrogate Set  Classification",
    "abstract": "Comments: ICML2021 camera-ready version",
    "descriptor": "\nComments: ICML2021 camera-ready version\n",
    "authors": [
      "Nan Lu",
      "Shida Lei",
      "Gang Niu",
      "Issei Sato",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.00678"
  },
  {
    "id": "arXiv:2102.02873",
    "title": "Optimal Construction of Hierarchical Overlap Graphs",
    "abstract": "Comments: 11 pages, 2 Figures",
    "descriptor": "\nComments: 11 pages, 2 Figures\n",
    "authors": [
      "Shahbaz Khan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2102.02873"
  },
  {
    "id": "arXiv:2102.03502",
    "title": "MSPM: A Modularized and Scalable Multi-Agent Reinforcement  Learning-based System for Financial Portfolio Management",
    "abstract": "MSPM: A Modularized and Scalable Multi-Agent Reinforcement  Learning-based System for Financial Portfolio Management",
    "descriptor": "",
    "authors": [
      "Zhenhan Huang",
      "Fumihide Tanaka"
    ],
    "subjectives": [
      "Portfolio Management (q-fin.PM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2102.03502"
  },
  {
    "id": "arXiv:2102.04002",
    "title": "Demystifying Assumptions in Learning to Discover Novel Classes",
    "abstract": "Demystifying Assumptions in Learning to Discover Novel Classes",
    "descriptor": "",
    "authors": [
      "Haoang Chi",
      "Feng Liu",
      "Wenjing Yang",
      "Long Lan",
      "Tongliang Liu",
      "Bo Han",
      "Gang Niu",
      "Mingyuan Zhou",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.04002"
  },
  {
    "id": "arXiv:2102.04764",
    "title": "Continuous-Time Model-Based Reinforcement Learning",
    "abstract": "Continuous-Time Model-Based Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "\u00c7a\u011fatay Y\u0131ld\u0131z",
      "Markus Heinonen",
      "Harri L\u00e4hdesm\u00e4ki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.04764"
  },
  {
    "id": "arXiv:2102.04906",
    "title": "Dynamic Neural Networks: A Survey",
    "abstract": "Dynamic Neural Networks: A Survey",
    "descriptor": "",
    "authors": [
      "Yizeng Han",
      "Gao Huang",
      "Shiji Song",
      "Le Yang",
      "Honghui Wang",
      "Yulin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.04906"
  },
  {
    "id": "arXiv:2102.05313",
    "title": "Conditional and Adversarial Euler-based Generators For Time Series",
    "abstract": "Comments: 14 page, 9 Figures",
    "descriptor": "\nComments: 14 page, 9 Figures\n",
    "authors": [
      "Carl Remlinger",
      "Joseph Mikael",
      "Romuald Elie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2102.05313"
  },
  {
    "id": "arXiv:2102.05363",
    "title": "Towards Certifying L-infinity Robustness using Neural Networks with  L-inf-dist Neurons",
    "abstract": "Comments: Appearing at International Conference on Machine Learning (ICML) 2021",
    "descriptor": "\nComments: Appearing at International Conference on Machine Learning (ICML) 2021\n",
    "authors": [
      "Bohang Zhang",
      "Tianle Cai",
      "Zhou Lu",
      "Di He",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.05363"
  },
  {
    "id": "arXiv:2102.05918",
    "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy  Text Supervision",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Chao Jia",
      "Yinfei Yang",
      "Ye Xia",
      "Yi-Ting Chen",
      "Zarana Parekh",
      "Hieu Pham",
      "Quoc V. Le",
      "Yunhsuan Sung",
      "Zhen Li",
      "Tom Duerig"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05918"
  },
  {
    "id": "arXiv:2102.06177",
    "title": "Multi-Task Reinforcement Learning with Context-based Representations",
    "abstract": "Comments: Accepted at the 38th International Conference on Machine Learning (ICML 2021). 17 pages, 4 figures, 20 tables",
    "descriptor": "\nComments: Accepted at the 38th International Conference on Machine Learning (ICML 2021). 17 pages, 4 figures, 20 tables\n",
    "authors": [
      "Shagun Sodhani",
      "Amy Zhang",
      "Joelle Pineau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2102.06177"
  },
  {
    "id": "arXiv:2102.06868",
    "title": "Fast, Accurate Barcode Detection in Ultra High-Resolution Images",
    "abstract": "Comments: 5 pages, 4 figures, 3 tables, GitHub Link added, Initial ArXiv Submission is 13 Feb 2021, Accepted at IEEE International Conference on Image Processing, September 2021, USA",
    "descriptor": "\nComments: 5 pages, 4 figures, 3 tables, GitHub Link added, Initial ArXiv Submission is 13 Feb 2021, Accepted at IEEE International Conference on Image Processing, September 2021, USA\n",
    "authors": [
      "Jerome Quenum",
      "Kehan Wang",
      "Avideh Zakhor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.06868"
  },
  {
    "id": "arXiv:2102.07006",
    "title": "Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections",
    "abstract": "Comments: Main paper of 12 pages, followed by appendix",
    "descriptor": "\nComments: Main paper of 12 pages, followed by appendix\n",
    "authors": [
      "Alexander Camuto",
      "Xiaoyu Wang",
      "Lingjiong Zhu",
      "Chris Holmes",
      "Mert G\u00fcrb\u00fczbalaban",
      "Umut \u015eim\u015fekli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07006"
  },
  {
    "id": "arXiv:2102.07437",
    "title": "Data Profiling for Adversarial Training: On the Ruin of Problematic Data",
    "abstract": "Data Profiling for Adversarial Training: On the Ruin of Problematic Data",
    "descriptor": "",
    "authors": [
      "Chengyu Dong",
      "Liyuan Liu",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07437"
  },
  {
    "id": "arXiv:2102.07753",
    "title": "Learning Intra-Batch Connections for Deep Metric Learning",
    "abstract": "Comments: Accepted to International Conference on Machine Learning (ICML) 2021, includes non-archival supplementary material",
    "descriptor": "\nComments: Accepted to International Conference on Machine Learning (ICML) 2021, includes non-archival supplementary material\n",
    "authors": [
      "Jenny Seidenschwarz",
      "Ismail Elezi",
      "Laura Leal-Taix\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.07753"
  },
  {
    "id": "arXiv:2102.08248",
    "title": "Hierarchical VAEs Know What They Don't Know",
    "abstract": "Comments: Appeared in Proceedings of the 38th International Conference on Machine Learning (ICML 2021). 18 pages, source code available at this https URL, this https URL and this https URL",
    "descriptor": "\nComments: Appeared in Proceedings of the 38th International Conference on Machine Learning (ICML 2021). 18 pages, source code available at this https URL, this https URL and this https URL\n",
    "authors": [
      "Jakob D. Havtorn",
      "Jes Frellsen",
      "S\u00f8ren Hauberg",
      "Lars Maal\u00f8e"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.08248"
  },
  {
    "id": "arXiv:2102.08358",
    "title": "Efficient Competitions and Online Learning with Strategic Forecasters",
    "abstract": "Comments: This paper will be presented at The Twenty-Second ACM Conference on Economics and Computation (EC '21), July 18-23, 2021, Budapest, Hungary",
    "descriptor": "\nComments: This paper will be presented at The Twenty-Second ACM Conference on Economics and Computation (EC '21), July 18-23, 2021, Budapest, Hungary\n",
    "authors": [
      "Rafael Frongillo",
      "Robert Gomez",
      "Anish Thilagar",
      "Bo Waggoner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2102.08358"
  },
  {
    "id": "arXiv:2102.08598",
    "title": "Leveraging Public Data for Practical Private Query Release",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Terrance Liu",
      "Giuseppe Vietri",
      "Thomas Steinke",
      "Jonathan Ullman",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.08598"
  },
  {
    "id": "arXiv:2102.09318",
    "title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm",
    "abstract": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm",
    "descriptor": "",
    "authors": [
      "Sajad Khodadadian",
      "Zaiwei Chen",
      "Siva Theja Maguluri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.09318"
  },
  {
    "id": "arXiv:2102.09430",
    "title": "State Entropy Maximization with Random Encoders for Efficient  Exploration",
    "abstract": "Comments: ICML 2021. First two authors contributed equally. Website: this https URL Code: this https URL",
    "descriptor": "\nComments: ICML 2021. First two authors contributed equally. Website: this https URL Code: this https URL\n",
    "authors": [
      "Younggyo Seo",
      "Lili Chen",
      "Jinwoo Shin",
      "Honglak Lee",
      "Pieter Abbeel",
      "Kimin Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09430"
  },
  {
    "id": "arXiv:2102.09690",
    "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Tony Z. Zhao",
      "Eric Wallace",
      "Shi Feng",
      "Dan Klein",
      "Sameer Singh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09690"
  },
  {
    "id": "arXiv:2102.09808",
    "title": "Improving Anytime Prediction with Parallel Cascaded Networks and a  Temporal-Difference Loss",
    "abstract": "Improving Anytime Prediction with Parallel Cascaded Networks and a  Temporal-Difference Loss",
    "descriptor": "",
    "authors": [
      "Michael L. Iuzzolino",
      "Michael C. Mozer",
      "Samy Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.09808"
  },
  {
    "id": "arXiv:2102.10618",
    "title": "Towards the Unification and Robustness of Perturbation and Gradient  Based Explanations",
    "abstract": "Comments: The short version of this paper appears in the proceedings of ICML-21",
    "descriptor": "\nComments: The short version of this paper appears in the proceedings of ICML-21\n",
    "authors": [
      "Sushant Agarwal",
      "Shahin Jabbari",
      "Chirag Agarwal",
      "Sohini Upadhyay",
      "Zhiwei Steven Wu",
      "Himabindu Lakkaraju"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.10618"
  },
  {
    "id": "arXiv:2102.10707",
    "title": "A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale  Black-Box Optimization",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "HanQin Cai",
      "Yuchen Lou",
      "Daniel McKenzie",
      "Wotao Yin"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.10707"
  },
  {
    "id": "arXiv:2102.11203",
    "title": "A Theory of Label Propagation for Subpopulation Shift",
    "abstract": "Comments: ICML 2021, Theory+Algorithm available",
    "descriptor": "\nComments: ICML 2021, Theory+Algorithm available\n",
    "authors": [
      "Tianle Cai",
      "Ruiqi Gao",
      "Jason D. Lee",
      "Qi Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11203"
  },
  {
    "id": "arXiv:2102.11351",
    "title": "Generative Archimedean Copulas",
    "abstract": "Comments: UAI 2021",
    "descriptor": "\nComments: UAI 2021\n",
    "authors": [
      "Yuting Ng",
      "Ali Hasan",
      "Khalil Elkhalil",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11351"
  },
  {
    "id": "arXiv:2102.11391",
    "title": "MagNet: A Neural Network for Directed Graphs",
    "abstract": "Comments: 22 pages, 4 figures, 15 tables. v2: Numerous revisions to the paper's content, including: a more general and informative presentation; many new numerical experiments; revised figures",
    "descriptor": "\nComments: 22 pages, 4 figures, 15 tables. v2: Numerous revisions to the paper's content, including: a more general and informative presentation; many new numerical experiments; revised figures\n",
    "authors": [
      "Xitong Zhang",
      "Yixuan He",
      "Nathan Brugnone",
      "Michael Perlmutter",
      "Matthew Hirn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11391"
  },
  {
    "id": "arXiv:2102.11456",
    "title": "Representation Disentanglement for Multi-modal brain MR Analysis",
    "abstract": "Comments: Accepted by Information Processing in Medical Imaging (IPMI) 2021",
    "descriptor": "\nComments: Accepted by Information Processing in Medical Imaging (IPMI) 2021\n",
    "authors": [
      "Jiahong Ouyang",
      "Ehsan Adeli",
      "Kilian M. Pohl",
      "Qingyu Zhao",
      "Greg Zaharchuk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.11456"
  },
  {
    "id": "arXiv:2102.11764",
    "title": "Quantum Entropic Causal Inference",
    "abstract": "Quantum Entropic Causal Inference",
    "descriptor": "",
    "authors": [
      "Mohammad Ali Javidian",
      "Vaneet Aggarwal",
      "Fanglin Bao",
      "Zubin Jacob"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2102.11764"
  },
  {
    "id": "arXiv:2102.12871",
    "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "abstract": "Comments: Accepted by ICML 2021",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.12871"
  },
  {
    "id": "arXiv:2102.13240",
    "title": "Adapting to Misspecification in Contextual Bandits with Offline  Regression Oracles",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Sanath Kumar Krishnamurthy",
      "Vitor Hadad",
      "Susan Athey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13240"
  },
  {
    "id": "arXiv:2102.13416",
    "title": "Moreau-Yosida $f$-divergences",
    "abstract": "Comments: ICML 2021 camera ready with appendix, 38 pages, 15 figures",
    "descriptor": "\nComments: ICML 2021 camera ready with appendix, 38 pages, 15 figures\n",
    "authors": [
      "D\u00e1vid Terj\u00e9k"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Functional Analysis (math.FA)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13416"
  },
  {
    "id": "arXiv:2103.01750",
    "title": "Nonparametric estimation of the preferential attachment function from  one network snapshot",
    "abstract": "Comments: 25 pages, 11 figures",
    "descriptor": "\nComments: 25 pages, 11 figures\n",
    "authors": [
      "Thong Pham",
      "Paul Sheridan",
      "Hidetoshi Shimodaira"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Physics and Society (physics.soc-ph)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2103.01750"
  },
  {
    "id": "arXiv:2103.02014",
    "title": "Online Adversarial Attacks",
    "abstract": "Comments: Preprint",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Andjela Mladenovic",
      "Avishek Joey Bose",
      "Hugo Berard",
      "William L. Hamilton",
      "Simon Lacoste-Julien",
      "Pascal Vincent",
      "Gauthier Gidel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2103.02014"
  },
  {
    "id": "arXiv:2103.02062",
    "title": "Variance Reduced Training with Stratified Sampling for Forecasting  Models",
    "abstract": "Variance Reduced Training with Stratified Sampling for Forecasting  Models",
    "descriptor": "",
    "authors": [
      "Yucheng Lu",
      "Youngsuk Park",
      "Lifan Chen",
      "Yuyang Wang",
      "Christopher De Sa",
      "Dean Foster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.02062"
  },
  {
    "id": "arXiv:2103.02438",
    "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design",
    "abstract": "Comments: Published as a conference paper at ICML 2021",
    "descriptor": "\nComments: Published as a conference paper at ICML 2021\n",
    "authors": [
      "Adam Foster",
      "Desi R. Ivanova",
      "Ilyas Malik",
      "Tom Rainforth"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2103.02438"
  },
  {
    "id": "arXiv:2103.02893",
    "title": "Lower-Bounded Proper Losses for Weakly Supervised Classification",
    "abstract": "Comments: ICML2021 camera ready, code available at this https URL",
    "descriptor": "\nComments: ICML2021 camera ready, code available at this https URL\n",
    "authors": [
      "Shuhei M. Yoshida",
      "Takashi Takenouchi",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.02893"
  },
  {
    "id": "arXiv:2103.03236",
    "title": "Of Moments and Matching: A Game-Theoretic Framework for Closing the  Imitation Gap",
    "abstract": "Of Moments and Matching: A Game-Theoretic Framework for Closing the  Imitation Gap",
    "descriptor": "",
    "authors": [
      "Gokul Swamy",
      "Sanjiban Choudhury",
      "J. Andrew Bagnell",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.03236"
  },
  {
    "id": "arXiv:2103.03864",
    "title": "Learning to Extend Molecular Scaffolds with Structural Motifs",
    "abstract": "Learning to Extend Molecular Scaffolds with Structural Motifs",
    "descriptor": "",
    "authors": [
      "Krzysztof Maziarz",
      "Henry Jackson-Flux",
      "Pashmina Cameron",
      "Finton Sirockin",
      "Nadine Schneider",
      "Nikolaus Stiefl",
      "Marwin Segler",
      "Marc Brockschmidt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2103.03864"
  },
  {
    "id": "arXiv:2103.03923",
    "title": "Surface Warping Incorporating Machine Learning Assisted Domain  Likelihood Estimation: A New Paradigm in Mine Geology Modelling and  Automation",
    "abstract": "Comments: Keywords: Bayesian computation, machine learning, ensemble classifiers, neural network, mesh geometry, surface warping, geochemistry, domain likelihood, geological boundaries. 23 pages, 15 figures, 11 tables",
    "descriptor": "\nComments: Keywords: Bayesian computation, machine learning, ensemble classifiers, neural network, mesh geometry, surface warping, geochemistry, domain likelihood, geological boundaries. 23 pages, 15 figures, 11 tables\n",
    "authors": [
      "Raymond Leung",
      "Mehala Balamurali",
      "Alexander Lowe"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.03923"
  },
  {
    "id": "arXiv:2103.06304",
    "title": "What is Multimodality?",
    "abstract": "Comments: Paper accepted for publication at MMSR 2021; 10 pages, 5 figures",
    "descriptor": "\nComments: Paper accepted for publication at MMSR 2021; 10 pages, 5 figures\n",
    "authors": [
      "Letitia Parcalabescu",
      "Nils Trost",
      "Anette Frank"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "General Literature (cs.GL)"
    ],
    "url": "https://arxiv.org/abs/2103.06304"
  },
  {
    "id": "arXiv:2103.06469",
    "title": "Generalizable Episodic Memory for Deep Reinforcement Learning",
    "abstract": "Generalizable Episodic Memory for Deep Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Hao Hu",
      "Jianing Ye",
      "Guangxiang Zhu",
      "Zhizhou Ren",
      "Chongjie Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.06469"
  },
  {
    "id": "arXiv:2103.07466",
    "title": "3D Semantic Scene Completion: a Survey",
    "abstract": "Comments: Journal submission",
    "descriptor": "\nComments: Journal submission\n",
    "authors": [
      "Luis Roldao",
      "Raoul de Charette",
      "Anne Verroust-Blondet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.07466"
  },
  {
    "id": "arXiv:2103.08694",
    "title": "Fast Compensated Algorithms for the Reciprocal Square Root, the  Reciprocal Hypotenuse, and Givens Rotations",
    "abstract": "Fast Compensated Algorithms for the Reciprocal Square Root, the  Reciprocal Hypotenuse, and Givens Rotations",
    "descriptor": "",
    "authors": [
      "Carlos F. Borges"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.08694"
  },
  {
    "id": "arXiv:2103.10834",
    "title": "Improved, Deterministic Smoothing for L_1 Certified Robustness",
    "abstract": "Comments: ICML 2021 Accepted Paper",
    "descriptor": "\nComments: ICML 2021 Accepted Paper\n",
    "authors": [
      "Alexander Levine",
      "Soheil Feizi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.10834"
  },
  {
    "id": "arXiv:2103.11883",
    "title": "Regularized Softmax Deep Multi-Agent $Q$-Learning",
    "abstract": "Regularized Softmax Deep Multi-Agent $Q$-Learning",
    "descriptor": "",
    "authors": [
      "Ling Pan",
      "Tabish Rashid",
      "Bei Peng",
      "Longbo Huang",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2103.11883"
  },
  {
    "id": "arXiv:2103.11888",
    "title": "Weakly Supervised Recovery of Semantic Attributes",
    "abstract": "Weakly Supervised Recovery of Semantic Attributes",
    "descriptor": "",
    "authors": [
      "Ameen Ali",
      "Tomer Galanti",
      "Evgeniy Zheltonozhskiy",
      "Chaim Baskin",
      "Lior Wolf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.11888"
  },
  {
    "id": "arXiv:2103.13748",
    "title": "Compressed Gradient Tracking Methods for Decentralized Optimization with  Linear Convergence",
    "abstract": "Compressed Gradient Tracking Methods for Decentralized Optimization with  Linear Convergence",
    "descriptor": "",
    "authors": [
      "Yiwei Liao",
      "Zhuorui Li",
      "Kun Huang",
      "Shi Pu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2103.13748"
  },
  {
    "id": "arXiv:2103.14203",
    "title": "Deep Two-Way Matrix Reordering for Relational Data Analysis",
    "abstract": "Deep Two-Way Matrix Reordering for Relational Data Analysis",
    "descriptor": "",
    "authors": [
      "Chihiro Watanabe",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.14203"
  },
  {
    "id": "arXiv:2103.17092",
    "title": "Inversion of $\u03b1$-sine and $\u03b1$-cosine transforms on  $\\mathbb{R}$",
    "abstract": "Inversion of $\u03b1$-sine and $\u03b1$-cosine transforms on  $\\mathbb{R}$",
    "descriptor": "",
    "authors": [
      "Ly Viet Hoang",
      "Evgeny Spodarev"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.17092"
  },
  {
    "id": "arXiv:2104.02710",
    "title": "The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions",
    "abstract": "Comments: Dataset: this https URL, Website: this https URL",
    "descriptor": "\nComments: Dataset: this https URL, Website: this https URL\n",
    "authors": [
      "Jennifer J. Sun",
      "Tomomi Karigo",
      "Dipam Chakraborty",
      "Sharada P. Mohanty",
      "Benjamin Wild",
      "Quan Sun",
      "Chen Chen",
      "David J. Anderson",
      "Pietro Perona",
      "Yisong Yue",
      "Ann Kennedy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.02710"
  },
  {
    "id": "arXiv:2104.05489",
    "title": "Continual Learning for Text Classification with Information  Disentanglement Based Regularization",
    "abstract": "Comments: NAACL 2021",
    "descriptor": "\nComments: NAACL 2021\n",
    "authors": [
      "Yufan Huang",
      "Yanzhe Zhang",
      "Jiaao Chen",
      "Xuezhi Wang",
      "Diyi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.05489"
  },
  {
    "id": "arXiv:2104.07749",
    "title": "Actionable Models: Unsupervised Offline Reinforcement Learning of  Robotic Skills",
    "abstract": "Actionable Models: Unsupervised Offline Reinforcement Learning of  Robotic Skills",
    "descriptor": "",
    "authors": [
      "Yevgen Chebotar",
      "Karol Hausman",
      "Yao Lu",
      "Ted Xiao",
      "Dmitry Kalashnikov",
      "Jake Varley",
      "Alex Irpan",
      "Benjamin Eysenbach",
      "Ryan Julian",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.07749"
  },
  {
    "id": "arXiv:2104.07788",
    "title": "PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural  Machine Learning Models",
    "abstract": "Comments: Source code at: this https URL",
    "descriptor": "\nComments: Source code at: this https URL\n",
    "authors": [
      "Benedek Rozemberczki",
      "Paul Scherer",
      "Yixuan He",
      "George Panagopoulos",
      "Alexander Riedel",
      "Maria Astefanoaei",
      "Oliver Kiss",
      "Ferenc Beres",
      "Guzm\u00e1n L\u00f3pez",
      "Nicolas Collignon",
      "Rik Sarkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2104.07788"
  },
  {
    "id": "arXiv:2104.08166",
    "title": "Overfitting in Bayesian Optimization: an empirical study and  early-stopping solution",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Anastasia Makarova",
      "Huibin Shen",
      "Valerio Perrone",
      "Aaron Klein",
      "Jean Baptiste Faddoul",
      "Andreas Krause",
      "Matthias Seeger",
      "Cedric Archambeau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.08166"
  },
  {
    "id": "arXiv:2104.08976",
    "title": "Anytime Ranking on Document-Ordered Indexes",
    "abstract": "Comments: Accepted to ACM TOIS, 2021",
    "descriptor": "\nComments: Accepted to ACM TOIS, 2021\n",
    "authors": [
      "Joel Mackenzie",
      "Matthias Petri",
      "Alistair Moffat"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2104.08976"
  },
  {
    "id": "arXiv:2104.10764",
    "title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer",
    "abstract": "Comments: Accepted by Interspeech 2021",
    "descriptor": "\nComments: Accepted by Interspeech 2021\n",
    "authors": [
      "Lu Huang",
      "Jingyu Sun",
      "Yufeng Tang",
      "Junfeng Hou",
      "Jinkun Chen",
      "Jun Zhang",
      "Zejun Ma"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.10764"
  },
  {
    "id": "arXiv:2104.12753",
    "title": "Vision Transformers with Patch Diversification",
    "abstract": "Comments: preprint",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Chengyue Gong",
      "Dilin Wang",
      "Meng Li",
      "Vikas Chandra",
      "Qiang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.12753"
  },
  {
    "id": "arXiv:2104.13840",
    "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
    "abstract": "Comments: Two simple and effective designs of vision transformer, which is on par with the Swin transformer",
    "descriptor": "\nComments: Two simple and effective designs of vision transformer, which is on par with the Swin transformer\n",
    "authors": [
      "Xiangxiang Chu",
      "Zhi Tian",
      "Yuqing Wang",
      "Bo Zhang",
      "Haibing Ren",
      "Xiaolin Wei",
      "Huaxia Xia",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.13840"
  },
  {
    "id": "arXiv:2105.00990",
    "title": "Hierarchical Reinforcement Learning for Air-to-Air Combat",
    "abstract": "Comments: 10 pages, 10 figures, The 2021 International Conference on Unmanned Aircraft System (ICUAS 21), June 15-18, 2021, Athens, Greece",
    "descriptor": "\nComments: 10 pages, 10 figures, The 2021 International Conference on Unmanned Aircraft System (ICUAS 21), June 15-18, 2021, Athens, Greece\n",
    "authors": [
      "Adrian P. Pope",
      "Jaime S. Ide",
      "Daria Micovic",
      "Henry Diaz",
      "David Rosenbluth",
      "Lee Ritholtz",
      "Jason C. Twedt",
      "Thayne T. Walker",
      "Kevin Alcedo",
      "Daniel Javorsek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.00990"
  },
  {
    "id": "arXiv:2105.01601",
    "title": "MLP-Mixer: An all-MLP Architecture for Vision",
    "abstract": "Comments: v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the x label in Figure 2(right)",
    "descriptor": "\nComments: v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the x label in Figure 2(right)\n",
    "authors": [
      "Ilya Tolstikhin",
      "Neil Houlsby",
      "Alexander Kolesnikov",
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Thomas Unterthiner",
      "Jessica Yung",
      "Andreas Steiner",
      "Daniel Keysers",
      "Jakob Uszkoreit",
      "Mario Lucic",
      "Alexey Dosovitskiy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.01601"
  },
  {
    "id": "arXiv:2105.02569",
    "title": "Machine Collaboration",
    "abstract": "Machine Collaboration",
    "descriptor": "",
    "authors": [
      "Qingfeng Liu",
      "Yang Feng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2105.02569"
  },
  {
    "id": "arXiv:2105.03788",
    "title": "Dynamic Game Theoretic Neural Optimizer",
    "abstract": "Comments: Accepted in International Conference on Machine Learning (ICML) 2021 as Oral",
    "descriptor": "\nComments: Accepted in International Conference on Machine Learning (ICML) 2021 as Oral\n",
    "authors": [
      "Guan-Horng Liu",
      "Tianrong Chen",
      "Evangelos A. Theodorou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2105.03788"
  },
  {
    "id": "arXiv:2105.05596",
    "title": "Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and  Semantic Embedding",
    "abstract": "Comments: Accepted by IJCAI 2021",
    "descriptor": "\nComments: Accepted by IJCAI 2021\n",
    "authors": [
      "Zhiyuan Qi",
      "Ziheng Zhang",
      "Jiaoyan Chen",
      "Xi Chen",
      "Yuejia Xiang",
      "Ningyu Zhang",
      "Yefeng Zheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.05596"
  },
  {
    "id": "arXiv:2105.06595",
    "title": "Analysis of stochastic Lanczos quadrature for spectrum approximation",
    "abstract": "Analysis of stochastic Lanczos quadrature for spectrum approximation",
    "descriptor": "",
    "authors": [
      "Tyler Chen",
      "Thomas Trogdon",
      "Shashanka Ubaru"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.06595"
  },
  {
    "id": "arXiv:2105.09043",
    "title": "Sentence Extraction-Based Machine Reading Comprehension for Vietnamese",
    "abstract": "Comments: Accepted by KSEM 2021 (International Conference on Knowledge Science, Engineering and Management)",
    "descriptor": "\nComments: Accepted by KSEM 2021 (International Conference on Knowledge Science, Engineering and Management)\n",
    "authors": [
      "Phong Nguyen-Thuan Do",
      "Nhat Duy Nguyen",
      "Tin Van Huynh",
      "Kiet Van Nguyen",
      "Anh Gia-Tuan Nguyen",
      "Ngan Luu-Thuy Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.09043"
  },
  {
    "id": "arXiv:2105.09163",
    "title": "High-Performance FPGA-based Accelerator for Bayesian Neural Networks",
    "abstract": "Comments: Design Automation Conference (DAC) 2021",
    "descriptor": "\nComments: Design Automation Conference (DAC) 2021\n",
    "authors": [
      "Hongxiang Fan",
      "Martin Ferianc",
      "Miguel Rodrigues",
      "Hongyu Zhou",
      "Xinyu Niu",
      "Wayne Luk"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2105.09163"
  },
  {
    "id": "arXiv:2105.09680",
    "title": "KLUE: Korean Language Understanding Evaluation",
    "abstract": "Comments: 76 pages, 10 figures, 36 tables",
    "descriptor": "\nComments: 76 pages, 10 figures, 36 tables\n",
    "authors": [
      "Sungjoon Park",
      "Jihyung Moon",
      "Sungdong Kim",
      "Won Ik Cho",
      "Jiyoon Han",
      "Jangwon Park",
      "Chisung Song",
      "Junseong Kim",
      "Yongsook Song",
      "Taehwan Oh",
      "Joohong Lee",
      "Juhyun Oh",
      "Sungwon Lyu",
      "Younghoon Jeong",
      "Inkwon Lee",
      "Sangwoo Seo",
      "Dongjun Lee",
      "Hyunwoo Kim",
      "Myeonghwa Lee",
      "Seongbo Jang",
      "Seungwon Do",
      "Sunkyoung Kim",
      "Kyungtae Lim",
      "Jongwon Lee",
      "Kyumin Park",
      "Jamin Shin",
      "Seonghyun Kim",
      "Lucy Park",
      "Alice Oh",
      "Jung-Woo Ha",
      "Kyunghyun Cho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.09680"
  },
  {
    "id": "arXiv:2105.10130",
    "title": "Convergence of a spatial semi-discretization for a backward semilinear  stochastic parabolic equation",
    "abstract": "Convergence of a spatial semi-discretization for a backward semilinear  stochastic parabolic equation",
    "descriptor": "",
    "authors": [
      "Binjie Li",
      "Xiaoping Xie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.10130"
  },
  {
    "id": "arXiv:2105.10446",
    "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate  Reduction",
    "abstract": "Comments: This paper integrates previous two manuscripts: arXiv:2006.08558 and arXiv:2010.14765, with significantly improved organization, presentation, and new results; V2 polishes writing and adds citation",
    "descriptor": "\nComments: This paper integrates previous two manuscripts: arXiv:2006.08558 and arXiv:2010.14765, with significantly improved organization, presentation, and new results; V2 polishes writing and adds citation\n",
    "authors": [
      "Kwan Ho Ryan Chan",
      "Yaodong Yu",
      "Chong You",
      "Haozhi Qi",
      "John Wright",
      "Yi Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.10446"
  },
  {
    "id": "arXiv:2105.10879",
    "title": "Precise Approximation of Convolutional Neural Networks for  Homomorphically Encrypted Data",
    "abstract": "Comments: Typos corrected, supplementary added",
    "descriptor": "\nComments: Typos corrected, supplementary added\n",
    "authors": [
      "Junghyun Lee",
      "Eunsang Lee",
      "Joon-Woo Lee",
      "Yongjune Kim",
      "Young-Sik Kim",
      "Jong-Seon No"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.10879"
  },
  {
    "id": "arXiv:2105.14742",
    "title": "Active Learning of Continuous-time Bayesian Networks through  Interventions",
    "abstract": "Comments: Accepted at ICML2021",
    "descriptor": "\nComments: Accepted at ICML2021\n",
    "authors": [
      "Dominik Linzner",
      "Heinz Koeppl"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.14742"
  },
  {
    "id": "arXiv:2105.14866",
    "title": "Variational Autoencoders: A Harmonic Perspective",
    "abstract": "Comments: 18 pages including Appendix, 7 Figures",
    "descriptor": "\nComments: 18 pages including Appendix, 7 Figures\n",
    "authors": [
      "Alexander Camuto",
      "Matthew Willetts"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2105.14866"
  },
  {
    "id": "arXiv:2106.00315",
    "title": "Ordering regular languages: a danger zone",
    "abstract": "Comments: 24 pages, 6 figures",
    "descriptor": "\nComments: 24 pages, 6 figures\n",
    "authors": [
      "Giovanna D'Agostino",
      "Davide Martincigh",
      "Alberto Policriti"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.00315"
  },
  {
    "id": "arXiv:2106.00565",
    "title": "Robust and accurate fine-grain power models for embedded systems with no  on-chip PMU",
    "abstract": "Robust and accurate fine-grain power models for embedded systems with no  on-chip PMU",
    "descriptor": "",
    "authors": [
      "Kris Nikov",
      "Marcos Martinez",
      "Simon Wegener",
      "Jose Nunez-Yanez",
      "Zbigniew Chamski",
      "Kyriakos Georgiou",
      "Kerstin Eder"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.00565"
  },
  {
    "id": "arXiv:2106.00651",
    "title": "Asymptotics of representation learning in finite Bayesian neural  networks",
    "abstract": "Comments: 12+28 pages, 2+1 figures; v2: fix typo in (8) and add references",
    "descriptor": "\nComments: 12+28 pages, 2+1 figures; v2: fix typo in (8) and add references\n",
    "authors": [
      "Jacob A. Zavatone-Veth",
      "Abdulkadir Canatar",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00651"
  },
  {
    "id": "arXiv:2106.00951",
    "title": "Finite-time bearing-based maneuver of acyclic leader-follower formations",
    "abstract": "Comments: Preprint, accepted to L-CSS",
    "descriptor": "\nComments: Preprint, accepted to L-CSS\n",
    "authors": [
      "Minh Hoang Trinh",
      "Hyo-Sung Ahn"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.00951"
  },
  {
    "id": "arXiv:2106.02078",
    "title": "Robust Learning via Persistency of Excitation",
    "abstract": "Robust Learning via Persistency of Excitation",
    "descriptor": "",
    "authors": [
      "Kaustubh Sridhar",
      "Oleg Sokolsky",
      "Insup Lee",
      "James Weimer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02078"
  },
  {
    "id": "arXiv:2106.02522",
    "title": "Price graphs: Utilizing the structural information of financial time  series for stock prediction",
    "abstract": "Price graphs: Utilizing the structural information of financial time  series for stock prediction",
    "descriptor": "",
    "authors": [
      "Junran Wu",
      "Ke Xu",
      "Xueyuan Chen",
      "Shangzhe Li",
      "Jichang Zhao"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02522"
  },
  {
    "id": "arXiv:2106.02731",
    "title": "Man-in-the-Middle Attack Resistant Secret Key Generation via Channel  Randomization",
    "abstract": "Comments: 13 pages, 8 figures, 4 tables",
    "descriptor": "\nComments: 13 pages, 8 figures, 4 tables\n",
    "authors": [
      "Yanjun Pan",
      "Ziqi Xu",
      "Ming Li",
      "Loukas Lazos"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02731"
  },
  {
    "id": "arXiv:2106.02770",
    "title": "Accelerating Stochastic Simulation with Interactive Neural Processes",
    "abstract": "Accelerating Stochastic Simulation with Interactive Neural Processes",
    "descriptor": "",
    "authors": [
      "Dongxia Wu",
      "Matteo Chinazzi",
      "Alessandro Vespignani",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.02770"
  },
  {
    "id": "arXiv:2106.03211",
    "title": "Distributed Learning and its Application for Time-Series Prediction",
    "abstract": "Comments: 8 pages, 10 figures, and 2 tables",
    "descriptor": "\nComments: 8 pages, 10 figures, and 2 tables\n",
    "authors": [
      "Nhuong V. Nguyen",
      "Sybille Legitime"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03211"
  },
  {
    "id": "arXiv:2106.03640",
    "title": "Making EfficientNet More Efficient: Exploring Batch-Independent  Normalization, Group Convolutions and Reduced Resolution Training",
    "abstract": "Making EfficientNet More Efficient: Exploring Batch-Independent  Normalization, Group Convolutions and Reduced Resolution Training",
    "descriptor": "",
    "authors": [
      "Dominic Masters",
      "Antoine Labatie",
      "Zach Eaton-Rosen",
      "Carlo Luschi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03640"
  },
  {
    "id": "arXiv:2106.03743",
    "title": "Proxy-Normalizing Activations to Match Batch Normalization while  Removing Batch Dependence",
    "abstract": "Proxy-Normalizing Activations to Match Batch Normalization while  Removing Batch Dependence",
    "descriptor": "",
    "authors": [
      "Antoine Labatie",
      "Dominic Masters",
      "Zach Eaton-Rosen",
      "Carlo Luschi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03743"
  },
  {
    "id": "arXiv:2106.04243",
    "title": "Parameter Inference with Bifurcation Diagrams",
    "abstract": "Comments: Under review at NeurIPS 2021",
    "descriptor": "\nComments: Under review at NeurIPS 2021\n",
    "authors": [
      "Gregory Szep",
      "Neil Dalchau",
      "Attila Csikasz-Nagy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.04243"
  },
  {
    "id": "arXiv:2106.04552",
    "title": "Neural Speaker Embeddings for Ultrasound-based Silent Speech Interfaces",
    "abstract": "Comments: 5 pages, 3 figures, 3 tables",
    "descriptor": "\nComments: 5 pages, 3 figures, 3 tables\n",
    "authors": [
      "Amin Honarmandi Shandiz",
      "L\u00e1szl\u00f3 T\u00f3th",
      "G\u00e1bor Gosztolya",
      "Alexandra Mark\u00f3",
      "Tam\u00e1s G\u00e1bor Csap\u00f3"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04552"
  },
  {
    "id": "arXiv:2106.04805",
    "title": "Streaming Belief Propagation for Community Detection",
    "abstract": "Comments: 36 pages, 13 figures",
    "descriptor": "\nComments: 36 pages, 13 figures\n",
    "authors": [
      "Yuchen Wu",
      "MohammadHossein Bateni",
      "Andre Linhares",
      "Filipe Miguel Goncalves de Almeida",
      "Andrea Montanari",
      "Ashkan Norouzi-Fard",
      "Jakab Tardos"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.04805"
  },
  {
    "id": "arXiv:2106.04810",
    "title": "Sleeping beauties and temporal evolution of the coronavirus literature",
    "abstract": "Sleeping beauties and temporal evolution of the coronavirus literature",
    "descriptor": "",
    "authors": [
      "Milad Haghani",
      "Pegah Varamini"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2106.04810"
  },
  {
    "id": "arXiv:2106.04815",
    "title": "ChaCha for Online AutoML",
    "abstract": "Comments: 16 pages (including supplementary appendix). Appearing at ICML 2021",
    "descriptor": "\nComments: 16 pages (including supplementary appendix). Appearing at ICML 2021\n",
    "authors": [
      "Qingyun Wu",
      "Chi Wang",
      "John Langford",
      "Paul Mineiro",
      "Marco Rossi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04815"
  },
  {
    "id": "arXiv:2106.04892",
    "title": "A 2D front-tracking Lagrangian model for the modeling of anisotropic  grain growth",
    "abstract": "A 2D front-tracking Lagrangian model for the modeling of anisotropic  grain growth",
    "descriptor": "",
    "authors": [
      "Sebastian Florez",
      "Julien Fausty",
      "Karen Alvarado",
      "Brayan Murgas",
      "Marc Bernacki"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.04892"
  },
  {
    "id": "arXiv:2106.04967",
    "title": "GP-ConvCNP: Better Generalization for Convolutional Conditional Neural  Processes on Time Series Data",
    "abstract": "Comments: UAI 2021",
    "descriptor": "\nComments: UAI 2021\n",
    "authors": [
      "Jens Petersen",
      "Gregor K\u00f6hler",
      "David Zimmerer",
      "Fabian Isensee",
      "Paul F. J\u00e4ger",
      "Klaus H. Maier-Hein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04967"
  },
  {
    "id": "arXiv:2106.05082",
    "title": "Agile wide-field imaging with selective high resolution",
    "abstract": "Comments: 12pages,6figures",
    "descriptor": "\nComments: 12pages,6figures\n",
    "authors": [
      "Lintao Peng",
      "Liheng Bian",
      "Tiexin Liu",
      "Jun Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.05082"
  },
  {
    "id": "arXiv:2106.05184",
    "title": "Tackling spam in the era of end-to-end encryption: A case study of  WhatsApp",
    "abstract": "Tackling spam in the era of end-to-end encryption: A case study of  WhatsApp",
    "descriptor": "",
    "authors": [
      "Pushkal Agarwal",
      "Aravindh Raman",
      "Kiran Garimella",
      "Damilola Ibosiola",
      "Gareth Tyson",
      "Nishanth Sastry"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.05184"
  },
  {
    "id": "arXiv:2106.05386",
    "title": "Artificial Intelligence in Drug Discovery: Applications and Techniques",
    "abstract": "Comments: Minor text revisions",
    "descriptor": "\nComments: Minor text revisions\n",
    "authors": [
      "Jianyuan Deng",
      "Zhibo Yang",
      "Dimitris Samaras",
      "Fusheng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05386"
  },
  {
    "id": "arXiv:2106.05407",
    "title": "Auditing Network Traffic and Privacy Policies in Oculus VR",
    "abstract": "Comments: 13 pages (and 4 pages of references), 7 figures, 4 tables",
    "descriptor": "\nComments: 13 pages (and 4 pages of references), 7 figures, 4 tables\n",
    "authors": [
      "Rahmadi Trimananda",
      "Hieu Le",
      "Hao Cui",
      "Janice Tran Ho",
      "Anastasia Shuba",
      "Athina Markopoulou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.05407"
  },
  {
    "id": "arXiv:2106.05530",
    "title": "Adversarial Option-Aware Hierarchical Imitation Learning",
    "abstract": "Comments: accepted by ICML 2021",
    "descriptor": "\nComments: accepted by ICML 2021\n",
    "authors": [
      "Mingxuan Jing",
      "Wenbing Huang",
      "Fuchun Sun",
      "Xiaojian Ma",
      "Tao Kong",
      "Chuang Gan",
      "Lei Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05530"
  },
  {
    "id": "arXiv:2106.05546",
    "title": "Progressive Multi-Granularity Training for Non-Autoregressive  Translation",
    "abstract": "Comments: ACL 2021, Short Findings",
    "descriptor": "\nComments: ACL 2021, Short Findings\n",
    "authors": [
      "Liang Ding",
      "Longyue Wang",
      "Xuebo Liu",
      "Derek F. Wong",
      "Dacheng Tao",
      "Zhaopeng Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05546"
  },
  {
    "id": "arXiv:2106.05554",
    "title": "Progressive Stage-wise Learning for Unsupervised Feature Representation  Enhancement",
    "abstract": "Comments: Accepted by the IEEE conference on computer vision and pattern recognition. 2021",
    "descriptor": "\nComments: Accepted by the IEEE conference on computer vision and pattern recognition. 2021\n",
    "authors": [
      "Zefan Li",
      "Chenxi Liu",
      "Alan Yuille",
      "Bingbing Ni",
      "Wenjun Zhang",
      "Wen Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05554"
  },
  {
    "id": "arXiv:2106.05577",
    "title": "Quantum-Resistant Security for Software Updates on Low-power Networked  Embedded Devices",
    "abstract": "Quantum-Resistant Security for Software Updates on Low-power Networked  Embedded Devices",
    "descriptor": "",
    "authors": [
      "Gustavo Banegas",
      "Koen Zandberg",
      "Adrian Herrmann",
      "Emmanuel Baccelli",
      "Benjamin Smith"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05577"
  },
  {
    "id": "arXiv:2106.05605",
    "title": "Statistical behaviour of interfaces subjected to curvature flow and  torque effects applied to microstructural evolutions",
    "abstract": "Statistical behaviour of interfaces subjected to curvature flow and  torque effects applied to microstructural evolutions",
    "descriptor": "",
    "authors": [
      "Sebastian Florez",
      "Karen Alvarado",
      "Marc Bernacki"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.05605"
  },
  {
    "id": "arXiv:2106.05607",
    "title": "Spatially Invariant Unsupervised 3D Object Segmentation with Graph  Neural Networks",
    "abstract": "Spatially Invariant Unsupervised 3D Object Segmentation with Graph  Neural Networks",
    "descriptor": "",
    "authors": [
      "Tianyu Wang",
      "Miaomiao Liu",
      "Kee Siong Ng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05607"
  },
  {
    "id": "arXiv:2106.05664",
    "title": "Ruddit: Norms of Offensiveness for English Reddit Comments",
    "abstract": "Comments: Camera-ready version in ACL 2021",
    "descriptor": "\nComments: Camera-ready version in ACL 2021\n",
    "authors": [
      "Rishav Hada",
      "Sohi Sudhir",
      "Pushkar Mishra",
      "Helen Yannakoudakis",
      "Saif M. Mohammad",
      "Ekaterina Shutova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05664"
  },
  {
    "id": "arXiv:2106.05729",
    "title": "GRASP: Graph Alignment through Spectral Signatures",
    "abstract": "Comments: Accepted to APWeb-WAIM",
    "descriptor": "\nComments: Accepted to APWeb-WAIM\n",
    "authors": [
      "Judith Hermanns",
      "Anton Tsitsulin",
      "Marina Munkhoeva",
      "Alex Bronstein",
      "Davide Mottin",
      "Panagiotis Karras"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.05729"
  },
  {
    "id": "arXiv:2106.05756",
    "title": "Lifting The Grey Curtain: A First Look at the Ecosystem of CULPRITWARE",
    "abstract": "Comments: 13 pages, 10 figures",
    "descriptor": "\nComments: 13 pages, 10 figures\n",
    "authors": [
      "Zhuo Chen",
      "Lei Wu",
      "Jing Cheng",
      "Yubo Hu",
      "Yajin Zhou",
      "Zhushou Tang",
      "Yexuan Chen",
      "Jinku Li",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.05756"
  },
  {
    "id": "arXiv:2106.05767",
    "title": "Meta-Learning for Symbolic Hyperparameter Defaults",
    "abstract": "Comments: Pieter Gijsbers and Florian Pfisterer contributed equally to the paper. V1: Two page GECCO poster paper accepted at GECCO 2021. V2: The original full length paper (8 pages) with appendix",
    "descriptor": "\nComments: Pieter Gijsbers and Florian Pfisterer contributed equally to the paper. V1: Two page GECCO poster paper accepted at GECCO 2021. V2: The original full length paper (8 pages) with appendix\n",
    "authors": [
      "Pieter Gijsbers",
      "Florian Pfisterer",
      "Jan N. van Rijn",
      "Bernd Bischl",
      "Joaquin Vanschoren"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05767"
  },
  {
    "id": "arXiv:2106.05823",
    "title": "Neural Text Classification and Stacked Heterogeneous Embeddings for  Named Entity Recognition in SMM4H 2021",
    "abstract": "Comments: NAACL 2021",
    "descriptor": "\nComments: NAACL 2021\n",
    "authors": [
      "Usama Yaseen",
      "Stefan Langer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05823"
  },
  {
    "id": "arXiv:2106.05968",
    "title": "Space-time Mixing Attention for Video Transformer",
    "abstract": "Comments: Updated results on SSv2",
    "descriptor": "\nComments: Updated results on SSv2\n",
    "authors": [
      "Adrian Bulat",
      "Juan-Manuel Perez-Rua",
      "Swathikiran Sudhakaran",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05968"
  }
]