[
  {
    "id": "arXiv:2106.07677",
    "title": "Planning to Fairly Allocate: Probabilistic Fairness in the Restless  Bandit Setting",
    "abstract": "Restless and collapsing bandits are commonly used to model constrained\nresource allocation in settings featuring arms with action-dependent transition\nprobabilities, such as allocating health interventions among patients [Whittle,\n1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based\napproaches to this planning problem either do not consider fairness among arms,\nor incentivize fairness without guaranteeing it [Mate et al., 2021].\nAdditionally, their optimality guarantees only apply when arms are indexable\nand threshold-optimal. We demonstrate that the incorporation of hard fairness\nconstraints necessitates the coupling of arms, which undermines the\ntractability, and by extension, indexability of the problem. We then introduce\nProbFair, a probabilistically fair stationary policy that maximizes total\nexpected reward and satisfies the budget constraint, while ensuring a strictly\npositive lower bound on the probability of being pulled at each timestep. We\nevaluate our algorithm on a real-world application, where interventions support\ncontinuous positive airway pressure (CPAP) therapy adherence among obstructive\nsleep apnea (OSA) patients, as well as simulations on a broader class of\nsynthetic transition matrices.",
    "descriptor": "\nComments: 27 pages, 19 figures\n",
    "authors": [
      "Christine Herlihy",
      "Aviva Prins",
      "Aravind Srinivasan",
      "John Dickerson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.07677"
  },
  {
    "id": "arXiv:2106.07682",
    "title": "Revisiting Model Stitching to Compare Neural Representations",
    "abstract": "We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology\nto study the internal representations of neural networks. Given two trained and\nfrozen models $A$ and $B$, we consider a \"stitched model'' formed by connecting\nthe bottom-layers of $A$ to the top-layers of $B$, with a simple trainable\nlayer between them. We argue that model stitching is a powerful and perhaps\nunder-appreciated tool, which reveals aspects of representations that measures\nsuch as centered kernel alignment (CKA) cannot. Through extensive experiments,\nwe use model stitching to obtain quantitative verifications for intuitive\nstatements such as \"good networks learn similar representations'', by\ndemonstrating that good networks of the same architecture, but trained in very\ndifferent ways (e.g.: supervised vs. self-supervised learning), can be stitched\nto each other without drop in performance. We also give evidence for the\nintuition that \"more is better'' by showing that representations learnt with\n(1) more data, (2) bigger width, or (3) more training time can be \"plugged in''\nto weaker models to improve performance. Finally, our experiments reveal a new\nstructural property of SGD which we call \"stitching connectivity'', akin to\nmode-connectivity: typical minima reached by SGD can all be stitched to each\nother with minimal change in accuracy.",
    "descriptor": "",
    "authors": [
      "Yamini Bansal",
      "Preetum Nakkiran",
      "Boaz Barak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07682"
  },
  {
    "id": "arXiv:2106.07687",
    "title": "Deep neural networks for geometric multigrid methods",
    "abstract": "We investigate scaling and efficiency of the deep neural network multigrid\nmethod (DNN-MG).\nDNN-MG is a novel neural network-based technique for the simulation of the\nNavier-Stokes equations that combines an adaptive geometric multigrid solver,\ni.e. a highly efficient\nclassical solution scheme, with a recurrent neural network with memory.\nThe neural network replaces in DNN-MG one or multiple finest multigrid layers\nand provides a correction for the classical solve in the next time step.\nThis leads to little degradation in the solution quality while substantially\nreducing the overall computational costs.\nAt the same time, the use of the multigrid solver at the coarse scales allows\nfor a compact network that is easy to train, generalizes well, and allows for\nthe incorporation of physical constraints.\nPrevious work on DNN-MG focused on the overall scheme and how to enforce\ndivergence freedom in the solution.\nIn this work, we investigate how the network size affects training and\nsolution quality and the overall runtime of the computations.\nOur results demonstrate that larger networks are able to capture the\nflow behavior better while requiring only little additional training time.\nAt runtime, the use of the neural network correction can even reduce the\ncomputation time compared to a classical multigrid simulation through a faster\nconvergence of the nonlinear solve that is required at every time step.",
    "descriptor": "\nComments: Submitted to YIC2021 VI ECCOMAS YOUNG INVESTIGATORS CONFERENCE\n",
    "authors": [
      "Nils Margenberg",
      "Robert Jendersie",
      "Thomas Richter",
      "Christian Lessig"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.07687"
  },
  {
    "id": "arXiv:2106.07688",
    "title": "Next Generation Reservoir Computing",
    "abstract": "Reservoir computing is a best-in-class machine learning algorithm for\nprocessing information generated by dynamical systems using observed\ntime-series data. Importantly, it requires very small training data sets, uses\nlinear optimization, and thus requires minimal computing resources. However,\nthe algorithm uses randomly sampled matrices to define the underlying recurrent\nneural network and has a multitude of metaparameters that must be optimized.\nRecent results demonstrate the equivalence of reservoir computing to nonlinear\nvector autoregression, which requires no random matrices, fewer metaparameters,\nand provides interpretable results. Here, we demonstrate that nonlinear vector\nautoregression excels at reservoir computing benchmark tasks and requires even\nshorter training data sets and training time, heralding the next generation of\nreservoir computing.",
    "descriptor": "",
    "authors": [
      "Daniel J. Gauthier",
      "Erik Bollt",
      "Aaron Griffith",
      "Wendson A.S. Barbosa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ],
    "url": "https://arxiv.org/abs/2106.07688"
  },
  {
    "id": "arXiv:2106.07689",
    "title": "Phase Transitions, Distance Functions, and Implicit Neural  Representations",
    "abstract": "Representing surfaces as zero level sets of neural networks recently emerged\nas a powerful modeling paradigm, named Implicit Neural Representations (INRs),\nserving numerous downstream applications in geometric deep learning and 3D\nvision. Training INRs previously required choosing between occupancy and\ndistance function representation and different losses with unknown limit\nbehavior and/or bias. In this paper we draw inspiration from the theory of\nphase transitions of fluids and suggest a loss for training INRs that learns a\ndensity function that converges to a proper occupancy function, while its log\ntransform converges to a distance function. Furthermore, we analyze the limit\nminimizer of this loss showing it satisfies the reconstruction constraints and\nhas minimal surface perimeter, a desirable inductive bias for surface\nreconstruction. Training INRs with this new loss leads to state-of-the-art\nreconstructions on a standard benchmark.",
    "descriptor": "",
    "authors": [
      "Yaron Lipman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07689"
  },
  {
    "id": "arXiv:2106.07691",
    "title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task",
    "abstract": "If two sentences have the same meaning, it should follow that they are\nequivalent in their inferential properties, i.e., each sentence should\ntextually entail the other. However, many paraphrase datasets currently in\nwidespread use rely on a sense of paraphrase based on word overlap and syntax.\nCan we teach them instead to identify paraphrases in a way that draws on the\ninferential properties of the sentences, and is not over-reliant on lexical and\nsyntactic similarities of a sentence pair? We apply the adversarial paradigm to\nthis question, and introduce a new adversarial method of dataset creation for\nparaphrase identification: the Adversarial Paraphrasing Task (APT), which asks\nparticipants to generate semantically equivalent (in the sense of mutually\nimplicative) but lexically and syntactically disparate paraphrases. These\nsentence pairs can then be used both to test paraphrase identification models\n(which get barely random accuracy) and then improve their performance. To\naccelerate dataset generation, we explore automation of APT using T5, and show\nthat the resulting dataset also improves accuracy. We discuss implications for\nparaphrase detection and release our dataset in the hope of making paraphrase\ndetection models better able to detect sentence-level meaning equivalence.",
    "descriptor": "",
    "authors": [
      "Animesh Nighojkar",
      "John Licato"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07691"
  },
  {
    "id": "arXiv:2106.07696",
    "title": "Face Age Progression With Attribute Manipulation",
    "abstract": "Face is one of the predominant means of person recognition. In the process of\nageing, human face is prone to many factors such as time, attributes, weather\nand other subject specific variations. The impact of these factors were not\nwell studied in the literature of face aging. In this paper, we propose a novel\nholistic model in this regard viz., ``Face Age progression With Attribute\nManipulation (FAWAM)\", i.e. generating face images at different ages while\nsimultaneously varying attributes and other subject specific characteristics.\nWe address the task in a bottom-up manner, as two submodules i.e. face age\nprogression and face attribute manipulation. For face aging, we use an\nattribute-conscious face aging model with a pyramidal generative adversarial\nnetwork that can model age-specific facial changes while maintaining intrinsic\nsubject specific characteristics. For facial attribute manipulation, the age\nprocessed facial image is manipulated with desired attributes while preserving\nother details unchanged, leveraging an attribute generative adversarial network\narchitecture. We conduct extensive analysis in standard large scale datasets\nand our model achieves significant performance both quantitatively and\nqualitatively.",
    "descriptor": "\nComments: -\n",
    "authors": [
      "Sinzith Tatikonda",
      "Athira Nambiar",
      "Anurag Mittal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07696"
  },
  {
    "id": "arXiv:2106.07699",
    "title": "Using heterogeneity in semi-supervised transcription hypotheses to  improve code-switched speech recognition",
    "abstract": "Modeling code-switched speech is an important problem in automatic speech\nrecognition (ASR). Labeled code-switched data are rare, so monolingual data are\noften used to model code-switched speech. These monolingual data may be more\nclosely matched to one of the languages in the code-switch pair. We show that\nsuch asymmetry can bias prediction toward the better-matched language and\ndegrade overall model performance. To address this issue, we propose a\nsemi-supervised approach for code-switched ASR. We consider the case of\nEnglish-Mandarin code-switching, and the problem of using monolingual data to\nbuild bilingual \"transcription models'' for annotation of unlabeled\ncode-switched data. We first build multiple transcription models so that their\nindividual predictions are variously biased toward either English or Mandarin.\nWe then combine these biased transcriptions using confidence-based selection.\nThis strategy generates a superior transcript for semi-supervised training, and\nobtains a 19% relative improvement compared to a semi-supervised system that\nrelies on a transcription model built with only the best-matched monolingual\ndata.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Andrew Slottje",
      "Shannon Wotherspoon",
      "William Hartmann",
      "Matthew Snover",
      "Owen Kimball"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07699"
  },
  {
    "id": "arXiv:2106.07704",
    "title": "Text Generation with Efficient (Soft) Q-Learning",
    "abstract": "Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many applications, such as generating adversarial\nattacks or generating prompts to control language models. Reinforcement\nlearning (RL) on the other hand offers a more flexible solution by allowing\nusers to plug in arbitrary task metrics as reward. Yet previous RL algorithms\nfor text generation, such as policy gradient (on-policy RL) and Q-learning\n(off-policy RL), are often notoriously inefficient or unstable to train due to\nthe large sequence space and the sparse reward received only at the end of\nsequences. In this paper, we introduce a new RL formulation for text generation\nfrom the soft Q-learning perspective. It further enables us to draw from the\nlatest RL advances, such as path consistency learning, to combine the best of\non-/off-policy updates, and learn effectively from sparse reward. We apply the\napproach to a wide range of tasks, including learning from noisy/negative\nexamples, adversarial attacks, and prompt generation. Experiments show our\napproach consistently outperforms both task-specialized algorithms and the\nprevious RL methods. On standard supervised tasks where MLE prevails, our\napproach also achieves competitive performance and stability by training text\ngeneration from scratch.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Han Guo",
      "Bowen Tan",
      "Zhengzhong Liu",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07704"
  },
  {
    "id": "arXiv:2106.07708",
    "title": "CathAI: Fully Automated Interpretation of Coronary Angiograms Using  Neural Networks",
    "abstract": "Coronary heart disease (CHD) is the leading cause of adult death in the\nUnited States and worldwide, and for which the coronary angiography procedure\nis the primary gateway for diagnosis and clinical management decisions. The\nstandard-of-care for interpretation of coronary angiograms depends upon ad-hoc\nvisual assessment by the physician operator. However, ad-hoc visual\ninterpretation of angiograms is poorly reproducible, highly variable and bias\nprone. Here we show for the first time that fully-automated angiogram\ninterpretation to estimate coronary artery stenosis is possible using a\nsequence of deep neural network algorithms. The algorithmic pipeline we\ndeveloped--called CathAI--achieves state-of-the art performance across the\nsequence of tasks required to accomplish automated interpretation of\nunselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated\npositive predictive value, sensitivity and F1 score of >=90% to identify the\nprojection angle overall and >=93% for left or right coronary artery angiogram\ndetection, the primary anatomic structures of interest. To predict obstructive\ncoronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an\narea under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:\n0.843-0.880). When externally validated in a healthcare system in another\ncountry, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive\ncoronary artery stenosis. Our results demonstrate that multiple purpose-built\nneural networks can function in sequence to accomplish the complex series of\ntasks required for automated analysis of real-world angiograms. Deployment of\nCathAI may serve to increase standardization and reproducibility in coronary\nstenosis assessment, while providing a robust foundation to accomplish future\ntasks for algorithmic angiographic interpretation.",
    "descriptor": "\nComments: 62 pages, 3 main figures, 2 main tables\n",
    "authors": [
      "Robert Avram",
      "Jeffrey E. Olgin",
      "Alvin Wan",
      "Zeeshan Ahmed",
      "Louis Verreault-Julien",
      "Sean Abreau",
      "Derek Wan",
      "Joseph E. Gonzalez",
      "Derek Y. So",
      "Krishan Soni",
      "Geoffrey H. Tison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.07708"
  },
  {
    "id": "arXiv:2106.07709",
    "title": "Eavesdropper and Jammer Selection in Wireless Source Localization  Networks",
    "abstract": "We consider a wireless source localization network in which a target node\nemits localization signals that are used by anchor nodes to estimate the target\nnode position. In addition to target and anchor nodes, there can also exist\neavesdropper nodes and jammer nodes which aim to estimate the position of the\ntarget node and to degrade the accuracy of localization, respectively. We first\npropose the problem of eavesdropper selection with the goal of optimally\nplacing a given number of eavesdropper nodes to a subset of possible positions\nin the network to estimate the target node position as accurately as possible.\nAs the performance metric, the Cramer-Rao lower bound (CRLB) related to the\nestimation of the target node position by eavesdropper nodes is derived, and\nits convexity and monotonicity properties are investigated. By relaxing the\ninteger constraints, the eavesdropper selection problem is approximated by a\nconvex optimization problem and algorithms are proposed for eavesdropper\nselection. Moreover, in the presence of parameter uncertainty, a robust version\nof the eavesdropper selection problem is developed. Then, the problem of jammer\nselection is proposed where the aim is to optimally place a given number of\njammer nodes to a subset of possible positions for degrading the localization\naccuracy of the network as much as possible. A CRLB expression from the\nliterature is used as the performance metric, and its concavity and\nmonotonicity properties are derived. Also, a convex optimization problem and\nits robust version are derived after relaxation. Moreover, the joint\neavesdropper and jammer selection problem is proposed with the goal of placing\ncertain numbers of eavesdropper and jammer nodes to a subset of possible\npositions. Simulation results are presented to illustrate performance of the\nproposed algorithms.",
    "descriptor": "\nComments: 16 pages, 10 figures\n",
    "authors": [
      "Cuneyd Ozturk",
      "Sinan Gezici"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.07709"
  },
  {
    "id": "arXiv:2106.07714",
    "title": "Learning Deep Morphological Networks with Neural Architecture Search",
    "abstract": "Deep Neural Networks (DNNs) are generated by sequentially performing linear\nand non-linear processes. Using a combination of linear and non-linear\nprocedures is critical for generating a sufficiently deep feature space. The\nmajority of non-linear operators are derivations of activation functions or\npooling functions. Mathematical morphology is a branch of mathematics that\nprovides non-linear operators for a variety of image processing problems. We\ninvestigate the utility of integrating these operations in an end-to-end deep\nlearning framework in this paper. DNNs are designed to acquire a realistic\nrepresentation for a particular job. Morphological operators give topological\ndescriptors that convey salient information about the shapes of objects\ndepicted in images. We propose a method based on meta-learning to incorporate\nmorphological operators into DNNs. The learned architecture demonstrates how\nour novel morphological operations significantly increase DNN performance on\nvarious tasks, including picture classification and edge detection.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Yufei Hu",
      "Nacim Belkhir",
      "Jesus Angulo",
      "Angela Yao",
      "Gianni Franchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07714"
  },
  {
    "id": "arXiv:2106.07715",
    "title": "How to Test the Randomness from the Wireless Channel for Security?",
    "abstract": "We revisit the traditional framework of wireless secret key generation, where\ntwo parties leverage the wireless channel randomness to establish a secret key.\nThe essence in the framework is to quantify channel randomness into bit\nsequences for key generation. Conducting randomness tests on such bit sequences\nhas been a common practice to provide the confidence to validate whether they\nare random. Interestingly, despite different settings in the tests, existing\nstudies interpret the results the same: passing tests means that the bit\nsequences are indeed random.\nIn this paper, we investigate how to properly test the wireless channel\nrandomness to ensure enough security strength and key generation efficiency. In\nparticular, we define an adversary model that leverages the imperfect\nrandomness of the wireless channel to search the generated key, and create a\nguideline to set up randomness testing and privacy amplification to eliminate\nsecurity loss and achieve efficient key generation rate. We use theoretical\nanalysis and comprehensive experiments to reveal that common practice misuses\nrandomness testing and privacy amplification: (i) no security insurance of key\nstrength, (ii) low efficiency of key generation rate. After revision by our\nguideline, security loss can be eliminated and key generation rate can be\nincreased significantly.",
    "descriptor": "",
    "authors": [
      "Zhe Qu",
      "Shangqing Zhao",
      "Jie Xu",
      "Zhuo Lu",
      "Yao Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.07715"
  },
  {
    "id": "arXiv:2106.07716",
    "title": "Overcoming Domain Mismatch in Low Resource Sequence-to-Sequence ASR  Models using Hybrid Generated Pseudotranscripts",
    "abstract": "Sequence-to-sequence (seq2seq) models are competitive with hybrid models for\nautomatic speech recognition (ASR) tasks when large amounts of training data\nare available. However, data sparsity and domain adaptation are more\nproblematic for seq2seq models than their hybrid counterparts. We examine\ncorpora of five languages from the IARPA MATERIAL program where the transcribed\ndata is conversational telephone speech (CTS) and evaluation data is broadcast\nnews (BN). We show that there is a sizable initial gap in such a data condition\nbetween hybrid and seq2seq models, and the hybrid model is able to further\nimprove through the use of additional language model (LM) data. We use an\nadditional set of untranscribed data primarily in the BN domain for\nsemisupervised training. In semisupervised training, a seed model trained on\ntranscribed data generates hypothesized transcripts for unlabeled\ndomain-matched data for further training. By using a hybrid model with an\nexpanded language model for pseudotranscription, we are able to improve our\nseq2seq model from an average word error rate (WER) of 66.7% across all five\nlanguages to 29.0% WER. While this puts the seq2seq model at a competitive\noperating point, hybrid models are still able to use additional LM data to\nmaintain an advantage.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Chak-Fai Li",
      "Francis Keith",
      "William Hartmann",
      "Matthew Snover",
      "Owen Kimball"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07716"
  },
  {
    "id": "arXiv:2106.07718",
    "title": "HUMAP: Hierarchical Uniform Manifold Approximation and Projection",
    "abstract": "Dimensionality reduction (DR) techniques help analysts to understand patterns\nin high-dimensional spaces. These techniques, often represented by scatter\nplots, are employed in diverse science domains and facilitate similarity\nanalysis among clusters and data samples. For datasets containing many\ngranularities or when analysis follows the information visualization mantra,\nhierarchical DR techniques are the most suitable approach since they present\nmajor structures beforehand and details on demand. However, current\nhierarchical DR techniques are not fully capable of addressing literature\nproblems because they do not preserve the projection mental map across\nhierarchical levels or are not suitable for most data types. This work presents\nHUMAP, a novel hierarchical dimensionality reduction technique designed to be\nflexible on preserving local and global structures and preserve the mental map\nthroughout hierarchical exploration. We provide empirical evidence of our\ntechnique's superiority compared with current hierarchical approaches and show\ntwo case studies to demonstrate its strengths.",
    "descriptor": "",
    "authors": [
      "Wilson E. Marc\u00edlio-Jr",
      "Danilo M. Eler",
      "Fernando V. Paulovich",
      "Rafael M. Martins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.07718"
  },
  {
    "id": "arXiv:2106.07719",
    "title": "Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines",
    "abstract": "In this paper, we present a multi-lingual sentence encoder that can be used\nin search engines as a query and document encoder. This embedding enables a\nsemantic similarity score between queries and documents that can be an\nimportant feature in document ranking and relevancy. To train such a customized\nsentence encoder, it is beneficial to leverage users search data in the form of\nquery-document clicked pairs however, we must avoid relying too much on search\nclick data as it is biased and does not cover many unseen cases. The search\ndata is heavily skewed towards short queries and for long queries is small and\noften noisy. The goal is to design a universal multi-lingual encoder that works\nfor all cases and covers both short and long queries. We select a number of\npublic NLI datasets in different languages and translation data and together\nwith user search data we train a language model using a multi-task approach. A\nchallenge is that these datasets are not homogeneous in terms of content, size\nand the balance ratio. While the public NLI datasets are usually two-sentence\nbased with the same portion of positive and negative pairs, the user search\ndata can contain multi-sentence documents and only positive pairs. We show how\nmulti-task training enables us to leverage all these datasets and exploit\nknowledge sharing across these tasks.",
    "descriptor": "",
    "authors": [
      "Mahdi Hajiaghayi",
      "Monir Hajiaghayi",
      "Mark Bolin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07719"
  },
  {
    "id": "arXiv:2106.07720",
    "title": "Incorporating Domain Knowledge into Health Recommender Systems using  Hyperbolic Embeddings",
    "abstract": "In contrast to many other domains, recommender systems in health services may\nbenefit particularly from the incorporation of health domain knowledge, as it\nhelps to provide meaningful and personalised recommendations catering to the\nindividual's health needs. With recent advances in representation learning\nenabling the hierarchical embedding of health knowledge into the hyperbolic\nPoincare space, this work proposes a content-based recommender system for\npatient-doctor matchmaking in primary care based on patients' health profiles,\nenriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer\nlearning. The proposed model outperforms its conventional counterpart in terms\nof recommendation accuracy and has several important business implications for\nimproving the patient-doctor relationship.",
    "descriptor": "\nComments: 12 pages, 3 figures, accepted at the 2020 International Conference on Complex Networks and Their Applications\n",
    "authors": [
      "Joel Peito",
      "Qiwei Han"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07720"
  },
  {
    "id": "arXiv:2106.07722",
    "title": "EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from  Literature",
    "abstract": "To interpret the genetic profile present in a patient sample, it is necessary\nto know which mutations have important roles in the development of the\ncorresponding cancer type. Named entity recognition is a core step in the text\nmining pipeline which facilitates mining valuable cancer information from the\nscientific literature. However, due to the scarcity of related datasets,\nprevious NER attempts in this domain either suffer from low performance when\ndeep learning based models are deployed, or they apply feature based machine\nlearning models or rule based models to tackle this problem, which requires\nintensive efforts from domain experts, and limit the model generalization\ncapability. In this paper, we propose EPICURE, an ensemble pre trained model\nequipped with a conditional random field pattern layer and a span prediction\npattern layer to extract cancer mutations from text. We also adopt a data\naugmentation strategy to expand our training set from multiple datasets.\nExperimental results on three benchmark datasets show competitive results\ncompared to the baseline models.",
    "descriptor": "",
    "authors": [
      "Jiarun Cao",
      "Elke M van Veen",
      "Niels Peek",
      "Andrew G Renehan",
      "Sophia Ananiadou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07722"
  },
  {
    "id": "arXiv:2106.07724",
    "title": "An Exponential Improvement on the Memorization Capacity of Deep  Threshold Networks",
    "abstract": "It is well known that modern deep neural networks are powerful enough to\nmemorize datasets even when the labels have been randomized. Recently,\nVershynin (2020) settled a long standing question by Baum (1988), proving that\n\\emph{deep threshold} networks can memorize $n$ points in $d$ dimensions using\n$\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}+\\sqrt{n})$ neurons and\n$\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}(d+\\sqrt{n})+n)$ weights, where $\\delta$\nis the minimum distance between the points. In this work, we improve the\ndependence on $\\delta$ from exponential to almost linear, proving that\n$\\widetilde{\\mathcal{O}}(\\frac{1}{\\delta}+\\sqrt{n})$ neurons and\n$\\widetilde{\\mathcal{O}}(\\frac{d}{\\delta}+n)$ weights are sufficient. Our\nconstruction uses Gaussian random weights only in the first layer, while all\nthe subsequent layers use binary or integer weights. We also prove new lower\nbounds by connecting memorization in neural networks to the purely geometric\nproblem of separating $n$ points on a sphere using hyperplanes.",
    "descriptor": "",
    "authors": [
      "Shashank Rajput",
      "Kartik Sreenivasan",
      "Dimitris Papailiopoulos",
      "Amin Karbasi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07724"
  },
  {
    "id": "arXiv:2106.07728",
    "title": "Targeted Data Acquisition for Evolving Negotiation Agents",
    "abstract": "Successful negotiators must learn how to balance optimizing for self-interest\nand cooperation. Yet current artificial negotiation agents often heavily depend\non the quality of the static datasets they were trained on, limiting their\ncapacity to fashion an adaptive response balancing self-interest and\ncooperation. For this reason, we find that these agents can achieve either high\nutility or cooperation, but not both. To address this, we introduce a targeted\ndata acquisition framework where we guide the exploration of a reinforcement\nlearning agent using annotations from an expert oracle. The guided exploration\nincentivizes the learning agent to go beyond its static dataset and develop new\nnegotiation strategies. We show that this enables our agents to obtain\nhigher-reward and more Pareto-optimal solutions when negotiating with both\nsimulated and human partners compared to standard supervised learning and\nreinforcement learning methods. This trend additionally holds when comparing\nagents using our targeted data acquisition framework to variants of agents\ntrained with a mix of supervised learning and reinforcement learning, or to\nagents using tailored reward functions that explicitly optimize for utility and\nPareto-optimality.",
    "descriptor": "\nComments: The Thirty-eighth International Conference on Machine Learning\n",
    "authors": [
      "Minae Kwon",
      "Siddharth Karamcheti",
      "Mariano-Florentino Cuellar",
      "Dorsa Sadigh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.07728"
  },
  {
    "id": "arXiv:2106.07731",
    "title": "Bivariate Polynomial Codes for Secure Distributed Matrix Multiplication",
    "abstract": "We consider the problem of secure distributed matrix multiplication. Coded\ncomputation has been shown to be an effective solution in distributed matrix\nmultiplication, both providing privacy against workers and boosting the\ncomputation speed by efficiently mitigating stragglers. In this work, we\npresent a non-direct secure extension of the recently introduced bivariate\npolynomial codes. Bivariate polynomial codes have been shown to be able to\nfurther speed up distributed matrix multiplication by exploiting the partial\nwork done by the stragglers rather than completely ignoring them while reducing\nthe upload communication cost and/or the workers' storage's capacity needs. We\nshow that, especially for upload communication or storage constrained settings,\nthe proposed approach reduces the average computation time of secure\ndistributed matrix multiplication compared to its competitors in the\nliterature.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2102.08304\n",
    "authors": [
      "Burak Hasircioglu",
      "Jesus Gomez-Vilardebo",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.07731"
  },
  {
    "id": "arXiv:2106.07732",
    "title": "Learning Audio-Visual Dereverberation",
    "abstract": "Reverberation from audio reflecting off surfaces and objects in the\nenvironment not only degrades the quality of speech for human perception, but\nalso severely impacts the accuracy of automatic speech recognition. Prior work\nattempts to remove reverberation based on the audio modality only. Our idea is\nto learn to dereverberate speech from audio-visual observations. The visual\nenvironment surrounding a human speaker reveals important cues about the room\ngeometry, materials, and speaker location, all of which influence the precise\nreverberation effects in the audio stream. We introduce Visually-Informed\nDereverberation of Audio (VIDA), an end-to-end approach that learns to remove\nreverberation based on both the observed sounds and visual scene. In support of\nthis new task, we develop a large-scale dataset that uses realistic acoustic\nrenderings of speech in real-world 3D scans of homes offering a variety of room\nacoustics. Demonstrating our approach on both simulated and real imagery for\nspeech enhancement, speech recognition, and speaker identification, we show it\nachieves state-of-the-art performance and substantially improves over\ntraditional audio-only methods. Project page:\nthis http URL",
    "descriptor": "",
    "authors": [
      "Changan Chen",
      "Wei Sun",
      "David Harwath",
      "Kristen Grauman"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07732"
  },
  {
    "id": "arXiv:2106.07734",
    "title": "CoDERT: Distilling Encoder Representations with Co-learning for  Transducer-based Speech Recognition",
    "abstract": "We propose a simple yet effective method to compress an RNN-Transducer\n(RNN-T) through the well-known knowledge distillation paradigm. We show that\nthe transducer's encoder outputs naturally have a high entropy and contain rich\ninformation about acoustically similar word-piece confusions. This rich\ninformation is suppressed when combined with the lower entropy decoder outputs\nto produce the joint network logits. Consequently, we introduce an auxiliary\nloss to distill the encoder logits from a teacher transducer's encoder, and\nexplore training strategies where this encoder distillation works effectively.\nWe find that tandem training of teacher and student encoders with an inplace\nencoder distillation outperforms the use of a pre-trained and static teacher\ntransducer. We also report an interesting phenomenon we refer to as implicit\ndistillation, that occurs when the teacher and student encoders share the same\ndecoder. Our experiments show 5.37-8.4% relative word error rate reductions\n(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test\nsets.",
    "descriptor": "\nComments: Accepted at InterSpeech 2021\n",
    "authors": [
      "Rupak Vignesh Swaminathan",
      "Brian King",
      "Grant P. Strimel",
      "Jasha Droppo",
      "Athanasios Mouchtaris"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07734"
  },
  {
    "id": "arXiv:2106.07742",
    "title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval  in the Archaeology Domain",
    "abstract": "The amount of archaeological literature is growing rapidly. Until recently,\nthese data were only accessible through metadata search. We implemented a text\nretrieval engine for a large archaeological text collection ($\\sim 658$ Million\nwords). In archaeological IR, domain-specific entities such as locations, time\nperiods, and artefacts, play a central role. This motivated the development of\na named entity recognition (NER) model to annotate the full collection with\narchaeological named entities. In this paper, we present ArcheoBERTje, a BERT\nmodel pre-trained on Dutch archaeological texts. We compare the model's quality\nand output on a Named Entity Recognition task to a generic multilingual model\nand a generic Dutch model. We also investigate ensemble methods for combining\nmultiple BERT models, and combining the best BERT model with a domain thesaurus\nusing Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms\nboth the multilingual and Dutch model significantly with a smaller standard\ndeviation between runs, reaching an average F1 score of 0.735. The model also\noutperforms ensemble methods combining the three models. Combining ArcheoBERTje\npredictions and explicit domain knowledge from the thesaurus did not increase\nthe F1 score. We quantitatively and qualitatively analyse the differences\nbetween the vocabulary and output of the BERT models on the full collection and\nprovide some valuable insights in the effect of fine-tuning for specific\ndomains. Our results indicate that for a highly specific text domain such as\narchaeology, further pre-training on domain-specific data increases the model's\nquality on NER by a much larger margin than shown for other domains in the\nliterature, and that domain-specific pre-training makes the addition of domain\nknowledge from a thesaurus unnecessary.",
    "descriptor": "",
    "authors": [
      "Alex Brandsen",
      "Suzan Verberne",
      "Karsten Lambers",
      "Milco Wansleeben"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07742"
  },
  {
    "id": "arXiv:2106.07744",
    "title": "Fundamentals of Partial Rejection Sampling",
    "abstract": "Partial Rejection Sampling is an algorithmic approach to obtaining a perfect\nsample from a specified distribution. The objects to be sampled are assumed to\nbe represented by a number of random variables. In contrast to classical\nrejection sampling, in which all variables are resampled until a feasible\nsolution is found, partial rejection sampling aims at greater efficiency by\nresampling only a subset of variables that `go wrong'. Partial rejection\nsampling is closely related to Moser and Tardos' algorithmic version of the\nLov\\'asz Local Lemma, but with the additional requirement that a specified\noutput distribution should be met. This article provides a largely\nself-contained account of the basic form of the algorithm and its analysis.",
    "descriptor": "",
    "authors": [
      "Mark Jerrum"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2106.07744"
  },
  {
    "id": "arXiv:2106.07750",
    "title": "Hip to Be (Latin) Square: Maximal Period Sequences from Orthogonal  Cellular Automata",
    "abstract": "Orthogonal Cellular Automata (OCA) have been recently investigated in the\nliterature as a new approach to construct orthogonal Latin squares for\ncryptographic applications such as secret sharing schemes. In this paper, we\nconsider OCA for a different cryptographic task, namely the generation of\npseudorandom sequences. The idea is to iterate a dynamical system where the\noutput of an OCA pair is fed back as a new set of coordinates on the superposed\nsquares. The main advantage is that OCA ensure a certain amount of diffusion in\nthe generated sequences, a property which is usually missing from traditional\nCA-based pseudorandom number generators. We study the problem of finding OCA\npairs with maximal period by first performing an exhaustive search up to local\nrules of diameter $d=5$, and then focusing on the subclass of linear\nbipermutive rules. In this case, we characterize the periods of the sequences\nin terms of the order of the subgroup generated by an invertible Sylvester\nmatrix. We finally devise an algorithm based on Lagrange's theorem to\nefficiently enumerate all linear OCA pairs of maximal period up to diameter\n$d=11$.",
    "descriptor": "\nComments: 16 pages, 3 figures, 2 tables\n",
    "authors": [
      "Luca Mariot"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.07750"
  },
  {
    "id": "arXiv:2106.07751",
    "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge",
    "abstract": "Non-intrusive load monitoring (NILM) helps disaggregate the household's main\nelectricity consumption to energy usages of individual appliances, thus greatly\ncutting down the cost in fine-grained household load monitoring. To address the\narisen privacy concern in NILM applications, federated learning (FL) could be\nleveraged for NILM model training and sharing. When applying the FL paradigm in\nreal-world NILM applications, however, we are faced with the challenges of edge\nresource restriction, edge model personalization and edge training data\nscarcity.\nIn this paper we present FedNILM, a practical FL paradigm for NILM\napplications at the edge client. Specifically, FedNILM is designed to deliver\nprivacy-preserving and personalized NILM services to large-scale edge clients,\nby leveraging i) secure data aggregation through federated learning, ii)\nefficient cloud model compression via filter pruning and multi-task learning,\nand iii) personalized edge model building with unsupervised transfer learning.\nOur experiments on real-world energy data show that, FedNILM is able to achieve\npersonalized energy disaggregation with the state-of-the-art accuracy, while\nensuring privacy preserving at the edge client.",
    "descriptor": "\nComments: 9 pages, 5 figures, 3 tables\n",
    "authors": [
      "Yu Zhang",
      "Guoming Tang",
      "Qianyi Huang",
      "Yi Wang",
      "Xudong Wang",
      "Jiadong Lou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07751"
  },
  {
    "id": "arXiv:2106.07752",
    "title": "Optimization-friendly generic mechanisms without money",
    "abstract": "The goal of this paper is to develop a generic framework for converting\nmodern optimization algorithms into mechanisms where inputs come from\nself-interested agents. We focus on aggregating preferences from $n$ players in\na context without money. Special cases of this setting include voting,\nallocation of items by lottery, and matching. Our key technical contribution is\na new meta-algorithm we call \\apex (Adaptive Pricing Equalizing Externalities).\nThe framework is sufficiently general to be combined with any optimization\nalgorithm that is based on local search. We outline an agenda for studying the\nalgorithm's properties and its applications. As a special case of applying the\nframework to the problem of one-sided assignment with lotteries, we obtain a\nstrengthening of the 1979 result by Hylland and Zeckhauser on allocation via a\ncompetitive equilibrium from equal incomes (CEEI). The [HZ79] result posits\nthat there is a (fractional) allocation and a set of item prices such that the\nallocation is a competitive equilibrium given prices. We further show that\nthere is always a reweighing of the players' utility values such that running\nunit-demand VCG with reweighed utilities leads to a HZ-equilibrium prices.\nInterestingly, not all HZ competitive equilibria come from VCG prices. As part\nof our proof, we re-prove the [HZ79] result using only Brouwer's fixed point\ntheorem (and not the more general Kakutani's theorem). This may be of\nindependent interest.",
    "descriptor": "",
    "authors": [
      "Mark Braverman"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.07752"
  },
  {
    "id": "arXiv:2106.07754",
    "title": "Counterfactual Explanations as Interventions in Latent Space",
    "abstract": "Explainable Artificial Intelligence (XAI) is a set of techniques that allows\nthe understanding of both technical and non-technical aspects of Artificial\nIntelligence (AI) systems. XAI is crucial to help satisfying the increasingly\nimportant demand of \\emph{trustworthy} Artificial Intelligence, characterized\nby fundamental characteristics such as respect of human autonomy, prevention of\nharm, transparency, accountability, etc. Within XAI techniques, counterfactual\nexplanations aim to provide to end users a set of features (and their\ncorresponding values) that need to be changed in order to achieve a desired\noutcome. Current approaches rarely take into account the feasibility of actions\nneeded to achieve the proposed explanations, and in particular they fall short\nof considering the causal impact of such actions. In this paper, we present\nCounterfactual Explanations as Interventions in Latent Space (CEILS), a\nmethodology to generate counterfactual explanations capturing by design the\nunderlying causal relations from the data, and at the same time to provide\nfeasible recommendations to reach the proposed profile. Moreover, our\nmethodology has the advantage that it can be set on top of existing\ncounterfactuals generator algorithms, thus minimising the complexity of\nimposing additional causal constrains. We demonstrate the effectiveness of our\napproach with a set of different experiments using synthetic and real datasets\n(including a proprietary dataset of the financial domain).",
    "descriptor": "\nComments: 34 pages, 4 figures, 4 tables\n",
    "authors": [
      "Riccardo Crupi",
      "Alessandro Castelnovo",
      "Daniele Regoli",
      "Beatriz San Miguel Gonzalez"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07754"
  },
  {
    "id": "arXiv:2106.07756",
    "title": "Counterfactual Explanations for Machine Learning: Challenges Revisited",
    "abstract": "Counterfactual explanations (CFEs) are an emerging technique under the\numbrella of interpretability of machine learning (ML) models. They provide\n``what if'' feedback of the form ``if an input datapoint were $x'$ instead of\n$x$, then an ML model's output would be $y'$ instead of $y$.'' Counterfactual\nexplainability for ML models has yet to see widespread adoption in industry. In\nthis short paper, we posit reasons for this slow uptake. Leveraging recent work\noutlining desirable properties of CFEs and our experience running the ML wing\nof a model monitoring startup, we identify outstanding obstacles hindering CFE\ndeployment in industry.",
    "descriptor": "\nComments: Presented at CHI HCXAI 2021 workshop\n",
    "authors": [
      "Sahil Verma",
      "John Dickerson",
      "Keegan Hines"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07756"
  },
  {
    "id": "arXiv:2106.07758",
    "title": "Pitfalls of Explainable ML: An Industry Perspective",
    "abstract": "As machine learning (ML) systems take a more prominent and central role in\ncontributing to life-impacting decisions, ensuring their trustworthiness and\naccountability is of utmost importance. Explanations sit at the core of these\ndesirable attributes of a ML system. The emerging field is frequently called\n``Explainable AI (XAI)'' or ``Explainable ML.'' The goal of explainable ML is\nto intuitively explain the predictions of a ML system, while adhering to the\nneeds to various stakeholders. Many explanation techniques were developed with\ncontributions from both academia and industry. However, there are several\nexisting challenges that have not garnered enough interest and serve as\nroadblocks to widespread adoption of explainable ML. In this short paper, we\nenumerate challenges in explainable ML from an industry perspective. We hope\nthese challenges will serve as promising future research directions, and would\ncontribute to democratizing explainable ML.",
    "descriptor": "\nComments: Presented at JOURNE workshop at MLSYS 2021 (this https URL)\n",
    "authors": [
      "Sahil Verma",
      "Aditya Lahiri",
      "John P. Dickerson",
      "Su-In Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07758"
  },
  {
    "id": "arXiv:2106.07760",
    "title": "RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised  Learning",
    "abstract": "Semi-supervised learning (SSL) algorithms have had great success in recent\nyears in limited labeled data regimes. However, the current state-of-the-art\nSSL algorithms are computationally expensive and entail significant compute\ntime and energy requirements. This can prove to be a huge limitation for many\nsmaller companies and academic groups. Our main insight is that training on a\nsubset of unlabeled data instead of entire unlabeled data enables the current\nSSL algorithms to converge faster, thereby reducing the computational costs\nsignificantly. In this work, we propose RETRIEVE, a coreset selection framework\nfor efficient and robust semi-supervised learning. RETRIEVE selects the coreset\nby solving a mixed discrete-continuous bi-level optimization problem such that\nthe selected coreset minimizes the labeled set loss. We use a one-step gradient\napproximation and show that the discrete optimization problem is approximately\nsubmodular, thereby enabling simple greedy algorithms to obtain the coreset. We\nempirically demonstrate on several real-world datasets that existing SSL\nalgorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve\na) faster training times, b) better performance when unlabeled data consists of\nOut-of-Distribution(OOD) data and imbalance. More specifically, we show that\nwith minimal accuracy degradation, RETRIEVE achieves a speedup of around 3X in\nthe traditional SSL setting and achieves a speedup of 5X compared to\nstate-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD\ndata.",
    "descriptor": "",
    "authors": [
      "Krishnateja Killamsetty",
      "Xujiang Zhao",
      "Feng Chen",
      "Rishabh Iyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07760"
  },
  {
    "id": "arXiv:2106.07763",
    "title": "String Diagrammatic Electrical Circuit Theory",
    "abstract": "We develop a comprehensive string diagrammatic treatment of electrical\ncircuits. Building on previous, limited case studies, we introduce controlled\nsources and meters as elements, and the impedance calculus, a powerful toolbox\nfor diagrammatic reasoning on circuit diagrams. We demonstrate the power of our\napproach by giving comprehensive proofs of several textbook results, including\nthe superposition theorem and Th\\'evenin's theorem.",
    "descriptor": "\nComments: 13 pages + appendices. Accepted for ACT2021\n",
    "authors": [
      "Guillaume Boisseau",
      "Pawe\u0142 Soboci\u0144ski"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.07763"
  },
  {
    "id": "arXiv:2106.07767",
    "title": "Improving Robustness of Graph Neural Networks with Heterophily-Inspired  Designs",
    "abstract": "Recent studies have exposed that many graph neural networks (GNNs) are\nsensitive to adversarial attacks, and can suffer from performance loss if the\ngraph structure is intentionally perturbed. A different line of research has\nshown that many GNN architectures implicitly assume that the underlying graph\ndisplays homophily, i.e., connected nodes are more likely to have similar\nfeatures and class labels, and perform poorly if this assumption is not\nfulfilled. In this work, we formalize the relation between these two seemingly\ndifferent issues. We theoretically show that in the standard scenario in which\nnode features exhibit homophily, impactful structural attacks always lead to\nincreased levels of heterophily. Then, inspired by GNN architectures that\ntarget heterophily, we present two designs -- (i) separate aggregators for ego-\nand neighbor-embeddings, and (ii) a reduced scope of aggregation -- that can\nsignificantly improve the robustness of GNNs. Our extensive empirical\nevaluations show that GNNs featuring merely these two designs can achieve\nsignificantly improved robustness compared to the best-performing unvaccinated\nmodel with 24.99% gain in average performance under targeted attacks, while\nhaving smaller computational overhead than existing defense mechanisms.\nFurthermore, these designs can be readily combined with explicit defense\nmechanisms to yield state-of-the-art robustness with up to 18.33% increase in\nperformance under attacks compared to the best-performing vaccinated model.",
    "descriptor": "\nComments: preprint with appendix; 30 pages, 1 figure\n",
    "authors": [
      "Jiong Zhu",
      "Junchen Jin",
      "Michael T. Schaub",
      "Danai Koutra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07767"
  },
  {
    "id": "arXiv:2106.07769",
    "title": "The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and  Regularization",
    "abstract": "Among the most successful methods for sparsifying deep (neural) networks are\nthose that adaptively mask the network weights throughout training. By\nexamining this masking, or dropout, in the linear case, we uncover a duality\nbetween such adaptive methods and regularization through the so-called\n\"$\\eta$-trick\" that casts both as iteratively reweighted optimizations. We show\nthat any dropout strategy that adapts to the weights in a monotonic way\ncorresponds to an effective subquadratic regularization penalty, and therefore\nleads to sparse solutions. We obtain the effective penalties for several\npopular sparsification strategies, which are remarkably similar to classical\npenalties commonly used in sparse optimization. Considering variational dropout\nas a case study, we demonstrate similar empirical behavior between the adaptive\ndropout method and classical methods on the task of deep network\nsparsification, validating our theory.",
    "descriptor": "\nComments: 19 pages, 2 figures. Submitted to NeurIPS 2021\n",
    "authors": [
      "Daniel LeJeune",
      "Hamid Javadi",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07769"
  },
  {
    "id": "arXiv:2106.07770",
    "title": "Potato Crop Stress Identification in Aerial Images using Deep  Learning-based Object Detection",
    "abstract": "Recent research on the application of remote sensing and deep learning-based\nanalysis in precision agriculture demonstrated a potential for improved crop\nmanagement and reduced environmental impacts of agricultural production.\nDespite the promising results, the practical relevance of these technologies\nfor actual field deployment requires novel algorithms that are customized for\nanalysis of agricultural images and robust to implementation on natural field\nimagery. The paper presents an approach for analyzing aerial images of a potato\ncrop using deep neural networks. The main objective is to demonstrate automated\nspatial recognition of a healthy versus stressed crop at a plant level.\nSpecifically, we examine premature plant senescence resulting in drought stress\non Russet Burbank potato plants. The proposed deep learning model, named\nRetina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes\nconnections from low-level semantic dense representation maps to the feature\npyramid network. The paper also introduces a dataset of field images acquired\nwith a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.\nExperimental validation demonstrated the ability for distinguishing healthy and\nstressed plants in field images, achieving an average Dice score coefficient of\n0.74. A comparison to related state-of-the-art deep learning models for object\ndetection revealed that the presented approach is effective for the task at\nhand. The method applied here is conducive toward the assessment and\nrecognition of potato crop stress (early plant senescence resulting from\ndrought stress in this case) in natural aerial field images collected under\nreal conditions.",
    "descriptor": "\nComments: 15 pages, 4 figures\n",
    "authors": [
      "Sujata Butte",
      "Aleksandar Vakanski",
      "Kasia Duellman",
      "Haotian Wang",
      "Amin Mirkouei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07770"
  },
  {
    "id": "arXiv:2106.07771",
    "title": "Flow Guided Transformable Bottleneck Networks for Motion Retargeting",
    "abstract": "Human motion retargeting aims to transfer the motion of one person in a\n\"driving\" video or set of images to another person. Existing efforts leverage a\nlong training video from each target person to train a subject-specific motion\ntransfer model. However, the scalability of such methods is limited, as each\nmodel can only generate videos for the given target subject, and such training\nvideos are labor-intensive to acquire and process. Few-shot motion transfer\ntechniques, which only require one or a few images from a target, have recently\ndrawn considerable attention. Methods addressing this task generally use either\n2D or explicit 3D representations to transfer motion, and in doing so,\nsacrifice either accurate geometric modeling or the flexibility of an\nend-to-end learned representation. Inspired by the Transformable Bottleneck\nNetwork, which renders novel views and manipulations of rigid objects, we\npropose an approach based on an implicit volumetric representation of the image\ncontent, which can then be spatially manipulated using volumetric flow fields.\nWe address the challenging question of how to aggregate information across\ndifferent body poses, learning flow fields that allow for combining content\nfrom the appropriate regions of input images of highly non-rigid human subjects\nperforming complex motions into a single implicit volumetric representation.\nThis allows us to learn our 3D representation solely from videos of moving\npeople. Armed with both 3D object understanding and end-to-end learned\nrendering, this categorically novel representation delivers state-of-the-art\nimage generation quality, as shown by our quantitative and qualitative\nevaluations.",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Jian Ren",
      "Menglei Chai",
      "Oliver J. Woodford",
      "Kyle Olszewski",
      "Sergey Tulyakov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07771"
  },
  {
    "id": "arXiv:2106.07778",
    "title": "fybrrLink: Efficient QoS-aware Routing in SDN enabled Next-Gen Satellite  Networks",
    "abstract": "Providing high-speed Internet service using satellite network has attracted\nresearchers from both academia and industry mainly due to the characteristics\nof Low Earth Orbit (LEO) satellite networks such as global coverage,\nscalability, and lower transmission delay. With the recent advancements in the\nSoftware-Defined Network (SDN), implementation of SDN in Non-Terrestrial\nNetworks (NTN) can help to achieve the set goals for 5G and beyond networks.\nSince satellite networks have a distinct architecture, some of the traditional\nprotocols no longer remain useful. Therefore, to satisfy the diverse Quality of\nService (QoS) requirements for a variety of applications, we propose a novel\nand centralized QoS-aware routing algorithm, called fybrrLink in which the\nglobal view of the network in SDN is utilized. We implement a modified\nBresenham's algorithm and Dijkstra's algorithm to find the optimal path in a\nsignificantly reduced computation time. Also, taking advantage of the\ndeterministic satellite constellation, we propose a flow rule transfer\nalgorithm and a topology monitoring algorithm. Further, fybrrLink is evaluated\nwith multiple NS3 simulations, and results confirm its supremacy over other\nstate-of-the-art algorithms.",
    "descriptor": "",
    "authors": [
      "Prashant Kumar",
      "Saksham Bhushan",
      "Debajyoti Halder",
      "Anand M. Baswade"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.07778"
  },
  {
    "id": "arXiv:2106.07779",
    "title": "Boosting in the Presence of Massart Noise",
    "abstract": "We study the problem of boosting the accuracy of a weak learner in the\n(distribution-independent) PAC model with Massart noise. In the Massart noise\nmodel, the label of each example $x$ is independently misclassified with\nprobability $\\eta(x) \\leq \\eta$, where $\\eta<1/2$. The Massart model lies\nbetween the random classification noise model and the agnostic model. Our main\npositive result is the first computationally efficient boosting algorithm in\nthe presence of Massart noise that achieves misclassification error arbitrarily\nclose to $\\eta$. Prior to our work, no non-trivial booster was known in this\nsetting. Moreover, we show that this error upper bound is best possible for\npolynomial-time black-box boosters, under standard cryptographic assumptions.\nOur upper and lower bounds characterize the complexity of boosting in the\ndistribution-independent PAC model with Massart noise. As a simple application\nof our positive result, we give the first efficient Massart learner for unions\nof high-dimensional rectangles.",
    "descriptor": "",
    "authors": [
      "Ilias Diakonikolas",
      "Russell Impagliazzo",
      "Daniel Kane",
      "Rex Lei",
      "Jessica Sorrell",
      "Christos Tzamos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07779"
  },
  {
    "id": "arXiv:2106.07780",
    "title": "KL Guided Domain Adaptation",
    "abstract": "Domain adaptation is an important problem and often needed for real-world\napplications. In this problem, instead of i.i.d. datapoints, we assume that the\nsource (training) data and the target (testing) data have different\ndistributions. With that setting, the empirical risk minimization training\nprocedure often does not perform well, since it does not account for the change\nin the distribution. A common approach in the domain adaptation literature is\nto learn a representation of the input that has the same distributions over the\nsource and the target domain. However, these approaches often require\nadditional networks and/or optimizing an adversarial (minimax) objective, which\ncan be very expensive or unstable in practice. To tackle this problem, we first\nderive a generalization bound for the target loss based on the training loss\nand the reverse Kullback-Leibler (KL) divergence between the source and the\ntarget representation distributions. Based on this bound, we derive an\nalgorithm that minimizes the KL term to obtain a better generalization to the\ntarget domain. We show that with a probabilistic representation network, the KL\nterm can be estimated efficiently via minibatch samples without any additional\nnetwork or a minimax objective. This leads to a theoretically sound alignment\nmethod which is also very efficient and stable in practice. Experimental\nresults also suggest that our method outperforms other representation-alignment\napproaches.",
    "descriptor": "",
    "authors": [
      "A. Tuan Nguyen",
      "Toan Tran",
      "Yarin Gal",
      "Philip H. S. Torr",
      "At\u0131l\u0131m G\u00fcne\u015f Baydin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07780"
  },
  {
    "id": "arXiv:2106.07781",
    "title": "On Declare MAX-SAT and a finite Herbrand Base for data-aware logs",
    "abstract": "This technical report provides some lightweight introduction motivating the\ndefinition of an alignment of log traces against Data-Aware Declare Models\npotentially containing correlation conditions. This technical report is only\nproviding the intuition of the logical framework as a feasibility study for a\nfuture formalization and experiment section.",
    "descriptor": "",
    "authors": [
      "Giacomo Bergami"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.07781"
  },
  {
    "id": "arXiv:2106.07785",
    "title": "Multivariate Public Key Cryptosystemfrom Sidon Spaces",
    "abstract": "A Sidon space is a subspace of an extension field over a base field in which\nthe product of any two elements can be factored uniquely, up to constants. This\npaper proposes a new public-key cryptosystem of the multivariate type which is\nbased on Sidon spaces, and has the potential to remain secure even if quantum\nsupremacy is attained. This system, whose security relies on the hardness of\nthe well-known MinRankproblem, is shown to be resilient to several\nstraightforward algebraic attacks. In particular, it is proved that the two\npopular attacks on the MinRank problem, the kernel attack, and the minor\nattack, succeed only with exponentially small probability. The system is\nimplemented in software, and its hardness is demonstrated experimentally.",
    "descriptor": "",
    "authors": [
      "Netanel Raviv",
      "Ben Langton",
      "Itzhak Tamo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.07785"
  },
  {
    "id": "arXiv:2106.07787",
    "title": "Tracing Back Music Emotion Predictions to Sound Sources and Intuitive  Perceptual Qualities",
    "abstract": "Music emotion recognition is an important task in MIR (Music Information\nRetrieval) research. Owing to factors like the subjective nature of the task\nand the variation of emotional cues between musical genres, there are still\nsignificant challenges in developing reliable and generalizable models. One\nimportant step towards better models would be to understand what a model is\nactually learning from the data and how the prediction for a particular input\nis made. In previous work, we have shown how to derive explanations of model\npredictions in terms of spectrogram image segments that connect to the\nhigh-level emotion prediction via a layer of easily interpretable perceptual\nfeatures. However, that scheme lacks intuitive musical comprehensibility at the\nspectrogram level. In the present work, we bridge this gap by merging audioLIME\n-- a source-separation based explainer -- with mid-level perceptual features,\nthus forming an intuitive connection chain between the input audio and the\noutput emotion predictions. We demonstrate the usefulness of this method by\napplying it to debug a biased emotion prediction model.",
    "descriptor": "\nComments: Sound and Music Computing Conference 2021\n",
    "authors": [
      "Shreyan Chowdhury",
      "Verena Praher",
      "Gerhard Widmer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07787"
  },
  {
    "id": "arXiv:2106.07791",
    "title": "DFM: A Performance Baseline for Deep Feature Matching",
    "abstract": "A novel image matching method is proposed that utilizes learned features\nextracted by an off-the-shelf deep neural network to obtain a promising\nperformance. The proposed method uses pre-trained VGG architecture as a feature\nextractor and does not require any additional training specific to improve\nmatching. Inspired by well-established concepts in the psychology area, such as\nthe Mental Rotation paradigm, an initial warping is performed as a result of a\npreliminary geometric transformation estimate. These estimates are simply based\non dense matching of nearest neighbors at the terminal layer of VGG network\noutputs of the images to be matched. After this initial alignment, the same\napproach is repeated again between reference and aligned images in a\nhierarchical manner to reach a good localization and matching performance. Our\nalgorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching\nAccuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches\ndataset, which indicates a better performance than the state-of-the-art.",
    "descriptor": "\nComments: CVPR 2021 Image Matching Workshop Camera Ready Version\n",
    "authors": [
      "Ufuk Efe",
      "Kutalmis Gokalp Ince",
      "A. Aydin Alatan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07791"
  },
  {
    "id": "arXiv:2106.07794",
    "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect  Transcripts",
    "abstract": "This work explores constituency parsing on automatically recognized\ntranscripts of conversational speech. The neural parser is based on a sentence\nencoder that leverages word vectors contextualized with prosodic features,\njointly learning prosodic feature extraction with parsing. We assess the\nutility of the prosody in parsing on imperfect transcripts, i.e. transcripts\nwith automatic speech recognition (ASR) errors, by applying the parser in an\nN-best reranking framework. In experiments on Switchboard, we obtain 13-15% of\nthe oracle N-best gain relative to parsing the 1-best ASR output, with\ninsignificant impact on word recognition error rate. Prosody provides a\nsignificant part of the gain, and analyses suggest that it leads to more\ngrammatical utterances via recovering function words.",
    "descriptor": "\nComments: Interspeech 2021\n",
    "authors": [
      "Trang Tran",
      "Mari Ostendorf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07794"
  },
  {
    "id": "arXiv:2106.07798",
    "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution  Triggers",
    "abstract": "In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.",
    "descriptor": "\nComments: 4 pages, 1 figure, Published at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\n",
    "authors": [
      "Chace Ashcraft",
      "Kiran Karra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.07798"
  },
  {
    "id": "arXiv:2106.07799",
    "title": "Launching into clinical space with medspaCy: a new clinical text  processing toolkit in Python",
    "abstract": "Despite impressive success of machine learning algorithms in clinical natural\nlanguage processing (cNLP), rule-based approaches still have a prominent role.\nIn this paper, we introduce medspaCy, an extensible, open-source cNLP library\nbased on spaCy framework that allows flexible integration of rule-based and\nmachine learning-based algorithms adapted to clinical text. MedspaCy includes a\nvariety of components that meet common cNLP needs such as context analysis and\nmapping to standard terminologies. By utilizing spaCy's clear and easy-to-use\nconventions, medspaCy enables development of custom pipelines that integrate\neasily with other spaCy-based modules. Our toolkit includes several core\ncomponents and facilitates rapid development of pipelines for clinical text.",
    "descriptor": "\nComments: Accepted to AMIA Annual Symposium 2021\n",
    "authors": [
      "Hannah Eyre",
      "Alec B Chapman",
      "Kelly S Peterson",
      "Jianlin Shi",
      "Patrick R Alba",
      "Makoto M Jones",
      "Tamara L Box",
      "Scott L DuVall",
      "Olga V Patterson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07799"
  },
  {
    "id": "arXiv:2106.07803",
    "title": "SynthASR: Unlocking Synthetic Data for Speech Recognition",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have recently\ndemonstrated superior performance over the traditional hybrid ASR models.\nTraining an E2E ASR model requires a large amount of data which is not only\nexpensive but may also raise dependency on production data. At the same time,\nsynthetic speech generated by the state-of-the-art text-to-speech (TTS) engines\nhas advanced to near-human naturalness. In this work, we propose to utilize\nsynthetic speech for ASR training (SynthASR) in applications where data is\nsparse or hard to get for ASR model training. In addition, we apply continual\nlearning with a novel multi-stage training strategy to address catastrophic\nforgetting, achieved by a mix of weighted multi-style training, data\naugmentation, encoder freezing, and parameter regularization. In our\nexperiments conducted on in-house datasets for a new application of recognizing\nmedication names, training ASR RNN-T models with synthetic audio via the\nproposed multi-stage training improved the recognition performance on new\napplication by more than 65% relative, without degradation on existing general\napplications. Our observations show that SynthASR holds great promise in\ntraining the state-of-the-art large-scale E2E ASR models for new applications\nwhile reducing the costs and dependency on production data.",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Amin Fazel",
      "Wei Yang",
      "Yulan Liu",
      "Roberto Barra-Chicote",
      "Yixiong Meng",
      "Roland Maas",
      "Jasha Droppo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07803"
  },
  {
    "id": "arXiv:2106.07804",
    "title": "Controlling Neural Networks with Rule Representations",
    "abstract": "We propose a novel training method to integrate rules into deep learning, in\na way their strengths are controllable at inference. Deep Neural Networks with\nControllable Rule Representations (DeepCTRL) incorporates a rule encoder into\nthe model coupled with a rule-based objective, enabling a shared representation\nfor decision making. DeepCTRL is agnostic to data type and model architecture.\nIt can be applied to any kind of rule defined for inputs and outputs. The key\naspect of DeepCTRL is that it does not require retraining to adapt the rule\nstrength -- at inference, the user can adjust it based on the desired operation\npoint on accuracy vs. rule verification ratio. In real-world domains where\nincorporating rules is critical -- such as Physics, Retail and Healthcare -- we\nshow the effectiveness of DeepCTRL in teaching rules for deep learning.\nDeepCTRL improves the trust and reliability of the trained models by\nsignificantly increasing their rule verification ratio, while also providing\naccuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use\ncases such as hypothesis testing of the rules on data samples, and unsupervised\nadaptation based on shared rules between datasets.",
    "descriptor": "",
    "authors": [
      "Sungyong Seo",
      "Sercan O. Arik",
      "Jinsung Yoon",
      "Xiang Zhang",
      "Kihyuk Sohn",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07804"
  },
  {
    "id": "arXiv:2106.07807",
    "title": "Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with  Unlabeled Data",
    "abstract": "Most existing works in few-shot learning rely on meta-learning the network on\na large base dataset which is typically from the same domain as the target\ndataset. We tackle the problem of cross-domain few-shot learning where there is\na large shift between the base and target domain. The problem of cross-domain\nfew-shot recognition with unlabeled target data is largely unaddressed in the\nliterature. STARTUP was the first method that tackles this problem using\nself-training. However, it uses a fixed teacher pretrained on a labeled base\ndataset to create soft labels for the unlabeled target samples. As the base\ndataset and unlabeled dataset are from different domains, projecting the target\nimages in the class-domain of the base dataset with a fixed pretrained model\nmight be sub-optimal. We propose a simple dynamic distillation-based approach\nto facilitate unlabeled images from the novel/base dataset. We impose\nconsistency regularization by calculating predictions from the weakly-augmented\nversions of the unlabeled images from a teacher network and matching it with\nthe strongly augmented versions of the same images from a student network. The\nparameters of the teacher network are updated as exponential moving average of\nthe parameters of the student network. We show that the proposed network learns\nrepresentation that can be easily adapted to the target domain even though it\nhas not been trained with target-specific classes during the pretraining phase.\nOur model outperforms the current state-of-the art method by 4.4% for 1-shot\nand 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows\ncompetitive performance on traditional in-domain few-shot learning task. Our\ncode will be available at: https://github.com/asrafulashiq/dynamic-cdfsl.",
    "descriptor": "",
    "authors": [
      "Ashraful Islam",
      "Chun-Fu Chen",
      "Rameswar Panda",
      "Leonid Karlinsky",
      "Rogerio Feris",
      "Richard J. Radke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07807"
  },
  {
    "id": "arXiv:2106.07813",
    "title": "To Infinity and Beyond! Accessibility is the Future for Kids' Search  Engines",
    "abstract": "Research in the area of search engines for children remains in its infancy.\nSeminal works have studied how children use mainstream search engines, as well\nas how to design and evaluate custom search engines explicitly for children.\nThese works, however, tend to take a one-size-fits-all view, treating children\nas a unit. Nevertheless, even at the same age, children are known to possess\nand exhibit different capabilities. These differences affect how children\naccess and use search engines. To better serve children, in this vision paper,\nwe spotlight accessibility and discuss why current research on children and\nsearch engines does not, but should, focus on this significant matter.",
    "descriptor": "\nComments: In the proceeding of IR for Children 2000-2020: Where Are We Now? (this https URL) -- Workshop co-located with the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval\n",
    "authors": [
      "Ashlee Milton",
      "Garrett Allen",
      "Maria Soledad Pera"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.07813"
  },
  {
    "id": "arXiv:2106.07814",
    "title": "Sample Efficient Reinforcement Learning In Continuous State Spaces: A  Perspective Beyond Linearity",
    "abstract": "Reinforcement learning (RL) is empirically successful in complex nonlinear\nMarkov decision processes (MDPs) with continuous state spaces. By contrast, the\nmajority of theoretical RL literature requires the MDP to satisfy some form of\nlinear structure, in order to guarantee sample efficient RL. Such efforts\ntypically assume the transition dynamics or value function of the MDP are\ndescribed by linear functions of the state features. To resolve this\ndiscrepancy between theory and practice, we introduce the Effective Planning\nWindow (EPW) condition, a structural condition on MDPs that makes no linearity\nassumptions. We demonstrate that the EPW condition permits sample efficient RL,\nby providing an algorithm which provably solves MDPs satisfying this condition.\nOur algorithm requires minimal assumptions on the policy class, which can\ninclude multi-layer neural networks with nonlinear activation functions.\nNotably, the EPW condition is directly motivated by popular gaming benchmarks,\nand we show that many classic Atari games satisfy this condition. We\nadditionally show the necessity of conditions like EPW, by demonstrating that\nsimple MDPs with slight nonlinearities cannot be solved sample efficiently.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Dhruv Malik",
      "Aldo Pacchiano",
      "Vishwak Srinivasan",
      "Yuanzhi Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07814"
  },
  {
    "id": "arXiv:2106.07815",
    "title": "Locally Differentially Private Frequency Estimation",
    "abstract": "We present two new local differentially private algorithms for frequency\nestimation. One solves the fundamental frequency oracle problem; the other\nsolves the well-known heavy hitters identification problem. Consistent with\nprior art, these are randomized algorithms. As a function of failure\nprobability~$\\beta$, the former achieves optimal worst-case estimation error\nfor every~$\\beta$, while the latter is optimal when~$\\beta$ is at least inverse\npolynomial in~$n$, the number of users. In both algorithms, server running time\nis~$\\tilde{O}(n)$ while user running time is~$\\tilde{O}(1)$. Our\nfrequency-oracle algorithm achieves lower estimation error than the prior works\nof Bassily et al. (NeurIPS 2017). On the other hand, our heavy hitters\nidentification method is as easily implementable as as TreeHist (Bassily et\nal., 2017) and has superior worst-case error, by a factor of $\\Omega(\\sqrt{\\log\nn})$.",
    "descriptor": "",
    "authors": [
      "Hao Wu",
      "Anthony Wirth"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.07815"
  },
  {
    "id": "arXiv:2106.07817",
    "title": "Efficient Facial Expression Analysis For Dimensional Affect Recognition  Using Geometric Features",
    "abstract": "Despite their continued popularity, categorical approaches to affect\nrecognition have limitations, especially in real-life situations. Dimensional\nmodels of affect offer important advantages for the recognition of subtle\nexpressions and more fine-grained analysis. We introduce a simple but effective\nfacial expression analysis (FEA) system for dimensional affect, solely based on\ngeometric features and Partial Least Squares (PLS) regression. The system\njointly learns to estimate Arousal and Valence ratings from a set of facial\nimages. The proposed approach is robust, efficient, and exhibits comparable\nperformance to contemporary deep learning models, while requiring a fraction of\nthe computational resources.",
    "descriptor": "",
    "authors": [
      "Vassilios Vonikakis",
      "Stefan Winkler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.07817"
  },
  {
    "id": "arXiv:2106.07820",
    "title": "On Large-Cohort Training for Federated Learning",
    "abstract": "Federated learning methods typically learn a model by iteratively sampling\nupdates from a population of clients. In this work, we explore how the number\nof clients sampled at each round (the cohort size) impacts the quality of the\nlearned model and the training dynamics of federated learning algorithms. Our\nwork poses three fundamental questions. First, what challenges arise when\ntrying to scale federated learning to larger cohorts? Second, what parallels\nexist between cohort sizes in federated learning and batch sizes in centralized\nlearning? Last, how can we design federated learning methods that effectively\nutilize larger cohort sizes? We give partial answers to these questions based\non extensive empirical evaluation. Our work highlights a number of challenges\nstemming from the use of larger cohorts. While some of these (such as\ngeneralization issues and diminishing returns) are analogs of large-batch\ntraining challenges, others (including training failures and fairness concerns)\nare unique to federated learning.",
    "descriptor": "",
    "authors": [
      "Zachary Charles",
      "Zachary Garrett",
      "Zhouyuan Huo",
      "Sergei Shmulyian",
      "Virginia Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.07820"
  },
  {
    "id": "arXiv:2106.07822",
    "title": "Canonical Face Embeddings",
    "abstract": "We present evidence that many common convolutional neural networks (CNNs)\ntrained for face verification learn functions that are nearly equivalent under\nrotation. More specifically, we demonstrate that one face verification model's\nembeddings (i.e. last--layer activations) can be compared directly to another\nmodel's embeddings after only a rotation or linear transformation, with little\nperformance penalty. This finding is demonstrated using IJB-C 1:1 verification\nacross the combinations of ten modern off-the-shelf CNN-based face verification\nmodels which vary in training dataset, CNN architecture, way of using angular\nloss, or some combination of the 3, and achieve a mean true accept rate of 0.96\nat a false accept rate of 0.01. When instead evaluating embeddings generated\nfrom two CNNs, where one CNN's embeddings are mapped with a linear\ntransformation, the mean true accept rate drops to 0.95 using the same\nverification paradigm. Restricting these linear maps to only perform rotation\nproduces a mean true accept rate of 0.91. These mappings' existence suggests\nthat a common representation is learned by models with variation in training or\nstructure. A discovery such as this likely has broad implications, and we\nprovide an application in which face embeddings can be de-anonymized using a\nlimited number of samples.",
    "descriptor": "\nComments: 10 pages, 6 figures, 2 tables\n",
    "authors": [
      "David McNeely-White",
      "Ben Sattelberg",
      "Nathaniel Blanchard",
      "Ross Beveridge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07822"
  },
  {
    "id": "arXiv:2106.07823",
    "title": "Challenges and Considerations with Code-Mixed NLP for Multilingual  Societies",
    "abstract": "Multilingualism refers to the high degree of proficiency in two or more\nlanguages in the written and oral communication modes. It often results in\nlanguage mixing, a.k.a. code-mixing, when a multilingual speaker switches\nbetween multiple languages in a single utterance of a text or speech. This\npaper discusses the current state of the NLP research, limitations, and\nforeseeable pitfalls in addressing five real-world applications for social good\ncrisis management, healthcare, political campaigning, fake news, and hate\nspeech for multilingual societies. We also propose futuristic datasets, models,\nand tools that can significantly advance the current research in multilingual\nNLP applications for the societal good. As a representative example, we\nconsider English-Hindi code-mixing but draw similar inferences for other\nlanguage pairs",
    "descriptor": "",
    "authors": [
      "Vivek Srivastava",
      "Mayank Singh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07823"
  },
  {
    "id": "arXiv:2106.07824",
    "title": "Communicating Natural Programs to Humans and Machines",
    "abstract": "The Abstraction and Reasoning Corpus (ARC) is a set of tasks that tests an\nagent's ability to flexibly solve novel problems. While most ARC tasks are easy\nfor humans, they are challenging for state-of-the-art AI. How do we build\nintelligent systems that can generalize to novel situations and understand\nhuman instructions in domains such as ARC? We posit that the answer may be\nfound by studying how humans communicate to each other in solving these tasks.\nWe present LARC, the Language-annotated ARC: a collection of natural language\ndescriptions by a group of human participants, unfamiliar both with ARC and\nwith each other, who instruct each other on how to solve ARC tasks. LARC\ncontains successful instructions for 88\\% of the ARC tasks. We analyze the\ncollected instructions as `natural programs', finding that most natural program\nconcepts have analogies in typical computer programs. However, unlike how one\nprecisely programs a computer, we find that humans both anticipate and exploit\nambiguities to communicate effectively. We demonstrate that a state-of-the-art\nprogram synthesis technique, which leverages the additional language\nannotations, outperforms its language-free counterpart.",
    "descriptor": "\nComments: equal contributions: (author 3, 4), (author 5,6,7)\n",
    "authors": [
      "Samuel Acquaviva",
      "Yewen Pu",
      "Marta Kryven",
      "Catherine Wong",
      "Gabrielle E Ecanow",
      "Maxwell Nye",
      "Theodoros Sechopoulos",
      "Michael Henry Tessler",
      "Joshua B. Tenenbaum"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07824"
  },
  {
    "id": "arXiv:2106.07825",
    "title": "Site-Agnostic 3D Dose Distribution Prediction with Deep Learning Neural  Networks",
    "abstract": "Typically, the current dose prediction models are limited to small amounts of\ndata and require re-training for a specific site, often leading to suboptimal\nperformance. We propose a site-agnostic, 3D dose distribution prediction model\nusing deep learning that can leverage data from any treatment site, thus\nincreasing the total data available to train the model. Applying our proposed\nmodel to a new target treatment site requires only a brief fine-tuning of the\nmodel to the new data and involves no modifications to the model input channels\nor its parameters. Thus, it can be efficiently adapted to a different treatment\nsite, even with a small training dataset.",
    "descriptor": "",
    "authors": [
      "Maryam Mashayekhi",
      "Itzel Ramirez Tapia",
      "Anjali Balagopal",
      "Xinran Zhong",
      "Azar Sadeghnejad Barkousaraie",
      "Rafe McBeth",
      "Mu-Han Lin",
      "Steve Jiang",
      "Dan Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.07825"
  },
  {
    "id": "arXiv:2106.07827",
    "title": "Improving the compromise between accuracy, interpretability and  personalization of rule-based machine learning in medical problems",
    "abstract": "One of the key challenges when developing a predictive model is the\ncapability to describe the domain knowledge and the cause-effect relationships\nin a simple way. Decision rules are a useful and important methodology in this\ncontext, justifying their application in several areas, in particular in\nclinical practice. Several machine-learning classifiers have exploited the\nadvantageous properties of decision rules to build intelligent prediction\nmodels, namely decision trees and ensembles of trees (ETs). However, such\nmethodologies usually suffer from a trade-off between interpretability and\npredictive performance. Some procedures consider a simplification of ETs, using\nheuristic approaches to select an optimal reduced set of decision rules. In\nthis paper, we introduce a novel step to those methodologies. We create a new\ncomponent to predict if a given rule will be correct or not for a particular\npatient, which introduces personalization into the procedure. Furthermore, the\nvalidation results using three public clinical datasets show that it also\nallows to increase the predictive performance of the selected set of rules,\nimproving the mentioned trade-off.",
    "descriptor": "",
    "authors": [
      "Francisco Valente",
      "Simao Paredes",
      "Jorge Henriques"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07827"
  },
  {
    "id": "arXiv:2106.07830",
    "title": "On the Convergence of Deep Learning with Differential Privacy",
    "abstract": "In deep learning with differential privacy (DP), the neural network achieves\nthe privacy usually at the cost of slower convergence (and thus lower\nperformance) than its non-private counterpart. This work gives the first\nconvergence analysis of the DP deep learning, through the lens of training\ndynamics and the neural tangent kernel (NTK). Our convergence theory\nsuccessfully characterizes the effects of two key components in the DP\ntraining: the per-sample clipping (flat or layerwise) and the noise addition.\nOur analysis not only initiates a general principled framework to understand\nthe DP deep learning with any network architecture and loss function, but also\nmotivates a new clipping method -- the global clipping, that significantly\nimproves the convergence while preserving the same privacy guarantee as the\nexisting local clipping.\nIn terms of theoretical results, we establish the precise connection between\nthe per-sample clipping and NTK matrix. We show that in the gradient flow,\ni.e., with infinitesimal learning rate, the noise level of DP optimizers does\nnot affect the convergence. We prove that DP gradient descent (GD) with global\nclipping guarantees the monotone convergence to zero loss, which can be\nviolated by the existing DP-GD with local clipping. Notably, our analysis\nframework easily extends to other optimizers, e.g., DP-Adam. Empirically\nspeaking, DP optimizers equipped with global clipping perform strongly on a\nwide range of classification and regression tasks. In particular, our global\nclipping is surprisingly effective at learning calibrated classifiers, in\ncontrast to the existing DP classifiers which are oftentimes over-confident and\nunreliable. Implementation-wise, the new clipping can be realized by adding one\nline of code into the Opacus library.",
    "descriptor": "",
    "authors": [
      "Zhiqi Bu",
      "Hua Wang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07830"
  },
  {
    "id": "arXiv:2106.07831",
    "title": "Efficient Asynchronous Byzantine Agreement without Private Setups",
    "abstract": "For asynchronous binary agreement (ABA) with optimal resilience, prior\nprivate-setup free protocols (Cachin et al., CCS' 2002; Kokoris-Kogias et al.,\nCCS' 2020) incur $O({\\lambda}n^4)$ bits and $O(n^3)$ messages; for asynchronous\nmulti-valued agreement with external validity (VBA), Abraham et al. [2] very\nrecently gave the first elegant construction with $O(n^3)$ messages, relying on\npublic key infrastructure (PKI), but still costs $O({\\lambda} n^3 \\log n)$\nbits. We for the first time close the remaining efficiency gap, i.e., reducing\ntheir communication to $O({\\lambda} n^3)$ bits on average. At the core of our\ndesign, we give a systematic treatment of reasonably fair common randomness:\n- We construct a reasonably fair common coin (Canetti and Rabin, STOC' 1993)\nin the asynchronous setting with PKI instead of private setup, using only\n$O({\\lambda} n^3)$ bit and constant asynchronous rounds. The common coin\nprotocol ensures that with at least 1/3 probability, all honest parties can\noutput a common bit that is as if uniformly sampled, rendering a more efficient\nprivate-setup free ABA with expected $O({\\lambda} n^3)$ bit communication and\nconstant running time.\n- More interestingly, we lift our reasonably fair common coin protocol to\nattain perfect agreement without incurring any extra factor in the asymptotic\ncomplexities, resulting in an efficient reasonably fair leader election\nprimitive pluggable in all existing VBA protocols, thus reducing the\ncommunication of private-setup free VBA to expected $O({\\lambda} n^3)$ bits\nwhile preserving expected constant running time.\n- Along the way, we improve an important building block, asynchronous\nverifiable secret sharing by presenting a private-setup free implementation\ncosting only $O({\\lambda} n^2)$ bits in the PKI setting. By contrast, prior art\nhaving the same complexity (Backes et al., CT-RSA' 2013) has to rely on a\nprivate setup.",
    "descriptor": "",
    "authors": [
      "Yingzi Gao",
      "Yuan Lu",
      "Zhenliang Lu",
      "Qiang Tang",
      "Jing Xu",
      "Zhenfeng Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.07831"
  },
  {
    "id": "arXiv:2106.07832",
    "title": "Learning Equivariant Energy Based Models with Equivariant Stein  Variational Gradient Descent",
    "abstract": "We focus on the problem of efficient sampling and learning of probability\ndensities by incorporating symmetries in probabilistic models. We first\nintroduce Equivariant Stein Variational Gradient Descent algorithm -- an\nequivariant sampling method based on Stein's identity for sampling from\ndensities with symmetries. Equivariant SVGD explicitly incorporates symmetry\ninformation in a density through equivariant kernels which makes the resultant\nsampler efficient both in terms of sample complexity and the quality of\ngenerated samples. Subsequently, we define equivariant energy based models to\nmodel invariant densities that are learned using contrastive divergence. By\nutilizing our equivariant SVGD for training equivariant EBMs, we propose new\nways of improving and scaling up training of energy based models. We apply\nthese equivariant energy models for modelling joint densities in regression and\nclassification tasks for image datasets, many-body particle systems and\nmolecular structure generation.",
    "descriptor": "",
    "authors": [
      "Priyank Jaini",
      "Lars Holdijk",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07832"
  },
  {
    "id": "arXiv:2106.07833",
    "title": "Temporal Consistency Checks to Detect LiDAR Spoofing Attacks on  Autonomous Vehicle Perception",
    "abstract": "LiDAR sensors are used widely in Autonomous Vehicles for better perceiving\nthe environment which enables safer driving decisions. Recent work has\ndemonstrated serious LiDAR spoofing attacks with alarming consequences. In\nparticular, model-level LiDAR spoofing attacks aim to inject fake depth\nmeasurements to elicit ghost objects that are erroneously detected by 3D Object\nDetectors, resulting in hazardous driving decisions. In this work, we explore\nthe use of motion as a physical invariant of genuine objects for detecting such\nattacks. Based on this, we propose a general methodology, 3D Temporal\nConsistency Check (3D-TC2), which leverages spatio-temporal information from\nmotion prediction to verify objects detected by 3D Object Detectors. Our\npreliminary design and implementation of a 3D-TC2 prototype demonstrates very\npromising performance, providing more than 98% attack detection rate with a\nrecall of 91% for detecting spoofed Vehicle (Car) objects, and is able to\nachieve real-time detection at 41Hz",
    "descriptor": "\nComments: Accepted in 1st Workshop on Security and Privacy for Mobile AI (MAISP 2021)\n",
    "authors": [
      "Chengzeng You",
      "Zhongyuan Hau",
      "Soteris Demetriou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07833"
  },
  {
    "id": "arXiv:2106.07836",
    "title": "Improved Regret Bounds for Online Submodular Maximization",
    "abstract": "In this paper, we consider an online optimization problem over $T$ rounds\nwhere at each step $t\\in[T]$, the algorithm chooses an action $x_t$ from the\nfixed convex and compact domain set $\\mathcal{K}$. A utility function\n$f_t(\\cdot)$ is then revealed and the algorithm receives the payoff $f_t(x_t)$.\nThis problem has been previously studied under the assumption that the\nutilities are adversarially chosen monotone DR-submodular functions and\n$\\mathcal{O}(\\sqrt{T})$ regret bounds have been derived. We first characterize\nthe class of strongly DR-submodular functions and then, we derive regret bounds\nfor the following new online settings: $(1)$ $\\{f_t\\}_{t=1}^T$ are monotone\nstrongly DR-submodular and chosen adversarially, $(2)$ $\\{f_t\\}_{t=1}^T$ are\nmonotone submodular (while the average $\\frac{1}{T}\\sum_{t=1}^T f_t$ is\nstrongly DR-submodular) and chosen by an adversary but they arrive in a\nuniformly random order, $(3)$ $\\{f_t\\}_{t=1}^T$ are drawn i.i.d. from some\nunknown distribution $f_t\\sim \\mathcal{D}$ where the expected function\n$f(\\cdot)=\\mathbb{E}_{f_t\\sim\\mathcal{D}}[f_t(\\cdot)]$ is monotone\nDR-submodular. For $(1)$, we obtain the first logarithmic regret bounds. In\nterms of the second framework, we show that it is possible to obtain similar\nlogarithmic bounds with high probability. Finally, for the i.i.d. model, we\nprovide algorithms with $\\tilde{\\mathcal{O}}(\\sqrt{T})$ stochastic regret\nbound, both in expectation and with high probability. Experimental results\ndemonstrate that our algorithms outperform the previous techniques in the\naforementioned three settings.",
    "descriptor": "",
    "authors": [
      "Omid Sadeghi",
      "Prasanna Raut",
      "Maryam Fazel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07836"
  },
  {
    "id": "arXiv:2106.07837",
    "title": "A Survey on Mining and Analysis of Uncertain Graphs",
    "abstract": "\\emph{Uncertain Graph} (also known as \\emph{Probabilistic Graph}) is a\ngeneric model to represent many real\\mbox{-}world networks from social to\nbiological. In recent times analysis and mining of uncertain graphs have drawn\nsignificant attention from the researchers of the data management community.\nSeveral noble problems have been introduced and efficient methodologies have\nbeen developed to solve those problems. Hence, there is a need to summarize the\nexisting results on this topic in a self\\mbox{-}organized way. In this paper,\nwe present a comprehensive survey on uncertain graph mining focusing on mainly\nthree aspects: (i) different problems studied, (ii) computational challenges\nfor solving those problems, and (iii) proposed methodologies. Finally, we list\nout important future research directions.",
    "descriptor": "\nComments: 46 Pages, 2 Figures\n",
    "authors": [
      "Suman Banerjee"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.07837"
  },
  {
    "id": "arXiv:2106.07838",
    "title": "Force-Sensing Tensegrity for Investigating Physical Human-Robot  Interaction in Compliant Robotic Systems",
    "abstract": "Advancements in the domain of physical human-robot interaction (pHRI) have\ntremendously improved the ability of humans and robots to communicate,\ncollaborate, and coexist. In particular, compliant robotic systems offer many\ncharacteristics that can be leveraged towards enabling physical interactions\nthat more efficiently and intuitively communicate intent, making compliant\nsystems potentially useful in more physically demanding subsets of human-robot\ncollaborative scenarios. Tensegrity robots are an example of compliant systems\nthat are well-suited to physical interactions while still retaining useful\nrigid properties that make them practical for a variety of applications. In\nthis paper, we present the design and preliminary testing of a 6-bar spherical\ntensegrity with force-sensing capabilities. Using this prototype, we\ndemonstrate the ability of its force-sensor array to detect a variety of\nphysical interaction types that might arise in a human context. We then train\nand test a series of classifiers using data from unique and representative\ninteractions in order to demonstrate the feasibility of using this physical\nmodality of sensing to reliably communicate goals and intents from a human\noperator in a human-robot collaborative setting.",
    "descriptor": "\nComments: 7 pages, 6 figures. To be published in IEEE ICRA 2021\n",
    "authors": [
      "Andrew R. Barkan",
      "Akhil Padmanabha",
      "Sala R. Tiemann",
      "Albert Lee",
      "Matthew P. Kanter",
      "Yash S. Agarwal",
      "Alice M. Agogino"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.07838"
  },
  {
    "id": "arXiv:2106.07840",
    "title": "The subfield codes and subfield subcodes of a family of MDS codes",
    "abstract": "Maximum distance separable (MDS) codes are very important in both theory and\npractice. There is a classical construction of a family of $[2^m+1, 2u-1,\n2^m-2u+3]$ MDS codes for $1 \\leq u \\leq 2^{m-1}$, which are cyclic, reversible\nand BCH codes over $\\mathrm{GF}(2^m)$. The objective of this paper is to study\nthe quaternary subfield subcodes and quaternary subfield codes of a subfamily\nof the MDS codes for even $m$. A family of quaternary cyclic codes is obtained.\nThese quaternary codes are distance-optimal in some cases and very good in\ngeneral. Furthermore, infinite families of $3$-designs from these quaternary\ncodes are presented.",
    "descriptor": "",
    "authors": [
      "Chunming Tang",
      "Qi Wang",
      "Cunsheng Ding"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2106.07840"
  },
  {
    "id": "arXiv:2106.07841",
    "title": "Randomized Exploration for Reinforcement Learning with General Value  Function Approximation",
    "abstract": "We propose a model-free reinforcement learning algorithm inspired by the\npopular randomized least squares value iteration (RLSVI) algorithm as well as\nthe optimism principle. Unlike existing upper-confidence-bound (UCB) based\napproaches, which are often computationally intractable, our algorithm drives\nexploration by simply perturbing the training data with judiciously chosen\ni.i.d. scalar noises. To attain optimistic value function estimation without\nresorting to a UCB-style bonus, we introduce an optimistic reward sampling\nprocedure. When the value functions can be represented by a function class\n$\\mathcal{F}$, our algorithm achieves a worst-case regret bound of\n$\\widetilde{O}(\\mathrm{poly}(d_EH)\\sqrt{T})$ where $T$ is the time elapsed, $H$\nis the planning horizon and $d_E$ is the $\\textit{eluder dimension}$ of\n$\\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a\nvariant of RLSVI, that enjoys an $\\widetilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$\nregret. We complement the theory with an empirical evaluation across known\ndifficult exploration tasks.",
    "descriptor": "\nComments: 32 page, 5 figures, in Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021\n",
    "authors": [
      "Haque Ishfaq",
      "Qiwen Cui",
      "Viet Nguyen",
      "Alex Ayoub",
      "Zhuoran Yang",
      "Zhaoran Wang",
      "Doina Precup",
      "Lin F. Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07841"
  },
  {
    "id": "arXiv:2106.07843",
    "title": "Teacher-Student MixIT for Unsupervised and Semi-supervised Speech  Separation",
    "abstract": "In this paper, we introduce a novel semi-supervised learning framework for\nend-to-end speech separation. The proposed method first uses mixtures of\nunseparated sources and the mixture invariant training (MixIT) criterion to\ntrain a teacher model. The teacher model then estimates separated sources that\nare used to train a student model with standard permutation invariant training\n(PIT). The student model can be fine-tuned with supervised data, i.e., paired\nartificial mixtures and clean speech sources, and further improved via model\ndistillation. Experiments with single and multi channel mixtures show that the\nteacher-student training resolves the over-separation problem observed in the\noriginal MixIT method. Further, the semisupervised performance is comparable to\na fully-supervised separation system trained using ten times the amount of\nsupervised data.",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Jisi Zhang",
      "Catalin Zorila",
      "Rama Doddipatla",
      "Jon Barker"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07843"
  },
  {
    "id": "arXiv:2106.07846",
    "title": "Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person  Re-Identification",
    "abstract": "Unsupervised person re-identification (Re-ID) aims to match pedestrian images\nfrom different camera views in unsupervised setting. Existing methods for\nunsupervised person Re-ID are usually built upon the pseudo labels from\nclustering. However, the quality of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated by the colors in\nimages especially in the unsupervised setting. In this paper, we propose a\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\nperson Re-ID, in which cluster structure is leveraged to guide the feature\nlearning in a properly designed asymmetric contrastive learning framework. To\nbe specific, we propose a novel cluster-level contrastive loss to help the\nsiamese network effectively mine the invariance in feature learning with\nrespect to the cluster structure within and between different data augmentation\nviews, respectively. Extensive experiments conducted on three benchmark\ndatasets demonstrate superior performance of our proposal.",
    "descriptor": "",
    "authors": [
      "Mingkun Li",
      "Chun-Guang Li",
      "Jun Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07846"
  },
  {
    "id": "arXiv:2106.07847",
    "title": "Learning Stable Classifiers by Transferring Unstable Features",
    "abstract": "We study transfer learning in the presence of spurious correlations. We\nexperimentally demonstrate that directly transferring the stable feature\nextractor learned on the source task may not eliminate these biases for the\ntarget task. However, we hypothesize that the unstable features in the source\ntask and those in the target task are directly related. By explicitly informing\nthe target classifier of the source task's unstable features, we can regularize\nthe biases in the target task. Specifically, we derive a representation that\nencodes the unstable features by contrasting different data environments in the\nsource task. On the target task, we cluster data from this representation, and\nachieve robustness by minimizing the worst-case risk across all clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.",
    "descriptor": "",
    "authors": [
      "Yujia Bao",
      "Shiyu Chang",
      "Regina Barzilay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07847"
  },
  {
    "id": "arXiv:2106.07849",
    "title": "Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks  with Knowledge Distillation",
    "abstract": "In recent years the ubiquitous deployment of AI has posed great concerns in\nregards to algorithmic bias, discrimination, and fairness. Compared to\ntraditional forms of bias or discrimination caused by humans, algorithmic bias\ngenerated by AI is more abstract and unintuitive therefore more difficult to\nexplain and mitigate. A clear gap exists in the current literature on\nevaluating and mitigating bias in pruned neural networks. In this work, we\nstrive to tackle the challenging issues of evaluating, mitigating, and\nexplaining induced bias in pruned neural networks. Our paper makes three\ncontributions. First, we propose two simple yet effective metrics, Combined\nError Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively\nevaluate the induced bias prevention quality of pruned models. Second, we\ndemonstrate that knowledge distillation can mitigate induced bias in pruned\nneural networks, even with unbalanced datasets. Third, we reveal that model\nsimilarity has strong correlations with pruning induced bias, which provides a\npowerful method to explain why bias occurs in pruned neural networks. Our code\nis available at https://github.com/codestar12/pruning-distilation-bias",
    "descriptor": "",
    "authors": [
      "Cody Blakeney",
      "Nathaniel Huish",
      "Yan Yan",
      "Ziliang Zong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07849"
  },
  {
    "id": "arXiv:2106.07851",
    "title": "Code Integrity Attestation for PLCs using Black Box Neural Network  Predictions",
    "abstract": "Cyber-physical systems (CPSs) are widespread in critical domains, and\nsignificant damage can be caused if an attacker is able to modify the code of\ntheir programmable logic controllers (PLCs). Unfortunately, traditional\ntechniques for attesting code integrity (i.e. verifying that it has not been\nmodified) rely on firmware access or roots-of-trust, neither of which\nproprietary or legacy PLCs are likely to provide. In this paper, we propose a\npractical code integrity checking solution based on privacy-preserving black\nbox models that instead attest the input/output behaviour of PLC programs.\nUsing faithful offline copies of the PLC programs, we identify their most\nimportant inputs through an information flow analysis, execute them on multiple\ncombinations to collect data, then train neural networks able to predict PLC\noutputs (i.e. actuator commands) from their inputs. By exploiting the black box\nnature of the model, our solution maintains the privacy of the original PLC\ncode and does not assume that attackers are unaware of its presence. The trust\ninstead comes from the fact that it is extremely hard to attack the PLC code\nand neural networks at the same time and with consistent outcomes. We evaluated\nour approach on a modern six-stage water treatment plant testbed, finding that\nit could predict actuator states from PLC inputs with near-100% accuracy, and\nthus could detect all 120 effective code mutations that we subjected the PLCs\nto. Finally, we found that it is not practically possible to simultaneously\nmodify the PLC code and apply discreet adversarial noise to our attesters in a\nway that leads to consistent (mis-)predictions.",
    "descriptor": "\nComments: Accepted by the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021)\n",
    "authors": [
      "Yuqi Chen",
      "Christopher M. Poskitt",
      "Jun Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07851"
  },
  {
    "id": "arXiv:2106.07852",
    "title": "Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo  Collection",
    "abstract": "Non-parametric face modeling aims to reconstruct 3D face only from images\nwithout shape assumptions. While plausible facial details are predicted, the\nmodels tend to over-depend on local color appearance and suffer from ambiguous\nnoise. To address such problem, this paper presents a novel Learning to\nAggregate and Personalize (LAP) framework for unsupervised robust 3D face\nmodeling. Instead of using controlled environment, the proposed method\nimplicitly disentangles ID-consistent and scene-specific face from\nunconstrained photo set. Specifically, to learn ID-consistent face, LAP\nadaptively aggregates intrinsic face factors of an identity based on a novel\ncurriculum learning approach with relaxed consistency loss. To adapt the face\nfor a personalized scene, we propose a novel attribute-refining network to\nmodify ID-consistent face with target attribute and details. Based on the\nproposed method, we make unsupervised 3D face modeling benefit from meaningful\nimage facial structure and possibly higher resolutions. Extensive experiments\non benchmarks show LAP recovers superior or competitive face shape and texture,\ncompared with state-of-the-art (SOTA) methods with or without prior and\nsupervision.",
    "descriptor": "\nComments: CVPR 2021 Oral, 11 pages, 9 figures\n",
    "authors": [
      "Zhenyu Zhang",
      "Yanhao Ge",
      "Renwang Chen",
      "Ying Tai",
      "Yan Yan",
      "Jian Yang",
      "Chengjie Wang",
      "Jilin Li",
      "Feiyue Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.07852"
  },
  {
    "id": "arXiv:2106.07853",
    "title": "G$^2$DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person  Re-Identification",
    "abstract": "RGB-Infrared (IR) person re-identification aims to retrieve\nperson-of-interest between heterogeneous modalities, suffering from large\nmodality discrepancy caused by different sensory devices. Existing methods\nmainly focus on global-level modality alignment, whereas neglect sample-level\nmodality divergence to some extent, leading to performance degradation. This\npaper attempts to find RGB-IR ReID solutions from tackling sample-level\nmodality difference, and presents a Geometry-Guided Dual-Alignment learning\nframework (G$^2$DA), which jointly enhances modality-invariance and reinforces\ndiscriminability with human topological structure in features to boost the\noverall matching performance. Specifically, G$^2$DA extracts accurate body part\nfeatures with a pose estimator, serving as a semantic bridge complementing the\nmissing local details in global descriptor. Based on extracted local and global\nfeatures, a novel distribution constraint derived from optimal transport is\nintroduced to mitigate the modality gap in a fine-grained sample-level manner.\nBeyond pair-wise relations across two modalities, it additionally measures the\nstructural similarity of different parts, thus both multi-level features and\ntheir relations are kept consistent in the common feature space. Considering\nthe inherent human-topology information, we further advance a geometry-guided\ngraph learning module to refine each part features, where relevant regions can\nbe emphasized while meaningless ones are suppressed, effectively facilitating\nrobust feature learning. Extensive experiments on two standard benchmark\ndatasets validate the superiority of our proposed method, yielding competitive\nperformance over the state-of-the-art approaches.",
    "descriptor": "\nComments: 13 pages, 6 figures\n",
    "authors": [
      "Lin Wan",
      "Zongyuan Sun",
      "Qianyan Jing",
      "Yehansen Chen",
      "Lijing Lu",
      "Zhihang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07853"
  },
  {
    "id": "arXiv:2106.07854",
    "title": "Population-coding and Dynamic-neurons improved Spiking Actor Network for  Reinforcement Learning",
    "abstract": "With the Deep Neural Networks (DNNs) as a powerful function approximator,\nDeep Reinforcement Learning (DRL) has been excellently demonstrated on robotic\ncontrol tasks. Compared to DNNs with vanilla artificial neurons, the\nbiologically plausible Spiking Neural Network (SNN) contains a diverse\npopulation of spiking neurons, making it naturally powerful on state\nrepresentation with spatial and temporal information. Based on a hybrid\nlearning framework, where a spike actor-network infers actions from states and\na deep critic network evaluates the actor, we propose a Population-coding and\nDynamic-neurons improved Spiking Actor Network (PDSAN) for efficient state\nrepresentation from two different scales: input coding and neuronal coding. For\ninput coding, we apply population coding with dynamically receptive fields to\ndirectly encode each input state component. For neuronal coding, we propose\ndifferent types of dynamic-neurons (containing 1st-order and 2nd-order neuronal\ndynamics) to describe much more complex neuronal dynamics. Finally, the PDSAN\nis trained in conjunction with deep critic networks using the Twin Delayed Deep\nDeterministic policy gradient algorithm (TD3-PDSAN). Extensive experimental\nresults show that our TD3-PDSAN model achieves better performance than\nstate-of-the-art models on four OpenAI gym benchmark tasks. It is an important\nattempt to improve RL with SNN towards the effective computation satisfying\nbiological plausibility.",
    "descriptor": "\nComments: 27 pages, 11 figures, accepted by Journal of Neural Networks\n",
    "authors": [
      "Duzhen Zhang",
      "Tielin Zhang",
      "Shuncheng Jia",
      "Xiang Cheng",
      "Bo Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07854"
  },
  {
    "id": "arXiv:2106.07855",
    "title": "Low-Energy and CPA-Resistant Adiabatic CMOS/MTJ Logic for IoT Devices",
    "abstract": "The tremendous growth in the number of Internet of Things (IoT) devices has\nincreased focus on the energy efficiency and security of an IoT device. In this\npaper, we will present a design level, non-volatile adiabatic architecture for\nlow-energy and Correlation Power Analysis (CPA) resistant IoT devices. IoT\ndevices constructed with CMOS integrated circuits suffer from high dynamic\nenergy and leakage power. To solve this, we look at both adiabatic logic and\nSTT-MTJs (Spin Transfer Torque Magnetic Tunnel Junctions) to reduce both\ndynamic energy and leakage power. Furthermore, CMOS integrated circuits suffer\nfrom side-channel leakage making them insecure against power analysis attacks.\nWe again look to adiabatic logic to design secure circuits with uniform power\nconsumption, thus, defending against power analysis attacks. We have developed\na hybrid adiabatic- MTJ architecture using two-phase adiabatic logic. We show\nthat hybrid adiabatic-MTJ circuits are both low energy and secure when compared\nwith CMOS circuits. As a case study, we have constructed one round of PRESENT\nand have shown energy savings of 64.29% at a frequency of 25 MHz. Furthermore,\nwe have performed a correlation power analysis attack on our proposed design\nand determined that the key was kept hidden.",
    "descriptor": "\nComments: 6 pages, 2021 IEEE Computer Society Annual Symposium on VLSI\n",
    "authors": [
      "Zachary Kahleifeh",
      "Himanshu Thapliyal"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.07855"
  },
  {
    "id": "arXiv:2106.07856",
    "title": "A Hybrid mmWave and Camera System for Long-Range Depth Imaging",
    "abstract": "mmWave radars offer excellent depth resolution owing to their high bandwidth\nat mmWave radio frequencies. Yet, they suffer intrinsically from poor angular\nresolution, that is an order-of-magnitude worse than camera systems, and are\ntherefore not a capable 3-D imaging solution in isolation. We propose\nMetamoran, a system that combines the complimentary strengths of radar and\ncamera systems to obtain depth images at high azimuthal resolutions at\ndistances of several tens of meters with high accuracy, all from a single fixed\nvantage point. Metamoran enables rich long-range depth imaging outdoors with\napplications to roadside safety infrastructure, surveillance and wide-area\nmapping. Our key insight is to use the high azimuth resolution from cameras\nusing computer vision techniques, including image segmentation and monocular\ndepth estimation, to obtain object shapes and use these as priors for our novel\nspecular beamforming algorithm. We also design this algorithm to work in\ncluttered environments with weak reflections and in partially occluded\nscenarios. We perform a detailed evaluation of Metamoran's depth imaging and\nsensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation\nshows that Metamoran estimates the depth of an object up to 60~m away with a\nmedian error of 28~cm, an improvement of 13$\\times$ compared to a naive\nradar+camera baseline and 23$\\times$ compared to monocular depth estimation.",
    "descriptor": "",
    "authors": [
      "Diana Zhang",
      "Akarsh Prabhakara",
      "Sirajum Munir",
      "Aswin Sankaranarayanan",
      "Swarun Kumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Networking and Internet Architecture (cs.NI)",
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.07856"
  },
  {
    "id": "arXiv:2106.07857",
    "title": "Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware  Fusion",
    "abstract": "Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nbilateral personalized dialogue generation (BPDG) method with dynamic\npersona-aware fusion via multi-task transfer learning to generate responses\nconsistent with both personas. The proposed method aims to accomplish three\nlearning tasks: 1) an encoder is trained with dialogue utterances added with\ncorresponded personalized attributes and relative position (language model\ntask), 2) a dynamic persona-aware fusion module predicts the persona presence\nto adaptively fuse the contextual and bilateral personas encodings (persona\nprediction task) and 3) a decoder generates natural, fluent and personalized\nresponses (dialogue generation task). To make the generated responses more\npersonalized and bilateral persona-consistent, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted to select the final response\nfrom the generated candidates. The experimental results show that the proposed\nmethod outperforms several state-of-the-art methods in terms of both automatic\nand manual evaluations.",
    "descriptor": "\nComments: 14 pages, 6 figures\n",
    "authors": [
      "Bin Li",
      "Bin Sun",
      "Shutao Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07857"
  },
  {
    "id": "arXiv:2106.07860",
    "title": "Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery",
    "abstract": "The use of Machine Learning has become a significant part of malware\ndetection efforts due to the influx of new malware, an ever changing threat\nlandscape, and the ability of Machine Learning methods to discover meaningful\ndistinctions between malicious and benign software. Antivirus vendors have also\nbegun to widely utilize malware classifiers based on dynamic and static malware\nanalysis features. Therefore, a malware author might make evasive binary\nmodifications against Machine Learning models as part of the malware\ndevelopment life cycle to execute an attack successfully. This makes the\nstudying of possible classifier evasion strategies an essential part of cyber\ndefense against malice. To this extent, we stage a grey box setup to analyze a\nscenario where the malware author does not know the target classifier\nalgorithm, and does not have access to decisions made by the classifier, but\nknows the features used in training. In this experiment, a malicious actor\ntrains a surrogate model using the EMBER-2018 dataset to discover binary\nmutations that cause an instance to be misclassified via a Monte Carlo tree\nsearch. Then, mutated malware is sent to the victim model that takes the place\nof an antivirus API to test whether it can evade detection.",
    "descriptor": "\nComments: Presented at the Malware Technical Exchange Meeting, Online,2021. Copyright 2021 by the author(s)\n",
    "authors": [
      "John Boutsikas",
      "Maksim E. Eren",
      "Charles Varga",
      "Edward Raff",
      "Cynthia Matuszek",
      "Charles Nicholas"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07860"
  },
  {
    "id": "arXiv:2106.07861",
    "title": "Keep CALM and Improve Visual Feature Attribution",
    "abstract": "The class activation mapping, or CAM, has been the cornerstone of feature\nattribution methods for multiple vision tasks. Its simplicity and effectiveness\nhave led to wide applications in the explanation of visual predictions and\nweakly-supervised localization tasks. However, CAM has its own shortcomings.\nThe computation of attribution maps relies on ad-hoc calibration steps that are\nnot part of the training computational graph, making it difficult for us to\nunderstand the real meaning of the attribution values. In this paper, we\nimprove CAM by explicitly incorporating a latent variable encoding the location\nof the cue for recognition in the formulation, thereby subsuming the\nattribution map into the training computational graph. The resulting model,\nclass activation latent mapping, or CALM, is trained with the\nexpectation-maximization algorithm. Our experiments show that CALM identifies\ndiscriminative attributes for image classifiers more accurately than CAM and\nother visual attribution baselines. CALM also shows performance improvements\nover prior arts on the weakly-supervised object localization benchmarks. Our\ncode is available at https://github.com/naver-ai/calm.",
    "descriptor": "\nComments: 20 pages, 11 figures\n",
    "authors": [
      "Jae Myung Kim",
      "Junsuk Choe",
      "Zeynep Akata",
      "Seong Joon Oh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07861"
  },
  {
    "id": "arXiv:2106.07862",
    "title": "Domain Adaptive SiamRPN++ for Object Tracking in the Wild",
    "abstract": "Benefit from large-scale training data, recent advances in Siamese-based\nobject tracking have achieved compelling results on the normal sequences.\nWhilst Siamese-based trackers assume training and test data follow an identical\ndistribution. Suppose there is a set of foggy or rainy test sequences, it\ncannot be guaranteed that the trackers trained on the normal images perform\nwell on the data belonging to other domains. The problem of domain shift among\ntraining and test data has already been discussed in object detection and\nsemantic segmentation areas, which, however, has not been investigated for\nvisual tracking. To this end, based on SiamRPN++, we introduce a Domain\nAdaptive SiamRPN++, namely DASiamRPN++, to improve the cross-domain\ntransferability and robustness of a tracker. Inspired by A-distance theory, we\npresent two domain adaptive modules, Pixel Domain Adaptation (PDA) and Semantic\nDomain Adaptation (SDA). The PDA module aligns the feature maps of template and\nsearch region images to eliminate the pixel-level domain shift caused by\nweather, illumination, etc. The SDA module aligns the feature representations\nof the tracking target's appearance to eliminate the semantic-level domain\nshift. PDA and SDA modules reduce the domain disparity by learning domain\nclassifiers in an adversarial training manner. The domain classifiers enforce\nthe network to learn domain-invariant feature representations. Extensive\nexperiments are performed on the standard datasets of two different domains,\nincluding synthetic foggy and TIR sequences, which demonstrate the\ntransferability and domain adaptability of the proposed tracker.",
    "descriptor": "\nComments: 10 pages,7 figures\n",
    "authors": [
      "Zhongzhou Zhang",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07862"
  },
  {
    "id": "arXiv:2106.07864",
    "title": "User-specific Adaptive Fine-tuning for Cross-domain Recommendations",
    "abstract": "Making accurate recommendations for cold-start users has been a longstanding\nand critical challenge for recommender systems (RS). Cross-domain\nrecommendations (CDR) offer a solution to tackle such a cold-start problem when\nthere is no sufficient data for the users who have rarely used the system. An\neffective approach in CDR is to leverage the knowledge (e.g., user\nrepresentations) learned from a related but different domain and transfer it to\nthe target domain. Fine-tuning works as an effective transfer learning\ntechnique for this objective, which adapts the parameters of a pre-trained\nmodel from the source domain to the target domain. However, current methods are\nmainly based on the global fine-tuning strategy: the decision of which layers\nof the pre-trained model to freeze or fine-tune is taken for all users in the\ntarget domain. In this paper, we argue that users in RS are personalized and\nshould have their own fine-tuning policies for better preference transfer\nlearning. As such, we propose a novel User-specific Adaptive Fine-tuning method\n(UAF), selecting which layers of the pre-trained network to fine-tune, on a\nper-user basis. Specifically, we devise a policy network with three alternative\nstrategies to automatically decide which layers to be fine-tuned and which\nlayers to have their parameters frozen for each user. Extensive experiments\nshow that the proposed UAF exhibits significantly better and more robust\nperformance for user cold-start recommendation.",
    "descriptor": "",
    "authors": [
      "Lei Chen",
      "Fajie Yuan",
      "Jiaxi Yang",
      "Xiangnan He",
      "Chengming Li",
      "Min Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.07864"
  },
  {
    "id": "arXiv:2106.07866",
    "title": "Fogging Jyaguchi Services in Tensai Gothalo",
    "abstract": "This paper describes the efficient method of fogging in Tensai Gothalo.\nTensai Gothalo is a novel dynamic router device developed in Gautam-Asami\nLaboratory of Wakkanai Hokusei Gakuen University which has sensing, actuating,\nmonitoring and movable capability. Similarly, fogging is a new concept of cloud\ncomputing at which the data plane is defined in user device. In this paper we\nwould like to present the stepwise explanation about how to fog in Tensai\nGothalo. Furthermore, we will elaborate a technique to decentralize data with\nimprovement in QoS and reducing latency without affecting the legacy services\nof clouds that can still work together while needed.",
    "descriptor": "\nComments: 7 pages with figures\n",
    "authors": [
      "Gautam Bishnu Prasad",
      "Batajoo Amit",
      "Wasaki Katsumi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.07866"
  },
  {
    "id": "arXiv:2106.07867",
    "title": "Defending Touch-based Continuous Authentication Systems from Active  Adversaries Using Generative Adversarial Networks",
    "abstract": "Previous studies have demonstrated that commonly studied (vanilla)\ntouch-based continuous authentication systems (V-TCAS) are susceptible to\npopulation attack. This paper proposes a novel Generative Adversarial Network\nassisted TCAS (G-TCAS) framework, which showed more resilience to the\npopulation attack. G-TCAS framework was tested on a dataset of 117 users who\ninteracted with a smartphone and tablet pair. On average, the increase in the\nfalse accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)\nfor the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%\ncompared to G-TCAS (6%) for the tablet.",
    "descriptor": "\nComments: 2021 IEEE International Joint Conference on Biometrics (IJCB), 8 pages\n",
    "authors": [
      "Mohit Agrawal",
      "Pragyan Mehrotra",
      "Rajesh Kumar",
      "Rajiv Ratn Shah"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.07867"
  },
  {
    "id": "arXiv:2106.07868",
    "title": "Voting for the right answer: Adversarial defense for speaker  verification",
    "abstract": "Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets. The code for reproducing main\nresults is available at https://github.com/thuhcsi/adsv_voting.",
    "descriptor": "\nComments: Accepted by Interspeech 2021. Code is available at this https URL\n",
    "authors": [
      "Haibin Wu",
      "Yang Zhang",
      "Zhiyong Wu",
      "Dong Wang",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07868"
  },
  {
    "id": "arXiv:2106.07873",
    "title": "Reverse Engineering of Generative Models: Inferring Model  Hyperparameters from Generated Images",
    "abstract": "State-of-the-art (SOTA) Generative Models (GMs) can synthesize\nphoto-realistic images that are hard for humans to distinguish from genuine\nphotos. We propose to perform reverse engineering of GMs to infer the model\nhyperparameters from the images generated by these models. We define a novel\nproblem, \"model parsing\", as estimating GM network architectures and training\nloss functions by examining their generated images -- a task seemingly\nimpossible for human beings. To tackle this problem, we propose a framework\nwith two components: a Fingerprint Estimation Network (FEN), which estimates a\nGM fingerprint from a generated image by training with four constraints to\nencourage the fingerprint to have desired properties, and a Parsing Network\n(PN), which predicts network architecture and loss functions from the estimated\nfingerprints. To evaluate our approach, we collect a fake image dataset with\n$100$K images generated by $100$ GMs. Extensive experiments show encouraging\nresults in parsing the hyperparameters of the unseen models. Finally, our\nfingerprint estimation can be leveraged for deepfake detection and image\nattribution, as we show by reporting SOTA results on both the recent Celeb-DF\nand image attribution benchmarks.",
    "descriptor": "",
    "authors": [
      "Vishal Asnani",
      "Xi Yin",
      "Tal Hassner",
      "Xiaoming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07873"
  },
  {
    "id": "arXiv:2106.07874",
    "title": "Towards the Objective Speech Assessment of Smoking Status based on Voice  Features: A Review of the Literature",
    "abstract": "In smoking cessation clinical research and practice, objective validation of\nself-reported smoking status is crucial for ensuring the reliability of the\nprimary outcome, that is, smoking abstinence. Speech signals convey important\ninformation about a speaker, such as age, gender, body size, emotional state,\nand health state. We investigated (1) if smoking could measurably alter voice\nfeatures, (2) if smoking cessation could lead to changes in voice, and\ntherefore (3) if the voice-based smoking status assessment has the potential to\nbe used as an objective smoking cessation validation method.",
    "descriptor": "",
    "authors": [
      "Zhizhong Ma",
      "Chris Bullen",
      "Joanna Ting Wai Chu",
      "Ruili Wang",
      "Yingchun Wang",
      "Satwinder Singh"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07874"
  },
  {
    "id": "arXiv:2106.07876",
    "title": "Vision-Language Navigation with Random Environmental Mixup",
    "abstract": "Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark.",
    "descriptor": "",
    "authors": [
      "Chong Liu",
      "Fengda Zhu",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Yi-Dong Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07876"
  },
  {
    "id": "arXiv:2106.07877",
    "title": "Learning Revenue-Maximizing Auctions With Differentiable Matching",
    "abstract": "We propose a new architecture to approximately learn incentive compatible,\nrevenue-maximizing auctions from sampled valuations. Our architecture uses the\nSinkhorn algorithm to perform a differentiable bipartite matching which allows\nthe network to learn strategyproof revenue-maximizing mechanisms in settings\nnot learnable by the previous RegretNet architecture. In particular, our\narchitecture is able to learn mechanisms in settings without free disposal\nwhere each bidder must be allocated exactly some number of items. In\nexperiments, we show our approach successfully recovers multiple known optimal\nmechanisms and high-revenue, low-regret mechanisms in larger settings where the\noptimal mechanism is unknown.",
    "descriptor": "",
    "authors": [
      "Michael J. Curry",
      "Uro Lyi",
      "Tom Goldstein",
      "John Dickerson"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07877"
  },
  {
    "id": "arXiv:2106.07880",
    "title": "Scaling Neural Tangent Kernels via Sketching and Random Features",
    "abstract": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide\nneural networks trained under least squares loss by gradient descent. Recent\nworks also report that NTK regression can outperform finitely-wide neural\nnetworks trained on small-scale datasets. However, the computational complexity\nof kernel methods has limited its use in large-scale learning tasks. To\naccelerate learning with NTK, we design a near input-sparsity time\napproximation algorithm for NTK, by sketching the polynomial expansions of\narc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)\ncan transform any image using a linear runtime in the number of pixels.\nFurthermore, we prove a spectral approximation guarantee for the NTK matrix, by\ncombining random features (based on leverage score sampling) of the arc-cosine\nkernels with a sketching algorithm. We benchmark our methods on various\nlarge-scale regression and classification tasks and show that a linear\nregressor trained on our CNTK features matches the accuracy of exact CNTK on\nCIFAR-10 dataset while achieving 150x speedup.",
    "descriptor": "\nComments: This is a merger of arXiv:2104.01351, arXiv:2104.00415\n",
    "authors": [
      "Amir Zandieh",
      "Insu Han",
      "Haim Avron",
      "Neta Shoham",
      "Chaewon Kim",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.07880"
  },
  {
    "id": "arXiv:2106.07881",
    "title": "Mixed Model OCR Training on Historical Latin Script for Out-of-the-Box  Recognition and Finetuning",
    "abstract": "In order to apply Optical Character Recognition (OCR) to historical printings\nof Latin script fully automatically, we report on our efforts to construct a\nwidely-applicable polyfont recognition model yielding text with a Character\nError Rate (CER) around 2% when applied out-of-the-box. Moreover, we show how\nthis model can be further finetuned to specific classes of printings with\nlittle manual and computational effort. The mixed or polyfont model is trained\non a wide variety of materials, in terms of age (from the 15th to the 19th\ncentury), typography (various types of Fraktur and Antiqua), and languages\n(among others, German, Latin, and French). To optimize the results we combined\nestablished techniques of OCR training like pretraining, data augmentation, and\nvoting. In addition, we used various preprocessing methods to enrich the\ntraining data and obtain more robust models. We also implemented a two-stage\napproach which first trains on all available, considerably unbalanced data and\nthen refines the output by training on a selected more balanced subset.\nEvaluations on 29 previously unseen books resulted in a CER of 1.73%,\noutperforming a widely used standard model with a CER of 2.84% by almost 40%.\nTraining a more specialized model for some unseen Early Modern Latin books\nstarting from our mixed model led to a CER of 1.47%, an improvement of up to\n50% compared to training from scratch and up to 30% compared to training from\nthe aforementioned standard model. Our new mixed model is made openly available\nto the community.",
    "descriptor": "\nComments: submitted to HIP'21\n",
    "authors": [
      "Christian Reul",
      "Christoph Wick",
      "Maximilian N\u00f6th",
      "Andreas B\u00fcttner",
      "Maximilian Wehner",
      "Uwe Springmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07881"
  },
  {
    "id": "arXiv:2106.07886",
    "title": "MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis",
    "abstract": "Recent developments in deep learning have significantly improved the quality\nof synthesized singing voice audio. However, prominent neural singing voice\nsynthesis systems suffer from slow inference speed due to their autoregressive\ndesign. Inspired by MLP-Mixer, a novel architecture introduced in the vision\nliterature for attention-free image classification, we propose MLP Singer, a\nparallel Korean singing voice synthesis system. To the best of our knowledge,\nthis is the first work that uses an entirely MLP-based architecture for voice\nsynthesis. Listening tests demonstrate that MLP Singer outperforms a larger\nautoregressive GAN-based system, both in terms of audio quality and synthesis\nspeed. In particular, MLP Singer achieves a real-time factor of up to 200 and\n3400 on CPUs and GPUs respectively, enabling order of magnitude faster\ngeneration on both environments.",
    "descriptor": "",
    "authors": [
      "Jaesung Tae",
      "Hyeongju Kim",
      "Younggun Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.07886"
  },
  {
    "id": "arXiv:2106.07887",
    "title": "Credit Assignment in Neural Networks through Deep Feedback Control",
    "abstract": "The success of deep learning sparked interest in whether the brain learns by\nusing similar techniques for assigning credit to each synaptic weight for its\ncontribution to the network output. However, the majority of current attempts\nat biologically-plausible learning methods are either non-local in time,\nrequire highly specific connectivity motives, or have no clear link to any\nknown mathematical optimization method. Here, we introduce Deep Feedback\nControl (DFC), a new learning method that uses a feedback controller to drive a\ndeep neural network to match a desired output target and whose control signal\ncan be used for credit assignment. The resulting learning rule is fully local\nin space and time and approximates Gauss-Newton optimization for a wide range\nof feedback connectivity patterns. To further underline its biological\nplausibility, we relate DFC to a multi-compartment model of cortical pyramidal\nneurons with a local voltage-dependent synaptic plasticity rule, consistent\nwith recent theories of dendritic processing. By combining dynamical system\ntheory with mathematical optimization theory, we provide a strong theoretical\nfoundation for DFC that we corroborate with detailed results on toy experiments\nand standard computer-vision benchmarks.",
    "descriptor": "\nComments: 14 pages and 3 figures in the main manuscript; 45 pages and 14 figures in the supplementary materials\n",
    "authors": [
      "Alexander Meulemans",
      "Matilde Tristany Farinha",
      "Javier Garc\u00eda Ord\u00f3\u00f1ez",
      "Pau Vilimelis Aceituno",
      "Jo\u00e3o Sacramento",
      "Benjamin F. Grewe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07887"
  },
  {
    "id": "arXiv:2106.07892",
    "title": "Towards Safe Control of Continuum Manipulator Using Shielded Multiagent  Reinforcement Learning",
    "abstract": "Continuum robotic manipulators are increasingly adopted in minimal invasive\nsurgery. However, their nonlinear behavior is challenging to model accurately,\nespecially when subject to external interaction, potentially leading to poor\ncontrol performance. In this letter, we investigate the feasibility of adopting\na model-free multiagent reinforcement learning (RL), namely multiagent deep Q\nnetwork (MADQN), to control a 2-degree of freedom (DoF) cable-driven continuum\nsurgical manipulator. The control of the robot is formulated as a one-DoF, one\nagent problem in the MADQN framework to improve the learning efficiency.\nCombined with a shielding scheme that enables dynamic variation of the action\nset boundary, MADQN leads to efficient and importantly safer control of the\nrobot. Shielded MADQN enabled the robot to perform point and trajectory\ntracking with submillimeter root mean square errors under external loads, soft\nobstacles, and rigid collision, which are common interaction scenarios\nencountered by surgical manipulators. The controller was further proven to be\neffective in a miniature continuum robot with high structural nonlinearitiy,\nachieving trajectory tracking with submillimeter accuracy under external\npayload.",
    "descriptor": "\nComments: 8 pages, 12 figs, 1 table, 2 pseudo-code\n",
    "authors": [
      "Guanglin Ji",
      "Junyan Yan",
      "Jingxin Du",
      "Wanquan Yan",
      "Jibiao Chen",
      "Yongkang Lu",
      "Juan Rojas",
      "Shing Shin Cheng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.07892"
  },
  {
    "id": "arXiv:2106.07893",
    "title": "A General Purpose Transpiler for Fully Homomorphic Encryption",
    "abstract": "Fully homomorphic encryption (FHE) is an encryption scheme which enables\ncomputation on encrypted data without revealing the underlying data. While\nthere have been many advances in the field of FHE, developing programs using\nFHE still requires expertise in cryptography. In this white paper, we present a\nfully homomorphic encryption transpiler that allows developers to convert\nhigh-level code (e.g., C++) that works on unencrypted data into high-level code\nthat operates on encrypted data. Thus, our transpiler makes transformations\npossible on encrypted data.\nOur transpiler builds on Google's open-source XLS SDK\n(https://github.com/google/xls) and uses an off-the-shelf FHE library, TFHE\n(https://tfhe.github.io/tfhe/), to perform low-level FHE operations. The\ntranspiler design is modular, which means the underlying FHE library as well as\nthe high-level input and output languages can vary. This modularity will help\naccelerate FHE research by providing an easy way to compare arbitrary programs\nin different FHE schemes side-by-side. We hope this lays the groundwork for\neventual easy adoption of FHE by software developers. As a proof-of-concept, we\nare releasing an experimental transpiler\n(https://github.com/google/fully-homomorphic-encryption/tree/main/transpiler)\nas open-source software.",
    "descriptor": "",
    "authors": [
      "Shruthi Gorantala",
      "Rob Springer",
      "Sean Purser-Haskell",
      "William Lam",
      "Royce Wilson",
      "Asra Ali",
      "Eric P. Astor",
      "Itai Zukerman",
      "Sam Ruth",
      "Christoph Dibak",
      "Phillipp Schoppmann",
      "Sasha Kulankhina",
      "Alain Forget",
      "David Marn",
      "Cameron Tew",
      "Rafael Misoczki",
      "Bernat Guillen",
      "Xinyu Ye",
      "Dennis Kraft",
      "Damien Desfontaines",
      "Aishe Krishnamurthy",
      "Miguel Guevara",
      "Irippuge Milinda Perera",
      "Yurii Sushko",
      "Bryant Gipson"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.07893"
  },
  {
    "id": "arXiv:2106.07894",
    "title": "S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural  Networks",
    "abstract": "Convolutional neural networks (CNNs) have achieved great success in\nperforming cognitive tasks. However, execution of CNNs requires a large amount\nof computing resources and generates heavy memory traffic, which imposes a\nsevere challenge on computing system design. Through optimizing parallel\nexecutions and data reuse in convolution, systolic architecture demonstrates\ngreat advantages in accelerating CNN computations. However, regular internal\ndata transmission path in traditional systolic architecture prevents the\nsystolic architecture from completely leveraging the benefits introduced by\nneural network sparsity. Deployment of fine-grained sparsity on the existing\nsystolic architectures is greatly hindered by the incurred computational\noverheads. In this work, we propose S2Engine $-$ a novel systolic architecture\nthat can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine\ntransmits compressed data internally and allows each processing element to\ndynamically select an aligned data from the compressed dataflow in convolution.\nCompared to the naive systolic array, S2Engine achieves about $3.2\\times$ and\nabout $3.0\\times$ improvements on speed and energy efficiency, respectively.",
    "descriptor": "\nComments: 13 pages, 17 figures\n",
    "authors": [
      "Jianlei Yang",
      "Wenzhi Fu",
      "Xingzhou Cheng",
      "Xucheng Ye",
      "Pengcheng Dai",
      "Weisheng Zhao"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07894"
  },
  {
    "id": "arXiv:2106.07895",
    "title": "CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an  In-Vehicle CAN Bus Based on Deep Features of Voltage Signals",
    "abstract": "The Controller Area Network (CAN) is used for communication between\nin-vehicle devices. The CAN bus has been shown to be vulnerable to remote\nattacks. To harden vehicles against such attacks, vehicle manufacturers have\ndivided in-vehicle networks into sub-networks, logically isolating critical\ndevices. However, attackers may still have physical access to various\nsub-networks where they can connect a malicious device. This threat has not\nbeen adequately addressed, as methods proposed to determine physical intrusion\npoints have shown weak results, emphasizing the need to develop more advanced\ntechniques. To address this type of threat, we propose a security hardening\nsystem for in-vehicle networks. The proposed system includes two mechanisms\nthat process deep features extracted from voltage signals measured on the CAN\nbus. The first mechanism uses data augmentation and deep learning to detect and\nlocate physical intrusions when the vehicle starts; this mechanism can detect\nand locate intrusions, even when the connected malicious devices are silent.\nThis mechanism's effectiveness (100% accuracy) is demonstrated in a wide\nvariety of insertion scenarios on a CAN bus prototype. The second mechanism is\na continuous device authentication mechanism, which is also based on deep\nlearning; this mechanism's robustness (99.8% accuracy) is demonstrated on a\nreal moving vehicle.",
    "descriptor": "",
    "authors": [
      "Efrat Levy",
      "Asaf Shabtai",
      "Bogdan Groza",
      "Pal-Stefan Murvay",
      "Yuval Elovici"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07895"
  },
  {
    "id": "arXiv:2106.07900",
    "title": "Augmented Tensor Decomposition with Stochastic Optimization",
    "abstract": "Tensor decompositions are powerful tools for dimensionality reduction and\nfeature interpretation of multidimensional data such as signals. Existing\ntensor decomposition objectives (e.g., Frobenius norm) are designed for fitting\nraw data under statistical assumptions, which may not align with downstream\nclassification tasks. Also, real-world tensor data are usually high-ordered and\nhave large dimensions with millions or billions of entries. Thus, it is\nexpensive to decompose the whole tensor with traditional algorithms. In\npractice, raw tensor data also contains redundant information while data\naugmentation techniques may be used to smooth out noise in samples. This paper\naddresses the above challenges by proposing augmented tensor decomposition\n(ATD), which effectively incorporates data augmentations to boost downstream\nclassification. To reduce the memory footprint of the decomposition, we propose\na stochastic algorithm that updates the factor matrices in a batch fashion. We\nevaluate ATD on multiple signal datasets. It shows comparable or better\nperformance (e.g., up to 15% in accuracy) over self-supervised and autoencoder\nbaselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy\ngain over other tensor-based baselines, and reduces the memory footprint by 9X\nwhen compared to standard tensor decomposition algorithms.",
    "descriptor": "\nComments: This paper proposes augmented tensor decomposition framework and use stochastic algorithm for optimization\n",
    "authors": [
      "Chaoqi Yang",
      "Cheng Qian",
      "Navjot Singh",
      "Cao Xiao",
      "M Brandon Westover",
      "Edgar Solomonik",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07900"
  },
  {
    "id": "arXiv:2106.07903",
    "title": "Robust Out-of-Distribution Detection on Deep Probabilistic Generative  Models",
    "abstract": "Out-of-distribution (OOD) detection is an important task in machine learning\nsystems for ensuring their reliability and safety. Deep probabilistic\ngenerative models facilitate OOD detection by estimating the likelihood of a\ndata sample. However, such models frequently assign a suspiciously high\nlikelihood to a specific outlier. Several recent works have addressed this\nissue by training a neural network with auxiliary outliers, which are generated\nby perturbing the input data. In this paper, we discover that these approaches\nfail for certain OOD datasets. Thus, we suggest a new detection metric that\noperates without outlier exposure. We observe that our metric is robust to\ndiverse variations of an image compared to the previous outlier-exposing\nmethods. Furthermore, our proposed score requires neither auxiliary models nor\nadditional training. Instead, this paper utilizes the likelihood ratio\nstatistic in a new perspective to extract genuine properties from the given\nsingle deep probabilistic generative model. We also apply a novel numerical\napproximation to enable fast implementation. Finally, we demonstrate\ncomprehensive experiments on various probabilistic generative models and show\nthat our method achieves state-of-the-art performance.",
    "descriptor": "\nComments: 23 pages, 11 figures\n",
    "authors": [
      "Jaemoo Choi",
      "Changyeon Yoon",
      "Jeongwoo Bae",
      "Myungjoo Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07903"
  },
  {
    "id": "arXiv:2106.07904",
    "title": "Probabilistic Margins for Instance Reweighting in Adversarial Training",
    "abstract": "Reweighting adversarial data during training has been recently shown to\nimprove adversarial robustness, where data closer to the current decision\nboundaries are regarded as more critical and given larger weights. However,\nexisting methods measuring the closeness are not very reliable: they are\ndiscrete and can take only a few values, and they are path-dependent, i.e.,\nthey may change given the same start and end points with different attack\npaths. In this paper, we propose three types of probabilistic margin (PM),\nwhich are continuous and path-independent, for measuring the aforementioned\ncloseness and reweighting adversarial data. Specifically, a PM is defined as\nthe difference between two estimated class-posterior probabilities, e.g., such\nthe probability of the true label minus the probability of the most confusing\nlabel given some natural data. Though different PMs capture different geometric\nproperties, all three PMs share a negative correlation with the vulnerability\nof data: data with larger/smaller PMs are safer/riskier and should have\nsmaller/larger weights. Experiments demonstrate that PMs are reliable\nmeasurements and PM-based reweighting methods outperform state-of-the-art\nmethods.",
    "descriptor": "\nComments: 17 pages, 4 figures\n",
    "authors": [
      "Qizhou Wang",
      "Feng Liu",
      "Bo Han",
      "Tongliang Liu",
      "Chen Gong",
      "Gang Niu",
      "Mingyuan Zhou",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07904"
  },
  {
    "id": "arXiv:2106.07905",
    "title": "Non-Gradient Manifold Neural Network",
    "abstract": "Deep neural network (DNN) generally takes thousands of iterations to optimize\nvia gradient descent and thus has a slow convergence. In addition, softmax, as\na decision layer, may ignore the distribution information of the data during\nclassification. Aiming to tackle the referred problems, we propose a novel\nmanifold neural network based on non-gradient optimization, i.e., the\nclosed-form solutions. Considering that the activation function is generally\ninvertible, we reconstruct the network via forward ridge regression and low\nrank backward approximation, which achieve the rapid convergence. Moreover, by\nunifying the flexible Stiefel manifold and adaptive support vector machine, we\ndevise the novel decision layer which efficiently fits the manifold structure\nof the data and label information. Consequently, a jointly non-gradient\noptimization method is designed to generate the network with closed-form\nresults. Eventually, extensive experiments validate the superior performance of\nthe model.",
    "descriptor": "",
    "authors": [
      "Rui Zhang",
      "Ziheng Jiao",
      "Hongyuan Zhang",
      "Xuelong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07905"
  },
  {
    "id": "arXiv:2106.07907",
    "title": "Rice's theorem for generic limit sets of cellular automata",
    "abstract": "The generic limit set of a cellular automaton is a topologically dened set of\ncongurations that intends to capture the asymptotic behaviours while avoiding\natypical ones. It was dened by Milnor then studied by Djenaoui and Guillon rst,\nand by T{\\\"o}rm{\\\"a} later. They gave properties of this set related to the\ndynamics of the cellular automaton, and the maximal complexity of its language.\nIn this paper, we prove that every non trivial property of these generic limit\nsets of cellular automata is undecidable.",
    "descriptor": "",
    "authors": [
      "Martin Delacourt"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.07907"
  },
  {
    "id": "arXiv:2106.07908",
    "title": "Machine learning-based conditional mean filter: a generalization of the  ensemble Kalman filter for nonlinear data assimilation",
    "abstract": "Filtering is a data assimilation technique that performs the sequential\ninference of dynamical systems states from noisy observations. Herein, we\npropose a machine learning-based ensemble conditional mean filter (ML-EnCMF)\nfor tracking possibly high-dimensional non-Gaussian state models with nonlinear\ndynamics based on sparse observations. The proposed filtering method is\ndeveloped based on the conditional expectation and numerically implemented\nusing machine learning (ML) techniques combined with the ensemble method. The\ncontribution of this work is twofold. First, we demonstrate that the ensembles\nassimilated using the ensemble conditional mean filter (EnCMF) provide an\nunbiased estimator of the Bayesian posterior mean, and their variance matches\nthe expected conditional variance. Second, we implement the EnCMF using\nartificial neural networks, which have a significant advantage in representing\nnonlinear functions over high-dimensional domains such as the conditional mean.\nFinally, we demonstrate the effectiveness of the ML-EnCMF for tracking the\nstates of Lorenz-63 and Lorenz-96 systems under the chaotic regime. Numerical\nresults show that the ML-EnCMF outperforms the ensemble Kalman filter.",
    "descriptor": "",
    "authors": [
      "Truong-Vinh Hoang",
      "Sebastian Krumscheid",
      "Hermann G. Matthies",
      "Ra\u00fal Tempone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07908"
  },
  {
    "id": "arXiv:2106.07909",
    "title": "Evaluating the Effect of the Financial Status to the Mobility Customs",
    "abstract": "In this article, we explore the relationship between cellular phone data and\nhousing prices in Budapest, Hungary. We determine mobility indicators from one\nmonths of Call Detail Records (CDR) data, while the property price data are\nused to characterize the socioeconomic status at the Capital of Hungary. First,\nwe validated the proposed methodology by comparing the Home and Work locations\nestimation and the commuting patterns derived from the cellular network dataset\nwith reports of the national mini census. We investigated the statistical\nrelationships between mobile phone indicators, such as Radius of Gyration, the\ndistance between Home and Work locations or the Entropy of visited cells, and\nmeasures of economic status based on housing prices. Our findings show that the\nmobility correlates significantly with the socioeconomic status. We performed\nPrincipal Component Analysis (PCA) on combined vectors of mobility indicators\nin order to characterize the dependence of mobility habits on socioeconomic\nstatus. The results of the PCA investigation showed remarkable correlation of\nhousing prices and mobility customs.",
    "descriptor": "",
    "authors": [
      "Gerg\u0151 Pint\u00e9r",
      "Imre Felde"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.07909"
  },
  {
    "id": "arXiv:2106.07914",
    "title": "Control Variates for Slate Off-Policy Evaluation",
    "abstract": "We study the problem of off-policy evaluation from batched contextual bandit\ndata with multidimensional actions, often termed slates. The problem is common\nto recommender systems and user-interface optimization, and it is particularly\nchallenging because of the combinatorially-sized action space. Swaminathan et\nal. (2017) have proposed the pseudoinverse (PI) estimator under the assumption\nthat the conditional mean rewards are additive in actions. Using control\nvariates, we consider a large class of unbiased estimators that includes as\nspecific cases the PI estimator and (asymptotically) its self-normalized\nvariant. By optimizing over this class, we obtain new estimators with risk\nimprovement guarantees over both the PI and self-normalized PI estimators.\nExperiments with real-world recommender data as well as synthetic data validate\nthese improvements in practice.",
    "descriptor": "",
    "authors": [
      "Nikos Vlassis",
      "Ashok Chandrashekar",
      "Fernando Amat Gil",
      "Nathan Kallus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.07914"
  },
  {
    "id": "arXiv:2106.07916",
    "title": "Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for  Better Single-Source Domain Generalization",
    "abstract": "Traditional deep learning algorithms often fail to generalize when they are\ntested outside of the domain of training data. Because data distributions can\nchange dynamically in real-life applications once a learned model is deployed,\nin this paper we are interested in single-source domain generalization (SDG)\nwhich aims to develop deep learning algorithms able to generalize from a single\ntraining domain where no information about the test domain is available at\ntraining time. Firstly, we design two simple MNISTbased SDG benchmarks, namely\nMNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different\nfundamental SDG issues of increasing difficulties: 1) a class-correlated\npattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the\nclass (SDG-UP), in the testing data domain. This is in sharp contrast with the\ncurrent domain generalization (DG) benchmarks which mix up different\ncorrelation and variation factors and thereby make hard to disentangle success\nor failure factors when benchmarking DG algorithms. We further evaluate several\nstate-of-the-art SDG algorithms through our simple benchmark, namely MNIST\nColor SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a\ndecade of efforts in developing DG algorithms. Finally, we also propose a\npartially reversed contrastive loss to encourage intra-class diversity and find\nless strongly correlated patterns, to deal with SDG-MP and show that the\nproposed approach is very effective on our MNIST Color SDG-MP benchmark.",
    "descriptor": "",
    "authors": [
      "Thomas Duboudin",
      "Emmanuel Dellandr\u00e9a",
      "Corentin Abgrall",
      "Gilles H\u00e9naff",
      "Liming Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07916"
  },
  {
    "id": "arXiv:2106.07921",
    "title": "Diagnosing the Impact of AI on Radiology in China",
    "abstract": "Artificial Intelligence will significantly impact the work environment of\nradiologists. I suggest that up to 50% of a radiologists work in 2021 will be\nperformed by AI-models in 2025. However, it won't increase beyond that 50%\nlevel, as radiologists remain key for human-centered aspects of their job. I\nproject that few to no radiologists will be laid off in China due to the\nexisting supply shortage of radiology services in 2021. The application of AI\nin radiology could contribute 1.7 billion USD to China's GDP in 2025. It will\nfurther allow radiologists to start productive work up to four years earlier.\nAI in radiology will positively impact the health of patients and radiologists\nthemselves.",
    "descriptor": "",
    "authors": [
      "Niklas Muennighoff"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07921"
  },
  {
    "id": "arXiv:2106.07922",
    "title": "An Automated Quality Evaluation Framework of Psychotherapy Conversations  with Local Quality Estimates",
    "abstract": "Computational approaches for assessing the quality of conversation-based\npsychotherapy, such as Cognitive Behavioral Therapy (CBT) and Motivational\nInterviewing (MI), have been developed recently to support quality assurance\nand clinical training. However, due to the long session lengths and limited\nmodeling resources, computational methods largely rely on frequency-based\nlexical features or distribution of dialogue acts. In this work, we propose a\nhierarchical framework to automatically evaluate the quality of a CBT\ninteraction. We divide each psychotherapy session into conversation segments\nand input those into a BERT-based model to produce segment embeddings. We first\nfine-tune BERT for predicting segment-level (local) quality scores and then use\nsegment embeddings as lower-level input to a Bidirectional LSTM-based neural\nnetwork to predict session-level (global) quality estimates. In particular, the\nsegment-level quality scores are initialized with the session-level scores and\nwe model the global quality as a function of the local quality scores to\nachieve the accurate segment-level quality estimates. These estimated\nsegment-level scores benefit theBERT fine-tuning and in learning better segment\nembeddings. We evaluate the proposed framework on data drawn from real-world\nCBT clinical session recordings to predict multiple session-level behavior\ncodes. The results indicate that our approach leads to improved evaluation\naccuracy for most codes in both regression and classification tasks.",
    "descriptor": "",
    "authors": [
      "Zhuohao Chen",
      "Nikolaos Flemotomos",
      "Karan Singla",
      "Torrey A. Creed",
      "David C. Atkins",
      "Shrikanth Narayanan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07922"
  },
  {
    "id": "arXiv:2106.07924",
    "title": "Improving Search by Utilizing State Information in OPTIC Planners  Compilation to LP",
    "abstract": "Automated planners are computer tools that allow autonomous agents to make\nstrategies and decisions by determining a set of actions for the agent that to\ntake, which will carry a system from a given initial state to the desired goal\nstate. Many planners are domain-independent, allowing their deployment in a\nvariety of domains. Such is the broad family of OPTIC planners. These planners\nperform Forward Search and call a Linear Programming (LP) solver multiple times\nat every state to check for consistency and to set bounds on the numeric\nvariables. These checks can be computationally costly, especially in real-life\napplications. This paper suggests a method for identifying information about\nthe specific state being evaluated, allowing the formulation of the equations\nto facilitate better solver selection and faster LP solving. The usefulness of\nthe method is demonstrated in six domains and is shown to enhance performance\nsignificantly.",
    "descriptor": "\nComments: 8 pages, 3 figures. Preprint, last submitted to the International Conference on Automated Planning and Scheduling (ICAPS 2021) at 21.01.2021\n",
    "authors": [
      "Elad Denenberg",
      "Amanda Coles",
      "Derek Long"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07924"
  },
  {
    "id": "arXiv:2106.07925",
    "title": "Machine Learning with Electronic Health Records is vulnerable to  Backdoor Trigger Attacks",
    "abstract": "Electronic Health Records (EHRs) provide a wealth of information for machine\nlearning algorithms to predict the patient outcome from the data including\ndiagnostic information, vital signals, lab tests, drug administration, and\ndemographic information. Machine learning models can be built, for example, to\nevaluate patients based on their predicted mortality or morbidity and to\npredict required resources for efficient resource management in hospitals. In\nthis paper, we demonstrate that an attacker can manipulate the machine learning\npredictions with EHRs easily and selectively at test time by backdoor attacks\nwith the poisoned training data. Furthermore, the poison we create has\nstatistically similar features to the original data making it hard to detect,\nand can also attack multiple machine learning models without any knowledge of\nthe models. With less than 5% of the raw EHR data poisoned, we achieve average\nattack success rates of 97% on mortality prediction tasks with MIMIC-III\ndatabase against Logistic Regression, Multilayer Perceptron, and Long\nShort-term Memory models simultaneously.",
    "descriptor": "",
    "authors": [
      "Byunggill Joe",
      "Akshay Mehra",
      "Insik Shin",
      "Jihun Hamm"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07925"
  },
  {
    "id": "arXiv:2106.07927",
    "title": "ReS2tAC -- UAV-Borne Real-Time SGM Stereo Optimized for Embedded ARM and  CUDA Devices",
    "abstract": "With the emergence of low-cost robotic systems, such as unmanned aerial\nvehicle, the importance of embedded high-performance image processing has\nincreased. For a long time, FPGAs were the only processing hardware that were\ncapable of high-performance computing, while at the same time preserving a low\npower consumption, essential for embedded systems. However, the recently\nincreasing availability of embedded GPU-based systems, such as the NVIDIA\nJetson series, comprised of an ARM CPU and a NVIDIA Tegra GPU, allows for\nmassively parallel embedded computing on graphics hardware. With this in mind,\nwe propose an approach for real-time embedded stereo processing on ARM and\nCUDA-enabled devices, which is based on the popular and widely used Semi-Global\nMatching algorithm. In this, we propose an optimization of the algorithm for\nembedded CUDA GPUs, by using massively parallel computing, as well as using the\nNEON intrinsics to optimize the algorithm for vectorized SIMD processing on\nembedded ARM CPUs. We have evaluated our approach with different configurations\non two public stereo benchmark datasets to demonstrate that they can reach an\nerror rate as low as 3.3%. Furthermore, our experiments show that the fastest\nconfiguration of our approach reaches up to 46 FPS on VGA image resolution.\nFinally, in a use-case specific qualitative evaluation, we have evaluated the\npower consumption of our approach and deployed it on the DJI Manifold 2-G\nattached to a DJI Matrix 210v2 RTK unmanned aerial vehicle (UAV), demonstrating\nits suitability for real-time stereo processing onboard a UAV.",
    "descriptor": "",
    "authors": [
      "Boitumelo Ruf",
      "Jonas Mohrs",
      "Martin Weinmann",
      "Stefan Hinz",
      "J\u00fcrgen Beyerer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07927"
  },
  {
    "id": "arXiv:2106.07929",
    "title": "Image Feature Information Extraction for Interest Point Detection: A  Comprehensive Review",
    "abstract": "Interest point detection is one of the most fundamental and critical problems\nin computer vision and image processing. In this paper, we carry out a\ncomprehensive review on image feature information (IFI) extraction techniques\nfor interest point detection. To systematically introduce how the existing\ninterest point detection methods extract IFI from an input image, we propose a\ntaxonomy of the IFI extraction techniques for interest point detection.\nAccording to this taxonomy, we discuss different types of IFI extraction\ntechniques for interest point detection. Furthermore, we identify the main\nunresolved issues related to the existing IFI extraction techniques for\ninterest point detection and any interest point detection methods that have not\nbeen discussed before. The existing popular datasets and evaluation standards\nare provided and the performances for eighteen state-of-the-art approaches are\nevaluated and discussed. Moreover, future research directions on IFI extraction\ntechniques for interest point detection are elaborated.",
    "descriptor": "",
    "authors": [
      "Junfeng Jing",
      "Tian Gao",
      "Weichuan Zhang",
      "Yongsheng Gao",
      "Changming Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07929"
  },
  {
    "id": "arXiv:2106.07930",
    "title": "Language Tags Matter for Zero-Shot Neural Machine Translation",
    "abstract": "Multilingual Neural Machine Translation (MNMT) has aroused widespread\ninterest due to its efficiency. An exciting advantage of MNMT models is that\nthey could also translate between unsupervised (zero-shot) language directions.\nLanguage tag (LT) strategies are often adopted to indicate the translation\ndirections in MNMT. In this paper, we demonstrate that the LTs are not only\nindicators for translation directions but also crucial to zero-shot translation\nqualities. Unfortunately, previous work tends to ignore the importance of LT\nstrategies. We demonstrate that a proper LT strategy could enhance the\nconsistency of semantic representations and alleviate the off-target issue in\nzero-shot directions. Experimental results show that by ignoring the source\nlanguage tag (SLT) and adding the target language tag (TLT) to the encoder, the\nzero-shot translations could achieve a +8 BLEU score difference over other LT\nstrategies in IWSLT17, Europarl, TED talks translation tasks.",
    "descriptor": "\nComments: 7 pages, 3 figures, Accepted by the Findings of ACL2021\n",
    "authors": [
      "Liwei Wu",
      "Shanbo Cheng",
      "Mingxuan Wang",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07930"
  },
  {
    "id": "arXiv:2106.07931",
    "title": "Does your robot know? Enhancing children's information retrieval through  spoken conversation with responsible robots",
    "abstract": "In this paper, we identify challenges in children's current information\nretrieval process, and propose conversational robots as an opportunity to ease\nthis process in a responsible way. Tools children currently use in this\nprocess, such as search engines on a computer or voice agents, do not always\nmeet their specific needs. The conversational robot we propose maintains\ncontext, asks clarifying questions, and gives suggestions in order to better\nmeet children's needs. Since children are often too trusting of robots, we\npropose to have the robot measure, monitor and adapt to the trust the child has\nin the robot. This way, we hope to induce a critical attitude with the children\nduring their information retrieval process.",
    "descriptor": "\nComments: IR4Children'21 workshop at SIGIR 2021 - this http URL\n",
    "authors": [
      "T. Beelen",
      "E. Velner",
      "R. Ordelman",
      "K.P. Truong",
      "V. Evers",
      "T. Huibers"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.07931"
  },
  {
    "id": "arXiv:2106.07932",
    "title": "Medical Code Prediction from Discharge Summary: Document to Sequence  BERT using Sequence Attention",
    "abstract": "Clinical notes are unstructured text generated by clinicians during patient\nencounters. Clinical notes are usually accompanied by a set of metadata codes\nfrom the international classification of diseases (ICD). ICD code is an\nimportant code used in a variety of operations, including insurance,\nreimbursement, medical diagnosis, etc. Therefore, it is important to classify\nICD codes quickly and accurately. However, annotating these codes is costly and\ntime-consuming. So we propose a model based on bidirectional encoder\nrepresentations from transformer (BERT) using the sequence attention method for\nautomatic ICD code assignment. We evaluate our ap-proach on the MIMIC-III\nbenchmark dataset. Our model achieved performance of Macro-aver-aged F1:\n0.62898 and Micro-averaged F1: 0.68555, and is performing better than a\nperformance of the previous state-of-the-art model. The contribution of this\nstudy proposes a method of using BERT that can be applied to documents and a\nsequence attention method that can capture im-portant sequence information\nappearing in documents.",
    "descriptor": "",
    "authors": [
      "Tak-Sung Heo",
      "Yongmin Yoo",
      "Yeongjoon Park",
      "Byeong-Cheol Jo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07932"
  },
  {
    "id": "arXiv:2106.07935",
    "title": "Knowledge-Rich BERT Embeddings for Readability Assessment",
    "abstract": "Automatic readability assessment (ARA) is the task of evaluating the level of\nease or difficulty of text documents for a target audience. For researchers,\none of the many open problems in the field is to make such models trained for\nthe task show efficacy even for low-resource languages. In this study, we\npropose an alternative way of utilizing the information-rich embeddings of BERT\nmodels through a joint-learning method combined with handcrafted linguistic\nfeatures for readability assessment. Results show that the proposed method\noutperforms classical approaches in readability assessment using English and\nFilipino datasets, and obtaining as high as 12.4% increase in F1 performance.\nWe also show that the knowledge encoded in BERT embeddings can be used as a\nsubstitute feature set for low-resource languages like Filipino with limited\nsemantic and syntactic NLP tools to explicitly extract feature values for the\ntask.",
    "descriptor": "",
    "authors": [
      "Joseph Marvin Imperial"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07935"
  },
  {
    "id": "arXiv:2106.07936",
    "title": "Modeling morphology with Linear Discriminative Learning: considerations  and design choices",
    "abstract": "This study addresses a series of methodological questions that arise when\nmodeling inflectional morphology with Linear Discriminative Learning. Taking\nthe semi-productive German noun system as example, we illustrate how decisions\nmade about the representation of form and meaning influence model performance.\nWe clarify that for modeling frequency effects in learning, it is essential to\nmake use of incremental learning rather than the endstate of learning. We also\ndiscuss how the model can be set up to approximate the learning of inflected\nwords in context. In addition, we illustrate how in this approach the wug task\ncan be modeled in considerable detail. In general, the model provides an\nexcellent memory for known words, but appropriately shows more limited\nperformance for unseen data, in line with the semi-productivity of German noun\ninflection and generalization performance of native German speakers.",
    "descriptor": "\nComments: 38 pages, 5 figures, 10 tables\n",
    "authors": [
      "Maria Heitmeier",
      "Yu-Ying Chuang",
      "R. Harald Baayen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07936"
  },
  {
    "id": "arXiv:2106.07938",
    "title": "User Pairing and Power Allocation for IRS-Assisted NOMA Systems with  Imperfect Phase Compensation",
    "abstract": "In this letter, we analyze the performance of the intelligent reflecting\nsurface (IRS) assisted downlink non-orthogonal multiple access (NOMA) systems\nin the presence of imperfect phase compensation. We derive an upper bound on\nthe imperfect phase compensation to achieve minimum required data rates for\neach user. Using this bound, we propose an adaptive user pairing algorithm to\nmaximize the network throughput. We then derive bounds on the power allocation\nfactors and propose power allocation algorithms for the paired users to achieve\nthe maximum sum rate or ensure fairness. Through extensive simulations, we show\nthat the proposed algorithms significantly outperform the state-of-the-art\nalgorithms.",
    "descriptor": "",
    "authors": [
      "Pavan Reddy M.",
      "Abhinav Kumar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.07938"
  },
  {
    "id": "arXiv:2106.07941",
    "title": "Direction-aware Feature-level Frequency Decomposition for Single Image  Deraining",
    "abstract": "We present a novel direction-aware feature-level frequency decomposition\nnetwork for single image deraining. Compared with existing solutions, the\nproposed network has three compelling characteristics. First, unlike previous\nalgorithms, we propose to perform frequency decomposition at feature-level\ninstead of image-level, allowing both low-frequency maps containing structures\nand high-frequency maps containing details to be continuously refined during\nthe training procedure. Second, we further establish communication channels\nbetween low-frequency maps and high-frequency maps to interactively capture\nstructures from high-frequency maps and add them back to low-frequency maps\nand, simultaneously, extract details from low-frequency maps and send them back\nto high-frequency maps, thereby removing rain streaks while preserving more\ndelicate features in the input image. Third, different from existing algorithms\nusing convolutional filters consistent in all directions, we propose a\ndirection-aware filter to capture the direction of rain streaks in order to\nmore effectively and thoroughly purge the input images of rain streaks. We\nextensively evaluate the proposed approach in three representative datasets and\nexperimental results corroborate our approach consistently outperforms\nstate-of-the-art deraining algorithms.",
    "descriptor": "",
    "authors": [
      "Sen Deng",
      "Yidan Feng",
      "Mingqiang Wei",
      "Haoran Xie",
      "Yiping Chen",
      "Jonathan Li",
      "Xiao-Ping Zhang",
      "Jing Qin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07941"
  },
  {
    "id": "arXiv:2106.07943",
    "title": "A Fast-Detection and Fault-Correction Algorithm against Persistent Fault  Attack",
    "abstract": "Persistent Fault Attack (PFA) is a recently proposed Fault Attack (FA) method\nin CHES 2018. It is able to recover full AES secret key in the\nSingle-Byte-Fault scenario. It is demonstrated that classical FA\ncountermeasures, such as Dual Modular Redundancy (DMR) and mask protection, are\nunable to thwart PFA. In this paper, we propose a fast-detection and\nfaultcorrection algorithm to prevent PFA. We construct a fixed input and output\npair to detect faults rapidly. Then we build two extra redundant tables to\nstore the relationship between the adjacent elements in the S-box, by which the\nalgorithm can correct the faulty elements in the S-box. Our experimental\nresults show that our algorithm can effectively prevent PFA in both\nSingle-ByteFault and Multiple-Bytes-Faults scenarios. Compared with the\nclassical FA countermeasures, our algorithm has a much better effect against\nPFA. Further, the time cost of our algorithm is 40% lower than the classical FA\ncountermeasures.",
    "descriptor": "",
    "authors": [
      "Yukun Cheng",
      "Mengce Zheng",
      "Fan Huang",
      "Jiajia Zhang",
      "Honggang Hu",
      "Nenghai Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.07943"
  },
  {
    "id": "arXiv:2106.07944",
    "title": "Simplifying Robot Programming using Augmented Reality and End-User  Development",
    "abstract": "Robots are widespread across diverse application contexts. Teaching robots to\nperform tasks, in their respective contexts, demands a high domain and\nprogramming expertise. However, robot programming faces high entry barriers due\nto the complexity of robot programming itself. Even for experts robot\nprogramming is a cumbersome and error-prone task where faulty robot programs\ncan be created, causing damage when being executed on a real robot. To simplify\nthe process of robot programming, we combine Augmented Reality (AR) with\nprinciples of end-user development. By combining them, the real environment is\nextended with useful virtual artifacts that can enable experts as well as\nnon-professionals to perform complex robot programming tasks. Therefore, Simple\nProgramming Environment in Augmented Reality with Enhanced Debugging (SPEARED)\nwas developed as a prototype for an AR-assisted robot programming environment.\nSPEARED makes use of AR to project a robot as well as a programming environment\nonto the target working space. To evaluate our approach, expert interviews with\ndomain experts from the area of industrial automation, robotics, and AR were\nperformed. The experts agreed that SPEARED has the potential to enrich and ease\ncurrent robot programming processes.",
    "descriptor": "\nComments: Preprint - Accepted as Full Paper at the 18th IFIP TC13 International Conference on Human-Computer Interaction (INTERACT 2021)\n",
    "authors": [
      "Enes Yigitbas",
      "Ivan Jovanovikj",
      "Gregor Engels"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.07944"
  },
  {
    "id": "arXiv:2106.07947",
    "title": "Deriving Word Vectors from Contextualized Language Models using  Topic-Aware Mention Selection",
    "abstract": "One of the long-standing challenges in lexical semantics consists in learning\nrepresentations of words which reflect their semantic properties. The\nremarkable success of word embeddings for this purpose suggests that\nhigh-quality representations can be obtained by summarizing the sentence\ncontexts of word mentions. In this paper, we propose a method for learning word\nrepresentations that follows this basic strategy, but differs from standard\nword embeddings in two important ways. First, we take advantage of\ncontextualized language models (CLMs) rather than bags of word vectors to\nencode contexts. Second, rather than learning a word vector directly, we use a\ntopic model to partition the contexts in which words appear, and then learn\ndifferent topic-specific vectors for each word. Finally, we use a task-specific\nsupervision signal to make a soft selection of the resulting vectors. We show\nthat this simple strategy leads to high-quality word vectors, which are more\npredictive of semantic properties than word embeddings and existing CLM-based\nstrategies.",
    "descriptor": "",
    "authors": [
      "Yixiao Wang",
      "Zied Bouraoui",
      "Luis Espinosa Anke",
      "Steven Schockaert"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07947"
  },
  {
    "id": "arXiv:2106.07951",
    "title": "Maximal regularity of backward difference time discretization for  evolving surface PDEs and its application to nonlinear problems",
    "abstract": "Maximal parabolic $L^p$-regularity of linear parabolic equations on an\nevolving surface is shown by pulling back the problem to the initial surface\nand studying the maximal $L^p$-regularity on a fixed surface. By freezing the\ncoefficients in the parabolic equations at a fixed time and utilizing a\nperturbation argument around the freezed time, it is shown that backward\ndifference time discretizations of linear parabolic equations on an evolving\nsurface along characteristic trajectories can preserve maximal $L^p$-regularity\nin the discrete setting. The result is applied to prove the stability and\nconvergence of time discretizations of nonlinear parabolic equations on an\nevolving surface, with linearly implicit backward differentiation formulae\ncharacteristic trajectories of the surface, for general locally Lipschitz\nnonlinearities. The discrete maximal $L^p$-regularity is used to prove the\nboundedness and stability of numerical solutions in the\n$L^\\infty(0,T;W^{1,\\infty})$ norm, which is used to bound the nonlinear terms\nin the stability analysis. Optimal-order error estimates of time\ndiscretizations in the $L^\\infty(0,T;W^{1,\\infty})$ norm is obtained by\ncombining the stability analysis with the consistency estimates.",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs Kov\u00e1cs",
      "Buyang Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.07951"
  },
  {
    "id": "arXiv:2106.07952",
    "title": "Enforcing Statistical Orthogonality in Massive MIMO Systems via  Covariance Shaping",
    "abstract": "This paper tackles the problem of downlink transmission in massive\nmultiple-input multiple-output(MIMO) systems where the equipments (UEs) exhibit\nhigh spatial correlation and the channel estimation is limited by strong pilot\ncontamination. Signal subspace separation among the UEs is, in fact, rarely\nrealized in practice and is generally beyond the control of the network\ndesigner (as it is dictated by the physical scattering environment). In this\ncontext, we propose a novel statistical beamforming technique, referred to\nasMIMO covariance shaping, that exploits multiple antennas at the UEs and\nleverages the realistic non-Kronecker structure of massive MIMO channels to\ntarget a suitable shaping of the channel statistics performed at the UE-side.\nTo optimize the covariance shaping strategies, we propose a low-complexity\nblock coordinate descent algorithm that is proved to converge to a limit point\nof the original nonconvex problem. For the two-UE case, this is shown to\nconverge to a stationary point of the original problem. Numerical results\nillustrate the sum-rate performance gains of the proposed method with respect\nto reference scenarios employing the multiple antennas at the UE for spatial\nmultiplexing.",
    "descriptor": "\nComments: Submitted for journal publication\n",
    "authors": [
      "Placido Mursia",
      "Italo Atzeni",
      "Laura Cottatellucci",
      "David Gesbert"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.07952"
  },
  {
    "id": "arXiv:2106.07954",
    "title": "CatBoost model with synthetic features in application to loan risk  assessment of small businesses",
    "abstract": "Loan risk for small business has long been a complex problem worthy of\nexploring. Predicting the loan risk approximately can benefit entrepreneurship\nby developing more jobs for the society. CatBoost (Categorical Boosting) is a\npowerful machine learning algorithm that is suitable for dataset with many\ncategorical variables like the dataset for forecasting loan risk. In this\npaper, we identify the important risk factors that contribute to loan status\nclassification problem. Then we compare the the performance between\nboosting-type algorithms(especially CatBoost) with other traditional yet\npopular ones. The dataset we adopt in the research comes from the U.S. Small\nBusiness Administration (SBA) and holds a very large sample size (899,164\nobservations and 27 features). We obtain a high accuracy of 95.74% and\nwell-performed AUC of 98.59% compared with the existent literature of related\nresearch. In order to make best use of the important features in the dataset,\nwe propose a technique named \"synthetic generation\" to develop more combined\nfeatures based on arithmetic operation, which ends up improving the accuracy\nand AUC of original CatBoost model.",
    "descriptor": "",
    "authors": [
      "Liexing Cheng",
      "Haoxue Wang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07954"
  },
  {
    "id": "arXiv:2106.07955",
    "title": "Cascading Convolutional Temporal Colour Constancy",
    "abstract": "Computational Colour Constancy (CCC) consists of estimating the colour of one\nor more illuminants in a scene and using them to remove unwanted chromatic\ndistortions. Much research has focused on illuminant estimation for CCC on\nsingle images, with few attempts of leveraging the temporal information\nintrinsic in sequences of correlated images (e.g., the frames in a video), a\ntask known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is\nTCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the\nencodings produced by CNN submodules for each image in a sequence. We extend\nthis architecture with different models obtained by (i) substituting the TCCNet\nsubmodules with C4, the state-of-the-art method for CCC targeting images; (ii)\nadding a cascading strategy to perform an iterative improvement of the estimate\nof the illuminant. We tested our models on the recently released TCC benchmark\nand achieved results that surpass the state-of-the-art. Analyzing the impact of\nthe number of frames involved in illuminant estimation on performance, we show\nthat it is possible to reduce inference time by training the models on few\nselected frames from the sequences while retaining comparable accuracy.",
    "descriptor": "",
    "authors": [
      "Matteo Rizzo",
      "Cristina Conati",
      "Daesik Jang",
      "Hui Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07955"
  },
  {
    "id": "arXiv:2106.07959",
    "title": "Zero-sample surface defect detection and classification based on  semantic feedback neural network",
    "abstract": "Defect detection and classification technology has changed from traditional\nartificial visual inspection to current intelligent automated inspection, but\nmost of the current defect detection methods are training related detection\nmodels based on a data-driven approach, taking into account the difficulty of\ncollecting some sample data in the industrial field. We apply zero-shot\nlearning technology to the industrial field. Aiming at the problem of the\nexisting \"Latent Feature Guide Attribute Attention\" (LFGAA) zero-shot image\nclassification network, the output latent attributes and artificially defined\nattributes are different in the semantic space, which leads to the problem of\nmodel performance degradation, proposed an LGFAA network based on semantic\nfeedback, and improved model performance by constructing semantic embedded\nmodules and feedback mechanisms. At the same time, for the common domain shift\nproblem in zero-shot learning, based on the idea of co-training algorithm using\nthe difference information between different views of data to learn from each\nother, we propose an Ensemble Co-training algorithm, which adaptively reduces\nthe prediction error in image tag embedding from multiple angles. Various\nexperiments conducted on the zero-shot dataset and the cylinder liner dataset\nin the industrial field provide competitive results.",
    "descriptor": "\nComments: 28 pages 10 figures\n",
    "authors": [
      "Yibo Guo",
      "Yiming Fan",
      "Zhiyang Xiang",
      "Haidi Wang",
      "Wenhua Meng",
      "Mingliang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07959"
  },
  {
    "id": "arXiv:2106.07961",
    "title": "On the Evaluation of Sequential Machine Learning for Network Intrusion  Detection",
    "abstract": "Recent advances in deep learning renewed the research interests in machine\nlearning for Network Intrusion Detection Systems (NIDS). Specifically,\nattention has been given to sequential learning models, due to their ability to\nextract the temporal characteristics of Network traffic Flows (NetFlows), and\nuse them for NIDS tasks. However, the applications of these sequential models\noften consist of transferring and adapting methodologies directly from other\nfields, without an in-depth investigation on how to leverage the specific\ncircumstances of cybersecurity scenarios; moreover, there is a lack of\ncomprehensive studies on sequential models that rely on NetFlow data, which\npresents significant advantages over traditional full packet captures. We\ntackle this problem in this paper. We propose a detailed methodology to extract\ntemporal sequences of NetFlows that denote patterns of malicious activities.\nThen, we apply this methodology to compare the efficacy of sequential learning\nmodels against traditional static learning models. In particular, we perform a\nfair comparison of a `sequential' Long Short-Term Memory (LSTM) against a\n`static' Feedforward Neural Networks (FNN) in distinct environments represented\nby two well-known datasets for NIDS: the CICIDS2017 and the CTU13. Our results\nhighlight that LSTM achieves comparable performance to FNN in the CICIDS2017\nwith over 99.5\\% F1-score; while obtaining superior performance in the CTU13,\nwith 95.7\\% F1-score against 91.5\\%. This paper thus paves the way to future\napplications of sequential learning models for NIDS.",
    "descriptor": "",
    "authors": [
      "Andrea Corsini",
      "Shanchieh Jay Yang",
      "Giovanni Apruzzese"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07961"
  },
  {
    "id": "arXiv:2106.07962",
    "title": "Cyclic codes over a non-chain ring $R_{e,q}$ and their application to  LCD codes",
    "abstract": "Let $\\mathbb{F}_q$ be a finite field of order $q$, a prime power integer such\nthat $q=et+1$ where $t\\geq 1,e\\geq 2$ are integers. In this paper, we study\ncyclic codes of length $n$ over a non-chain ring\n$R_{e,q}=\\mathbb{F}_q[u]/\\langle u^e-1\\rangle$. We define a Gray map $\\varphi$\nand obtain many { maximum-distance-separable} (MDS) and optimal\n$\\mathbb{F}_q$-linear codes from the Gray images of cyclic codes. Under certain\nconditions we determine { linear complementary dual} (LCD) codes of length $n$\nwhen $\\gcd(n,q)\\neq 1$ and $\\gcd(n,q)= 1$, respectively. It is proved that { a}\ncyclic code $\\mathcal{C}$ of length $n$ is an LCD code if and only if its Gray\nimage $\\varphi(\\mathcal{C})$ is an LCD code of length $4n$ over $\\mathbb{F}_q$.\nAmong others, we present the conditions for existence of free and non-free LCD\ncodes. Moreover, we obtain many optimal LCD codes as the Gray images of\nnon-free LCD codes over $R_{e,q}$.",
    "descriptor": "\nComments: Submitted to Discrete Mathematics\n",
    "authors": [
      "Habibul Islam",
      "Edgar Mart\u00ednez-Moro",
      "Om Prakash"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.07962"
  },
  {
    "id": "arXiv:2106.07964",
    "title": "Improving the List Decoding Version of the Cyclically Equivariant Neural  Decoder",
    "abstract": "The cyclically equivariant neural decoder was recently proposed in [Chen-Ye,\nInternational Conference on Machine Learning, 2021] to decode cyclic codes. In\nthe same paper, a list decoding procedure was also introduced for two widely\nused classes of cyclic codes -- BCH codes and punctured Reed-Muller (RM) codes.\nWhile the list decoding procedure significantly improves the Frame Error Rate\n(FER) of the cyclically equivariant neural decoder, the Bit Error Rate (BER) of\nthe list decoding procedure is even worse than the unique decoding algorithm\nwhen the list size is small. In this paper, we propose an improved version of\nthe list decoding algorithm for BCH codes and punctured RM codes. Our new\nproposal significantly reduces the BER while maintaining the same (in some\ncases even smaller) FER. More specifically, our new decoder provides up to\n$2$dB gain over the previous list decoder when measured by BER, and the running\ntime of our new decoder is $15\\%$ smaller. Code available at\nhttps://github.com/improvedlistdecoder/code",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Xiangyu Chen",
      "Min Ye"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07964"
  },
  {
    "id": "arXiv:2106.07967",
    "title": "Incorporating Word Sense Disambiguation in Neural Language Models",
    "abstract": "We present two supervised (pre-)training methods to incorporate gloss\ndefinitions from lexical resources into neural language models (LMs). The\ntraining improves our models' performance for Word Sense Disambiguation (WSD)\nbut also benefits general language understanding tasks while adding almost no\nparameters. We evaluate our techniques with seven different neural LMs and find\nthat XLNet is more suitable for WSD than BERT. Our best-performing methods\nexceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1\nand increase BERT's performance on the GLUE benchmark by 1.1% on average.",
    "descriptor": "",
    "authors": [
      "Jan Philip Wahle",
      "Terry Ruas",
      "Norman Meuschke",
      "Bela Gipp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.07967"
  },
  {
    "id": "arXiv:2106.07971",
    "title": "Very Deep Graph Neural Networks Via Noise Regularisation",
    "abstract": "Graph Neural Networks (GNNs) perform learned message passing over an input\ngraph, but conventional wisdom says performing more than handful of steps makes\ntraining difficult and does not yield improved performance. Here we show the\ncontrary. We train a deep GNN with up to 100 message passing steps and achieve\nseveral state-of-the-art results on two challenging molecular property\nprediction benchmarks, Open Catalyst 2020 IS2RE and QM9. Our approach depends\ncrucially on a novel but simple regularisation method, which we call ``Noisy\nNodes'', in which we corrupt the input graph with noise and add an auxiliary\nnode autoencoder loss if the task is graph property prediction. Our results\nshow this regularisation method allows the model to monotonically improve in\nperformance with increased message passing steps. Our work opens new\nopportunities for reaping the benefits of deep neural networks in the space of\ngraph and other structured prediction problems.",
    "descriptor": "",
    "authors": [
      "Jonathan Godwin",
      "Michael Schaarschmidt",
      "Alexander Gaunt",
      "Alvaro Sanchez-Gonzalez",
      "Yulia Rubanova",
      "Petar Veli\u010dkovi\u0107",
      "James Kirkpatrick",
      "Peter Battaglia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07971"
  },
  {
    "id": "arXiv:2106.07976",
    "title": "Federated Learning for Internet of Things: A Federated Learning  Framework for On-device Anomaly Data Detection",
    "abstract": "Federated learning can be a promising solution for enabling IoT cybersecurity\n(i.e., anomaly detection in the IoT environment) while preserving data privacy\nand mitigating the high communication/storage overhead (e.g., high-frequency\ndata from time-series sensors) of centralized over-the-cloud approaches. In\nthis paper, to further push forward this direction with a comprehensive study\nin both algorithm and system design, we build FedIoT platform that contains a\nsynthesized dataset using N-BaIoT, FedDetect algorithm, and a system design for\nIoT devices. Furthermore, the proposed FedDetect learning framework improves\nthe performance by utilizing an adaptive optimizer (e.g., Adam) and a\ncross-round learning rate scheduler. In a network of realistic IoT devices\n(Raspberry PI), we evaluate FedIoT platform and FedDetect algorithm in both\nmodel and system performance. Our results demonstrate the efficacy of federated\nlearning in detecting a large range of attack types. The system efficiency\nanalysis indicates that both end-to-end training time and memory cost are\naffordable and promising for resource-constrained IoT devices. The source code\nis publicly available.",
    "descriptor": "",
    "authors": [
      "Tuo Zhang",
      "Chaoyang He",
      "Tianhao Ma",
      "Mark Ma",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.07976"
  },
  {
    "id": "arXiv:2106.07980",
    "title": "Grounds for Suspicion: Physics-based Early Warnings for Stealthy Attacks  on Industrial Control Systems",
    "abstract": "Stealthy attacks on Industrial Control Systems can cause significant damage\nwhile evading detection. In this paper, instead of focusing on the detection of\nstealthy attacks, we aim to provide early warnings to operators, in order to\navoid physical damage and preserve in advance data that may serve as an\nevidence during an investigation. We propose a framework to provide grounds for\nsuspicion, i.e. preliminary indicators reflecting the likelihood of success of\na stealthy attack. We propose two grounds for suspicion based on the behaviour\nof the physical process: (i) feasibility of a stealthy attack, and (ii)\nproximity to unsafe operating regions. We propose a metric to measure grounds\nfor suspicion in real-time and provide soundness principles to ensure that such\na metric is consistent with the grounds for suspicion. We apply our framework\nto Linear Time-Invariant (LTI) systems and formulate the suspicion metric\ncomputation as a real-time reachability problem. We validate our framework on a\ncase study involving the benchmark Tennessee-Eastman process. We show through\nnumerical simulation that we can provide early warnings well before a potential\nstealthy attack can cause damage, while incurring minimal load on the network.\nFinally, we apply our framework on a use case to illustrate its usefulness in\nsupporting early evidence collection.",
    "descriptor": "",
    "authors": [
      "Mazen Azzam",
      "Liliana Pasquale",
      "Gregory Provan",
      "Bashar Nuseibeh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.07980"
  },
  {
    "id": "arXiv:2106.07984",
    "title": "Learning Autonomy in Management of Wireless Random Networks",
    "abstract": "This paper presents a machine learning strategy that tackles a distributed\noptimization task in a wireless network with an arbitrary number of randomly\ninterconnected nodes. Individual nodes decide their optimal states with\ndistributed coordination among other nodes through randomly varying backhaul\nlinks. This poses a technical challenge in distributed universal optimization\npolicy robust to a random topology of the wireless network, which has not been\nproperly addressed by conventional deep neural networks (DNNs) with rigid\nstructural configurations. We develop a flexible DNN formalism termed\ndistributed message-passing neural network (DMPNN) with forward and backward\ncomputations independent of the network topology. A key enabler of this\napproach is an iterative message-sharing strategy through arbitrarily connected\nbackhaul links. The DMPNN provides a convergent solution for iterative\ncoordination by learning numerous random backhaul interactions. The DMPNN is\ninvestigated for various configurations of the power control in wireless\nnetworks, and intensive numerical results prove its universality and viability\nover conventional optimization and DNN approaches.",
    "descriptor": "\nComments: to appear in IEEE TWC\n",
    "authors": [
      "Hoon Lee",
      "Sang Hyun Lee",
      "Tony Q. S. Quek"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07984"
  },
  {
    "id": "arXiv:2106.07989",
    "title": "Compression Implies Generalization",
    "abstract": "Explaining the surprising generalization performance of deep neural networks\nis an active and important line of research in theoretical machine learning.\nInfluential work by Arora et al. (ICML'18) showed that, noise stability\nproperties of deep nets occurring in practice can be used to provably compress\nmodel representations. They then argued that the small representations of\ncompressed networks imply good generalization performance albeit only of the\ncompressed nets. Extending their compression framework to yield generalization\nbounds for the original uncompressed networks remains elusive.\nOur main contribution is the establishment of a compression-based framework\nfor proving generalization bounds. The framework is simple and powerful enough\nto extend the generalization bounds by Arora et al. to also hold for the\noriginal network. To demonstrate the flexibility of the framework, we also show\nthat it allows us to give simple proofs of the strongest known generalization\nbounds for other popular machine learning models, namely Support Vector\nMachines and Boosting.",
    "descriptor": "",
    "authors": [
      "Allan Gr\u00f8nlund",
      "Mikael H\u00f8gsgaard",
      "Lior Kamma",
      "Kasper Green Larsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07989"
  },
  {
    "id": "arXiv:2106.07990",
    "title": "The Connection between Process Complexity of Event Sequences and Models  discovered by Process Mining",
    "abstract": "Process mining is a research area focusing on the design of algorithms that\ncan automatically provide insights into business processes by analysing\nhistoric process execution data, known as event logs. Among the most popular\nalgorithms are those for automated process discovery, whose ultimate goal is to\ngenerate the best process model that summarizes the behaviour recorded in the\ninput event log. Over the past decade, several process discovery algorithms\nhave been proposed but, until now, this research was driven by the implicit\nassumption that a better algorithm would discover better process models, no\nmatter the characteristics of the input event log. In this paper, we take a\nstep back and question that assumption. Specifically, we investigate what are\nthe relations between measures capturing characteristics of the input event log\nand the quality of the discovered process models. To this end, we review the\nstate-of-the-art process complexity measures, propose a new process complexity\nmeasure based on graph entropy, and analyze this set of complexity measures on\nan extensive collection of event logs and corresponding automatically\ndiscovered process models. Our analysis shows that many process complexity\nmeasures correlate with the quality of the discovered process models,\ndemonstrating the potential of using complexity measures as predictors for the\nquality of process models discovered with state-of-the-art process discovery\nalgorithms. This finding is important for process mining research, as it\nhighlights that not only algorithms, but also connections between input data\nand output quality should be studied.",
    "descriptor": "",
    "authors": [
      "Adriano Augusto",
      "Jan Mendling",
      "Maxim Vidgof",
      "Bastian Wurm"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.07990"
  },
  {
    "id": "arXiv:2106.07992",
    "title": "Time Series Anomaly Detection for Cyber-physical Systems via Neural  System Identification and Bayesian Filtering",
    "abstract": "Recent advances in AIoT technologies have led to an increasing popularity of\nutilizing machine learning algorithms to detect operational failures for\ncyber-physical systems (CPS). In its basic form, an anomaly detection module\nmonitors the sensor measurements and actuator states from the physical plant,\nand detects anomalies in these measurements to identify abnormal operation\nstatus. Nevertheless, building effective anomaly detection models for CPS is\nrather challenging as the model has to accurately detect anomalies in presence\nof highly complicated system dynamics and unknown amount of sensor noise. In\nthis work, we propose a novel time series anomaly detection method called\nNeural System Identification and Bayesian Filtering (NSIBF) in which a\nspecially crafted neural network architecture is posed for system\nidentification, i.e., capturing the dynamics of CPS in a dynamical state-space\nmodel; then a Bayesian filtering algorithm is naturally applied on top of the\n\"identified\" state-space model for robust anomaly detection by tracking the\nuncertainty of the hidden state of the system recursively over time. We provide\nqualitative as well as quantitative experiments with the proposed method on a\nsynthetic and three real-world CPS datasets, showing that NSIBF compares\nfavorably to the state-of-the-art methods with considerable improvements on\nanomaly detection in CPS.",
    "descriptor": "\nComments: Accepted to appear in KDD 2021\n",
    "authors": [
      "Cheng Feng",
      "Pengwei Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07992"
  },
  {
    "id": "arXiv:2106.07995",
    "title": "End-to-End Learning of Keypoint Representations for Continuous Control  from Images",
    "abstract": "In many control problems that include vision, optimal controls can be\ninferred from the location of the objects in the scene. This information can be\nrepresented using keypoints, which is a list of spatial locations in the input\nimage. Previous works show that keypoint representations learned during\nunsupervised pre-training using encoder-decoder architectures can provide good\nfeatures for control tasks. In this paper, we show that it is possible to learn\nefficient keypoint representations end-to-end, without the need for\nunsupervised pre-training, decoders, or additional losses. Our proposed\narchitecture consists of a differentiable keypoint extractor that feeds the\ncoordinates of the estimated keypoints directly to a soft actor-critic agent.\nThe proposed algorithm yields performance competitive to the state-of-the art\non DeepMind Control Suite tasks.",
    "descriptor": "",
    "authors": [
      "Rinu Boney",
      "Alexander Ilin",
      "Juho Kannala"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.07995"
  },
  {
    "id": "arXiv:2106.07997",
    "title": "Intelligent Reflecting Surface Aided Wireless Energy and Information  Transmission: An Overview",
    "abstract": "Intelligent reflecting surface (IRS) is a promising technology for achieving\nspectrum and energy efficient wireless networks cost-effectively. While most\nexisting works focus on exploiting IRS to enhance the performance of wireless\ninformation transmission (WIT), the high beamforming gain achieved by passive\nIRS is also appealing for boosting the efficiency of radio-frequency (RF) based\nwireless power transfer (WPT) and improving its service coverage. Although\nIRS-aided WPT shares certain similar characteristics with IRS-aided WIT, they\nalso differ significantly in terms of the design objectives,\ntransmitter/receiver architectures and hardware/power constraints, and so on,\nwhich thus may lead to distinct solutions in practice. In this paper, we\nprovide a tutorial overview on IRS-aided WPT and its more complicated\nintegrations with WIT, namely IRS-aided simultaneous wireless information and\npower transfer (SWIPT) and IRS-aided wireless powered communication networks\n(WPCNs), from the communication and signal processing perspective. In\nparticular, we identify their fundamental challenges in e.g. passive reflection\noptimization and channel estimation, propose potential solutions and draw\nuseful insights for practical design. Furthermore, we point out important\ndirections worthy of further investigation in future work.",
    "descriptor": "\nComments: This is a submission invited by Proceedings of the IEEE\n",
    "authors": [
      "Qingqing Wu",
      "Xinrong Guan",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Hardware Architecture (cs.AR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.07997"
  },
  {
    "id": "arXiv:2106.07998",
    "title": "Revisiting the Calibration of Modern Neural Networks",
    "abstract": "Accurate estimation of predictive uncertainty (model calibration) is\nessential for the safe application of neural networks. Many instances of\nmiscalibration in modern neural networks have been reported, suggesting a trend\nthat newer, more accurate models produce poorly calibrated predictions. Here,\nwe revisit this question for recent state-of-the-art image classification\nmodels. We systematically relate model calibration and accuracy, and find that\nthe most recent models, notably those not using convolutions, are among the\nbest calibrated. Trends observed in prior model generations, such as decay of\ncalibration with distribution shift or model size, are less pronounced in\nrecent architectures. We also show that model size and amount of pretraining do\nnot fully explain these differences, suggesting that architecture is a major\ndeterminant of calibration properties.",
    "descriptor": "",
    "authors": [
      "Matthias Minderer",
      "Josip Djolonga",
      "Rob Romijnders",
      "Frances Hubis",
      "Xiaohua Zhai",
      "Neil Houlsby",
      "Dustin Tran",
      "Mario Lucic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07998"
  },
  {
    "id": "arXiv:2106.07999",
    "title": "ARTA: Collection and Classification of Ambiguous Requests and Thoughtful  Actions",
    "abstract": "Human-assisting systems such as dialogue systems must take thoughtful,\nappropriate actions not only for clear and unambiguous user requests, but also\nfor ambiguous user requests, even if the users themselves are not aware of\ntheir potential requirements. To construct such a dialogue agent, we collected\na corpus and developed a model that classifies ambiguous user requests into\ncorresponding system actions. In order to collect a high-quality corpus, we\nasked workers to input antecedent user requests whose pre-defined actions could\nbe regarded as thoughtful. Although multiple actions could be identified as\nthoughtful for a single user request, annotating all combinations of user\nrequests and system actions is impractical. For this reason, we fully annotated\nonly the test data and left the annotation of the training data incomplete. In\norder to train the classification model on such training data, we applied the\npositive/unlabeled (PU) learning method, which assumes that only a part of the\ndata is labeled with positive examples. The experimental results show that the\nPU learning method achieved better performance than the general\npositive/negative (PN) learning method to classify thoughtful actions given an\nambiguous user request.",
    "descriptor": "\nComments: Accepted by The 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL2021)\n",
    "authors": [
      "Shohei Tanaka",
      "Koichiro Yoshino",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07999"
  },
  {
    "id": "arXiv:2106.08003",
    "title": "On the Queue Number of Planar Graphs",
    "abstract": "A k-queue layout is a special type of a linear layout, in which the linear\norder avoids (k+1)-rainbows, i.e., k+1 independent edges that pairwise form a\nnested pair. The optimization goal is to determine the queue number of a graph,\ni.e., the minimum value of k for which a k-queue layout is feasible. Recently,\nDujmovi\\'c et al. [J. ACM, 67(4), 22:1-38, 2020] showed that the queue number\nof planar graphs is at most 49, thus settling in the positive a long-standing\nconjecture by Heath, Leighton and Rosenberg. To achieve this breakthrough\nresult, their approach involves three different techniques: (i) an algorithm to\nobtain straight-line drawings of outerplanar graphs, in which the y-distance of\nany two adjacent vertices is 1 or 2, (ii) an algorithm to obtain 5-queue\nlayouts of planar 3-trees, and (iii) a decomposition of a planar graph into\nso-called tripods. In this work, we push further each of these techniques to\nobtain the first non-trivial improvement on the upper bound from 49 to 42.",
    "descriptor": "",
    "authors": [
      "Michael A. Bekos",
      "Martin Gronemann",
      "Chrysanthi N. Raftopoulou"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.08003"
  },
  {
    "id": "arXiv:2106.08004",
    "title": "Adaptive Margin Circle Loss for Speaker Verification",
    "abstract": "Deep-Neural-Network (DNN) based speaker verification sys-tems use the angular\nsoftmax loss with margin penalties toenhance the intra-class compactness of\nspeaker embeddings,which achieved remarkable performance. In this paper, we\npro-pose a novel angular loss function called adaptive margin cir-cle loss for\nspeaker verification. The stage-based margin andchunk-based margin are applied\nto improve the angular discrim-ination of circle loss on the training set. The\nanalysis on gradi-ents shows that, compared with the previous angular loss\nlikeAdditive Margin Softmax(Am-Softmax), circle loss has flexi-ble optimization\nand definite convergence status. Experimentsare carried out on the Voxceleb and\nSITW. By applying adap-tive margin circle loss, our best system achieves\n1.31%EER onVoxceleb1 and 2.13% on SITW core-core.",
    "descriptor": "\nComments: Accepted by Interspeech 2021\n",
    "authors": [
      "Runqiu Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.08004"
  },
  {
    "id": "arXiv:2106.08005",
    "title": "SAR Image Classification Based on Spiking Neural Network through  Spike-Time Dependent Plasticity and Gradient Descent",
    "abstract": "At present, the Synthetic Aperture Radar (SAR) image classification method\nbased on convolution neural network (CNN) has faced some problems such as poor\nnoise resistance and generalization ability. Spiking neural network (SNN) is\none of the core components of brain-like intelligence and has good application\nprospects. This article constructs a complete SAR image classifier based on\nunsupervised and supervised learning of SNN by using spike sequences with\ncomplex spatio-temporal information. We firstly expound the spiking neuron\nmodel, the receptive field of SNN, and the construction of spike sequence. Then\nwe put forward an unsupervised learning algorithm based on STDP and a\nsupervised learning algorithm based on gradient descent. The average\nclassification accuracy of single layer and bilayer unsupervised learning SNN\nin three categories images on MSTAR dataset is 80.8\\% and 85.1\\%, respectively.\nFurthermore, the convergent output spike sequences of unsupervised learning can\nbe used as teaching signals. Based on the TensorFlow framework, a single layer\nsupervised learning SNN is built from the bottom, and the classification\naccuracy reaches 90.05\\%. By comparing noise resistance and model parameters\nbetween SNNs and CNNs, the effectiveness and outstanding advantages of SNN are\nverified. Code to reproduce our experiments is available at\n\\url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.",
    "descriptor": "",
    "authors": [
      "Jiankun Chen",
      "Xiaolan Qiu",
      "Chibiao Ding",
      "Yirong Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08005"
  },
  {
    "id": "arXiv:2106.08007",
    "title": "Unsupervised Abstractive Opinion Summarization by Generating Sentences  with Tree-Structured Topic Guidance",
    "abstract": "This paper presents a novel unsupervised abstractive summarization method for\nopinionated texts. While the basic variational autoencoder-based models assume\na unimodal Gaussian prior for the latent code of sentences, we alternate it\nwith a recursive Gaussian mixture, where each mixture component corresponds to\nthe latent code of a topic sentence and is mixed by a tree-structured topic\ndistribution. By decoding each Gaussian component, we generate sentences with\ntree-structured topic guidance, where the root sentence conveys generic\ncontent, and the leaf sentences describe specific topics. Experimental results\ndemonstrate that the generated topic sentences are appropriate as a summary of\nopinionated texts, which are more informative and cover more input contents\nthan those generated by the recent unsupervised summarization model\n(Bra\\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of\nlatent Gaussians represents the granularity of sentences, analogous to Gaussian\nword embedding (Vilnis and McCallum, 2015).",
    "descriptor": "\nComments: accepted to TACL, pre-MIT Press publication version\n",
    "authors": [
      "Masaru Isonuma",
      "Junichiro Mori",
      "Danushka Bollegala",
      "Ichiro Sakata"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08007"
  },
  {
    "id": "arXiv:2106.08009",
    "title": "Compositional Sketch Search",
    "abstract": "We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.",
    "descriptor": "\nComments: ICIP 2021 camera-ready version\n",
    "authors": [
      "Alexander Black",
      "Tu Bui",
      "Long Mai",
      "Hailin Jin",
      "John Collomosse"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08009"
  },
  {
    "id": "arXiv:2106.08011",
    "title": "Over-the-Air Decentralized Federated Learning",
    "abstract": "In this paper, we consider decentralized federated learning (FL) over\nwireless networks, where over-the-air computation (AirComp) is adopted to\nfacilitate the local model consensus in a device-to-device (D2D) communication\nmanner. However, the AirComp-based consensus phase brings the additive noise in\neach algorithm iterate and the consensus needs to be robust to wireless network\ntopology changes, which introduce a coupled and novel challenge of establishing\nthe convergence for wireless decentralized FL algorithm. To facilitate\nconsensus phase, we propose an AirComp-based DSGD with gradient tracking and\nvariance reduction (DSGT-VR) algorithm, where both precoding and decoding\nstrategies are developed for D2D communication. Furthermore, we prove that the\nproposed algorithm converges linearly and establish the optimality gap for\nstrongly convex and smooth loss functions, taking into account the channel\nfading and noise. The theoretical result shows that the additional error bound\nin the optimality gap depends on the number of devices. Extensive simulations\nverify the theoretical results and show that the proposed algorithm outperforms\nother benchmark decentralized FL algorithms over wireless networks.",
    "descriptor": "\nComments: Accepted by ISIT 2021\n",
    "authors": [
      "Yandong Shi",
      "Yong Zhou",
      "Yuanming Shi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.08011"
  },
  {
    "id": "arXiv:2106.08013",
    "title": "Securing Face Liveness Detection Using Unforgeable Lip Motion Patterns",
    "abstract": "Face authentication usually utilizes deep learning models to verify users\nwith high recognition accuracy. However, face authentication systems are\nvulnerable to various attacks that cheat the models by manipulating the digital\ncounterparts of human faces. So far, lots of liveness detection schemes have\nbeen developed to prevent such attacks. Unfortunately, the attacker can still\nbypass these schemes by constructing wide-ranging sophisticated attacks. We\nstudy the security of existing face authentication services (e.g., Microsoft,\nAmazon, and Face++) and typical liveness detection approaches. Particularly, we\ndevelop a new type of attack, i.e., the low-cost 3D projection attack that\nprojects manipulated face videos on a 3D face model, which can easily evade\nthese face authentication services and liveness detection approaches. To this\nend, we propose FaceLip, a novel liveness detection scheme for face\nauthentication, which utilizes unforgeable lip motion patterns built upon\nwell-designed acoustic signals to enable a strong security guarantee. The\nunique lip motion patterns for each user are unforgeable because FaceLip\nverifies the patterns by capturing and analyzing the acoustic signals that are\ndynamically generated according to random challenges, which ensures that our\nsignals for liveness detection cannot be manipulated. Specially, we develop\nrobust algorithms for FaceLip to eliminate the impact of noisy signals in the\nenvironment and thus can accurately infer the lip motions at larger distances.\nWe prototype FaceLip on off-the-shelf smartphones and conduct extensive\nexperiments under different settings. Our evaluation with 44 participants\nvalidates the effectiveness and robustness of FaceLip.",
    "descriptor": "",
    "authors": [
      "Man Zhou",
      "Qian Wang",
      "Qi Li",
      "Peipei Jiang",
      "Jingxiao Yang",
      "Chao Shen",
      "Cong Wang",
      "Shouhong Ding"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.08013"
  },
  {
    "id": "arXiv:2106.08015",
    "title": "NeuroBEM: Hybrid Aerodynamic Quadrotor Model",
    "abstract": "Quadrotors are extremely agile, so much in fact, that classic\nfirst-principle-models come to their limits. Aerodynamic effects, while\ninsignificant at low speeds, become the dominant model defect during high\nspeeds or agile maneuvers. Accurate modeling is needed to design robust\nhigh-performance control systems and enable flying close to the platform's\nphysical limits. We propose a hybrid approach fusing first principles and\nlearning to model quadrotors and their aerodynamic effects with unprecedented\naccuracy. First principles fail to capture such aerodynamic effects, rendering\ntraditional approaches inaccurate when used for simulation or controller\ntuning. Data-driven approaches try to capture aerodynamic effects with blackbox\nmodeling, such as neural networks; however, they struggle to robustly\ngeneralize to arbitrary flight conditions. Our hybrid approach unifies and\noutperforms both first-principles blade-element theory and learned residual\ndynamics. It is evaluated in one of the world's largest motion-capture systems,\nusing autonomous-quadrotor-flight data at speeds up to 65km/h. The resulting\nmodel captures the aerodynamic thrust, torques, and parasitic effects with\nastonishing accuracy, outperforming existing models with 50% reduced prediction\nerrors, and shows strong generalization capabilities beyond the training set.",
    "descriptor": "\nComments: 9 pages + 1 pages references\n",
    "authors": [
      "Leonard Bauersfeld",
      "Elia Kaufmann",
      "Philipp Foehn",
      "Sihao Sun",
      "Davide Scaramuzza"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08015"
  },
  {
    "id": "arXiv:2106.08017",
    "title": "Color2Style: Real-Time Exemplar-Based Image Colorization with  Self-Reference Learning and Deep Feature Modulation",
    "abstract": "Legacy black-and-white photos are riddled with people's nostalgia and\nglorious memories of the past. To better relive the elapsed frozen moments, in\nthis paper, we present a deep exemplar-based image colorization approach named\nColor2Style to resurrect these grayscale image media by filling them with\nvibrant colors. Generally, for exemplar-based colorization, unsupervised and\nunpaired training are usually adopted, due to the difficulty of obtaining input\nand ground truth image pairs. To train an exemplar-based colorization model,\ncurrent algorithms usually strive to achieve two procedures: i) retrieving a\nlarge number of reference images with high similarity in advance, which is\ninevitably time-consuming and tedious; ii) designing complicated modules to\ntransfer the colors of the reference image to the grayscale image, by\ncalculating and leveraging the deep semantic correspondence between them (e.g.,\nnon-local operation). Contrary to the previous methods, we solve and simplify\nthe above two steps in one end-to-end learning procedure. First, we adopt a\nself-augmented self-reference training scheme, where the reference image is\ngenerated by graphical transformations from the original colorful one whereby\nthe training can be formulated in a paired manner. Second, instead of computing\ncomplex and inexplicable correspondence maps, our method exploits a simple yet\neffective deep feature modulation (DFM) module, which injects the color\nembeddings extracted from the reference image into the deep representations of\nthe input grayscale image. Such design is much more lightweight and\nintelligible, achieving appealing performance with real-time processing speed.\nMoreover, our model does not require multifarious loss functions and\nregularization terms like existing methods, but only two widely used loss\nfunctions. Codes and models will be available at\nhttps://github.com/zhaohengyuan1/Color2Style.",
    "descriptor": "\nComments: 16 pages, 21 figures\n",
    "authors": [
      "Hengyuan Zhao",
      "Wenhao Wu",
      "Yihao Liu",
      "Dongliang He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2106.08017"
  },
  {
    "id": "arXiv:2106.08019",
    "title": "Towards Axiomatic Explanations for Neural Ranking Models",
    "abstract": "Recently, neural networks have been successfully employed to improve upon\nstate-of-the-art performance in ad-hoc retrieval tasks via machine-learned\nranking functions. While neural retrieval models grow in complexity and impact,\nlittle is understood about their correspondence with well-studied IR\nprinciples. Recent work on interpretability in machine learning has provided\ntools and techniques to understand neural models in general, yet there has been\nlittle progress towards explaining ranking models.\nWe investigate whether one can explain the behavior of neural ranking models\nin terms of their congruence with well understood principles of document\nranking by using established theories from axiomatic IR. Axiomatic analysis of\ninformation retrieval models has formalized a set of constraints on ranking\ndecisions that reasonable retrieval models should fulfill. We operationalize\nthis axiomatic thinking to reproduce rankings based on combinations of\nelementary constraints. This allows us to investigate to what extent the\nranking decisions of neural rankers can be explained in terms of retrieval\naxioms, and which axioms apply in which situations. Our experimental study\nconsiders a comprehensive set of axioms over several representative neural\nrankers. While the existing axioms can already explain the particularly\nconfident ranking decisions rather well, future work should extend the axiom\nset to also cover the other still \"unexplainable\" neural IR rank decisions.",
    "descriptor": "\nComments: 11 pages, 2 figures. To be published in the proceedings of ICTIR 2021\n",
    "authors": [
      "Michael V\u00f6lske",
      "Alexander Bondarenko",
      "Maik Fr\u00f6be",
      "Matthias Hagen",
      "Benno Stein",
      "Jaspreet Singh",
      "Avishek Anand"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.08019"
  },
  {
    "id": "arXiv:2106.08021",
    "title": "A Clinically Inspired Approach for Melanoma classification",
    "abstract": "Melanoma is a leading cause of deaths due to skin cancer deaths and hence,\nearly and effective diagnosis of melanoma is of interest. Current approaches\nfor automated diagnosis of melanoma either use pattern recognition or\nanalytical recognition like ABCDE (asymmetry, border, color, diameter and\nevolving) criterion. In practice however, a differential approach wherein\noutliers (ugly duckling) are detected and used to evaluate nevi/lesions.\nIncorporation of differential recognition in Computer Aided Diagnosis (CAD)\nsystems has not been explored but can be beneficial as it can provide a\nclinical justification for the derived decision. We present a method for\nidentifying and quantifying ugly ducklings by performing Intra-Patient\nComparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a\nCAD system design for melanoma detection. This design ensures flexibility to\nhandle cases where IPCA is not possible. Our experiments on a public dataset\nshow that the outlier information helps boost the sensitivity of detection by\nat least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a\nstrong (EfficientNet) or moderately strong (VGG or ResNet) classifier.",
    "descriptor": "\nComments: 5 pages, 3 figures, 1 table\n",
    "authors": [
      "Prathyusha Akundi",
      "Soumyasis Gun",
      "Jayanthi Sivaswamy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08021"
  },
  {
    "id": "arXiv:2106.08022",
    "title": "Zero-shot Node Classification with Decomposed Graph Prototype Network",
    "abstract": "Node classification is a central task in graph data analysis. Scarce or even\nno labeled data of emerging classes is a big challenge for existing methods. A\nnatural question arises: can we classify the nodes from those classes that have\nnever been seen? In this paper, we study this zero-shot node classification\n(ZNC) problem which has a two-stage nature: (1) acquiring high-quality class\nsemantic descriptions (CSDs) for knowledge transfer, and (2) designing a well\ngeneralized graph-based learning model. For the first stage, we give a novel\nquantitative CSDs evaluation strategy based on estimating the real class\nrelationships, so as to get the \"best\" CSDs in a completely automatic way. For\nthe second stage, we propose a novel Decomposed Graph Prototype Network (DGPN)\nmethod, following the principles of locality and compositionality for zero-shot\nmodel generalization. Finally, we conduct extensive experiments to demonstrate\nthe effectiveness of our solutions.",
    "descriptor": "\nComments: Accepted by KDD 2021\n",
    "authors": [
      "Zheng Wang",
      "Jialong Wang",
      "Yuchen Guo",
      "Zhiguo Gong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.08022"
  },
  {
    "id": "arXiv:2106.08024",
    "title": "Snail Mail Beats Email Any Day: On Effective Operator Security  Notifications in the Internet",
    "abstract": "In the era of large-scale internet scanning, misconfigured websites are a\nfrequent cause of data leaks and security incidents. Previous research has\ninvestigated sending automated email notifications to operators of insecure or\ncompromised websites, but has often met with limited success due to challenges\nin address data quality, spam filtering, and operator distrust and disinterest.\nWhile several studies have investigated the design and phrasing of notification\nemails in a bid to increase their effectiveness, the use of other contact\nchannels has remained almost completely unexplored due to the required effort\nand cost. In this paper, we investigate two methods to increase notification\nsuccess: the use of letters as an alternative delivery medium, and the\ndescription of attack scenarios to incentivize remediation. We evaluate these\nfactors as part of a notification campaign utilizing manually-collected address\ninformation from 1359 German website operators and focusing on unintentional\ninformation leaks from web servers. We find that manually collected addresses\nlead to large increases in delivery rates compared to previous work, and\nletters were markedly more effective than emails, increasing remediation rates\nby up to 25 percentage points. Counterintuitively, providing detailed\ndescriptions of possible attacks can actually *decrease* remediation rates,\nhighlighting the need for more research into how notifications are perceived by\nrecipients.",
    "descriptor": "\nComments: Accepted at The 16th International Conference on Availability, Reliability and Security (ARES '21). Code and data: this https URL\n",
    "authors": [
      "Max Maass",
      "Marc-Pascal Clement",
      "Matthias Hollick"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.08024"
  },
  {
    "id": "arXiv:2106.08026",
    "title": "Modeling memory bandwidth patterns on NUMA machines with performance  counters",
    "abstract": "Computers used for data analytics are often NUMA systems with multiple\nsockets per machine, multiple cores per socket, and multiple thread contexts\nper core. To get the peak performance out of these machines requires the\ncorrect number of threads to be placed in the correct positions on the machine.\nOne particularly interesting element of the placement of memory and threads is\nthe way it effects the movement of data around the machine, and the increased\nlatency this can introduce to reads and writes. In this paper we describe work\non modeling the bandwidth requirements of an application on a NUMA compute node\nbased on the placement of threads. The model is parameterized by sampling\nperformance counters during 2 application runs with carefully chosen thread\nplacements. Evaluating the model with thousands of measurements shows a median\ndifference from predictions of 2.34% of the bandwidth. The results of this\nmodeling can be used in a number of ways varying from: Performance debugging\nduring development where the programmer can be alerted to potentially\nproblematic memory access patterns; To systems such as Pandia which take an\napplication and predict the performance and system load of a proposed thread\ncount and placement; To libraries of data structures such as Parallel\nCollections and Smart Arrays that can abstract from the user memory placement\nand thread placement issues when parallelizing code.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Daniel Goodman",
      "Roni Haecki",
      "Tim Harris"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2106.08026"
  },
  {
    "id": "arXiv:2106.08027",
    "title": "Multivariate Business Process Representation Learning utilizing Gramian  Angular Fields and Convolutional Neural Networks",
    "abstract": "Learning meaningful representations of data is an important aspect of machine\nlearning and has recently been successfully applied to many domains like\nlanguage understanding or computer vision. Instead of training a model for one\nspecific task, representation learning is about training a model to capture all\nuseful information in the underlying data and make it accessible for a\npredictor. For predictive process analytics, it is essential to have all\nexplanatory characteristics of a process instance available when making\npredictions about the future, as well as for clustering and anomaly detection.\nDue to the large variety of perspectives and types within business process\ndata, generating a good representation is a challenging task. In this paper, we\npropose a novel approach for representation learning of business process\ninstances which can process and combine most perspectives in an event log. In\nconjunction with a self-supervised pre-training method, we show the\ncapabilities of the approach through a visualization of the representation\nspace and case retrieval. Furthermore, the pre-trained model is fine-tuned to\nmultiple process prediction tasks and demonstrates its effectiveness in\ncomparison with existing approaches.",
    "descriptor": "\nComments: Accepted at the Business Process Management Conference 2021\n",
    "authors": [
      "Peter Pfeiffer",
      "Johannes Lahann",
      "Peter Fettke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08027"
  },
  {
    "id": "arXiv:2106.08029",
    "title": "Best Practices for Notification Studies for Security and Privacy Issues  on the Internet",
    "abstract": "Researchers help operators of vulnerable and non-compliant internet services\nby individually notifying them about security and privacy issues uncovered in\ntheir research. To improve efficiency and effectiveness of such efforts,\ndedicated notification studies are imperative. As of today, there is no\ncomprehensive documentation of pitfalls and best practices for conducting such\nnotification studies, which limits validity of results and impedes\nreproducibility. Drawing on our experience with such studies and guidance from\nrelated work, we present a set of guidelines and practical recommendations,\nincluding initial data collection, sending of notifications, interacting with\nthe recipients, and publishing the results. We note that future studies can\nespecially benefit from extensive planning and automation of crucial processes,\ni.e., activities that take place well before the first notifications are sent.",
    "descriptor": "\nComments: Accepted to the 3rd International Workshop on Information Security Methodology and Replication Studies (IWSMR '21), colocated with ARES '21\n",
    "authors": [
      "Max Maass",
      "Henning Prid\u00f6hl",
      "Dominik Herrmann",
      "Matthias Hollick"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.08029"
  },
  {
    "id": "arXiv:2106.08033",
    "title": "Selecting a Match: Exploration vs Decision",
    "abstract": "In a dynamic matching market, such as a marriage or job market, how should\nagents balance accepting a proposed match with the cost of continuing their\nsearch? We consider this problem in a discrete setting, in which agents have\ncardinal values and finite lifetimes, and proposed matches are random.\nWe seek to quantify how well the agents can do. We provide upper and lower\nbounds on the collective losses of the agents, with a polynomially small\nfailure probability, where the notion of loss is with respect to a plausible\nbaseline we define. These bounds are tight up to constant factors.\nWe highlight two aspects of this work. First, in our model, agents have a\nfinite time in which to enjoy their matches, namely the minimum of their\nremaining lifetime and that of their partner; this implies that unmatched\nagents become less desirable over time, and suggests that their decision rules\nshould change over time. Second, we use a discrete rather than a continuum\nmodel for the population. The discreteness causes variance which induces\nlocalized imbalances in the two sides of the market. One of the main technical\nchallenges we face is to bound these imbalances.\nIn addition, we present the results of simulations on moderate-sized problems\nfor both the discrete and continuum versions. For these size problems, there\nare substantial ongoing fluctuations in the discrete setting whereas the\ncontinuum version converges reasonably quickly.",
    "descriptor": "",
    "authors": [
      "Ishan Agarwal",
      "Richard Cole",
      "Yixin Tao"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.08033"
  },
  {
    "id": "arXiv:2106.08034",
    "title": "Real-Time Denoising of Volumetric Path Tracing for Direct Volume  Rendering",
    "abstract": "Direct Volume Rendering (DVR) using Volumetric Path Tracing (VPT) is a\nscientific visualization technique that simulates light transport with objects'\nmatter using physically-based lighting models. Monte Carlo (MC) path tracing is\noften used with surface models, yet its application for volumetric models is\ndifficult due to the complexity of integrating MC light-paths in volumetric\nmedia with none or smooth material boundaries. Moreover, auxiliary\ngeometry-buffers (G-buffers) produced for volumes are typically very noisy,\nfailing to guide image denoisers relying on that information to preserve image\ndetails. This makes existing real-time denoisers, which take noise-free\nG-buffers as their input, less effective when denoising VPT images. We propose\nthe necessary modifications to an image-based denoiser previously used when\nrendering surface models, and demonstrate effective denoising of VPT images. In\nparticular, our denoising exploits temporal coherence between frames, without\nrelying on noise-free G-buffers, which has been a common assumption of existing\ndenoisers for surface-models. Our technique preserves high-frequency details\nthrough a weighted recursive least squares that handles heterogeneous noise for\nvolumetric models. We show for various real data sets that our method improves\nthe visual fidelity and temporal stability of VPT during classic DVR operations\nsuch as camera movements, modifications of the light sources, and editions to\nthe volume transfer function.",
    "descriptor": "\nComments: 13 pages, 19 figures, project page available at this http URL IEEE Transactions on Visualization and Computer Graphics (2020)\n",
    "authors": [
      "Jose A. Iglesias-Guitian",
      "Prajita Mane",
      "Bochang Moon"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.08034"
  },
  {
    "id": "arXiv:2106.08037",
    "title": "The Possible, the Plausible, and the Desirable: Event-Based Modality  Detection for Language Processing",
    "abstract": "Modality is the linguistic ability to describe events with added information\nsuch as how desirable, plausible, or feasible they are. Modality is important\nfor many NLP downstream tasks such as the detection of hedging, uncertainty,\nspeculation, and more. Previous studies that address modality detection in NLP\noften restrict modal expressions to a closed syntactic class, and the modal\nsense labels are vastly different across different studies, lacking an accepted\nstandard. Furthermore, these senses are often analyzed independently of the\nevents that they modify. This work builds on the theoretical foundations of the\nGeorgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to\npropose an event-based modality detection task where modal expressions can be\nwords of any syntactic class and sense labels are drawn from a comprehensive\ntaxonomy which harmonizes the modal concepts contributed by the different\nstudies. We present experiments on the GME corpus aiming to detect and classify\nfine-grained modal concepts and associate them with their modified events. We\nshow that detecting and classifying modal expressions is not only feasible, but\nalso improves the detection of modal events in their own right.",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Valentina Pyatkin",
      "Shoval Sadde",
      "Aynat Rubinstein",
      "Paul Portner",
      "Reut Tsarfaty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08037"
  },
  {
    "id": "arXiv:2106.08038",
    "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of  Representations",
    "abstract": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
    "descriptor": "",
    "authors": [
      "Arsenii Ashukha",
      "Andrei Atanov",
      "Dmitry Vetrov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08038"
  },
  {
    "id": "arXiv:2106.08041",
    "title": "The BDF3/EP3 scheme for MBE with no slope selection is stable",
    "abstract": "We consider the classical molecular beam epitaxy (MBE) model with logarithmic\ntype potential known as no-slope-selection. We employ a third order backward\ndifferentiation (BDF3) in time with implicit treatment of the surface diffusion\nterm. The nonlinear term is approximated by a third order explicit\nextrapolation (EP3) formula. We exhibit mild time step constraints under which\nthe modified energy dissipation law holds. We break the second Dahlquist\nbarrier and develop a new theoretical framework to prove unconditional uniform\nenergy boundedness with no size restrictions on the time step. This is the\nfirst unconditional result for third order BDF methods applied to the MBE\nmodels without introducing any stabilization terms or fictitious variables. A\nnovel theoretical framework is also established for the error analysis of high\norder methods.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Dong Li",
      "Chaoyu Quan",
      "Wen Yang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.08041"
  },
  {
    "id": "arXiv:2106.08042",
    "title": "Hotel Recognition via Latent Image Embedding",
    "abstract": "We approach the problem of hotel recognition with deep metric learning. We\noverview the existing approaches and propose a modification to Contrastive loss\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\nbenchmarking metric learning models and perform experiments on Hotels-50K and\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\non Hotels-50k. We open-source our code.",
    "descriptor": "\nComments: IWANN 2021\n",
    "authors": [
      "Boris Tseytlin",
      "Ilya Makarov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08042"
  },
  {
    "id": "arXiv:2106.08043",
    "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text",
    "abstract": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Arun S. Maiya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08043"
  },
  {
    "id": "arXiv:2106.08045",
    "title": "Object detection and Autoencoder-based 6D pose estimation for highly  cluttered Bin Picking",
    "abstract": "Bin picking is a core problem in industrial environments and robotics, with\nits main module as 6D pose estimation. However, industrial depth sensors have a\nlack of accuracy when it comes to small objects. Therefore, we propose a\nframework for pose estimation in highly cluttered scenes with small objects,\nwhich mainly relies on RGB data and makes use of depth information only for\npose refinement. In this work, we compare synthetic data generation approaches\nfor object detection and pose estimation and introduce a pose filtering\nalgorithm that determines the most accurate estimated poses. We will make our",
    "descriptor": "\nComments: 5 pages, 1 page references. Accepted to ICIP 2021\n",
    "authors": [
      "Timon H\u00f6fer",
      "Faranak Shamsafar",
      "Nuri Benbarka",
      "Andreas Zell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08045"
  },
  {
    "id": "arXiv:2106.08049",
    "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance  Effect",
    "abstract": "Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.",
    "descriptor": "\nComments: 5 pages, 6 figures\n",
    "authors": [
      "Pawel Drozdowski",
      "Christian Rathgeb",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.08049"
  },
  {
    "id": "arXiv:2106.08050",
    "title": "Residual Reinforcement Learning from Demonstrations",
    "abstract": "Residual reinforcement learning (RL) has been proposed as a way to solve\nchallenging robotic tasks by adapting control actions from a conventional\nfeedback controller to maximize a reward signal. We extend the residual\nformulation to learn from visual inputs and sparse rewards using\ndemonstrations. Learning from images, proprioceptive inputs and a sparse\ntask-completion reward relaxes the requirement of accessing full state\nfeatures, such as object and target positions. In addition, replacing the base\ncontroller with a policy learned from demonstrations removes the dependency on\na hand-engineered controller in favour of a dataset of demonstrations, which\ncan be provided by non-experts. Our experimental evaluation on simulated\nmanipulation tasks on a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates\nthat residual RL from demonstrations is able to generalize to unseen\nenvironment conditions more flexibly than either behavioral cloning or RL\nfine-tuning, and is capable of solving high-dimensional, sparse-reward tasks\nout of reach for RL from scratch.",
    "descriptor": "",
    "authors": [
      "Minttu Alakuijala",
      "Gabriel Dulac-Arnold",
      "Julien Mairal",
      "Jean Ponce",
      "Cordelia Schmid"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08050"
  },
  {
    "id": "arXiv:2106.08053",
    "title": "On the Power of Multitask Representation Learning in Linear MDP",
    "abstract": "While multitask representation learning has become a popular approach in\nreinforcement learning (RL), theoretical understanding of why and when it works\nremains limited. This paper presents analyses for the statistical benefit of\nmultitask representation learning in linear Markov Decision Process (MDP) under\na generative model. In this paper, we consider an agent to learn a\nrepresentation function $\\phi$ out of a function class $\\Phi$ from $T$ source\ntasks with $N$ data per task, and then use the learned $\\hat{\\phi}$ to reduce\nthe required number of sample for a new task. We first discover a\n\\emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\\kappa$,\nwith which we prove that a straightforward least-square algorithm learns a\npolicy which is $\\tilde{O}(H^2\\sqrt{\\frac{\\mathcal{C}(\\Phi)^2 \\kappa\nd}{NT}+\\frac{\\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon,\n$\\mathcal{C}(\\Phi)$ is $\\Phi$'s complexity measure, $d$ is the dimension of the\nrepresentation (usually $d\\ll \\mathcal{C}(\\Phi)$) and $n$ is the number of\nsamples for the new task. Thus the required $n$ is $O(\\kappa d H^4)$ for the\nsub-optimality to be close to zero, which is much smaller than\n$O(\\mathcal{C}(\\Phi)^2\\kappa d H^4)$ in the setting without multitask\nrepresentation learning, whose sub-optimality gap is\n$\\tilde{O}(H^2\\sqrt{\\frac{\\kappa \\mathcal{C}(\\Phi)^2d}{n}})$. This\ntheoretically explains the power of multitask representation learning in\nreducing sample complexity. Further, we note that to ensure high sample\nefficiency, the LAFA criterion $\\kappa$ should be small. In fact, $\\kappa$\nvaries widely in magnitude depending on the different sampling distribution for\nnew task. This indicates adaptive sampling technique is important to make\n$\\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy\ngrid-world environment to corroborate our theoretical findings.",
    "descriptor": "",
    "authors": [
      "Rui Lu",
      "Gao Huang",
      "Simon S. Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.08053"
  },
  {
    "id": "arXiv:2106.08056",
    "title": "Coupled Gradient Estimators for Discrete Latent Variables",
    "abstract": "Training models with discrete latent variables is challenging due to the high\nvariance of unbiased gradient estimators. While low-variance reparameterization\ngradients of a continuous relaxation can provide an effective solution, a\ncontinuous relaxation is not always available or tractable. Dong et al. (2020)\nand Yin et al. (2020) introduced a performant estimator that does not rely on\ncontinuous relaxations; however, it is limited to binary random variables. We\nintroduce a novel derivation of their estimator based on importance sampling\nand statistical couplings, which we extend to the categorical setting.\nMotivated by the construction of a stick-breaking coupling, we introduce\ngradient estimators based on reparameterizing categorical variables as\nsequences of binary variables and Rao-Blackwellization. In systematic\nexperiments, we show that our proposed categorical gradient estimators provide\nstate-of-the-art performance, whereas even with additional\nRao-Blackwellization, previous estimators (Yin et al., 2019) underperform a\nsimpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019).",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Zhe Dong",
      "Andriy Mnih",
      "George Tucker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08056"
  },
  {
    "id": "arXiv:2106.08059",
    "title": "Real-time Pose and Shape Reconstruction of Two Interacting Hands With a  Single Depth Camera",
    "abstract": "We present a novel method for real-time pose and shape reconstruction of two\nstrongly interacting hands. Our approach is the first two-hand tracking\nsolution that combines an extensive list of favorable properties, namely it is\nmarker-less, uses a single consumer-level depth camera, runs in real time,\nhandles inter- and intra-hand collisions, and automatically adjusts to the\nuser's hand shape. In order to achieve this, we embed a recent parametric hand\npose and shape model and a dense correspondence predictor based on a deep\nneural network into a suitable energy minimization framework. For training the\ncorrespondence prediction network, we synthesize a two-hand dataset based on\nphysical simulations that includes both hand pose and shape annotations while\nat the same time avoiding inter-hand penetrations. To achieve real-time rates,\nwe phrase the model fitting in terms of a nonlinear least-squares problem so\nthat the energy can be optimized based on a highly efficient GPU-based\nGauss-Newton optimizer. We show state-of-the-art results in scenes that exceed\nthe complexity level demonstrated by previous work, including tight two-hand\ngrasps, significant inter-hand occlusions, and gesture interaction.",
    "descriptor": "\nComments: ACM Transactions on Graphics (Proceedings SIGGRAPH 2019)\n",
    "authors": [
      "Franziska Mueller",
      "Micah Davis",
      "Florian Bernard",
      "Oleksandr Sotnychenko",
      "Mickeal Verschoor",
      "Miguel A. Otaduy",
      "Dan Casas",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08059"
  },
  {
    "id": "arXiv:2106.08060",
    "title": "Privacy Assessment of Federated Learning using Private Personalized  Layers",
    "abstract": "Federated Learning (FL) is a collaborative scheme to train a learning model\nacross multiple participants without sharing data. While FL is a clear step\nforward towards enforcing users' privacy, different inference attacks have been\ndeveloped. In this paper, we quantify the utility and privacy trade-off of a FL\nscheme using private personalized layers. While this scheme has been proposed\nas local adaptation to improve the accuracy of the model through local\npersonalization, it has also the advantage to minimize the information about\nthe model exchanged with the server. However, the privacy of such a scheme has\nnever been quantified. Our evaluations using motion sensor dataset show that\npersonalized layers speed up the convergence of the model and slightly improve\nthe accuracy for all users compared to a standard FL scheme while better\npreventing both attribute and membership inferences compared to a FL scheme\nusing local differential privacy.",
    "descriptor": "",
    "authors": [
      "Th\u00e9o Jourdan",
      "Antoine Boutet",
      "Carole Frindel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08060"
  },
  {
    "id": "arXiv:2106.08061",
    "title": "Relation Modeling in Spatio-Temporal Action Localization",
    "abstract": "This paper presents our solution to the AVA-Kinetics Crossover Challenge of\nActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of\nrelation modeling methods for spatio-temporal action detection and adopts a\ntraining strategy to integrate multiple relation modeling in end-to-end\ntraining over the two large-scale video datasets. Learning with memory bank and\nfinetuning for long-tailed distribution are also investigated to further\nimprove the performance. In this paper, we detail the implementations of our\nsolution and provide experiments results and corresponding discussions. We\nfinally achieve 40.67 mAP on the test set of AVA-Kinetics.",
    "descriptor": "",
    "authors": [
      "Yutong Feng",
      "Jianwen Jiang",
      "Ziyuan Huang",
      "Zhiwu Qing",
      "Xiang Wang",
      "Shiwei Zhang",
      "Mingqian Tang",
      "Yue Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08061"
  },
  {
    "id": "arXiv:2106.08062",
    "title": "SSMix: Saliency-Based Span Mixup for Text Classification",
    "abstract": "Data augmentation with mixup has shown to be effective on various computer\nvision tasks. Despite its great success, there has been a hurdle to apply mixup\nto NLP tasks since text consists of discrete tokens with variable length. In\nthis work, we propose SSMix, a novel mixup method where the operation is\nperformed on input text rather than on hidden vectors like previous approaches.\nSSMix synthesizes a sentence while preserving the locality of two original\ntexts by span-based mixing and keeping more tokens related to the prediction\nrelying on saliency information. With extensive experiments, we empirically\nvalidate that our method outperforms hidden-level mixup methods on a wide range\nof text classification benchmarks, including textual entailment, sentiment\nclassification, and question-type classification. Our code is available at\nhttps://github.com/clovaai/ssmix.",
    "descriptor": "\nComments: Findings of ACL 2021\n",
    "authors": [
      "Soyoung Yoon",
      "Gyuwan Kim",
      "Kyumin Park"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08062"
  },
  {
    "id": "arXiv:2106.08064",
    "title": "Generating Contrastive Explanations for Inductive Logic Programming  Based on a Near Miss Approach",
    "abstract": "In recent research, human-understandable explanations of machine learning\nmodels have received a lot of attention. Often explanations are given in form\nof model simplifications or visualizations. However, as shown in cognitive\nscience as well as in early AI research, concept understanding can also be\nimproved by the alignment of a given instance for a concept with a similar\ncounterexample. Contrasting a given instance with a structurally similar\nexample which does not belong to the concept highlights what characteristics\nare necessary for concept membership. Such near misses have been proposed by\nWinston (1970) as efficient guidance for learning in relational domains. We\nintroduce an explanation generation algorithm for relational concepts learned\nwith Inductive Logic Programming (\\textsc{GeNME}). The algorithm identifies\nnear miss examples from a given set of instances and ranks these examples by\ntheir degree of closeness to a specific positive instance. A modified rule\nwhich covers the near miss but not the original instance is given as an\nexplanation. We illustrate \\textsc{GeNME} with the well known family domain\nconsisting of kinship relations, the visual relational Winston arches domain\nand a real-world domain dealing with file management. We also present a\npsychological experiment comparing human preferences of rule-based,\nexample-based, and near miss explanations in the family and the arches domains.",
    "descriptor": "",
    "authors": [
      "Johannes Rabold",
      "Michael Siebers",
      "Ute Schmid"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.08064"
  },
  {
    "id": "arXiv:2106.08068",
    "title": "An Analytical Theory of Curriculum Learning in Teacher-Student Networks",
    "abstract": "In humans and animals, curriculum learning -- presenting data in a curated\norder - is critical to rapid learning and effective pedagogy. Yet in machine\nlearning, curricula are not widely used and empirically often yield only\nmoderate benefits. This stark difference in the importance of curriculum raises\na fundamental theoretical question: when and why does curriculum learning help?\nIn this work, we analyse a prototypical neural network model of curriculum\nlearning in the high-dimensional limit, employing statistical physics methods.\nCurricula could in principle change both the learning speed and asymptotic\nperformance of a model. To study the former, we provide an exact description of\nthe online learning setting, confirming the long-standing experimental\nobservation that curricula can modestly speed up learning. To study the latter,\nwe derive performance in a batch learning setting, in which a network trains to\nconvergence in successive phases of learning on dataset slices of varying\ndifficulty. With standard training losses, curriculum does not provide\ngeneralisation benefit, in line with empirical observations. However, we show\nthat by connecting different learning phases through simple Gaussian priors,\ncurriculum can yield a large improvement in test performance. Taken together,\nour reduced analytical descriptions help reconcile apparently conflicting\nempirical results and trace regimes where curriculum learning yields the\nlargest gains. More broadly, our results suggest that fully exploiting a\ncurriculum may require explicit changes to the loss function at curriculum\nboundaries.",
    "descriptor": "\nComments: 10 pages + appendix\n",
    "authors": [
      "Luca Saglietti",
      "Stefano Sarao Mannelli",
      "Andrew Saxe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08068"
  },
  {
    "id": "arXiv:2106.08072",
    "title": "Full Bitcoin Blockchain Data Made Easy",
    "abstract": "Despite the fact that it is publicly available, collecting and processing the\nfull bitcoin blockchain data is not trivial. Its mere size, history, and other\nfeatures indeed raise quite specific challenges, that we address in this paper.\nThe strengths of our approach are the following: it relies on very basic and\nstandard tools, which makes the procedure reliable and easily reproducible; it\nis a purely lossless procedure ensuring that we catch and preserve all existing\ndata; it provides additional indexing that makes it easy to further process the\nwhole data and select appropriate subsets of it. We present our procedure in\ndetails and illustrate its added value on large-scale use cases, like address\nclustering. We provide an implementation online, as well as the obtained\ndataset.",
    "descriptor": "",
    "authors": [
      "Jules Azad Emery",
      "Matthieu Latapy"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2106.08072"
  },
  {
    "id": "arXiv:2106.08073",
    "title": "Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with  Adaptive Hybrid Label",
    "abstract": "Unmanned aerial vehicle (UAV) based visual tracking has been confronted with\nnumerous challenges, e.g., object motion and occlusion. These challenges\ngenerally introduce unexpected mutations of target appearance and result in\ntracking failure. However, prevalent discriminative correlation filter (DCF)\nbased trackers are insensitive to target mutations due to a predefined label,\nwhich concentrates on merely the centre of the training region. Meanwhile,\nappearance mutations caused by occlusion or similar objects usually lead to the\ninevitable learning of wrong information. To cope with appearance mutations,\nthis paper proposes a novel DCF-based method to enhance the sensitivity and\nresistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal\nlabel is optimized jointly with the correlation filter and remains temporal\nconsistency. Besides, a novel measurement of mutations called mutation threat\nfactor (MTF) is applied to correct the label dynamically. Considerable\nexperiments are conducted on widely used UAV benchmarks. The results indicate\nthat the performance of MSCF tracker surpasses other 26 state-of-the-art\nDCF-based and deep-based trackers. With a real-time speed of _38 frames/s, the\nproposed approach is sufficient for UAV tracking commissions.",
    "descriptor": "\nComments: Accepted by ICRA 2021, Github: this https URL\n",
    "authors": [
      "Guangze Zheng",
      "Changhong Fu",
      "Junjie Ye",
      "Fuling Lin",
      "Fangqiang Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08073"
  },
  {
    "id": "arXiv:2106.08074",
    "title": "On Star Expressions and Coalgebraic Completeness Theorems",
    "abstract": "An open problem posed by Milner asks for a proof that a certain\naxiomatisation, which Milner showed is sound with respect to bisimilarity for\nregular expressions, is also complete. One of the main difficulties of the\nproblem is the lack of a full Kleene theorem, since there are automata that can\nnot be specified, up to bisimilarity, by an expression. Grabmayer and Fokkink\n(2020) characterise those automata that can be expressed by regular expressions\nwithout the constant 1, and use this characterisation to give a positive answer\nto Milner's question for this subset of expressions. In this paper, we analyse\nGrabmayer and Fokkink's proof of completeness from the perspective of universal\ncoalgebra, and thereby give an abstract account of their proof method. We then\ncompare this proof method to another approach to completeness proofs from\ncoalgebraic language theory. This culminates in two abstract proof methods for\ncompleteness, what we call the local and global approaches, and a description\nof when one method can be used in place of the other.",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Todd Schmid",
      "Jurriaan Rot",
      "Alexandra Silva"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.08074"
  },
  {
    "id": "arXiv:2106.08077",
    "title": "Computer-aided Interpretable Features for Leaf Image Classification",
    "abstract": "Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Jayani P. G. Lakshika",
      "Thiyanga S. Talagala"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.08077"
  },
  {
    "id": "arXiv:2106.08078",
    "title": "Time-free solution to independent set problem using P systems with  active membranes",
    "abstract": "Membrane computing is a branch of natural computingwhich abstracts fromthe\nstructure and the functioning of living cells. The computation models obtained\nin the field of membrane computing are usually called P systems. P systems have\nbeen used to solve computationally hard problems efficiently on the assumption\nthat the execution of each rule is completed in exactly one time-unit (a global\nclock is assumed for timing and synchronizing the execution of rules). However,\nin biological reality, different biological processes take different times to\nbe completed, which can also be influenced by many environmental factors. In\nthis work, with this biological reality, we give a time-free solution to\nindependent set problemusing P systems with active membranes, which solve the\nproblem independent of the execution time of the involved rules.",
    "descriptor": "",
    "authors": [
      "Yu Jin",
      "Bosheng Song",
      "Yanyan Li"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.08078"
  },
  {
    "id": "arXiv:2106.08085",
    "title": "Natural continual learning: success is a journey, not (just) a  destination",
    "abstract": "Biological agents are known to learn many different tasks over the course of\ntheir lives, and to be able to revisit previous tasks and behaviors with little\nto no loss in performance. In contrast, artificial agents are prone to\n'catastrophic forgetting' whereby performance on previous tasks deteriorates\nrapidly as new ones are acquired. This shortcoming has recently been addressed\nusing methods that encourage parameters to stay close to those used for\nprevious tasks. This can be done by (i) using specific parameter regularizers\nthat map out suitable destinations in parameter space, or (ii) guiding the\noptimization journey by projecting gradients into subspaces that do not\ninterfere with previous tasks. However, parameter regularization has been shown\nto be relatively ineffective in recurrent neural networks (RNNs), a setting\nrelevant to the study of neural dynamics supporting biological continual\nlearning. Similarly, projection based methods can reach capacity and fail to\nlearn any further as the number of tasks increases. To address these\nlimitations, we propose Natural Continual Learning (NCL), a new method that\nunifies weight regularization and projected gradient descent. NCL uses Bayesian\nweight regularization to encourage good performance on all tasks at convergence\nand combines this with gradient projections designed to prevent catastrophic\nforgetting during optimization. NCL formalizes gradient projection as a trust\nregion algorithm based on the Fisher information metric, and achieves\nscalability via a novel Kronecker-factored approximation strategy. Our method\noutperforms both standard weight regularization techniques and projection based\napproaches when applied to continual learning problems in RNNs. The trained\nnetworks evolve task-specific dynamics that are strongly preserved as new tasks\nare learned, similar to experimental findings in biological circuits.",
    "descriptor": "",
    "authors": [
      "Ta-Chu Kao",
      "Kristopher T. Jensen",
      "Alberto Bernacchia",
      "Guillaume Hennequin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.08085"
  },
  {
    "id": "arXiv:2106.08087",
    "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark",
    "abstract": "Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.",
    "descriptor": "",
    "authors": [
      "Ningyu Zhang",
      "Zhen Bi",
      "Xiaozhuan Liang",
      "Lei Li",
      "Xiang Chen",
      "Shumin Deng",
      "Luoqiu Li",
      "Xin Xie",
      "Hongbin Ye",
      "Xin Shang",
      "Kangping Yin",
      "Chuanqi Tan",
      "Jian Xu",
      "Mosha Chen",
      "Fei Huang",
      "Luo Si",
      "Yuan Ni",
      "Guotong Xie",
      "Zhifang Sui",
      "Baobao Chang",
      "Hui Zong",
      "Zheng Yuan",
      "Linfeng Li",
      "Jun Yan",
      "Hongying Zan",
      "Kunli Zhang",
      "Huajun Chen",
      "Buzhou Tang",
      "Qingcai Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08087"
  },
  {
    "id": "arXiv:2106.08088",
    "title": "Heterogeneous Multi-sensor Fusion with Random Finite Set Multi-object  Densities",
    "abstract": "This paper addresses the density based multi-sensor cooperative fusion using\nrandom finite set (RFS) type multi-object densities (MODs). Existing fusion\nmethods use scalar weights to characterize the relative information confidence\namong the local MODs, and in this way the portion of contribution of each local\nMOD to the fused global MOD can be tuned via adjusting these weights. Our\nanalysis shows that the fusion mechanism of using a scalar coefficient can be\noversimplified for practical scenarios, as the information confidence of an MOD\nis complex and usually space-varying due to the imperfection of sensor ability\nand the various impacts from surveillance environment. Consequently, severe\nfusion performance degradation can be observed when these scalar weights fail\nto reflect the actual situation. We make two contributions towards addressing\nthis problem. Firstly, we propose a novel heterogeneous fusion method to\nperform the information averaging among local RFS MODs. By factorizing each\nlocal MODs into a number of smaller size sub-MODs, it can transform the\noriginal complicated fusion problem into a much easier parallelizable\nmulti-cluster fusion problem. Secondly, as the proposed fusion strategy is a\ngeneral procedure without any particular model assumptions, we further derive\nthe detailed heterogeneous fusion equations, with centralized network\narchitecture, for both the probability hypothesis density (PHD) filter and the\nmulti-Bernoulli (MB) filter. The Gaussian mixture implementations of the\nproposed fusion algorithms are also presented. Various numerical experiments\nare designed to demonstrate the efficacy of the proposed fusion methods.",
    "descriptor": "",
    "authors": [
      "Wei Yi",
      "Lei Chai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.08088"
  },
  {
    "id": "arXiv:2106.08090",
    "title": "Tensors in computations",
    "abstract": "The notion of a tensor captures three great ideas: equivariance,\nmultilinearity, separability. But trying to be three things at once makes the\nnotion difficult to understand. We will explain tensors in an accessible and\nelementary way through the lens of linear algebra and numerical linear algebra,\nelucidated with examples from computational and applied mathematics.",
    "descriptor": "\nComments: 208 pages, 5 figures\n",
    "authors": [
      "Lek-Heng Lim"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.08090"
  },
  {
    "id": "arXiv:2106.08091",
    "title": "Generating Thermal Human Faces for Physiological Assessment Using  Thermal Sensor Auxiliary Labels",
    "abstract": "Thermal images reveal medically important physiological information about\nhuman stress, signs of inflammation, and emotional mood that cannot be seen on\nvisible images. Providing a method to generate thermal faces from visible\nimages would be highly valuable for the telemedicine community in order to show\nthis medical information. To the best of our knowledge, there are limited works\non visible-to-thermal (VT) face translation, and many current works go the\nopposite direction to generate visible faces from thermal surveillance images\n(TV) for law enforcement applications. As a result, we introduce favtGAN, a VT\nGAN which uses the pix2pix image translation model with an auxiliary sensor\nlabel prediction network for generating thermal faces from visible images.\nSince most TV methods are trained on only one data source drawn from one\nthermal sensor, we combine datasets from faces and cityscapes. These combined\ndata are captured from similar sensors in order to bootstrap the training and\ntransfer learning task, especially valuable because visible-thermal face\ndatasets are limited. Experiments on these combined datasets show that favtGAN\ndemonstrates an increase in SSIM and PSNR scores of generated thermal faces,\ncompared to training on a single face dataset alone.",
    "descriptor": "",
    "authors": [
      "Catherine Ordun",
      "Edward Raff",
      "Sanjay Purushotham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08091"
  },
  {
    "id": "arXiv:2106.08104",
    "title": "Detect and remove watermark in deep neural networks via generative  adversarial networks",
    "abstract": "Deep neural networks (DNN) have achieved remarkable performance in various\nfields. However, training a DNN model from scratch requires a lot of computing\nresources and training data. It is difficult for most individual users to\nobtain such computing resources and training data. Model copyright infringement\nis an emerging problem in recent years. For instance, pre-trained models may be\nstolen or abuse by illegal users without the authorization of the model owner.\nRecently, many works on protecting the intellectual property of DNN models have\nbeen proposed. In these works, embedding watermarks into DNN based on backdoor\nis one of the widely used methods. However, when the DNN model is stolen, the\nbackdoor-based watermark may face the risk of being detected and removed by an\nadversary. In this paper, we propose a scheme to detect and remove watermark in\ndeep neural networks via generative adversarial networks (GAN). We demonstrate\nthat the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based\nwatermark removal attack. The proposed attack method includes two phases. In\nthe first phase, we use the GAN and few clean images to detect and reverse the\nwatermark in the DNN model. In the second phase, we fine-tune the watermarked\nDNN based on the reversed backdoor images. Experimental evaluations on the\nMNIST and CIFAR10 datasets demonstrate that, the proposed method can\neffectively remove about 98% of the watermark in DNN models, as the watermark\nretention rate reduces from 100% to less than 2% after applying the proposed\nattack. In the meantime, the proposed attack hardly affects the model's\nperformance. The test accuracy of the watermarked DNN on the MNIST and the\nCIFAR10 datasets drops by less than 1% and 3%, respectively.",
    "descriptor": "",
    "authors": [
      "Haoqi Wang",
      "Mingfu Xue",
      "Shichang Sun",
      "Yushu Zhang",
      "Jian Wang",
      "Weiqiang Liu"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.08104"
  },
  {
    "id": "arXiv:2106.08112",
    "title": "Contextualizing Multiple Tasks via Learning to Decompose",
    "abstract": "One single instance could possess multiple portraits and reveal diverse\nrelationships with others according to different contexts. Those ambiguities\nincrease the difficulty of learning a generalizable model when there exists one\nconcept or mixed concepts in a task. We propose a general approach Learning to\nDecompose Network (LeadNet) for both two cases, which contextualizes a model\nthrough meta-learning multiple maps for concepts discovery -- the\nrepresentations of instances are decomposed and adapted conditioned on the\ncontexts. Through taking a holistic view over multiple latent components over\ninstances in a sampled pseudo task, LeadNet learns to automatically select the\nright concept via incorporating those rich semantics inside and between\nobjects. LeadNet demonstrates its superiority in various applications,\nincluding exploring multiple views of confusing tasks, out-of-distribution\nrecognition, and few-shot image classification.",
    "descriptor": "",
    "authors": [
      "Han-Jia Ye",
      "Da-Wei Zhou",
      "Lanqing Hong",
      "Zhenguo Li",
      "Xiu-Shen Wei",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08112"
  },
  {
    "id": "arXiv:2106.08114",
    "title": "Don't Count on One to Carry the Ball: Scaling BFT without Sacrifing  Efficiency",
    "abstract": "With the emergence of large-scale decentralized applications, a scalable and\nefficient Byzantine Fault Tolerant protocol of hundreds of replicas is ideal.\nAlthough the throughput of existing leader-based BFT protocols has reached a\nhigh level of 10^5 operations per second for a small scale of replicas, it\ndrops significantly when the number of replicas increases, which leads to a\nlack of practicality. This paper focuses on the scalability of BFT protocols\nand identifies a major bottleneck to leader-based BFT protocols due to the\nexcessive workload of the leader at large scales. A new metric of scaling\nfactor is defined to capture whether a BFT protocol will get stuck when it\nscales out, which can be used to measure the performance of efficiency and\nscalability of BFT protocols. We propose Leopard, the first leader-based BFT\nprotocol that scales to multiple hundreds of replicas, and more importantly,\npreserves high efficiency. It is secure with the optimal resilience bound\n(i.e., 1/3) in the partial synchronous network model. We remove the bottleneck\nby introducing a technique of achieving constant scaling factor, which takes\nfull advantage of the idle resource and adaptively balances the workload of the\nleader among all replicas. We implement Leopard and evaluate its performance\ncompared to HotStuff, the state-of-the-art BFT protocol. We run extensive\nexperiments on the two systems back-to-back in the same environments with up to\n600 replicas. The results show that Leopard achieves significant performance\nimprovements both on throughput and scalability. In particular, the throughput\nof Leopard remains at a high level of 10^5 when the system scales out to 600\nreplicas. It achieves a 5 times throughput over HotStuff when the scale is 300\n(which is already the largest scale we can see the progress of the latter in\nour experiments), and the gap becomes wider as the scale further increases.",
    "descriptor": "",
    "authors": [
      "Kexin Hu",
      "Kaiwen Guo",
      "Qiang Tang",
      "Zhenfeng Zhang",
      "Hao Cheng",
      "Zhiyang Zhao"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.08114"
  },
  {
    "id": "arXiv:2106.08115",
    "title": "Archify: A Recommender System of Architectural Design Decisions",
    "abstract": "Software architectures play a critical role in software quality assurance.\nHowever, small and medium companies (SMC) often suffer from the absence of\nprofessionals with skills and expertise in software architecture. That\nsituation potentially affects the final quality of the software products and\npressures projects budget with extra costs with consulting. This paper presents\na recommender system of architectural design decisions called Archify. The goal\nis to support SMC companies in part of the effort of architecturally designing\ntheir products. Archify implements a wizard-styled interface that guides the\ndeveloper or project manager through a set of specific questions. While the\nuser answers these questions, Archify buffers a set of corresponding\narchitectural decision recommendations. As the final result, the system\nrecommends a set of architectural decisions matching the project's needs\naccording to the requirements (as provided by the user) of the software under\ndevelopment. Nineteen professionals from academia and industry evaluated\nArchify through two surveys. The findings reveal that 94.7% of the participants\napproved Archify as a supporting tool. Respondents also highlighted the lack of\ntools supporting software architecture design, remarking the relevance of the\nproposed system.",
    "descriptor": "\nComments: 14 pages, 6 figures, conference\n",
    "authors": [
      "Breno Cruvinel Marinho",
      "Renato Bulc\u00e3o-Neto",
      "Valdemar Vicente Graciano Neto"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.08115"
  },
  {
    "id": "arXiv:2106.08117",
    "title": "Semantic Representation and Inference for NLP",
    "abstract": "Semantic representation and inference is essential for Natural Language\nProcessing (NLP). The state of the art for semantic representation and\ninference is deep learning, and particularly Recurrent Neural Networks (RNNs),\nConvolutional Neural Networks (CNNs), and transformer Self-Attention models.\nThis thesis investigates the use of deep learning for novel semantic\nrepresentation and inference, and makes contributions in the following three\nareas: creating training data, improving semantic representations and extending\ninference learning. In terms of creating training data, we contribute the\nlargest publicly available dataset of real-life factual claims for the purpose\nof automatic claim verification (MultiFC), and we present a novel inference\nmodel composed of multi-scale CNNs with different kernel sizes that learn from\nexternal sources to infer fact checking labels. In terms of improving semantic\nrepresentations, we contribute a novel model that captures non-compositional\nsemantic indicators. By definition, the meaning of a non-compositional phrase\ncannot be inferred from the individual meanings of its composing words (e.g.,\nhot dog). Motivated by this, we operationalize the compositionality of a phrase\ncontextually by enriching the phrase representation with external word\nembeddings and knowledge graphs. Finally, in terms of inference learning, we\npropose a series of novel deep learning architectures that improve inference by\nusing syntactic dependencies, by ensembling role guided attention heads,\nincorporating gating layers, and concatenating multiple heads in novel and\neffective ways. This thesis consists of seven publications (five published and\ntwo under review).",
    "descriptor": "\nComments: PhD thesis, the University of Copenhagen\n",
    "authors": [
      "Dongsheng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08117"
  },
  {
    "id": "arXiv:2106.08118",
    "title": "A Monte-Carlo Based Construction of Polarization-Adjusted Convolutional  (PAC) Codes",
    "abstract": "This paper proposes a rate-profile construction method for\npolarization-adjusted convolutional (PAC) codes of any code length and rate,\nwhich is capable of maintaining trade-off between the error-correction\nperformance and decoding complexity of PAC code. The proposed method can\nimprove the error-correction performance of PAC codes while guaranteeing a low\nmean sequential decoding complexity for signal-to-noise ratio (SNR) values\nbeyond a target SNR value.",
    "descriptor": "",
    "authors": [
      "Mohsen Moradi",
      "Amir Mozammel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.08118"
  },
  {
    "id": "arXiv:2106.08122",
    "title": "Sequence-Level Training for Non-Autoregressive Neural Machine  Translation",
    "abstract": "In recent years, Neural Machine Translation (NMT) has achieved notable\nresults in various translation tasks. However, the word-by-word generation\nmanner determined by the autoregressive mechanism leads to high translation\nlatency of the NMT and restricts its low-latency applications.\nNon-Autoregressive Neural Machine Translation (NAT) removes the autoregressive\nmechanism and achieves significant decoding speedup through generating target\nwords independently and simultaneously. Nevertheless, NAT still takes the\nword-level cross-entropy loss as the training objective, which is not optimal\nbecause the output of NAT cannot be properly evaluated due to the multimodality\nproblem. In this paper, we propose using sequence-level training objectives to\ntrain NAT models, which evaluate the NAT outputs as a whole and correlates well\nwith the real translation quality. Firstly, we propose training NAT models to\noptimize sequence-level evaluation metrics (e.g., BLEU) based on several novel\nreinforcement algorithms customized for NAT, which outperforms the conventional\nmethod by reducing the variance of gradient estimation. Secondly, we introduce\na novel training objective for NAT models, which aims to minimize the\nBag-of-Ngrams (BoN) difference between the model output and the reference\nsentence. The BoN training objective is differentiable and can be calculated\nefficiently without doing any approximations. Finally, we apply a three-stage\ntraining strategy to combine these two methods to train the NAT model. We\nvalidate our approach on four translation tasks (WMT14 En$\\leftrightarrow$De,\nWMT16 En$\\leftrightarrow$Ro), which shows that our approach largely outperforms\nNAT baselines and achieves remarkable performance on all translation tasks.",
    "descriptor": "",
    "authors": [
      "Chenze Shao",
      "Yang Feng",
      "Jinchao Zhang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08122"
  },
  {
    "id": "arXiv:2106.08129",
    "title": "Human movement augmentation and how to make it a reality",
    "abstract": "Augmenting the body with artificial limbs controlled concurrently to the\nnatural limbs has long appeared in science fiction, but recent technological\nand neuroscientific advances have begun to make this vision possible. By\nallowing individuals to achieve otherwise impossible actions, this movement\naugmentation could revolutionize medical and industrial applications and\nprofoundly change the way humans interact with their environment. Here, we\nconstruct a movement augmentation taxonomy through what is augmented and how it\nis achieved. With this framework, we analyze augmentation that extends the\nnumber of degrees-of-freedom, discuss critical features of effective\naugmentation such as physiological control signals, sensory feedback and\nlearning, and propose a vision for the field.",
    "descriptor": "",
    "authors": [
      "Jonathan Eden",
      "Mario Br\u00e4cklein",
      "Jaime Ib\u00e1\u00f1ez Pereda",
      "Deren Yusuf Barsakcioglu",
      "Giovanni Di Pino",
      "Dario Farina",
      "Etienne Burdet",
      "Carsten Mehring"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08129"
  },
  {
    "id": "arXiv:2106.08142",
    "title": "On Doctrines and Cartesian Bicategories",
    "abstract": "We study the relationship between cartesian bicategories and a specialisation\nof Lawvere's hyperdoctrines, namely elementary existential doctrines. Both\nprovide different ways of abstracting the structural properties of logical\nsystems: the former in algebraic terms based on a string diagrammatic calculus,\nthe latter in universal terms using the fundamental notion of adjoint functor.\nWe prove that these two approaches are related by an adjunction, which can be\nstrengthened to an equivalence by imposing further constraints on doctrines.",
    "descriptor": "",
    "authors": [
      "Filippo Bonchi",
      "Alessio Santamaria",
      "Jens Seeber",
      "Pawe\u0142 Soboci\u0144ski"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2106.08142"
  },
  {
    "id": "arXiv:2106.08143",
    "title": "Rcall: Calling R from Matlab",
    "abstract": "Summary: R and Matlab are two high-level scientific programming languages\nwhich are frequently applied in computational biology. To extend the wide\nvariety of available and approved implementations, we present the Rcall\ninterface which runs in MATLAB and provides direct access to methods and\nsoftware packages implemented in R. Rcall involves passing the relevant data to\nR, executing the specified R commands and forwarding the results to MATLAB for\nfurther use. The evaluation and conversion of the basic data types in R and\nMATLAB are provided. Due to the easy embedding of R facilities, Rcall greatly\nextends the functionality of the MATLAB programming language.\nAvailability: Source code is freely available at\nhttps://github.com/kreutz-lab/Rcall, implemented in MATLAB and supported on\nLinux, MS Windows and Mac OS X.",
    "descriptor": "\nComments: 3 pages, 1 figure\n",
    "authors": [
      "Janine Egert",
      "Clemens Kreutz"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.08143"
  },
  {
    "id": "arXiv:2106.08146",
    "title": "Graphical Gaussian Process Regression Model for Aqueous Solvation Free  Energy Prediction of Organic Molecules in Redox Flow Battery",
    "abstract": "The solvation free energy of organic molecules is a critical parameter in\ndetermining emergent properties such as solubility, liquid-phase equilibrium\nconstants, and pKa and redox potentials in an organic redox flow battery. In\nthis work, we present a machine learning (ML) model that can learn and predict\nthe aqueous solvation free energy of an organic molecule using Gaussian process\nregression method based on a new molecular graph kernel. To investigate the\nperformance of the ML model on electrostatic interaction, the nonpolar\ninteraction contribution of solvent and the conformational entropy of solute in\nsolvation free energy, three data sets with implicit or explicit water solvent\nmodels, and contribution of conformational entropy of solute are tested. We\ndemonstrate that our ML model can predict the solvation free energy of\nmolecules at chemical accuracy with a mean absolute error of less than 1\nkcal/mol for subsets of the QM9 dataset and the Freesolv database. To solve the\ngeneral data scarcity problem for a graph-based ML model, we propose a\ndimension reduction algorithm based on the distance between molecular graphs,\nwhich can be used to examine the diversity of the molecular data set. It\nprovides a promising way to build a minimum training set to improve prediction\nfor certain test sets where the space of molecular structures is predetermined.",
    "descriptor": "",
    "authors": [
      "Peiyuan Gao",
      "Xiu Yang",
      "Yu-Hang Tang",
      "Muqing Zheng",
      "Amity Anderson",
      "Vijayakumar Murugesan",
      "Aaron Hollas",
      "Wei Wang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2106.08146"
  },
  {
    "id": "arXiv:2106.08148",
    "title": "Weakly-Supervised Photo-realistic Texture Generation for 3D Face  Reconstruction",
    "abstract": "Although much progress has been made recently in 3D face reconstruction, most\nprevious work has been devoted to predicting accurate and fine-grained 3D\nshapes. In contrast, relatively little work has focused on generating\nhigh-fidelity face textures. Compared with the prosperity of photo-realistic 2D\nface image generation, high-fidelity 3D face texture generation has yet to be\nstudied. In this paper, we proposed a novel UV map generation model that\npredicts the UV map from a single face image. The model consists of a UV\nsampler and a UV generator. By selectively sampling the input face image's\npixels and adjusting their relative locations, the UV sampler generates an\nincomplete UV map that could faithfully reconstruct the original face. Missing\ntextures in the incomplete UV map are further full-filled by the UV generator.\nThe training is based on pseudo ground truth blended by the 3DMM texture and\nthe input face texture, thus weakly supervised. To deal with the artifacts in\nthe imperfect pseudo UV map, multiple partial UV map discriminators are\nleveraged.",
    "descriptor": "",
    "authors": [
      "Xiangnan Yin",
      "Di Huang",
      "Zehua Fu",
      "Yunhong Wang",
      "Liming Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08148"
  },
  {
    "id": "arXiv:2106.08159",
    "title": "Maximum Spanning Trees Are Invariant to Temperature Scaling in  Graph-based Dependency Parsing",
    "abstract": "Modern graph-based syntactic dependency parsers operate by predicting, for\neach token within a sentence, a probability distribution over its possible\nsyntactic heads (i.e., all other tokens) and then extracting a maximum spanning\ntree from the resulting log-probabilities. Nowadays, virtually all such parsers\nutilize deep neural networks and may thus be susceptible to miscalibration (in\nparticular, overconfident predictions). In this paper, we prove that\ntemperature scaling, a popular technique for post-hoc calibration of neural\nnetworks, cannot change the output of the aforementioned procedure. We conclude\nthat other techniques are needed to tackle miscalibration in graph-based\ndependency parsers in a way that improves parsing accuracy.",
    "descriptor": "\nComments: 4 pages, 2 figures\n",
    "authors": [
      "Stefan Gr\u00fcnewald"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08159"
  },
  {
    "id": "arXiv:2106.08164",
    "title": "Task Allocation and Coordinated Motion Planning for Autonomous  Multi-Robot Optical Inspection Systems",
    "abstract": "Autonomous multi-robot optical inspection systems are increasingly applied\nfor obtaining inline measurements in process monitoring and quality control.\nNumerous methods for path planning and robotic coordination have been developed\nfor static and dynamic environments and applied to different fields. However,\nthese approaches may not work for the autonomous multi-robot optical inspection\nsystem due to fast computation requirements of inline optimization, unique\ncharacteristics on robotic end-effector orientations, and complex large-scale\nfree-form product surfaces. This paper proposes a novel task allocation\nmethodology for coordinated motion planning of multi-robot inspection.\nSpecifically, (1) a local robust inspection task allocation is proposed to\nachieve efficient and well-balanced measurement assignment among robots; (2)\ncollision-free path planning and coordinated motion planning are developed via\ndynamic searching in robotic coordinate space and perturbation of probe poses\nor local paths in the conflicting robots. A case study shows that the proposed\napproach can mitigate the risk of collisions between robots and environments,\nresolve conflicts among robots, and reduce the inspection cycle time\nsignificantly and consistently.",
    "descriptor": "",
    "authors": [
      "Yinhua Liu",
      "Wenzheng Zhao",
      "Tim Lutz",
      "Xiaowei Yue"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.08164"
  },
  {
    "id": "arXiv:2106.08165",
    "title": "QoE Driven VR 360 Video Massive MIMO Transmission",
    "abstract": "Massive multiple-input and multiple-output (MIMO) enables ultra-high\nthroughput and low latency for tile-based adaptive virtual reality (VR) 360\nvideo transmission in wireless network. In this paper, we consider a massive\nMIMO system where multiple users in a single-cell theater watch an identical VR\n360 video. Based on tile prediction, base station (BS) deliveries the tiles in\npredicted field of view (FoV) to users. By introducing practical supplementary\ntransmission for missing tiles and unacceptable VR sickness, we propose the\nfirst stable transmission scheme for VR video. we formulate an integer\nnon-linear programming (INLP) problem to maximize users' average quality of\nexperience (QoE) score. Moreover, we derive the achievable spectral efficiency\n(SE) expression of predictive tile groups and the approximately achievable SE\nexpression of missing tile groups, respectively. Analytically, the overall\nthroughput is related to the number of tile groups and the length of pilot\nsequences. By exploiting the relationship between the structure of viewport\ntiles and SE expression, we propose a multi-lattice multi-stream grouping\nmethod aimed at improving the overall throughput for VR video transmission.\nMoreover, we analyze the relationship between QoE objective and number of\npredictive tile. We transform the original INLP problem into an integer linear\nprogramming problem by setting the predictive tiles groups as some constants.\nWith variable relaxation and recovery, we obtain the optimal average QoE.\nExtensive simulation results validate that the proposed algorithm effectively\nimproves QoE.",
    "descriptor": "\nComments: Acceptede by IEEE transactions on wireless communications\n",
    "authors": [
      "Long Teng",
      "Guangtao Zhai",
      "Yongpeng Wu",
      "Xiongkuo Min",
      "Wenjun Zhang",
      "Zhi Ding",
      "Chengshang Xiao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.08165"
  },
  {
    "id": "arXiv:2106.08166",
    "title": "Query Embedding on Hyper-relational Knowledge Graphs",
    "abstract": "Multi-hop logical reasoning is an established problem in the field of\nrepresentation learning on knowledge graphs (KGs). It subsumes both one-hop\nlink prediction as well as other more complex types of logical queries.\nExisting algorithms operate only on classical, triple-based graphs, whereas\nmodern KGs often employ a hyper-relational modeling paradigm. In this paradigm,\ntyped edges may have several key-value pairs known as qualifiers that provide\nfine-grained context for facts. In queries, this context modifies the meaning\nof relations, and usually reduces the answer set. Hyper-relational queries are\noften observed in real-world KG applications, and existing approaches for\napproximate query answering cannot make use of qualifier pairs. In this work,\nwe bridge this gap and extend the multi-hop reasoning problem to\nhyper-relational KGs allowing to tackle this new type of complex queries.\nBuilding upon recent advancements in Graph Neural Networks and query embedding\ntechniques, we study how to embed and answer hyper-relational conjunctive\nqueries. Besides that, we propose a method to answer such queries and\ndemonstrate in our experiments that qualifiers improve query answering on a\ndiverse set of query patterns.",
    "descriptor": "",
    "authors": [
      "Dimitrios Alivanistos",
      "Max Berrendorf",
      "Michael Cochez",
      "Mikhail Galkin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08166"
  },
  {
    "id": "arXiv:2106.08167",
    "title": "ShortcutFusion: From Tensorflow to FPGA-based accelerator with  reuse-aware memory allocation for shortcut data",
    "abstract": "Residual block is a very common component in recent state-of-the art CNNs\nsuch as EfficientNet or EfficientDet. Shortcut data accounts for nearly 40% of\nfeature-maps access in ResNet152 [8]. Most of the previous DNN compilers,\naccelerators ignore the shortcut data optimization. This paper presents\nShortcutFusion, an optimization tool for FPGA-based accelerator with a\nreuse-aware static memory allocation for shortcut data, to maximize on-chip\ndata reuse given resource constraints. From TensorFlow DNN models, the proposed\ndesign generates instruction sets for a group of nodes which uses an optimized\ndata reuse for each residual block. The accelerator design implemented on the\nXilinx KCU1500 FPGA card significantly outperforms NVIDIA RTX 2080 Ti, Titan\nXp, and GTX 1080 Ti for the EfficientNet inference. Compared to RTX 2080 Ti,\nthe proposed design is 1.35-2.33x faster and 6.7-7.9x more power efficient.\nCompared to the result from baseline, in which the weights, inputs, and outputs\nare accessed from the off-chip memory exactly once per each layer,\nShortcutFusion reduces the DRAM access by 47.8-84.8% for RetinaNet, Yolov3,\nResNet152, and EfficientNet. Given a similar buffer size to ShortcutMining [8],\nwhich also mine the shortcut data in hardware, the proposed work reduces\noff-chip access for feature-maps 5.27x while accessing weight from off-chip\nmemory exactly once.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Duy Thanh Nguyen",
      "Hyeonseung Je",
      "Tuan Nghia Nguyen",
      "Soojung Ryu",
      "Kyujung Lee",
      "Hyuk-Jae Lee"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2106.08167"
  },
  {
    "id": "arXiv:2106.08170",
    "title": "How Modular Should Neural Module Networks Be for Systematic  Generalization?",
    "abstract": "Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via\ncomposition of modules that tackle a sub-task. NMNs are a promising strategy to\nachieve systematic generalization, i.e. overcoming biasing factors in the\ntraining distribution. However, the aspects of NMNs that facilitate systematic\ngeneralization are not fully understood. In this paper, we demonstrate that the\nstage and the degree at which modularity is defined has large influence on\nsystematic generalization. In a series of experiments on three VQA datasets\n(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal\nthat tuning the degree of modularity in the network, especially at the image\nencoder stage, reaches substantially higher systematic generalization. These\nfindings lead to new NMN architectures that outperform previous ones in terms\nof systematic generalization.",
    "descriptor": "",
    "authors": [
      "Vanessa D'Amario",
      "Tomotake Sasaki",
      "Xavier Boix"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08170"
  },
  {
    "id": "arXiv:2106.08171",
    "title": "Evaluating Modules in Graph Contrastive Learning",
    "abstract": "The recent emergence of contrastive learning approaches facilitates the\nresearch on graph representation learning (GRL), introducing graph contrastive\nlearning (GCL) into the literature. These methods contrast semantically similar\nand dissimilar sample pairs to encode the semantics into node or graph\nembeddings. However, most existing works only performed model-level evaluation,\nand did not explore the combination space of modules for more comprehensive and\nsystematic studies. For effective module-level evaluation, we propose a\nframework that decomposes GCL models into four modules: (1) a sampler to\ngenerate anchor, positive and negative data samples (nodes or graphs); (2) an\nencoder and a readout function to get sample embeddings; (3) a discriminator to\nscore each sample pair (anchor-positive and anchor-negative); and (4) an\nestimator to define the loss function. Based on this framework, we conduct\ncontrolled experiments over a wide range of architectural designs and\nhyperparameter settings on node and graph classification tasks. Specifically,\nwe manage to quantify the impact of a single module, investigate the\ninteraction between modules, and compare the overall performance with current\nmodel architectures. Our key findings include a set of module-level guidelines\nfor GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an\nMLP encoder associated with Sum readout could achieve competitive performance\non graph classification. Finally, we release our implementations and results as\nOpenGCL, a modularized toolkit that allows convenient reproduction, standard\nmodel and module evaluation, and easy extension.",
    "descriptor": "",
    "authors": [
      "Ganqu Cui",
      "Yufeng Du",
      "Cheng Yang",
      "Jie Zhou",
      "Liang Xu",
      "Lifeng Wang",
      "Zhiyuan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08171"
  },
  {
    "id": "arXiv:2106.08177",
    "title": "The Reliability and Acceptance of Biometric System in Bangladesh: Users  Perspective",
    "abstract": "Biometric systems are the latest technologies of unique identification.\nPeople all over the world prefer to use this unique identification technology\nfor their authentication security. The goal of this research is to evaluate the\nbiometric systems based on system reliability and user satisfaction. As\ntechnology fully depends on personal data, so in terms of the quality and\nreliability of biometric systems, user satisfaction is a principal factor. To\nwalk with the digital era, it is extremely important to assess users' concerns\nabout data security as the systems are conducted the authentication by\nanalyzing users' personal data. The study shows that users are satisfied by\nusing biometric systems rather than other security systems. Besides, hardware\nfailure is a big issue faced by biometric systems users. Finally, a matrix is\ngenerated to compare the performance of popular biometric systems from the\nusers' opinions. As system reliability and user satisfaction are the focused\nissue of this research, biometric service providers can use these phenomena to\nfind what aspect of improvement they need for their services. Also, this study\ncan be a great visualizer for Bangladeshi users, so that they can easily\nrealize which biometric system they have to choose.",
    "descriptor": "\nComments: 7 pages, 4 figures, Published with International Journal of Computer Trends and Technology (IJCTT)\n",
    "authors": [
      "Shaykh Siddique",
      "Monica Yasmin",
      "Tasnova Bintee Taher",
      "Mushfiqul Alam"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.08177"
  },
  {
    "id": "arXiv:2106.08181",
    "title": "Direction is what you need: Improving Word Embedding Compression in  Large Language Models",
    "abstract": "The adoption of Transformer-based models in natural language processing (NLP)\nhas led to great success using a massive number of parameters. However, due to\ndeployment constraints in edge devices, there has been a rising interest in the\ncompression of these models to improve their inference time and memory\nfootprint. This paper presents a novel loss objective to compress token\nembeddings in the Transformer-based models by leveraging an AutoEncoder\narchitecture. More specifically, we emphasize the importance of the direction\nof compressed embeddings with respect to original uncompressed embeddings. The\nproposed method is task-agnostic and does not require further language modeling\npre-training. Our method significantly outperforms the commonly used SVD-based\nmatrix-factorization approach in terms of initial language model Perplexity.\nMoreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several\ndownstream tasks from the GLUE benchmark, where we also outperform the baseline\nin most scenarios. Our code is public.",
    "descriptor": "",
    "authors": [
      "Klaudia Ba\u0142azy",
      "Mohammadreza Banaei",
      "R\u00e9mi Lebret",
      "Jacek Tabor",
      "Karl Aberer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08181"
  },
  {
    "id": "arXiv:2106.08186",
    "title": "A Spacecraft Dataset for Detection, Segmentation and Parts Recognition",
    "abstract": "Virtually all aspects of modern life depend on space technology. Thanks to\nthe great advancement of computer vision in general and deep learning-based\ntechniques in particular, over the decades, the world witnessed the growing use\nof deep learning in solving problems for space applications, such as\nself-driving robot, tracers, insect-like robot on cosmos and health monitoring\nof spacecraft. These are just some prominent examples that has advanced space\nindustry with the help of deep learning. However, the success of deep learning\nmodels requires a lot of training data in order to have decent performance,\nwhile on the other hand, there are very limited amount of publicly available\nspace datasets for the training of deep learning models. Currently, there is no\npublic datasets for space-based object detection or instance segmentation,\npartly because manually annotating object segmentation masks is very time\nconsuming as they require pixel-level labelling, not to mention the challenge\nof obtaining images from space. In this paper, we aim to fill this gap by\nreleasing a dataset for spacecraft detection, instance segmentation and part\nrecognition. The main contribution of this work is the development of the\ndataset using images of space stations and satellites, with rich annotations\nincluding bounding boxes of spacecrafts and masks to the level of object parts,\nwhich are obtained with a mixture of automatic processes and manual efforts. We\nalso provide evaluations with state-of-the-art methods in object detection and\ninstance segmentation as a benchmark for the dataset. The link for downloading\nthe proposed dataset can be found on\nhttps://github.com/Yurushia1998/SatelliteDataset.",
    "descriptor": "",
    "authors": [
      "Dung Anh Hoang",
      "Bo Chen",
      "Tat-Jun Chin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08186"
  },
  {
    "id": "arXiv:2106.08187",
    "title": "Thompson Sampling for Unimodal Bandits",
    "abstract": "In this paper, we propose a Thompson Sampling algorithm for \\emph{unimodal}\nbandits, where the expected reward is unimodal over the partially ordered arms.\nTo exploit the unimodal structure better, at each step, instead of exploration\nfrom the entire decision space, our algorithm makes decision according to\nposterior distribution only in the neighborhood of the arm that has the highest\nempirical mean estimate. We theoretically prove that, for Bernoulli rewards,\nthe regret of our algorithm reaches the lower bound of unimodal bandits, thus\nit is asymptotically optimal. For Gaussian rewards, the regret of our algorithm\nis $\\mathcal{O}(\\log T)$, which is far better than standard Thompson Sampling\nalgorithms. Extensive experiments demonstrate the effectiveness of the proposed\nalgorithm on both synthetic data sets and the real-world applications.",
    "descriptor": "",
    "authors": [
      "Long Yang",
      "Zhao Li",
      "Zehong Hu",
      "Shasha Ruan",
      "Shijian Li",
      "Gang Pan",
      "Hongyang Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.08187"
  },
  {
    "id": "arXiv:2106.08188",
    "title": "Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in  Medical Image Segmentation",
    "abstract": "This paper addresses the domain shift problem for segmentation. As a\nsolution, we propose OLVA, a novel and lightweight unsupervised domain\nadaptation method based on a Variational Auto-Encoder (VAE) and Optimal\nTransport (OT) theory. Thanks to the VAE, our model learns a shared\ncross-domain latent space that follows a normal distribution, which reduces the\ndomain shift. To guarantee valid segmentations, our shared latent space is\ndesigned to model the shape rather than the intensity variations. We further\nrely on an OT loss to match and align the remaining discrepancy between the two\ndomains in the latent space. We demonstrate OLVA's effectiveness for the\nsegmentation of multiple cardiac structures on the public Multi-Modality Whole\nHeart Segmentation (MM-WHS) dataset, where the source domain consists of\nannotated 3D MR images and the unlabelled target domain of 3D CTs. Our results\nshow remarkable improvements with an additional margin of 12.5\\% dice score\nover concurrent generative training approaches.",
    "descriptor": "\nComments: 10 pages, 3 figures, conference MICCAI\n",
    "authors": [
      "Dawood Al Chanti",
      "Diana Mateus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08188"
  },
  {
    "id": "arXiv:2106.08190",
    "title": "Question Answering Infused Pre-training of General-Purpose  Contextualized Representations",
    "abstract": "This paper proposes a pre-training objective based on question answering (QA)\nfor learning general-purpose contextual representations, motivated by the\nintuition that the representation of a phrase in a passage should encode all\nquestions that the phrase can answer in context. We accomplish this goal by\ntraining a bi-encoder QA model, which independently encodes passages and\nquestions, to match the predictions of a more accurate cross-encoder model on\n80 million synthesized QA pairs. By encoding QA-relevant information, the\nbi-encoder's token-level representations are useful for non-QA downstream tasks\nwithout extensive (or in some cases, any) fine-tuning. We show large\nimprovements over both RoBERTa-large and previous state-of-the-art results on\nzero-shot and few-shot paraphrase detection on four datasets, few-shot named\nentity recognition on two datasets, and zero-shot sentiment analysis on three\ndatasets.",
    "descriptor": "",
    "authors": [
      "Robin Jia",
      "Mike Lewis",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08190"
  },
  {
    "id": "arXiv:2106.08195",
    "title": "A Linear-Time $n^{0.4}$-Approximation for Longest Common Subsequence",
    "abstract": "We consider the classic problem of computing the Longest Common Subsequence\n(LCS) of two strings of length $n$. While a simple quadratic algorithm has been\nknown for the problem for more than 40 years, no faster algorithm has been\nfound despite an extensive effort. The lack of progress on the problem has\nrecently been explained by Abboud, Backurs, and Vassilevska Williams [FOCS'15]\nand Bringmann and K\\\"unnemann [FOCS'15] who proved that there is no\nsubquadratic algorithm unless the Strong Exponential Time Hypothesis fails.\nThis has led the community to look for subquadratic approximation algorithms\nfor the problem.\nYet, unlike the edit distance problem for which a constant-factor\napproximation in almost-linear time is known, very little progress has been\nmade on LCS, making it a notoriously difficult problem also in the realm of\napproximation. For the general setting, only a naive\n$O(n^{\\varepsilon/2})$-approximation algorithm with running time\n$\\tilde{O}(n^{2-\\varepsilon})$ has been known, for any constant $0 <\n\\varepsilon \\le 1$. Recently, a breakthrough result by Hajiaghayi, Seddighin,\nSeddighin, and Sun [SODA'19] provided a linear-time algorithm that yields a\n$O(n^{0.497956})$-approximation in expectation; improving upon the naive\n$O(\\sqrt{n})$-approximation for the first time.\nIn this paper, we provide an algorithm that in time $O(n^{2-\\varepsilon})$\ncomputes an $\\tilde{O}(n^{2\\varepsilon/5})$-approximation with high\nprobability, for any $0 < \\varepsilon \\le 1$. Our result (1) gives an\n$\\tilde{O}(n^{0.4})$-approximation in linear time, improving upon the bound of\nHajiaghayi, Seddighin, Seddighin, and Sun, (2) provides an algorithm whose\napproximation scales with any subquadratic running time $O(n^{2-\\varepsilon})$,\nimproving upon the naive bound of $O(n^{\\varepsilon/2})$ for any $\\varepsilon$,\nand (3) instead of only in expectation, succeeds with high probability.",
    "descriptor": "\nComments: full version of ICALP'21 paper, abstract shortened to fit Arxiv requirements\n",
    "authors": [
      "Karl Bringmann",
      "Vincent Cohen-Addad",
      "Debarati Das"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.08195"
  },
  {
    "id": "arXiv:2106.08199",
    "title": "On Multi-objective Policy Optimization as a Tool for Reinforcement  Learning",
    "abstract": "Many advances that have improved the robustness and efficiency of deep\nreinforcement learning (RL) algorithms can, in one way or another, be\nunderstood as introducing additional objectives, or constraints, in the policy\noptimization step. This includes ideas as far ranging as exploration bonuses,\nentropy regularization, and regularization toward teachers or data priors when\nlearning from experts or in offline RL. Often, task reward and auxiliary\nobjectives are in conflict with each other and it is therefore natural to treat\nthese examples as instances of multi-objective (MO) optimization problems. We\nstudy the principles underlying MORL and introduce a new algorithm,\nDistillation of a Mixture of Experts (DiME), that is intuitive and\nscale-invariant under some conditions. We highlight its strengths on standard\nMO benchmark problems and consider case studies in which we recast offline RL\nand learning from experts as MO problems. This leads to a natural algorithmic\nformulation that sheds light on the connection between existing approaches. For\noffline RL, we use the MO perspective to derive a simple algorithm, that\noptimizes for the standard RL objective plus a behavioral cloning term. This\noutperforms state-of-the-art on two established offline RL benchmarks.",
    "descriptor": "",
    "authors": [
      "Abbas Abdolmaleki",
      "Sandy H. Huang",
      "Giulia Vezzani",
      "Bobak Shahriari",
      "Jost Tobias Springenberg",
      "Shruti Mishra",
      "Dhruva TB",
      "Arunkumar Byravan",
      "Konstantinos Bousmalis",
      "Andras Gyorgy",
      "Csaba Szepesvari",
      "Raia Hadsell",
      "Nicolas Heess",
      "Martin Riedmiller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08199"
  },
  {
    "id": "arXiv:2106.08204",
    "title": "Achieving digital-driven patient agility in the era of big data",
    "abstract": "There is still a limited understanding of the necessary skill, talent, and\nexpertise to manage digital technologies as a crucial enabler of the hospitals\nability to adequately sense and respond to patient needs and wishes, i.e.,\npatient agility. Therefore, this investigates how hospital departments can\nleverage a digital dy-namic capability to enable the departments patient\nagility. This study embraces the dynamic capabilities theory, develops a\nresearch model, and tests it accordingly using data from 90 clinical hospital\ndepartments from the Netherlands through an online survey. The model's\nhypothesized relationships are tested using structural equation modeling (SEM).\nThe outcomes demonstrate the significance of digital dynamic capability in\ndeveloping patient sensing and responding capabili-ties that, in turn,\npositively influence patient service performance. Outcomes are very relevant\nfor the hospital practice now, as hospitals worldwide need to trans-form\nhealthcare delivery processes using digital technologies and increase clinical\nproductivity.",
    "descriptor": "\nComments: 13 pages, 1 figure, The 20th IFIP Conference e-Business, e-Services, and e-Society I3E2021. arXiv admin note: text overlap with arXiv:2105.09013\n",
    "authors": [
      "Rogier van de Wetering"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.08204"
  },
  {
    "id": "arXiv:2106.08206",
    "title": "Hypergraph Dissimilarity Measures",
    "abstract": "In this paper, we propose two novel approaches for hypergraph comparison. The\nfirst approach transforms the hypergraph into a graph representation for use of\nstandard graph dissimilarity measures. The second approach exploits the\nmathematics of tensors to intrinsically capture multi-way relations. For each\napproach, we present measures that assess hypergraph dissimilarity at a\nspecific scale or provide a more holistic multi-scale comparison. We test these\nmeasures on synthetic hypergraphs and apply them to biological datasets.",
    "descriptor": "",
    "authors": [
      "Amit Surana",
      "Can Chen",
      "Indika Rajapakse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08206"
  },
  {
    "id": "arXiv:2106.08207",
    "title": "Graph-based Label Propagation for Semi-Supervised Speaker Identification",
    "abstract": "Speaker identification in the household scenario (e.g., for smart speakers)\nis typically based on only a few enrollment utterances but a much larger set of\nunlabeled data, suggesting semisupervised learning to improve speaker profiles.\nWe propose a graph-based semi-supervised learning approach for speaker\nidentification in the household scenario, to leverage the unlabeled speech\nsamples. In contrast to most of the works in speaker recognition that focus on\nspeaker-discriminative embeddings, this work focuses on speaker label inference\n(scoring). Given a pre-trained embedding extractor, graph-based learning allows\nus to integrate information about both labeled and unlabeled utterances.\nConsidering each utterance as a graph node, we represent pairwise utterance\nsimilarity scores as edge weights. Graphs are constructed per household, and\nspeaker identities are propagated to unlabeled nodes to optimize a global\nconsistency criterion. We show in experiments on the VoxCeleb dataset that this\napproach makes effective use of unlabeled data and improves speaker\nidentification accuracy compared to two state-of-the-art scoring methods as\nwell as their semi-supervised variants based on pseudo-labels.",
    "descriptor": "\nComments: To appear in Interspeech 2021\n",
    "authors": [
      "Long Chen",
      "Venkatesh Ravichandran",
      "Andreas Stolcke"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08207"
  },
  {
    "id": "arXiv:2106.08209",
    "title": "Can Evil IoT Twins Be Identified? Now Yes, a Hardware Behavioral  Fingerprinting Methodology",
    "abstract": "The connectivity and resource-constrained nature of IoT, and in particular\nsingle-board devices, opens up to cybersecurity concerns affecting the\nIndustrial Internet of Things (IIoT). One of the most important is the presence\nof evil IoT twins. Evil IoT twins are malicious devices, with identical\nhardware and software configurations to authorized ones, that can provoke\nsensitive information leakages, data poisoning, or privilege escalation in\nindustrial scenarios. Combining behavioral fingerprinting and Machine/Deep\nLearning (ML/DL) techniques is a promising solution to identify evil IoT twins\nby detecting minor performance differences generated by imperfections in\nmanufacturing. However, existing solutions are not suitable for single-board\ndevices because they do not consider their hardware and software limitations,\nunderestimate critical aspects during the identification performance\nevaluation, and do not explore the potential of ML/DL techniques. Moreover,\nthere is a dramatic lack of work explaining essential aspects to considering\nduring the identification of identical devices. This work proposes an\nML/DL-oriented methodology that uses behavioral fingerprinting to identify\nidentical single-board devices. The methodology leverages the different\nbuilt-in components of the system, comparing their internal behavior with each\nother to detect variations that occurred in manufacturing processes. The\nvalidation has been performed in a real environment composed of identical\nRaspberry Pi 4 Model B devices, achieving the identification for all devices by\nsetting a 50% threshold in the evaluation process. Finally, a discussion\ncompares the proposed solution with related work and provides important lessons\nlearned and limitations.",
    "descriptor": "",
    "authors": [
      "Pedro Miguel S\u00e1nchez S\u00e1nchez",
      "Jos\u00e9 Mar\u00eda Jorquera Valero",
      "Alberto Huertas Celdr\u00e1n",
      "G\u00e9r\u00f4me Bovet",
      "Manuel Gil P\u00e9rez",
      "Gregorio Mart\u00ednez P\u00e9rez"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.08209"
  },
  {
    "id": "arXiv:2106.08214",
    "title": "Efficient multi-level hp-finite elements in arbitrary dimensions",
    "abstract": "We present an efficient algorithmic framework for constructing multi-level\nhp-bases that uses a data-oriented approach that easily extends to any number\nof dimensions and provides a natural framework for performance-optimized\nimplementations. We only operate on the bounding faces of finite elements\nwithout considering their lower-dimensional topological features and\ndemonstrate the potential of the presented methods using a newly written\nopen-source library. First, we analyze a Fichera corner and show that the\nframework does not increase runtime and memory consumption when compared\nagainst the classical p-version of the finite element method. Then, we compute\na transient example with dynamic refinement and derefinement, where we also\nobtain the expected convergence rates and excellent performance in computing\ntime and memory usage.",
    "descriptor": "",
    "authors": [
      "Philipp Kopp",
      "Ernst Rank",
      "Victor M. Calo",
      "Stefan Kollmannsberger"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.08214"
  },
  {
    "id": "arXiv:2106.08226",
    "title": "Consistency Regularization for Cross-Lingual Fine-Tuning",
    "abstract": "Fine-tuning pre-trained cross-lingual language models can transfer\ntask-specific supervision from one language to the others. In this work, we\npropose to improve cross-lingual fine-tuning with consistency regularization.\nSpecifically, we use example consistency regularization to penalize the\nprediction sensitivity to four types of data augmentations, i.e., subword\nsampling, Gaussian noise, code-switch substitution, and machine translation. In\naddition, we employ model consistency to regularize the models trained with two\naugmented versions of the same training set. Experimental results on the XTREME\nbenchmark show that our method significantly improves cross-lingual fine-tuning\nacross various tasks, including text classification, question answering, and\nsequence labeling.",
    "descriptor": "\nComments: ACL-2021\n",
    "authors": [
      "Bo Zheng",
      "Li Dong",
      "Shaohan Huang",
      "Wenhui Wang",
      "Zewen Chi",
      "Saksham Singhal",
      "Wanxiang Che",
      "Ting Liu",
      "Xia Song",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08226"
  },
  {
    "id": "arXiv:2106.08229",
    "title": "MICo: Learning improved representations via sampling-based state  similarity for Markov decision processes",
    "abstract": "We present a new behavioural distance over the state space of a Markov\ndecision process, and demonstrate the use of this distance as an effective\nmeans of shaping the learnt representations of deep reinforcement learning\nagents. While existing notions of state similarity are typically difficult to\nlearn at scale due to high computational cost and lack of sample-based\nalgorithms, our newly-proposed distance addresses both of these issues. In\naddition to providing detailed theoretical analysis, we provide empirical\nevidence that learning this distance alongside the value function yields\nstructured and informative representations, including strong results on the\nArcade Learning Environment benchmark.",
    "descriptor": "",
    "authors": [
      "Pablo Samuel Castro",
      "Tyler Kastner",
      "Prakash Panangaden",
      "Mark Rowland"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.08229"
  },
  {
    "id": "arXiv:2106.08233",
    "title": "Spot the Difference: Topological Anomaly Detection via Geometric  Alignment",
    "abstract": "Geometric alignment appears in a variety of applications, ranging from domain\nadaptation, optimal transport, and normalizing flows in machine learning;\noptical flow and learned augmentation in computer vision and deformable\nregistration within biomedical imaging. A recurring challenge is the alignment\nof domains whose topology is not the same; a problem that is routinely ignored,\npotentially introducing bias in downstream analysis. As a first step towards\nsolving such alignment problems, we propose an unsupervised topological\ndifference detection algorithm. The model is based on a conditional variational\nauto-encoder and detects topological anomalies with regards to a reference\nalongside the registration step. We consider both a) topological changes in the\nimage under spatial variation and b) unexpected transformations. Our approach\nis validated on a proxy task of unsupervised anomaly detection in images.",
    "descriptor": "\nComments: Preprint, under review\n",
    "authors": [
      "Steffen Czolbe",
      "Aasa Feragen",
      "Oswin Krause"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.08233"
  },
  {
    "id": "arXiv:2106.08235",
    "title": "PairConnect: A Compute-Efficient MLP Alternative to Attention",
    "abstract": "Transformer models have demonstrated superior performance in natural language\nprocessing. The dot product self-attention in Transformer allows us to model\ninteractions between words. However, this modeling comes with significant\ncomputational overhead. In this work, we revisit the memory-compute trade-off\nassociated with Transformer, particularly multi-head attention, and show a\nmemory-heavy but significantly more compute-efficient alternative to\nTransformer. Our proposal, denoted as PairConnect, a multilayer perceptron\n(MLP), models the pairwise interaction between words by explicit pairwise word\nembeddings. As a result, PairConnect substitutes self dot product with a simple\nembedding lookup. We show mathematically that despite being an MLP, our\ncompute-efficient PairConnect is strictly more expressive than Transformer. Our\nexperiment on language modeling tasks suggests that PairConnect could achieve\ncomparable results with Transformer while reducing the computational cost\nassociated with inference significantly.",
    "descriptor": "",
    "authors": [
      "Zhaozhuo Xu",
      "Minghao Yan",
      "Junyan Zhang",
      "Anshumali Shrivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08235"
  },
  {
    "id": "arXiv:2106.08250",
    "title": "Constrained Motion Planning of A Cable-Driven Soft Robot With  Compressible Curvature Modeling",
    "abstract": "A cable-driven soft-bodied robot with redundancy can conduct the trajectory\ntracking task and in the meanwhile fulfill some extra constraints, such as\ntracking through an end-effector in designated orientation, or get rid of the\nevitable manipulator-obstacle collision. Those constraints require rational\nplanning of the robot motion. In this work, we derived the compressible\ncurvature kinematics of a cable-driven soft robot which takes the compressible\nsoft segment into account. The motion planning of the soft robot for a\ntrajectory tracking task in constrained conditions, including fixed orientation\nend-effector and manipulator-obstacle collision avoidance, has been\ninvestigated. The inverse solution of cable actuation was formulated as a\ndamped least-square optimization problem and iteratively computed off-line. The\nperformance of trajectory tracking and the obedience to constraints were\nevaluated via the simulation we made open-source, as well as the prototype\nexperiments. The method can be generalized to the similar multisegment\ncable-driven soft robotic systems by customizing the robot parameters for the\nprior motion planning of the manipulator.",
    "descriptor": "\nComments: 8 pages, 9 figures\n",
    "authors": [
      "Jiewen Lai",
      "Bo Lu",
      "Qingxiang Zhao",
      "Henry K. Chu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08250"
  },
  {
    "id": "arXiv:2106.08252",
    "title": "Interpretable Self-supervised Multi-task Learning for COVID-19  Information Retrieval and Extraction",
    "abstract": "The rapidly evolving literature of COVID-19 related articles makes it\nchallenging for NLP models to be effectively trained for information retrieval\nand extraction with the corresponding labeled data that follows the current\ndistribution of the pandemic. On the other hand, due to the uncertainty of the\nsituation, human experts' supervision would always be required to double check\nthe decision making of these models highlighting the importance of\ninterpretability. In the light of these challenges, this study proposes an\ninterpretable self-supervised multi-task learning model to jointly and\neffectively tackle the tasks of information retrieval (IR) and extraction (IE)\nduring the current emergency health crisis situation. Our results show that our\nmodel effectively leverage the multi-task and self-supervised learning to\nimprove generalization, data efficiency and robustness to the ongoing dataset\nshift problem. Our model outperforms baselines in IE and IR tasks, respectively\nby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In\nIE the zero- and few-shot learning performances are on average 0.32 and 0.19\nmicro-f score higher than those of the baselines.",
    "descriptor": "",
    "authors": [
      "Nima Ebadi",
      "Peyman Najafirad"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08252"
  },
  {
    "id": "arXiv:2106.08253",
    "title": "A Syntax-Guided Edit Decoder for Neural Program Repair",
    "abstract": "Automated Program Repair (APR) helps improve the efficiency of software\ndevelopment and maintenance. Recent APR techniques use deep learning,\nparticularly the encoder-decoder architecture, to generate patches.\nThough existing DL-based APR approaches have proposed different encoder\narchitectures, the decoder remains to be the standard one, which generates a\nsequence of tokens one by one to replace the faulty statement.\nThis decoder has multiple limitations: 1) allowing to generate syntactically\nincorrect programs, 2) inefficiently representing small edits, and 3) not being\nable to generate project-specific identifiers.\nIn this paper, we propose Recoder, a syntax-guided edit decoder with\nplaceholder generation. Recoder is novel in multiple aspects: 1) Recoder\ngenerates edits rather than modified code, allowing efficient representation of\nsmall edits; 2) Recoder is syntax-guided, with the novel provider/decider\narchitecture to ensure the syntactic correctness of the patched program and\naccurate generation; 3) Recoder generates placeholders that could be\ninstantiated as project-specific identifiers later.\nWe conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2,\n420 additional bugs from Defects4J v2.0, 297 bugs from IntroClassJava and 40\nbugs from QuixBugs. Our results show that Recoder repairs 53 bugs on Defects4J\nv1.2, which achieves 26.2% (11 bugs) improvement over the previous\nstate-of-the-art approach for single-hunk bugs (TBar). Importantly, to our\nknowledge, Recoder is the first DL-based APR approach that has outperformed the\ntraditional APR approaches on this benchmark.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Qihao Zhu",
      "Zeyu Sun",
      "Yuan-an Xiao",
      "Wenjie Zhang",
      "Kang Yuan",
      "Yingfei Xiong",
      "Lu Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.08253"
  },
  {
    "id": "arXiv:2106.08254",
    "title": "BEiT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BEiT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT developed in the natural language processing area, we propose a masked\nimage modeling task to pretrain vision Transformers. Specifically, each image\nhas two views in our pre-training, i.e, image patches (such as 16x16 pixels),\nand visual tokens (i.e., discrete tokens). We first \"tokenize\" the original\nimage into visual tokens. Then we randomly mask some image patches and fed them\ninto the backbone Transformer. The pre-training objective is to recover the\noriginal visual tokens based on the corrupted image patches. After pre-training\nBEiT, we directly fine-tune the model parameters on downstream tasks by\nappending task layers upon the pretrained encoder. Experimental results on\nimage classification and semantic segmentation show that our model achieves\ncompetitive results with previous pre-training methods. For example, base-size\nBEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming\nfrom-scratch DeiT training (81.8%) with the same setup. Moreover, large-size\nBEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with\nsupervised pre-training on ImageNet-22K (85.2%). The code and pretrained models\nare available at https://aka.ms/beit.",
    "descriptor": "\nComments: A Path to the BERT Moment of CV. Work in progress\n",
    "authors": [
      "Hangbo Bao",
      "Li Dong",
      "Furu Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08254"
  },
  {
    "id": "arXiv:2106.08258",
    "title": "Identifying Roles, Requirements and Responsibilities in Trustworthy AI  Systems",
    "abstract": "Artificial Intelligence (AI) systems are being deployed around the globe in\ncritical fields such as healthcare and education. In some cases, expert\npractitioners in these domains are being tasked with introducing or using such\nsystems, but have little or no insight into what data these complex systems are\nbased on, or how they are put together. In this paper, we consider an AI system\nfrom the domain practitioner's perspective and identify key roles that are\ninvolved in system deployment. We consider the differing requirements and\nresponsibilities of each role, and identify a tension between transparency and\nprivacy that needs to be addressed so that domain practitioners are able to\nintelligently assess whether a particular AI system is appropriate for use in\ntheir domain.",
    "descriptor": "\nComments: Pre-print of paper submitted to Workshop on Reviewable and Auditable Pervasive Systems (WRAPS)\n",
    "authors": [
      "Iain Barclay",
      "Will Abramson"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.08258"
  },
  {
    "id": "arXiv:2106.08259",
    "title": "Fairness as Equality of Opportunity: Normative Guidance from Political  Philosophy",
    "abstract": "Recent interest in codifying fairness in Automated Decision Systems (ADS) has\nresulted in a wide range of formulations of what it means for an algorithmic\nsystem to be fair. Most of these propositions are inspired by, but inadequately\ngrounded in, political philosophy scholarship. This paper aims to correct that\ndeficit. We introduce a taxonomy of fairness ideals using doctrines of Equality\nof Opportunity (EOP) from political philosophy, clarifying their conceptions in\nphilosophy and the proposed codification in fair machine learning. We arrange\nthese fairness ideals onto an EOP spectrum, which serves as a useful frame to\nguide the design of a fair ADS in a given context.\nWe use our fairness-as-EOP framework to re-interpret the impossibility\nresults from a philosophical perspective, as the in-compatibility between\ndifferent value systems, and demonstrate the utility of the framework with\nseveral real-world and hypothetical examples. Through our EOP-framework we hope\nto answer what it means for an ADS to be fair from a moral and political\nphilosophy standpoint, and to pave the way for similar scholarship from ethics\nand legal experts.",
    "descriptor": "",
    "authors": [
      "Falaah Arif Khan",
      "Eleni Manis",
      "Julia Stoyanovich"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08259"
  },
  {
    "id": "arXiv:2106.08261",
    "title": "Physion: Evaluating Physical Prediction from Vision in Humans and  Machines",
    "abstract": "While machine learning algorithms excel at many challenging visual tasks, it\nis unclear that they can make predictions about commonplace real world physical\nevents. Here, we present a visual and physical prediction benchmark that\nprecisely measures this capability. In realistically simulating a wide variety\nof physical phenomena -- rigid and soft-body collisions, stable multi-object\nconfigurations, rolling and sliding, projectile motion -- our dataset presents\na more comprehensive challenge than existing benchmarks. Moreover, we have\ncollected human responses for our stimuli so that model predictions can be\ndirectly compared to human judgments. We compare an array of algorithms --\nvarying in their architecture, learning objective, input-output structure, and\ntraining data -- on their ability to make diverse physical predictions. We find\nthat graph neural networks with access to the physical state best capture human\nbehavior, whereas among models that receive only visual input, those with\nobject-centric representations or pretraining do best but fall far short of\nhuman accuracy. This suggests that extracting physically meaningful\nrepresentations of scenes is the main bottleneck to achieving human-like visual\nprediction. We thus demonstrate how our benchmark can identify areas for\nimprovement and measure progress on this key aspect of physical understanding.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Daniel M. Bear",
      "Elias Wang",
      "Damian Mrowca",
      "Felix J. Binder",
      "Hsiau-Yu Fish Tung",
      "R.T. Pramod",
      "Cameron Holdaway",
      "Sirui Tao",
      "Kevin Smith",
      "Li Fei-Fei",
      "Nancy Kanwisher",
      "Joshua B. Tenenbaum",
      "Daniel L.K. Yamins",
      "Judith E. Fan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08261"
  },
  {
    "id": "arXiv:2106.08265",
    "title": "Towards Total Recall in Industrial Anomaly Detection",
    "abstract": "Being able to spot defective parts is a critical component in large-scale\nindustrial manufacturing. A particular challenge that we address in this work\nis the cold-start problem: fit a model using nominal (non-defective) example\nimages only. While handcrafted solutions per class are possible, the goal is to\nbuild systems that work well simultaneously on many different tasks\nautomatically. The best peforming approaches combine embeddings from ImageNet\nmodels with an outlier detection model. In this paper, we extend on this line\nof work and propose PatchCore, which uses a maximally representative memory\nbank of nominal patch-features. PatchCore offers competitive inference times\nwhile achieving state-of-the-art performance for both detection and\nlocalization. On the standard dataset MVTec AD, PatchCore achieves an\nimage-level anomaly detection AUROC score of $99.1\\%$, more than halving the\nerror compared to the next best competitor. We further report competitive\nresults on two additional datasets and also find competitive results in the few\nsamples regime.",
    "descriptor": "",
    "authors": [
      "Karsten Roth",
      "Latha Pemula",
      "Joaquin Zepeda",
      "Bernhard Sch\u00f6lkopf",
      "Thomas Brox",
      "Peter Gehler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08265"
  },
  {
    "id": "arXiv:2106.08267",
    "title": "Multi-script Handwritten Digit Recognition Using Multi-task Learning",
    "abstract": "Handwritten digit recognition is one of the extensively studied area in\nmachine learning. Apart from the wider research on handwritten digit\nrecognition on MNIST dataset, there are many other research works on various\nscript recognition. However, it is not very common for multi-script digit\nrecognition which encourage the development of robust and multipurpose systems.\nAdditionally working on multi-script digit recognition enables multi-task\nlearning, considering the script classification as a related task for instance.\nIt is evident that multi-task learning improves model performance through\ninductive transfer using the information contained in related tasks. Therefore,\nin this study multi-script handwritten digit recognition using multi-task\nlearning will be investigated. As a specific case of demonstrating the solution\nto the problem, Amharic handwritten character recognition will also be\nexperimented. The handwritten digits of three scripts including Latin, Arabic\nand Kannada are studied to show that multi-task models with reformulation of\nthe individual tasks have shown promising results. In this study a novel way of\nusing the individual tasks predictions was proposed to help classification\nperformance and regularize the different loss for the purpose of the main task.\nThis finding has outperformed the baseline and the conventional multi-task\nlearning models. More importantly, it avoided the need for weighting the\ndifferent losses of the tasks, which is one of the challenges in multi-task\nlearning.",
    "descriptor": "",
    "authors": [
      "Mesay Samuel Gondere",
      "Lars Schmidt-Thieme",
      "Durga Prasad Sharma",
      "Randolf Scholz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08267"
  },
  {
    "id": "arXiv:2106.08269",
    "title": "Generating Data Augmentation samples for Semantic Segmentation of Salt  Bodies in a Synthetic Seismic Image Dataset",
    "abstract": "Nowadays, subsurface salt body localization and delineation, also called\nsemantic segmentation of salt bodies, are among the most challenging\ngeophysicist tasks. Thus, identifying large salt bodies is notoriously tricky\nand is crucial for identifying hydrocarbon reservoirs and drill path planning.\nThis work proposes a Data Augmentation method based on training two generative\nmodels to augment the number of samples in a seismic image dataset for the\nsemantic segmentation of salt bodies. Our method uses deep learning models to\ngenerate pairs of seismic image patches and their respective salt masks for the\nData Augmentation. The first model is a Variational Autoencoder and is\nresponsible for generating patches of salt body masks. The second is a\nConditional Normalizing Flow model, which receives the generated masks as\ninputs and generates the associated seismic image patches. We evaluate the\nproposed method by comparing the performance of ten distinct state-of-the-art\nmodels for semantic segmentation, trained with and without the generated\naugmentations, in a dataset from two synthetic seismic images. The proposed\nmethodology yields an average improvement of 8.57% in the IoU metric across all\ncompared models. The best result is achieved by a DeeplabV3+ model variant,\nwhich presents an IoU score of 95.17% when trained with our augmentations.\nAdditionally, our proposal outperformed six selected data augmentation methods,\nand the most significant improvement in the comparison, of 9.77%, is achieved\nby composing our DA with augmentations from an elastic transformation. At last,\nwe show that the proposed method is adaptable for a larger context size by\nachieving results comparable to the obtained on the smaller context size.",
    "descriptor": "",
    "authors": [
      "Luis Felipe Henriques",
      "S\u00e9rgio Colcher",
      "Ruy Luiz Milidi\u00fa",
      "Andr\u00e9 Bulc\u00e3o",
      "Pablo Barros"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08269"
  },
  {
    "id": "arXiv:2106.08272",
    "title": "Deep Reinforcement Learning for Conservation Decisions",
    "abstract": "Can machine learning help us make better decisions about a changing planet?\nIn this paper, we illustrate and discuss the potential of a promising corner of\nmachine learning known as _reinforcement learning_ (RL) to help tackle the most\nchallenging conservation decision problems. RL is uniquely well suited to\nconservation and global change challenges for three reasons: (1) RL explicitly\nfocuses on designing an agent who _interacts_ with an environment which is\ndynamic and uncertain, (2) RL approaches do not require massive amounts of\ndata, (3) RL approaches would utilize rather than replace existing models,\nsimulations, and the knowledge they contain. We provide a conceptual and\ntechnical introduction to RL and its relevance to ecological and conservation\nchallenges, including examples of a problem in setting fisheries quotas and in\nmanaging ecological tipping points. Four appendices with annotated code provide\na tangible introduction to researchers looking to adopt, evaluate, or extend\nthese approaches.",
    "descriptor": "\nComments: 4 appendices\n",
    "authors": [
      "Marcus Lapeyrolerie",
      "Melissa S. Chapman",
      "Kari E. A. Norman",
      "Carl Boettiger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.08272"
  },
  {
    "id": "arXiv:2106.08274",
    "title": "Elasticity Based Demand Forecasting and Price Optimization for Online  Retail",
    "abstract": "We study a problem of an online retailer who observes the unit sales of a\nproduct, and dynamically changes the retail price, in order to maximize the\nexpected revenue. Assuming the demand of the product is price sensitive, we are\ninterested in the optimal pricing policy when future demand is uncertain. We\nbuild a system to investigate the relationship between retail price and demand\nand estimate the demand function. The system predicts demand and revenue at a\ngiven retail price. We formulate a revenue maximization problem over a discrete\nfinite time horizon with discrete retail price. The optimal pricing policy is\nsolved based on the predicted demand and revenue values. With computational\nexperiments, we investigate the effect of optimal pricing policy to inventory\nmanagement.",
    "descriptor": "",
    "authors": [
      "Chengcheng Liu",
      "M\u00e1ty\u00e1s A. Sustik"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.08274"
  },
  {
    "id": "arXiv:2106.08279",
    "title": "Awardee Solution of KDD Cup 2021 OGB Large-Scale Challenge Graph-Level  Track",
    "abstract": "In this technical report, we present our solution of KDD Cup 2021 OGB\nLarge-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our\nbasic models. We train each model by 8-fold cross-validation, and additionally\ntrain two Graphormer models on the union of training and validation sets with\ndifferent random seeds. For final submission, we use a naive ensemble for these\n18 models by taking average of their outputs. Using our method, our team\nMachineLearning achieved 0.1200 MAE on test set.",
    "descriptor": "",
    "authors": [
      "Chengxuan Ying",
      "Mingqi Yang",
      "Shuxin Zheng",
      "Guolin Ke",
      "Shengjie Luo",
      "Tianle Cai",
      "Chenglin Wu",
      "Yuxin Wang",
      "Yanming Shen",
      "Di He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08279"
  },
  {
    "id": "arXiv:2106.08283",
    "title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks",
    "abstract": "Federated Learning (FL) as a distributed learning paradigm that aggregates\ninformation from diverse clients to train a shared global model, has\ndemonstrated great success. However, malicious clients can perform poisoning\nattacks and model replacement to introduce backdoors into the trained global\nmodel. Although there have been intensive studies designing robust aggregation\nmethods and empirical robust federated training protocols against backdoors,\nexisting approaches lack robustness certification. This paper provides the\nfirst general framework, Certifiably Robust Federated Learning (CRFL), to train\ncertifiably robust FL models against backdoors. Our method exploits clipping\nand smoothing on model parameters to control the global model smoothness, which\nyields a sample-wise robustness certification on backdoors with limited\nmagnitude. Our certification also specifies the relation to federated learning\nparameters, such as poisoning ratio on instance level, number of attackers, and\ntraining iterations. Practically, we conduct comprehensive experiments across a\nrange of federated datasets, and provide the first benchmark for certified\nrobustness against backdoor attacks in federated learning. Our code is\navailable at https://github.com/AI-secure/CRFL.",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Chulin Xie",
      "Minghao Chen",
      "Pin-Yu Chen",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08283"
  },
  {
    "id": "arXiv:2106.08285",
    "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell  Microscopy",
    "abstract": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards\ncompletely in silico experiments, is to synthesise the imagery itself. Here, we\npropose Multi-StyleGAN as a descriptive approach to simulate time-lapse\nfluorescence microscopy imagery of living cells, based on a past experiment.\nThis novel generative adversarial network synthesises a multi-domain sequence\nof consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple\nlive yeast cells in microstructured environments and train on a dataset\nrecorded in our laboratory. The simulation captures underlying biophysical\nfactors and time dependencies, such as cell morphology, growth, physical\ninteractions, as well as the intensity of a fluorescent reporter protein. An\nimmediate application is to generate additional training and validation data\nfor feature extraction algorithms or to aid and expedite development of\nadvanced experimental techniques such as online monitoring or control of cells.\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
    "descriptor": "\nComments: accepted to MICCAI 2021. (Tim Prangemeier and Christoph Reich --- both authors contributed equally)\n",
    "authors": [
      "Tim Prangemeier",
      "Christoph Reich",
      "Christian Wildner",
      "Heinz Koeppl"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08285"
  },
  {
    "id": "arXiv:2106.08286",
    "title": "Framework Artifact for the Road-Based Physical Internet based on  Internet Protocols",
    "abstract": "The Physical Internet (PI) raises high expectations for efficiency gains in\ntransport and logistics. The PI represents the network of logistics networks\nfor physical objects in analogy to the Data Internet (DI). Road based traffic\nrepresents one of these logistics networks. Here, many empty runs and\nunderutilized trips still take place. Hence, there is a lot of potential in the\nroad-based Physical Internet (RBPI), which will have an impact on transport and\nlogistics strategies, but also on vehicle design. On the DI, logistics\nstrategies are implemented in protocols. In order to transfer such concepts to\nthe RBPI, relevant protocols of the DI had been analyzed and transferred to the\nworld of physical objects. However, not all functionalities can be transferred\none-to-one, e.g. a data packet in the DI can simply be re-generated by a hub in\ncase of damage or loss. To compensate for the challenges, a framework artifact\nhas been designed with appropriate transformation customizations based on\ndesign science principles. From this, resulting requirements for future\nvehicles were derived. This paper makes a contribution to the implementation of\nthe RBPI in order to fit road based vehicles to the future world of transport\nand logistics.",
    "descriptor": "\nComments: 16 pages, 8th International Physical Internet Conference, Athens\n",
    "authors": [
      "Steffen Kaup",
      "Andr\u00e9 Ludwig",
      "Bogdan Franczyk"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.08286"
  },
  {
    "id": "arXiv:2106.08290",
    "title": "Coded Privacy-Preserving Computation at Edge Networks",
    "abstract": "Multi-party computation (MPC) is promising for privacy-preserving machine\nlearning algorithms at edge networks, like federated learning. Despite their\npotential, existing MPC algorithms fail short of adapting to the limited\nresources of edge devices. A promising solution, and the focus of this work, is\ncoded computation, which advocates the use of error-correcting codes to improve\nthe performance of distributed computing through ``smart'' data redundancy. In\nthis paper, we focus on coded privacy-preserving computation using Shamir's\nsecret sharing. In particular, we design novel coded privacy-preserving\ncomputation mechanisms; MatDot coded MPC (MatDot-CMPC) and PolyDot coded MPC\n(PolyDot-CMPC) by employing recently proposed coded computation algorithms;\nMatDot and PolyDot. We take advantage of the ``garbage terms'' that naturally\narise when polynomials are constructed in the design of MatDot-CMPC and\nPolyDot-CMPC to reduce the number of workers needed for privacy-preserving\ncomputation. Also, we analyze MatDot-CMPC and PolyDot-CMPC in terms of their\ncomputation, storage, communication overhead as well as recovery threshold, so\nthey can easily adapt to the limited resources of edge devices.",
    "descriptor": "",
    "authors": [
      "Elahe Vedadi - Yasaman Keshtkarjahromi - Hulya Seferoglu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.08290"
  },
  {
    "id": "arXiv:2106.08294",
    "title": "Three-part diachronic semantic change dataset for Russian",
    "abstract": "We present a manually annotated lexical semantic change dataset for Russian:\nRuShiftEval. Its novelty is ensured by a single set of target words annotated\nfor their diachronic semantic shifts across three time periods, while the\nprevious work either used only two time periods, or different sets of target\nwords. The paper describes the composition and annotation procedure for the\ndataset. In addition, it is shown how the ternary nature of RuShiftEval allows\nto trace specific diachronic trajectories: `changed at a particular time period\nand stable afterwards' or `was changing throughout all time periods'. Based on\nthe analysis of the submissions to the recent shared task on semantic change\ndetection for Russian, we argue that correctly identifying such trajectories\ncan be an interesting sub-task itself.",
    "descriptor": "\nComments: Accepted to the 2nd International Workshop on Computational Approaches to Historical Language Change 2021 (LChange'21)\n",
    "authors": [
      "Andrey Kutuzov",
      "Lidia Pivovarova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.08294"
  },
  {
    "id": "arXiv:2106.08295",
    "title": "A White Paper on Neural Network Quantization",
    "abstract": "While neural networks have advanced the frontiers in many applications, they\noften come at a high computational cost. Reducing the power and latency of\nneural network inference is key if we want to integrate modern networks into\nedge devices with strict power and compute requirements. Neural network\nquantization is one of the most effective ways of achieving these savings but\nthe additional noise it induces can lead to accuracy degradation. In this white\npaper, we introduce state-of-the-art algorithms for mitigating the impact of\nquantization noise on the network's performance while maintaining low-bit\nweights and activations. We start with a hardware motivated introduction to\nquantization and then consider two main classes of algorithms: Post-Training\nQuantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no\nre-training or labelled data and is thus a lightweight push-button approach to\nquantization. In most cases, PTQ is sufficient for achieving 8-bit quantization\nwith close to floating-point accuracy. QAT requires fine-tuning and access to\nlabeled training data but enables lower bit quantization with competitive\nresults. For both solutions, we provide tested pipelines based on existing\nliterature and extensive experimentation that lead to state-of-the-art\nperformance for common deep learning models and tasks.",
    "descriptor": "",
    "authors": [
      "Markus Nagel",
      "Marios Fournarakis",
      "Rana Ali Amjad",
      "Yelysei Bondarenko",
      "Mart van Baalen",
      "Tijmen Blankevoort"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08295"
  },
  {
    "id": "arXiv:2106.08298",
    "title": "StockBabble: A Conversational Financial Agent to support Stock Market  Investors",
    "abstract": "We introduce StockBabble, a conversational agent designed to support\nunderstanding and engagement with the stock market. StockBabble's value and\nnovelty is in its ability to empower retail investors -- many of which may be\nnew to investing -- and supplement their informational needs using a\nuser-friendly agent. Users have the ability to query information on companies\nto retrieve a general and financial overview of a stock, including accessing\nthe latest news and trading recommendations. They can also request charts which\ncontain live prices and technical investment indicators, and add shares to a\npersonal portfolio to allow performance monitoring over time. To evaluate our\nagent's potential, we conducted a user study with 15 participants. In total,\n73% (11/15) of respondents said that they felt more confident in investing\nafter using StockBabble, and all 15 would consider recommending it to others.\nThese results are encouraging and suggest a wider appeal for such agents.\nMoreover, we believe this research can help to inform the design and\ndevelopment of future intelligent, financial personal assistants.",
    "descriptor": "\nComments: CUI 2021 - 3rd Conference on Conversational User Interfaces\n",
    "authors": [
      "Suraj Sharma",
      "Joseph Brennan",
      "Jason R. C. Nurse"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.08298"
  },
  {
    "id": "arXiv:2106.08299",
    "title": "Model Extraction and Adversarial Attacks on Neural Networks using  Switching Power Information",
    "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the\nlast decade for solving narrow AI problems in domains such as healthcare,\ntransportation, and defense. As ANNs become more ubiquitous, it is imperative\nto understand their associated safety, security, and privacy vulnerabilities.\nRecently, it has been shown that ANNs are susceptible to a number of\nadversarial evasion attacks--inputs that cause the ANN to make high-confidence\nmisclassifications despite being almost indistinguishable from the data used to\ntrain and test the network. This work explores to what degree finding these\nexamples maybe aided by using side-channel information, specifically switching\npower consumption, of hardware implementations of ANNs. A black-box threat\nscenario is assumed, where an attacker has access to the ANN hardware's input,\noutputs, and topology, but the trained model parameters are unknown. Then, a\nsurrogate model is trained to have similar functional (i.e. input-output\nmapping) and switching power characteristics as the oracle (black-box) model.\nOur results indicate that the inclusion of power consumption data increases the\nfidelity of the model extraction by up to 30 percent based on a mean square\nerror comparison of the oracle and surrogate weights. However, transferability\nof adversarial examples from the surrogate to the oracle model was not\nsignificantly affected.",
    "descriptor": "",
    "authors": [
      "Tommy Li",
      "Cory Merkel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08299"
  },
  {
    "id": "arXiv:2106.08301",
    "title": "Efficient Micro-Structured Weight Unification for Neural Network  Compression",
    "abstract": "Compressing Deep Neural Network (DNN) models to alleviate the storage and\ncomputation requirements is essential for practical applications, especially\nfor resource limited devices. Although capable of reducing a reasonable amount\nof model parameters, previous unstructured or structured weight pruning methods\ncan hardly truly accelerate inference, either due to the poor hardware\ncompatibility of the unstructured sparsity or due to the low sparse rate of the\nstructurally pruned network. Aiming at reducing both storage and computation,\nas well as preserving the original task performance, we propose a generalized\nweight unification framework at a hardware compatible micro-structured level to\nachieve high amount of compression and acceleration. Weight coefficients of a\nselected micro-structured block are unified to reduce the storage and\ncomputation of the block without changing the neuron connections, which turns\nto a micro-structured pruning special case when all unified coefficients are\nset to zero, where neuron connections (hence storage and computation) are\ncompletely removed. In addition, we developed an effective training framework\nbased on the alternating direction method of multipliers (ADMM), which converts\nour complex constrained optimization into separately solvable subproblems.\nThrough iteratively optimizing the subproblems, the desired micro-structure can\nbe ensured with high compression ratio and low performance degradation. We\nextensively evaluated our method using a variety of benchmark models and\ndatasets for different applications. Experimental results demonstrate\nstate-of-the-art performance.",
    "descriptor": "\nComments: 10 pages, 3 figures and 5 tables\n",
    "authors": [
      "Sheng Lin",
      "Wei Jiang",
      "Wei Wang",
      "Kaidi Xu",
      "Yanzhi Wang",
      "Shan Liu",
      "Songnan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08301"
  },
  {
    "id": "arXiv:2106.08307",
    "title": "Learning Incident Prediction Models Over Large Geographical Areas for  Emergency Response Systems",
    "abstract": "Principled decision making in emergency response management necessitates the\nuse of statistical models that predict the spatial-temporal likelihood of\nincident occurrence. These statistical models are then used for proactive\nstationing which allocates first responders across the spatial area in order to\nreduce overall response time. Traditional methods that simply aggregate past\nincidents over space and time fail to make useful short-term predictions when\nthe spatial region is large and focused on fine-grained spatial entities like\ninterstate highway networks. This is partially due to the sparsity of incidents\nwith respect to the area in consideration. Further, accidents are affected by\nseveral covariates, and collecting, cleaning, and managing multiple streams of\ndata from various sources is challenging for large spatial areas. In this\npaper, we highlight how this problem is being solved for the state of\nTennessee, a state in the USA with a total area of over 100,000 sq. km. Our\npipeline, based on a combination of synthetic resampling, non-spatial\nclustering, and learning from data can efficiently forecast the spatial and\ntemporal dynamics of accident occurrence, even under sparse conditions. In the\npaper, we describe our pipeline that uses data related to roadway geometry,\nweather, historical accidents, and real-time traffic congestion to aid accident\nforecasting. To understand how our forecasting model can affect allocation and\ndispatch, we improve upon a classical resource allocation approach.\nExperimental results show that our approach can significantly reduce response\ntimes in the field in comparison with current approaches followed by first\nresponders.",
    "descriptor": "",
    "authors": [
      "Sayyed Mohsen Vazirizade",
      "Ayan Mukhopadhyay",
      "Geoffrey Pettet",
      "Said El Said",
      "Hiba Baroud",
      "Abhishek Dubey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08307"
  },
  {
    "id": "arXiv:2106.08314",
    "title": "Causal Navigation by Continuous-time Neural Networks",
    "abstract": "Imitation learning enables high-fidelity, vision-based learning of policies\nwithin rich, photorealistic environments. However, such techniques often rely\non traditional discrete-time neural models and face difficulties in\ngeneralizing to domain shifts by failing to account for the causal\nrelationships between the agent and the environment. In this paper, we propose\na theoretical and experimental framework for learning causal representations\nusing continuous-time neural networks, specifically over their discrete-time\ncounterparts. We evaluate our method in the context of visual-control learning\nof drones over a series of complex tasks, ranging from short- and long-term\nnavigation, to chasing static and dynamic objects through photorealistic\nenvironments. Our results demonstrate that causal continuous-time deep models\ncan perform robust navigation tasks, where advanced recurrent models fail.\nThese models learn complex causal control representations directly from raw\nvisual inputs and scale to solve a variety of tasks using imitation learning.",
    "descriptor": "\nComments: 23 Pages\n",
    "authors": [
      "Charles Vorbach",
      "Ramin Hasani",
      "Alexander Amini",
      "Mathias Lechner",
      "Daniela Rus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08314"
  },
  {
    "id": "arXiv:2106.08316",
    "title": "Structure-preserving Nonlinear Filtering for Continuous and  Discontinuous Galerkin Spectral/hp Element Methods",
    "abstract": "Finite element simulations have been used to solve various partial\ndifferential equations (PDEs) that model physical, chemical, and biological\nphenomena. The resulting discretized solutions to PDEs often do not satisfy\nrequisite physical properties, such as positivity or monotonicity. Such invalid\nsolutions pose both modeling challenges, since the physical interpretation of\nsimulation results is not possible, and computational challenges, since such\nproperties may be required to advance the scheme. We, therefore, consider the\nproblem of computing solutions that preserve these structural solution\nproperties, which we enforce as additional constraints on the solution. We\nconsider in particular the class of convex constraints, which includes\npositivity and monotonicity. By embedding such constraints as a postprocessing\nconvex optimization procedure, we can compute solutions that satisfy general\ntypes of convex constraints. For certain types of constraints (including\npositivity and monotonicity), the optimization is a filter, i.e., a\nnorm-decreasing operation. We provide a variety of tests on one-dimensional\ntime-dependent PDEs that demonstrate the method's efficacy, and we empirically\nshow that rates of convergence are unaffected by the inclusion of the\nconstraints.",
    "descriptor": "\nComments: 20 pages, 10 figures\n",
    "authors": [
      "Vidhi Zala",
      "Robert M. Kirby",
      "Akil Narayan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.08316"
  },
  {
    "id": "arXiv:2106.08318",
    "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling",
    "abstract": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
    "descriptor": "\nComments: Accepted to CVPR 2021. arXiv admin note: text overlap with arXiv:2001.06232\n",
    "authors": [
      "Mateusz Malinowski",
      "Dimitrios Vytiniotis",
      "Grzegorz Swirszcz",
      "Viorica Patraucean",
      "Joao Carreira"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2106.08318"
  },
  {
    "id": "arXiv:2106.08322",
    "title": "Dynamic Head: Unifying Object Detection Heads with Attentions",
    "abstract": "The complex nature of combining localization and classification in object\ndetection has resulted in the flourished development of methods. Previous works\ntried to improve the performance in various object detection heads but failed\nto present a unified view. In this paper, we present a novel dynamic head\nframework to unify object detection heads with attentions. By coherently\ncombining multiple self-attention mechanisms between feature levels for\nscale-awareness, among spatial locations for spatial-awareness, and within\noutput channels for task-awareness, the proposed approach significantly\nimproves the representation ability of object detection heads without any\ncomputational overhead. Further experiments demonstrate that the effectiveness\nand efficiency of the proposed dynamic head on the COCO benchmark. With a\nstandard ResNeXt-101-DCN backbone, we largely improve the performance over\npopular object detectors and achieve a new state-of-the-art at 54.0 AP.\nFurthermore, with latest transformer backbone and extra data, we can push\ncurrent best COCO result to a new record at 60.6 AP. The code will be released\nat https://github.com/microsoft/DynamicHead.",
    "descriptor": "\nComments: CVPR 2021 camera ready with extensions\n",
    "authors": [
      "Xiyang Dai",
      "Yinpeng Chen",
      "Bin Xiao",
      "Dongdong Chen",
      "Mengchen Liu",
      "Lu Yuan",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08322"
  },
  {
    "id": "arXiv:2106.08323",
    "title": "Is this Harmful? Learning to Predict Harmfulness Ratings from Video",
    "abstract": "Automatically identifying harmful content in video is an important task with\na wide range of applications. However, due to the difficulty of collecting\nhigh-quality labels as well as demanding computational requirements, the task\nhas not had a satisfying general approach. Typically, only small subsets of the\nproblem are considered, such as identifying violent content. In cases where the\ngeneral problem is tackled, rough approximations and simplifications are made\nto deal with the lack of labels and computational complexity. In this work, we\nidentify and tackle the two main obstacles. First, we create a dataset of\napproximately 4000 video clips, annotated by professionals in the field.\nSecondly, we demonstrate that advances in video recognition enable training\nmodels on our dataset that consider the full context of the scene. We conduct\nan in-depth study on our modeling choices and find that we greatly benefit from\ncombining the visual and audio modality and that pretraining on large-scale\nvideo recognition datasets and class balanced sampling further improves\nperformance. We additionally perform a qualitative study that reveals the\nheavily multi-modal nature of our dataset. Our dataset will be made available\nupon publication.",
    "descriptor": "\nComments: 11 pages, 15 figures\n",
    "authors": [
      "Johan Edstedt",
      "Johan Karlsson",
      "Francisca Benavente",
      "Anette Novak",
      "Amanda Berg",
      "Michael Felsberg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08323"
  },
  {
    "id": "arXiv:2106.08325",
    "title": "Fuel-Economical Distributed Model Predictive Control for Heavy-Duty  Truck Platoon",
    "abstract": "This paper proposes a fuel-economical distributed model predictive control\ndesign (Eco-DMPC) for a homogenous heavy-duty truck platoon. The proposed\ncontrol strategy integrates a fuel-optimal control strategy for the leader\ntruck with a distributed formation control for the following trucks in the\nheavy-duty truck platoon. The fuel-optimal control strategy is implemented by a\nnonlinear model predictive control (NMPC) design with an instantaneous fuel\nconsumption model. The proposed fuel-optimal control strategy utilizes the\npreview information of the preceding traffic to achieve the fuel-economical\nspeed planning by avoiding energy-inefficient maneuvers, particularly under\ntransient traffic conditions. The distributed formation control is designed\nwith a serial distributed model predictive control (DMPC) strategy with\nguaranteed local and string stability. In the DMPC strategy, each following\ntruck acquires the future predicted state information of its predecessor\nthrough vehicle connectivity and then applies local optimal control to maintain\nconstant spacing. Simulation studies are conducted to investigate the fuel\neconomy performance of the proposed control strategy and to validate the local\nand string stability of the platoon under a realistic traffic scenario.\nCompared with a human-operated platoon and a benchmark formation-controlled\nplatoon, the proposed Eco-DMPC significantly improves the fuel economy and road\nutilization.",
    "descriptor": "\nComments: accepted for publication at the 24th IEEE Intelligent Transportation Systems Conference (ITSC 2021)\n",
    "authors": [
      "Mehmet Fatih Ozkan",
      "Yao Ma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.08325"
  },
  {
    "id": "arXiv:2006.10656",
    "title": "Canonical Construction of Quantum Oracles",
    "abstract": "Selecting a set of basis states is a common task in quantum computing, in\norder to increase and/or evaluate their probabilities. This is similar to\ndesigning WHERE clauses in classical database queries. Even though one can find\nheuristic methods to achieve this, it is desirable to automate the process. A\ncommon, but inefficient automation approach is to use oracles with classical\nevaluation of all the states at circuit design time. In this paper, we present\na novel, canonical way to produce a quantum oracle from an algebraic expression\n(in particular, an Ising model), that maps a set of selected states to the same\nvalue, coupled with a simple oracle that matches that particular value. We also\nintroduce a general form of the Grover iterate that standardizes this type of\noracle. We then apply this new methodology to particular cases of Ising\nHamiltonians that model the zero-sum subset problem and the computation of\nFibonacci numbers. In addition, this paper presents experimental results\nobtained on real quantum hardware, the new Honeywell computer based on\ntrapped-ion technology with quantum volume 64.",
    "descriptor": "\nComments: 11 pages, 19 figures\n",
    "authors": [
      "Austin Gilliam",
      "Marco Pistoia",
      "Constantin Gonciulea"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2006.10656"
  },
  {
    "id": "arXiv:2106.07644",
    "title": "A Continuized View on Nesterov Acceleration for Stochastic Gradient  Descent and Randomized Gossip",
    "abstract": "We introduce the continuized Nesterov acceleration, a close variant of\nNesterov acceleration whose variables are indexed by a continuous time\nparameter. The two variables continuously mix following a linear ordinary\ndifferential equation and take gradient steps at random times. This continuized\nvariant benefits from the best of the continuous and the discrete frameworks:\nas a continuous process, one can use differential calculus to analyze\nconvergence and obtain analytical expressions for the parameters; and a\ndiscretization of the continuized process can be computed exactly with\nconvergence rates similar to those of Nesterov original acceleration. We show\nthat the discretization has the same structure as Nesterov acceleration, but\nwith random parameters. We provide continuized Nesterov acceleration under\ndeterministic as well as stochastic gradients, with either additive or\nmultiplicative noise. Finally, using our continuized framework and expressing\nthe gossip averaging problem as the stochastic minimization of a certain energy\nfunction, we provide the first rigorous acceleration of asynchronous gossip\nalgorithms.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2102.06035\n",
    "authors": [
      "Mathieu Even",
      "Rapha\u00ebl Berthier",
      "Francis Bach",
      "Nicolas Flammarion",
      "Pierre Gaillard",
      "Hadrien Hendrikx",
      "Laurent Massouli\u00e9",
      "Adrien Taylor"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07644"
  },
  {
    "id": "arXiv:2106.07645",
    "title": "PhyMask: Robust Sensing of Brain Activity and Physiological Signals  During Sleep with an All-textile Eye Mask",
    "abstract": "Clinical-grade wearable sleep monitoring is a challenging problem since it\nrequires concurrently monitoring brain activity, eye movement, muscle activity,\ncardio-respiratory features and gross body movements. This requires multiple\nsensors to be worn at different locations as well as uncomfortable adhesives\nand discrete electronic components to be placed on the head. As a result,\nexisting wearables either compromise comfort or compromise accuracy in tracking\nsleep variables. We propose PhyMask, an all-textile sleep monitoring solution\nthat is practical and comfortable for continuous use and that acquires all\nsignals of interest to sleep solely using comfortable textile sensors placed on\nthe head. We show that PhyMask can be used to accurately measure sleep stages\nand advanced sleep markers such as spindles and k-complexes robustly in the\nreal-world setting. We validate PhyMask against polysomnography and show that\nit significantly outperforms two commercially-available sleep tracking\nwearables, Fitbit and Oura Ring.",
    "descriptor": "",
    "authors": [
      "Soha Rostaminia",
      "S. Zohreh Homayounfar",
      "Ali Kiaghadi",
      "Trisha L. Andrew",
      "Deepak Ganesan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.07645"
  },
  {
    "id": "arXiv:2106.07683",
    "title": "Extracting Global Dynamics of Loss Landscape in Deep Learning Models",
    "abstract": "Deep learning models evolve through training to learn the manifold in which\nthe data exists to satisfy an objective. It is well known that evolution leads\nto different final states which produce inconsistent predictions of the same\ntest data points. This calls for techniques to be able to empirically quantify\nthe difference in the trajectories and highlight problematic regions. While\nmuch focus is placed on discovering what models learn, the question of how a\nmodel learns is less studied beyond theoretical landscape characterizations and\nlocal geometric approximations near optimal conditions. Here, we present a\ntoolkit for the Dynamical Organization Of Deep Learning Loss Landscapes, or\nDOODL3. DOODL3 formulates the training of neural networks as a dynamical\nsystem, analyzes the learning process, and presents an interpretable global\nview of trajectories in the loss landscape. Our approach uses the coarseness of\ntopology to capture the granularity of geometry to mitigate against states of\ninstability or elongated training. Overall, our analysis presents an empirical\nframework to extract the global dynamics of a model and to use that information\nto guide the training of neural networks.",
    "descriptor": "\nComments: 9 pages, 3 figures, Supplementary\n",
    "authors": [
      "Mohammed Eslami",
      "Hamed Eramian",
      "Marcio Gameiro",
      "William Kalies",
      "Konstantin Mischaikow"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07683"
  },
  {
    "id": "arXiv:2106.07736",
    "title": "Unique sparse decomposition of low rank matrices",
    "abstract": "The problem of finding the unique low dimensional decomposition of a given\nmatrix has been a fundamental and recurrent problem in many areas. In this\npaper, we study the problem of seeking a unique decomposition of a low rank\nmatrix $Y\\in \\mathbb{R}^{p\\times n}$ that admits a sparse representation.\nSpecifically, we consider $Y = A X\\in \\mathbb{R}^{p\\times n}$ where the matrix\n$A\\in \\mathbb{R}^{p\\times r}$ has full column rank, with $r < \\min\\{n,p\\}$, and\nthe matrix $X\\in \\mathbb{R}^{r\\times n}$ is element-wise sparse. We prove that\nthis sparse decomposition of $Y$ can be uniquely identified, up to some\nintrinsic signed permutation. Our approach relies on solving a nonconvex\noptimization problem constrained over the unit sphere. Our geometric analysis\nfor the nonconvex optimization landscape shows that any {\\em strict} local\nsolution is close to the ground truth solution, and can be recovered by a\nsimple data-driven initialization followed with any second order descent\nalgorithm. At last, we corroborate these theoretical results with numerical\nexperiments.",
    "descriptor": "",
    "authors": [
      "Dian Jin",
      "Xin Bing",
      "Yuqian Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.07736"
  },
  {
    "id": "arXiv:2106.07759",
    "title": "Kaizen: Continuously improving teacher using Exponential Moving Average  for semi-supervised speech recognition",
    "abstract": "In this paper, we introduce the Kaizen framework that uses a continuously\nimproving teacher to generate pseudo-labels for semi-supervised training. The\nproposed approach uses a teacher model which is updated as the exponential\nmoving average of the student model parameters. This can be seen as a\ncontinuous version of the iterative pseudo-labeling approach for\nsemi-supervised training. It is applicable for different training criteria, and\nin this paper we demonstrate it for frame-level hybrid hidden Markov model -\ndeep neural network (HMM-DNN) models and sequence-level connectionist temporal\nclassification (CTC) based models. The proposed approach shows more than 10%\nword error rate (WER) reduction over standard teacher-student training and more\nthan 50\\% relative WER reduction over 10 hour supervised baseline when using\nlarge scale realistic unsupervised public videos in UK English and Italian\nlanguages.",
    "descriptor": "\nComments: 4 figures, 7 pages; fixed author list going out of margin\n",
    "authors": [
      "Vimal Manohar",
      "Tatiana Likhomanenko",
      "Qiantong Xu",
      "Wei-Ning Hsu",
      "Ronan Collobert",
      "Yatharth Saraf",
      "Geoffrey Zweig",
      "Abdelrahman Mohamed"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07759"
  },
  {
    "id": "arXiv:2106.07761",
    "title": "Linear-Time Probabilistic Solutions of Boundary Value Problems",
    "abstract": "We propose a fast algorithm for the probabilistic solution of boundary value\nproblems (BVPs), which are ordinary differential equations subject to boundary\nconditions. In contrast to previous work, we introduce a Gauss--Markov prior\nand tailor it specifically to BVPs, which allows computing a posterior\ndistribution over the solution in linear time, at a quality and cost comparable\nto that of well-established, non-probabilistic methods. Our model further\ndelivers uncertainty quantification, mesh refinement, and hyperparameter\nadaptation. We demonstrate how these practical considerations positively impact\nthe efficiency of the scheme. Altogether, this results in a practically usable\nprobabilistic BVP solver that is (in contrast to non-probabilistic algorithms)\nnatively compatible with other parts of the statistical modelling tool-chain.",
    "descriptor": "",
    "authors": [
      "Nicholas Kr\u00e4mer",
      "Philipp Hennig"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.07761"
  },
  {
    "id": "arXiv:2106.07795",
    "title": "Interpretation of Plug-and-Play (PnP) algorithms from a different angle",
    "abstract": "It's well-known that inverse problems are ill-posed and to solve them\nmeaningfully one has to employ regularization methods. Traditionally, the most\npopular regularization approaches are Variational-type approaches, i.e.,\npenalized/constrained functional minimization. In recent years, the classical\nregularization approaches have been replaced by the so-called plug-and-play\n(PnP) algorithms, which copies the proximal gradient minimization processes,\nsuch as ADMM or FISTA, but with any general denoiser. However, unlike the\ntraditional proximal gradient methods, the theoretical analysis and convergence\nresults have been insufficient for these PnP-algorithms. Hence, the results\nfrom these algorithms, though empirically outstanding, are not well-defined, in\nthe sense of, being a minimizer of a Variational problem. In this paper, we\naddress this question of ``well-definedness\", but from a different angle. We\nexplain these algorithms from the viewpoint of a semi-iterative regularization\nmethod. In addition, we expand the family of regularized solutions,\ncorresponding to the classical semi-iterative methods, to further generalize\nthe explainability of these algorithms, as well as, enhance the recovery\nprocess. We conclude with several numerical results which validate the\ndeveloped theories and reflect the improvements over the traditional\nPnP-algorithms, such as ADMM-PnP and FISTA-PnP.",
    "descriptor": "\nComments: Any comments/remarks are welcomed\n",
    "authors": [
      "Abinash Nayak"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.07795"
  },
  {
    "id": "arXiv:2106.07801",
    "title": "Non-Autoregressive Electron Redistribution Modeling for Reaction  Prediction",
    "abstract": "Reliably predicting the products of chemical reactions presents a fundamental\nchallenge in synthetic chemistry. Existing machine learning approaches\ntypically produce a reaction product by sequentially forming its subparts or\nintermediate molecules. Such autoregressive methods, however, not only require\na pre-defined order for the incremental construction but preclude the use of\nparallel decoding for efficient computation. To address these issues, we devise\na non-autoregressive learning paradigm that predicts reaction in one shot.\nLeveraging the fact that chemical reactions can be described as a\nredistribution of electrons in molecules, we formulate a reaction as an\narbitrary electron flow and predict it with a novel multi-pointer decoding\nnetwork. Experiments on the USPTO-MIT dataset show that our approach has\nestablished a new state-of-the-art top-1 accuracy and achieves at least 27\ntimes inference speedup over the state-of-the-art methods. Also, our\npredictions are easier for chemists to interpret owing to predicting the\nelectron flows.",
    "descriptor": "",
    "authors": [
      "Hangrui Bi",
      "Hengyi Wang",
      "Chence Shi",
      "Connor Coley",
      "Jian Tang",
      "Hongyu Guo"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07801"
  },
  {
    "id": "arXiv:2106.07802",
    "title": "GeoMol: Torsional Geometric Generation of Molecular 3D Conformer  Ensembles",
    "abstract": "Prediction of a molecule's 3D conformer ensemble from the molecular graph\nholds a key role in areas of cheminformatics and drug discovery. Existing\ngenerative models have several drawbacks including lack of modeling important\nmolecular geometry elements (e.g. torsion angles), separate optimization stages\nprone to error accumulation, and the need for structure fine-tuning based on\napproximate classical force-fields or computationally expensive methods such as\nmetadynamics with approximate quantum mechanics calculations at each geometry.\nWe propose GeoMol--an end-to-end, non-autoregressive and SE(3)-invariant\nmachine learning approach to generate distributions of low-energy molecular 3D\nconformers. Leveraging the power of message passing neural networks (MPNNs) to\ncapture local and global graph information, we predict local atomic 3D\nstructures and torsion angles, avoiding unnecessary over-parameterization of\nthe geometric degrees of freedom (e.g. one angle per non-terminal bond). Such\nlocal predictions suffice both for the training loss computation, as well as\nfor the full deterministic conformer assembly (at test time). We devise a\nnon-adversarial optimal transport based loss function to promote diverse\nconformer generation. GeoMol predominantly outperforms popular open-source,\ncommercial, or state-of-the-art machine learning (ML) models, while achieving\nsignificant speed-ups. We expect such differentiable 3D structure generators to\nsignificantly impact molecular modeling and related applications.",
    "descriptor": "",
    "authors": [
      "Octavian-Eugen Ganea",
      "Lagnajit Pattanaik",
      "Connor W. Coley",
      "Regina Barzilay",
      "Klavs F. Jensen",
      "William H. Green",
      "Tommi S. Jaakkola"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07802"
  },
  {
    "id": "arXiv:2106.07806",
    "title": "Highdicom: A Python library for standardized encoding of image  annotations and machine learning model outputs in pathology and radiology",
    "abstract": "Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.",
    "descriptor": "",
    "authors": [
      "Christopher P. Bridge",
      "Chris Gorman",
      "Steven Pieper",
      "Sean W. Doyle",
      "Jochen K. Lennerz",
      "Jayashree Kalpathy-Cramer",
      "David A. Clunie",
      "Andriy Y. Fedorov",
      "Markus D. Herrmann"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07806"
  },
  {
    "id": "arXiv:2106.07875",
    "title": "S-LIME: Stabilized-LIME for Model Explanation",
    "abstract": "An increasing number of machine learning models have been deployed in domains\nwith high stakes such as finance and healthcare. Despite their superior\nperformances, many models are black boxes in nature which are hard to explain.\nThere are growing efforts for researchers to develop methods to interpret these\nblack-box models. Post hoc explanations based on perturbations, such as LIME,\nare widely used approaches to interpret a machine learning model after it has\nbeen built. This class of methods has been shown to exhibit large instability,\nposing serious challenges to the effectiveness of the method itself and harming\nuser trust. In this paper, we propose S-LIME, which utilizes a hypothesis\ntesting framework based on central limit theorem for determining the number of\nperturbation points needed to guarantee stability of the resulting explanation.\nExperiments on both simulated and real world data sets are provided to\ndemonstrate the effectiveness of our method.",
    "descriptor": "\nComments: In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21), August 14--18, 2021, Virtual Event, Singapore\n",
    "authors": [
      "Zhengze Zhou",
      "Giles Hooker",
      "Fei Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07875"
  },
  {
    "id": "arXiv:2106.07879",
    "title": "A Lightweight ReLU-Based Feature Fusion for Aerial Scene Classification",
    "abstract": "In this paper, we propose a transfer-learning based model construction\ntechnique for the aerial scene classification problem. The core of our\ntechnique is a layer selection strategy, named ReLU-Based Feature Fusion\n(RBFF), that extracts feature maps from a pretrained CNN-based single-object\nimage classification model, namely MobileNetV2, and constructs a model for the\naerial scene classification task. RBFF stacks features extracted from the batch\nnormalization layer of a few selected blocks of MobileNetV2, where the\ncandidate blocks are selected based on the characteristics of the ReLU\nactivation layers present in those blocks. The feature vector is then\ncompressed into a low-dimensional feature space using dimension reduction\nalgorithms on which we train a low-cost SVM classifier for the classification\nof the aerial images. We validate our choice of selected features based on the\nsignificance of the extracted features with respect to our classification\npipeline. RBFF remarkably does not involve any training of the base CNN model\nexcept for a few parameters for the classifier, which makes the technique very\ncost-effective for practical deployments. The constructed model despite being\nlightweight outperforms several recently proposed models in terms of accuracy\nfor a number of aerial scene datasets.",
    "descriptor": "\nComments: To be presented in IEEE ICIP'21\n",
    "authors": [
      "Md Adnan Arefeen",
      "Sumaiya Tabassum Nimi",
      "Md Yusuf Sarwar Uddin",
      "Zhu Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07879"
  },
  {
    "id": "arXiv:2106.07889",
    "title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram  Discriminators for High-Fidelity Waveform Generation",
    "abstract": "Most neural vocoders employ band-limited mel-spectrograms to generate\nwaveforms. If full-band spectral features are used as the input, the vocoder\ncan be provided with as much acoustic information as possible. However, in some\nmodels employing full-band mel-spectrograms, an over-smoothing problem occurs\nas part of which non-sharp spectrograms are generated. To address this problem,\nwe propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms\nin real time. Inspired by works in the field of voice activity detection, we\nadded a multi-resolution spectrogram discriminator that employs multiple linear\nspectrogram magnitudes computed using various parameter sets. Using full-band\nmel-spectrograms as input, we expect to generate high-resolution signals by\nadding a discriminator that employs spectrograms of multiple resolutions as the\ninput. In an evaluation on a dataset containing information on hundreds of\nspeakers, UnivNet obtained the best objective and subjective results among\ncompeting models for both seen and unseen speakers. These results, including\nthe best subjective score for text-to-speech, demonstrate the potential for\nfast adaptation to new speakers without a need for training from scratch.",
    "descriptor": "\nComments: Accepted to INTERSPEECH 2021\n",
    "authors": [
      "Won Jang",
      "Dan Lim",
      "Jaesam Yoon",
      "Bongwan Kim",
      "Juntae Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.07889"
  },
  {
    "id": "arXiv:2106.07890",
    "title": "An enriched category theory of language: from syntax to semantics",
    "abstract": "Given a piece of text, the ability to generate a coherent extension of it\nimplies some sophistication, including a knowledge of grammar and semantics. In\nthis paper, we propose a mathematical framework for passing from probability\ndistributions on extensions of given texts to an enriched category containing\nsemantic information. Roughly speaking, we model probability distributions on\ntexts as a category enriched over the unit interval. Objects of this category\nare expressions in language and hom objects are conditional probabilities that\none expression is an extension of another. This category is syntactical: it\ndescribes what goes with what. We then pass to the enriched category of unit\ninterval-valued copresheaves on this syntactical category to find semantic\ninformation.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Tai-Danae Bradley",
      "John Terilla",
      "Yiannis Vlassopoulos"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07890"
  },
  {
    "id": "arXiv:2106.07898",
    "title": "Divergence Frontiers for Generative Models: Sample Complexity,  Quantization Level, and Frontier Integral",
    "abstract": "The spectacular success of deep generative models calls for quantitative\ntools to measure their statistical performance. Divergence frontiers have\nrecently been proposed as an evaluation framework for generative models, due to\ntheir ability to measure the quality-diversity trade-off inherent to deep\ngenerative modeling. However, the statistical behavior of divergence frontiers\nestimated from data remains unknown to this day. In this paper, we establish\nnon-asymptotic bounds on the sample complexity of the plug-in estimator of\ndivergence frontiers. Along the way, we introduce a novel integral summary of\ndivergence frontiers. We derive the corresponding non-asymptotic bounds and\ndiscuss the choice of the quantization level by balancing the two types of\napproximation errors arisen from its computation. We also augment the\ndivergence frontier framework by investigating the statistical performance of\nsmoothed distribution estimators such as the Good-Turing estimator. We\nillustrate the theoretical results with numerical examples from natural\nlanguage processing and computer vision.",
    "descriptor": "",
    "authors": [
      "Lang Liu",
      "Krishna Pillutla",
      "Sean Welleck",
      "Sewoong Oh",
      "Yejin Choi",
      "Zaid Harchaoui"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07898"
  },
  {
    "id": "arXiv:2106.07910",
    "title": "Wavelength-based Attributed Deep Neural Network for Underwater Image  Restoration",
    "abstract": "Underwater images, in general, suffer from low contrast and high color\ndistortions due to the non-uniform attenuation of the light as it propagates\nthrough the water. In addition, the degree of attenuation varies with the\nwavelength resulting in the asymmetric traversing of colors. Despite the\nprolific works for underwater image restoration (UIR) using deep learning, the\nabove asymmetricity has not been addressed in the respective network\nengineering. As the first novelty, this paper shows that attributing the right\nreceptive field size (context) based on the traversing range of the color\nchannel may lead to a substantial performance gain for the task of UIR.\nFurther, it is important to suppress the irrelevant multi-contextual features\nand increase the representational power of the model. Therefore, as a second\nnovelty, we have incorporated an attentive skip mechanism to adaptively refine\nthe learned multi-contextual features. The proposed framework, called Deep\nWaveNet, is optimized using the traditional pixel-wise and feature-based cost\nfunctions. An extensive set of experiments have been carried out to show the\nefficacy of the proposed scheme over existing best-published literature on\nbenchmark datasets. More importantly, we have demonstrated a comprehensive\nvalidation of enhanced images across various high-level vision tasks, e.g.,\nunderwater image semantic segmentation, and diver's 2D pose estimation. A\nsample video to exhibit our real-world performance is available at\n\\url{https://www.youtube.com/watch?v=8qtuegBdfac}.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Prasen Kumar Sharma",
      "Ira Bisht",
      "Arijit Sur"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.07910"
  },
  {
    "id": "arXiv:2106.07919",
    "title": "A stochastic metapopulation state-space approach to modeling and  estimating Covid-19 spread",
    "abstract": "Mathematical models are widely recognized as an important tool for analyzing\nand understanding the dynamics of infectious disease outbreaks, predict their\nfuture trends, and evaluate public health intervention measures for disease\ncontrol and elimination. We propose a novel stochastic metapopulation\nstate-space model for COVID-19 transmission, based on a discrete-time\nspatio-temporal susceptible/exposed/infected/recovered/deceased (SEIRD) model.\nThe proposed framework allows the hidden SEIRD states and unknown transmission\nparameters to be estimated from noisy, incomplete time series of reported\nepidemiological data, by application of unscented Kalman filtering (UKF),\nmaximum-likelihood adaptive filtering, and metaheuristic optimization.\nExperiments using both synthetic data and real data from the Fall 2020 Covid-19\nwave in the state of Texas demonstrate the effectiveness of the proposed model.",
    "descriptor": "\nComments: 17 pages, 5 figures\n",
    "authors": [
      "Yukun Tan",
      "Durward Cator III",
      "Martial Ndeffo-Mbah",
      "Ulisses Braga-Neto"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Social and Information Networks (cs.SI)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2106.07919"
  },
  {
    "id": "arXiv:2106.07939",
    "title": "Attention-based distributed speech enhancement for unconstrained  microphone arrays with varying number of nodes",
    "abstract": "Speech enhancement promises higher efficiency in ad-hoc microphone arrays\nthan in constrained microphone arrays thanks to the wide spatial coverage of\nthe devices in the acoustic scene. However, speech enhancement in ad-hoc\nmicrophone arrays still raises many challenges. In particular, the algorithms\nshould be able to handle a variable number of microphones, as some devices in\nthe array might appear or disappear. In this paper, we propose a solution that\ncan efficiently process the spatial information captured by the different\ndevices of the microphone array, while being robust to a link failure. To do\nthis, we use an attention mechanism in order to put more weight on the relevant\nsignals sent throughout the array and to neglect the redundant or empty\nchannels.",
    "descriptor": "",
    "authors": [
      "Nicolas Furnon",
      "Romain Serizel",
      "Slim Essid",
      "Irina Illina"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07939"
  },
  {
    "id": "arXiv:2106.07953",
    "title": "Learning to Compensate: A Deep Neural Network Framework for 5G Power  Amplifier Compensation",
    "abstract": "Owing to the complicated characteristics of 5G communication system,\ndesigning RF components through mathematical modeling becomes a challenging\nobstacle. Moreover, such mathematical models need numerous manual adjustments\nfor various specification requirements. In this paper, we present a\nlearning-based framework to model and compensate Power Amplifiers (PAs) in 5G\ncommunication. In the proposed framework, Deep Neural Networks (DNNs) are used\nto learn the characteristics of the PAs, while, correspondent Digital\nPre-Distortions (DPDs) are also learned to compensate for the nonlinear and\nmemory effects of PAs. On top of the framework, we further propose two\nfrequency domain losses to guide the learning process to better optimize the\ntarget, compared to naive time domain Mean Square Error (MSE). The proposed\nframework serves as a drop-in replacement for the conventional approach. The\nproposed approach achieves an average of 56.7% reduction of nonlinear and\nmemory effects, which converts to an average of 16.3% improvement over a\ncarefully-designed mathematical model, and even reaches 34% enhancement in\nsevere distortion scenarios.",
    "descriptor": "\nComments: IEEE International Conference on Communications (ICC) 2021\n",
    "authors": [
      "Po-Yu Chen",
      "Hao Chen",
      "Yi-Min Tsai",
      "Hsien-Kai Kuo",
      "Hantao Huang",
      "Hsin-Hung Chen",
      "Sheng-Hong Yan",
      "Wei-Lun Ou",
      "Chia-Ming Cheng"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07953"
  },
  {
    "id": "arXiv:2106.07963",
    "title": "Capabilities of Deep Learning Models on Learning Physical Relationships:  Case of Rainfall-Runoff Modeling with LSTM",
    "abstract": "This study investigates the relationships which deep learning methods can\nidentify between the input and output data. As a case study, rainfall-runoff\nmodeling in a snow-dominated watershed by means of a long- and short-term\nmemory (LSTM) network is selected. Daily precipitation and mean air temperature\nwere used as model input to estimate daily flow discharge. After model training\nand verification, two experimental simulations were conducted with hypothetical\ninputs instead of observed meteorological data to clarify the response of the\ntrained model to the inputs. The first numerical experiment showed that even\nwithout input precipitation, the trained model generated flow discharge,\nparticularly winter low flow and high flow during the snow-melting period. The\neffects of warmer and colder conditions on the flow discharge were also\nreplicated by the trained model without precipitation. Additionally, the model\nreflected only 17-39% of the total precipitation mass during the snow\naccumulation period in the total annual flow discharge, revealing a strong lack\nof water mass conservation. The results of this study indicated that a deep\nlearning method may not properly learn the explicit physical relationships\nbetween input and target variables, although they are still capable of\nmaintaining strong goodness-of-fit results.",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Kazuki Yokoo",
      "Kei Ishida",
      "Ali Ercan",
      "Tongbi Tu",
      "Takeyoshi Nagasato",
      "Masato Kiyama",
      "Motoki Amagasaki"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07963"
  },
  {
    "id": "arXiv:2106.07970",
    "title": "Jamming Detection With Subcarrier Blanking for 5G and Beyond in Industry  4.0 Scenarios",
    "abstract": "Security attacks at the physical layer, in the form of radio jamming for\ndenial of service, are an increasing threat in the Industry 4.0 scenarios. In\nthis paper, we consider the problem of jamming detection in 5G-and-beyond\ncommunication systems and propose a defense mechanism based on pseudo-random\nblanking of subcarriers with orthogonal frequency division multiplexing (OFDM).\nWe then design a detector by applying the generalized likelihood ratio test\n(GLRT) on those subcarriers. We finally evaluate the performance of the\nproposed technique against a smart jammer, which is pursuing one of the\nfollowing objectives: maximize stealthiness, minimize spectral efficiency (SE)\nwith mobile broadband (MBB) type of traffic, and maximize block error rate\n(BLER) with ultra-reliable low-latency communications (URLLC). Numerical\nresults show that a smart jammer a) needs to compromise between missed\ndetection (MD) probability and SE reduction with MBB and b) can achieve low\ndetectability and high system performance degradation with URLLC only if it has\nsufficiently high power.",
    "descriptor": "\nComments: Accepted at the IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), Sep. 2021\n",
    "authors": [
      "Leonardo Chiarello",
      "Paolo Baracca",
      "Karthik Upadhya",
      "Saeed R. Khosravirad",
      "Thorsten Wild"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.07970"
  },
  {
    "id": "arXiv:2106.07972",
    "title": "SRIB Submission to Interspeech 2021 DiCOVA Challenge",
    "abstract": "The COVID-19 pandemic has resulted in more than 125 million infections and\nmore than 2.7 million casualties. In this paper, we attempt to classify covid\nvs non-covid cough sounds using signal processing and deep learning methods.\nAir turbulence, the vibration of tissues, movement of fluid through airways,\nopening, and closure of glottis are some of the causes for the production of\nthe acoustic sound signals during cough. Does the COVID-19 alter the acoustic\ncharacteristics of breath, cough, and speech sounds produced through the\nrespiratory system? This is an open question waiting for answers. In this\npaper, we incorporated novel data augmentation methods for cough sound\naugmentation and multiple deep neural network architectures and methods along\nwith handcrafted features. Our proposed system gives 14% absolute improvement\nin area under the curve (AUC). The proposed system is developed as part of\nInterspeech 2021 special sessions and challenges viz. diagnosing of COVID-19\nusing acoustics (DiCOVA). Our proposed method secured the 5th position on the\nleaderboard among 29 participants.",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Vishwanath Pratap Singh",
      "Shashi Kumar",
      "Ravi Shekhar Jha",
      "Abhishek Pandey"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.07972"
  },
  {
    "id": "arXiv:2106.07991",
    "title": "A Value-Function-based Interior-point Method for Non-convex Bi-level  Optimization",
    "abstract": "Bi-level optimization model is able to capture a wide range of complex\nlearning tasks with practical interest. Due to the witnessed efficiency in\nsolving bi-level programs, gradient-based methods have gained popularity in the\nmachine learning community. In this work, we propose a new gradient-based\nsolution scheme, namely, the Bi-level Value-Function-based Interior-point\nMethod (BVFIM). Following the main idea of the log-barrier interior-point\nscheme, we penalize the regularized value function of the lower level problem\ninto the upper level objective. By further solving a sequence of differentiable\nunconstrained approximation problems, we consequently derive a sequential\nprogramming scheme. The numerical advantage of our scheme relies on the fact\nthat, when gradient methods are applied to solve the approximation problem, we\nsuccessfully avoid computing any expensive Hessian-vector or Jacobian-vector\nproduct. We prove the convergence without requiring any convexity assumption on\neither the upper level or the lower level objective. Experiments demonstrate\nthe efficiency of the proposed BVFIM on non-convex bi-level problems.",
    "descriptor": "\nComments: Accepted at ICML 2021\n",
    "authors": [
      "Risheng Liu",
      "Xuan Liu",
      "Xiaoming Yuan",
      "Shangzhi Zeng",
      "Jin Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07991"
  },
  {
    "id": "arXiv:2106.07994",
    "title": "Multi-channel Opus compression for far-field automatic speech  recognition with a fixed bitrate budget",
    "abstract": "Automatic speech recognition (ASR) in the cloud allows the use of larger\nmodels and more powerful multi-channel signal processing front-ends compared to\non-device processing. However, it also adds an inherent latency due to the\ntransmission of the audio signal, especially when transmitting multiple\nchannels of a microphone array. One way to reduce the network bandwidth\nrequirements is client-side compression with a lossy codec such as Opus.\nHowever, this compression can have a detrimental effect especially on\nmulti-channel ASR front-ends, due to the distortion and loss of spatial\ninformation introduced by the codec. In this publication, we propose an\nimproved approach for the compression of microphone array signals based on\nOpus, using a modified joint channel coding approach and additionally\nintroducing a multi-channel spatial decorrelating transform to reduce\nredundancy in the transmission. We illustrate the effect of the proposed\napproach on the spatial information retained in multi-channel signals after\ncompression, and evaluate the performance on far-field ASR with a multi-channel\nbeamforming front-end. We demonstrate that our approach can lead to a 37.5 %\nbitrate reduction or a 5.1 % relative word error rate reduction for a fixed\nbitrate budget in a seven channel setup.",
    "descriptor": "\nComments: Accepted at Interspeech 2021\n",
    "authors": [
      "Lukas Drude",
      "Jahn Heymann",
      "Andreas Schwarz",
      "Jean-Marc Valin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.07994"
  },
  {
    "id": "arXiv:2106.08008",
    "title": "Towards Long-term Non-invasive Monitoring for Epilepsy via Wearable EEG  Devices",
    "abstract": "We present the implementation of seizure detection algorithms based on a\nminimal number of EEG channels on a parallel ultra-low-power embedded platform.\nThe analyses are based on the CHB-MIT dataset, and include explorations of\ndifferent classification approaches (Support Vector Machines, Random Forest,\nExtra Trees, AdaBoost) and different pre/post-processing techniques to maximize\nsensitivity while guaranteeing no false alarms. We analyze global and\nsubject-specific approaches, considering all 23-electrodes or only 4 temporal\nchannels. For 8s window size and subject-specific approach, we report zero\nfalse positives and 100% sensitivity. These algorithms are parallelized and\noptimized for a parallel ultra-low power (PULP) platform, enabling 300h of\ncontinuous monitoring on a 300 mAh battery, in a wearable form factor and power\nbudget. These results pave the way for the implementation of affordable,\nwearable, long-term epilepsy monitoring solutions with low false-positive rates\nand high sensitivity, meeting both patient and caregiver requirements.",
    "descriptor": "\nComments: 4 pages, 3 figures, 2 tables, pre-print\n",
    "authors": [
      "Thorir Mar Ingolfsson",
      "Andrea Cossettini",
      "Xiaying Wang",
      "Enrico Tabanelli",
      "Guiseppe Tagliavini",
      "Philippe Ryvlin",
      "Luca Benini"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08008"
  },
  {
    "id": "arXiv:2106.08010",
    "title": "Dynamical representations of constrained multicomponent nonlinear  Schr\u00f6dinger equations in arbitrary dimensions",
    "abstract": "We present new approaches for solving constrained multicomponent nonlinear\nSchr\\\"odinger equations in arbitrary dimensions. The idea is to introduce an\nartificial time and solve an extended damped second order dynamic system whose\nstationary solution is the solution to the time-independent nonlinear\nSchr\\\"odinger equation. Constraints are often considered by projection onto the\nconstraint set, here we include them explicitly into the dynamical system. We\nshow the applicability and efficiency of the methods on examples of relevance\nin modern physics applications.",
    "descriptor": "\nComments: Note that Eq. (74) here corrects Eq. (74) in the published version\n",
    "authors": [
      "M Gulliksson",
      "M Ogren"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Quantum Gases (cond-mat.quant-gas)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2106.08010"
  },
  {
    "id": "arXiv:2106.08048",
    "title": "Epidemic modelling of multiple virus strains:a case study of SARS-CoV-2  B.1.1.7 in Moscow",
    "abstract": "During a long-running pandemic a pathogen can mutate, producing new strains\nwith different epidemiological parameters. Existing approaches to epidemic\nmodelling only consider one virus strain. We have developed a modified SEIR\nmodel to simulate multiple virus strains within the same population. As a case\nstudy, we investigate the potential effects of SARS-CoV-2 strain B.1.1.7 on the\ncity of Moscow. Our analysis indicates a high risk of a new wave of infections\nin September-October 2021 with up to 35 000 daily infections at peak. We\nopen-source our code and data.",
    "descriptor": "",
    "authors": [
      "Boris Tseytlin",
      "Ilya Makarov"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2106.08048"
  },
  {
    "id": "arXiv:2106.08084",
    "title": "Asymptotic analysis of domain decomposition for optimal transport",
    "abstract": "Large optimal transport problems can be approached via domain decomposition,\ni.e. by iteratively solving small partial problems independently and in\nparallel. Convergence to the global minimizers under suitable assumptions has\nbeen shown in the unregularized and entropy regularized setting and its\ncomputational efficiency has been demonstrated experimentally. An accurate\ntheoretical understanding of its convergence speed in geometric settings is\nstill lacking. In this article we work towards such an understanding by\nderiving, via $\\Gamma$-convergence, an asymptotic description of the algorithm\nin the limit of infinitely fine partition cells. The limit trajectory of\ncouplings is described by a continuity equation on the product space where the\nmomentum is purely horizontal and driven by the gradient of the cost function.\nConvergence hinges on a regularity assumption that we investigate in detail.\nGlobal optimality of the limit trajectories remains an interesting open\nproblem, even when global optimality is established at finite scales. Our\nresult provides insights about the efficiency of the domain decomposition\nalgorithm at finite resolutions and in combination with coarse-to-fine schemes.",
    "descriptor": "",
    "authors": [
      "Mauro Bonafini",
      "Ismael Medina",
      "Bernhard Schmitzer"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.08084"
  },
  {
    "id": "arXiv:2106.08086",
    "title": "Decomposition of Global Feature Importance into Direct and Associative  Components (DEDACT)",
    "abstract": "Global model-agnostic feature importance measures either quantify whether\nfeatures are directly used for a model's predictions (direct importance) or\nwhether they contain prediction-relevant information (associative importance).\nDirect importance provides causal insight into the model's mechanism, yet it\nfails to expose the leakage of information from associated but not directly\nused variables. In contrast, associative importance exposes information leakage\nbut does not provide causal insight into the model's mechanism. We introduce\nDEDACT - a framework to decompose well-established direct and associative\nimportance measures into their respective associative and direct components.\nDEDACT provides insight into both the sources of prediction-relevant\ninformation in the data and the direct and indirect feature pathways by which\nthe information enters the model. We demonstrate the method's usefulness on\nsimulated examples.",
    "descriptor": "",
    "authors": [
      "Gunnar K\u00f6nig",
      "Timo Freiesleben",
      "Bernd Bischl",
      "Giuseppe Casalicchio",
      "Moritz Grosse-Wentrup"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08086"
  },
  {
    "id": "arXiv:2106.08094",
    "title": "Cine-MRI detection of abdominal adhesions with spatio-temporal deep  learning",
    "abstract": "Adhesions are an important cause of chronic pain following abdominal surgery.\nRecent developments in abdominal cine-MRI have enabled the non-invasive\ndiagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of\nsliding motion during movement. Diagnosis and mapping of adhesions improves the\nmanagement of patients with pain. Detection of abdominal adhesions on cine-MRI\nis challenging from both a radiological and deep learning perspective. We focus\non classifying presence or absence of adhesions in sagittal abdominal cine-MRI\nseries. We experimented with spatio-temporal deep learning architectures\ncentered around a ConvGRU architecture. A hybrid architecture comprising a\nResNet followed by a ConvGRU model allows to classify a whole time-series.\nCompared to a stand-alone ResNet with a two time-point (inspiration/expiration)\ninput, we show an increase in classification performance (AUROC) from 0.74 to\n0.83 ($p<0.05$). Our full temporal classification approach adds only a small\namount (5%) of parameters to the entire architecture, which may be useful for\nother medical imaging problems with a temporal dimension.",
    "descriptor": "\nComments: Accepted at MIDL 2021 as short paper\n",
    "authors": [
      "Bram de Wilde",
      "Richard P. G. ten Broek",
      "Henkjan Huisman"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08094"
  },
  {
    "id": "arXiv:2106.08105",
    "title": "Employing an Adjusted Stability Measure for Multi-Criteria Model Fitting  on Data Sets with Similar Features",
    "abstract": "Fitting models with high predictive accuracy that include all relevant but no\nirrelevant or redundant features is a challenging task on data sets with\nsimilar (e.g. highly correlated) features. We propose the approach of tuning\nthe hyperparameters of a predictive model in a multi-criteria fashion with\nrespect to predictive accuracy and feature selection stability. We evaluate\nthis approach based on both simulated and real data sets and we compare it to\nthe standard approach of single-criteria tuning of the hyperparameters as well\nas to the state-of-the-art technique \"stability selection\". We conclude that\nour approach achieves the same or better predictive performance compared to the\ntwo established approaches. Considering the stability during tuning does not\ndecrease the predictive accuracy of the resulting models. Our approach succeeds\nat selecting the relevant features while avoiding irrelevant or redundant\nfeatures. The single-criteria approach fails at avoiding irrelevant or\nredundant features and the stability selection approach fails at selecting\nenough relevant features for achieving acceptable predictive accuracy. For our\napproach, for data sets with many similar features, the feature selection\nstability must be evaluated with an adjusted stability measure, that is, a\nmeasure that considers similarities between features. For data sets with only\nfew similar features, an unadjusted stability measure suffices and is faster to\ncompute.",
    "descriptor": "",
    "authors": [
      "Andrea Bommert",
      "J\u00f6rg Rahnenf\u00fchrer",
      "Michel Lang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08105"
  },
  {
    "id": "arXiv:2106.08107",
    "title": "ResDepth: A Deep Prior For 3D Reconstruction From High-resolution  Satellite Images",
    "abstract": "Modern optical satellite sensors enable high-resolution stereo reconstruction\nfrom space. But the challenging imaging conditions when observing the Earth\nfrom space push stereo matching to its limits. In practice, the resulting\ndigital surface models (DSMs) are fairly noisy and often do not attain the\naccuracy needed for high-resolution applications such as 3D city modeling.\nArguably, stereo correspondence based on low-level image similarity is\ninsufficient and should be complemented with a-priori knowledge about the\nexpected surface geometry beyond basic local smoothness. To that end, we\nintroduce ResDepth, a convolutional neural network that learns such an\nexpressive geometric prior from example data. ResDepth refines an initial, raw\nstereo DSM while conditioning the refinement on the images. I.e., it acts as a\nsmart, learned post-processing filter and can seamlessly complement any stereo\nmatching pipeline. In a series of experiments, we find that the proposed method\nconsistently improves stereo DSMs both quantitatively and qualitatively. We\nshow that the prior encoded in the network weights captures meaningful\ngeometric characteristics of urban design, which also generalize across\ndifferent districts and even from one city to another. Moreover, we demonstrate\nthat, by training on a variety of stereo pairs, ResDepth can acquire a\nsufficient degree of invariance against variations in imaging conditions and\nacquisition geometry.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Corinne Stucker",
      "Konrad Schindler"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08107"
  },
  {
    "id": "arXiv:2106.08119",
    "title": "When the positive semidefinite relaxation guarantees a solution to a  system of real quadratic equations",
    "abstract": "By solving a positive semidefinite program, one can reduce a system of real\nquadratic equations to a system of the type $q_i(x)=\\alpha_i$, $i=1, \\ldots,\nm$, where $q_i: {\\Bbb R}^n \\longrightarrow {\\Bbb R}$ are quadratic forms and\n$\\alpha_i=\\operatorname{trace} q_i$. We prove a sufficient condition for the\nlatter system to have a solution $x \\in {\\Bbb R}^n$: assuming that the operator\nnorms of the $n \\times n$ matrices $Q_i$ of $q_i$ do not exceed 1, the smallest\neigenvalue the $m \\times m$ matrix with the $(i,j)$-th entry equal\n$\\operatorname{tr} (Q_i Q_j)$ is at least $\\gamma n^{2/3} m^2 \\ln n$ for an\nabsolute constant $\\gamma >0$. In particular, this happens when $n \\gg m^6$ and\nthe forms $q_i$ are sufficiently generic. We prove a similar sufficient\ncondition for a homogeneous system of quadratic equations to have a non-trivial\nsolution.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Alexander Barvinok"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2106.08119"
  },
  {
    "id": "arXiv:2106.08123",
    "title": "Super-resolving star clusters with sheaves",
    "abstract": "This article explains an optimization-based approach for counting and\nlocalizing stars within a small cluster, based on photon counts in a focal\nplane array. The array need not be arranged in any particular way, and\nrelatively small numbers of photons are required in order to ensure\nconvergence. The stars can be located close to one another, as the location and\nbrightness errors were found to be low when the separation was larger than\n$0.2$ Rayleigh radii. To ensure generality of our approach, it was constructed\nas a special case of a general theory built upon topological signal processing\nusing the mathematics of sheaves.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2106.04445\n",
    "authors": [
      "Michael Robinson",
      "Christopher Capraro"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2106.08123"
  },
  {
    "id": "arXiv:2106.08126",
    "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to  Standard German Text: Microsoft's Submission to SwissText 2021",
    "abstract": "This paper describes the winning approach in the public SwissText 2021\ncompetition on dialect recognition and translation of Swiss German speech to\nstandard German text. Swiss German refers to the multitude of Alemannic\ndialects spoken in the German-speaking parts of Switzerland. Swiss German\ndiffers significantly from standard German in pronunciation, word inventory and\ngrammar. It is mostly incomprehensible to native German speakers. Moreover, it\nlacks a standardized written script. To solve the challenging task, we propose\na hybrid automatic speech recognition system with a lexicon that incorporates\ntranslations, a 1st pass language model that deals with Swiss German\nparticularities, a transfer-learned acoustic model and a strong neural language\nmodel for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind\nconversational test set and outperforms the second best competitor by a 12%\nrelative margin.",
    "descriptor": "",
    "authors": [
      "Yuriy Arabskyy",
      "Aashish Agarwal",
      "Subhadeep Dey",
      "Oscar Koller"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.08126"
  },
  {
    "id": "arXiv:2106.08138",
    "title": "Application of the Quantum Potential Neural Network to multi-electronic  atoms",
    "abstract": "In this report, the application of the Quantum Potential Neural Network\n(QPNN) framework to many electron atomic systems is presented. For this study,\nfull configuration interaction (FCI) one--electron density functions within\npredefined limits of accuracy were used to train the QPNN. The obtained results\nsuggest that this new neural network is capable of learning the effective\npotential functions of many electron atoms in a completely unsupervised manner,\nand using only limited information from the probability density. Using the\neffective potential functions learned for each of the studied systems the QPNN\nwas able to estimate the total energies of each of the systems (with a maximum\nof 10 trials) with a remarkable accuracy when compared to the FCI energies.",
    "descriptor": "\nComments: Follow up of arXiv:2006.13297. First Draft. Work in progress. Comments welcome\n",
    "authors": [
      "Hector H. Corzo",
      "Arijit Sehanobish",
      "Onur Kara"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08138"
  },
  {
    "id": "arXiv:2106.08147",
    "title": "Perceptually-inspired super-resolution of compressed videos",
    "abstract": "Spatial resolution adaptation is a technique which has often been employed in\nvideo compression to enhance coding efficiency. This approach encodes a lower\nresolution version of the input video and reconstructs the original resolution\nduring decoding. Instead of using conventional up-sampling filters, recent work\nhas employed advanced super-resolution methods based on convolutional neural\nnetworks (CNNs) to further improve reconstruction quality. These approaches are\nusually trained to minimise pixel-based losses such as Mean-Squared Error\n(MSE), despite the fact that this type of loss metric does not correlate well\nwith subjective opinions. In this paper, a perceptually-inspired\nsuper-resolution approach (M-SRGAN) is proposed for spatial up-sampling of\ncompressed video using a modified CNN model, which has been trained using a\ngenerative adversarial network (GAN) on compressed content with perceptual loss\nfunctions. The proposed method was integrated with HEVC HM 16.20, and has been\nevaluated on the JVET Common Test Conditions (UHD test sequences) using the\nRandom Access configuration. The results show evident perceptual quality\nimprovement over the original HM 16.20, with an average bitrate saving of 35.6%\n(Bj{\\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.",
    "descriptor": "",
    "authors": [
      "Di Ma",
      "Mariana Afonso",
      "Fan Zhang",
      "David R. Bull"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08147"
  },
  {
    "id": "arXiv:2106.08150",
    "title": "MetaCache-GPU: Ultra-Fast Metagenomic Classification",
    "abstract": "The cost of DNA sequencing has dropped exponentially over the past decade,\nmaking genomic data accessible to a growing number of scientists. In\nbioinformatics, localization of short DNA sequences (reads) within large\ngenomic sequences is commonly facilitated by constructing index data structures\nwhich allow for efficient querying of substrings. Recent metagenomic\nclassification pipelines annotate reads with taxonomic labels by analyzing\ntheir $k$-mer histograms with respect to a reference genome database. CPU-based\nindex construction is often performed in a preprocessing phase due to the\nrelatively high cost of building irregular data structures such as hash maps.\nHowever, the rapidly growing amount of available reference genomes establishes\nthe need for index construction and querying at interactive speeds. In this\npaper, we introduce MetaCache-GPU -- an ultra-fast metagenomic short read\nclassifier specifically tailored to fit the characteristics of CUDA-enabled\naccelerators. Our approach employs a novel hash table variant featuring\nefficient minhash fingerprinting of reads for locality-sensitive hashing and\ntheir rapid insertion using warp-aggregated operations. Our performance\nevaluation shows that MetaCache-GPU is able to build large reference databases\nin a matter of seconds, enabling instantaneous operability, while popular\nCPU-based tools such as Kraken2 require over an hour for index construction on\nthe same data. In the context of an ever-growing number of reference genomes,\nMetaCache-GPU is the first metagenomic classifier that makes analysis pipelines\nwith on-demand composition of large-scale reference genome sets practical. The\nsource code is publicly available at https://github.com/muellan/metacache .",
    "descriptor": "\nComments: 11 pages. To be published in ICPP 2021\n",
    "authors": [
      "Robin Kobus",
      "Andr\u00e9 M\u00fcller",
      "Daniel J\u00fcnger",
      "Christian Hundt",
      "Bertil Schmidt"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.08150"
  },
  {
    "id": "arXiv:2106.08151",
    "title": "EuroCrops: A Pan-European Dataset for Time Series Crop Type  Classification",
    "abstract": "We present EuroCrops, a dataset based on self-declared field annotations for\ntraining and evaluating methods for crop type classification and mapping,\ntogether with its process of acquisition and harmonisation. By this, we aim to\nenrich the research efforts and discussion for data-driven land cover\nclassification via Earth observation and remote sensing. Additionally, through\ninclusion of self-declarations gathered in the scope of subsidy control from\nall countries of the European Union (EU), this dataset highlights the\ndifficulties and pitfalls one comes across when operating on a transnational\nlevel. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that\naspires to capture all the aspects of reference data originating from\nadministrative and agency databases. To address researchers from both the\nremote sensing and the computer vision and machine learning communities, we\npublish the dataset in different formats and processing levels.",
    "descriptor": "\nComments: 4 pages, website: this https URL\n",
    "authors": [
      "Maja Schneider",
      "Amelie Broszeit",
      "Marco K\u00f6rner"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08151"
  },
  {
    "id": "arXiv:2106.08153",
    "title": "Now You See It, Now You Dont: Adversarial Vulnerabilities in  Computational Pathology",
    "abstract": "Deep learning models are routinely employed in computational pathology\n(CPath) for solving problems of diagnostic and prognostic significance.\nTypically, the generalization performance of CPath models is analyzed using\nevaluation protocols such as cross-validation and testing on multi-centric\ncohorts. However, to ensure that such CPath solutions are robust and safe for\nuse in a clinical setting, a critical analysis of their predictive performance\nand vulnerability to adversarial attacks is required, which is the focus of\nthis paper. Specifically, we show that a highly accurate model for\nclassification of tumour patches in pathology images (AUC > 0.95) can easily be\nattacked with minimal perturbations which are imperceptible to lay humans and\ntrained pathologists alike. Our analytical results show that it is possible to\ngenerate single-instance white-box attacks on specific input images with high\nsuccess rate and low perturbation energy. Furthermore, we have also generated a\nsingle universal perturbation matrix using the training dataset only which,\nwhen added to unseen test images, results in forcing the trained neural network\nto flip its prediction labels with high confidence at a success rate of > 84%.\nWe systematically analyze the relationship between perturbation energy of an\nadversarial attack, its impact on morphological constructs of clinical\nsignificance, their perceptibility by a trained pathologist and saliency maps\nobtained using deep learning models. Based on our analysis, we strongly\nrecommend that computational pathology models be critically analyzed using the\nproposed adversarial validation strategy prior to clinical adoption.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Alex Foote",
      "Amina Asif",
      "Ayesha Azam",
      "Nasir Rajpoot",
      "Fayyaz Minhas"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08153"
  },
  {
    "id": "arXiv:2106.08157",
    "title": "CeFi vs. DeFi -- Comparing Centralized to Decentralized Finance",
    "abstract": "To non-experts, the traditional Centralized Finance (CeFi) ecosystem may seem\nobscure, because users are typically not aware of the underlying rules or\nagreements of financial assets and products. Decentralized Finance (DeFi),\nhowever, is making its debut as an ecosystem claiming to offer transparency and\ncontrol, which are partially attributable to the underlying integrity-protected\nblockchain, as well as currently higher financial asset yields than CeFi. Yet,\nthe boundaries between CeFi and DeFi may not be always so clear cut.\nIn this work, we systematically analyze the differences between CeFi and\nDeFi, covering legal, economic, security, privacy and market manipulation. We\nprovide a structured methodology to differentiate between a CeFi and a DeFi\nservice. Our findings show that certain DeFi assets (such as USDC or USDT\nstablecoins) do not necessarily classify as DeFi assets, and may endanger the\neconomic security of intertwined DeFi protocols. We conclude this work with the\nexploration of possible synergies between CeFi and DeFi.",
    "descriptor": "",
    "authors": [
      "Kaihua Qin",
      "Liyi Zhou",
      "Yaroslav Afonin",
      "Ludovico Lazzaretti",
      "Arthur Gervais"
    ],
    "subjectives": [
      "General Finance (q-fin.GN)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.08157"
  },
  {
    "id": "arXiv:2106.08161",
    "title": "Contrastive Mixture of Posteriors for Counterfactual Inference, Data  Integration and Fairness",
    "abstract": "Learning meaningful representations of data that can address challenges such\nas batch effect correction, data integration and counterfactual inference is a\ncentral problem in many domains including computational biology. Adopting a\nConditional VAE framework, we identify the mathematical principle that unites\nthese challenges: learning a representation that is marginally independent of a\ncondition variable. We therefore propose the Contrastive Mixture of Posteriors\n(CoMP) method that uses a novel misalignment penalty to enforce this\nindependence. This penalty is defined in terms of mixtures of the variational\nposteriors themselves, unlike prior work which uses external discrepancy\nmeasures such as MMD to ensure independence in latent space. We show that CoMP\nhas attractive theoretical properties compared to previous approaches,\nespecially when there is complex global structure in latent space. We further\ndemonstrate state of the art performance on a number of real-world problems,\nincluding the challenging tasks of aligning human tumour samples with cancer\ncell-lines and performing counterfactual inference on single-cell RNA\nsequencing data. Incidentally, we find parallels with the fair representation\nlearning literature, and demonstrate CoMP has competitive performance in\nlearning fair yet expressive latent representations.",
    "descriptor": "",
    "authors": [
      "Adam Foster",
      "\u00c1rpi Vez\u00e9r",
      "Craig A Glastonbury",
      "P\u00e1id\u00ed Creed",
      "Sam Abujudeh",
      "Aaron Sim"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)"
    ],
    "url": "https://arxiv.org/abs/2106.08161"
  },
  {
    "id": "arXiv:2106.08174",
    "title": "Automatic linear measurements of the fetal brain on MRI with deep neural  networks",
    "abstract": "Timely, accurate and reliable assessment of fetal brain development is\nessential to reduce short and long-term risks to fetus and mother. Fetal MRI is\nincreasingly used for fetal brain assessment. Three key biometric linear\nmeasurements important for fetal brain evaluation are Cerebral Biparietal\nDiameter (CBD), Bone Biparietal Diameter (BBD), and Trans-Cerebellum Diameter\n(TCD), obtained manually by expert radiologists on reference slices, which is\ntime consuming and prone to human error. The aim of this study was to develop a\nfully automatic method computing the CBD, BBD and TCD measurements from fetal\nbrain MRI. The input is fetal brain MRI volumes which may include the fetal\nbody and the mother's abdomen. The outputs are the measurement values and\nreference slices on which the measurements were computed. The method, which\nfollows the manual measurements principle, consists of five stages: 1)\ncomputation of a Region Of Interest that includes the fetal brain with an\nanisotropic 3D U-Net classifier; 2) reference slice selection with a\nConvolutional Neural Network; 3) slice-wise fetal brain structures segmentation\nwith a multiclass U-Net classifier; 4) computation of the fetal brain\nmidsagittal line and fetal brain orientation, and; 5) computation of the\nmeasurements. Experimental results on 214 volumes for CBD, BBD and TCD\nmeasurements yielded a mean $L_1$ difference of 1.55mm, 1.45mm and 1.23mm\nrespectively, and a Bland-Altman 95% confidence interval ($CI_{95}$) of 3.92mm,\n3.98mm and 2.25mm respectively. These results are similar to the manual\ninter-observer variability. The proposed automatic method for computing\nbiometric linear measurements of the fetal brain from MR imaging achieves human\nlevel performance. It has the potential of being a useful method for the\nassessment of fetal brain biometry in normal and pathological cases, and of\nimproving routine clinical practice.",
    "descriptor": "\nComments: 15 pages, 8 figures, presented in CARS 2020, submitted to IJCARS\n",
    "authors": [
      "Netanell Avisdris",
      "Bossmat Yehuda",
      "Ori Ben-Zvi",
      "Daphna Link-Sourani",
      "Liat Ben-Sira",
      "Elka Miller",
      "Elena Zharkov",
      "Dafna Ben Bashat",
      "Leo Joskowicz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08174"
  },
  {
    "id": "arXiv:2106.08176",
    "title": "Automated triaging of head MRI examinations using convolutional neural  networks",
    "abstract": "The growing demand for head magnetic resonance imaging (MRI) examinations,\nalong with a global shortage of radiologists, has led to an increase in the\ntime taken to report head MRI scans around the world. For many neurological\nconditions, this delay can result in increased morbidity and mortality. An\nautomated triaging tool could reduce reporting times for abnormal examinations\nby identifying abnormalities at the time of imaging and prioritizing the\nreporting of these scans. In this work, we present a convolutional neural\nnetwork for detecting clinically-relevant abnormalities in\n$\\text{T}_2$-weighted head MRI scans. Using a validated neuroradiology report\nclassifier, we generated a labelled dataset of 43,754 scans from two large UK\nhospitals for model training, and demonstrate accurate classification (area\nunder the receiver operating curve (AUC) = 0.943) on a test set of 800 scans\nlabelled by a team of neuroradiologists. Importantly, when trained on scans\nfrom only a single hospital the model generalized to scans from the other\nhospital ($\\Delta$AUC $\\leq$ 0.02). A simulation study demonstrated that our\nmodel would reduce the mean reporting time for abnormal examinations from 28\ndays to 14 days and from 9 days to 5 days at the two hospitals, demonstrating\nfeasibility for use in a clinical triage environment.",
    "descriptor": "\nComments: Accepted as an oral presentation at Medical Imaging with Deep Learning (MIDL) 2021\n",
    "authors": [
      "David A. Wood",
      "Sina Kafiabadi",
      "Ayisha Al Busaidi",
      "Emily Guilhem",
      "Antanas Montvila",
      "Siddharth Agarwal",
      "Jeremy Lynch",
      "Matthew Townend",
      "Gareth Barker",
      "Sebastien Ourselin",
      "James H. Cole",
      "Thomas C. Booth"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08176"
  },
  {
    "id": "arXiv:2106.08185",
    "title": "Kernel Identification Through Transformers",
    "abstract": "Kernel selection plays a central role in determining the performance of\nGaussian Process (GP) models, as the chosen kernel determines both the\ninductive biases and prior support of functions under the GP prior. This work\naddresses the challenge of constructing custom kernel functions for\nhigh-dimensional GP regression models. Drawing inspiration from recent progress\nin deep learning, we introduce a novel approach named KITT: Kernel\nIdentification Through Transformers. KITT exploits a transformer-based\narchitecture to generate kernel recommendations in under 0.1 seconds, which is\nseveral orders of magnitude faster than conventional kernel search algorithms.\nWe train our model using synthetic data generated from priors over a vocabulary\nof known kernels. By exploiting the nature of the self-attention mechanism,\nKITT is able to process datasets with inputs of arbitrary dimension. We\ndemonstrate that kernels chosen by KITT yield strong performance over a diverse\ncollection of regression benchmarks.",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Fergus Simpson",
      "Ian Davies",
      "Vidhi Lalchand",
      "Alessandro Vullo",
      "Nicolas Durrande",
      "Carl Rasmussen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08185"
  },
  {
    "id": "arXiv:2106.08201",
    "title": "How to Determine an Optimal Noise Subspace?",
    "abstract": "The Multiple Signal Classification (MUSIC) algorithm based on the\northogonality between the signal subspace and noise subspace is one of the most\nfrequently used method in the estimation of Direction Of Arrival (DOA), and its\nperformance of DOA estimation mainly depends on the accuracy of the noise\nsubspace. In the most existing researches, the noise subspace is formed by\n(defined as) the eigenvectors corresponding to all small eigenvalues of the\narray output covariance matrix. However, we found that the estimation of DOA\nthrough the noise subspace in the traditional formation is not optimal in\nalmost all cases, and using a partial noise subspace can always obtain optimal\nestimation results. In other words, the subspace spanned by the eigenvectors\ncorresponding to a part of the small eigenvalues is more representative of the\nnoise subspace. We demonstrate this conclusion through a number of experiments.\nThus, it seems that which and how many eigenvectors should be selected to form\nthe partial noise subspace would be an interesting issue. In addition, this\nresearch poses a much general problem: how to select eigenvectors to determine\nan optimal noise subspace?",
    "descriptor": "",
    "authors": [
      "Kaijie Xu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.08201"
  },
  {
    "id": "arXiv:2106.08208",
    "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients",
    "abstract": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
    "descriptor": "\nComments: 11 pages, 3 figures\n",
    "authors": [
      "Feihu Huang",
      "Junyi Li",
      "Heng Huang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08208"
  },
  {
    "id": "arXiv:2106.08217",
    "title": "RFpredInterval: An R Package for Prediction Intervals with Random  Forests and Boosted Forests",
    "abstract": "Like many predictive models, random forests provide a point prediction for a\nnew observation. Besides the point prediction, it is important to quantify the\nuncertainty in the prediction. Prediction intervals provide information about\nthe reliability of the point predictions. We have developed a comprehensive R\npackage, RFpredInterval, that integrates 16 methods to build prediction\nintervals with random forests and boosted forests. The methods implemented in\nthe package are a new method to build prediction intervals with boosted forests\n(PIBF) and 15 different variants to produce prediction intervals with random\nforests proposed by Roy and Larocque (2020). We perform an extensive simulation\nstudy and apply real data analyses to compare the performance of the proposed\nmethod to ten existing methods to build prediction intervals with random\nforests. The results show that the proposed method is very competitive and,\nglobally, it outperforms the competing methods.",
    "descriptor": "\nComments: 32 pages, 14 figures, 5 tables\n",
    "authors": [
      "Cansu Alakus",
      "Denis Larocque",
      "Aurelie Labbe"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08217"
  },
  {
    "id": "arXiv:2106.08247",
    "title": "Canonical-Correlation-Based Fast Feature Selection",
    "abstract": "This paper proposes a canonical-correlation-based filter method for feature\nselection. The sum of squared canonical correlation coefficients is adopted as\nthe feature ranking criterion. The proposed method boosts the computational\nspeed of the ranking criterion in greedy search. The supporting theorems\ndeveloped for the feature selection method are fundamental to the understanding\nof the canonical correlation analysis. In empirical studies, a synthetic\ndataset is used to demonstrate the speed advantage of the proposed method, and\neight real datasets are applied to show the effectiveness of the proposed\nfeature ranking criterion in both classification and regression. The results\nshow that the proposed method is considerably faster than the definition-based\nmethod, and the proposed ranking criterion is competitive compared with the\nseven mutual-information-based criteria.",
    "descriptor": "",
    "authors": [
      "Sikai Zhang",
      "Tingna Wang",
      "Keith Worden",
      "Elizabeth J. Cross"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08247"
  },
  {
    "id": "arXiv:2106.08291",
    "title": "Asymptotic Distribution of Parameters in Trivalent Maps and Linear  Lambda Terms",
    "abstract": "Structural properties of large random maps and lambda-terms may be gleaned by\nstudying the limit distributions of various parameters of interest. In our work\nwe focus on restricted classes of maps and their counterparts in the\nlambda-calculus, building on recent bijective connections between these two\ndomains. In such cases, parameters in maps naturally correspond to parameters\nin lambda-terms and vice versa. By an interplay between lambda-terms and maps,\nwe obtain various combinatorial specifications which allow us to access the\ndistributions of pairs of related parameters such as: the number of bridges in\nrooted trivalent maps and of subterms in closed linear lambda-terms, the number\nof vertices of degree 1 in (1,3)-valent maps and of free variables in open\nlinear lambda-terms etc. To analyse asymptotically these distributions, we\nintroduce appropriate tools: a moment-pumping schema for differential equations\nand a composition schema inspired by Bender's theorem.",
    "descriptor": "\nComments: 40 pages, 16 figures\n",
    "authors": [
      "Olivier Bodini",
      "Alexandros Singh",
      "Noam Zeilberger"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.08291"
  },
  {
    "id": "arXiv:2106.08315",
    "title": "Decentralized Local Stochastic Extra-Gradient for Variational  Inequalities",
    "abstract": "We consider decentralized stochastic variational inequalities where the\nproblem data is distributed across many participating devices (heterogeneous,\nor non-IID data setting). We propose a novel method - based on stochastic\nextra-gradient - where participating devices can communicate over arbitrary,\npossibly time-varying network topologies. This covers both the fully\ndecentralized optimization setting and the centralized topologies commonly used\nin Federated Learning. Our method further supports multiple local updates on\nthe workers for reducing the communication frequency between workers. We\ntheoretically analyze the proposed scheme in the strongly monotone, monotone\nand non-monotone setting. As a special case, our method and analysis apply in\nparticular to decentralized stochastic min-max problems which are being studied\nwith increased interest in Deep Learning. For example, the training objective\nof Generative Adversarial Networks (GANs) are typically saddle point problems\nand the decentralized training of GANs has been reported to be extremely\nchallenging. While SOTA techniques rely on either repeated gossip rounds or\nproximal updates, we alleviate both of these requirements. Experimental results\nfor decentralized GAN demonstrate the effectiveness of our proposed algorithm.",
    "descriptor": "",
    "authors": [
      "Aleksandr Beznosikov",
      "Pavel Dvurechensky",
      "Anastasia Koloskova",
      "Valentin Samokhin",
      "Sebastian U Stich",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08315"
  },
  {
    "id": "arXiv:2106.08317",
    "title": "Active feature selection discovers minimal gene-sets for classifying  cell-types and disease states in single-cell mRNA-seq data",
    "abstract": "Sequencing costs currently prohibit the application of single cell mRNA-seq\nfor many biological and clinical tasks of interest. Here, we introduce an\nactive learning framework that constructs compressed gene sets that enable high\naccuracy classification of cell-types and physiological states while analyzing\na minimal number of gene transcripts. Our active feature selection procedure\nconstructs gene sets through an iterative cell-type classification task where\nmisclassified cells are examined at each round to identify maximally\ninformative genes through an `active' support vector machine (SVM) classifier.\nOur active SVM procedure automatically identifies gene sets that enables\n$>90\\%$ cell-type classification accuracy in the Tabula Muris mouse tissue\nsurvey as well as a $\\sim 40$ gene set that enables classification of multiple\nmyeloma patient samples with $>95\\%$ accuracy. Broadly, the discovery of\ncompact but highly informative gene sets might enable drastic reductions in\nsequencing requirements for applications of single-cell mRNA-seq.",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "Xiaoqiao Chen",
      "Sisi Chen",
      "Matt Thomson"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08317"
  },
  {
    "id": "arXiv:2106.08320",
    "title": "Self-Supervised Learning with Kernel Dependence Maximization",
    "abstract": "We approach self-supervised learning of image representations from a\nstatistical dependence perspective, proposing Self-Supervised Learning with the\nHilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes\ndependence between representations of transformed versions of an image and the\nimage identity, while minimizing the kernelized variance of those features.\nThis self-supervised learning framework yields a new understanding of InfoNCE,\na variational lower bound on the mutual information (MI) between different\ntransformations. While the MI itself is known to have pathologies which can\nresult in meaningless representations being learned, its bound is much better\nbehaved: we show that it implicitly approximates SSL-HSIC (with a slightly\ndifferent regularizer). Our approach also gives us insight into BYOL, since\nSSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to\ndirectly optimize statistical dependence in time linear in the batch size,\nwithout restrictive data assumptions or indirect mutual information estimators.\nTrained with or without a target network, SSL-HSIC matches the current\nstate-of-the-art for standard linear evaluation on ImageNet, semi-supervised\nlearning and transfer to other classification and vision tasks such as semantic\nsegmentation, depth estimation and object recognition.",
    "descriptor": "",
    "authors": [
      "Yazhe Li",
      "Roman Pogodin",
      "Danica J. Sutherland",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08320"
  },
  {
    "id": "arXiv:1712.05517",
    "title": "New and Improved Algorithms for Unordered Tree Inclusion",
    "abstract": "Comments: 22 pages, 9 figures. To appear in Theoretical Computer Science",
    "descriptor": "\nComments: 22 pages, 9 figures. To appear in Theoretical Computer Science\n",
    "authors": [
      "Tatsuya Akutsu",
      "Jesper Jansson",
      "Ruiming Li",
      "Atsuhiro Takasu",
      "Takeyuki Tamura"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1712.05517"
  },
  {
    "id": "arXiv:1901.06732",
    "title": "Fundamental limits of many-user MAC with finite payloads and fading",
    "abstract": "Comments: 34 pages, accepted for publication in IEEE Transactions on Information Theory",
    "descriptor": "\nComments: 34 pages, accepted for publication in IEEE Transactions on Information Theory\n",
    "authors": [
      "Suhas S Kowshik",
      "Yury Polyanskiy"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1901.06732"
  },
  {
    "id": "arXiv:1901.10002",
    "title": "A Framework for Understanding Sources of Harm throughout the Machine  Learning Life Cycle",
    "abstract": "Comments: 11 pages plus references; updated with corrections to text and figures, new examples, and a more thorough walkthrough of ML",
    "descriptor": "\nComments: 11 pages plus references; updated with corrections to text and figures, new examples, and a more thorough walkthrough of ML\n",
    "authors": [
      "Harini Suresh",
      "John V. Guttag"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1901.10002"
  },
  {
    "id": "arXiv:1903.04556",
    "title": "Embarrassingly parallel MCMC using deep invertible transformations",
    "abstract": "Comments: Accepted to UAI 2019",
    "descriptor": "\nComments: Accepted to UAI 2019\n",
    "authors": [
      "Diego Mesquita",
      "Paul Blomstedt",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1903.04556"
  },
  {
    "id": "arXiv:1903.05631",
    "title": "ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series  Modeling",
    "abstract": "ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series  Modeling",
    "descriptor": "",
    "authors": [
      "Bing Yu",
      "Haoteng Yin",
      "Zhanxing Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1903.05631"
  },
  {
    "id": "arXiv:1905.04210",
    "title": "An LP-Based Approach for Goal Recognition as Planning",
    "abstract": "Comments: 8 pages, 4 tables, 3 figures. Published in AAAI 2021. Updated final authorship and text",
    "descriptor": "\nComments: 8 pages, 4 tables, 3 figures. Published in AAAI 2021. Updated final authorship and text\n",
    "authors": [
      "Lu\u00edsa R. de A. Santos",
      "Felipe Meneguzzi",
      "Ramon Fraga Pereira",
      "Andr\u00e9 Grahl Pereira"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1905.04210"
  },
  {
    "id": "arXiv:1906.04199",
    "title": "Deciding the Computability of Regular Functions over Infinite Words",
    "abstract": "Deciding the Computability of Regular Functions over Infinite Words",
    "descriptor": "",
    "authors": [
      "V. Dave",
      "E. Filiot",
      "S. Krishna",
      "N. Lhote"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/1906.04199"
  },
  {
    "id": "arXiv:1908.00461",
    "title": "Low-Rank plus Sparse Decomposition of Covariance Matrices using Neural  Network Parametrization",
    "abstract": "Low-Rank plus Sparse Decomposition of Covariance Matrices using Neural  Network Parametrization",
    "descriptor": "",
    "authors": [
      "Michel Baes",
      "Calypso Herrera",
      "Ariel Neufeld",
      "Pierre Ruyssen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1908.00461"
  },
  {
    "id": "arXiv:1908.01407",
    "title": "GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on  the GPU",
    "abstract": "Comments: 50 pages, 14 figures, 14 tables, to appear in ACM Transactions on Mathematical Software",
    "descriptor": "\nComments: 50 pages, 14 figures, 14 tables, to appear in ACM Transactions on Mathematical Software\n",
    "authors": [
      "Carl Yang",
      "Aydin Buluc",
      "John D. Owens"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/1908.01407"
  },
  {
    "id": "arXiv:1909.00453",
    "title": "Topics to Avoid: Demoting Latent Confounds in Text Classification",
    "abstract": "Comments: 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019)",
    "descriptor": "\nComments: 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019)\n",
    "authors": [
      "Sachin Kumar",
      "Shuly Wintner",
      "Noah A. Smith",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1909.00453"
  },
  {
    "id": "arXiv:1909.00508",
    "title": "Two-Stage Electricity Markets with Renewable Energy Integration: Market  Mechanisms and Equilibrium Analysis",
    "abstract": "Two-Stage Electricity Markets with Renewable Energy Integration: Market  Mechanisms and Equilibrium Analysis",
    "descriptor": "",
    "authors": [
      "Nathan Dahlin",
      "Rahul Jain"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "General Economics (econ.GN)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/1909.00508"
  },
  {
    "id": "arXiv:1909.06723",
    "title": "Natural Language Adversarial Defense through Synonym Encoding",
    "abstract": "Comments: Accepted by UAI 2021, code is avaliable at this https URL",
    "descriptor": "\nComments: Accepted by UAI 2021, code is avaliable at this https URL\n",
    "authors": [
      "Xiaosen Wang",
      "Hao Jin",
      "Yichen Yang",
      "Kun He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1909.06723"
  },
  {
    "id": "arXiv:1909.07589",
    "title": "A Linear Exponential Comonad in s-finite Transition Kernels and  Probabilistic Coherent Spaces",
    "abstract": "Comments: 31 pages",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Masahiro Hamano"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/1909.07589"
  },
  {
    "id": "arXiv:1910.10897",
    "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta  Reinforcement Learning",
    "abstract": "Comments: This is an update version of a manuscript that originally appeared at CoRL 2019. Videos are here: meta-world.github.io, open-sourced code are available at: this https URL, and the baselines can be found at this https URL",
    "descriptor": "\nComments: This is an update version of a manuscript that originally appeared at CoRL 2019. Videos are here: meta-world.github.io, open-sourced code are available at: this https URL, and the baselines can be found at this https URL\n",
    "authors": [
      "Tianhe Yu",
      "Deirdre Quillen",
      "Zhanpeng He",
      "Ryan Julian",
      "Avnish Narayan",
      "Hayden Shively",
      "Adithya Bellathur",
      "Karol Hausman",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.10897"
  },
  {
    "id": "arXiv:1910.12016",
    "title": "Tensor Q-Rank: New Data Dependent Definition of Tensor Rank",
    "abstract": "Tensor Q-Rank: New Data Dependent Definition of Tensor Rank",
    "descriptor": "",
    "authors": [
      "Hao Kong",
      "Canyi Lu",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.12016"
  },
  {
    "id": "arXiv:1910.14215",
    "title": "Multivariate Uncertainty in Deep Learning",
    "abstract": "Comments: To be published in IEEE Transactions on Neural Networks and Learning Systems",
    "descriptor": "\nComments: To be published in IEEE Transactions on Neural Networks and Learning Systems\n",
    "authors": [
      "Rebecca L. Russell",
      "Christopher Reale"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.14215"
  },
  {
    "id": "arXiv:1911.01529",
    "title": "Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation",
    "abstract": "Comments: Accepted to RoboCup Symposium 2021",
    "descriptor": "\nComments: Accepted to RoboCup Symposium 2021\n",
    "authors": [
      "Jan Blumenkamp",
      "Andreas Baude",
      "Tim Laue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/1911.01529"
  },
  {
    "id": "arXiv:1912.04177",
    "title": "Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation",
    "abstract": "Comments: minor edits in technical overview",
    "descriptor": "\nComments: minor edits in technical overview\n",
    "authors": [
      "Ainesh Bakshi",
      "Nadiia Chepurko",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1912.04177"
  },
  {
    "id": "arXiv:1912.08421",
    "title": "Learning to Prevent Leakage: Privacy-Preserving Inference in the Mobile  Cloud",
    "abstract": "Learning to Prevent Leakage: Privacy-Preserving Inference in the Mobile  Cloud",
    "descriptor": "",
    "authors": [
      "Shuang Zhang",
      "Liyao Xiang",
      "Congcong Li",
      "Yixuan Wang",
      "Quanshi Zhang",
      "Wei Wang",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1912.08421"
  },
  {
    "id": "arXiv:2001.10119",
    "title": "Unsupervised Program Synthesis for Images By Sampling Without  Replacement",
    "abstract": "Comments: Accepted to UAI 2021",
    "descriptor": "\nComments: Accepted to UAI 2021\n",
    "authors": [
      "Chenghui Zhou",
      "Chun-Liang Li",
      "Barnabas Poczos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2001.10119"
  },
  {
    "id": "arXiv:2002.03206",
    "title": "Characterizing Structural Regularities of Labeled Data in  Overparameterized Models",
    "abstract": "Comments: 17 pages, 20 figures, ICML 2021",
    "descriptor": "\nComments: 17 pages, 20 figures, ICML 2021\n",
    "authors": [
      "Ziheng Jiang",
      "Chiyuan Zhang",
      "Kunal Talwar",
      "Michael C. Mozer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.03206"
  },
  {
    "id": "arXiv:2002.07687",
    "title": "Mind Your Weight(s): A Large-scale Study on Insufficient Machine  Learning Model Protection in Mobile Apps",
    "abstract": "Comments: 16 pages, 7 figures",
    "descriptor": "\nComments: 16 pages, 7 figures\n",
    "authors": [
      "Zhichuang Sun",
      "Ruimin Sun",
      "Long Lu",
      "Alan Mislove"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2002.07687"
  },
  {
    "id": "arXiv:2002.11743",
    "title": "Composing Normalizing Flows for Inverse Problems",
    "abstract": "Composing Normalizing Flows for Inverse Problems",
    "descriptor": "",
    "authors": [
      "Jay Whang",
      "Erik M. Lindgren",
      "Alexandros G. Dimakis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2002.11743"
  },
  {
    "id": "arXiv:2002.12308",
    "title": "Fillings of skew shapes avoiding diagonal patterns",
    "abstract": "Comments: 23 pages, 14 figures; formatting changes for publication in DMTCS, no changes in content",
    "descriptor": "\nComments: 23 pages, 14 figures; formatting changes for publication in DMTCS, no changes in content\n",
    "authors": [
      "V\u00edt Jel\u00ednek",
      "Mark Karpilovskij"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2002.12308"
  },
  {
    "id": "arXiv:2004.04351",
    "title": "Multi-feature super-resolution network for cloth wrinkle synthesis",
    "abstract": "Multi-feature super-resolution network for cloth wrinkle synthesis",
    "descriptor": "",
    "authors": [
      "Lan Chen",
      "Juntao Ye",
      "Xiaopeng Zhang"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2004.04351"
  },
  {
    "id": "arXiv:2004.11231",
    "title": "Federated Stochastic Gradient Langevin Dynamics",
    "abstract": "Comments: Accepted to UAI 2021",
    "descriptor": "\nComments: Accepted to UAI 2021\n",
    "authors": [
      "Khaoula El Mekkaoui",
      "Diego Mesquita",
      "Paul Blomstedt",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2004.11231"
  },
  {
    "id": "arXiv:2004.11468",
    "title": "How to find a unicorn: a novel model-free, unsupervised anomaly  detection method for time series",
    "abstract": "How to find a unicorn: a novel model-free, unsupervised anomaly  detection method for time series",
    "descriptor": "",
    "authors": [
      "Zsigmond Benk\u0151",
      "Tam\u00e1s B\u00e1bel",
      "Zolt\u00e1n Somogyv\u00e1ri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.11468"
  },
  {
    "id": "arXiv:2004.14180",
    "title": "Quantized Adam with Error Feedback",
    "abstract": "Comments: Accepted to ACM Transactions on Intelligent Systems and Technology",
    "descriptor": "\nComments: Accepted to ACM Transactions on Intelligent Systems and Technology\n",
    "authors": [
      "Congliang Chen",
      "Li Shen",
      "Haozhi Huang",
      "Wei Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.14180"
  },
  {
    "id": "arXiv:2005.05547",
    "title": "List homomorphism problems for signed graphs",
    "abstract": "Comments: various enhancements, full graph-theoretical classification of complexity for cycle-separable graphs, 44 pages",
    "descriptor": "\nComments: various enhancements, full graph-theoretical classification of complexity for cycle-separable graphs, 44 pages\n",
    "authors": [
      "Jan Bok",
      "Richard Brewster",
      "Tom\u00e1s Feder",
      "Pavol Hell",
      "Nikola Jedli\u010dkov\u00e1"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2005.05547"
  },
  {
    "id": "arXiv:2005.08898",
    "title": "Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled  Gradient Descent",
    "abstract": "Comments: Accepted to Journal of Machine Learning Research",
    "descriptor": "\nComments: Accepted to Journal of Machine Learning Research\n",
    "authors": [
      "Tian Tong",
      "Cong Ma",
      "Yuejie Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.08898"
  },
  {
    "id": "arXiv:2006.01017",
    "title": "Improved SVRG for quadratic functions",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Nabil Kahale"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.01017"
  },
  {
    "id": "arXiv:2006.07002",
    "title": "Double Double Descent: On Generalization Errors in Transfer Learning  between Linear Regression Tasks",
    "abstract": "Double Double Descent: On Generalization Errors in Transfer Learning  between Linear Regression Tasks",
    "descriptor": "",
    "authors": [
      "Yehuda Dar",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.07002"
  },
  {
    "id": "arXiv:2006.10246",
    "title": "The Recurrent Neural Tangent Kernel",
    "abstract": "The Recurrent Neural Tangent Kernel",
    "descriptor": "",
    "authors": [
      "Sina Alemohammad",
      "Zichao Wang",
      "Randall Balestriero",
      "Richard Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.10246"
  },
  {
    "id": "arXiv:2006.14512",
    "title": "Uncovering the Connections Between Adversarial Transferability and  Knowledge Transferability",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Kaizhao Liang",
      "Jacky Y. Zhang",
      "Boxin Wang",
      "Zhuolin Yang",
      "Oluwasanmi Koyejo",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.14512"
  },
  {
    "id": "arXiv:2006.15512",
    "title": "Parallel Weighted Model Counting with Tensor Networks",
    "abstract": "Comments: Published at MCW-2020",
    "descriptor": "\nComments: Published at MCW-2020\n",
    "authors": [
      "Jeffrey M. Dudek",
      "Moshe Y. Vardi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2006.15512"
  },
  {
    "id": "arXiv:2007.00674",
    "title": "Sliced Iterative Normalizing Flows",
    "abstract": "Comments: 19 pages, 12 figures, 7 tables. Code available at this https URL",
    "descriptor": "\nComments: 19 pages, 12 figures, 7 tables. Code available at this https URL\n",
    "authors": [
      "Biwei Dai",
      "Uros Seljak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.00674"
  },
  {
    "id": "arXiv:2007.02203",
    "title": "Understanding Accuracy-Efficiency Trade-Offs as a Means for Holding  Distributed ML Systems Accountable",
    "abstract": "Understanding Accuracy-Efficiency Trade-Offs as a Means for Holding  Distributed ML Systems Accountable",
    "descriptor": "",
    "authors": [
      "A. Feder Cooper",
      "Karen Levy",
      "Christopher De Sa"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.02203"
  },
  {
    "id": "arXiv:2007.05426",
    "title": "Variational Inference with Continuously-Indexed Normalizing Flows",
    "abstract": "Comments: Accepted for publication at UAI 2021",
    "descriptor": "\nComments: Accepted for publication at UAI 2021\n",
    "authors": [
      "Anthony Caterini",
      "Rob Cornish",
      "Dino Sejdinovic",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.05426"
  },
  {
    "id": "arXiv:2007.06808",
    "title": "An algorithm for estimating volumes and other integrals in $n$  dimensions",
    "abstract": "An algorithm for estimating volumes and other integrals in $n$  dimensions",
    "descriptor": "",
    "authors": [
      "Arun I.",
      "Murugesan Venkatapathi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2007.06808"
  },
  {
    "id": "arXiv:2007.10306",
    "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk  Prediction",
    "abstract": "Comments: Published in the Journal of Biomedical Informatics (this https URL). Version 3 updates acknowledgements and fixes typos",
    "descriptor": "\nComments: Published in the Journal of Biomedical Informatics (this https URL). Version 3 updates acknowledgements and fixes typos\n",
    "authors": [
      "Stephen R. Pfohl",
      "Agata Foryciarz",
      "Nigam H. Shah"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2007.10306"
  },
  {
    "id": "arXiv:2007.14514",
    "title": "Computing Weighted Subset Transversals in $H$-Free Graphs",
    "abstract": "Computing Weighted Subset Transversals in $H$-Free Graphs",
    "descriptor": "",
    "authors": [
      "Nick Brettell",
      "Matthew Johnson",
      "Daniel Paulusma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2007.14514"
  },
  {
    "id": "arXiv:2007.15588",
    "title": "Data-efficient Hindsight Off-policy Option Learning",
    "abstract": "Comments: Published at ICML2021",
    "descriptor": "\nComments: Published at ICML2021\n",
    "authors": [
      "Markus Wulfmeier",
      "Dushyant Rao",
      "Roland Hafner",
      "Thomas Lampe",
      "Abbas Abdolmaleki",
      "Tim Hertweck",
      "Michael Neunert",
      "Dhruva Tirumala",
      "Noah Siegel",
      "Nicolas Heess",
      "Martin Riedmiller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.15588"
  },
  {
    "id": "arXiv:2008.03129",
    "title": "Brain Drain and Brain Gain in Russia: Analyzing International Migration  of Researchers by Discipline using Scopus Bibliometric Data 1996-2020",
    "abstract": "Comments: 26 pages, 9 figures, 2 tables, peer-reviewed and accepted author-copy. Publisher's verified version: Subbotin, A. and Aref, S. (2021) Brain Drain and Brain Gain in Russia: Analyzing International Migration of Researchers by Discipline using Scopus Bibliometric Data 1996-2020. Forthcoming in Scientometrics",
    "descriptor": "\nComments: 26 pages, 9 figures, 2 tables, peer-reviewed and accepted author-copy. Publisher's verified version: Subbotin, A. and Aref, S. (2021) Brain Drain and Brain Gain in Russia: Analyzing International Migration of Researchers by Discipline using Scopus Bibliometric Data 1996-2020. Forthcoming in Scientometrics\n",
    "authors": [
      "Alexander Subbotin",
      "Samin Aref"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2008.03129"
  },
  {
    "id": "arXiv:2008.03945",
    "title": "On Commonsense Cues in BERT for Solving Commonsense Tasks",
    "abstract": "On Commonsense Cues in BERT for Solving Commonsense Tasks",
    "descriptor": "",
    "authors": [
      "Leyang Cui",
      "Sijie Cheng",
      "Yu Wu",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2008.03945"
  },
  {
    "id": "arXiv:2008.07428",
    "title": "Fast decentralized non-convex finite-sum optimization with recursive  variance reduction",
    "abstract": "Fast decentralized non-convex finite-sum optimization with recursive  variance reduction",
    "descriptor": "",
    "authors": [
      "Ran Xin",
      "Usman A. Khan",
      "Soummya Kar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.07428"
  },
  {
    "id": "arXiv:2009.02732",
    "title": "Convergence Analysis of the Hessian Estimation Evolution Strategy",
    "abstract": "Convergence Analysis of the Hessian Estimation Evolution Strategy",
    "descriptor": "",
    "authors": [
      "Tobias Glasmachers",
      "Oswin Krause"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2009.02732"
  },
  {
    "id": "arXiv:2009.04651",
    "title": "Universal consistency of Wasserstein $k$-NN classifier",
    "abstract": "Comments: 22 pages",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Donlapark Ponnoprat"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2009.04651"
  },
  {
    "id": "arXiv:2009.07101",
    "title": "Approximate spectral clustering using both reference vectors and  topology of the network generated by growing neural gas",
    "abstract": "Approximate spectral clustering using both reference vectors and  topology of the network generated by growing neural gas",
    "descriptor": "",
    "authors": [
      "Kazuhisa Fujita"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.07101"
  },
  {
    "id": "arXiv:2009.07840",
    "title": "Typical and Extremal Aspects of Friends-and-Strangers Graphs",
    "abstract": "Comments: 31 pages, 4 figures",
    "descriptor": "\nComments: 31 pages, 4 figures\n",
    "authors": [
      "Noga Alon",
      "Colin Defant",
      "Noah Kravitz"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2009.07840"
  },
  {
    "id": "arXiv:2009.08372",
    "title": "A Principle of Least Action for the Training of Neural Networks",
    "abstract": "Comments: ECML PKDD 2020",
    "descriptor": "\nComments: ECML PKDD 2020\n",
    "authors": [
      "Skander Karkar",
      "Ibrahim Ayed",
      "Emmanuel de B\u00e9zenac",
      "Patrick Gallinari"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.08372"
  },
  {
    "id": "arXiv:2009.09931",
    "title": "Field-Embedded Factorization Machines for Click-through rate prediction",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Harshit Pande"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.09931"
  },
  {
    "id": "arXiv:2009.10761",
    "title": "On the Locality of Nash-Williams Forest Decomposition and Star-Forest  Decomposition",
    "abstract": "On the Locality of Nash-Williams Forest Decomposition and Star-Forest  Decomposition",
    "descriptor": "",
    "authors": [
      "David G. Harris",
      "Hsin-Hao Su",
      "Hoa T. Vu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2009.10761"
  },
  {
    "id": "arXiv:2009.13566",
    "title": "Graph Neural Networks with Heterophily",
    "abstract": "Comments: Proceedings version of AAAI 2021 with appendix and additional typo fixes; 12 pages, 4 figures",
    "descriptor": "\nComments: Proceedings version of AAAI 2021 with appendix and additional typo fixes; 12 pages, 4 figures\n",
    "authors": [
      "Jiong Zhu",
      "Ryan A. Rossi",
      "Anup Rao",
      "Tung Mai",
      "Nedim Lipka",
      "Nesreen K. Ahmed",
      "Danai Koutra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.13566"
  },
  {
    "id": "arXiv:2010.00060",
    "title": "Constructions and Comparisons of Pooling Matrices for Pooled Testing of  COVID-19",
    "abstract": "Constructions and Comparisons of Pooling Matrices for Pooled Testing of  COVID-19",
    "descriptor": "",
    "authors": [
      "Yi-Jheng Lin",
      "Che-Hao Yu",
      "Tzu-Hsuan Liu",
      "Cheng-Shang Chang",
      "Wen-Tsuen Chen"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Discrete Mathematics (cs.DM)",
      "Information Theory (cs.IT)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2010.00060"
  },
  {
    "id": "arXiv:2010.04597",
    "title": "Computing Dynamic User Equilibrium on Large-Scale Networks Without  Knowing Global Parameters",
    "abstract": "Comments: This paper replaces and extends the previous work arXiv:1810.00777. The paper is accepted for publication at Networks and Spatial Economics",
    "descriptor": "\nComments: This paper replaces and extends the previous work arXiv:1810.00777. The paper is accepted for publication at Networks and Spatial Economics\n",
    "authors": [
      "Duong Viet Thong",
      "Aviv Gibali",
      "Mathias Staudigl",
      "Phan Tu Vuong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2010.04597"
  },
  {
    "id": "arXiv:2010.04879",
    "title": "Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework",
    "abstract": "Comments: 11 pages, 5 figures. Accepted by ICML 2021",
    "descriptor": "\nComments: 11 pages, 5 figures. Accepted by ICML 2021\n",
    "authors": [
      "Wenxiao Wang",
      "Minghao Chen",
      "Shuai Zhao",
      "Long Chen",
      "Jinming Hu",
      "Haifeng Liu",
      "Deng Cai",
      "Xiaofei He",
      "Wei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.04879"
  },
  {
    "id": "arXiv:2010.06728",
    "title": "$L^p$-Bernstein inequalities on $C^2$-domains",
    "abstract": "Comments: the material in this article is based heavily on a part of arXiv:1910.11719",
    "descriptor": "\nComments: the material in this article is based heavily on a part of arXiv:1910.11719\n",
    "authors": [
      "Feng Dai",
      "Andriy Prymak"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2010.06728"
  },
  {
    "id": "arXiv:2010.11066",
    "title": "Contextualized Attention-based Knowledge Transfer for Spoken  Conversational Question Answering",
    "abstract": "Contextualized Attention-based Knowledge Transfer for Spoken  Conversational Question Answering",
    "descriptor": "",
    "authors": [
      "Chenyu You",
      "Nuo Chen",
      "Yuexian Zou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2010.11066"
  },
  {
    "id": "arXiv:2010.12007",
    "title": "PRANK: motion Prediction based on RANKing",
    "abstract": "Comments: Accepted to NeurIPS 2020",
    "descriptor": "\nComments: Accepted to NeurIPS 2020\n",
    "authors": [
      "Yuriy Biktairov",
      "Maxim Stebelev",
      "Irina Rudenko",
      "Oleh Shliazhko",
      "Boris Yangel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.12007"
  },
  {
    "id": "arXiv:2010.12230",
    "title": "Coping with Label Shift via Distributionally Robust Optimisation",
    "abstract": "Coping with Label Shift via Distributionally Robust Optimisation",
    "descriptor": "",
    "authors": [
      "Jingzhao Zhang",
      "Aditya Menon",
      "Andreas Veit",
      "Srinadh Bhojanapalli",
      "Sanjiv Kumar",
      "Suvrit Sra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2010.12230"
  },
  {
    "id": "arXiv:2010.12341",
    "title": "Abstracting the Traffic of Nonlinear Event-Triggered Control Systems",
    "abstract": "Abstracting the Traffic of Nonlinear Event-Triggered Control Systems",
    "descriptor": "",
    "authors": [
      "Giannis Delimpaltadakis",
      "Manuel Mazo Jr"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2010.12341"
  },
  {
    "id": "arXiv:2010.13511",
    "title": "Efficient Optimization Methods for Extreme Similarity Learning with  Nonlinear Embeddings",
    "abstract": "Comments: Published as a conference paper at KDD 2021",
    "descriptor": "\nComments: Published as a conference paper at KDD 2021\n",
    "authors": [
      "Bowen Yuan",
      "Yu-Sheng Li",
      "Pengrui Quan",
      "Chih-Jen Lin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.13511"
  },
  {
    "id": "arXiv:2010.14860",
    "title": "The Evidence Lower Bound of Variational Autoencoders Converges to a Sum  of Three Entropies",
    "abstract": "The Evidence Lower Bound of Variational Autoencoders Converges to a Sum  of Three Entropies",
    "descriptor": "",
    "authors": [
      "J\u00f6rg L\u00fccke",
      "Dennis Forster",
      "Zhenwen Dai"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.14860"
  },
  {
    "id": "arXiv:2010.15727",
    "title": "Amortized Probabilistic Detection of Communities in Graphs",
    "abstract": "Amortized Probabilistic Detection of Communities in Graphs",
    "descriptor": "",
    "authors": [
      "Yueqi Wang",
      "Yoonho Lee",
      "Pallab Basu",
      "Juho Lee",
      "Yee Whye Teh",
      "Liam Paninski",
      "Ari Pakman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.15727"
  },
  {
    "id": "arXiv:2011.00472",
    "title": "Optimal minimal-contact routing of randomly arriving agents through  connected networks",
    "abstract": "Optimal minimal-contact routing of randomly arriving agents through  connected networks",
    "descriptor": "",
    "authors": [
      "Diptangshu Sen",
      "Prasanna Ramamoorthy",
      "Varun Ramamohan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.00472"
  },
  {
    "id": "arXiv:2011.01489",
    "title": "On Computing Stable Extensions of Abstract Argumentation Frameworks",
    "abstract": "On Computing Stable Extensions of Abstract Argumentation Frameworks",
    "descriptor": "",
    "authors": [
      "Samer Nofal",
      "Amani Abu Jabal",
      "Abdullah Alfarrarjeh",
      "Ismail Hababeh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2011.01489"
  },
  {
    "id": "arXiv:2011.03639",
    "title": "Graph cuts always find a global optimum for Potts models (with a catch)",
    "abstract": "Comments: Published at ICML 2021. 18 pages, 2 figures",
    "descriptor": "\nComments: Published at ICML 2021. 18 pages, 2 figures\n",
    "authors": [
      "Hunter Lang",
      "David Sontag",
      "Aravindan Vijayaraghavan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.03639"
  },
  {
    "id": "arXiv:2011.05530",
    "title": "On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU  Networks",
    "abstract": "On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU  Networks",
    "descriptor": "",
    "authors": [
      "Ramy E. Ali",
      "Jinhyun So",
      "A. Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2011.05530"
  },
  {
    "id": "arXiv:2011.05905",
    "title": "ShadowNet: A Secure and Efficient System for On-device Model Inference",
    "abstract": "Comments: single column, 21 pages (29 pages include appendix), 12 figures",
    "descriptor": "\nComments: single column, 21 pages (29 pages include appendix), 12 figures\n",
    "authors": [
      "Zhichuang Sun",
      "Ruimin Sun",
      "Changming Liu",
      "Amrita Roy Chowdhury",
      "Somesh Jha",
      "Long Lu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.05905"
  },
  {
    "id": "arXiv:2011.10538",
    "title": "Improving RNN-T ASR Accuracy Using Context Audio",
    "abstract": "Improving RNN-T ASR Accuracy Using Context Audio",
    "descriptor": "",
    "authors": [
      "Andreas Schwarz",
      "Ilya Sklyar",
      "Simon Wiesler"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2011.10538"
  },
  {
    "id": "arXiv:2011.10781",
    "title": "Chitrakar: Robotic System for Drawing Jordan Curve of Facial Portrait",
    "abstract": "Comments: Submitted to International Journal of Social Robotics 2021",
    "descriptor": "\nComments: Submitted to International Journal of Social Robotics 2021\n",
    "authors": [
      "Aniruddha Singhal",
      "Ayush Kumar",
      "Shivam Thukral",
      "Deepak Raina",
      "Swagat Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2011.10781"
  },
  {
    "id": "arXiv:2011.12589",
    "title": "Combining Semantic Guidance and Deep Reinforcement Learning For  Generating Human Level Paintings",
    "abstract": "Combining Semantic Guidance and Deep Reinforcement Learning For  Generating Human Level Paintings",
    "descriptor": "",
    "authors": [
      "Jaskirat Singh",
      "Liang Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.12589"
  },
  {
    "id": "arXiv:2011.12690",
    "title": "DeepKoCo: Efficient latent planning with a robust Koopman representation",
    "abstract": "DeepKoCo: Efficient latent planning with a robust Koopman representation",
    "descriptor": "",
    "authors": [
      "Bas van der Heijden",
      "Laura Ferranti",
      "Jens Kober",
      "Robert Babuska"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.12690"
  },
  {
    "id": "arXiv:2011.13511",
    "title": "Physics-Informed Neural Network for Modelling the Thermochemical Curing  Process of Composite-Tool Systems During Manufacture",
    "abstract": "Physics-Informed Neural Network for Modelling the Thermochemical Curing  Process of Composite-Tool Systems During Manufacture",
    "descriptor": "",
    "authors": [
      "Sina Amini Niaki",
      "Ehsan Haghighat",
      "Trevor Campbell",
      "Anoush Poursartip",
      "Reza Vaziri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.13511"
  },
  {
    "id": "arXiv:2012.00952",
    "title": "Mechanism Design for Demand Management in Energy Communities",
    "abstract": "Mechanism Design for Demand Management in Energy Communities",
    "descriptor": "",
    "authors": [
      "Xupeng Wei",
      "Achilleas Anastasopoulos"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2012.00952"
  },
  {
    "id": "arXiv:2012.01604",
    "title": "Going Beyond Classification Accuracy Metrics in Model Compression",
    "abstract": "Going Beyond Classification Accuracy Metrics in Model Compression",
    "descriptor": "",
    "authors": [
      "Vinu Joseph",
      "Shoaib Ahmed Siddiqui",
      "Aditya Bhaskara",
      "Ganesh Gopalakrishnan",
      "Saurav Muralidharan",
      "Michael Garland",
      "Sheraz Ahmed",
      "Andreas Dengel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01604"
  },
  {
    "id": "arXiv:2012.02298",
    "title": "Exploration in Online Advertising Systems with Deep Uncertainty-Aware  Learning",
    "abstract": "Comments: SIGKDD 2021",
    "descriptor": "\nComments: SIGKDD 2021\n",
    "authors": [
      "Chao Du",
      "Zhifeng Gao",
      "Shuo Yuan",
      "Lining Gao",
      "Ziyan Li",
      "Yifan Zeng",
      "Xiaoqiang Zhu",
      "Jian Xu",
      "Kun Gai",
      "Kuang-chih Lee"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.02298"
  },
  {
    "id": "arXiv:2012.02409",
    "title": "When does gradient descent with logistic loss find interpolating  two-layer networks?",
    "abstract": "Comments: 44 pages, 4 figures",
    "descriptor": "\nComments: 44 pages, 4 figures\n",
    "authors": [
      "Niladri S. Chatterji",
      "Philip M. Long",
      "Peter L. Bartlett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2012.02409"
  },
  {
    "id": "arXiv:2012.03837",
    "title": "Parallel Training of Deep Networks with Local Updates",
    "abstract": "Comments: First two authors - Michael Laskin and Luke Metz - contributed equally. Order was determined by a coin flip",
    "descriptor": "\nComments: First two authors - Michael Laskin and Luke Metz - contributed equally. Order was determined by a coin flip\n",
    "authors": [
      "Michael Laskin",
      "Luke Metz",
      "Seth Nabarro",
      "Mark Saroufim",
      "Badreddine Noune",
      "Carlo Luschi",
      "Jascha Sohl-Dickstein",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2012.03837"
  },
  {
    "id": "arXiv:2012.04056",
    "title": "Semantics Altering Modifications for Evaluating Comprehension in Machine  Reading",
    "abstract": "Comments: AAAI 2021, final version. 7 pages content + 2 pages references",
    "descriptor": "\nComments: AAAI 2021, final version. 7 pages content + 2 pages references\n",
    "authors": [
      "Viktor Schlegel",
      "Goran Nenadic",
      "Riza Batista-Navarro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.04056"
  },
  {
    "id": "arXiv:2012.04262",
    "title": "Overcomplete Representations Against Adversarial Videos",
    "abstract": "Comments: Accepted at IEEE International Conference on Image Processing (ICIP) 2021",
    "descriptor": "\nComments: Accepted at IEEE International Conference on Image Processing (ICIP) 2021\n",
    "authors": [
      "Shao-Yuan Lo",
      "Jeya Maria Jose Valanarasu",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2012.04262"
  },
  {
    "id": "arXiv:2012.07805",
    "title": "Extracting Training Data from Large Language Models",
    "abstract": "Extracting Training Data from Large Language Models",
    "descriptor": "",
    "authors": [
      "Nicholas Carlini",
      "Florian Tramer",
      "Eric Wallace",
      "Matthew Jagielski",
      "Ariel Herbert-Voss",
      "Katherine Lee",
      "Adam Roberts",
      "Tom Brown",
      "Dawn Song",
      "Ulfar Erlingsson",
      "Alina Oprea",
      "Colin Raffel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.07805"
  },
  {
    "id": "arXiv:2012.07919",
    "title": "A Software Engineering Perspective on Engineering Machine Learning  Systems: State of the Art and Challenges",
    "abstract": "Comments: accepted by the Journal of Systems and Software on 15 June 2021",
    "descriptor": "\nComments: accepted by the Journal of Systems and Software on 15 June 2021\n",
    "authors": [
      "G\u00f6rkem Giray"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.07919"
  },
  {
    "id": "arXiv:2012.08388",
    "title": "Dynamic driving and routing games for autonomous vehicles on networks: A  mean field game approach",
    "abstract": "Comments: 32 pages, 13 figures",
    "descriptor": "\nComments: 32 pages, 13 figures\n",
    "authors": [
      "Kuang Huang",
      "Xu Chen",
      "Xuan Di",
      "Qiang Du"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2012.08388"
  },
  {
    "id": "arXiv:2012.08677",
    "title": "Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge  Learning",
    "abstract": "Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge  Learning",
    "descriptor": "",
    "authors": [
      "Sheng Yue",
      "Ju Ren",
      "Jiang Xin",
      "Sen Lin",
      "Junshan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2012.08677"
  },
  {
    "id": "arXiv:2012.09760",
    "title": "End-to-End Human Pose and Mesh Reconstruction with Transformers",
    "abstract": "Comments: CVPR 2021",
    "descriptor": "\nComments: CVPR 2021\n",
    "authors": [
      "Kevin Lin",
      "Lijuan Wang",
      "Zicheng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.09760"
  },
  {
    "id": "arXiv:2012.10018",
    "title": "NeurST: Neural Speech Translation Toolkit",
    "abstract": "Comments: Accepted by ACL 2021 (system demonstration)",
    "descriptor": "\nComments: Accepted by ACL 2021 (system demonstration)\n",
    "authors": [
      "Chengqi Zhao",
      "Mingxuan Wang",
      "Qianqian Dong",
      "Rong Ye",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2012.10018"
  },
  {
    "id": "arXiv:2012.11025",
    "title": "DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep  neural networks",
    "abstract": "Comments: Presented at CVPR 2021",
    "descriptor": "\nComments: Presented at CVPR 2021\n",
    "authors": [
      "Abhishek Singh",
      "Ayush Chopra",
      "Vivek Sharma",
      "Ethan Garza",
      "Emily Zhang",
      "Praneeth Vepakomma",
      "Ramesh Raskar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.11025"
  },
  {
    "id": "arXiv:2012.15721",
    "title": "Coded Machine Unlearning",
    "abstract": "Comments: Accepted for publication in IEEE Access",
    "descriptor": "\nComments: Accepted for publication in IEEE Access\n",
    "authors": [
      "Nasser Aldaghri",
      "Hessam Mahdavifar",
      "Ahmad Beirami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.15721"
  },
  {
    "id": "arXiv:2101.00311",
    "title": "Disclosure Risk from Homogeneity Attack in Differentially Private  Frequency Distribution",
    "abstract": "Disclosure Risk from Homogeneity Attack in Differentially Private  Frequency Distribution",
    "descriptor": "",
    "authors": [
      "Fang Liu",
      "Xingyuan Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2101.00311"
  },
  {
    "id": "arXiv:2101.00420",
    "title": "Learning to Generate Task-Specific Adapters from Task Description",
    "abstract": "Comments: Accepted to ACL 2021. Camera-ready version. Code: this https URL",
    "descriptor": "\nComments: Accepted to ACL 2021. Camera-ready version. Code: this https URL\n",
    "authors": [
      "Qinyuan Ye",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.00420"
  },
  {
    "id": "arXiv:2101.03624",
    "title": "Compliant Fins for Locomotion in Granular Media",
    "abstract": "Comments: \\c{opyright} 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "descriptor": "\nComments: \\c{opyright} 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Dongting Li",
      "Sichuan Huang",
      "Yong Tang",
      "Hamidreza Marvi",
      "Junliang Tao",
      "Daniel M. Aukes"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2101.03624"
  },
  {
    "id": "arXiv:2101.07726",
    "title": "Anticoncentration versus the number of subset sums",
    "abstract": "Comments: 10 pages; revised version",
    "descriptor": "\nComments: 10 pages; revised version\n",
    "authors": [
      "Vishesh Jain",
      "Ashwin Sah",
      "Mehtaab Sawhney"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2101.07726"
  },
  {
    "id": "arXiv:2101.09048",
    "title": "Selfish Sparse RNN Training",
    "abstract": "Comments: Published in Proceedings of the 38th International Conference on Machine Learning. Code can be found in this https URL",
    "descriptor": "\nComments: Published in Proceedings of the 38th International Conference on Machine Learning. Code can be found in this https URL\n",
    "authors": [
      "Shiwei Liu",
      "Decebal Constantin Mocanu",
      "Yulong Pei",
      "Mykola Pechenizkiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.09048"
  },
  {
    "id": "arXiv:2101.09152",
    "title": "Semi-discrete and fully discrete mixed finite element methods for  Maxwell viscoelastic model of wave propagation",
    "abstract": "Comments: 19 pages, 1 figure",
    "descriptor": "\nComments: 19 pages, 1 figure\n",
    "authors": [
      "Hao Yuan",
      "Xiaoping Xie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2101.09152"
  },
  {
    "id": "arXiv:2101.09451",
    "title": "Error Diffusion Halftoning Against Adversarial Examples",
    "abstract": "Comments: Accepted at IEEE International Conference on Image Processing (ICIP) 2021",
    "descriptor": "\nComments: Accepted at IEEE International Conference on Image Processing (ICIP) 2021\n",
    "authors": [
      "Shao-Yuan Lo",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2101.09451"
  },
  {
    "id": "arXiv:2101.09671",
    "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey",
    "abstract": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey",
    "descriptor": "",
    "authors": [
      "Tailin Liang",
      "John Glossner",
      "Lei Wang",
      "Shaobo Shi",
      "Xiaotong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.09671"
  },
  {
    "id": "arXiv:2101.10902",
    "title": "Maximum n-times Coverage for Vaccine Design",
    "abstract": "Comments: 10 pages, 5 figures",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Ge Liu",
      "Alexander Dimitrakakis",
      "Brandon Carter",
      "David Gifford"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.10902"
  },
  {
    "id": "arXiv:2101.12349",
    "title": "Logical Characterizations of Fuzzy Bisimulations in Fuzzy Modal Logics  over Residuated Lattices",
    "abstract": "Logical Characterizations of Fuzzy Bisimulations in Fuzzy Modal Logics  over Residuated Lattices",
    "descriptor": "",
    "authors": [
      "Linh Anh Nguyen"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2101.12349"
  },
  {
    "id": "arXiv:2102.01757",
    "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Elizabeth Salesky",
      "Matthew Wiesner",
      "Jacob Bremerman",
      "Roldano Cattoni",
      "Matteo Negri",
      "Marco Turchi",
      "Douglas W. Oard",
      "Matt Post"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.01757"
  },
  {
    "id": "arXiv:2102.02887",
    "title": "Do We Actually Need Dense Over-Parameterization? In-Time  Over-Parameterization in Sparse Training",
    "abstract": "Comments: 16 pages; 10 figures; Published in Proceedings of the 38th International Conference on Machine Learning. Code can be found this https URL",
    "descriptor": "\nComments: 16 pages; 10 figures; Published in Proceedings of the 38th International Conference on Machine Learning. Code can be found this https URL\n",
    "authors": [
      "Shiwei Liu",
      "Lu Yin",
      "Decebal Constantin Mocanu",
      "Mykola Pechenizkiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.02887"
  },
  {
    "id": "arXiv:2102.05246",
    "title": "Memory-Associated Differential Learning",
    "abstract": "Comments: 7 pages, 4 figures, 2 tables",
    "descriptor": "\nComments: 7 pages, 4 figures, 2 tables\n",
    "authors": [
      "Yi Luo",
      "Aiguo Chen",
      "Bei Hui",
      "Ke Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.05246"
  },
  {
    "id": "arXiv:2102.07024",
    "title": "Interactive Learning from Activity Description",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Khanh Nguyen",
      "Dipendra Misra",
      "Robert Schapire",
      "Miro Dud\u00edk",
      "Patrick Shafto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07024"
  },
  {
    "id": "arXiv:2102.07043",
    "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations",
    "abstract": "Comments: Accepted at the 38th International Conference on Machine Learning, PMLR 139, 2021",
    "descriptor": "\nComments: Accepted at the 38th International Conference on Machine Learning, PMLR 139, 2021\n",
    "authors": [
      "Haitian Sun",
      "Pat Verga",
      "Bhuwan Dhingra",
      "Ruslan Salakhutdinov",
      "William W. Cohen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07043"
  },
  {
    "id": "arXiv:2102.07367",
    "title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via  Double-Momentum",
    "abstract": "Comments: 36 Pages, 10 Figures",
    "descriptor": "\nComments: 36 Pages, 10 Figures\n",
    "authors": [
      "Prashant Khanduri",
      "Siliang Zeng",
      "Mingyi Hong",
      "Hoi-To Wai",
      "Zhaoran Wang",
      "Zhuoran Yang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.07367"
  },
  {
    "id": "arXiv:2102.08244",
    "title": "Differentially Private Quantiles",
    "abstract": "Comments: This version adds the FFT optimization and corresponds to the ICML 2021 camera-ready",
    "descriptor": "\nComments: This version adds the FFT optimization and corresponds to the ICML 2021 camera-ready\n",
    "authors": [
      "Jennifer Gillenwater",
      "Matthew Joseph",
      "Alex Kulesza"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2102.08244"
  },
  {
    "id": "arXiv:2102.09030",
    "title": "Bringing Differential Private SGD to Practice: On the Independence of  Gaussian Noise and the Number of Training Rounds",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2007.09208",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2007.09208\n",
    "authors": [
      "Marten van Dijk",
      "Nhuong V. Nguyen",
      "Toan N. Nguyen",
      "Lam M. Nguyen",
      "Phuong Ha Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.09030"
  },
  {
    "id": "arXiv:2102.09704",
    "title": "Fair Sparse Regression with Clustering: An Invex Relaxation for a  Combinatorial Problem",
    "abstract": "Fair Sparse Regression with Clustering: An Invex Relaxation for a  Combinatorial Problem",
    "descriptor": "",
    "authors": [
      "Adarsh Barik",
      "Jean Honorio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09704"
  },
  {
    "id": "arXiv:2102.09914",
    "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with  Predicted Future Text Input",
    "abstract": "Comments: 4 pages",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Brooke Stephenson",
      "Thomas Hueber",
      "Laurent Girin",
      "Laurent Besacier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2102.09914"
  },
  {
    "id": "arXiv:2102.10086",
    "title": "Compact and adaptive multiplane images for view synthesis",
    "abstract": "Comments: ICIP 2021",
    "descriptor": "\nComments: ICIP 2021\n",
    "authors": [
      "Julia Navarro",
      "Neus Sabater"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.10086"
  },
  {
    "id": "arXiv:2102.10330",
    "title": "Decoupling Value and Policy for Generalization in Reinforcement Learning",
    "abstract": "Decoupling Value and Policy for Generalization in Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Roberta Raileanu",
      "Rob Fergus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.10330"
  },
  {
    "id": "arXiv:2102.10769",
    "title": "MobILE: Model-Based Imitation Learning From Observation Alone",
    "abstract": "Comments: 27 pages, 5 figures, 2 tabular columns",
    "descriptor": "\nComments: 27 pages, 5 figures, 2 tabular columns\n",
    "authors": [
      "Rahul Kidambi",
      "Jonathan Chang",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.10769"
  },
  {
    "id": "arXiv:2102.11086",
    "title": "Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding",
    "abstract": "Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding",
    "descriptor": "",
    "authors": [
      "Yangjun Ruan",
      "Karen Ullrich",
      "Daniel Severo",
      "James Townsend",
      "Ashish Khisti",
      "Arnaud Doucet",
      "Alireza Makhzani",
      "Chris J. Maddison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2102.11086"
  },
  {
    "id": "arXiv:2102.11436",
    "title": "Model-Based Domain Generalization",
    "abstract": "Model-Based Domain Generalization",
    "descriptor": "",
    "authors": [
      "Alexander Robey",
      "George J. Pappas",
      "Hamed Hassani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11436"
  },
  {
    "id": "arXiv:2102.11872",
    "title": "Dont Just Divide; Polarize and Conquer!",
    "abstract": "Comments: 19 Pages, 5 figures",
    "descriptor": "\nComments: 19 Pages, 5 figures\n",
    "authors": [
      "Shivin Srivastava",
      "Siddharth Bhatia",
      "Lingxiao Huang",
      "Lim Jun Heng",
      "Kenji Kawaguchi",
      "Vaibhav Rajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.11872"
  },
  {
    "id": "arXiv:2102.11925",
    "title": "Chasm in Hegemony: Explaining and Reproducing Disparities in Homophilous  Networks",
    "abstract": "Chasm in Hegemony: Explaining and Reproducing Disparities in Homophilous  Networks",
    "descriptor": "",
    "authors": [
      "Yiguang Zhang",
      "Jessy Xinyi Han",
      "Ilica Mahajan",
      "Priyanjana Bengani",
      "Augustin Chaintreau"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2102.11925"
  },
  {
    "id": "arXiv:2102.12321",
    "title": "AGENT: A Benchmark for Core Psychological Reasoning",
    "abstract": "Comments: ICML 2021, 12 pages, 7 figures",
    "descriptor": "\nComments: ICML 2021, 12 pages, 7 figures\n",
    "authors": [
      "Tianmin Shu",
      "Abhishek Bhandwaldar",
      "Chuang Gan",
      "Kevin A. Smith",
      "Shari Liu",
      "Dan Gutfreund",
      "Elizabeth Spelke",
      "Joshua B. Tenenbaum",
      "Tomer D. Ullman"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.12321"
  },
  {
    "id": "arXiv:2102.12661",
    "title": "Online Learning for Unknown Partially Observable MDPs",
    "abstract": "Online Learning for Unknown Partially Observable MDPs",
    "descriptor": "",
    "authors": [
      "Mehdi Jafarnia-Jahromi",
      "Rahul Jain",
      "Ashutosh Nayyar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.12661"
  },
  {
    "id": "arXiv:2102.12675",
    "title": "Computing Accurate Probabilistic Estimates of One-D Entropy from  Equiprobable Random Samples",
    "abstract": "Comments: 23 pages, 12 figures",
    "descriptor": "\nComments: 23 pages, 12 figures\n",
    "authors": [
      "Hoshin V Gupta",
      "Mohammed Reza Ehsani",
      "Tirthankar Roy",
      "Maria A Sans-Fuentes",
      "Uwe Ehret",
      "Ali Behrangi"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.12675"
  },
  {
    "id": "arXiv:2103.00005",
    "title": "Competitive Online Peak-Demand Minimization Using Energy Storage",
    "abstract": "Competitive Online Peak-Demand Minimization Using Energy Storage",
    "descriptor": "",
    "authors": [
      "Yanfang Mo",
      "Qiulin Lin",
      "Minghua Chen",
      "Si-Zhao Joe Qin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2103.00005"
  },
  {
    "id": "arXiv:2103.00671",
    "title": "Robust learning under clean-label attack",
    "abstract": "Robust learning under clean-label attack",
    "descriptor": "",
    "authors": [
      "Avrim Blum",
      "Steve Hanneke",
      "Jian Qian",
      "Han Shao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.00671"
  },
  {
    "id": "arXiv:2103.01400",
    "title": "Smoothness Analysis of Adversarial Training",
    "abstract": "Comments: 22 pages, 7 figures. In V3, we add the results of EntropySGD for adversarial training",
    "descriptor": "\nComments: 22 pages, 7 figures. In V3, we add the results of EntropySGD for adversarial training\n",
    "authors": [
      "Sekitoshi Kanai",
      "Masanori Yamada",
      "Hiroshi Takahashi",
      "Yuki Yamanaka",
      "Yasutoshi Ida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.01400"
  },
  {
    "id": "arXiv:2103.01500",
    "title": "LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-body  Tracking Signals",
    "abstract": "LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-body  Tracking Signals",
    "descriptor": "",
    "authors": [
      "Dongseok Yang",
      "Doyeon Kim",
      "Sung-Hee Lee"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2103.01500"
  },
  {
    "id": "arXiv:2103.01620",
    "title": "Disentangling Syntax and Semantics in the Brain with Deep Networks",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Charlotte Caucheteux",
      "Alexandre Gramfort",
      "Jean-Remi King"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2103.01620"
  },
  {
    "id": "arXiv:2103.05541",
    "title": "Constrained Contextual Bandit Learning for Adaptive Radar Waveform  Selection",
    "abstract": "Comments: 16 pages, 9 figures. arXiv admin note: text overlap with arXiv:2010.15698",
    "descriptor": "\nComments: 16 pages, 9 figures. arXiv admin note: text overlap with arXiv:2010.15698\n",
    "authors": [
      "Charles E. Thornton",
      "R. Michael Buehrer",
      "Anthony F. Martone"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2103.05541"
  },
  {
    "id": "arXiv:2103.06064",
    "title": "Graph Neural Networks Inspired by Classical Iterative Algorithms",
    "abstract": "Comments: accepted as long oral for ICML 2021",
    "descriptor": "\nComments: accepted as long oral for ICML 2021\n",
    "authors": [
      "Yongyi Yang",
      "Tang Liu",
      "Yangkun Wang",
      "Jinjing Zhou",
      "Quan Gan",
      "Zhewei Wei",
      "Zheng Zhang",
      "Zengfeng Huang",
      "David Wipf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.06064"
  },
  {
    "id": "arXiv:2103.06492",
    "title": "Preventing Extreme Polarization of Political Attitudes",
    "abstract": "Comments: 23 pages, 15 figures, 1 table",
    "descriptor": "\nComments: 23 pages, 15 figures, 1 table\n",
    "authors": [
      "Robert Axelrod",
      "Joshua J. Daymude",
      "Stephanie Forrest"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2103.06492"
  },
  {
    "id": "arXiv:2103.08482",
    "title": "Machine Learning for Nondestructive Wear Assessment in Large Internal  Combustion Engines",
    "abstract": "Machine Learning for Nondestructive Wear Assessment in Large Internal  Combustion Engines",
    "descriptor": "",
    "authors": [
      "Christoph Angermann",
      "Steinbj\u00f6rn J\u00f3nsson",
      "Markus Haltmeier",
      "Ad\u00e9la Moravov\u00e1",
      "Christian Laubichler",
      "Constantin Kiesling",
      "Martin Kober",
      "Wolfgang Fimml"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.08482"
  },
  {
    "id": "arXiv:2103.09284",
    "title": "Learning in Nonzero-Sum Stochastic Games with Potentials",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "David Mguni",
      "Yutong Wu",
      "Yali Du",
      "Yaodong Yang",
      "Ziyi Wang",
      "Minne Li",
      "Ying Wen",
      "Joel Jennings",
      "Jun Wang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2103.09284"
  },
  {
    "id": "arXiv:2103.12535",
    "title": "NNrepair: Constraint-based Repair of Neural Network Classifiers",
    "abstract": "NNrepair: Constraint-based Repair of Neural Network Classifiers",
    "descriptor": "",
    "authors": [
      "Muhammad Usman",
      "Divya Gopinath",
      "Youcheng Sun",
      "Yannic Noller",
      "Corina Pasareanu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.12535"
  },
  {
    "id": "arXiv:2103.16858",
    "title": "SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic  Scene Classification",
    "abstract": "Comments: Submitted to Interspeech 2021",
    "descriptor": "\nComments: Submitted to Interspeech 2021\n",
    "authors": [
      "Helin Wang",
      "Yuexian Zou",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2103.16858"
  },
  {
    "id": "arXiv:2104.00769",
    "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
    "abstract": "Comments: Proceedings of INTERSPEECH",
    "descriptor": "\nComments: Proceedings of INTERSPEECH\n",
    "authors": [
      "Axel Berg",
      "Mark O'Connor",
      "Miguel Tairum Cruz"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.00769"
  },
  {
    "id": "arXiv:2104.00877",
    "title": "S2R-DepthNet: Learning a Generalizable Depth-specific Structural  Representation",
    "abstract": "Comments: Accepted by CVPR2021(oral)",
    "descriptor": "\nComments: Accepted by CVPR2021(oral)\n",
    "authors": [
      "Xiaotian Chen",
      "Yuwang Wang",
      "Xuejin Chen",
      "Wenjun Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.00877"
  },
  {
    "id": "arXiv:2104.00995",
    "title": "Exponential Reduction in Sample Complexity with Learning of Ising Model  Dynamics",
    "abstract": "Comments: Accepted to ICML 2021",
    "descriptor": "\nComments: Accepted to ICML 2021\n",
    "authors": [
      "Arkopal Dutt",
      "Andrey Y. Lokhov",
      "Marc Vuffray",
      "Sidhant Misra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.00995"
  },
  {
    "id": "arXiv:2104.01271",
    "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation  of Teacher Ensembles for Spoken Command Classification",
    "abstract": "Comments: Accepted to Interspeech 2021",
    "descriptor": "\nComments: Accepted to Interspeech 2021\n",
    "authors": [
      "Chao-Han Huck Yang",
      "Sabato Marco Siniscalchi",
      "Chin-Hui Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.01271"
  },
  {
    "id": "arXiv:2104.01894",
    "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
    "abstract": "Comments: Accepted to INTERSPEECH 2021",
    "descriptor": "\nComments: Accepted to INTERSPEECH 2021\n",
    "authors": [
      "Ramon Sanabria",
      "Austin Waters",
      "Jason Baldridge"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.01894"
  },
  {
    "id": "arXiv:2104.02095",
    "title": "Analytic function approximation by path norm regularized deep networks",
    "abstract": "Analytic function approximation by path norm regularized deep networks",
    "descriptor": "",
    "authors": [
      "Aleksandr Beknazaryan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.02095"
  },
  {
    "id": "arXiv:2104.02207",
    "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition",
    "abstract": "Comments: Proc. of Interspeech 2021",
    "descriptor": "\nComments: Proc. of Interspeech 2021\n",
    "authors": [
      "Yuan Shangguan",
      "Rohit Prabhavalkar",
      "Hang Su",
      "Jay Mahadeokar",
      "Yangyang Shi",
      "Jiatong Zhou",
      "Chunyang Wu",
      "Duc Le",
      "Ozlem Kalinli",
      "Christian Fuegen",
      "Michael L. Seltzer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.02207"
  },
  {
    "id": "arXiv:2104.02397",
    "title": "ProsoBeast Prosody Annotation Tool",
    "abstract": "Comments: Accepted at Interspeech 2021",
    "descriptor": "\nComments: Accepted at Interspeech 2021\n",
    "authors": [
      "Branislav Gerazov",
      "Michael Wagner"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.02397"
  },
  {
    "id": "arXiv:2104.02469",
    "title": "Speaker Diarization using Two-pass Leave-One-Out Gaussian PLDA  Clustering of DNN Embeddings",
    "abstract": "Comments: 5 pages, 2 figures, accepted at INTERSPEECH 2021",
    "descriptor": "\nComments: 5 pages, 2 figures, accepted at INTERSPEECH 2021\n",
    "authors": [
      "Kiran Karra",
      "Alan McCree"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2104.02469"
  },
  {
    "id": "arXiv:2104.02518",
    "title": "An Initial Investigation for Detecting Partially Spoofed Audio",
    "abstract": "Comments: INTERSPEECH 2021",
    "descriptor": "\nComments: INTERSPEECH 2021\n",
    "authors": [
      "Lin Zhang",
      "Xin Wang",
      "Erica Cooper",
      "Junichi Yamagishi",
      "Jose Patino",
      "Nicholas Evans"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.02518"
  },
  {
    "id": "arXiv:2104.03279",
    "title": "Modern Hopfield Networks for Few- and Zero-Shot Reaction Template  Prediction",
    "abstract": "Comments: 14 pages + 12 pages appendix",
    "descriptor": "\nComments: 14 pages + 12 pages appendix\n",
    "authors": [
      "Philipp Seidl",
      "Philipp Renz",
      "Natalia Dyubankova",
      "Paulo Neves",
      "Jonas Verhoeven",
      "Marwin Segler",
      "J\u00f6rg K. Wegner",
      "Sepp Hochreiter",
      "G\u00fcnter Klambauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.03279"
  },
  {
    "id": "arXiv:2104.04975",
    "title": "Scalable Marginal Likelihood Estimation for Model Selection in Deep  Learning",
    "abstract": "Comments: ICML 2021",
    "descriptor": "\nComments: ICML 2021\n",
    "authors": [
      "Alexander Immer",
      "Matthias Bauer",
      "Vincent Fortuin",
      "Gunnar R\u00e4tsch",
      "Mohammad Emtiyaz Khan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.04975"
  },
  {
    "id": "arXiv:2104.05441",
    "title": "Unsuitability of NOTEARS for Causal Graph Discovery",
    "abstract": "Comments: 6 pages, 4 figures",
    "descriptor": "\nComments: 6 pages, 4 figures\n",
    "authors": [
      "Marcus Kaiser",
      "Maksim Sipos"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2104.05441"
  },
  {
    "id": "arXiv:2104.06104",
    "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of  Concept",
    "abstract": "Comments: accepted at Interspeech2021",
    "descriptor": "\nComments: accepted at Interspeech2021\n",
    "authors": [
      "Wei Zhou",
      "Albert Zeyer",
      "Andr\u00e9 Merboldt",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.06104"
  },
  {
    "id": "arXiv:2104.06346",
    "title": "A Distributed Mixed-Integer Framework to Stochastic Optimal Microgrid  Control",
    "abstract": "A Distributed Mixed-Integer Framework to Stochastic Optimal Microgrid  Control",
    "descriptor": "",
    "authors": [
      "Andrea Camisa",
      "Giuseppe Notarstefano"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.06346"
  },
  {
    "id": "arXiv:2104.08918",
    "title": "Motion Vector Extrapolation for Video Object Detection",
    "abstract": "Motion Vector Extrapolation for Video Object Detection",
    "descriptor": "",
    "authors": [
      "Julian True",
      "Naimul Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.08918"
  },
  {
    "id": "arXiv:2104.09106",
    "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition",
    "abstract": "Comments: accepted at Interspeech2021",
    "descriptor": "\nComments: accepted at Interspeech2021\n",
    "authors": [
      "Wei Zhou",
      "Mohammad Zeineldeen",
      "Zuoyun Zheng",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.09106"
  },
  {
    "id": "arXiv:2104.10611",
    "title": "Programmable 3D snapshot microscopy with Fourier convolutional networks",
    "abstract": "Comments: Make zebrafish Types A,B,C,D more clear",
    "descriptor": "\nComments: Make zebrafish Types A,B,C,D more clear\n",
    "authors": [
      "Diptodip Deb",
      "Zhenfei Jiao",
      "Alex B. Chen",
      "Misha B. Ahrens",
      "Kaspar Podgorski",
      "Srinivas C. Turaga"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.10611"
  },
  {
    "id": "arXiv:2104.11014",
    "title": "Network Space Search for Pareto-Efficient Spaces",
    "abstract": "Comments: CVPR2021 Workshop (Efficient Deep Learning for Computer Vision). Website: this http URL",
    "descriptor": "\nComments: CVPR2021 Workshop (Efficient Deep Learning for Computer Vision). Website: this http URL\n",
    "authors": [
      "Min-Fong Hong",
      "Hao-Yun Chen",
      "Min-Hung Chen",
      "Yu-Syuan Xu",
      "Hsien-Kai Kuo",
      "Yi-Min Tsai",
      "Hung-Jen Chen",
      "Kevin Jou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.11014"
  },
  {
    "id": "arXiv:2104.12672",
    "title": "A Novel Interaction-based Methodology Towards Explainable AI with Better  Understanding of Pneumonia Chest X-ray Images",
    "abstract": "A Novel Interaction-based Methodology Towards Explainable AI with Better  Understanding of Pneumonia Chest X-ray Images",
    "descriptor": "",
    "authors": [
      "Shaw-Hwa Lo",
      "Yiqiao Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2104.12672"
  },
  {
    "id": "arXiv:2104.13970",
    "title": "Personalized Keyphrase Detection using Speaker and Environment  Information",
    "abstract": "Personalized Keyphrase Detection using Speaker and Environment  Information",
    "descriptor": "",
    "authors": [
      "Rajeev Rikhye",
      "Quan Wang",
      "Qiao Liang",
      "Yanzhang He",
      "Ding Zhao",
      "Yiteng",
      "Huang",
      "Arun Narayanan",
      "Ian McGraw"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2104.13970"
  },
  {
    "id": "arXiv:2104.14937",
    "title": "Federated Learning with Fair Averaging",
    "abstract": "Comments: Accepted by IJCAI2021",
    "descriptor": "\nComments: Accepted by IJCAI2021\n",
    "authors": [
      "Zheng Wang",
      "Xiaoliang Fan",
      "Jianzhong Qi",
      "Chenglu Wen",
      "Cheng Wang",
      "Rongshan Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.14937"
  },
  {
    "id": "arXiv:2105.04051",
    "title": "Aggregating From Multiple Target-Shifted Sources",
    "abstract": "Aggregating From Multiple Target-Shifted Sources",
    "descriptor": "",
    "authors": [
      "Changjian Shui",
      "Zijian Li",
      "Jiaqi Li",
      "Christian Gagn\u00e9",
      "Charles Ling",
      "Boyu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.04051"
  },
  {
    "id": "arXiv:2105.05535",
    "title": "OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and  Robustness for Lexical Complexity Prediction",
    "abstract": "OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and  Robustness for Lexical Complexity Prediction",
    "descriptor": "",
    "authors": [
      "Yuki Taya",
      "Lis Kanashiro Pereira",
      "Fei Cheng",
      "Ichiro Kobayashi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.05535"
  },
  {
    "id": "arXiv:2105.05752",
    "title": "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained  Models into Speech Translation Encoders",
    "abstract": "Comments: ACL 2021",
    "descriptor": "\nComments: ACL 2021\n",
    "authors": [
      "Chen Xu",
      "Bojie Hu",
      "Yanyang Li",
      "Yuhao Zhang",
      "shen huang",
      "Qi Ju",
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2105.05752"
  },
  {
    "id": "arXiv:2105.07639",
    "title": "Traffic Scenario Clustering by Iterative Optimisation of Self-Supervised  Networks Using a Random Forest Activation Pattern Similarity",
    "abstract": "Comments: Accepted for IEEE Intelligent Vehicles 2021",
    "descriptor": "\nComments: Accepted for IEEE Intelligent Vehicles 2021\n",
    "authors": [
      "Lakshman Balasubramanian",
      "Jonas Wurst",
      "Michael Botsch",
      "Ke Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.07639"
  },
  {
    "id": "arXiv:2105.08692",
    "title": "Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team  Composition",
    "abstract": "Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team  Composition",
    "descriptor": "",
    "authors": [
      "Bo Liu",
      "Qiang Liu",
      "Peter Stone",
      "Animesh Garg",
      "Yuke Zhu",
      "Animashree Anandkumar"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.08692"
  },
  {
    "id": "arXiv:2105.10005",
    "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments",
    "abstract": "Comments: Accepted to IEEE ICIP 2021",
    "descriptor": "\nComments: Accepted to IEEE ICIP 2021\n",
    "authors": [
      "C.-H. Huck Yang",
      "Mohit Chhabra",
      "Y.-C. Liu",
      "Quan Kong",
      "Tomoaki Yoshinaga",
      "Tomokazu Murakami"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2105.10005"
  },
  {
    "id": "arXiv:2105.10329",
    "title": "Polyjuice: High-Performance Transactions via Learned Concurrency Control",
    "abstract": "Polyjuice: High-Performance Transactions via Learned Concurrency Control",
    "descriptor": "",
    "authors": [
      "Jiachen Wang",
      "Ding Ding",
      "Huan Wang",
      "Conrad Christensen",
      "Zhaoguo Wang",
      "Haibo Chen",
      "Jinyang Li"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2105.10329"
  },
  {
    "id": "arXiv:2105.11088",
    "title": "Towards Book Cover Design via Layout Graphs",
    "abstract": "Comments: Accepted at ICDAR2021",
    "descriptor": "\nComments: Accepted at ICDAR2021\n",
    "authors": [
      "Wensheng Zhang",
      "Yan Zheng",
      "Taiga Miyazono",
      "Seiichi Uchida",
      "Brian Kenji Iwana"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.11088"
  },
  {
    "id": "arXiv:2105.13493",
    "title": "Efficient and Accurate Gradients for Neural SDEs",
    "abstract": "Comments: Submitted to NeurIPS 2021",
    "descriptor": "\nComments: Submitted to NeurIPS 2021\n",
    "authors": [
      "Patrick Kidger",
      "James Foster",
      "Xuechen Li",
      "Terry Lyons"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.13493"
  },
  {
    "id": "arXiv:2105.14713",
    "title": "1$\\times$N Block Pattern for Network Sparsity",
    "abstract": "1$\\times$N Block Pattern for Network Sparsity",
    "descriptor": "",
    "authors": [
      "Mingbao Lin",
      "Yuchao Li",
      "Yuxin Zhang",
      "Bohong Chen",
      "Fei Chao",
      "Mengdi Wang",
      "Shen Li",
      "Jun Yang",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.14713"
  },
  {
    "id": "arXiv:2105.14901",
    "title": "User Experience Design for E-Voting: How mental models align with  security mechanisms",
    "abstract": "Comments: E-Vote-ID 2019 TalTech Proceedings",
    "descriptor": "\nComments: E-Vote-ID 2019 TalTech Proceedings\n",
    "authors": [
      "Marie-Laure Zollinger",
      "Verena Distler",
      "Peter B. Roenne",
      "Peter Y. A. Ryan",
      "Carine Lallemand",
      "Vincent Koenig"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2105.14901"
  },
  {
    "id": "arXiv:2106.00538",
    "title": "A Markov Reward Process-Based Approach to Spatial Interpolation",
    "abstract": "Comments: This is a Master Thesis for the Computer Science MSc programme at Leiden University",
    "descriptor": "\nComments: This is a Master Thesis for the Computer Science MSc programme at Leiden University\n",
    "authors": [
      "Laurens Arp"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.00538"
  },
  {
    "id": "arXiv:2106.00644",
    "title": "A normal form for grid forming power grid components",
    "abstract": "A normal form for grid forming power grid components",
    "descriptor": "",
    "authors": [
      "Raphael Kogler",
      "Anton Plietzsch",
      "Paul Schultz",
      "Frank Hellmann"
    ],
    "subjectives": [
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.00644"
  },
  {
    "id": "arXiv:2106.00774",
    "title": "Optimizing Functionals on the Space of Probabilities with Input Convex  Neural Networks",
    "abstract": "Optimizing Functionals on the Space of Probabilities with Input Convex  Neural Networks",
    "descriptor": "",
    "authors": [
      "David Alvarez-Melis",
      "Yair Schiff",
      "Youssef Mroueh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.00774"
  },
  {
    "id": "arXiv:2106.02182",
    "title": "Self-supervised Dialogue Learning for Spoken Conversational Question  Answering",
    "abstract": "Comments: To Appear Interspeech 2021",
    "descriptor": "\nComments: To Appear Interspeech 2021\n",
    "authors": [
      "Nuo Chen",
      "Chenyu You",
      "Yuexian Zou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.02182"
  },
  {
    "id": "arXiv:2106.03152",
    "title": "Technical Report: Temporal Aggregate Representations",
    "abstract": "Technical Report: Temporal Aggregate Representations",
    "descriptor": "",
    "authors": [
      "Fadime Sener",
      "Dibyadip Chatterjee",
      "Angela Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03152"
  },
  {
    "id": "arXiv:2106.03456",
    "title": "Are best approximations really better than Chebyshev?",
    "abstract": "Comments: 23 pages",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Haiyong Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.03456"
  },
  {
    "id": "arXiv:2106.03594",
    "title": "Learning Combinatorial Node Labeling Algorithms",
    "abstract": "Learning Combinatorial Node Labeling Algorithms",
    "descriptor": "",
    "authors": [
      "Lukas Gianinazzi",
      "Maximilian Fries",
      "Nikoli Dryden",
      "Tal Ben-Nun",
      "Maciej Besta",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03594"
  },
  {
    "id": "arXiv:2106.03640",
    "title": "Making EfficientNet More Efficient: Exploring Batch-Independent  Normalization, Group Convolutions and Reduced Resolution Training",
    "abstract": "Making EfficientNet More Efficient: Exploring Batch-Independent  Normalization, Group Convolutions and Reduced Resolution Training",
    "descriptor": "",
    "authors": [
      "Dominic Masters",
      "Antoine Labatie",
      "Zach Eaton-Rosen",
      "Carlo Luschi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03640"
  },
  {
    "id": "arXiv:2106.04312",
    "title": "Speech BERT Embedding For Improving Prosody in Neural TTS",
    "abstract": "Speech BERT Embedding For Improving Prosody in Neural TTS",
    "descriptor": "",
    "authors": [
      "Liping Chen",
      "Yan Deng",
      "Xi Wang",
      "Frank K. Soong",
      "Lei He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2106.04312"
  },
  {
    "id": "arXiv:2106.04554",
    "title": "A Survey of Transformers",
    "abstract": "A Survey of Transformers",
    "descriptor": "",
    "authors": [
      "Tianyang Lin",
      "Yuxin Wang",
      "Xiangyang Liu",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.04554"
  },
  {
    "id": "arXiv:2106.04735",
    "title": "Validating Static Warnings via Testing Code Fragments",
    "abstract": "Comments: In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis July 11 to 17, 2021, Denmark. 13 pages",
    "descriptor": "\nComments: In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis July 11 to 17, 2021, Denmark. 13 pages\n",
    "authors": [
      "Ashwin Kallingal Joshy",
      "Xueyuan Chen",
      "Benjamin Steenhoek",
      "Wei Le"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.04735"
  },
  {
    "id": "arXiv:2106.04748",
    "title": "Online Optimization in Games via Control Theory: Connecting Regret,  Passivity and Poincar\u00e9 Recurrence",
    "abstract": "Comments: In ICML 2021",
    "descriptor": "\nComments: In ICML 2021\n",
    "authors": [
      "Yun Kuen Cheung",
      "Georgios Piliouras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04748"
  },
  {
    "id": "arXiv:2106.05387",
    "title": "Eye of the Beholder: Improved Relation Generalization for Text-based  Reinforcement Learning Agents",
    "abstract": "Eye of the Beholder: Improved Relation Generalization for Text-based  Reinforcement Learning Agents",
    "descriptor": "",
    "authors": [
      "Keerthiram Murugesan",
      "Subhajit Chaudhury",
      "Kartik Talamadupula"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.05387"
  },
  {
    "id": "arXiv:2106.05452",
    "title": "Nonlinear mixed-dimension model for embedded tubular networks with  application to root water uptake",
    "abstract": "Comments: 34 pages, 16 figures",
    "descriptor": "\nComments: 34 pages, 16 figures\n",
    "authors": [
      "Timo Koch",
      "Hanchuan Wu",
      "Martin Schneider"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2106.05452"
  },
  {
    "id": "arXiv:2106.05470",
    "title": "Automated Self-Supervised Learning for Graphs",
    "abstract": "Automated Self-Supervised Learning for Graphs",
    "descriptor": "",
    "authors": [
      "Wei Jin",
      "Xiaorui Liu",
      "Xiangyu Zhao",
      "Yao Ma",
      "Neil Shah",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.05470"
  },
  {
    "id": "arXiv:2106.05522",
    "title": "A Mathematical Foundation for Robust Machine Learning based on  Bias-Variance Trade-off",
    "abstract": "A Mathematical Foundation for Robust Machine Learning based on  Bias-Variance Trade-off",
    "descriptor": "",
    "authors": [
      "Ou Wu",
      "Weiyao Zhu",
      "Yingjun Deng",
      "Haixiang Zhang",
      "Qinghu Hou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05522"
  },
  {
    "id": "arXiv:2106.05564",
    "title": "FRI-TEM: Time Encoding Sampling of Finite-Rate-of-Innovation Signals",
    "abstract": "Comments: 11 pages, 9 figures",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Hila Naaman",
      "Satish Mulleti",
      "Yonina C. Eldar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.05564"
  },
  {
    "id": "arXiv:2106.05746",
    "title": "The 2021 Hotel-ID to Combat Human Trafficking Competition Dataset",
    "abstract": "Comments: CVPR 2021 Workshop on Fine-Grained Visual Categorization (FGVC)",
    "descriptor": "\nComments: CVPR 2021 Workshop on Fine-Grained Visual Categorization (FGVC)\n",
    "authors": [
      "Rashmi Kamath",
      "Gregory Rolwes",
      "Samuel Black",
      "Abby Stylianou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.05746"
  },
  {
    "id": "arXiv:2106.05779",
    "title": "Deep Implicit Surface Point Prediction Networks",
    "abstract": "Comments: 22 pages, 17 figures",
    "descriptor": "\nComments: 22 pages, 17 figures\n",
    "authors": [
      "Rahul Venkatesh",
      "Tejan Karmali",
      "Sarthak Sharma",
      "Aurobrata Ghosh",
      "R. Venkatesh Babu",
      "L\u00e1szl\u00f3 A. Jeni",
      "Maneesh Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.05779"
  },
  {
    "id": "arXiv:2106.05907",
    "title": "Disentangled Attention as Intrinsic Regularization for Bimanual  Multi-Object Manipulation",
    "abstract": "Comments: Webpage: this https URL",
    "descriptor": "\nComments: Webpage: this https URL\n",
    "authors": [
      "Minghao Zhang",
      "Pingcheng Jian",
      "Yi Wu",
      "Huazhe Xu",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05907"
  },
  {
    "id": "arXiv:2106.06044",
    "title": "Convergence and Alignment of Gradient Descent with Random Back  Propagation Weights",
    "abstract": "Comments: 33 pages",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Ganlin Song",
      "Ruitu Xu",
      "John Lafferty"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06044"
  },
  {
    "id": "arXiv:2106.06170",
    "title": "Taylor Expansion of Discount Factors",
    "abstract": "Comments: Accepted at International Conference of Machine Learning (ICML), 2021",
    "descriptor": "\nComments: Accepted at International Conference of Machine Learning (ICML), 2021\n",
    "authors": [
      "Yunhao Tang",
      "Mark Rowland",
      "R\u00e9mi Munos",
      "Michal Valko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06170"
  },
  {
    "id": "arXiv:2106.06232",
    "title": "GDI: Rethinking What Makes Reinforcement Learning Different From  Supervised Learning",
    "abstract": "GDI: Rethinking What Makes Reinforcement Learning Different From  Supervised Learning",
    "descriptor": "",
    "authors": [
      "Jiajun Fan",
      "Changnan Xiao",
      "Yue Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.06232"
  },
  {
    "id": "arXiv:2106.06404",
    "title": "Multilevel Spectral Domain Decomposition",
    "abstract": "Comments: 25 pages",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Peter Bastian",
      "Robert Scheichl",
      "Linus Seelinger",
      "Arne Strehlow"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.06404"
  },
  {
    "id": "arXiv:2106.06411",
    "title": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers",
    "abstract": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers",
    "descriptor": "",
    "authors": [
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Dilek Hakkani-T\u00fcr"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06411"
  },
  {
    "id": "arXiv:2106.06885",
    "title": "Online Learning with Optimism and Delay",
    "abstract": "Comments: ICML 2021. 9 pages of main paper and 26 pages of appendix text",
    "descriptor": "\nComments: ICML 2021. 9 pages of main paper and 26 pages of appendix text\n",
    "authors": [
      "Genevieve Flaspohler",
      "Francesco Orabona",
      "Judah Cohen",
      "Soukayna Mouatadid",
      "Miruna Oprescu",
      "Paulo Orenstein",
      "Lester Mackey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06885"
  },
  {
    "id": "arXiv:2106.06922",
    "title": "Cross-sentence Neural Language Models for Conversational Speech  Recognition",
    "abstract": "Comments: The wordings and organizations of the draft still have room for improvement",
    "descriptor": "\nComments: The wordings and organizations of the draft still have room for improvement\n",
    "authors": [
      "Shih-Hsuan Chiu",
      "Tien-Hong Lo",
      "Berlin Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.06922"
  },
  {
    "id": "arXiv:2106.06971",
    "title": "NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image  Enhancement",
    "abstract": "Comments: 14 pages, 11 figures",
    "descriptor": "\nComments: 14 pages, 11 figures\n",
    "authors": [
      "Hao Hou",
      "Yingkun Hou",
      "Yuxuan Shi",
      "Benzheng Wei",
      "Jun Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.06971"
  },
  {
    "id": "arXiv:2106.06976",
    "title": "Game of GANs: Game Theoretical Models for Generative Adversarial  Networks",
    "abstract": "Comments: 16 pages, 5 Tables, 2 Figures, Review paper",
    "descriptor": "\nComments: 16 pages, 5 Tables, 2 Figures, Review paper\n",
    "authors": [
      "Monireh Mohebbi Moghadam",
      "Bahar Boroomand",
      "Mohammad Jalali",
      "Arman Zareian",
      "Alireza DaeiJavad",
      "Mohammad Hossein Manshaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.06976"
  },
  {
    "id": "arXiv:2106.06988",
    "title": "NDPNet: A novel non-linear data projection network for few-shot  fine-grained image classification",
    "abstract": "NDPNet: A novel non-linear data projection network for few-shot  fine-grained image classification",
    "descriptor": "",
    "authors": [
      "Weichuan Zhang",
      "Xuefang Liu",
      "Zhe Xue",
      "Yongsheng Gao",
      "Changming Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06988"
  },
  {
    "id": "arXiv:2106.07049",
    "title": "Weakly-supervised High-resolution Segmentation of Mammography Images for  Breast Cancer Diagnosis",
    "abstract": "Comments: The last two authors contributed equally. Accepted to Medical Imaging with Deep Learning (MIDL) 2021",
    "descriptor": "\nComments: The last two authors contributed equally. Accepted to Medical Imaging with Deep Learning (MIDL) 2021\n",
    "authors": [
      "Kangning Liu",
      "Yiqiu Shen",
      "Nan Wu",
      "Jakub Ch\u0142\u0119dowski",
      "Carlos Fernandez-Granda",
      "Krzysztof J. Geras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07049"
  },
  {
    "id": "arXiv:2106.07075",
    "title": "A baseline for semi-supervised learning of efficient semantic  segmentation models",
    "abstract": "A baseline for semi-supervised learning of efficient semantic  segmentation models",
    "descriptor": "",
    "authors": [
      "Ivan Grubi\u0161i\u0107",
      "Marin Or\u0161i\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07075"
  },
  {
    "id": "arXiv:2106.07116",
    "title": "The Power of Randomization: Efficient and Effective Algorithms for  Constrained Submodular Maximization",
    "abstract": "Comments: Part of the contribution appears in ICML2021",
    "descriptor": "\nComments: Part of the contribution appears in ICML2021\n",
    "authors": [
      "Kai Han",
      "Shuang Cui",
      "Tianshuai Zhu",
      "Jing Tang",
      "Benwei Wu",
      "He Huang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.07116"
  },
  {
    "id": "arXiv:2106.07139",
    "title": "Pre-Trained Models: Past, Present and Future",
    "abstract": "Pre-Trained Models: Past, Present and Future",
    "descriptor": "",
    "authors": [
      "Xu Han",
      "Zhengyan Zhang",
      "Ning Ding",
      "Yuxian Gu",
      "Xiao Liu",
      "Yuqi Huo",
      "Jiezhong Qiu",
      "Liang Zhang",
      "Wentao Han",
      "Minlie Huang",
      "Qin Jin",
      "Yanyan Lan",
      "Yang Liu",
      "Zhiyuan Liu",
      "Zhiwu Lu",
      "Xipeng Qiu",
      "Ruihua Song",
      "Jie Tang",
      "Ji-Rong Wen",
      "Jinhui Yuan",
      "Wayne Xin Zhao",
      "Jun Zhu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07139"
  },
  {
    "id": "arXiv:2106.07306",
    "title": "Constraining Linear-chain CRFs to Regular Languages",
    "abstract": "Constraining Linear-chain CRFs to Regular Languages",
    "descriptor": "",
    "authors": [
      "Sean Papay",
      "Roman Klinger",
      "Sebastian Pad\u00f3"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07306"
  },
  {
    "id": "arXiv:2106.07333",
    "title": "Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class  Classification",
    "abstract": "Comments: This work was carried out as a collaboration between the Department of Computer Science and Engineering -- the University of Dhaka and the National Institute of Neuroscience (NINS), Bangladesh. We created a novel neurological discord dataset of 37 disease categories",
    "descriptor": "\nComments: This work was carried out as a collaboration between the Department of Computer Science and Engineering -- the University of Dhaka and the National Institute of Neuroscience (NINS), Bangladesh. We created a novel neurological discord dataset of 37 disease categories\n",
    "authors": [
      "Yusuf Brima",
      "Mossadek Hossain Kamal Tushar",
      "Upama Kabir",
      "Tariqul Islam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07333"
  },
  {
    "id": "arXiv:2106.07400",
    "title": "Determinantal Beam Search",
    "abstract": "Determinantal Beam Search",
    "descriptor": "",
    "authors": [
      "Clara Meister",
      "Martina Forster",
      "Ryan Cotterell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07400"
  },
  {
    "id": "arXiv:2106.07417",
    "title": "Online Estimation of Resource Overload Risk in 5G Multi-Tenancy Network",
    "abstract": "Comments: To appear at ESREL 2021",
    "descriptor": "\nComments: To appear at ESREL 2021\n",
    "authors": [
      "Yasameen Shihab Hamad",
      "Bin Han",
      "Osman Nuri ucan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.07417"
  },
  {
    "id": "arXiv:2106.07577",
    "title": "F-T-LSTM based Complex Network for Joint Acoustic Echo Cancellation and  Speech Enhancement",
    "abstract": "Comments: Accepted by Interspeech 2021",
    "descriptor": "\nComments: Accepted by Interspeech 2021\n",
    "authors": [
      "Shimin Zhang",
      "Yuxiang Kong",
      "Shubo Lv",
      "Yanxin Hu",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07577"
  },
  {
    "id": "arXiv:2106.07583",
    "title": "Biomedical Entity Linking with Contrastive Context Matching",
    "abstract": "Biomedical Entity Linking with Contrastive Context Matching",
    "descriptor": "",
    "authors": [
      "Shogo Ujiie",
      "Hayate Iso",
      "Eiji Aramaki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07583"
  }
]