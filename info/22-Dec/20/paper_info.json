[
  {
    "id": "arXiv:2212.08656",
    "title": "MTMD: Multi-Scale Temporal Memory Learning and Efficient Debiasing  Framework for Stock Trend Forecasting",
    "abstract": "Recently, machine learning methods have shown the prospects of stock trend\nforecasting. However, the volatile and dynamic nature of the stock market makes\nit difficult to directly apply machine learning techniques. Previous methods\nusually use the temporal information of historical stock price patterns to\npredict future stock trends, but the multi-scale temporal dependence of\nfinancial data and stable trading opportunities are still difficult to capture.\nThe main problem can be ascribed to the challenge of recognizing the patterns\nof real profit signals from noisy information. In this paper, we propose a\nframework called Multiscale Temporal Memory Learning and Efficient Debiasing\n(MTMD). Specifically, through self-similarity, we design a learnable embedding\nwith external attention as memory block, in order to reduce the noise issues\nand enhance the temporal consistency of the model. This framework not only\naggregates comprehensive local information in each timestamp, but also\nconcentrates the global important historical patterns in the whole time stream.\nMeanwhile, we also design the graph network based on global and local\ninformation to adaptively fuse the heterogeneous multi-scale information.\nExtensive ablation studies and experiments demonstrate that MTMD outperforms\nthe state-of-the-art approaches by a significant margin on the benchmark\ndatasets. The source code of our proposed method is available at\nhttps://github.com/MingjieWang0606/MDMT-Public.",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Mingjie Wang",
      "Mingze Zhang",
      "Jianxiong Guo",
      "Weijia Jia"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2212.08656"
  },
  {
    "id": "arXiv:2212.08658",
    "title": "IMAGINE: An Integrated Model of Artificial Intelligence-Mediated  Communication Effects",
    "abstract": "Artificial Intelligence (AI) is transforming all fields of knowledge and\nproduction. From surgery, autonomous driving, to image and video creation, AI\nseems to make possible hitherto unimaginable processes of automation and\nefficient creation. Media and communication are not an exception, and we are\ncurrently witnessing the dawn of powerful AI tools capable of creating artistic\nimages from simple keywords, or to capture emotions from facial expression.\nThese examples may be only the beginning of what can be in the future the\nengines for automatic AI real time creation of media content linked to the\nemotional and behavioural responses of individuals. Although it may seem we are\nstill far from there, it is already the moment to adapt our theories about\nmedia to the hypothetical scenario in which content production can be done\nwithout human intervention, and governed by the controlled any reactions of the\nindividual to the exposure to media content. Following that, I propose the\ndefinition of the Integrated Model of Artificial Intelligence-Mediated\nCommunication Effects (IMAGINE), and its consequences on the way we understand\nmedia evolution (Scolari, 2012) and we think about media effects (Potter,\n2010). The conceptual framework proposed is aimed to help scholars theorizing\nand doing research in a scenario of continuous real-time connection between AI\nmeasurement of people's responses to media, and the AI creation of content,\nwith the objective of optimizing and maximizing the processes of influence.\nParasocial interaction and real-time beautification are used as examples to\nmodel the functioning of the IMAGINE process.",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Frederic Guerrero-Sole"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2212.08658"
  },
  {
    "id": "arXiv:2212.08659",
    "title": "A Hierarchical Framework for Collaborative Artificial Intelligence",
    "abstract": "We propose a hierarchical framework for collaborative intelligent systems.\nThis framework organizes research challenges based on the nature of the\ncollaborative activity and the information that must be shared, with each level\nbuilding on capabilities provided by lower levels. We review research paradigms\nat each level, with a description of classical engineering-based approaches and\nmodern alternatives based on machine learning, illustrated with a running\nexample using a hypothetical personal service robot. We discuss cross-cutting\nissues that occur at all levels, focusing on the problem of communicating and\nsharing comprehension, the role of explanation and the social nature of\ncollaboration. We conclude with a summary of research challenges and a\ndiscussion of the potential for economic and societal impact provided by\ntechnologies that enhance human abilities and empower people and society\nthrough collaboration with Intelligent Systems.",
    "descriptor": "",
    "authors": [
      "James L. Crowley",
      "Jo\u00eblle L Coutaz",
      "Jasmin Grosinger",
      "Javier V\u00e1zquez-Salceda",
      "Cecilio Angulo",
      "Alberto Sanfeliu",
      "Luca Iocchi",
      "Anthony G. Cohn"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08659"
  },
  {
    "id": "arXiv:2212.08660",
    "title": "Learning Inter-Annual Flood Loss Risk Models From Historical Flood  Insurance Claims and Extreme Rainfall Data",
    "abstract": "Flooding is one of the most disastrous natural hazards, responsible for\nsubstantial economic losses. A predictive model for flood-induced financial\ndamages is useful for many applications such as climate change adaptation\nplanning and insurance underwriting. This research assesses the predictive\ncapability of regressors constructed on the National Flood Insurance Program\n(NFIP) dataset using neural networks (Conditional Generative Adversarial\nNetworks), decision trees (Extreme Gradient Boosting), and kernel-based\nregressors (Gaussian Process). The assessment highlights the most informative\npredictors for regression. The distribution for claims amount inference is\nmodeled with a Burr distribution permitting the introduction of a bias\ncorrection scheme and increasing the regressor's predictive capability. Aiming\nto study the interaction with physical variables, we incorporate Daymet\nrainfall estimation to NFIP as an additional predictor. A study on the coastal\ncounties in the eight US South-West states resulted in an $R^2=0.807$. Further\nanalysis of 11 counties with a significant number of claims in the NFIP dataset\nreveals that Extreme Gradient Boosting provides the best results, that bias\ncorrection significantly improves the similarity with the reference\ndistribution, and that the rainfall predictor strengthens the regressor\nperformance.",
    "descriptor": "\nComments: 10 pages, 7 figures, 2 tables. An appendix with one more table and two figures\n",
    "authors": [
      "Joaquin Salas",
      "Anamitra Saha",
      "Sai Ravela"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.08660"
  },
  {
    "id": "arXiv:2212.08661",
    "title": "EffMulti: Efficiently Modeling Complex Multimodal Interactions for  Emotion Analysis",
    "abstract": "Humans are skilled in reading the interlocutor's emotion from multimodal\nsignals, including spoken words, simultaneous speech, and facial expressions.\nIt is still a challenge to effectively decode emotions from the complex\ninteractions of multimodal signals. In this paper, we design three kinds of\nmultimodal latent representations to refine the emotion analysis process and\ncapture complex multimodal interactions from different views, including a\nintact three-modal integrating representation, a modality-shared\nrepresentation, and three modality-individual representations. Then, a\nmodality-semantic hierarchical fusion is proposed to reasonably incorporate\nthese representations into a comprehensive interaction representation. The\nexperimental results demonstrate that our EffMulti outperforms the\nstate-of-the-art methods. The compelling performance benefits from its\nwell-designed framework with ease of implementation, lower computing\ncomplexity, and less trainable parameters.",
    "descriptor": "\nComments: 6 pages,1 figure\n",
    "authors": [
      "Feng Qiu",
      "Chengyang Xie",
      "Yu Ding",
      "Wanzeng Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08661"
  },
  {
    "id": "arXiv:2212.08663",
    "title": "Randomized Quantization for Data Agnostic Representation Learning",
    "abstract": "Self-supervised representation learning follows a paradigm of withholding\nsome part of the data and tasking the network to predict it from the remaining\npart. Towards this end, masking has emerged as a generic and powerful tool\nwhere content is withheld along the sequential dimension, e.g., spatial in\nimages, temporal in audio, and syntactic in language. In this paper, we explore\nthe orthogonal channel dimension for generic data augmentation. The data for\neach channel is quantized through a non-uniform quantizer, with the quantized\nvalue sampled randomly within randomly sampled quantization bins. From another\nperspective, quantization is analogous to channel-wise masking, as it removes\nthe information within each bin, but preserves the information across bins. We\napply the randomized quantization in conjunction with sequential augmentations\non self-supervised contrastive models. This generic approach achieves results\non par with modality-specific augmentation on vision tasks, and\nstate-of-the-art results on 3D point clouds as well as on audio. We also\ndemonstrate this method to be applicable for augmenting intermediate embeddings\nin a deep neural network on the comprehensive DABS benchmark which is comprised\nof various data modalities. Code is availabel at\nthis http URL",
    "descriptor": "\nComments: Technical Report\n",
    "authors": [
      "Huimin Wu",
      "Chenyang Lei",
      "Xiao Sun",
      "Peng-Shuai Wang",
      "Qifeng Chen",
      "Kwang-Ting Cheng",
      "Stephen Lin",
      "Zhirong Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08663"
  },
  {
    "id": "arXiv:2212.08664",
    "title": "On the Optimum Scenarios for Single Row Equidistant Facility Layout  Problem",
    "abstract": "Single Row Equidistant Facility Layout Problem SREFLP is with an NP-Hard\nnature to mimic material handling costs along with equally spaced straight-line\nfacilities layout. Based on literature, it is obvious that efforts of\nresearchers for solving SREFLP turn from exact methods into release the running\ntime tracing the principle of the approximate methods in time race, regardless\nsearching their time complexity release in conjunction with a provable quality\nof solutions. This study focuses on Lower bounding LB techniques as an\nindependent potential solution tool for SREFLP. In particular, Best-known\nSREFLP LBs are reported from literature and significantly LBs optimum scenarios\nare highlighted. Initially, one gap of the SREFLP bidirectional LB is enhanced.\nFrom the integration between the enhanced LB and the best-known Gilmore-Lawler\nGL bounding, a new SREFLP optimum scenario is provided. Further improvements to\nGLB lead to guarantee an exact Shipping/Receiving Facility assignment and\npropose a conjecture of at most 4/3 approximation scheme for SREFLP.",
    "descriptor": "",
    "authors": [
      "Shrouq Gamal",
      "Ahmed A. Hawam",
      "Ahmed M. El-Kassas"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2212.08664"
  },
  {
    "id": "arXiv:2212.08665",
    "title": "Hard Sample Aware Network for Contrastive Deep Graph Clustering",
    "abstract": "Contrastive deep graph clustering, which aims to divide nodes into disjoint\ngroups via contrastive mechanisms, is a challenging research spot. Among the\nrecent works, hard sample mining-based algorithms have achieved great attention\nfor their promising performance. However, we find that the existing hard sample\nmining methods have two problems as follows. 1) In the hardness measurement,\nthe important structural information is overlooked for similarity calculation,\ndegrading the representativeness of the selected hard negative samples. 2)\nPrevious works merely focus on the hard negative sample pairs while neglecting\nthe hard positive sample pairs. Nevertheless, samples within the same cluster\nbut with low similarity should also be carefully learned. To solve the\nproblems, we propose a novel contrastive deep graph clustering method dubbed\nHard Sample Aware Network (HSAN) by introducing a comprehensive similarity\nmeasure criterion and a general dynamic sample weighing strategy. Concretely,\nin our algorithm, the similarities between samples are calculated by\nconsidering both the attribute embeddings and the structure embeddings, better\nrevealing sample relationships and assisting hardness measurement. Moreover,\nunder the guidance of the carefully collected high-confidence clustering\ninformation, our proposed weight modulating function will first recognize the\npositive and negative samples and then dynamically up-weight the hard sample\npairs while down-weighting the easy ones. In this way, our method can mine not\nonly the hard negative samples but also the hard positive sample, thus\nimproving the discriminative capability of the samples further. Extensive\nexperiments and analyses demonstrate the superiority and effectiveness of our\nproposed method.",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Yue Liu",
      "Xihong Yang",
      "Sihang Zhou",
      "Xinwang Liu",
      "Zhen Wang",
      "Ke Liang",
      "Wenxuan Tu",
      "Liang Li",
      "Jingcan Duan",
      "Cancan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08665"
  },
  {
    "id": "arXiv:2212.08681",
    "title": "Plansformer: Generating Symbolic Plans using Transformers",
    "abstract": "Large Language Models (LLMs) have been the subject of active research,\nsignificantly advancing the field of Natural Language Processing (NLP). From\nBERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural\nlanguage tasks such as question answering, summarization, and text generation.\nMany ongoing efforts focus on understanding LLMs' capabilities, including their\nknowledge of the world, syntax, and semantics. However, extending the textual\nprowess of LLMs to symbolic reasoning has been slow and predominantly focused\non tackling problems related to the mathematical field. In this paper, we\nexplore the use of LLMs for automated planning - a branch of AI concerned with\nthe realization of action sequences (plans) to achieve a goal, typically\nexecuted by intelligent agents, autonomous robots, and unmanned vehicles. We\nintroduce Plansformer; an LLM fine-tuned on planning problems and capable of\ngenerating plans with favorable behavior in terms of correctness and length\nwith reduced knowledge-engineering efforts. We also demonstrate the\nadaptability of Plansformer in solving different planning domains with varying\ncomplexities, owing to the transfer learning abilities of LLMs. For one\nconfiguration of Plansformer, we achieve ~97% valid plans, out of which ~95%\nare optimal for Towers of Hanoi - a puzzle-solving domain.",
    "descriptor": "\nComments: 44 pages including supplementary material\n",
    "authors": [
      "Vishal Pallagani",
      "Bharath Muppasani",
      "Keerthiram Murugesan",
      "Francesca Rossi",
      "Lior Horesh",
      "Biplav Srivastava",
      "Francesco Fabiano",
      "Andrea Loreggia"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08681"
  },
  {
    "id": "arXiv:2212.08686",
    "title": "The Impact of Symbolic Representations on In-context Learning for  Few-shot Reasoning",
    "abstract": "Pre-trained language models (LMs) have shown remarkable reasoning performance\nusing explanations (or ``chain-of-thought'' (CoT)) for in-context learning. On\nthe other hand, these reasoning tasks are usually presumed to be more\napproachable for symbolic programming. To make progress towards understanding\nin-context learning, we curate synthetic datasets containing equivalent\n(natural, symbolic) data pairs, where symbolic examples contain first-order\nlogic rules and predicates from knowledge bases (KBs). Then we revisit\nneuro-symbolic approaches and use Language Models as Logic Programmer (LMLP)\nthat learns from demonstrations containing logic rules and corresponding\nexamples to iteratively reason over KBs, recovering Prolog's backward chaining\nalgorithm. Comprehensive experiments are included to systematically compare\nLMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more\nthan 25% higher accuracy than CoT on length generalization benchmarks even with\nfewer parameters.",
    "descriptor": "\nComments: NeurIPS Neuro Causal and Symbolic AI Workshop, 2022\n",
    "authors": [
      "Hanlin Zhang",
      "Yi-Fan Zhang",
      "Li Erran Li",
      "Eric Xing"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08686"
  },
  {
    "id": "arXiv:2212.08688",
    "title": "Assessment of user-interaction strategies for neurosurgical data  navigation and annotation in virtual reality",
    "abstract": "While virtual-reality (VR) has shown great promise in radiological tasks,\neffective user-interaction strategies that can improve efficiency and\nergonomics are still under-explored and systematic evaluations of VR\ninteraction techniques in the context of complex anatomical models are rare.\nTherefore, our study aims to identify the most effective interaction techniques\nfor two common neurosurgical planning tasks in VR (point annotation and\nnote-taking) from the state-of-the-arts, and propose a novel technique for\nefficient sub-volume selection necessary in neuroanatomical navigation. We\nassessed seven user-interaction methods with multiple input modalities (gaze,\nhead motion, controller, and voice) for point placement and note-taking in the\ncontext of annotating brain aneurysms for cerebrovascular surgery. Furthermore,\nwe proposed and evaluated a novel technique, called magnified selection diorama\n(Maserama) for easy navigation and selection of complex 3D anatomies in VR.\nBoth quantitative and semi-quantitative (i.e., NASA Task Load Index) metrics\nwere employed through user studies to reveal the performance of each\ninteraction scheme in terms of accuracy, efficiency, and usability. Our\nevaluations demonstrated that controller-based interaction is preferred over\neye-tracking-based methods for point placement while voice recording and\nvirtual keyboard typing are better than freehand writing for note-taking.\nFurthermore, our new Maserama sub-volume selection technique was proven to be\nhighly efficient and easy-to-use. Our study is the first to provide a\nsystematic assessment of existing and new VR interaction schemes for\nneurosurgical data navigation and annotation. It offers valuable insights and\ntools to guide the design of future VR systems for radiological and surgical\napplications.",
    "descriptor": "",
    "authors": [
      "Owen Hellum",
      "Marta Kersten-Oertel",
      "Yiming Xiao"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2212.08688"
  },
  {
    "id": "arXiv:2212.08689",
    "title": "TopoImb: Toward Topology-level Imbalance in Learning from Graphs",
    "abstract": "Graph serves as a powerful tool for modeling data that has an underlying\nstructure in non-Euclidean space, by encoding relations as edges and entities\nas nodes. Despite developments in learning from graph-structured data over the\nyears, one obstacle persists: graph imbalance. Although several attempts have\nbeen made to target this problem, they are limited to considering only\nclass-level imbalance. In this work, we argue that for graphs, the imbalance is\nlikely to exist at the sub-class topology group level. Due to the flexibility\nof topology structures, graphs could be highly diverse, and learning a\ngeneralizable classification boundary would be difficult. Therefore, several\nmajority topology groups may dominate the learning process, rendering others\nunder-represented. To address this problem, we propose a new framework\n{\\method} and design (1 a topology extractor, which automatically identifies\nthe topology group for each instance with explicit memory cells, (2 a training\nmodulator, which modulates the learning process of the target GNN model to\nprevent the case of topology-group-wise under-representation. {\\method} can be\nused as a key component in GNN models to improve their performances under the\ndata imbalance setting. Analyses on both topology-level imbalance and the\nproposed {\\method} are provided theoretically, and we empirically verify its\neffectiveness with both node-level and graph-level classification as the target\ntasks.",
    "descriptor": "\nComments: Accepted by LOG22\n",
    "authors": [
      "Tianxiang Zhao",
      "Dongsheng Luo",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08689"
  },
  {
    "id": "arXiv:2212.08698",
    "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion  Models",
    "abstract": "Generative models have been widely studied in computer vision. Recently,\ndiffusion models have drawn substantial attention due to the high quality of\ntheir generated images. A key desired property of image generative models is\nthe ability to disentangle different attributes, which should enable\nmodification towards a style without changing the semantic content, and the\nmodification parameters should generalize to different images. Previous studies\nhave found that generative adversarial networks (GANs) are inherently endowed\nwith such disentanglement capability, so they can perform disentangled image\nediting without re-training or fine-tuning the network. In this work, we\nexplore whether diffusion models are also inherently equipped with such a\ncapability. Our finding is that for stable diffusion models, by partially\nchanging the input text embedding from a neutral description (e.g., \"a photo of\nperson\") to one with style (e.g., \"a photo of person with smile\") while fixing\nall the Gaussian random noises introduced during the denoising process, the\ngenerated images can be modified towards the target style without changing the\nsemantic content. Based on this finding, we further propose a simple,\nlight-weight image editing algorithm where the mixing weights of the two text\nembeddings are optimized for style matching and content preservation. This\nentire process only involves optimizing over around 50 parameters and does not\nfine-tune the diffusion model itself. Experiments show that the proposed method\ncan modify a wide range of attributes, with the performance outperforming\ndiffusion-model-based image-editing algorithms that require fine-tuning. The\noptimized weights generalize well to different images. Our code is publicly\navailable at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.",
    "descriptor": "\nComments: 23 pages, 18 figures\n",
    "authors": [
      "Qiucheng Wu",
      "Yujian Liu",
      "Handong Zhao",
      "Ajinkya Kale",
      "Trung Bui",
      "Tong Yu",
      "Zhe Lin",
      "Yang Zhang",
      "Shiyu Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08698"
  },
  {
    "id": "arXiv:2212.08700",
    "title": "'Rarely' a problem? Language models exhibit inverse scaling in their  predictions following 'few'-type quantifiers",
    "abstract": "Language Models appear to perform poorly on quantification. We ask how badly.\n'Few'-type quantifiers, as in 'few children like vegetables' might pose a\nparticular challenge for Language Models, since the sentence components without\nthe quantifier are likely to co-occur, and because 'few'-type quantifiers are\nrare. We present 960 sentences stimuli from two human neurolinguistic\nexperiments to 22 autoregressive transformer models of differing sizes. Not\nonly do the models perform poorly on 'few'-type quantifiers, but overall the\nlarger the model, the worse its performance. We interpret this inverse scaling\nas suggesting that larger models increasingly reflect online rather than\noffline human processing, and argue that decreasing performance of larger\nmodels may challenge uses of Language Models as the basis for Natural Language\nSystems.",
    "descriptor": "",
    "authors": [
      "James A. Michaelov",
      "Benjamin K. Bergen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08700"
  },
  {
    "id": "arXiv:2212.08701",
    "title": "An Upper Bound for the Distribution Overlap Index and Its Applications",
    "abstract": "This paper proposes an easy-to-compute upper bound for the overlap index\nbetween two probability distributions without requiring any knowledge of the\ndistribution models. The computation of our bound is time-efficient and\nmemory-efficient and only requires finite samples. The proposed bound shows its\nvalue in one-class classification and domain shift analysis. Specifically, in\none-class classification, we build a novel one-class classifier by converting\nthe bound into a confidence score function. Unlike most one-class classifiers,\nthe training process is not needed for our classifier. Additionally, the\nexperimental results show that our classifier \\textcolor{\\colorname}{can be\naccurate with} only a small number of in-class samples and outperforms many\nstate-of-the-art methods on various datasets in different one-class\nclassification scenarios. In domain shift analysis, we propose a theorem based\non our bound. The theorem is useful in detecting the existence of domain shift\nand inferring data information. The detection and inference processes are both\ncomputation-efficient and memory-efficient. Our work shows significant promise\ntoward broadening the applications of overlap-based metrics.",
    "descriptor": "",
    "authors": [
      "Hao Fu",
      "Prashanth Krishnamurthy",
      "Siddharth Garg",
      "Farshad Khorrami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08701"
  },
  {
    "id": "arXiv:2212.08704",
    "title": "Speech Aware Dialog System Technology Challenge (DSTC11)",
    "abstract": "Most research on task oriented dialog modeling is based on written text\ninput. However, users interact with practical dialog systems often using speech\nas input. Typically, systems convert speech into text using an Automatic Speech\nRecognition (ASR) system, introducing errors. Furthermore, these systems do not\naddress the differences in written and spoken language. The research on this\ntopic is stymied by the lack of a public corpus. Motivated by these\nconsiderations, our goal in hosting the speech-aware dialog state tracking\nchallenge was to create a public corpus or task which can be used to\ninvestigate the performance gap between the written and spoken forms of input,\ndevelop models that could alleviate this gap, and establish whether\nText-to-Speech-based (TTS) systems is a reasonable surrogate to the more-labor\nintensive human data collection. We created three spoken versions of the\npopular written-domain MultiWoz task -- (a) TTS-Verbatim: written user inputs\nwere converted into speech waveforms using a TTS system, (b) Human-Verbatim:\nhumans spoke the user inputs verbatim, and (c) Human-paraphrased: humans\nparaphrased the user inputs. Additionally, we provided different forms of ASR\noutput to encourage wider participation from teams that may not have access to\nstate-of-the-art ASR systems. These included ASR transcripts, word time stamps,\nand latent representations of the audio (audio encoder outputs). In this paper,\nwe describe the corpus, report results from participating teams, provide\npreliminary analyses of their results, and summarize the current\nstate-of-the-art in this domain.",
    "descriptor": "",
    "authors": [
      "Hagen Soltau",
      "Izhak Shafran",
      "Mingqiu Wang",
      "Abhinav Rastogi",
      "Jeffrey Zhao",
      "Ye Jia",
      "Wei Han",
      "Yuan Cao",
      "Aramys Miranda"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08704"
  },
  {
    "id": "arXiv:2212.08709",
    "title": "On the Complexities of Understanding Matching Mechanisms",
    "abstract": "We study various novel complexity measures for two-sided matching mechanisms,\napplied to the popular real-world school choice mechanisms of Deferred\nAcceptance (DA) and Top Trading Cycles (TTC). In contrast to typical bounds in\ncomputer science, our metrics are not aimed to capture how hard the mechanisms\nare to compute. Rather, they aim to capture certain aspects of the difficulty\nof understanding or explaining the mechanisms and their properties.\nFirst, we study a set of questions regarding the complexity of how one\nagent's report can affect other facets of the mechanism. We show that in both\nDA and TTC, one agent's report can have a structurally complex effect on the\nfinal matching. Considering how one agent's report can affect another agent's\nset of obtainable options, we show that this effect has high complexity for\nTTC, but low complexity for DA, showing that one agent can only affect another\nin DA in a quantitatively controlled way.\nSecond, we study a set of questions about the complexity of communicating\nvarious facets of the outcome matching, after calculating it. We find that when\nthere are many more students than schools, it is provably harder to\nconcurrently describe to each student her match in TTC than in DA. In contrast,\nwe show that the outcomes of TTC and DA are equally hard to jointly verify, and\nthat all agents' sets of obtainable options are equally hard to describe,\nshowcasing ways in which the two mechanisms are comparably complex.\nOur results uncover new lenses into how TTC may be more complex than DA. This\nstands in contrast with recent results under different models, emphasizing the\nrichness of the landscape of complexities of matching mechanisms. Our proofs\nuncover novel structural properties of TTC and DA, which may be of independent\ninterest.",
    "descriptor": "",
    "authors": [
      "Yannai A. Gonczarowski",
      "Clayton Thomas"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2212.08709"
  },
  {
    "id": "arXiv:2212.08710",
    "title": "JFP: Joint Future Prediction with Interactive Multi-Agent Modeling for  Autonomous Driving",
    "abstract": "We propose JFP, a Joint Future Prediction model that can learn to generate\naccurate and consistent multi-agent future trajectories. For this task, many\ndifferent methods have been proposed to capture social interactions in the\nencoding part of the model, however, considerably less focus has been placed on\nrepresenting interactions in the decoder and output stages. As a result, the\npredicted trajectories are not necessarily consistent with each other, and\noften result in unrealistic trajectory overlaps. In contrast, we propose an\nend-to-end trainable model that learns directly the interaction between pairs\nof agents in a structured, graphical model formulation in order to generate\nconsistent future trajectories. It sets new state-of-the-art results on Waymo\nOpen Motion Dataset (WOMD) for the interactive setting. We also investigate a\nmore complex multi-agent setting for both WOMD and a larger internal dataset,\nwhere our approach improves significantly on the trajectory overlap metrics\nwhile obtaining on-par or better performance on single-agent trajectory\nmetrics.",
    "descriptor": "",
    "authors": [
      "Wenjie Luo",
      "Cheolho Park",
      "Andre Cornman",
      "Benjamin Sapp",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08710"
  },
  {
    "id": "arXiv:2212.08712",
    "title": "Towards Causal Temporal Reasoning for Markov Decision Processes",
    "abstract": "We introduce a new probabilistic temporal logic for the verification of\nMarkov Decision Processes (MDP). Our logic is the first to include operators\nfor causal reasoning, allowing us to express interventional and counterfactual\nqueries. Given a path formula $\\phi$, an interventional property is concerned\nwith the satisfaction probability of $\\phi$ if we apply a particular change $I$\nto the MDP (e.g., switching to a different policy); a counterfactual allows us\nto compute, given an observed MDP path $\\tau$, what the outcome of $\\phi$ would\nhave been had we applied $I$ in the past. For its ability to reason about\ndifferent configurations of the MDP, our approach represents a departure from\nexisting probabilistic temporal logics that can only reason about a fixed\nsystem configuration. From a syntactic viewpoint, we introduce a generalized\ncounterfactual operator that subsumes both interventional and counterfactual\nprobabilities as well as the traditional probabilistic operator found in e.g.,\nPCTL. From a semantics viewpoint, our logic is interpreted over a structural\ncausal model (SCM) translation of the MDP, which gives us a representation\namenable to counterfactual reasoning. We provide a proof-of-concept evaluation\nof our logic on a reach-avoid task in a grid-world model.",
    "descriptor": "\nComments: 27 pages and 8 figures\n",
    "authors": [
      "Milad Kazemi",
      "Nicola Paoletti"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08712"
  },
  {
    "id": "arXiv:2212.08713",
    "title": "Improved decoding of symmetric rank metric errors",
    "abstract": "We consider the decoding of rank metric codes assuming the error matrix is\nsymmetric. We prove two results. First, for rates $<1/2$ there exists a broad\nfamily of rank metric codes for which any symmetric error pattern, even of\nmaximal rank can be corrected. Moreover, the corresponding family of decodable\ncodes includes Gabidulin codes of rate $<1/2$. Second, for rates $>1/2$, we\npropose a decoder correcting symmetric errors of rank up to $n-k$. The two\nmentioned decoders are deterministic and worst case.",
    "descriptor": "",
    "authors": [
      "Alain Couvreur"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.08713"
  },
  {
    "id": "arXiv:2212.08717",
    "title": "Containerisation for High Performance Computing Systems: Survey and  Prospects",
    "abstract": "Containers improve the efficiency in application deployment and thus have\nbeen widely utilised on Cloud and lately in High Performance Computing (HPC)\nenvironments. Containers encapsulate complex programs with their dependencies\nin isolated environments making applications more compatible and portable.\nOften HPC systems have higher security levels compared to Cloud systems, which\nrestrict users' ability to customise environments. Therefore, containers on HPC\nneed to include a heavy package of libraries making their size relatively\nlarge. These libraries usually are specifically optimised for the hardware,\nwhich compromises portability of containers. Per contra, a Cloud container has\nsmaller volume and is more portable. Furthermore, containers would benefit from\norchestrators that facilitate deployment and management of containers at a\nlarge scale. Cloud systems in practice usually incorporate sophisticated\ncontainer orchestration mechanisms as opposed to HPC systems. Nevertheless,\nsome solutions to enable container orchestration on HPC systems have been\nproposed in state of the art. This paper gives a survey and taxonomy of efforts\nin both containerisation and its orchestration strategies on HPC systems. It\nhighlights differences thereof between Cloud and HPC. Lastly, challenges are\ndiscussed and the potentials for research and engineering are envisioned.",
    "descriptor": "\nComments: IEEE Transactions on Software Engineering\n",
    "authors": [
      "Naweiluo Zhou",
      "Huan Zhou",
      "Dennis Hoppe"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.08717"
  },
  {
    "id": "arXiv:2212.08718",
    "title": "Neural Story Planning",
    "abstract": "Automated plot generation is the challenge of generating a sequence of events\nthat will be perceived by readers as the plot of a coherent story. Traditional\nsymbolic planners plan a story from a goal state and guarantee logical causal\nplot coherence but rely on a library of hand-crafted actions with their\npreconditions and effects. This closed world setting limits the length and\ndiversity of what symbolic planners can generate. On the other hand,\npre-trained neural language models can generate stories with great diversity,\nwhile being generally incapable of ending a story in a specified manner and can\nhave trouble maintaining coherence. In this paper, we present an approach to\nstory plot generation that unifies causal planning with neural language models.\nWe propose to use commonsense knowledge extracted from large language models to\nrecursively expand a story plot in a backward chaining fashion. Specifically,\nour system infers the preconditions for events in the story and then events\nthat will cause those conditions to become true. We performed automatic\nevaluation to measure narrative coherence as indicated by the ability to answer\nquestions about whether different events in the story are causally related to\nother events. Results indicate that our proposed method produces more coherent\nplotlines than several strong baselines.",
    "descriptor": "",
    "authors": [
      "Anbang Ye",
      "Christopher Cui",
      "Taiwei Shi",
      "Mark O. Riedl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08718"
  },
  {
    "id": "arXiv:2212.08720",
    "title": "Imitation Learning based Auto-Correction of Extrinsic Parameters for A  Mixed-Reality Setup",
    "abstract": "In this paper, we discuss an imitation learning based method for reducing the\ncalibration error for a mixed reality system consisting of a vision sensor and\na projector. Unlike a head mounted display, in this setup, augmented\ninformation is available to a human subject via the projection of a scene into\nthe real world. Inherently, the camera and projector need to be calibrated as a\nstereo setup to project accurate information in 3D space. Previous calibration\nprocesses require multiple recording and parameter tuning steps to achieve the\ndesired calibration, which is usually time consuming process. In order to avoid\nsuch tedious calibration, we train a CNN model to iteratively correct the\nextrinsic offset given a QR code and a projected pattern. We discuss the\noverall system setup, data collection for training, and results of the\nauto-correction model.",
    "descriptor": "\nComments: Horizons of an Extended Robotics Reality (XR2) Workshop, IROS 2022\n",
    "authors": [
      "Shubham Sonawani",
      "Yifan Zhou",
      "Heni Ben Amor"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08720"
  },
  {
    "id": "arXiv:2212.08724",
    "title": "DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text  Generation",
    "abstract": "Self-training (ST) has prospered again in language understanding by\naugmenting the fine-tuning of pre-trained language models when labeled data is\ninsufficient. However, it remains challenging to incorporate ST into\nattribute-controllable language generation. Augmented by only self-generated\npseudo text, generation models over-emphasize exploitation of the previously\nlearned space, suffering from a constrained generalization boundary. We revisit\nST and propose a novel method, DuNST to alleviate this problem. DuNST jointly\nmodels text generation and classification with a shared Variational AutoEncoder\nand corrupts the generated pseudo text by two kinds of flexible noise to\ndisturb the space. In this way, our model could construct and utilize both\npseudo text from given labels and pseudo labels from available unlabeled text,\nwhich are gradually refined during the ST process. We theoretically demonstrate\nthat DuNST can be regarded as enhancing exploration towards the potential real\ntext space, providing a guarantee of improved performance. Experiments on three\ncontrollable generation tasks show that DuNST could significantly boost control\naccuracy while maintaining comparable generation fluency and diversity against\nseveral strong baselines.",
    "descriptor": "",
    "authors": [
      "Yuxi Feng",
      "Xiaoyuan Yi",
      "Xiting Wang",
      "Laks V.S. Lakshmanan",
      "Xing Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08724"
  },
  {
    "id": "arXiv:2212.08726",
    "title": "Learning Non-robustness using Simulation-based Testing: a Network  Traffic-shaping Case Study",
    "abstract": "An input to a system reveals a non-robust behaviour when, by making a small\nchange in the input, the output of the system changes from acceptable (passing)\nto unacceptable (failing) or vice versa. Identifying inputs that lead to\nnon-robust behaviours is important for many types of systems, e.g.,\ncyber-physical and network systems, whose inputs are prone to perturbations. In\nthis paper, we propose an approach that combines simulation-based testing with\nregression tree models to generate value ranges for inputs in response to which\na system is likely to exhibit non-robust behaviours. We apply our approach to a\nnetwork traffic-shaping system (NTSS) -- a novel case study from the network\ndomain. In this case study, developed and conducted in collaboration with a\nnetwork solutions provider, RabbitRun Technologies, input ranges that lead to\nnon-robustness are of interest as a way to identify and mitigate network\nquality-of-service issues. We demonstrate that our approach accurately\ncharacterizes non-robust test inputs of NTSS by achieving a precision of 84%\nand a recall of 100%, significantly outperforming a standard baseline. In\naddition, we show that there is no statistically significant difference between\nthe results obtained from our simulated testbed and a hardware testbed with\nidentical configurations. Finally we describe lessons learned from our\nindustrial collaboration, offering insights about how simulation helps discover\nunknown and undocumented behaviours as well as a new perspective on using\nnon-robustness as a measure for system re-configuration.",
    "descriptor": "",
    "authors": [
      "Baharin Aliashrafi Jodat",
      "Shiva Nejati",
      "Mehrdad Sabetzadeh",
      "Patricio Saavedra"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.08726"
  },
  {
    "id": "arXiv:2212.08729",
    "title": "Distribution-aware Goal Prediction and Conformant Model-based Planning  for Safe Autonomous Driving",
    "abstract": "The feasibility of collecting a large amount of expert demonstrations has\ninspired growing research interests in learning-to-drive settings, where models\nlearn by imitating the driving behaviour from experts. However, exclusively\nrelying on imitation can limit agents' generalisability to novel scenarios that\nare outside the support of the training data. In this paper, we address this\nchallenge by factorising the driving task, based on the intuition that modular\narchitectures are more generalisable and more robust to changes in the\nenvironment compared to monolithic, end-to-end frameworks. Specifically, we\ndraw inspiration from the trajectory forecasting community and reformulate the\nlearning-to-drive task as obstacle-aware perception and grounding,\ndistribution-aware goal prediction, and model-based planning. Firstly, we train\nthe obstacle-aware perception module to extract salient representation of the\nvisual context. Then, we learn a multi-modal goal distribution by performing\nconditional density-estimation using normalising flow. Finally, we ground\ncandidate trajectory predictions road geometry, and plan the actions based on\non vehicle dynamics. Under the CARLA simulator, we report state-of-the-art\nresults on the CARNOVEL benchmark.",
    "descriptor": "\nComments: Accepted: 1st Workshop on Safe Learning for Autonomous Driving, at the International Conference on Machine Learning (ICML 2022); Best Paper Award\n",
    "authors": [
      "Jonathan Francis",
      "Bingqing Chen",
      "Weiran Yao",
      "Eric Nyberg",
      "Jean Oh"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08729"
  },
  {
    "id": "arXiv:2212.08731",
    "title": "Multi-person 3D pose estimation from unlabelled data",
    "abstract": "Its numerous applications make multi-human 3D pose estimation a remarkably\nimpactful area of research. Nevertheless, assuming a multiple-view system\ncomposed of several regular RGB cameras, 3D multi-pose estimation presents\nseveral challenges. First of all, each person must be uniquely identified in\nthe different views to separate the 2D information provided by the cameras.\nSecondly, the 3D pose estimation process from the multi-view 2D information of\neach person must be robust against noise and potential occlusions in the\nscenario. In this work, we address these two challenges with the help of deep\nlearning. Specifically, we present a model based on Graph Neural Networks\ncapable of predicting the cross-view correspondence of the people in the\nscenario along with a Multilayer Perceptron that takes the 2D points to yield\nthe 3D poses of each person. These two models are trained in a self-supervised\nmanner, thus avoiding the need for large datasets with 3D annotations.",
    "descriptor": "",
    "authors": [
      "Daniel Rodriguez-Criado",
      "Pilar Bachiller",
      "George Vogiatzis",
      "Luis J. Manso"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08731"
  },
  {
    "id": "arXiv:2212.08733",
    "title": "Counterfactual Explanations for Misclassified Images: How Human and  Machine Explanations Differ",
    "abstract": "Counterfactual explanations have emerged as a popular solution for the\neXplainable AI (XAI) problem of elucidating the predictions of black-box\ndeep-learning systems due to their psychological validity, flexibility across\nproblem domains and proposed legal compliance. While over 100 counterfactual\nmethods exist, claiming to generate plausible explanations akin to those\npreferred by people, few have actually been tested on users ($\\sim7\\%$). So,\nthe psychological validity of these counterfactual algorithms for effective XAI\nfor image data is not established. This issue is addressed here using a novel\nmethodology that (i) gathers ground truth human-generated counterfactual\nexplanations for misclassified images, in two user studies and, then, (ii)\ncompares these human-generated ground-truth explanations to\ncomputationally-generated explanations for the same misclassifications. Results\nindicate that humans do not \"minimally edit\" images when generating\ncounterfactual explanations. Instead, they make larger, \"meaningful\" edits that\nbetter approximate prototypes in the counterfactual class.",
    "descriptor": "",
    "authors": [
      "Eoin Delaney",
      "Arjun Pakrashi",
      "Derek Greene",
      "Mark T. Keane"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08733"
  },
  {
    "id": "arXiv:2212.08736",
    "title": "A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle  Scattering Problem",
    "abstract": "We consider the inverse acoustic obstacle problem for sound-soft star-shaped\nobstacles in two dimensions wherein the boundary of the obstacle is determined\nfrom measurements of the scattered field at a collection of receivers outside\nthe object. One of the standard approaches for solving this problem is to\nreformulate it as an optimization problem: finding the boundary of the domain\nthat minimizes the $L^2$ distance between computed values of the scattered\nfield and the given measurement data. The optimization problem is\ncomputationally challenging since the local set of convexity shrinks with\nincreasing frequency and results in an increasing number of local minima in the\nvicinity of the true solution. In many practical experimental settings, low\nfrequency measurements are unavailable due to limitations of the experimental\nsetup or the sensors used for measurement. Thus, obtaining a good initial guess\nfor the optimization problem plays a vital role in this environment.\nWe present a neural network warm-start approach for solving the inverse\nscattering problem, where an initial guess for the optimization problem is\nobtained using a trained neural network. We demonstrate the effectiveness of\nour method with several numerical examples. For high frequency problems, this\napproach outperforms traditional iterative methods such as Gauss-Newton\ninitialized without any prior (i.e., initialized using a unit circle), or\ninitialized using the solution of a direct method such as the linear sampling\nmethod. The algorithm remains robust to noise in the scattered field\nmeasurements and also converges to the true solution for limited aperture data.\nHowever, the number of training samples required to train the neural network\nscales exponentially in frequency and the complexity of the obstacles\nconsidered. We conclude with a discussion of this phenomenon and potential\ndirections for future research.",
    "descriptor": "",
    "authors": [
      "Mo Zhou",
      "Jiequn Han",
      "Manas Rachh",
      "Carlos Borges"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2212.08736"
  },
  {
    "id": "arXiv:2212.08738",
    "title": "SkillFence: A Systems Approach to Practically Mitigating Voice-Based  Confusion Attacks",
    "abstract": "Voice assistants are deployed widely and provide useful functionality.\nHowever, recent work has shown that commercial systems like Amazon Alexa and\nGoogle Home are vulnerable to voice-based confusion attacks that exploit design\nissues. We propose a systems-oriented defense against this class of attacks and\ndemonstrate its functionality for Amazon Alexa. We ensure that only the skills\na user intends execute in response to voice commands. Our key insight is that\nwe can interpret a user's intentions by analyzing their activity on counterpart\nsystems of the web and smartphones. For example, the Lyft ride-sharing Alexa\nskill has an Android app and a website. Our work shows how information from\ncounterpart apps can help reduce dis-ambiguities in the skill invocation\nprocess. We build SkilIFence, a browser extension that existing voice assistant\nusers can install to ensure that only legitimate skills run in response to\ntheir commands. Using real user data from MTurk (N = 116) and experimental\ntrials involving synthetic and organic speech, we show that SkillFence provides\na balance between usability and security by securing 90.83% of skills that a\nuser will need with a False acceptance rate of 19.83%.",
    "descriptor": "",
    "authors": [
      "Ashish Hooda",
      "Matthew Wallace",
      "Kushal Jhunjhunwalla",
      "Earlence Fernandes",
      "Kassem Fawaz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08738"
  },
  {
    "id": "arXiv:2212.08742",
    "title": "Attentiveness Map Estimation for Haptic Teleoperation of Mobile Robot  Obstacle Avoidance and Approach",
    "abstract": "Haptic feedback can improve safety of teleoperated robots when situational\nawareness is limited or operators are inattentive. Standard potential field\napproaches increase haptic resistance as an obstacle is approached, which is\ndesirable when the operator is unaware of the obstacle but undesirable when the\nmovement is intentional, such as when the operator wishes to inspect or\nmanipulate an object. This paper presents a novel haptic teleoperation\nframework that estimates the operator's attentiveness to dampen haptic feedback\nfor intentional movement. A biologically-inspired attention model is developed\nbased on computational working memory theories to integrate visual saliency\nestimation with spatial mapping. This model generates an attentiveness map in\nreal-time, and the haptic rendering system generates lower haptic forces for\nobstacles that the operator is estimated to be aware of. Experimental results\nin simulation show that the proposed framework outperforms haptic teleoperation\nwithout attentiveness estimation in terms of task performance, robot safety,\nand user experience.",
    "descriptor": "",
    "authors": [
      "Ninghan Zhong",
      "Kris Hauser"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08742"
  },
  {
    "id": "arXiv:2212.08743",
    "title": "Addressing Data Heterogeneity in Decentralized Learning via Topological  Pre-processing",
    "abstract": "Recently, local peer topology has been shown to influence the overall\nconvergence of decentralized learning (DL) graphs in the presence of data\nheterogeneity. In this paper, we demonstrate the advantages of constructing a\nproxy-based locally heterogeneous DL topology to enhance convergence and\nmaintain data privacy. In particular, we propose a novel peer clumping strategy\nto efficiently cluster peers before arranging them in a final training graph.\nBy showing how locally heterogeneous graphs outperform locally homogeneous\ngraphs of similar size and from the same global data distribution, we present a\nstrong case for topological pre-processing. Moreover, we demonstrate the\nscalability of our approach by showing how the proposed topological\npre-processing overhead remains small in large graphs while the performance\ngains get even more pronounced. Furthermore, we show the robustness of our\napproach in the presence of network partitions.",
    "descriptor": "",
    "authors": [
      "Waqwoya Abebe",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.08743"
  },
  {
    "id": "arXiv:2212.08744",
    "title": "Machine Learning Strategies to Improve Generalization in EEG-based  Emotion Assessment: \\\\a Systematic Review",
    "abstract": "A systematic review on machine-learning strategies for improving\ngeneralizability (cross-subjects and cross-sessions) electroencephalography\n(EEG) based in emotion classification was realized. In this context, the\nnon-stationarity of EEG signals is a critical issue and can lead to the Dataset\nShift problem. Several architectures and methods have been proposed to address\nthis issue, mainly based on transfer learning methods. 418 papers were\nretrieved from the Scopus, IEEE Xplore and PubMed databases through a search\nquery focusing on modern machine learning techniques for generalization in\nEEG-based emotion assessment. Among these papers, 75 were found eligible based\non their relevance to the problem. Studies lacking a specific cross-subject and\ncross-session validation strategy and making use of other biosignals as support\nwere excluded. On the basis of the selected papers' analysis, a taxonomy of the\nstudies employing Machine Learning (ML) methods was proposed, together with a\nbrief discussion on the different ML approaches involved. The studies with the\nbest results in terms of average classification accuracy were identified,\nsupporting that transfer learning methods seem to perform better than other\napproaches. A discussion is proposed on the impact of (i) the emotion\ntheoretical models and (ii) psychological screening of the experimental sample\non the classifier performances.",
    "descriptor": "\nComments: under review\n",
    "authors": [
      "Andrea Apicella",
      "Pasquale Arpaia",
      "Giovanni D'Errico",
      "Davide Marocco",
      "Giovanna Mastrati",
      "Nicola Moccaldi",
      "Roberto Prevete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08744"
  },
  {
    "id": "arXiv:2212.08749",
    "title": "Analysis and application of multispectral data for water segmentation  using machine learning",
    "abstract": "Monitoring water is a complex task due to its dynamic nature, added\npollutants, and land build-up. The availability of high-resolu-tion data by\nSentinel-2 multispectral products makes implementing remote sensing\napplications feasible. However, overutilizing or underutilizing multispectral\nbands of the product can lead to inferior performance. In this work, we compare\nthe performances of ten out of the thirteen bands available in a Sentinel-2\nproduct for water segmentation using eight machine learning algorithms. We find\nthat the shortwave infrared bands (B11 and B12) are the most superior for\nsegmenting water bodies. B11 achieves an overall accuracy of $71\\%$ while B12\nachieves $69\\%$ across all algorithms on the test site. We also find that the\nSupport Vector Machine (SVM) algorithm is the most favourable for single-band\nwater segmentation. The SVM achieves an overall accuracy of $69\\%$ across the\ntested bands over the given test site. Finally, to demonstrate the\neffectiveness of choosing the right amount of data, we use only B11 reflectance\ndata to train an artificial neural network, BandNet. Even with a basic\narchitecture, BandNet is proportionate to known architectures for semantic and\nwater segmentation, achieving a $92.47$ mIOU on the test site. BandNet requires\nonly a fraction of the time and resources to train and run inference, making it\nsuitable to be deployed on web applications to run and monitor water bodies in\nlocalized regions. Our codebase is available at\nhttps://github.com/IamShubhamGupto/BandNet.",
    "descriptor": "",
    "authors": [
      "Shubham Gupta",
      "Uma D.",
      "Ramachandra Hebbar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08749"
  },
  {
    "id": "arXiv:2212.08751",
    "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
    "abstract": "While recent work on text-conditional 3D object generation has shown\npromising results, the state-of-the-art methods typically require multiple\nGPU-hours to produce a single sample. This is in stark contrast to\nstate-of-the-art generative image models, which produce samples in a number of\nseconds or minutes. In this paper, we explore an alternative method for 3D\nobject generation which produces 3D models in only 1-2 minutes on a single GPU.\nOur method first generates a single synthetic view using a text-to-image\ndiffusion model, and then produces a 3D point cloud using a second diffusion\nmodel which conditions on the generated image. While our method still falls\nshort of the state-of-the-art in terms of sample quality, it is one to two\norders of magnitude faster to sample from, offering a practical trade-off for\nsome use cases. We release our pre-trained point cloud diffusion models, as\nwell as evaluation code and models, at https://github.com/openai/point-e.",
    "descriptor": "\nComments: 8 pages, 11 figures\n",
    "authors": [
      "Alex Nichol",
      "Heewoo Jun",
      "Prafulla Dhariwal",
      "Pamela Mishkin",
      "Mark Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08751"
  },
  {
    "id": "arXiv:2212.08753",
    "title": "Quasistatic fracture evolution",
    "abstract": "Nonlocal quasistatic fracture evolution for interacting cracks is developed\nand supporting numerical examples are presented. The approach is implicit and\nis based on local stationarity and fixed point methods. It is proved that the\nfracture evolution decreases stored elastic energy with each load step as the\ncracks advance; provided the load increments are chosen sufficiently small.\nThis is also seen in the numerical examples. The numerical examples include\nevolution of a straight crack, a crack propagating inside an L-shaped domain,\nand two offset inward propagating cracks.",
    "descriptor": "",
    "authors": [
      "Debdeep Bhattacharya",
      "Robert Lipton",
      "Patrick Diehl"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2212.08753"
  },
  {
    "id": "arXiv:2212.08754",
    "title": "A systematic literature review on Internet of Vehicles Security",
    "abstract": "The Internet of Vehicles IoV commonly referred to as connected automobiles is\na vast network that connects various entities including users sensors and\nvehicles They will connect across a network to lessen traffic accidents and\nimprove both the security and safety of smart vehicles The Internet of Vehicles\nis subject to a wide variety of threats including spoofing attacks recognition\nattacks privacy attacks and verification attacks Our the primary concern when\ncreating any new smart gadget is the users safety which will be improved by\nidentifying solutions to the various cyber threats Therefore we will cover the\nsecurity of smart automobiles in this literature review including their attacks\nand solutions.",
    "descriptor": "\nComments: This article have 10 pages and 6 figures\n",
    "authors": [
      "Priyank Sharma",
      "Meet Patel",
      "Apoorva Prasad"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.08754"
  },
  {
    "id": "arXiv:2212.08755",
    "title": "Implicit Actions and Non-blocking Failure Recovery with MPI",
    "abstract": "Scientific applications have long embraced the MPI as the environment of\nchoice to execute on large distributed systems. The User-Level Failure\nMitigation (ULFM) specification extends the MPI standard to address resilience\nand enable MPI applications to restore their communication capability after a\nfailure. This works builds upon the wide body of experience gained in the field\nto eliminate a gap between current practice and the ideal, more asynchronous,\nrecovery model in which the fault tolerance activities of multiple components\ncan be carried out simultaneously and overlap. This work proposes to: (1)\nprovide the required consistency in fault reporting to applications (i.e.,\nenable an application to assess the success of a computational phase without\nincurring an unacceptable performance hit); (2) bring forward the building\nblocks that permit the effective scoping of fault recovery in an application,\nso that independent components in an application can recover without\ninterfering with each other, and separate groups of processes in the\napplication can recover independently or in unison; and (3) overlap recovery\nactivities necessary to restore the consistency of the system (e.g., eviction\nof faulty processes from the communication group) with application recovery\nactivities (e.g., dataset restoration from checkpoints).",
    "descriptor": "\nComments: Accepted in FTXS'22 this https URL\n",
    "authors": [
      "Aurelien Bouteiller",
      "George Bosilca"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.08755"
  },
  {
    "id": "arXiv:2212.08756",
    "title": "Multi-Scales Data Augmentation Approach In Natural Language Inference  For Artifacts Mitigation And Pre-Trained Model Optimization",
    "abstract": "Machine learning models can reach high performance on benchmark natural\nlanguage processing (NLP) datasets but fail in more challenging settings. We\nstudy this issue when a pre-trained model learns dataset artifacts in natural\nlanguage inference (NLI), the topic of studying the logical relationship\nbetween a pair of text sequences. We provide a variety of techniques for\nanalyzing and locating dataset artifacts inside the crowdsourced Stanford\nNatural Language Inference (SNLI) corpus. We study the stylistic pattern of\ndataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a\nunique multi-scale data augmentation technique with two distinct frameworks: a\nbehavioral testing checklist at the sentence level and lexical synonym criteria\nat the word level. Specifically, our combination method enhances our model's\nresistance to perturbation testing, enabling it to continuously outperform the\npre-trained baseline.",
    "descriptor": "",
    "authors": [
      "Zhenyuan Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2212.08756"
  },
  {
    "id": "arXiv:2212.08757",
    "title": "Short-term Prediction of Household Electricity Consumption Using  Customized LSTM and GRU Models",
    "abstract": "With the evolution of power systems as it is becoming more intelligent and\ninteractive system while increasing in flexibility with a larger penetration of\nrenewable energy sources, demand prediction on a short-term resolution will\ninevitably become more and more crucial in designing and managing the future\ngrid, especially when it comes to an individual household level. Projecting the\ndemand for electricity for a single energy user, as opposed to the aggregated\npower consumption of residential load on a wide scale, is difficult because of\na considerable number of volatile and uncertain factors. This paper proposes a\ncustomized GRU (Gated Recurrent Unit) and Long Short-Term Memory (LSTM)\narchitecture to address this challenging problem. LSTM and GRU are\ncomparatively newer and among the most well-adopted deep learning approaches.\nThe electricity consumption datasets were obtained from individual household\nsmart meters. The comparison shows that the LSTM model performs better for\nhome-level forecasting than alternative prediction techniques-GRU in this case.\nTo compare the NN-based models with contrast to the conventional statistical\ntechnique-based model, ARIMA based model was also developed and benchmarked\nwith LSTM and GRU model outcomes in this study to show the performance of the\nproposed model on the collected time series data.",
    "descriptor": "\nComments: 11 pages, 30 Figures and 2 Tables\n",
    "authors": [
      "Saad Emshagin",
      "Wayes Koroni Halim",
      "Rasha Kashef"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.08757"
  },
  {
    "id": "arXiv:2212.08762",
    "title": "Iterative RNDOP-Optimal Anchor Placement for Beyond Convex Hull  ToA-based Localization: Performance Bounds and Heuristic Algorithms",
    "abstract": "Localizing targets outside the anchors' convex hull is an understudied but\nprevalent scenario in vehicle-centric, UAV-based, and self-localization\napplications. Considering such scenarios, this paper studies the optimal anchor\nplacement problem for Time-of-Arrival (ToA)-based localization schemes such\nthat the worst-case Dilution of Precision (DOP) is minimized.\nBuilding on prior results on DOP scaling laws for beyond convex hull\nToA-based localization, we propose a novel metric termed the Range-Normalized\nDOP (RNDOP). We show that the worst-case DOP-optimal anchor placement problem\nsimplifies to a min-max RNDOP-optimal anchor placement problem. Unfortunately,\nthis formulation results in a non-convex and intractable problem under\nrealistic constraints. To overcome this, we propose iterative anchor addition\nschemes, which result in a tractable albeit non-convex problem. By exploiting\nthe structure arising from the resultant rank-1 update, we devise three\nheuristic schemes with varying performance-complexity tradeoffs. In addition,\nwe also derive the upper and lower bounds for scenarios where we are placing\nanchors to optimize the worst-case (a) 3D positioning error and (b) 2D\npositioning error. We build on these results to design a cohesive iterative\nalgorithmic framework for robust anchor placement and then discuss the\ncomputational complexity of the proposed schemes. Using numerical results, we\nvalidate the accuracy of our theoretical results. We also present comprehensive\nMonte-Carlo simulation results to compare the positioning error and execution\ntime performance of each iterative scheme, discuss the tradeoffs, and provide\nvaluable system design insights for beyond convex hull localization scenarios.",
    "descriptor": "\nComments: 14 pages, 14 figures\n",
    "authors": [
      "Raghunandan M. Rao",
      "Don-Roberts Emenonye"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.08762"
  },
  {
    "id": "arXiv:2212.08764",
    "title": "Occupancy Grid Based Reactive Planner",
    "abstract": "This paper proposes a perception and path planning pipeline for autonomous\nracing in an unknown bounded course. The pipeline was initially created for the\n2021 evGrandPrix autonomous division and was further improved for the 2022\nevent, both of which resulting in first place finishes. Using a simple\nLiDAR-based perception pipeline feeding into an occupancy grid based expansion\nalgorithm, we determine a goal point to drive. This pipeline successfully\nachieved reliable and consistent laps in addition with occupancy grid algorithm\nto know the ways around a cone-defined track with an averaging speeds of 6.85\nm/s over a distance 434.2 meters for a total lap time of 63.4 seconds.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Benjamin Hall",
      "Andrew Goeden",
      "Sahan Reddy",
      "Timothy Gallion",
      "Charles Koduru",
      "M. Hassan Tanveer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08764"
  },
  {
    "id": "arXiv:2212.08765",
    "title": "Latent Variable Representation for Reinforcement Learning",
    "abstract": "Deep latent variable models have achieved significant empirical successes in\nmodel-based reinforcement learning (RL) due to their expressiveness in modeling\ncomplex transition dynamics. On the other hand, it remains unclear\ntheoretically and empirically how latent variable models may facilitate\nlearning, planning, and exploration to improve the sample efficiency of RL. In\nthis paper, we provide a representation view of the latent variable models for\nstate-action value functions, which allows both tractable variational learning\nalgorithm and effective implementation of the optimism/pessimism principle in\nthe face of uncertainty for exploration. In particular, we propose a\ncomputationally efficient planning algorithm with UCB exploration by\nincorporating kernel embeddings of latent variable models. Theoretically, we\nestablish the sample complexity of the proposed approach in the online and\noffline settings. Empirically, we demonstrate superior performance over current\nstate-of-the-art algorithms across various benchmarks.",
    "descriptor": "\nComments: The first two authors contribute equally\n",
    "authors": [
      "Tongzheng Ren",
      "Chenjun Xiao",
      "Tianjun Zhang",
      "Na Li",
      "Zhaoran Wang",
      "Sujay Sanghavi",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.08765"
  },
  {
    "id": "arXiv:2212.08769",
    "title": "Improving Levenberg-Marquardt Algorithm for Neural Networks",
    "abstract": "We explore the usage of the Levenberg-Marquardt (LM) algorithm for regression\n(non-linear least squares) and classification (generalized Gauss-Newton\nmethods) tasks in neural networks. We compare the performance of the LM method\nwith other popular first-order algorithms such as SGD and Adam, as well as\nother second-order algorithms such as L-BFGS , Hessian-Free and KFAC. We\nfurther speed up the LM method by using adaptive momentum, learning rate line\nsearch, and uphill step acceptance.",
    "descriptor": "",
    "authors": [
      "Omead Pooladzandi",
      "Yiming Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08769"
  },
  {
    "id": "arXiv:2212.08774",
    "title": "Annotation by Clicks: A Point-Supervised Contrastive Variance Method for  Medical Semantic Segmentation",
    "abstract": "Medical image segmentation methods typically rely on numerous dense annotated\nimages for model training, which are notoriously expensive and time-consuming\nto collect. To alleviate this burden, weakly supervised techniques have been\nexploited to train segmentation models with less expensive annotations. In this\npaper, we propose a novel point-supervised contrastive variance method (PSCV)\nfor medical image semantic segmentation, which only requires one pixel-point\nfrom each organ category to be annotated. The proposed method trains the base\nsegmentation network by using a novel contrastive variance (CV) loss to exploit\nthe unlabeled pixels and a partial cross-entropy loss on the labeled pixels.\nThe CV loss function is designed to exploit the statistical spatial\ndistribution properties of organs in medical images and their variance\ndistribution map representations to enforce discriminative predictions over the\nunlabeled pixels. Experimental results on two standard medical image datasets\ndemonstrate that the proposed method outperforms the state-of-the-art weakly\nsupervised methods on point-supervised medical image semantic segmentation\ntasks.",
    "descriptor": "",
    "authors": [
      "Qing En",
      "Yuhong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08774"
  },
  {
    "id": "arXiv:2212.08775",
    "title": "RISE: Leveraging Retrieval Techniques for Summarization Evaluation",
    "abstract": "Evaluating automatically-generated text summaries is a challenging task.\nWhile there have been many interesting approaches, they still fall short of\nhuman evaluations. We present RISE, a new approach for evaluating summaries by\nleveraging techniques from information retrieval. RISE is first trained as a\nretrieval task using a dual-encoder retrieval setup, and can then be\nsubsequently utilized for evaluating a generated summary given an input\ndocument, without gold reference summaries. RISE is especially well suited when\nworking on new datasets where one may not have reference summaries available\nfor evaluation. We conduct comprehensive experiments on the SummEval benchmark\n(Fabbri et al., 2021) and the results show that RISE has higher correlation\nwith human evaluations compared to many past approaches to summarization\nevaluation. Furthermore, RISE also demonstrates data-efficiency and\ngeneralizability across languages.",
    "descriptor": "",
    "authors": [
      "David Uthus",
      "Jianmo Ni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08775"
  },
  {
    "id": "arXiv:2212.08779",
    "title": "Personalized Federated Recommender Systems with Private and Partially  Federated AutoEncoders",
    "abstract": "Recommender Systems (RSs) have become increasingly important in many\napplication domains, such as digital marketing. Conventional RSs often need to\ncollect users' data, centralize them on the server-side, and form a global\nmodel to generate reliable recommendations. However, they suffer from two\ncritical limitations: the personalization problem that the RSs trained\ntraditionally may not be customized for individual users, and the privacy\nproblem that directly sharing user data is not encouraged. We propose\nPersonalized Federated Recommender Systems (PersonalFR), which introduces a\npersonalized autoencoder-based recommendation model with Federated Learning\n(FL) to address these challenges. PersonalFR guarantees that each user can\nlearn a personal model from the local dataset and other participating users'\ndata without sharing local data, data embeddings, or models. PersonalFR\nconsists of three main components, including AutoEncoder-based RSs (ARSs) that\nlearn the user-item interactions, Partially Federated Learning (PFL) that\nupdates the encoder locally and aggregates the decoder on the server-side, and\nPartial Compression (PC) that only computes and transmits active model\nparameters. Extensive experiments on two real-world datasets demonstrate that\nPersonalFR can achieve private and personalized performance comparable to that\ntrained by centralizing all users' data. Moreover, PersonalFR requires\nsignificantly less computation and communication overhead than standard FL\nbaselines.",
    "descriptor": "",
    "authors": [
      "Qi Le",
      "Enmao Diao",
      "Xinran Wang",
      "Ali Anwar",
      "Vahid Tarokh",
      "Jie Ding"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.08779"
  },
  {
    "id": "arXiv:2212.08780",
    "title": "Improving Cross-task Generalization of Unified Table-to-text Models with  Compositional Task Configurations",
    "abstract": "There has been great progress in unifying various table-to-text tasks using a\nsingle encoder-decoder model trained via multi-task learning (Xie et al.,\n2022). However, existing methods typically encode task information with a\nsimple dataset name as a prefix to the encoder. This not only limits the\neffectiveness of multi-task learning, but also hinders the model's ability to\ngeneralize to new domains or tasks that were not seen during training, which is\ncrucial for real-world applications. In this paper, we propose compositional\ntask configurations, a set of prompts prepended to the encoder to improve\ncross-task generalization of unified models. We design the task configurations\nto explicitly specify the task type, as well as its input and output types. We\nshow that this not only allows the model to better learn shared knowledge\nacross different tasks at training, but also allows us to control the model by\ncomposing new configurations that apply novel input-output combinations in a\nzero-shot manner. We demonstrate via experiments over ten table-to-text tasks\nthat our method outperforms the UnifiedSKG baseline by noticeable margins in\nboth in-domain and zero-shot settings, with average improvements of +0.5 and\n+12.6 from using a T5-large backbone, respectively.",
    "descriptor": "",
    "authors": [
      "Jifan Chen",
      "Yuhao Zhang",
      "Lan Liu",
      "Rui Dong",
      "Xinchi Chen",
      "Patrick Ng",
      "William Yang Wang",
      "Zhiheng Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08780"
  },
  {
    "id": "arXiv:2212.08781",
    "title": "Multi-Scale Relational Graph Convolutional Network for Multiple Instance  Learning in Histopathology Images",
    "abstract": "Graph convolutional neural networks have shown significant potential in\nnatural and histopathology images. However, their use has only been studied in\na single magnification or multi-magnification with late fusion. In order to\nleverage the multi-magnification information and early fusion with graph\nconvolutional networks, we handle different embedding spaces at each\nmagnification by introducing the Multi-Scale Relational Graph Convolutional\nNetwork (MS-RGCN) as a multiple instance learning method. We model\nhistopathology image patches and their relation with neighboring patches and\npatches at other scales (i.e., magnifications) as a graph. To pass the\ninformation between different magnification embedding spaces, we define\nseparate message-passing neural networks based on the node and edge type. We\nexperiment on prostate cancer histopathology images to predict the grade groups\nbased on the extracted features from patches. We also compare our MS-RGCN with\nmultiple state-of-the-art methods with evaluations on both source and held-out\ndatasets. Our method outperforms the state-of-the-art on both datasets and\nespecially on the classification of grade groups 2 and 3, which are significant\nfor clinical decisions for patient management. Through an ablation study, we\ntest and show the value of the pertinent design features of the MS-RGCN.",
    "descriptor": "",
    "authors": [
      "Roozbeh Bazargani",
      "Ladan Fazli",
      "Larry Goldenberg",
      "Martin Gleave",
      "Ali Bashashati",
      "Septimiu Salcudean"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08781"
  },
  {
    "id": "arXiv:2212.08785",
    "title": "Importance of Synthesizing High-quality Data for Text-to-SQL Parsing",
    "abstract": "Recently, there has been increasing interest in synthesizing data to improve\ndownstream text-to-SQL tasks. In this paper, we first examined the existing\nsynthesized datasets and discovered that state-of-the-art text-to-SQL\nalgorithms did not further improve on popular benchmarks when trained with\naugmented synthetic data. We observed two shortcomings: illogical synthetic SQL\nqueries from independent column sampling and arbitrary table joins. To address\nthese issues, we propose a novel synthesis framework that incorporates key\nrelationships from schema, imposes strong typing, and conducts\nschema-distance-weighted column sampling. We also adopt an intermediate\nrepresentation (IR) for the SQL-to-text task to further improve the quality of\nthe generated natural language questions. When existing powerful semantic\nparsers are pre-finetuned on our high-quality synthesized data, our experiments\nshow that these models have significant accuracy boosts on popular benchmarks,\nincluding new state-of-the-art performance on Spider.",
    "descriptor": "",
    "authors": [
      "Yiyun Zhao",
      "Jiarong Jiang",
      "Yiqun Hu",
      "Wuwei Lan",
      "Henry Zhu",
      "Anuj Chauhan",
      "Alexander Li",
      "Lin Pan",
      "Jun Wang",
      "Chung-Wei Hang",
      "Sheng Zhang",
      "Marvin Dong",
      "Joe Lilien",
      "Patrick Ng",
      "Zhiguo Wang",
      "Vittorio Castelli",
      "Bing Xiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08785"
  },
  {
    "id": "arXiv:2212.08787",
    "title": "Conditional Predictive Behavior Planning with Inverse Reinforcement  Learning for Human-like Autonomous Driving",
    "abstract": "Making safe and human-like decisions is an essential capability of autonomous\ndriving systems and learning-based behavior planning is a promising pathway\ntoward this objective. Distinguished from existing learning-based methods that\ndirectly output decisions, this work introduces a predictive behavior planning\nframework that learns to predict and evaluate from human driving data.\nConcretely, a behavior generation module first produces a diverse set of\ncandidate behaviors in the form of trajectory proposals. Then the proposed\nconditional motion prediction network is employed to forecast other agents'\nfuture trajectories conditioned on each trajectory proposal. Given the\ncandidate plans and associated prediction results, we learn a scoring module to\nevaluate the plans using maximum entropy inverse reinforcement learning (IRL).\nWe conduct comprehensive experiments to validate the proposed framework on a\nlarge-scale real-world urban driving dataset. The results reveal that the\nconditional prediction model is able to forecast multiple possible future\ntrajectories given a candidate behavior and the prediction results are reactive\nto different plans. Moreover, the IRL-based scoring module can properly\nevaluate the trajectory proposals and select close-to-human ones. The proposed\nframework outperforms other baseline methods in terms of similarity to human\ndriving trajectories. Moreover, we find that the conditional prediction model\ncan improve both prediction and planning performance compared to the\nnon-conditional model, and learning the scoring module is critical to correctly\nevaluating the candidate plans to align with human drivers.",
    "descriptor": "",
    "authors": [
      "Zhiyu Huang",
      "Haochen Liu",
      "Jingda Wu",
      "Chen Lv"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08787"
  },
  {
    "id": "arXiv:2212.08790",
    "title": "Estimating Cloth Elasticity Parameters Using Position-Based Simulation  of Compliant Constrained Dynamics",
    "abstract": "Clothing plays a vital role in real life and hence, is also important for\nvirtual realities and virtual applications, such as online retail, virtual\ntry-on, and real-time digital avatar interactions. However, choosing the\ncorrect parameters to generate realistic clothing requires expert knowledge and\nis often an arduous manual process. To alleviate this issue, we develop a\npipeline for automatically determining the static material parameters required\nto simulate clothing of a particular material based on easily captured\nreal-world fabrics. We use differentiable simulation to find an optimal set of\nparameters that minimizes the difference between simulated cloth and deformed\ntarget cloth. Our novel well-suited loss function is optimized through\nnon-linear least squares. We designed our objective function to capture\nmaterial-specific behavior, resulting in similar values for different wrinkle\nconfigurations of the same material. While existing methods carefully design\nexperiments to isolate stretch parameters from bending modes, we embrace that\nstretching fabrics causes wrinkling. We estimate bending first, given that\nmembrane stiffness has little effect on bending. Furthermore, our pipeline\ndecouples the capture method from the optimization by registering a template\nmesh to the scanned data. These choices simplify the capture system and allow\nfor wrinkles in scanned fabrics. We use a differentiable extended\nposition-based dynamics (XPBD) cloth simulator, which is capable of real-time\nsimulation. We demonstrate our method on captured data of three different\nreal-world fabrics and on three digital fabrics produced by a third-party\nsimulator.",
    "descriptor": "",
    "authors": [
      "Egor Larionov",
      "Marie-Lena Eckert",
      "Katja Wolff",
      "Tuur Stuyck"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2212.08790"
  },
  {
    "id": "arXiv:2212.08792",
    "title": "Stateful Switch: Optimized Time Series Release with Local Differential  Privacy",
    "abstract": "Time series data have numerous applications in big data analytics. However,\nthey often cause privacy issues when collected from individuals. To address\nthis problem, most existing works perturb the values in the time series while\nretaining their temporal order, which may lead to significant distortion of the\nvalues. Recently, we propose TLDP model that perturbs temporal perturbation to\nensure privacy guarantee while retaining original values. It has shown great\npromise to achieve significantly higher utility than value perturbation\nmechanisms in many time series analysis. However, its practicability is still\nundermined by two factors, namely, utility cost of extra missing or empty\nvalues, and inflexibility of privacy budget settings. To address them, in this\npaper we propose {\\it switch} as a new two-way operation for temporal\nperturbation, as opposed to the one-way {\\it dispatch} operation. The former\ninherently eliminates the cost of missing, empty or repeated values. Optimizing\nswitch operation in a {\\it stateful} manner, we then propose $StaSwitch$\nmechanism for time series release under TLDP. Through both analytical and\nempirical studies, we show that $StaSwitch$ has significantly higher utility\nfor the published time series than any state-of-the-art temporal- or\nvalue-perturbation mechanism, while allowing any combination of privacy budget\nsettings.",
    "descriptor": "\nComments: This paper is accepted and will appear in INFOCOM 2023 as regular research paper\n",
    "authors": [
      "Qingqing Ye",
      "Haibo Hu",
      "Kai Huang",
      "Man Ho Au",
      "Qiao Xue"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.08792"
  },
  {
    "id": "arXiv:2212.08796",
    "title": "A Survey on Password Guessing",
    "abstract": "Text password has served as the most popular method for user authentication\nso far, and is not likely to be totally replaced in foreseeable future.\nPassword authentication offers several desirable properties (e.g., low-cost,\nhighly available, easy-to-implement, reusable). However, it suffers from a\ncritical security issue mainly caused by the inability to memorize complicated\nstrings of human. Users tend to choose easy-to-remember passwords which are not\nuniformly distributed in the key space, and are susceptible to guessing attack.\nIn order to encourage and support users to use strong passwords, it is\nnecessary to simulate automate password guessing methods to determine the\npasswords' strength and identify weak passwords. A large number of password\nguessing models have been proposed in the literature. However, little attention\nwas paid on the task of providing a systematic survey which is necessary to\nreview the state-of-the-art approaches, identify gaps, and avoid duplicate\nstudy. Motivated from that, we conduct a comprehensive survey on all password\nguessing studies presented in the literature from 1979 to 2022. We propose a\ngeneric methodology map of existing models to present an overview of this\nfield, then, subsequently explain each approach in detail. The experimental\nprocedures and available datasets used for evaluating password guessing models\nare summarized, along with the reported performances of representative studies.\nFinally, the current limitations and the open problems as future research\ndirections are discussed. We believe that this survey is helpful to both the\nexperts and newcomers who are interested in password security.",
    "descriptor": "\nComments: 35 pages, 5 figures, 5 tables\n",
    "authors": [
      "Lam Tran",
      "Thuc Nguyen",
      "Changho Seo",
      "Hyunil Kim",
      "Deokjai Choi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.08796"
  },
  {
    "id": "arXiv:2212.08797",
    "title": "A semi-randomized block Kaczmarz method with simple random sampling for  large-scale linear systems with multiple right-hand sides",
    "abstract": "Randomized block Kaczmraz method plays an important role in solving\nlarge-scale linear system. One of the keys of this method is how to effectively\nselect working rows. However, to select the working rows, in most of the\nstate-of-the-art randomized block Kaczmarz-type methods, one has to scan all\nthe rows of the coefficient matrix in advance to compute probabilities, or to\ncompute the residual vector of the linear system in each iteration. Thus, we\nhave to access all the rows of the data matrix in these methods, which are\nunfavorable for big-data problems. Moreover, to the best of our knowledge, how\nto efficiently choose working rows in randomized block Kaczmarz-type methods\nfor multiple linear systems is still an open problem. In order to deal with\nthese problems, we propose semi-randomized block Kaczmarz methods with simple\nrandom sampling for linear systems with single and multiple right-hand sizes,\nrespectively. In these methods, there is no need to scan or pave all the rows\nof the coefficient matrix, nor to compute probabilities and the residual vector\nof the linear system in each outer iteration. Specifically, we propose a scheme\nfor choosing working rows effectively in randomized block Kaczmarz-type\nmethods. The convergence of the proposed methods are given. Numerical\nexperiments on both real-world and synthetic data sets show that the proposed\nmethods are superior to many state-of-the-art randomized Kaczmarz-type methods\nfor large-scale linear systems.",
    "descriptor": "",
    "authors": [
      "Gang Wu",
      "Qiao Chang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.08797"
  },
  {
    "id": "arXiv:2212.08798",
    "title": "Leveraging Wastewater Monitoring for COVID-19 Forecasting in the US: a  Deep Learning study",
    "abstract": "The outburst of COVID-19 in late 2019 was the start of a health crisis that\nshook the world and took millions of lives in the ensuing years. Many\ngovernments and health officials failed to arrest the rapid circulation of\ninfection in their communities. The long incubation period and the large\nproportion of asymptomatic cases made COVID-19 particularly elusive to track.\nHowever, wastewater monitoring soon became a promising data source in addition\nto conventional indicators such as confirmed daily cases, hospitalizations, and\ndeaths. Despite the consensus on the effectiveness of wastewater viral load\ndata, there is a lack of methodological approaches that leverage viral load to\nimprove COVID-19 forecasting. This paper proposes using deep learning to\nautomatically discover the relationship between daily confirmed cases and viral\nload data. We trained one Deep Temporal Convolutional Networks (DeepTCN) and\none Temporal Fusion Transformer (TFT) model to build a global forecasting\nmodel. We supplement the daily confirmed cases with viral loads and other\nsocio-economic factors as covariates to the models. Our results suggest that\nTFT outperforms DeepTCN and learns a better association between viral load and\ndaily cases. We demonstrated that equipping the models with the viral load\nimproves their forecasting performance significantly. Moreover, viral load is\nshown to be the second most predictive input, following the containment and\nhealth index. Our results reveal the feasibility of training a\nlocation-agnostic deep-learning model to capture the dynamics of infection\ndiffusion when wastewater viral load data is provided.",
    "descriptor": "",
    "authors": [
      "Mehrdad Fazli",
      "Heman Shakeri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08798"
  },
  {
    "id": "arXiv:2212.08800",
    "title": "Level-$k$ Meta-Learning for Pedestrian-Aware Self-Driving",
    "abstract": "One challenge for self-driving cars is their interactions not only with other\nvehicles but also with pedestrians in urban environments. The unpredictability\nof pedestrian behaviors at intersections can lead to a high rate of accidents.\nThe first pedestrian fatality caused by autonomous vehicles was reported in\n2018 when a self-driving Uber vehicle struck a woman crossing an intersection\nin Tempe, Arizona in the nighttime. There is a need for creating machine\nintelligence that allows autonomous vehicles to control the car and adapt to\ndifferent pedestrian behaviors to prevent accidents. In this work, (a) We\ndevelop a Level-$k$ Meta Reinforcement Learning model for the vehicle-human\ninteractions and define its solution concept; (b) We test our LK-MRL structure\nin level-$0$ pedestrians interacting with level-$1$ car scenario, compare the\ntrained policy with multiple baseline methods, and demonstrate its advantage in\nroad safety; (c) Furthermore, based on the properties of level-$k$ thinking, we\ntest our LK-MRL structure in level-$1$ pedestrians interacting with level-$2$\ncar scenario and verify by experimental results that LK-MRL maintains its\nadvantageous with the using of reinforcement learning of producing different\nlevel of agents with strategies of the best response of their lower level\nthinkers, which provides us possible to create higher level scenarios.",
    "descriptor": "",
    "authors": [
      "Haozhe Lei",
      "Quanyan Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2212.08800"
  },
  {
    "id": "arXiv:2212.08801",
    "title": "Comparison of Model-Free and Model-Based Learning-Informed Planning for  PointGoal Navigation",
    "abstract": "In recent years several learning approaches to point goal navigation in\npreviously unseen environments have been proposed. They vary in the\nrepresentations of the environments, problem decomposition, and experimental\nevaluation. In this work, we compare the state-of-the-art Deep Reinforcement\nLearning based approaches with Partially Observable Markov Decision Process\n(POMDP) formulation of the point goal navigation problem. We adapt the (POMDP)\nsub-goal framework proposed by [1] and modify the component that estimates\nfrontier properties by using partial semantic maps of indoor scenes built from\nimages' semantic segmentation. In addition to the well-known completeness of\nthe model-based approach, we demonstrate that it is robust and efficient in\nthat it leverages informative, learned properties of the frontiers compared to\nan optimistic frontier-based planner. We also demonstrate its data efficiency\ncompared to the end-to-end deep reinforcement learning approaches. We compare\nour results against an optimistic planner, ANS and DD-PPO on Matterport3D\ndataset using the Habitat Simulator. We show comparable, though slightly worse\nperformance than the SOTA DD-PPO approach, yet with far fewer data.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2211.07898\n",
    "authors": [
      "Yimeng Li",
      "Arnab Debnath",
      "Gregory J. Stein",
      "Jana Kosecka"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08801"
  },
  {
    "id": "arXiv:2212.08802",
    "title": "Relational Sentence Embedding for Flexible Semantic Matching",
    "abstract": "We present Relational Sentence Embedding (RSE), a new paradigm to further\ndiscover the potential of sentence embeddings. Prior work mainly models the\nsimilarity between sentences based on their embedding distance. Because of the\ncomplex semantic meanings conveyed, sentence pairs can have various relation\ntypes, including but not limited to entailment, paraphrasing, and\nquestion-answer. It poses challenges to existing embedding methods to capture\nsuch relational information. We handle the problem by learning associated\nrelational embeddings. Specifically, a relation-wise translation operation is\napplied to the source sentence to infer the corresponding target sentence with\na pre-trained Siamese-based encoder. The fine-grained relational similarity\nscores can be computed from learned embeddings. We benchmark our method on 19\ndatasets covering a wide range of tasks, including semantic textual similarity,\ntransfer, and domain-specific tasks. Experimental results show that our method\nis effective and flexible in modeling sentence relations and outperforms a\nseries of state-of-the-art sentence embedding methods.\nhttps://github.com/BinWang28/RSE",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Bin Wang",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08802"
  },
  {
    "id": "arXiv:2212.08805",
    "title": "Understanding the Impact of Input Entropy on FPU, CPU, and GPU Power",
    "abstract": "Power is increasingly becoming a limiting resource in high-performance,\nGPU-accelerated computing systems. Understanding the range and sources of power\nvariation is essential in setting realistic bounds on rack and system peak\npower, and developing techniques that minimize energy. While variations arising\nduring manufacturing and other factors like algorithm among others have been\npreviously studied, this work shows that the program inputs can also severely\nimpact the power consumed not only on the GPU but also CPUs. Power variations\nof up to 67% were observed on an NVIDIA Ampere A100 GPU for the same algorithm\n(DGEMM benchmark) and input size with different matrix values. Our\ninvestigation shows that the values used as matrix elements, their position,\nand their uniqueness strongly influence power consumption. The implications of\nthis result on supercomputer performance and energy efficiency are further\ndiscussed.",
    "descriptor": "",
    "authors": [
      "Sridutt Bhalachandra",
      "Brian Austin",
      "Samuel Williams",
      "Nicholas J. Wright"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2212.08805"
  },
  {
    "id": "arXiv:2212.08808",
    "title": "Convergence, Consensus and Dissensus in the Weighted-Median Opinion  Dynamics",
    "abstract": "Mechanistic and tractable mathematical models play a key role in\nunderstanding how social influence shapes public opinions. Recently, a\nweighted-median mechanism has been proposed as a new micro-foundation of\nopinion dynamics and validated via experimental data. Numerical studies also\nindicate that this new mechanism recreates some non-trivial real-world features\nof opinion evolution. In this paper, we conduct a thorough theoretical analysis\nof the weighted-median opinion dynamics. We fully characterize the set of all\nequilibria, and we establish the almost-sure finite-time convergence for any\ninitial condition. Moreover, we prove a necessary and sufficient\ngraph-theoretic condition for the almost-sure convergence to consensus, as well\nas a sufficient graph-theoretic condition for almost-sure persistent dissensus.\nIt turns out that the weighted-median opinion dynamics, despite its simplicity\nin form, exhibit rich dynamical behavior that depends on some delicate network\nstructures. To complement our sufficient conditions for almost-sure dissensus,\nwe further prove that, given the influence network, determining whether the\nsystem almost surely achieves persistent dissensus is NP-hard, which reflects\nthe complexity the network topology contributes to opinion evolution.",
    "descriptor": "",
    "authors": [
      "Wenjun Mei",
      "Julien M. Hendrickx",
      "Ge Chen",
      "Francesco Bullo",
      "Florian D\u00f6rfler"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08808"
  },
  {
    "id": "arXiv:2212.08815",
    "title": "FSCNN: A Fast Sparse Convolution Neural Network Inference System",
    "abstract": "Convolution neural networks (CNNs) have achieved remarkable success, but\ntypically accompany high computation cost and numerous redundant weight\nparameters. To reduce the FLOPs, structure pruning is a popular approach to\nremove the entire hidden structures via introducing coarse-grained sparsity.\nMeanwhile, plentiful pruning works leverage fine-grained sparsity instead\n(sparsity are randomly distributed), whereas their sparse models lack special\ndesigned computing library for potential speedup. In this technical report, we\nstudy and present an efficient convolution neural network inference system to\naccelerate its forward pass by utilizing the fine-grained sparsity of\ncompressed CNNs. Our developed FSCNN is established based on a set of\nspecialized designed sparse data structures, operators and associated\nalgorithms. Experimentally, we validate that FSCNN outperforms standard deep\nlearning library PyTorch on popular CNN architectures such as VGG16 if\nsufficiently high sparsity exhibits. However, due to the contiguity issue of\nsparse operators, FSCNN is typically not comparable with highly optimized dense\noperator. Therefore, coarse-grained (structured) sparsity is our recommendation\nfor generic model compression.",
    "descriptor": "\nComments: technical report, sparse CNN\n",
    "authors": [
      "Bo Ji",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08815"
  },
  {
    "id": "arXiv:2212.08816",
    "title": "Improving Unsupervised Video Object Segmentation with Motion-Appearance  Synergy",
    "abstract": "We present IMAS, a method that segments the primary objects in videos without\nmanual annotation in training or inference. Previous methods in unsupervised\nvideo object segmentation (UVOS) have demonstrated the effectiveness of motion\nas either input or supervision for segmentation. However, motion signals may be\nuninformative or even misleading in cases such as deformable objects and\nobjects with reflections, causing unsatisfactory segmentation.\nIn contrast, IMAS achieves Improved UVOS with Motion-Appearance Synergy. Our\nmethod has two training stages: 1) a motion-supervised object discovery stage\nthat deals with motion-appearance conflicts through a learnable residual\npathway; 2) a refinement stage with both low- and high-level appearance\nsupervision to correct model misconceptions learned from misleading motion\ncues.\nAdditionally, we propose motion-semantic alignment as a model-agnostic\nannotation-free hyperparam tuning method. We demonstrate its effectiveness in\ntuning critical hyperparams previously tuned with human annotation or\nhand-crafted hyperparam-specific metrics.\nIMAS greatly improves the segmentation quality on several common UVOS\nbenchmarks. For example, we surpass previous methods by 8.3% on DAVIS16\nbenchmark with only standard ResNet and convolutional heads. We intend to\nrelease our code for future research and applications.",
    "descriptor": "\nComments: 15 pages, 10 figures\n",
    "authors": [
      "Long Lian",
      "Zhirong Wu",
      "Stella X. Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08816"
  },
  {
    "id": "arXiv:2212.08817",
    "title": "Accurate Open-set Recognition for Memory Workload",
    "abstract": "How can we accurately identify new memory workloads while classifying known\nmemory workloads? Verifying DRAM (Dynamic Random Access Memory) using various\nworkloads is an important task to guarantee the quality of DRAM. A crucial\ncomponent in the process is open-set recognition which aims to detect new\nworkloads not seen in the training phase. Despite its importance, however,\nexisting open-set recognition methods are unsatisfactory in terms of accuracy\nsince they fail to exploit the characteristics of workload sequences. In this\npaper, we propose Acorn, an accurate open-set recognition method capturing the\ncharacteristics of workload sequences. Acorn extracts two types of feature\nvectors to capture sequential patterns and spatial locality patterns in memory\naccess. Acorn then uses the feature vectors to accurately classify a\nsubsequence into one of the known classes or identify it as the unknown class.\nExperiments show that Acorn achieves state-of-the-art accuracy, giving up to\n37% points higher unknown class detection accuracy while achieving comparable\nknown class classification accuracy than existing methods.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Jun-Gi Jang",
      "Sooyeon Shim",
      "Vladimir Egay",
      "Jeeyong Lee",
      "Jongmin Park",
      "Suhyun Chae",
      "U Kang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08817"
  },
  {
    "id": "arXiv:2212.08818",
    "title": "Latent Evolution Model for Change Point Detection in Time-varying  Networks",
    "abstract": "Graph-based change point detection (CPD) play an irreplaceable role in\ndiscovering anomalous graphs in the time-varying network. While several\ntechniques have been proposed to detect change points by identifying whether\nthere is a significant difference between the target network and successive\nprevious ones, they neglect the natural evolution of the network. In practice,\nreal-world graphs such as social networks, traffic networks, and rating\nnetworks are constantly evolving over time. Considering this problem, we treat\nthe problem as a prediction task and propose a novel CPD method for dynamic\ngraphs via a latent evolution model. Our method focuses on learning the\nlow-dimensional representations of networks and capturing the evolving patterns\nof these learned latent representations simultaneously. After having the\nevolving patterns, a prediction of the target network can be achieved. Then, we\ncan detect the change points by comparing the prediction and the actual network\nby leveraging a trade-off strategy, which balances the importance between the\nprediction network and the normal graph pattern extracted from previous\nnetworks. Intensive experiments conducted on both synthetic and real-world\ndatasets show the effectiveness and superiority of our model.",
    "descriptor": "",
    "authors": [
      "Yongshun Gong",
      "Xue Dong",
      "Jian Zhang",
      "Meng Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08818"
  },
  {
    "id": "arXiv:2212.08820",
    "title": "Most Probable Densest Subgraphs",
    "abstract": "Computing the densest subgraph is a primitive graph operation with critical\napplications in detecting communities, events, and anomalies in biological,\nsocial, Web, and financial networks. In this paper, we study the novel problem\nof Most Probable Densest Subgraph (MPDS) discovery in uncertain graphs: Find\nthe node set that is the most likely to induce a densest subgraph in an\nuncertain graph. We further extend our problem by considering various notions\nof density, e.g., clique and pattern densities, studying the top-k MPDSs, and\nfinding the node set with the largest containment probability within densest\nsubgraphs. We show that it is #P-hard to compute the probability of a node set\ninducing a densest subgraph. We then devise sampling-based efficient\nalgorithms, with end-to-end accuracy guarantees, to compute the MPDS. Our\nthorough experimental results and real-world case studies on brain and social\nnetworks validate the effectiveness, efficiency, and usefulness of our\nsolution.",
    "descriptor": "\nComments: International Conference on Data Engineering (ICDE) 2023\n",
    "authors": [
      "Arkaprava Saha",
      "Xiangyu Ke",
      "Arijit Khan",
      "Cheng Long"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2212.08820"
  },
  {
    "id": "arXiv:2212.08821",
    "title": "Context-dependent Explainability and Contestability for Trustworthy  Medical Artificial Intelligence: Misclassification Identification of  Morbidity Recognition Models in Preterm Infants",
    "abstract": "Although machine learning (ML) models of AI achieve high performances in\nmedicine, they are not free of errors. Empowering clinicians to identify\nincorrect model recommendations is crucial for engendering trust in medical AI.\nExplainable AI (XAI) aims to address this requirement by clarifying AI\nreasoning to support the end users. Several studies on biomedical imaging\nachieved promising results recently. Nevertheless, solutions for models using\ntabular data are not sufficient to meet the requirements of clinicians yet.\nThis paper proposes a methodology to support clinicians in identifying failures\nof ML models trained with tabular data. We built our methodology on three main\npillars: decomposing the feature set by leveraging clinical context latent\nspace, assessing the clinical association of global explanations, and Latent\nSpace Similarity (LSS) based local explanations. We demonstrated our\nmethodology on ML-based recognition of preterm infant morbidities caused by\ninfection. The risk of mortality, lifelong disability, and antibiotic\nresistance due to model failures was an open research question in this domain.\nWe achieved to identify misclassification cases of two models with our\napproach. By contextualizing local explanations, our solution provides\nclinicians with actionable insights to support their autonomy for informed\nfinal decisions.",
    "descriptor": "\nComments: 27 pages, 5 figures\n",
    "authors": [
      "Isil Guzey",
      "Ozlem Ucar",
      "Nukhet Aladag Ciftdemir",
      "Betul Acunas"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08821"
  },
  {
    "id": "arXiv:2212.08822",
    "title": "Better Datastore, Better Translation: Generating Datastores from  Pre-Trained Models for Nearest Neural Machine Translation",
    "abstract": "Nearest Neighbor Machine Translation (kNNMT) is a simple and effective method\nof augmenting neural machine translation (NMT) with a token-level nearest\nneighbor retrieval mechanism. The effectiveness of kNNMT directly depends on\nthe quality of retrieved neighbors. However, original kNNMT builds datastores\nbased on representations from NMT models, which would result in poor retrieval\naccuracy when NMT models are not good enough, leading to sub-optimal\ntranslation performance. In this paper, we propose PRED, a framework that\nleverages Pre-trained models for Datastores in kNN-MT. Better representations\nfrom pre-trained models allow us to build datastores of better quality. We also\ndesign a novel contrastive alignment objective to mitigate the representation\ngap between the NMT model and pre-trained models, enabling the NMT model to\nretrieve from better datastores. We conduct extensive experiments on both\nbilingual and multilingual translation benchmarks, including WMT17 English\n$\\leftrightarrow$ Chinese, WMT14 English $\\leftrightarrow$ German, IWSLT14\nGerman $\\leftrightarrow$ English, and IWSLT14 multilingual datasets. Empirical\nresults demonstrate the effectiveness of PRED.",
    "descriptor": "",
    "authors": [
      "Jiahuan Li",
      "Shanbo Cheng",
      "Zewei Sun",
      "Mingxuan Wang",
      "Shujian Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08822"
  },
  {
    "id": "arXiv:2212.08829",
    "title": "Just Testing",
    "abstract": "The concept of must testing is naturally parametrised with a chosen\ncompleteness criterion, defining the complete runs of a system. Here I employ\njustness as this completeness criterion, instead of the traditional choice of\nprogress. The resulting must-testing preorder is incomparable with the default\none, and can be characterised as the fair failure preorder of Vogler. It also\nis the coarsest precongruence preserving linear time properties when assuming\njustness.\nAs my system model I here employ Petri nets with read arcs. Through their\nPetri net semantics, this work applies equally well to process algebras. I\nprovide a Petri net semantics for a standard process algebra extended with\nsignals; the read arcs are necessary to capture those signals.",
    "descriptor": "",
    "authors": [
      "Rob van Glabbeek"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2212.08829"
  },
  {
    "id": "arXiv:2212.08830",
    "title": "Inductive Attention for Video Action Anticipation",
    "abstract": "Anticipating future actions based on video observations is an important task\nin video understanding, which would be useful for some precautionary systems\nthat require response time to react before an event occurs. Since the input in\naction anticipation is only pre-action frames, models do not have enough\ninformation about the target action; moreover, similar pre-action frames may\nlead to different futures. Consequently, any solution using existing action\nrecognition models can only be suboptimal. Recently, researchers have proposed\nusing a longer video context to remedy the insufficient information in\npre-action intervals, as well as the self-attention to query past relevant\nmoments to address the anticipation problem. However, the indirect use of video\ninput features as the query might be inefficient, as it only serves as the\nproxy to the anticipation goal. To this end, we propose an inductive attention\nmodel, which transparently uses prior prediction as the query to derive the\nanticipation result by induction from past experience. Our method naturally\nconsiders the uncertainty of multiple futures via the many-to-many association.\nOn the large-scale egocentric video datasets, our model not only shows\nconsistently better performance than state of the art using the same backbone,\nand is competitive to the methods that employ a stronger backbone, but also\nsuperior efficiency in less model parameters.",
    "descriptor": "",
    "authors": [
      "Tsung-Ming Tai",
      "Giuseppe Fiameni",
      "Cheng-Kuang Lee",
      "Simon See",
      "Oswald Lanz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08830"
  },
  {
    "id": "arXiv:2212.08832",
    "title": "Performance Analysis and Optimization of Network-Assisted Full-Duplex  Systems under Low-Resolution ADCs",
    "abstract": "Network-assisted full-duplex (NAFD) distributed massive multiple input\nmultiple output (M-MIMO) enables the in-band full-duplex with existing\nhalf-duplex devices at the network level, which exceptionally improves spectral\nefficiency. This paper analyzes the impact of low-resolution analog-to-digital\nconverters (ADCs) on NAFD distributed M-MIMO and designs an efficient bit\nallocation algorithm for low-resolution ADCs. The beamforming training\nmechanism relieves the heavy pilot overhead for channel estimation, which\nremarkably enhances system performance by guiding the interference cancellation\nand coherence detection. Furthermore, closed-form expressions for spectral and\nenergy efficiency with low-resolution ADCs are derived. The multi-objective\noptimization problem (MOOP) for spectral and energy efficiency is solved by the\ndeep Q network and the non-dominated sorting genetic algorithm II. The\nsimulation results corroborate the theoretical derivation and verify the\neffectiveness of introducing low-resolution ADCs in NAFD distributed M-MIMO\nsystems. Meanwhile, a set of Pareto-optimal solutions for ADC accuracy flexibly\nprovide guidelines for deploying in a practical NAFD distributed M-MIMO system.",
    "descriptor": "",
    "authors": [
      "Xiangning Song",
      "Zhenhao Ji",
      "Jiamin Li",
      "Pengcheng Zhu",
      "Dongming Wang",
      "Xiaohu You"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.08832"
  },
  {
    "id": "arXiv:2212.08834",
    "title": "Towards Robust Handwritten Text Recognition with On-the-fly User  Participation",
    "abstract": "Long-term OCR services aim to provide high-quality output to their users at\ncompetitive costs. It is essential to upgrade the models because of the complex\ndata loaded by the users. The service providers encourage the users who provide\ndata where the OCR model fails by rewarding them based on data complexity,\nreadability, and available budget. Hitherto, the OCR works include preparing\nthe models on standard datasets without considering the end-users. We propose a\nstrategy of consistently upgrading an existing Handwritten Hindi OCR model\nthree times on the dataset of 15 users. We fix the budget of 4 users for each\niteration. For the first iteration, the model directly trains on the dataset\nfrom the first four users. For the rest iteration, all remaining users write a\npage each, which service providers later analyze to select the 4 (new) best\nusers based on the quality of predictions on the human-readable words. Selected\nusers write 23 more pages for upgrading the model. We upgrade the model with\nCurriculum Learning (CL) on the data available in the current iteration and\ncompare the subset from previous iterations. The upgraded model is tested on a\nheld-out set of one page each from all 23 users. We provide insights into our\ninvestigations on the effect of CL, user selection, and especially the data\nfrom unseen writing styles. Our work can be used for long-term OCR services in\ncrowd-sourcing scenarios for the service providers and end users.",
    "descriptor": "",
    "authors": [
      "Ajoy Mondal",
      "Rohit saluja",
      "C. V. Jawahar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08834"
  },
  {
    "id": "arXiv:2212.08839",
    "title": "Convergence of the tamed-Euler-Maruyama method for SDEs with  discontinuous and polynomially growing drift",
    "abstract": "Numerical methods for SDEs with irregular coefficients are intensively\nstudied in the literature, with different types of irregularities usually being\nattacked separately. In this paper we combine two different types of\nirregularities: polynomially growing drift coefficients and discontinuous drift\ncoefficients. For SDEs that suffer from both irregularities we prove strong\nconvergence of order $1/2$ of the tamed-Euler-Maruyama scheme.",
    "descriptor": "",
    "authors": [
      "Kathrin Spendier",
      "Michaela Sz\u00f6lgyenyi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2212.08839"
  },
  {
    "id": "arXiv:2212.08841",
    "title": "Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable  Augmentation with Query Extraction and Generation",
    "abstract": "Dense retrievers have made significant strides in obtaining state-of-the-art\nresults on text retrieval and open-domain question answering (ODQA). Yet most\nof these achievements were made possible with the help of large annotated\ndatasets, unsupervised learning for dense retrieval models remains an open\nproblem. In this work, we explore two categories of methods for creating pseudo\nquery-document pairs, named query extraction (QExt) and transferred query\ngeneration (TQGen), to augment the retriever training in an annotation-free and\nscalable manner. Specifically, QExt extracts pseudo queries by document\nstructures or selecting salient random spans, and TQGen utilizes generation\nmodels trained for other NLP tasks (e.g., summarization) to produce pseudo\nqueries. Extensive experiments show that dense retrievers trained with\nindividual augmentation methods can perform comparably well with multiple\nstrong baselines, and combining them leads to further improvements, achieving\nstate-of-the-art performance of unsupervised dense retrieval on both BEIR and\nODQA datasets.",
    "descriptor": "",
    "authors": [
      "Rui Meng",
      "Ye Liu",
      "Semih Yavuz",
      "Divyansh Agarwal",
      "Lifu Tu",
      "Ning Yu",
      "Jianguo Zhang",
      "Meghana Bhat",
      "Yingbo Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.08841"
  },
  {
    "id": "arXiv:2212.08844",
    "title": "Asymptotic-preserving schemes for kinetic-fluid modeling of mixture  flows with distinct particle sizes",
    "abstract": "We consider coupled models for particulate flows, where the disperse phase is\nmade of particles with distinct sizes. We are thus led to a system coupling the\nincompressible Navier-Stokes equations to the multi-component\nVlasov-Fokker-Planck equations. We design an asymptotic-preserving numerical\nscheme to approximate the system. The scheme is based on suitable implicit\ntreatment of the stiff drag force term as well as the Fokker-Planck operator,\nand can be formally shown to capture the hydrodynamic limit with time step and\nmesh size independent of the Stokes number. Numerical examples illustrate the\naccuracy and asymptotic behavior of the scheme, with several interesting\napplications.",
    "descriptor": "",
    "authors": [
      "Shi Jin",
      "Yiwen Lin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.08844"
  },
  {
    "id": "arXiv:2212.08846",
    "title": "Painterly Image Harmonization in Dual Domains",
    "abstract": "Image harmonization aims to produce visually harmonious composite images by\nadjusting the foreground appearance to be compatible with the background. When\nthe composite image has photographic foreground and painterly background, the\ntask is called painterly image harmonization. There are only few works on this\ntask, which are either time-consuming or weak in generating well-harmonized\nresults. In this work, we propose a novel painterly harmonization network\nconsisting of a dual-domain generator and a dual-domain discriminator, which\nharmonizes the composite image in both spatial domain and frequency domain. The\ndual-domain generator performs harmonization by using AdaIn modules in the\nspatial domain and our proposed ResFFT modules in the frequency domain. The\ndual-domain discriminator attempts to distinguish the inharmonious patches\nbased on the spatial feature and frequency feature of each patch, which can\nenhance the ability of generator in an adversarial manner. Extensive\nexperiments on the benchmark dataset show the effectiveness of our method. Our\ncode and model are available at\nhttps://github.com/bcmi/PHDNet-Painterly-Image-Harmonization.",
    "descriptor": "\nComments: Accepted by AAAI2023\n",
    "authors": [
      "Junyan Cao",
      "Yan Hong",
      "Li Niu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08846"
  },
  {
    "id": "arXiv:2212.08853",
    "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden  Representation Perturbation",
    "abstract": "Language models with the Transformers structure have shown great performance\nin natural language processing. However, there still poses problems when\nfine-tuning pre-trained language models on downstream tasks, such as\nover-fitting or representation collapse. In this work, we propose HyPe, a\nsimple yet effective fine-tuning technique to alleviate such problems by\nperturbing hidden representations of Transformers layers. Unlike previous works\nthat only add noise to inputs or parameters, we argue that the hidden\nrepresentations of Transformers layers convey more diverse and meaningful\nlanguage information. Therefore, making the Transformers layers more robust to\nhidden representation perturbations can further benefit the fine-tuning of PLMs\nen bloc. We conduct extensive experiments and analyses on GLUE and other\nnatural language inference datasets. Results demonstrate that HyPe outperforms\nvanilla fine-tuning and enhances generalization of hidden representations from\ndifferent layers. In addition, HyPe acquires negligible computational\noverheads, and is better than and compatible with previous state-of-the-art\nfine-tuning techniques.",
    "descriptor": "\nComments: 17 pages; 5 figures\n",
    "authors": [
      "Hongyi Yuan",
      "Zheng Yuan",
      "Chuanqi Tan",
      "Fei Huang",
      "Songfang Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08853"
  },
  {
    "id": "arXiv:2212.08854",
    "title": "An Evolutionary Multitasking Algorithm with Multiple Filtering for  High-Dimensional Feature Selection",
    "abstract": "Recently, evolutionary multitasking (EMT) has been successfully used in the\nfield of high-dimensional classification. However, the generation of multiple\ntasks in the existing EMT-based feature selection (FS) methods is relatively\nsimple, using only the Relief-F method to collect related features with similar\nimportance into one task, which cannot provide more diversified tasks for\nknowledge transfer. Thus, this paper devises a new EMT algorithm for FS in\nhigh-dimensional classification, which first adopts different filtering methods\nto produce multiple tasks and then modifies a competitive swarm optimizer to\nefficiently solve these related tasks via knowledge transfer. First, a\ndiversified multiple task generation method is designed based on multiple\nfiltering methods, which generates several relevant low-dimensional FS tasks by\neliminating irrelevant features. In this way, useful knowledge for solving\nsimple and relevant tasks can be transferred to simplify and speed up the\nsolution of the original high-dimensional FS task. Then, a competitive swarm\noptimizer is modified to simultaneously solve these relevant FS tasks by\ntransferring useful knowledge among them. Numerous empirical results\ndemonstrate that the proposed EMT-based FS method can obtain a better feature\nsubset than several state-of-the-art FS methods on eighteen high-dimensional\ndatasets.",
    "descriptor": "",
    "authors": [
      "Lingjie Li",
      "Manlin Xuan",
      "Qiuzhen Lin",
      "Min Jiang",
      "Zhong Ming",
      "Kay Chen Tan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.08854"
  },
  {
    "id": "arXiv:2212.08857",
    "title": "Automata and automatic sequences",
    "abstract": "In the following pages we discuss infinite sequences defined on a finite\nalphabet, and more specially those which are generated by finite automata. We\nhave divided our paper into seven parts which are more or less self-contained.\nNeedless to say, we feel that the order we propose is the most natural one.\nReferences appear at the end of each one of the parts which implies some\nredundancy. Extra references are listed at the very end of our paper.",
    "descriptor": "",
    "authors": [
      "Jean-Paul Allouche",
      "Michel Mend\u00e8s France"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Discrete Mathematics (cs.DM)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2212.08857"
  },
  {
    "id": "arXiv:2212.08859",
    "title": "iCub! Do you recognize what I am doing?: multimodal human action  recognition on multisensory-enabled iCub robot",
    "abstract": "This study uses multisensory data (i.e., color and depth) to recognize human\nactions in the context of multimodal human-robot interaction. Here we employed\nthe iCub robot to observe the predefined actions of the human partners by using\nfour different tools on 20 objects. We show that the proposed multimodal\nensemble learning leverages complementary characteristics of three color\ncameras and one depth sensor that improves, in most cases, recognition accuracy\ncompared to the models trained with a single modality. The results indicate\nthat the proposed models can be deployed on the iCub robot that requires\nmultimodal action recognition, including social tasks such as partner-specific\nadaptation, and contextual behavior understanding, to mention a few.",
    "descriptor": "\nComments: 7 pages, 5 figures and 1 table. International Conference on Social Robotics\n",
    "authors": [
      "Kas Kniesmeijer",
      "Murat Kirtay"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08859"
  },
  {
    "id": "arXiv:2212.08860",
    "title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement  Learning",
    "abstract": "Learning generalizable policies that can adapt to unseen environments remains\nchallenging in visual Reinforcement Learning (RL). Existing approaches try to\nacquire a robust representation via diversifying the appearances of in-domain\nobservations for better generalization. Limited by the specific observations of\nthe environment, these methods ignore the possibility of exploring diverse\nreal-world image datasets. In this paper, we investigate how a visual RL agent\nwould benefit from the off-the-shelf visual representations. Surprisingly, we\nfind that the early layers in an ImageNet pre-trained ResNet model could\nprovide rather generalizable representations for visual RL. Hence, we propose\nPre-trained Image Encoder for Generalizable visual reinforcement learning\n(PIE-G), a simple yet effective framework that can generalize to the unseen\nvisual scenarios in a zero-shot manner. Extensive experiments are conducted on\nDMControl Generalization Benchmark, DMControl Manipulation Tasks, Drawer World,\nand CARLA to verify the effectiveness of PIE-G. Empirical evidence suggests\nPIE-G improves sample efficiency and significantly outperforms previous\nstate-of-the-art methods in terms of generalization performance. In particular,\nPIE-G boasts a 55% generalization performance gain on average in the\nchallenging video background setting. Project Page:\nhttps://sites.google.com/view/pie-g/home.",
    "descriptor": "",
    "authors": [
      "Zhecheng Yuan",
      "Zhengrong Xue",
      "Bo Yuan",
      "Xueqian Wang",
      "Yi Wu",
      "Yang Gao",
      "Huazhe Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08860"
  },
  {
    "id": "arXiv:2212.08861",
    "title": "DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models",
    "abstract": "In recent years, generative models have undergone significant advancement due\nto the success of diffusion models. The success of these models is often\nattributed to their use of guidance techniques, such as classifier and\nclassifier-free methods, which provides effective mechanisms to trade-off\nbetween fidelity and diversity. However, these methods are not capable of\nguiding a generated image to be aware of its geometric configuration, e.g.,\ndepth, which hinders the application of diffusion models to areas that require\na certain level of depth awareness. To address this limitation, we propose a\nnovel guidance approach for diffusion models that uses estimated depth\ninformation derived from the rich intermediate representations of diffusion\nmodels. To do this, we first present a label-efficient depth estimation\nframework using the internal representations of diffusion models. At the\nsampling phase, we utilize two guidance techniques to self-condition the\ngenerated image using the estimated depth map, the first of which uses\npseudo-labeling, and the subsequent one uses a depth-domain diffusion prior.\nExperiments and extensive ablation studies demonstrate the effectiveness of our\nmethod in guiding the diffusion models toward geometrically plausible image\ngeneration. Project page is available at https://ku-cvlab.github.io/DAG/.",
    "descriptor": "\nComments: Project page is available at this https URL\n",
    "authors": [
      "Gyeongnyeon Kim",
      "Wooseok Jang",
      "Gyuseong Lee",
      "Susung Hong",
      "Junyoung Seo",
      "Seungryong Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08861"
  },
  {
    "id": "arXiv:2212.08864",
    "title": "'If you build they will come': Automatic Identification of  News-Stakeholders to detect Party Preference in News Coverage",
    "abstract": "The coverage of different stakeholders mentioned in the news articles\nsignificantly impacts the slant or polarity detection of the concerned news\npublishers. For instance, the pro-government media outlets would give more\ncoverage to the government stakeholders to increase their accessibility to the\nnews audiences. In contrast, the anti-government news agencies would focus more\non the views of the opponent stakeholders to inform the readers about the\nshortcomings of government policies. In this paper, we address the problem of\nstakeholder extraction from news articles and thereby determine the inherent\nbias present in news reporting. Identifying potential stakeholders in\nmulti-topic news scenarios is challenging because each news topic has different\nstakeholders. The research presented in this paper utilizes both contextual\ninformation and external knowledge to identify the topic-specific stakeholders\nfrom news articles. We also apply a sequential incremental clustering algorithm\nto group the entities with similar stakeholder types. We carried out all our\nexperiments on news articles on four Indian government policies published by\nnumerous national and international news agencies. We also further generalize\nour system, and the experimental results show that the proposed model can be\nextended to other news topics.",
    "descriptor": "\nComments: Accepted in AAAI-2023 Workshop, AI for Credible Elections\n",
    "authors": [
      "Alapan Kuila",
      "Sudeshna Sarkar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2212.08864"
  },
  {
    "id": "arXiv:2212.08872",
    "title": "Pilot Reuse in Cell-Free Massive MIMO Systems: A Diverse Clustering  Approach",
    "abstract": "Distributed or Cell-free (CF) massive Multiple-Input, Multiple-Output\n(mMIMO), has been recently proposed as an answer to the limitations of the\ncurrent network-centric systems in providing high-rate ubiquitous transmission.\nThe capability of providing uniform service level makes CF mMIMO a potential\ntechnology for beyond-5G and 6G networks. The acquisition of accurate Channel\nState Information (CSI) is critical for different CF mMIMO operations. Hence,\nan uplink pilot training phase is used to efficiently estimate transmission\nchannels. The number of available orthogonal pilot signals is limited, and\nreusing these pilots will increase co-pilot interference. This causes an\nundesirable effect known as pilot contamination that could reduce the system\nperformance. Hence, a proper pilot reuse strategy is needed to mitigate the\neffects of pilot contamination. In this paper, we formulate pilot assignment in\nCF mMIMO as a diverse clustering problem and propose an iterative maxima search\nscheme to solve it. In this approach, we first form the clusters of User\nEquipments (UEs) so that the intra-cluster diversity maximizes and then assign\nthe same pilots for all UEs in the same cluster. The numerical results show the\nproposed techniques' superiority over other methods concerning the achieved\nuplink and downlink average and per-user data rate.",
    "descriptor": "\nComments: 29 pages, 9 figures, submitted to IEEE\n",
    "authors": [
      "Salman Mohebi",
      "Andrea Zanella",
      "Michele Zorzi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.08872"
  },
  {
    "id": "arXiv:2212.08876",
    "title": "The generalized e-bundle",
    "abstract": "In previous work, we introduced the notion of an impact bundle, showing how\ne.g., the h-index and the g-index can lead to such a bundle. Here we extend the\nset of impact bundles by a new impact bundle, based on the Zhang e-index. It\nis, moreover, shown that some other plausible definitions do not lead to an\nimpact bundle.",
    "descriptor": "",
    "authors": [
      "Leo Egghe",
      "Ronald Rousseau"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2212.08876"
  },
  {
    "id": "arXiv:2212.08879",
    "title": "Study on Domain Name System (DNS) Abuse: Technical Report",
    "abstract": "A safe and secure Domain Name System (DNS) is of paramount importance for the\ndigital economy and society. Malicious activities on the DNS, generally\nreferred to as \"DNS abuse\" are frequent and severe problems affecting online\nsecurity and undermining users' trust in the Internet. The proposed definition\nof DNS abuse is as follows: Domain Name System (DNS) abuse is any activity that\nmakes use of domain names or the DNS protocol to carry out harmful or illegal\nactivity. DNS abuse exploits the domain name registration process, the domain\nname resolution process, or other services associated with the domain name\n(e.g., shared web hosting service). Notably, we distinguish between:\nmaliciously registered domain names: domain name registered with the malicious\nintent to carry out harmful or illegal activity compromised domain names:\ndomain name registered by bona fide third-party for legitimate purposes,\ncompromised by malicious actors to carry out harmful and illegal activity. DNS\nabuse disrupts, damages, or otherwise adversely impacts the DNS and the\nInternet infrastructure, their users or other persons.",
    "descriptor": "",
    "authors": [
      "Jan Bayer",
      "Yevheniya Nosyk",
      "Olivier Hureau",
      "Simon Fernandez",
      "Ivett Paulovics",
      "Andrzej Duda",
      "Maciej Korczy\u0144ski"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.08879"
  },
  {
    "id": "arXiv:2212.08882",
    "title": "ProNet: Adaptive Process Noise Estimation for INS/DVL Fusion",
    "abstract": "Inertial and Doppler velocity log sensors are commonly used to provide the\nnavigation solution for autonomous underwater vehicles (AUV). To this end, a\nnonlinear filter is adopted for the fusion task. The filter's process noise\ncovariance matrix is critical for filter accuracy and robustness. While this\nmatrix varies over time during the AUV mission, the filter assumes a constant\nmatrix. Several models and learning approaches in the literature suggest tuning\nthe process noise covariance during operation. In this work, we propose ProNet,\na hybrid, adaptive process, noise estimation approach for a velocity-aided\nnavigation filter. ProNet requires only the inertial sensor reading to regress\nthe process noise covariance. Once learned, it is fed into the model-based\nnavigation filter, resulting in a hybrid filter. Simulation results show the\nbenefits of our approach compared to other models and learning adaptive\napproaches.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2211.01329\n",
    "authors": [
      "Barak Or",
      "Itzik Klein"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08882"
  },
  {
    "id": "arXiv:2212.08883",
    "title": "Modeling Global Distribution for Federated Learning with Label  Distribution Skew",
    "abstract": "Federated learning achieves joint training of deep models by connecting\ndecentralized data sources, which can significantly mitigate the risk of\nprivacy leakage. However, in a more general case, the distributions of labels\namong clients are different, called ``label distribution skew''. Directly\napplying conventional federated learning without consideration of label\ndistribution skew issue significantly hurts the performance of the global\nmodel. To this end, we propose a novel federated learning method, named FedMGD,\nto alleviate the performance degradation caused by the label distribution skew\nissue. It introduces a global Generative Adversarial Network to model the\nglobal data distribution without access to local datasets, so the global model\ncan be trained using the global information of data distribution without\nprivacy leakage. The experimental results demonstrate that our proposed method\nsignificantly outperforms the state-of-the-art on several public benchmarks.\nCode is available at \\url{https://github.com/Sheng-T/FedMGD}.",
    "descriptor": "",
    "authors": [
      "Tao Sheng",
      "Chengchao Shen",
      "Yuan Liu",
      "Yeyu Ou",
      "Zhe Qu",
      "Jianxin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08883"
  },
  {
    "id": "arXiv:2212.08888",
    "title": "Exploiting Rich Textual User-Product Context for Improving Sentiment  Analysis",
    "abstract": "User and product information associated with a review is useful for sentiment\npolarity prediction. Typical approaches incorporating such information focus on\nmodeling users and products as implicitly learned representation vectors. Most\ndo not exploit the potential of historical reviews, or those that currently do\nrequire unnecessary modifications to model architecture or do not make full use\nof user/product associations. The contribution of this work is twofold: i) a\nmethod to explicitly employ historical reviews belonging to the same\nuser/product to initialize representations, and ii) efficient incorporation of\ntextual associations between users and products via a user-product\ncross-context module. Experiments on IMDb, Yelp-2013 and Yelp-2014 benchmarks\nshow that our approach substantially outperforms previous state-of-the-art.\nSince we employ BERT-base as the encoder, we additionally provide experiments\nin which our approach performs well with Span-BERT and Longformer. Furthermore,\nexperiments where the reviews of each user/product in the training data are\ndownsampled demonstrate the effectiveness of our approach under a low-resource\nsetting.",
    "descriptor": "",
    "authors": [
      "Chenyang Lyu",
      "Linyi Yang",
      "Yue Zhang",
      "Yvette Graham",
      "Jennifer Foster"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08888"
  },
  {
    "id": "arXiv:2212.08890",
    "title": "TCFimt: Temporal Counterfactual Forecasting from Individual Multiple  Treatment Perspective",
    "abstract": "Determining causal effects of temporal multi-intervention assists\ndecision-making. Restricted by time-varying bias, selection bias, and\ninteractions of multiple interventions, the disentanglement and estimation of\nmultiple treatment effects from individual temporal data is still rare. To\ntackle these challenges, we propose a comprehensive framework of temporal\ncounterfactual forecasting from an individual multiple treatment perspective\n(TCFimt). TCFimt constructs adversarial tasks in a seq2seq framework to\nalleviate selection and time-varying bias and designs a contrastive\nlearning-based block to decouple a mixed treatment effect into separated main\ntreatment effects and causal interactions which further improves estimation\naccuracy. Through implementing experiments on two real-world datasets from\ndistinct fields, the proposed method shows satisfactory performance in\npredicting future outcomes with specific treatments and in choosing optimal\ntreatment type and timing than state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Pengfei Xi",
      "Guifeng Wang",
      "Zhipeng Hu",
      "Yu Xiong",
      "Mingming Gong",
      "Wei Huang",
      "Runze Wu",
      "Yu Ding",
      "Tangjie Lv",
      "Changjie Fan",
      "Xiangnan Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.08890"
  },
  {
    "id": "arXiv:2212.08892",
    "title": "Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud  Analysis",
    "abstract": "Point clouds are characterized by irregularity and unstructuredness, which\npose challenges in efficient data exploitation and discriminative feature\nextraction. In this paper, we present an unsupervised deep neural architecture\ncalled Flattening-Net to represent irregular 3D point clouds of arbitrary\ngeometry and topology as a completely regular 2D point geometry image (PGI)\nstructure, in which coordinates of spatial points are captured in colors of\nimage pixels. \\mr{Intuitively, Flattening-Net implicitly approximates a locally\nsmooth 3D-to-2D surface flattening process while effectively preserving\nneighborhood consistency.} \\mr{As a generic representation modality, PGI\ninherently encodes the intrinsic property of the underlying manifold structure\nand facilitates surface-style point feature aggregation.} To demonstrate its\npotential, we construct a unified learning framework directly operating on PGIs\nto achieve \\mr{diverse types of high-level and low-level} downstream\napplications driven by specific task networks, including classification,\nsegmentation, reconstruction, and upsampling. Extensive experiments demonstrate\nthat our methods perform favorably against the current state-of-the-art\ncompetitors. We will make the code and data publicly available at\nhttps://github.com/keeganhk/Flattening-Net.",
    "descriptor": "",
    "authors": [
      "Qijian Zhang",
      "Junhui Hou",
      "Yue Qian",
      "Yiming Zeng",
      "Juyong Zhang",
      "Ying He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08892"
  },
  {
    "id": "arXiv:2212.08896",
    "title": "Human Image Generation: A Comprehensive Survey",
    "abstract": "Image and video synthesis has become a blooming topic in computer vision and\nmachine learning communities along with the developments of deep generative\nmodels, due to its great academic and application value. Many researchers have\nbeen devoted to synthesizing high-fidelity human images as one of the most\ncommonly seen object categories in daily lives, where a large number of studies\nare performed based on various deep generative models, task settings and\napplications. Thus, it is necessary to give a comprehensive overview on these\nvariant methods on human image generation. In this paper, we divide human image\ngeneration techniques into three paradigms, i.e., data-driven methods,\nknowledge-guided methods and hybrid methods. For each route, the most\nrepresentative models and the corresponding variants are presented, where the\nadvantages and characteristics of different methods are summarized in terms of\nmodel architectures and input/output requirements. Besides, the main public\nhuman image datasets and evaluation metrics in the literature are also\nsummarized. Furthermore, due to the wide application potentials, two typical\ndownstream usages of synthesized human images are covered, i.e., data\naugmentation for person recognition tasks and virtual try-on for fashion\ncustomers. Finally, we discuss the challenges and potential directions of human\nimage generation to shed light on future research.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Zhen Jia",
      "Zhang Zhang",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08896"
  },
  {
    "id": "arXiv:2212.08897",
    "title": "Improving Question Answering Performance through Manual Annotation:  Costs, Benefits and Strategies",
    "abstract": "Recently proposed systems for open-domain question answering (OpenQA) require\nlarge amounts of training data to achieve state-of-the-art performance.\nHowever, data annotation is known to be time-consuming and therefore expensive\nto acquire. As a result, the appropriate datasets are available only for a\nhandful of languages (mainly English and Chinese). In this work, we introduce\nand publicly release PolQA, the first Polish dataset for OpenQA. It consists of\n7,000 questions, 87,525 manually labeled evidence passages, and a corpus of\nover 7,097,322 candidate passages. Each question is classified according to its\nformulation, type, as well as entity type of the answer. This resource allows\nus to evaluate the impact of different annotation choices on the performance of\nthe QA system and propose an efficient annotation strategy that increases the\npassage retrieval performance by 10.55 p.p. while reducing the annotation cost\nby 82%.",
    "descriptor": "",
    "authors": [
      "Piotr Rybak",
      "Piotr Przyby\u0142a",
      "Maciej Ogrodniczuk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08897"
  },
  {
    "id": "arXiv:2212.08898",
    "title": "A Unified Approach for Resilience and Causal Responsibility with Integer  Linear Programming (ILP) and LP Relaxations",
    "abstract": "Resilience is one of the key algorithmic problems underlying various forms of\nreverse data management (such as view maintenance, deletion propagation, and\nvarious interventions for fairness): What is the minimal number of tuples to\ndelete from a database in order to remove all answers from a query? A long-open\nquestion is determining those conjunctive queries (CQs) for which this problem\ncan be solved in guaranteed PTIME. We shed new light on this and the related\nproblem of causal responsibility by proposing a unified Integer Linear\nProgramming (ILP) formulation. It is unified in that it can solve both prior\nstudied restrictions (e.g., self-join-free CQs under set semantics that allow a\nPTIME solution) and new cases (e.g., all CQs under set or bag semantics It is\nalso unified in that all queries and all instances are treated with the same\napproach, and the algorithm is guaranteed to terminate in PTIME for the easy\ncases. We prove that, for all easy self-join-free CQs, the Linear Programming\n(LP) relaxation of our encoding is identical to the ILP solution and thus\nstandard ILP solvers are guaranteed to return the solution in PTIME. Our\napproach opens up the door to new variants and new fine-grained analysis: 1) It\nalso works under bag semantics and we give the first dichotomy result for bags\nsemantics in the problem space. 2) We give a more fine-grained analysis of the\ncomplexity of causal responsibility. 3) We recover easy instances for generally\nhard queries, such as instances with read-once provenance and instances that\nbecome easy because of Functional Dependencies in the data. 4) We solve an open\nconjecture from PODS 2020. 5) Experiments confirm that our results indeed\npredict the asymptotic running times, and that our universal ILP encoding is at\ntimes even faster to solve for the PTIME cases than a prior proposed dedicated\nflow algorithm.",
    "descriptor": "\nComments: 17 pages, 15 figures\n",
    "authors": [
      "Neha Makhija",
      "Wolfgang Gatterbauer"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2212.08898"
  },
  {
    "id": "arXiv:2212.08899",
    "title": "Design and Simulation of a Micro-coiled Digitally-Controlled Variable  Inductor with a Monolithically Integrated MEMS Switch",
    "abstract": "This work introduces the design analysis simulation and a standard MEMS\nfabrication process for a three dimensional microcoil with a magnetic core and\na digital switch configuration using a completely integrated fully MEMS\ncompatible process to achieve a digitally controlled inductance. The proposed\ndesign can also be utilized as a micro transformer. The proposed design\nconsists of five identical 3D coils and their corresponding MEMS switches.\nThese coils are digitally controlled to achieve a variable inductor ranging\nfrom one fifth of the coil inductance up to five times the coil inductance. A\nstandard five layer Polymumps process is proposed to fabricate the microcoils\nand the integrated switches. Each micro coil is anchored directly on chip\nconnected to the input signal from one side and the other is connected to the\nswitch. The Ni based magnetic core improves the coil response by confining and\nguiding the magnetic field in the magnetic device compared to Si core based by\nmore than five times. The presented coil has the number of windings limited by\nthe designed length and the minimum spacing that can be realized by standard\noptical lithography. The coil diameter is also restricted by the limits defined\nby optical lithography whereas the maximum height realizable by the Polymumps\nprocess limits the height of the magnetic core and accordingly results in lower\ninductor performance. Based on this technique we present coils ranging from 100\num in length and ten winding up to 1000 um in length and 100 windings. The new\nmonolithically integrated MEMS switches act as selectors to achieve a variable\ninductance with digital control to allow the selection among n inductance steps\nwhere n is the number of coils.",
    "descriptor": "\nComments: Submitted to Microelectronic Engineering Journal - Elsevier\n",
    "authors": [
      "Abdelhameed Sharaf",
      "S. M. Eladl",
      "A. Nasr",
      "Mohamed Serry"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08899"
  },
  {
    "id": "arXiv:2212.08900",
    "title": "Robust Predictive Output-Feedback Safety Filter for Uncertain Nonlinear  Control Systems",
    "abstract": "In real-world applications, we often require reliable decision making under\ndynamics uncertainties using noisy high-dimensional sensory data. Recently, we\nhave seen an increasing number of learning-based control algorithms developed\nto address the challenge of decision making under dynamics uncertainties. These\nalgorithms often make assumptions about the underlying unknown dynamics and, as\na result, can provide safety guarantees. This is more challenging for other\nwidely used learning-based decision making algorithms such as reinforcement\nlearning. Furthermore, the majority of existing approaches assume access to\nstate measurements, which can be restrictive in practice. In this paper,\ninspired by the literature on safety filters and robust output-feedback\ncontrol, we present a robust predictive output-feedback safety filter (RPOF-SF)\nframework that provides safety certification to an arbitrary controller applied\nto an uncertain nonlinear control system. The proposed RPOF-SF combines a\nrobustly stable observer that estimates the system state from noisy measurement\ndata and a predictive safety filter that renders an arbitrary controller safe\nby (possibly) minimally modifying the controller input to guarantee safety. We\nshow in theory that the proposed RPOF-SF guarantees constraint satisfaction\ndespite disturbances applied to the system. We demonstrate the efficacy of the\nproposed RPOF-SF algorithm using an uncertain mass-spring-damper system.",
    "descriptor": "\nComments: 9 pages, 2 figures, accepted for publication at the Conference on Decision and Control (CDC), 2022\n",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Angela P. Schoellig"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08900"
  },
  {
    "id": "arXiv:2212.08902",
    "title": "Know What I don't Know: Handling Ambiguous and Unanswerable Questions  for Text-to-SQL",
    "abstract": "The task of text-to-SQL is to convert a natural language question to its\ncorresponding SQL query in the context of relational tables. Existing\ntext-to-SQL parsers generate a \"plausible\" SQL query for an arbitrary user\nquestion, thereby failing to correctly handle problematic user questions. To\nformalize this problem, we conduct a preliminary study on the observed\nambiguous and unanswerable cases in text-to-SQL and summarize them into 6\nfeature categories. Correspondingly, we identify the causes behind each\ncategory and propose requirements for handling ambiguous and unanswerable\nquestions. Following this study, we propose a simple yet effective\ncounterfactual example generation approach for the automatic generation of\nambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a\nweakly supervised model DTE (Detecting-Then-Explaining) for error detection,\nlocalization, and explanation. Experimental results show that our model\nachieves the best result on both real-world examples and generated examples\ncompared with various baselines. We will release data and code for future\nresearch.",
    "descriptor": "\nComments: DTE\n",
    "authors": [
      "Bing Wang",
      "Yan Gao",
      "Zhoujun Li",
      "Jian-Guang Lou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08902"
  },
  {
    "id": "arXiv:2212.08904",
    "title": "Hyperbolic Hierarchical Contrastive Hashing",
    "abstract": "Hierarchical semantic structures, naturally existing in real-world datasets,\ncan assist in capturing the latent distribution of data to learn robust hash\ncodes for retrieval systems. Although hierarchical semantic structures can be\nsimply expressed by integrating semantically relevant data into a high-level\ntaxon with coarser-grained semantics, the construction, embedding, and\nexploitation of the structures remain tricky for unsupervised hash learning. To\ntackle these problems, we propose a novel unsupervised hashing method named\nHyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed\ncontinuous hash codes into hyperbolic space for accurate semantic expression\nsince embedding hierarchies in hyperbolic space generates less distortion than\nin hyper-sphere space and Euclidean space. In addition, we extend the K-Means\nalgorithm to hyperbolic space and perform the proposed hierarchical hyperbolic\nK-Means algorithm to construct hierarchical semantic structures adaptively. To\nexploit the hierarchical semantic structures in hyperbolic space, we designed\nthe hierarchical contrastive learning algorithm, including hierarchical\ninstance-wise and hierarchical prototype-wise contrastive learning. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed method\noutperforms the state-of-the-art unsupervised hashing methods. Codes will be\nreleased.",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Rukai Wei",
      "Yu Liu",
      "Jingkuan Song",
      "Yanzhao Xie",
      "Ke Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08904"
  },
  {
    "id": "arXiv:2212.08905",
    "title": "IoT Device Identification Based on Network Traffic Characteristics",
    "abstract": "IoT device identification plays an important role in monitoring and improving\nthe performance and security of IoT devices. Compared to traditional non-IoT\ndevices, IoT devices provide us with both unique challenges and opportunities\nin detecting the types of IoT devices. Based on critical insights obtained in\nour previous work on understanding the network traffic characteristics of IoT\ndevices, in this paper we develop an effective machine-learning based IoT\ndevice identification scheme, named iotID. In developing iotID, we extract 70\nfeatures of TCP flows from three complementary aspects: remote network servers\nand port numbers, packet-level traffic characteristics such as packet\ninter-arrival times, and flow-level traffic characteristics such as flow\nduration. Different from existing work, we take into account the imbalance\nnature of network traffic generated by various devices in both the learning and\nevaluation phases of iotID. Our performance studies based on network traffic\ncollected on a typical smart home environment consisting of both IoT and\nnon-IoT devices show that iotID can achieve a balanced accuracy score of above\n99%.",
    "descriptor": "",
    "authors": [
      "Md Mainuddin",
      "Zhenhai Duan",
      "Yingfei Dong",
      "Shaeke Salman",
      "Tania Taami"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.08905"
  },
  {
    "id": "arXiv:2212.08909",
    "title": "Controlling Styles in Neural Machine Translation with Activation Prompt",
    "abstract": "Neural machine translation(NMT) has aroused wide attention due to its\nimpressive quality. Beyond quality, controlling translation styles is also an\nimportant demand for many languages. Previous related studies mainly focus on\ncontrolling formality and gain some improvements. However, they still face two\nchallenges. The first is the evaluation limitation. Style contains abundant\ninformation including lexis, syntax, etc. But only formality is well studied.\nThe second is the heavy reliance on iterative fine-tuning when new styles are\nrequired. Correspondingly, this paper contributes in terms of the benchmark and\napproach. First, we re-visit this task and propose a multiway stylized machine\ntranslation (MSMT) benchmark, which includes multiple categories of styles in\nfour language directions to push the boundary of this task. Second, we propose\na method named style activation prompt (StyleAP) by retrieving prompts from\nstylized monolingual corpus, which needs no extra fine-tuning. Experiments show\nthat StyleAP could effectively control the style of translation and achieve\nremarkable performance. All of our data and code are released at\nhttps://github.com/IvanWang0730/StyleAP.",
    "descriptor": "",
    "authors": [
      "Yifan Wang",
      "Zewei Sun",
      "Shanbo Cheng",
      "Weiguo Zheng",
      "Mingxuan Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08909"
  },
  {
    "id": "arXiv:2212.08911",
    "title": "AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech  Translation",
    "abstract": "To alleviate the data scarcity problem in End-to-end speech translation (ST),\npre-training on data for speech recognition and machine translation is\nconsidered as an important technique. However, the modality gap between speech\nand text prevents the ST model from efficiently inheriting knowledge from the\npre-trained models. In this work, we propose AdaTranS for end-to-end ST. It\nadapts the speech features with a new shrinking mechanism to mitigate the\nlength mismatch between speech and text features by predicting word boundaries.\nExperiments on the MUST-C dataset demonstrate that AdaTranS achieves better\nperformance than the other shrinking-based methods, with higher inference speed\nand lower memory usage. Further experiments also show that AdaTranS can be\nequipped with additional alignment losses to further improve performance.",
    "descriptor": "",
    "authors": [
      "Xingshan Zeng",
      "Liangyou Li",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.08911"
  },
  {
    "id": "arXiv:2212.08912",
    "title": "Data-Driven Models for Traffic Flow at Junctions",
    "abstract": "Traffic flow on networks requires knowledge on the behavior across traffic\nintersections. For macroscopic models based on hyperbolic conservation laws\nthere exist nowadays many ad-hoc models describing this behavior. Based on car\ntrajectory data we propose a novel framework combining data-fitted models with\nthe requirements of consistent coupling conditions for macroscopic models of\ntraffic junctions. A method for deriving density and flux corresponding to the\ntraffic close to the junction for data-driven models is presented. Within the\nmodels parameter fitting as well as machine-learning approaches enter to obtain\nsuitable boundary conditions for macroscopic first and second-order traffic\nflow models. The prediction of various models are compared considering also\nexisting coupling rules at the junction. Numerical results imposing the\ndata-fitted coupling models on a traffic network are presented.",
    "descriptor": "\nComments: 27 pages, 7 figures, 6 tables\n",
    "authors": [
      "Michael Herty",
      "Niklas Kolbe"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.08912"
  },
  {
    "id": "arXiv:2212.08913",
    "title": "Claim Optimization in Computational Argumentation",
    "abstract": "An optimal delivery of arguments is key to persuasion in any debate, both for\nhumans and for AI systems. This requires the use of clear and fluent claims\nrelevant to the given debate. Prior work has studied the automatic assessment\nof argument quality extensively. Yet, no approach actually improves the quality\nso far. Our work is the first step towards filling this gap. We propose the\ntask of claim optimization: to rewrite argumentative claims to optimize their\ndelivery. As an initial approach, we first generate a candidate set of\noptimized claims using a sequence-to-sequence model, such as BART, while taking\ninto account contextual information. Our key idea is then to rerank generated\ncandidates with respect to different quality metrics to find the best\noptimization. In automatic and human evaluation, we outperform different\nreranking baselines on an English corpus, improving 60% of all claims\n(worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our\napproach often specifies claims with details, whereas it adds less evidence\nthan humans do. Moreover, its capabilities generalize well to other domains,\nsuch as instructional texts.",
    "descriptor": "",
    "authors": [
      "Gabriella Skitalinskaya",
      "Maximilian Splieth\u00f6ver",
      "Henning Wachsmuth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08913"
  },
  {
    "id": "arXiv:2212.08914",
    "title": "Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP  Benchmark",
    "abstract": "In recent years, vision-centric perception has flourished in various\nautonomous driving tasks, including 3D detection, semantic map construction,\nmotion forecasting, and depth estimation. Nevertheless, the latency of\nvision-centric approaches is too high for practical deployment (e.g., most\ncamera-based 3D detectors have a runtime greater than 300ms). To bridge the gap\nbetween ideal research and real-world applications, it is necessary to quantify\nthe trade-off between performance and efficiency. Traditionally,\nautonomous-driving perception benchmarks perform the offline evaluation,\nneglecting the inference time delay. To mitigate the problem, we propose the\nAutonomous-driving StreAming Perception (ASAP) benchmark, which is the first\nbenchmark to evaluate the online performance of vision-centric perception in\nautonomous driving. On the basis of the 2Hz annotated nuScenes dataset, we\nfirst propose an annotation-extending pipeline to generate high-frame-rate\nlabels for the 12Hz raw images. Referring to the practical deployment, the\nStreaming Perception Under constRained-computation (SPUR) evaluation protocol\nis further constructed, where the 12Hz inputs are utilized for streaming\nevaluation under the constraints of different computational resources. In the\nASAP benchmark, comprehensive experiment results reveal that the model rank\nalters under different constraints, suggesting that the model latency and\ncomputation budget should be considered as design choices to optimize the\npractical deployment. To facilitate further research, we establish baselines\nfor camera-based streaming 3D detection, which consistently enhance the\nstreaming performance across various hardware. ASAP project page:\nhttps://github.com/JeffWang987/ASAP.",
    "descriptor": "\nComments: code: this https URL\n",
    "authors": [
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Yunpeng Zhang",
      "Guan Huang",
      "Yun Ye",
      "Wenbo Xu",
      "Ziwei Chen",
      "Xingang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08914"
  },
  {
    "id": "arXiv:2212.08923",
    "title": "Analyzing the Traffic of MANETs using Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have been taking role in many areas, thanks to\ntheir expressive power on graph-structured data. On the other hand, Mobile\nAd-Hoc Networks (MANETs) are gaining attention as network technologies have\nbeen taken to the 5G level. However, there is no study that evaluates the\nefficiency of GNNs on MANETs. In this study, we aim to fill this absence by\nimplementing a MANET dataset in a popular GNN framework, i.e., PyTorch\nGeometric; and show how GNNs can be utilized to analyze the traffic of MANETs.\nWe operate an edge prediction task on the dataset with GraphSAGE (SAG) model,\nwhere SAG model tries to predict whether there is a link between two nodes. We\nconstrue several evaluation metrics to measure the performance and efficiency\nof GNNs on MANETs. SAG model showed 82.1 accuracy on average in the\nexperiments.",
    "descriptor": "\nComments: International Conference on Machine Learning & Applied Network Technologies (ICMLANT) 2022\n",
    "authors": [
      "Taha Tekdogan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2212.08923"
  },
  {
    "id": "arXiv:2212.08924",
    "title": "Convergence Analysis for Training Stochastic Neural Networks via  Stochastic Gradient Descent",
    "abstract": "In this paper, we carry out numerical analysis to prove convergence of a\nnovel sample-wise back-propagation method for training a class of stochastic\nneural networks (SNNs). The structure of the SNN is formulated as\ndiscretization of a stochastic differential equation (SDE). A stochastic\noptimal control framework is introduced to model the training procedure, and a\nsample-wise approximation scheme for the adjoint backward SDE is applied to\nimprove the efficiency of the stochastic optimal control solver, which is\nequivalent to the back-propagation for training the SNN. The convergence\nanalysis is derived with and without convexity assumption for optimization of\nthe SNN parameters. Especially, our analysis indicates that the number of SNN\ntraining steps should be proportional to the square of the number of layers in\nthe convex optimization case. Numerical experiments are carried out to validate\nthe analysis results, and the performance of the sample-wise back-propagation\nmethod for training SNNs is examined by benchmark machine learning examples.",
    "descriptor": "",
    "authors": [
      "Richard Archibald",
      "Feng Bao",
      "Yanzhao Cao",
      "Hui Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08924"
  },
  {
    "id": "arXiv:2212.08926",
    "title": "A Simple Baseline for Beam Search Reranking",
    "abstract": "Reranking methods in machine translation aim to close the gap between common\nevaluation metrics (e.g. BLEU) and maximum likelihood learning and decoding\nalgorithms. Prior works address this challenge by training models to rerank\nbeam search candidates according to their predicted BLEU scores, building upon\nlarge models pretrained on massive monolingual corpora -- a privilege that was\nnever made available to the baseline translation model. In this work, we\nexamine a simple approach for training rerankers to predict translation\ncandidates' BLEU scores without introducing additional data or parameters. Our\napproach can be used as a clean baseline, decoupled from external factors, for\nfuture research in this area.",
    "descriptor": "",
    "authors": [
      "Lior Vassertail",
      "Omer Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08926"
  },
  {
    "id": "arXiv:2212.08927",
    "title": "Increasing Physical Layer Security through Hyperchaos in VLC Systems",
    "abstract": "Visible Light Communication (VLC) systems have relatively higher security\ncompared with traditional Radio Frequency (RF) channels due to line-of-sight\n(LOS) propagation. However, they still are susceptible to eavesdropping. The\nproposed solution of the papers have been built on existing work on\nhyperchaos-based security measure to increase physical layer security from\neavesdroppers. A fourth-order Henon map is used to scramble the constellation\ndiagrams of the transmitted signals. The scramblers change the constellation\nsymbol of the system using a key. That key on the receiver side de-scrambles\nthe received data. The presented modulation scheme takes advantage of a higher\ndegree of the map to isolate the data transmission to a single dimension,\nallowing for better scrambling and synchronization. A sliding mode controller\nis used at the receiver in a master-slave configuration for projective\nsynchronization of the two Henon maps, which helps de-scramble the received\ndata. The data is only isolated for the users aware of the key for\nsynchronization, providing security against eavesdroppers. The proposed VLC\nsystem is compared against various existing approaches based on various\nmetrics. An improved Bit Error Rate and a lower information leakage are\nachieved for a variety of modulation schemes at an acceptable Signal-to-Noise\nRatio.",
    "descriptor": "",
    "authors": [
      "Ashish Sharma",
      "Harshil Bhatt"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.08927"
  },
  {
    "id": "arXiv:2212.08929",
    "title": "Joint Information Extraction with Cross-Task and Cross-Instance  High-Order Modeling",
    "abstract": "Prior works on Information Extraction (IE) typically predict different tasks\nand instances (e.g., event triggers, entities, roles, relations) independently,\nwhile neglecting their interactions and leading to model inefficiency. In this\nwork, we introduce a joint IE framework, HighIE, that learns and predicts\nmultiple IE tasks by integrating high-order cross-task and cross-instance\ndependencies. Specifically, we design two categories of high-order factors:\nhomogeneous factors and heterogeneous factors. Then, these factors are utilized\nto jointly predict labels of all instances. To address the intractability\nproblem of exact high-order inference, we incorporate a high-order neural\ndecoder that is unfolded from a mean-field variational inference method. The\nexperimental results show that our approach achieves consistent improvements on\nthree IE tasks compared with our baseline and prior work.",
    "descriptor": "",
    "authors": [
      "Zixia Jia",
      "Zhaohui Yan",
      "Wenjuan Han",
      "Zilong Zheng",
      "Kewei Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08929"
  },
  {
    "id": "arXiv:2212.08930",
    "title": "On Noisy Evaluation in Federated Hyperparameter Tuning",
    "abstract": "Hyperparameter tuning is critical to the success of federated learning\napplications. Unfortunately, appropriately selecting hyperparameters is\nchallenging in federated networks. Issues of scale, privacy, and heterogeneity\nintroduce noise in the tuning process and make it difficult to evaluate the\nperformance of various hyperparameters. In this work, we perform the first\nsystematic study on the effect of noisy evaluation in federated hyperparameter\ntuning. We first identify and rigorously explore key sources of noise,\nincluding client subsampling, data and systems heterogeneity, and data privacy.\nSurprisingly, our results indicate that even small amounts of noise can\nsignificantly impact tuning methods-reducing the performance of\nstate-of-the-art approaches to that of naive baselines. To address noisy\nevaluation in such scenarios, we propose a simple and effective approach that\nleverages public proxy data to boost the evaluation signal. Our work\nestablishes general challenges, baselines, and best practices for future work\nin federated hyperparameter tuning.",
    "descriptor": "\nComments: 19 pages, 15 figures, submitted to MLSys2023 v2: Fixed citation formatting\n",
    "authors": [
      "Kevin Kuo",
      "Pratiksha Thaker",
      "Mikhail Khodak",
      "John Ngyuen",
      "Daniel Jiang",
      "Ameet Talwalkar",
      "Virginia Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08930"
  },
  {
    "id": "arXiv:2212.08939",
    "title": "Neural-Network-Augmented Projection-Based Model Order Reduction for  Mitigating the Kolmogorov Barrier to Reducibility of CFD Models",
    "abstract": "Inspired by our previous work on mitigating the Kolmogorov barrier using a\nquadratic approximation manifold, we propose in this paper a computationally\ntractable approach for combining a projection-based reduced-order model (PROM)\nand an artificial neural network (ANN) for mitigating the Kolmogorov barrier to\nreducibility of convection-dominated flow problems. The main objective the\nPROM-ANN concept that we propose is to reduce the dimensionality of the online\napproximation of the solution beyond what is possible using affine and\nquadratic approximation manifolds. In contrast to previous approaches for\nconstructing arbitrarily nonlinear manifold approximations for nonlinear model\nreduction that exploited one form or another of ANN, the training of the\nPROM-ANN we propose in this paper does not involve data whose dimension scales\nwith that of the high-dimensional model; and this PROM-ANN is hyperreducible\nusing any well-established hyperreduction method. Hence, unlike many other\nANN-based approaches, the PROM-ANN concept we propose in this paper is\npractical for large-scale and industry-relevant CFD problems. Its potential is\ndemonstrated here for a parametric, shock-dominated, benchmark problem.",
    "descriptor": "",
    "authors": [
      "Joshua L Barnett",
      "Charbel Farhat",
      "Yvon Maday"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.08939"
  },
  {
    "id": "arXiv:2212.08944",
    "title": "Toward Data Heterogeneity of Federated Learning",
    "abstract": "Federated learning is a popular paradigm for machine learning. Ideally,\nfederated learning works best when all clients share a similar data\ndistribution. However, it is not always the case in the real world. Therefore,\nthe topic of federated learning on heterogeneous data has gained more and more\neffort from both academia and industry. In this project, we first do extensive\nexperiments to show how data skew and quantity skew will affect the performance\nof state-of-art federated learning algorithms. Then we propose a new algorithm\nFedMix which adjusts existing federated learning algorithms and we show its\nperformance. We find that existing state-of-art algorithms such as FedProx and\nFedNova do not have a significant improvement in all testing cases. But by\ntesting the existing and new algorithms, it seems that tweaking the client side\nis more effective than tweaking the server side.",
    "descriptor": "",
    "authors": [
      "Yuchuan Huang",
      "Chen Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08944"
  },
  {
    "id": "arXiv:2212.08946",
    "title": "Towards leveraging latent knowledge and Dialogue context for real-world  conversational question answering",
    "abstract": "In many real-world scenarios, the absence of external knowledge source like\nWikipedia restricts question answering systems to rely on latent internal\nknowledge in limited dialogue data. In addition, humans often seek answers by\nasking several questions for more comprehensive information. As the dialog\nbecomes more extensive, machines are challenged to refer to previous\nconversation rounds to answer questions. In this work, we propose to leverage\nlatent knowledge in existing conversation logs via a neural Retrieval-Reading\nsystem, enhanced with a TFIDF-based text summarizer refining lengthy\nconversational history to alleviate the long context issue. Our experiments\nshow that our Retrieval-Reading system can exploit retrieved background\nknowledge to generate significantly better answers. The results also indicate\nthat our context summarizer significantly helps both the retriever and the\nreader by introducing more concise and less noisy contextual information.",
    "descriptor": "",
    "authors": [
      "Shaomu Tan",
      "Denis Paperno"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08946"
  },
  {
    "id": "arXiv:2212.08949",
    "title": "Managing Temporal Resolution in Continuous Value Estimation: A  Fundamental Trade-off",
    "abstract": "A default assumption in reinforcement learning and optimal control is that\nexperience arrives at discrete time points on a fixed clock cycle. Many\napplications, however, involve continuous systems where the time discretization\nis not fixed but instead can be managed by a learning algorithm. By analyzing\nMonte-Carlo value estimation for LQR systems in both finite-horizon and\ninfinite-horizon settings, we uncover a fundamental trade-off between\napproximation and statistical error in value estimation. Importantly, these two\nerrors behave differently with respect to time discretization, which implies\nthat there is an optimal choice for the temporal resolution that depends on the\ndata budget. These findings show how adapting the temporal resolution can\nprovably improve value estimation quality in LQR systems from finite data.\nEmpirically, we demonstrate the trade-off in numerical simulations of LQR\ninstances and several non-linear environments.",
    "descriptor": "",
    "authors": [
      "Zichen Zhang",
      "Johannes Kirschner",
      "Junxi Zhang",
      "Francesco Zanini",
      "Alex Ayoub",
      "Masood Dehghan",
      "Dale Schuurmans"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.08949"
  },
  {
    "id": "arXiv:2212.08950",
    "title": "Beyond the C: Retargetable Decompilation using Neural Machine  Translation",
    "abstract": "The problem of reversing the compilation process, decompilation, is an\nimportant tool in reverse engineering of computer software. Recently,\nresearchers have proposed using techniques from neural machine translation to\nautomate the process in decompilation. Although such techniques hold the\npromise of targeting a wider range of source and assembly languages, to date\nthey have primarily targeted C code. In this paper we argue that existing\nneural decompilers have achieved higher accuracy at the cost of requiring\nlanguage-specific domain knowledge such as tokenizers and parsers to build an\nabstract syntax tree (AST) for the source language, which increases the\noverhead of supporting new languages. We explore a different tradeoff that, to\nthe extent possible, treats the assembly and source languages as plain text,\nand show that this allows us to build a decompiler that is easily retargetable\nto new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on\nGo, Fortran, OCaml, and C, and examine the impact of parameters such as\ntokenization and training data selection on the quality of decompilation,\nfinding that it achieves comparable decompilation results to prior work in\nneural decompilation with significantly less domain knowledge. We will release\nour training data, trained decompilation models, and code to help encourage\nfuture research into language-agnostic decompilation.",
    "descriptor": "",
    "authors": [
      "Iman Hosseini",
      "Brendan Dolan-Gavitt"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2212.08950"
  },
  {
    "id": "arXiv:2212.08952",
    "title": "Learning from Taxonomy: Multi-label Few-Shot Classification for Everyday  Sound Recognition",
    "abstract": "Everyday sound recognition aims to infer types of sound events in audio\nstreams. While many works succeeded in training models with high performance in\na fully-supervised manner, they are still restricted to the demand of large\nquantities of labelled data and the range of predefined classes. To overcome\nthese drawbacks, this work firstly curates a new database named FSD-FS for\nmulti-label few-shot audio classification. It then explores how to incorporate\naudio taxonomy in few-shot learning. Specifically, this work proposes\nlabel-dependent prototypical networks (LaD-protonet) to exploit parent-children\nrelationships between labels. Plus, it applies taxonomy-aware label smoothing\ntechniques to boost model performance. Experiments demonstrate that\nLaD-protonet outperforms original prototypical networks as well as other\nstate-of-the-art methods. Moreover, its performance can be further boosted when\ncombined with taxonomy-aware label smoothing.",
    "descriptor": "\nComments: submitted to ICASSP2023\n",
    "authors": [
      "Jinhua Liang",
      "Huy Phan",
      "Emmanouil Benetos"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.08952"
  },
  {
    "id": "arXiv:2212.08954",
    "title": "Cascaded Compositional Residual Learning for Complex Interactive  Behaviors",
    "abstract": "Real-world autonomous missions often require rich interaction with nearby\nobjects, such as doors or switches, along with effective navigation. However,\nsuch complex behaviors are difficult to learn because they involve both\nhigh-level planning and low-level motor control. We present a novel framework,\nCascaded Compositional Residual Learning (CCRL), which learns composite skills\nby recursively leveraging a library of previously learned control policies. Our\nframework learns multiplicative policy composition, task-specific residual\nactions, and synthetic goal information simultaneously while freezing the\nprerequisite policies. We further explicitly control the style of the motion by\nregularizing residual actions. We show that our framework learns joint-level\ncontrol policies for a diverse set of motor skills ranging from basic\nlocomotion to complex interactive navigation, including navigating around\nobstacles, pushing objects, crawling under a table, pushing a door open with\nits leg, and holding it open while walking through it. The proposed CCRL\nframework leads to policies with consistent styles and lower joint torques,\nwhich we successfully transfer to a real Unitree A1 robot without any\nadditional fine-tuning.",
    "descriptor": "\nComments: Website: this https URL\n",
    "authors": [
      "K. Niranjan Kumar",
      "Irfan Essa",
      "Sehoon Ha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08954"
  },
  {
    "id": "arXiv:2212.08955",
    "title": "Trusting the Explainers: Teacher Validation of Explainable Artificial  Intelligence for Course Design",
    "abstract": "Deep learning models for learning analytics have become increasingly popular\nover the last few years; however, these approaches are still not widely adopted\nin real-world settings, likely due to a lack of trust and transparency. In this\npaper, we tackle this issue by implementing explainable AI methods for\nblack-box neural networks. This work focuses on the context of online and\nblended learning and the use case of student success prediction models. We use\na pairwise study design, enabling us to investigate controlled differences\nbetween pairs of courses. Our analyses cover five course pairs that differ in\none educationally relevant aspect and two popular instance-based explainable AI\nmethods (LIME and SHAP). We quantitatively compare the distances between the\nexplanations across courses and methods. We then validate the explanations of\nLIME and SHAP with 26 semi-structured interviews of university-level educators\nregarding which features they believe contribute most to student success, which\nexplanations they trust most, and how they could transform these insights into\nactionable course design decisions. Our results show that quantitatively,\nexplainers significantly disagree with each other about what is important, and\nqualitatively, experts themselves do not agree on which explanations are most\ntrustworthy. All code, extended results, and the interview protocol are\nprovided at https://github.com/epfl-ml4ed/trusting-explainers.",
    "descriptor": "\nComments: Accepted as a full paper at LAK 2023: The 13th International Learning Analytics and Knowledge Conference, March 13-17, 2023, Arlington, Texas, USA\n",
    "authors": [
      "Vinitra Swamy",
      "Sijia Du",
      "Mirko Marras",
      "Tanja K\u00e4ser"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08955"
  },
  {
    "id": "arXiv:2212.08960",
    "title": "Two-sample test based on Self-Organizing Maps",
    "abstract": "Machine-learning classifiers can be leveraged as a two-sample statistical\ntest. Suppose each sample is assigned a different label and that a classifier\ncan obtain a better-than-chance result discriminating them. In this case, we\ncan infer that both samples originate from different populations. However, many\ntypes of models, such as neural networks, behave as a black-box for the user:\nthey can reject that both samples originate from the same population, but they\ndo not offer insight into how both samples differ. Self-Organizing Maps are a\ndimensionality reduction initially devised as a data visualization tool that\ndisplays emergent properties, being also useful for classification tasks. Since\nthey can be used as classifiers, they can be used also as a two-sample\nstatistical test. But since their original purpose is visualization, they can\nalso offer insights.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Alejandro \u00c1lvarez-Ayll\u00f3n",
      "Manuel Palomo-Duarte",
      "Juan-Manuel Dodero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.08960"
  },
  {
    "id": "arXiv:2212.08961",
    "title": "Training Robots to Evaluate Robots: Example-Based Interactive Reward  Functions for Policy Learning",
    "abstract": "Physical interactions can often help reveal information that is not readily\napparent. For example, we may tug at a table leg to evaluate whether it is\nbuilt well, or turn a water bottle upside down to check that it is watertight.\nWe propose to train robots to acquire such interactive behaviors automatically,\nfor the purpose of evaluating the result of an attempted robotic skill\nexecution. These evaluations in turn serve as \"interactive reward functions\"\n(IRFs) for training reinforcement learning policies to perform the target\nskill, such as screwing the table leg tightly. In addition, even after task\npolicies are fully trained, IRFs can serve as verification mechanisms that\nimprove online task execution. For any given task, our IRFs can be conveniently\ntrained using only examples of successful outcomes, and no further\nspecification is needed to train the task policy thereafter. In our evaluations\non door locking and weighted block stacking in simulation, and screw tightening\non a real robot, IRFs enable large performance improvements, even outperforming\nbaselines with access to demonstrations or carefully engineered rewards.\nProject website: https://sites.google.com/view/lirf-corl-2022/",
    "descriptor": "\nComments: CoRL 2022\n",
    "authors": [
      "Kun Huang",
      "Edward S. Hu",
      "Dinesh Jayaraman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08961"
  },
  {
    "id": "arXiv:2212.08964",
    "title": "GPU Load Balancing",
    "abstract": "Fine-grained workload and resource balancing is the key to high performance\nfor regular and irregular computations on the GPUs. In this dissertation, we\nconduct an extensive survey of existing load-balancing techniques to build an\nabstraction that addresses the difficulty of scheduling computations on the\nGPU.\nWe propose a GPU fine-grained load-balancing abstraction that decouples load\nbalancing from work processing and aims to support both static and dynamic\nschedules with a programmable interface to implement new load-balancing\nschedules. Prior to our work, the only way to unleash the GPU's potential on\nirregular problems has been to workload-balance through application-specific,\ntightly coupled load-balancing techniques. With our open-source framework for\nload-balancing, we hope to improve programmers' productivity when developing\nirregular-parallel algorithms on the GPU, and also improve the overall\nperformance characteristics for such applications by allowing a quick path to\nexperimentation with a variety of existing load-balancing techniques.\nUsing our insights from load-balancing irregular workloads, we build\nStream-K, a work-centric parallelization of matrix multiplication (GEMM) and\nrelated computations in dense linear algebra. Whereas contemporary\ndecompositions are primarily tile-based, our method operates by partitioning an\neven share of the aggregate inner loop iterations among physical processing\nelements. This provides a near-perfect utilization of computing resources,\nregardless of how efficiently the output tiling for any given problem quantizes\nacross the underlying processing elements. On GPU processors, our Stream-K\nparallelization of GEMM produces a peak speedup of up to 14x and 6.7x, and an\naverage performance response that is both higher and more consistent across 32K\nGEMM problem geometries than state-of-the-art math libraries such as CUTLASS\nand cuBLAS.",
    "descriptor": "\nComments: Submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy\n",
    "authors": [
      "Muhammad Osama"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2212.08964"
  },
  {
    "id": "arXiv:2212.08965",
    "title": "Physics-informed Neural Networks with Periodic Activation Functions for  Solute Transport in Heterogeneous Porous Media",
    "abstract": "Solute transport in porous media is relevant to a wide range of applications\nin hydrogeology, geothermal energy, underground CO2 storage, and a variety of\nchemical engineering systems. Due to the complexity of solute transport in\nheterogeneous porous media, traditional solvers require high resolution meshing\nand are therefore expensive computationally. This study explores the\napplication of a mesh-free method based on deep learning to accelerate the\nsimulation of solute transport. We employ Physics-informed Neural Networks\n(PiNN) to solve solute transport problems in homogeneous and heterogeneous\nporous media governed by the advection-dispersion equation. Unlike traditional\nneural networks that learn from large training datasets, PiNNs only leverage\nthe strong form mathematical models to simultaneously solve for multiple\ndependent or independent field variables (e.g., pressure and solute\nconcentration fields). In this study, we construct PiNN using a periodic\nactivation function to better represent the complex physical signals (i.e.,\npressure) and their derivatives (i.e., velocity). Several case studies are\ndesigned with the intention of investigating the proposed PiNN's capability to\nhandle different degrees of complexity. A manual hyperparameter tuning method\nis used to find the best PiNN architecture for each test case. Point-wise error\nand mean square error (MSE) measures are employed to assess the performance of\nPiNNs' predictions against the ground truth solutions obtained analytically or\nnumerically using the finite element method. Our findings show that the\npredictions of PiNN are in good agreement with the ground truth solutions while\nreducing computational complexity and cost by, at least, three orders of\nmagnitude.",
    "descriptor": "",
    "authors": [
      "Salah A Faroughi",
      "Pingki Datta",
      "Seyed Kourosh Mahjour",
      "Shirko Faroughi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08965"
  },
  {
    "id": "arXiv:2212.08966",
    "title": "Graph Learning: A Comprehensive Survey and Future Directions",
    "abstract": "Graph learning aims to learn complex relationships among nodes and the\ntopological structure of graphs, such as social networks, academic networks and\ne-commerce networks, which are common in the real world. Those relationships\nmake graphs special compared with traditional tabular data in which nodes are\ndependent on non-Euclidean space and contain rich information to explore. Graph\nlearning developed from graph theory to graph data mining and now is empowered\nwith representation learning, making it achieve great performances in various\nscenarios, even including text, image, chemistry, and biology. Due to the broad\napplication prospects in the real world, graph learning has become a popular\nand promising area in machine learning. Thousands of works have been proposed\nto solve various kinds of problems in graph learning and is appealing more and\nmore attention in academic community, which makes it pivotal to survey previous\nvaluable works. Although some of the researchers have noticed this phenomenon\nand finished impressive surveys on graph learning. However, they failed to link\nrelated objectives, methods and applications in a more logical way and cover\ncurrent ample scenarios as well as challenging problems due to the rapid\nexpansion of the graph learning.",
    "descriptor": "\nComments: 27 pages, 6 figures, 4 tables\n",
    "authors": [
      "Shaopeng Wei",
      "Yu Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08966"
  },
  {
    "id": "arXiv:2212.08967",
    "title": "Foundation models in brief: A historical, socio-technical focus",
    "abstract": "Foundation models can be disruptive for future AI development by scaling up\ndeep learning in terms of model size and training data's breadth and size.\nThese models achieve state-of-the-art performance (often through further\nadaptation) on a variety of tasks in domains such as natural language\nprocessing and computer vision. Foundational models exhibit a novel {emergent\nbehavior}: {In-context learning} enables users to provide a query and a few\nexamples from which a model derives an answer without being trained on such\nqueries. Additionally, {homogenization} of models might replace a myriad of\ntask-specific models with fewer very large models controlled by few\ncorporations leading to a shift in power and control over AI. This paper\nprovides a short introduction to foundation models. It contributes by crafting\na crisp distinction between foundation models and prior deep learning models,\nproviding a history of machine learning leading to foundation models,\nelaborating more on socio-technical aspects, i.e., organizational issues and\nend-user interaction, and a discussion of future research.",
    "descriptor": "",
    "authors": [
      "Johannes Schneider"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08967"
  },
  {
    "id": "arXiv:2212.08969",
    "title": "A Brief Survey on Person Recognition at a Distance",
    "abstract": "Person recognition at a distance entails recognizing the identity of an\nindividual appearing in images or videos collected by long-range imaging\nsystems such as drones or surveillance cameras. Despite recent advances in deep\nconvolutional neural networks (DCNNs), this remains challenging. Images or\nvideos collected by long-range cameras often suffer from atmospheric\nturbulence, blur, low-resolution, unconstrained poses, and poor illumination.\nIn this paper, we provide a brief survey of recent advances in person\nrecognition at a distance. In particular, we review recent work in\nmulti-spectral face verification, person re-identification, and gait-based\nanalysis techniques. Furthermore, we discuss the merits and drawbacks of\nexisting approaches and identify important, yet under explored challenges for\ndeploying remote person recognition systems in-the-wild.",
    "descriptor": "\nComments: This work has been accepted to the IEEE Asilomar Conference on Signals, Systems, and Computers (ACSSC) 2022\n",
    "authors": [
      "Chrisopher B. Nalty",
      "Neehar Peri",
      "Joshua Gleason",
      "Carlos D. Castillo",
      "Shuowen Hu",
      "Thirimachos Bourlai",
      "Rama Chellappa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08969"
  },
  {
    "id": "arXiv:2212.08973",
    "title": "Enhancing Cyber Resilience of Networked Microgrids using Vertical  Federated Reinforcement Learning",
    "abstract": "This paper presents a novel federated reinforcement learning (Fed-RL)\nmethodology to enhance the cyber resiliency of networked microgrids. We\nformulate a resilient reinforcement learning (RL) training setup which (a)\ngenerates episodic trajectories injecting adversarial actions at primary\ncontrol reference signals of the grid forming (GFM) inverters and (b) trains\nthe RL agents (or controllers) to alleviate the impact of the injected\nadversaries. To circumvent data-sharing issues and concerns for proprietary\nprivacy in multi-party-owned networked grids, we bring in the aspects of\nfederated machine learning and propose a novel Fed-RL algorithm to train the RL\nagents. To this end, the conventional horizontal Fed-RL approaches using\ndecoupled independent environments fail to capture the coupled dynamics in a\nnetworked microgrid, which leads us to propose a multi-agent vertically\nfederated variation of actor-critic algorithms, namely federated soft\nactor-critic (FedSAC) algorithm. We created a customized simulation setup\nencapsulating microgrid dynamics in the GridLAB-D/HELICS co-simulation platform\ncompatible with the OpenAI Gym interface for training RL agents. Finally, the\nproposed methodology is validated with numerical examples of modified IEEE\n123-bus benchmark test systems consisting of three coupled microgrids.",
    "descriptor": "\nComments: 13 pages, 5 figures\n",
    "authors": [
      "Sayak Mukherjee",
      "Ramij R. Hossain",
      "Yuan Liu",
      "Wei Du",
      "Veronica Adetola",
      "Sheik M. Mohiuddin",
      "Qiuhua Huang",
      "Tianzhixi Yin",
      "Ankit Singhal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08973"
  },
  {
    "id": "arXiv:2212.08974",
    "title": "3D Point Cloud Pre-training with Knowledge Distillation from 2D Images",
    "abstract": "The recent success of pre-trained 2D vision models is mostly attributable to\nlearning from large-scale datasets. However, compared with 2D image datasets,\nthe current pre-training data of 3D point cloud is limited. To overcome this\nlimitation, we propose a knowledge distillation method for 3D point cloud\npre-trained models to acquire knowledge directly from the 2D representation\nlearning model, particularly the image encoder of CLIP, through concept\nalignment. Specifically, we introduce a cross-attention mechanism to extract\nconcept features from 3D point cloud and compare them with the semantic\ninformation from 2D images. In this scheme, the point cloud pre-trained models\nlearn directly from rich information contained in 2D teacher models. Extensive\nexperiments demonstrate that the proposed knowledge distillation scheme\nachieves higher accuracy than the state-of-the-art 3D pre-training methods for\nsynthetic and real-world datasets on downstream tasks, including object\nclassification, object detection, semantic segmentation, and part segmentation.",
    "descriptor": "",
    "authors": [
      "Yuan Yao",
      "Yuanhan Zhang",
      "Zhenfei Yin",
      "Jiebo Luo",
      "Wanli Ouyang",
      "Xiaoshui Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08974"
  },
  {
    "id": "arXiv:2212.08975",
    "title": "Clinical Deterioration Prediction in Brazilian Hospitals Based on  Artificial Neural Networks and Tree Decision Models",
    "abstract": "Early recognition of clinical deterioration (CD) has vital importance in\npatients' survival from exacerbation or death. Electronic health records (EHRs)\ndata have been widely employed in Early Warning Scores (EWS) to measure CD risk\nin hospitalized patients. Recently, EHRs data have been utilized in Machine\nLearning (ML) models to predict mortality and CD. The ML models have shown\nsuperior performance in CD prediction compared to EWS. Since EHRs data are\nstructured and tabular, conventional ML models are generally applied to them,\nand less effort is put into evaluating the artificial neural network's\nperformance on EHRs data. Thus, in this article, an extremely boosted neural\nnetwork (XBNet) is used to predict CD, and its performance is compared to\neXtreme Gradient Boosting (XGBoost) and random forest (RF) models. For this\npurpose, 103,105 samples from thirteen Brazilian hospitals are used to generate\nthe models. Moreover, the principal component analysis (PCA) is employed to\nverify whether it can improve the adopted models' performance. The performance\nof ML models and Modified Early Warning Score (MEWS), an EWS candidate, are\nevaluated in CD prediction regarding the accuracy, precision, recall, F1-score,\nand geometric mean (G-mean) metrics in a 10-fold cross-validation approach.\nAccording to the experiments, the XGBoost model obtained the best results in\npredicting CD among Brazilian hospitals' data.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Hamed Yazdanpanah",
      "Augusto C. M. Silva",
      "Murilo Guedes",
      "Hugo M. P. Morales",
      "Leandro dos S. Coelho",
      "Fernando G. Moro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08975"
  },
  {
    "id": "arXiv:2212.08978",
    "title": "Magnus Exponential Integrators for Stiff Time-Varying Stochastic Systems",
    "abstract": "We introduce exponential numerical integration methods for stiff stochastic\ndynamical systems of the form $d\\mathbf{z}_t = L(t)\\mathbf{z}_tdt +\n\\mathbf{f}(t)dt + Q(t)d\\mathbf{W}_t$. We consider the setting of time-varying\noperators $L(t), Q(t)$ where they may not commute $L(t_1)L(t_2) \\neq\nL(t_2)L(t_1)$, raising challenges for exponentiation. We develop stochastic\nnumerical integration methods using Mangus expansions for preserving\nstatistical structures and for maintaining fluctuation-dissipation balance for\nphysical systems. For computing the contributions of the fluctuation terms, our\nmethods provide alternative approaches without needing directly to evaluate\nstochastic integrals. We present results for our methods for a class of SDEs\narising in particle simulations and for SPDEs for fluctuations of concentration\nfields in spatially-extended systems. For time-varying stochastic dynamical\nsystems, our introduced discretization approaches provide general exponential\nnumerical integrators for preserving statistical structures while handling\nstiffness.",
    "descriptor": "\nComments: 7 figures\n",
    "authors": [
      "Dev Jasuja",
      "P. J. Atzberger"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2212.08978"
  },
  {
    "id": "arXiv:2212.08979",
    "title": "Language model acceptability judgements are not always robust to context",
    "abstract": "Targeted syntactic evaluations of language models ask whether models show\nstable preferences for syntactically acceptable content over minimal-pair\nunacceptable inputs. Most targeted syntactic evaluation datasets ask models to\nmake these judgements with just a single context-free sentence as input. This\ndoes not match language models' training regime, in which input sentences are\nalways highly contextualized by the surrounding corpus. This mismatch raises an\nimportant question: how robust are models' syntactic judgements in different\ncontexts? In this paper, we investigate the stability of language models'\nperformance on targeted syntactic evaluations as we vary properties of the\ninput context: the length of the context, the types of syntactic phenomena it\ncontains, and whether or not there are violations of grammaticality. We find\nthat model judgements are generally robust when placed in randomly sampled\nlinguistic contexts. However, they are substantially unstable for contexts\ncontaining syntactic structures matching those in the critical test content.\nAmong all tested models (GPT-2 and five variants of OPT), we significantly\nimprove models' judgements by providing contexts with matching syntactic\nstructures, and conversely significantly worsen them using unacceptable\ncontexts with matching but violated syntactic structures. This effect is\namplified by the length of the context, except for unrelated inputs. We show\nthat these changes in model performance are not explainable by simple features\nmatching the context and the test inputs, such as lexical overlap and\ndependency overlap. This sensitivity to highly specific syntactic features of\nthe context can only be explained by the models' implicit in-context learning\nabilities.",
    "descriptor": "",
    "authors": [
      "Koustuv Sinha",
      "Jon Gauthier",
      "Aaron Mueller",
      "Kanishka Misra",
      "Keren Fuentes",
      "Roger Levy",
      "Adina Williams"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08979"
  },
  {
    "id": "arXiv:2212.08981",
    "title": "A Layered Architecture for Universal Causality",
    "abstract": "We propose a layered hierarchical architecture called UCLA (Universal\nCausality Layered Architecture), which combines multiple levels of categorical\nabstraction for causal inference. At the top-most level, causal interventions\nare modeled combinatorially using a simplicial category of ordinal numbers. At\nthe second layer, causal models are defined by a graph-type category. The\nnon-random ``surgical\" operations on causal structures, such as edge deletion,\nare captured using degeneracy and face operators from the simplicial layer\nabove. The third categorical abstraction layer corresponds to the data layer in\ncausal inference. The fourth homotopy layer comprises of additional structure\nimposed on the instance layer above, such as a topological space, which enables\nevaluating causal models on datasets. Functors map between every pair of layers\nin UCLA. Each functor between layers is characterized by a universal arrow,\nwhich defines an isomorphism between every pair of categorical layers. These\nuniversal arrows define universal elements and representations through the\nYoneda Lemma, and in turn lead to a new category of elements based on a\nconstruction introduced by Grothendieck. Causal inference between each pair of\nlayers is defined as a lifting problem, a commutative diagram whose objects are\ncategories, and whose morphisms are functors that are characterized as\ndifferent types of fibrations. We illustrate the UCLA architecture using a\nrange of examples, including integer-valued multisets that represent a\nnon-graphical framework for conditional independence, and causal models based\non graphs and string diagrams using symmetric monoidal categories. We define\ncausal effect in terms of the homotopy colimit of the nerve of the category of\nelements.",
    "descriptor": "\nComments: 33\n",
    "authors": [
      "Sridhar Mahadevan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2212.08981"
  },
  {
    "id": "arXiv:2212.08983",
    "title": "Adaptive Uncertainty Distribution in Deep Learning for Unsupervised  Underwater Image Enhancement",
    "abstract": "One of the main challenges in deep learning-based underwater image\nenhancement is the limited availability of high-quality training data.\nUnderwater images are difficult to capture and are often of poor quality due to\nthe distortion and loss of colour and contrast in water. This makes it\ndifficult to train supervised deep learning models on large and diverse\ndatasets, which can limit the model's performance. In this paper, we explore an\nalternative approach to supervised underwater image enhancement. Specifically,\nwe propose a novel unsupervised underwater image enhancement framework that\nemploys a conditional variational autoencoder (cVAE) to train a deep learning\nmodel with probabilistic adaptive instance normalization (PAdaIN) and\nstatistically guided multi-colour space stretch that produces realistic\nunderwater images. The resulting framework is composed of a U-Net as a feature\nextractor and a PAdaIN to encode the uncertainty, which we call UDnet. To\nimprove the visual quality of the images generated by UDnet, we use a\nstatistically guided multi-colour space stretch module that ensures visual\nconsistency with the input image and provides an alternative to training using\na ground truth image. The proposed model does not need manual human annotation\nand can learn with a limited amount of data and achieves state-of-the-art\nresults on underwater images. We evaluated our proposed framework on eight\npublicly-available datasets. The results show that our proposed framework\nyields competitive performance compared to other state-of-the-art approaches in\nquantitative as well as qualitative metrics. Code available at\nhttps://github.com/alzayats/UDnet .",
    "descriptor": "\nComments: 18 pages, 8 figures, 4 tables\n",
    "authors": [
      "Alzayat Saleh",
      "Marcus Sheaves",
      "Dean Jerry",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08983"
  },
  {
    "id": "arXiv:2212.08984",
    "title": "Decentralized Control of Minimalistic Robotic Swarms For Guaranteed  Target Encapsulation",
    "abstract": "We propose a decentralized control algorithm for a minimalistic robotic swarm\nwith limited capabilities such that the desired global behavior emerges. We\nconsider the problem of searching for and encapsulating various targets present\nin the environment while avoiding collisions with both static and dynamic\nobstacles. The novelty of this work is the guaranteed generation of desired\ncomplex swarm behavior with constrained individual robots which have no memory,\nno localization, and no knowledge of the exact relative locations of their\nneighbors. Moreover, we analyze how the emergent behavior changes with\ndifferent parameters of the task, noise in the sensor reading, and asynchronous\nexecution.",
    "descriptor": "",
    "authors": [
      "Himani Sinhmar",
      "Hadas Kress-Gazit"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.08984"
  },
  {
    "id": "arXiv:2212.08985",
    "title": "Efficient Image Captioning for Edge Devices",
    "abstract": "Recent years have witnessed the rapid progress of image captioning. However,\nthe demands for large memory storage and heavy computational burden prevent\nthese captioning models from being deployed on mobile devices. The main\nobstacles lie in the heavyweight visual feature extractors (i.e., object\ndetectors) and complicated cross-modal fusion networks. To this end, we propose\nLightCap, a lightweight image captioner for resource-limited devices. The core\ndesign is built on the recent CLIP model for efficient image captioning. To be\nspecific, on the one hand, we leverage the CLIP model to extract the compact\ngrid features without relying on the time-consuming object detectors. On the\nother hand, we transfer the image-text retrieval design of CLIP to image\ncaptioning scenarios by devising a novel visual concept extractor and a\ncross-modal modulator. We further optimize the cross-modal fusion model and\nparallel prediction heads via sequential and ensemble distillations. With the\ncarefully designed architecture, our model merely contains 40M parameters,\nsaving the model size by more than 75% and the FLOPs by more than 98% in\ncomparison with the current state-of-the-art methods. In spite of the low\ncapacity, our model still exhibits state-of-the-art performance on prevalent\ndatasets, e.g., 136.6 CIDEr on COCO Karpathy test split. Testing on the\nsmartphone with only a single CPU, the proposed LightCap exhibits a fast\ninference speed of 188ms per image, which is ready for practical applications.",
    "descriptor": "\nComments: To appear in AAAI 2023\n",
    "authors": [
      "Ning Wang",
      "Jiangrong Xie",
      "Hang Luo",
      "Qinglin Cheng",
      "Jihao Wu",
      "Mingbo Jia",
      "Linlin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08985"
  },
  {
    "id": "arXiv:2212.08986",
    "title": "Low-Resource Authorship Style Transfer with In-Context Learning",
    "abstract": "Authorship style transfer involves altering the style of text to match the\nstyle of some target author whilst preserving the semantic meaning of the\noriginal text. Existing approaches to unsupervised authorship style transfer\nlike STRAP have largely focused on style transfer for target authors with many\nexamples of their writing style through books, speeches, or other published\nworks (Krishna et al., 2020). Due to this high-resource training data\nrequirement (often greater than 100,000 words), these approaches are often only\nuseful for style transfer to the style of published authors, politicians, or\nother well-known figures and authorship styles. In this paper, we attempt to\nperform low-resource authorship style transfer, a more challenging class of\nauthorship style transfer where only a limited amount of text in the target\nauthor's style may exist. In our experiments, we specifically choose source and\ntarget authors from Reddit to perform style transfer over their Reddit posts,\nlimiting ourselves to just 16 posts (on average $\\approx$ 500 words) of the\ntarget author's style. We then propose a method for automatic evaluation on the\nlow-resource authorship style transfer task utilizing authorship and style\nrepresentation embeddings (Rivera-Soto et al., 2021; Wegmann et al., 2022). We\nevaluate our style transferred outputs with the proposed automatic evaluation\nmethod and find that our method, STYLL, is able to outperform STRAP and a\ncomprehensive set of baselines.",
    "descriptor": "",
    "authors": [
      "Ajay Patel",
      "Nicholas Andrews",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08986"
  },
  {
    "id": "arXiv:2212.08987",
    "title": "A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter  Dataset",
    "abstract": "Most recent semantic frame parsing systems for spoken language understanding\n(SLU) are designed based on recurrent neural networks. These systems display\ndecent performance on benchmark SLU datasets such as ATIS or SNIPS, which\ncontain short utterances with relatively simple patterns. However, the current\nsemantic frame parsing models lack a mechanism to handle out-of-distribution\n(\\emph{OOD}) patterns and out-of-vocabulary (\\emph{OOV}) tokens. In this paper,\nwe introduce a robust semantic frame parsing pipeline that can handle both\n\\emph{OOD} patterns and \\emph{OOV} tokens in conjunction with a new complex\nTwitter dataset that contains long tweets with more \\emph{OOD} patterns and\n\\emph{OOV} tokens. The new pipeline demonstrates much better results in\ncomparison to state-of-the-art baseline SLU models on both the SNIPS dataset\nand the new Twitter dataset (Our new Twitter dataset can be downloaded from\nhttps://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also\nbuild an E2E application to demo the feasibility of our algorithm and show why\nit is useful in real application.",
    "descriptor": "\nComments: 9 pages, 7 figures\n",
    "authors": [
      "Yu Wang",
      "Hongxia Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08987"
  },
  {
    "id": "arXiv:2212.08989",
    "title": "Deep learning applied to computational mechanics: A comprehensive  review, state of the art, and the classics",
    "abstract": "Three recent breakthroughs due to AI in arts and science serve as motivation:\nAn award winning digital image, protein folding, fast matrix multiplication.\nMany recent developments in artificial neural networks, particularly deep\nlearning (DL), applied and relevant to computational mechanics (solid, fluids,\nfinite-element technology) are reviewed in detail. Both hybrid and pure machine\nlearning (ML) methods are discussed. Hybrid methods combine traditional PDE\ndiscretizations with ML methods either (1) to help model complex nonlinear\nconstitutive relations, (2) to nonlinearly reduce the model order for efficient\nsimulation (turbulence), or (3) to accelerate the simulation by predicting\ncertain components in the traditional integration methods. Here, methods (1)\nand (2) relied on Long-Short-Term Memory (LSTM) architecture, with method (3)\nrelying on convolutional neural networks.. Pure ML methods to solve (nonlinear)\nPDEs are represented by Physics-Informed Neural network (PINN) methods, which\ncould be combined with attention mechanism to address discontinuous solutions.\nBoth LSTM and attention architectures, together with modern and generalized\nclassic optimizers to include stochasticity for DL networks, are extensively\nreviewed. Kernel machines, including Gaussian processes, are provided to\nsufficient depth for more advanced works such as shallow networks with infinite\nwidth. Not only addressing experts, readers are assumed familiar with\ncomputational mechanics, but not with DL, whose concepts and applications are\nbuilt up from the basics, aiming at bringing first-time learners quickly to the\nforefront of research. History and limitations of AI are recounted and\ndiscussed, with particular attention at pointing out misstatements or\nmisconceptions of the classics, even in well-known references. Positioning and\npointing control of a large-deformable beam is given as an example.",
    "descriptor": "\nComments: 274 pages, 151 figures. Submitted to CMES-Computer Modeling in Engineering & Sciences\n",
    "authors": [
      "Loc Vu-Quoc",
      "Alexander Humer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08989"
  },
  {
    "id": "arXiv:2212.08990",
    "title": "Plankton-FL: Exploration of Federated Learning for Privacy-Preserving  Training of Deep Neural Networks for Phytoplankton Classification",
    "abstract": "Creating high-performance generalizable deep neural networks for\nphytoplankton monitoring requires utilizing large-scale data coming from\ndiverse global water sources. A major challenge to training such networks lies\nin data privacy, where data collected at different facilities are often\nrestricted from being transferred to a centralized location. A promising\napproach to overcome this challenge is federated learning, where training is\ndone at site level on local data, and only the model parameters are exchanged\nover the network to generate a global model. In this study, we explore the\nfeasibility of leveraging federated learning for privacy-preserving training of\ndeep neural networks for phytoplankton classification. More specifically, we\nsimulate two different federated learning frameworks, federated learning (FL)\nand mutually exclusive FL (ME-FL), and compare their performance to a\ntraditional centralized learning (CL) framework. Experimental results from this\nstudy demonstrate the feasibility and potential of federated learning for\nphytoplankton monitoring.",
    "descriptor": "",
    "authors": [
      "Daniel Zhang",
      "Vikram Voleti",
      "Alexander Wong",
      "Jason Deglint"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08990"
  },
  {
    "id": "arXiv:2212.08992",
    "title": "PoE: a Panel of Experts for Generalized Automatic Dialogue Assessment",
    "abstract": "Chatbots are expected to be knowledgeable across multiple domains, e.g. for\ndaily chit-chat, exchange of information, and grounding in emotional\nsituations. To effectively measure the quality of such conversational agents, a\nmodel-based automatic dialogue evaluation metric (ADEM) is expected to perform\nwell across multiple domains. Despite significant progress, an ADEM that works\nwell in one domain does not necessarily generalize to another. This calls for a\ndedicated network architecture for domain generalization. To tackle the\nmulti-domain dialogue evaluation task, we propose a Panel of Experts (PoE), a\nmultitask network that consists of a shared transformer encoder and a\ncollection of lightweight adapters. The shared encoder captures the general\nknowledge of dialogues across domains, while each adapter specializes in one\nspecific domain and serves as a domain expert. To validate the idea, we\nconstruct a high-quality multi-domain dialogue dataset leveraging data\naugmentation and pseudo-labeling. The PoE network is comprehensively assessed\non 16 dialogue evaluation datasets spanning a wide range of dialogue domains.\nIt achieves state-of-the-art performance in terms of mean Spearman correlation\nover all the evaluation datasets. It exhibits better zero-shot generalization\nthan existing state-of-the-art ADEMs and the ability to easily adapt to new\ndomains with few-shot transfer learning.",
    "descriptor": "\nComments: Currently under review at TASLP, upload to arxiv for easy cross-reference\n",
    "authors": [
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Qiquan Zhang",
      "Thomas Friedrichs",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08992"
  },
  {
    "id": "arXiv:2212.08993",
    "title": "An Efficient NVM based Architecture for Intermittent Computing under  Energy Constraints",
    "abstract": "Battery-less technology evolved to replace battery technology. Non-volatile\nmemory (NVM) based processors were explored to store the program state during a\npower failure. The energy stored in a capacitor is used for a backup during a\npower failure. Since the size of a capacitor is fixed and limited, the\navailable energy in a capacitor is also limited and fixed. Thus, the capacitor\nenergy is insufficient to store the entire program state during frequent power\nfailures. This paper proposes an architecture that assures safe backup of\nvolatile contents during a power failure under energy constraints. Using a\nproposed dirty block table (DBT) and writeback queue (WBQ), this work limits\nthe number of dirty blocks in the L1 cache at any given time. We further\nconducted a set of experiments by varying the parameter sizes to help the user\nmake appropriate design decisions concerning their energy requirements. The\nproposed architecture decreases energy consumption by 17.56%, the number of\nwrites to NVM by 18.97% at LLC, and 10.66% at a main-memory level compared to\nbaseline architecture.",
    "descriptor": "",
    "authors": [
      "SatyaJaswanth Badri",
      "Mukesh Saini",
      "Neeraj Goel"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2212.08993"
  },
  {
    "id": "arXiv:2212.08995",
    "title": "Impact of Sentiment Analysis in Fake Review Detection",
    "abstract": "Fake review identification is an important topic and has gained the interest\nof experts all around the world. Identifying fake reviews is challenging for\nresearchers, and there are several primary challenges to fake review detection.\nWe propose developing an initial research paper for investigating fake reviews\nby using sentiment analysis. Ten research papers are identified that show fake\nreviews, and they discuss currently available solutions for predicting or\ndetecting fake reviews. They also show the distribution of fake and truthful\nreviews through the analysis of sentiment. We summarize and compare previous\nstudies related to fake reviews. We highlight the most significant challenges\nin the sentiment evaluation process and demonstrate that there is a significant\nimpact on sentiment scores used to identify fake feedback.",
    "descriptor": "",
    "authors": [
      "Amira Yousif",
      "James Buckley"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08995"
  },
  {
    "id": "arXiv:2212.08996",
    "title": "Smart Face Shield: A Sensor-Based Wearable Face Shield Utilizing  Computer Vision Algorithms",
    "abstract": "The study aims the development of a wearable device to combat the onslaught\nof covid-19. Likewise, to enhance the regular face shield available in the\nmarket. Furthermore, to raise awareness of the health and safety protocols\ninitiated by the government and its affiliates in the enforcement of social\ndistancing with the integration of computer vision algorithms. The wearable\ndevice was composed of various hardware and software components such as a\ntransparent polycarbonate face shield, microprocessor, sensors, camera,\nthin-film transistor on-screen display, jumper wires, power bank, and python\nprogramming language. The algorithm incorporated in the study was object\ndetection under computer vision machine learning. The front camera with OpenCV\ntechnology determines the distance of a person in front of the user. Utilizing\nTensorFlow, the target object identifies and detects the image or live feed to\nget its bounding boxes. The focal length lens requires the determination of the\ndistance from the camera to the target object. To get the focal length,\nmultiply the pixel width by the known distance and divide it by the known width\n(Rosebrock, 2020). The deployment of unit testing ensures that the parameters\nare valid in terms of design and specifications.",
    "descriptor": "",
    "authors": [
      "Manuel Luis C. Delos Santos",
      "Ronaldo S. Tinio",
      "Darwin B. Diaz",
      "Karlene Emily I. Tolosa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08996"
  },
  {
    "id": "arXiv:2212.08997",
    "title": "Multi-Instance Partial-Label Learning: Towards Exploiting Dual Inexact  Supervision",
    "abstract": "Weakly supervised machine learning algorithms are able to learn from\nambiguous samples or labels, e.g., multi-instance learning or partial-label\nlearning. However, in some real-world tasks, each training sample is associated\nwith not only multiple instances but also a candidate label set that contains\none ground-truth label and some false positive labels. Specifically, at least\none instance pertains to the ground-truth label while no instance belongs to\nthe false positive labels. In this paper, we formalize such problems as\nmulti-instance partial-label learning (MIPL). Existing multi-instance learning\nalgorithms and partial-label learning algorithms are suboptimal for solving\nMIPL problems since the former fail to disambiguate a candidate label set, and\nthe latter cannot handle a multi-instance bag. To address these issues, a\ntailored algorithm named MIPLGP, i.e., Multi-Instance Partial-Label learning\nwith Gaussian Processes, is proposed. MIPLGP first assigns each instance with a\ncandidate label set in an augmented label space, then transforms the candidate\nlabel set into a logarithmic space to yield the disambiguated and continuous\nlabels via an exclusive disambiguation strategy, and last induces a model based\non the Gaussian processes. Experimental results on various datasets validate\nthat MIPLGP is superior to well-established multi-instance learning and\npartial-label learning algorithms for solving MIPL problems. Our code and\ndatasets will be made publicly available.",
    "descriptor": "",
    "authors": [
      "Wei Tang",
      "Weijia Zhang",
      "Min-Ling Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08997"
  },
  {
    "id": "arXiv:2212.08999",
    "title": "Sentence-level Feedback Generation for English Language Learners: Does  Data Augmentation Help?",
    "abstract": "In this paper, we present strong baselines for the task of Feedback Comment\nGeneration for Writing Learning. Given a sentence and an error span, the task\nis to generate a feedback comment explaining the error. Sentences and feedback\ncomments are both in English. We experiment with LLMs and also create multiple\npseudo datasets for the task, investigating how it affects the performance of\nour system. We present our results for the task along with extensive analysis\nof the generated comments with the aim of aiding future studies in feedback\ncomment generation for English language learners.",
    "descriptor": "\nComments: GenChal 2022: FCG, INLG 2023\n",
    "authors": [
      "Shabnam Behzad",
      "Amir Zeldes",
      "Nathan Schneider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.08999"
  },
  {
    "id": "arXiv:2212.09000",
    "title": "Confidence-aware Training of Smoothed Classifiers for Certified  Robustness",
    "abstract": "Any classifier can be \"smoothed out\" under Gaussian noise to build a new\nclassifier that is provably robust to $\\ell_2$-adversarial perturbations, viz.,\nby averaging its predictions over the noise via randomized smoothing. Under the\nsmoothed classifiers, the fundamental trade-off between accuracy and\n(adversarial) robustness has been well evidenced in the literature: i.e.,\nincreasing the robustness of a classifier for an input can be at the expense of\ndecreased accuracy for some other inputs. In this paper, we propose a simple\ntraining method leveraging this trade-off to obtain robust smoothed\nclassifiers, in particular, through a sample-wise control of robustness over\nthe training samples. We make this control feasible by using \"accuracy under\nGaussian noise\" as an easy-to-compute proxy of adversarial robustness for an\ninput. Specifically, we differentiate the training objective depending on this\nproxy to filter out samples that are unlikely to benefit from the worst-case\n(adversarial) objective. Our experiments show that the proposed method, despite\nits simplicity, consistently exhibits improved certified robustness upon\nstate-of-the-art training methods. Somewhat surprisingly, we find these\nimprovements persist even for other notions of robustness, e.g., to various\ntypes of common corruptions.",
    "descriptor": "\nComments: 21 pages; AAAI 2023; Code is available at this https URL\n",
    "authors": [
      "Jongheon Jeong",
      "Seojin Kim",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09000"
  },
  {
    "id": "arXiv:2212.09004",
    "title": "Rare-Seed Generation for Fuzzing",
    "abstract": "Starting with a random initial seed, fuzzers search for inputs that trigger\nbugs or vulnerabilities. However, fuzzers often fail to generate inputs for\nprogram paths guarded by restrictive branch conditions. In this paper, we show\nthat by first identifying rare-paths in programs (i.e., program paths with path\nconstraints that are unlikely to be satisfied by random input generation), and\nthen, generating inputs/seeds that trigger rare-paths, one can improve the\ncoverage of fuzzing tools. In particular, we present techniques 1) that\nidentify rare paths using quantitative symbolic analysis, and 2) generate\ninputs that can explore these rare paths using path-guided concolic execution.\nWe provide these inputs as initial seed sets to three state of the art fuzzers.\nOur experimental evaluation on a set of programs (that contain a lot of\nrestrictive branch conditions) shows that the fuzzers achieve better coverage\nwith the rare-path based seed set compared to a random initial seed.",
    "descriptor": "",
    "authors": [
      "Seemanta Saha",
      "Laboni Sarker",
      "Md Shafiuzzaman",
      "Chaofan Shou",
      "Albert Li",
      "Ganesh Sankaran",
      "Tevfik Bultan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.09004"
  },
  {
    "id": "arXiv:2212.09005",
    "title": "High-Performance Filters For GPUs",
    "abstract": "Filters approximately store a set of items while trading off accuracy for\nspace-efficiency and can address the limited memory on accelerators, such as\nGPUs. However, there is a lack of high-performance and feature-rich GPU filters\nas most advancements in filter research has focused on CPUs.\nIn this paper, we explore the design space of filters with a goal to develop\nmassively parallel, high performance, and feature rich filters for GPUs. We\nevaluate various filter designs in terms of performance, usability, and\nsupported features and identify two filter designs that offer the right trade\noff in terms of performance, features, and usability.\nWe present two new GPU-based filters, the TCF and GQF, that can be employed\nin various high performance data analytics applications. The TCF is a set\nmembership filter and supports faster inserts and queries, whereas the GQF\nsupports counting which comes at an additional performance cost. Both the GQF\nand TCF provide point and bulk insertion API and are designed to exploit the\nmassive parallelism in the GPU without sacrificing usability and necessary\nfeatures. The TCF and GQF are up to $4.4\\times$ and $1.4\\times$ faster than the\nprevious GPU filters in our benchmarks and at the same time overcome the\nfundamental constraints in performance and usability in current GPU filters.",
    "descriptor": "\nComments: Published at PPOPP 2023\n",
    "authors": [
      "Hunter McCoy",
      "Steven Hofmeyr",
      "Katherine Yelick",
      "Prashant Pandey"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2212.09005"
  },
  {
    "id": "arXiv:2212.09006",
    "title": "A Review of Speech-centric Trustworthy Machine Learning: Privacy,  Safety, and Fairness",
    "abstract": "Speech-centric machine learning systems have revolutionized many leading\ndomains ranging from transportation and healthcare to education and defense,\nprofoundly changing how people live, work, and interact with each other.\nHowever, recent studies have demonstrated that many speech-centric ML systems\nmay need to be considered more trustworthy for broader deployment.\nSpecifically, concerns over privacy breaches, discriminating performance, and\nvulnerability to adversarial attacks have all been discovered in ML research\nfields. In order to address the above challenges and risks, a significant\nnumber of efforts have been made to ensure these ML systems are trustworthy,\nespecially private, safe, and fair. In this paper, we conduct the first\ncomprehensive survey on speech-centric trustworthy ML topics related to\nprivacy, safety, and fairness. In addition to serving as a summary report for\nthe research community, we point out several promising future research\ndirections to inspire the researchers who wish to explore further in this area.",
    "descriptor": "",
    "authors": [
      "Tiantian Feng",
      "Rajat Hebbar",
      "Nicholas Mehlman",
      "Xuan Shi",
      "Aditya Kommineni",
      "and Shrikanth Narayanan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.09006"
  },
  {
    "id": "arXiv:2212.09008",
    "title": "Hidden State Approximation in Recurrent Neural Networks Using Continuous  Particle Filtering",
    "abstract": "Using historical data to predict future events has many applications in the\nreal world, such as stock price prediction; the robot localization. In the past\ndecades, the Convolutional long short-term memory (LSTM) networks have achieved\nextraordinary success with sequential data in the related field. However,\ntraditional recurrent neural networks (RNNs) keep the hidden states in a\ndeterministic way. In this paper, we use the particles to approximate the\ndistribution of the latent state and show how it can extend into a more complex\nform, i.e., the Encoder-Decoder mechanism. With the proposed continuous\ndifferentiable scheme, our model is capable of adaptively extracting valuable\ninformation and updating the latent state according to the Bayes rule. Our\nempirical studies demonstrate the effectiveness of our method in the prediction\ntasks.",
    "descriptor": "",
    "authors": [
      "Dexun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09008"
  },
  {
    "id": "arXiv:2212.09010",
    "title": "Risk-Sensitive Reinforcement Learning with Exponential Criteria",
    "abstract": "While risk-neutral reinforcement learning has shown experimental success in a\nnumber of applications, it is well-known to be non-robust with respect to noise\nand perturbations in the parameters of the system. For this reason,\nrisk-sensitive reinforcement learning algorithms have been studied to introduce\nrobustness and sample efficiency, and lead to better real-life performance. In\nthis work, we introduce new model-free risk-sensitive reinforcement learning\nalgorithms as variations of widely-used Policy Gradient algorithms with similar\nimplementation properties. In particular, we study the effect of exponential\ncriteria on the risk-sensitivity of the policy of a reinforcement learning\nagent, and develop variants of the Monte Carlo Policy Gradient algorithm and\nthe online (temporal-difference) Actor-Critic algorithm. Analytical results\nshowcase that the use of exponential criteria generalize commonly used ad-hoc\nregularization approaches. The implementation, performance, and robustness\nproperties of the proposed methods are evaluated in simulated experiments.",
    "descriptor": "",
    "authors": [
      "Erfaun Noorani",
      "Christos Mavridis",
      "John Baras"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09010"
  },
  {
    "id": "arXiv:2212.09013",
    "title": "Graph Neural Network based Child Activity Recognition",
    "abstract": "This paper presents an implementation on child activity recognition (CAR)\nwith a graph convolution network (GCN) based deep learning model since prior\nimplementations in this domain have been dominated by CNN, LSTM and other\nmethods despite the superior performance of GCN. To the best of our knowledge,\nwe are the first to use a GCN model in child activity recognition domain. In\novercoming the challenges of having small size publicly available child action\ndatasets, several learning methods such as feature extraction, fine-tuning and\ncurriculum learning were implemented to improve the model performance. Inspired\nby the contradicting claims made on the use of transfer learning in CAR, we\nconducted a detailed implementation and analysis on transfer learning together\nwith a study on negative transfer learning effect on CAR as it hasn't been\naddressed previously. As the principal contribution, we were able to develop a\nST-GCN based CAR model which, despite the small size of the dataset, obtained\naround 50% accuracy on vanilla implementations. With feature extraction and\nfine-tuning methods, accuracy was improved by 20%-30% with the highest accuracy\nbeing 82.24%. Furthermore, the results provided on activity datasets\nempirically demonstrate that with careful selection of pre-train model datasets\nthrough methods such as curriculum learning could enhance the accuracy levels.\nFinally, we provide preliminary evidence on possible frame rate effect on the\naccuracy of CAR models, a direction future research can explore.",
    "descriptor": "\nComments: Accepted to 23rd IEEE ICIT Conference (2022), 8 pages, 4 figures\n",
    "authors": [
      "Sanka Mohottala",
      "Pradeepa Samarasinghe",
      "Dharshana Kasthurirathna",
      "Charith Abhayaratne"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09013"
  },
  {
    "id": "arXiv:2212.09015",
    "title": "GAN-based Tabular Data Generator for Constructing Synopsis in  Approximate Query Processing: Challenges and Solutions",
    "abstract": "In data-driven systems, data exploration is imperative for making real-time\ndecisions. However, big data is stored in massive databases that are difficult\nto retrieve. Approximate Query Processing (AQP) is a technique for providing\napproximate answers to aggregate queries based on a summary of the data\n(synopsis) that closely replicates the behavior of the actual data, which can\nbe useful where an approximate answer to the queries would be acceptable in a\nfraction of the real execution time. In this paper, we discuss the use of\nGenerative Adversarial Networks (GANs) for generating tabular data that can be\nemployed in AQP for synopsis construction. We first discuss the challenges\nassociated with constructing synopses in relational databases and then\nintroduce solutions to those challenges. Following that, we organized\nstatistical metrics to evaluate the quality of the generated synopses. We\nconclude that tabular data complexity makes it difficult for algorithms to\nunderstand relational database semantics during training, and improved versions\nof tabular GANs are capable of constructing synopses to revolutionize\ndata-driven decision-making systems.",
    "descriptor": "",
    "authors": [
      "Mohammadali Fallahian",
      "Mohsen Dorodchi",
      "Kyle Kreth"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09015"
  },
  {
    "id": "arXiv:2212.09017",
    "title": "Neural Rankers for Effective Screening Prioritisation in Medical  Systematic Review Literature Search",
    "abstract": "Medical systematic reviews typically require assessing all the documents\nretrieved by a search. The reason is two-fold: the task aims for ``total\nrecall''; and documents retrieved using Boolean search are an unordered set,\nand thus it is unclear how an assessor could examine only a subset. Screening\nprioritisation is the process of ranking the (unordered) set of retrieved\ndocuments, allowing assessors to begin the downstream processes of the\nsystematic review creation earlier, leading to earlier completion of the\nreview, or even avoiding screening documents ranked least relevant.\nScreening prioritisation requires highly effective ranking methods.\nPre-trained language models are state-of-the-art on many IR tasks but have yet\nto be applied to systematic review screening prioritisation. In this paper, we\napply several pre-trained language models to the systematic review document\nranking task, both directly and fine-tuned. An empirical analysis compares how\neffective neural methods compare to traditional methods for this task. We also\ninvestigate different types of document representations for neural methods and\ntheir impact on ranking performance.\nOur results show that BERT-based rankers outperform the current\nstate-of-the-art screening prioritisation methods. However, BERT rankers and\nexisting methods can actually be complementary, and thus, further improvements\nmay be achieved if used in conjunction.",
    "descriptor": "",
    "authors": [
      "Shuai Wang",
      "Harrisen Scells",
      "Bevan Koopman",
      "Guido Zuccon"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09017"
  },
  {
    "id": "arXiv:2212.09018",
    "title": "MeSH Suggester: A Library and System for MeSH Term Suggestion for  Systematic Review Boolean Query Construction",
    "abstract": "Boolean query construction is often critical for medical systematic review\nliterature search. To create an effective Boolean query, systematic review\nresearchers typically spend weeks coming up with effective query terms and\ncombinations. One challenge to creating an effective systematic review Boolean\nquery is the selection of effective MeSH Terms to include in the query. In our\nprevious work, we created neural MeSH term suggestion methods and compared them\nto state-of-the-art MeSH term suggestion methods. We found neural MeSH term\nsuggestion methods to be highly effective.\nIn this demonstration, we build upon our previous work by creating (1) a\nWeb-based MeSH term suggestion prototype system that allows users to obtain\nsuggestions from a number of underlying methods and (2) a Python library that\nimplements ours and others' MeSH term suggestion methods and that is aimed at\nresearchers who want to further investigate, create or deploy such type of\nmethods. We describe the architecture of the web-based system and how to use it\nfor the MeSH term suggestion task. For the Python library, we describe how the\nlibrary can be used for advancing further research and experimentation, and we\nvalidate the results of the methods contained in the library on standard\ndatasets. Our web-based prototype system is available at\nthis http URL, while our Python library is at\nhttps://github.com/ielab/meshsuggestlib.",
    "descriptor": "",
    "authors": [
      "Shuai Wang",
      "Hang Li",
      "Guido Zuccon"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09018"
  },
  {
    "id": "arXiv:2212.09027",
    "title": "2D Pose Estimation based Child Action Recognition",
    "abstract": "We present a graph convolutional network with 2D pose estimation for the\nfirst time on child action recognition task achieving on par results with an\nRGB modality based model on a novel benchmark dataset containing unconstrained\nenvironment based videos.",
    "descriptor": "\nComments: Paper Accepted for the IEEE TENCON Conference (2022). 7 pages, 5 figures\n",
    "authors": [
      "Sanka Mohottala",
      "Sandun Abeygunawardana",
      "Pradeepa Samarasinghe",
      "Dharshana Kasthurirathna",
      "Charith Abhayaratne"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09027"
  },
  {
    "id": "arXiv:2212.09028",
    "title": "Neural Coreference Resolution based on Reinforcement Learning",
    "abstract": "The target of a coreference resolution system is to cluster all mentions that\nrefer to the same entity in a given context. All coreference resolution systems\nneed to solve two subtasks; one task is to detect all of the potential\nmentions, and the other is to learn the linking of an antecedent for each\npossible mention. In this paper, we propose a reinforcement learning\nactor-critic-based neural coreference resolution system, which can achieve both\nmention detection and mention clustering by leveraging an actor-critic deep\nreinforcement learning technique and a joint training algorithm. We experiment\non the BERT model to generate different input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Yu Wang",
      "Hongxia Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09028"
  },
  {
    "id": "arXiv:2212.09029",
    "title": "SurfaceVoronoi: Efficiently Computing Voronoi Diagrams over Mesh  Surfaces with Arbitrary Distance Solvers",
    "abstract": "In this paper, we propose to compute Voronoi diagrams over mesh surfaces\ndriven by an arbitrary geodesic distance solver, assuming that the input is a\ntriangle mesh as well as a collection of sites $P=\\{p_i\\}_{i=1}^m$ on the\nsurface. We propose two key techniques to solve this problem. First, as the\npartition is determined by minimizing the $m$ distance fields, each of which\nrooted at a source site, we suggest keeping one or more distance triples, for\neach triangle, that may help determine the Voronoi bisectors when one uses a\nmark-and-sweep geodesic algorithm to predict the multi-source distance field.\nSecond, rather than keep the distance itself at a mesh vertex, we use the\nsquared distance to characterize the linear change of distance field restricted\nin a triangle, which is proved to induce an exact VD when the base surface\nreduces to a planar triangle mesh. Specially, our algorithm also supports the\nEuclidean distance, which can handle thin-sheet models (e.g. leaf) and runs\nfaster than the traditional restricted Voronoi diagram~(RVD) algorithm. It is\nvery extensible to deal with various variants of surface-based Voronoi diagrams\nincluding (1)surface-based power diagram, (2)constrained Voronoi diagram with\ncurve-type breaklines, and (3)curve-type generators. We conduct extensive\nexperimental results to validate the ability to approximate the exact VD in\ndifferent distance-driven scenarios.",
    "descriptor": "",
    "authors": [
      "Shiqing Xin",
      "Pengfei Wang",
      "Rui Xu",
      "Dongming Yan",
      "Shuangmin Chen",
      "Wenping Wang",
      "Caiming Zhang",
      "Changhe Tu"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2212.09029"
  },
  {
    "id": "arXiv:2212.09030",
    "title": "Contextually Enhanced ES-dRNN with Dynamic Attention for Short-Term Load  Forecasting",
    "abstract": "In this paper, we propose a new short-term load forecasting (STLF) model\nbased on contextually enhanced hybrid and hierarchical architecture combining\nexponential smoothing (ES) and a recurrent neural network (RNN). The model is\ncomposed of two simultaneously trained tracks: the context track and the main\ntrack. The context track introduces additional information to the main track.\nIt is extracted from representative series and dynamically modulated to adjust\nto the individual series forecasted by the main track. The RNN architecture\nconsists of multiple recurrent layers stacked with hierarchical dilations and\nequipped with recently proposed attentive dilated recurrent cells. These cells\nenable the model to capture short-term, long-term and seasonal dependencies\nacross time series as well as to weight dynamically the input information. The\nmodel produces both point forecasts and predictive intervals. The experimental\npart of the work performed on 35 forecasting problems shows that the proposed\nmodel outperforms in terms of accuracy its predecessor as well as standard\nstatistical models and state-of-the-art machine learning models.",
    "descriptor": "",
    "authors": [
      "Slawek Smyl",
      "Grzegorz Dudek",
      "Pawe\u0142 Pe\u0142ka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.09030"
  },
  {
    "id": "arXiv:2212.09031",
    "title": "Marginal-Certainty-aware Fair Ranking Algorithm",
    "abstract": "Ranking systems are ubiquitous in modern Internet services, including online\nmarketplaces, social media, and search engines. Traditionally, ranking systems\nonly focus on how to get better relevance estimation. When relevance estimation\nis available, they usually adopt a user-centric optimization strategy where\nranked lists are generated by sorting items according to their estimated\nrelevance. However, such user-centric optimization ignores the fact that item\nproviders also draw utility from ranking systems. It has been shown in existing\nresearch that such user-centric optimization will cause much unfairness to item\nproviders, followed by unfair opportunities and unfair economic gains for item\nproviders.\nTo address ranking fairness, many fair ranking methods have been proposed.\nHowever, as we show in this paper, these methods could be suboptimal as they\ndirectly rely on the relevance estimation without being aware of the\nuncertainty (i.e., the variance of the estimated relevance). To address this\nuncertainty, we propose a novel Marginal-Certainty-aware Fair algorithm named\nMCFair. MCFair jointly optimizes fairness and user utility, while relevance\nestimation is constantly updated in an online manner.\nIn MCFair, we first develop a ranking objective that includes uncertainty,\nfairness, and user utility. Then we directly use the gradient of the ranking\nobjective as the ranking score. We theoretically prove that MCFair based on\ngradients is optimal for the aforementioned ranking objective. Empirically, we\nfind that on semi-synthesized datasets, MCFair is effective and practical and\ncan deliver superior performance compared to state-of-the-art fair ranking\nmethods. To facilitate reproducibility, we release our code\nhttps://github.com/Taosheng-ty/WSDM22-MCFair.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Tao Yang",
      "Zhichao Xu",
      "Zhenduo Wang",
      "Anh Tran",
      "Qingyao Ai"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.09031"
  },
  {
    "id": "arXiv:2212.09032",
    "title": "AutoSlicer: Scalable Automated Data Slicing for ML Model Analysis",
    "abstract": "Automated slicing aims to identify subsets of evaluation data where a trained\nmodel performs anomalously. This is an important problem for machine learning\npipelines in production since it plays a key role in model debugging and\ncomparison, as well as the diagnosis of fairness issues. Scalability has become\na critical requirement for any automated slicing system due to the large search\nspace of possible slices and the growing scale of data. We present Autoslicer,\na scalable system that searches for problematic slices through distributed\nmetric computation and hypothesis testing. We develop an efficient strategy\nthat reduces the search space through pruning and prioritization. In the\nexperiments, we show that our search strategy finds most of the anomalous\nslices by inspecting a small portion of the search space.",
    "descriptor": "\nComments: 11 pages, 5 figures, NeurIPS 2022 Workshop on Challenges in Deploying and Monitoring Machine Learning Systems\n",
    "authors": [
      "Zifan Liu",
      "Evan Rosen",
      "Paul Suganthan G. C"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2212.09032"
  },
  {
    "id": "arXiv:2212.09033",
    "title": "Planning Immediate Landmarks of Targets for Model-Free Skill Transfer  across Agents",
    "abstract": "In reinforcement learning applications like robotics, agents usually need to\ndeal with various input/output features when specified with different\nstate/action spaces by their developers or physical restrictions. This\nindicates unnecessary re-training from scratch and considerable sample\ninefficiency, especially when agents follow similar solution steps to achieve\ntasks. In this paper, we aim to transfer similar high-level goal-transition\nknowledge to alleviate the challenge. Specifically, we propose PILoT, i.e.,\nPlanning Immediate Landmarks of Targets. PILoT utilizes the universal decoupled\npolicy optimization to learn a goal-conditioned state planner; then, distills a\ngoal-planner to plan immediate landmarks in a model-free style that can be\nshared among different agents. In our experiments, we show the power of PILoT\non various transferring challenges, including few-shot transferring across\naction spaces and dynamics, from low-dimensional vector states to image inputs,\nfrom simple robot to complicated morphology; and we also illustrate a zero-shot\ntransfer solution from a simple 2D navigation task to the harder Ant-Maze task.",
    "descriptor": "",
    "authors": [
      "Minghuan Liu",
      "Zhengbang Zhu",
      "Menghui Zhu",
      "Yuzheng Zhuang",
      "Weinan Zhang",
      "Jianye Hao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09033"
  },
  {
    "id": "arXiv:2212.09034",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by  Bridging GNNs and MLPs",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation\nlearning on graphs, are built upon the multi-layer perceptrons (MLP)\narchitecture with additional message passing layers to allow features to flow\nacross nodes. While conventional wisdom largely attributes the success of GNNs\nto their advanced expressivity for learning desired functions on nodes'\nego-graphs, we conjecture that this is \\emph{not} the main cause of GNNs'\nsuperiority in node prediction tasks. This paper pinpoints the major source of\nGNNs' performance gain to their intrinsic generalization capabilities, by\nintroducing an intermediate model class dubbed as P(ropagational)MLP, which is\nidentical to standard MLP in training, and then adopt GNN's architecture in\ntesting. Intriguingly, we observe that PMLPs consistently perform on par with\n(or even exceed) their GNN counterparts across ten benchmarks and different\nexperimental settings, despite the fact that PMLPs share the same (trained)\nweights with poorly-performed MLP. This critical finding opens a door to a\nbrand new perspective for understanding the power of GNNs, and allow bridging\nGNNs and MLPs for dissecting their generalization behaviors. As an initial step\nto analyze PMLP, we show its essential difference with MLP at infinite-width\nlimit lies in the NTK feature map in the post-training stage. Moreover, though\nMLP and PMLP cannot extrapolate non-linear functions for extreme OOD data, PMLP\nhas more freedom to generalize near the training support.",
    "descriptor": "",
    "authors": [
      "Chenxiao Yang",
      "Qitian Wu",
      "Jiahua Wang",
      "Junchi Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09034"
  },
  {
    "id": "arXiv:2212.09035",
    "title": "Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted  Attacks",
    "abstract": "In this work, we study the black-box targeted attack problem from the model\ndiscrepancy perspective. On the theoretical side, we present a generalization\nerror bound for black-box targeted attacks, which gives a rigorous theoretical\nanalysis for guaranteeing the success of the attack. We reveal that the attack\nerror on a target model mainly depends on empirical attack error on the\nsubstitute model and the maximum model discrepancy among substitute models. On\nthe algorithmic side, we derive a new algorithm for black-box targeted attacks\nbased on our theoretical analysis, in which we additionally minimize the\nmaximum model discrepancy(M3D) of the substitute models when training the\ngenerator to generate adversarial examples. In this way, our model is capable\nof crafting highly transferable adversarial examples that are robust to the\nmodel variation, thus improving the success rate for attacking the black-box\nmodel. We conduct extensive experiments on the ImageNet dataset with different\nclassification models, and our proposed approach outperforms existing\nstate-of-the-art methods by a significant margin. Our codes will be released.",
    "descriptor": "",
    "authors": [
      "Anqi Zhao",
      "Tong Chu",
      "Yahao Liu",
      "Wen Li",
      "Jingjing Li",
      "Lixin Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09035"
  },
  {
    "id": "arXiv:2212.09039",
    "title": "Automated Optical Inspection of FAST's Reflector Surface using Drones  and Computer Vision",
    "abstract": "The Five-hundred-meter Aperture Spherical radio Telescope (FAST) is the\nworld's largest single-dish radio telescope. Its large reflecting surface\nachieves unprecedented sensitivity but is prone to damage, such as dents and\nholes, caused by naturally-occurring falling objects. Hence, the timely and\naccurate detection of surface defects is crucial for FAST's stable operation.\nConventional manual inspection involves human inspectors climbing up and\nexamining the large surface visually, a time-consuming and potentially\nunreliable process. To accelerate the inspection process and increase its\naccuracy, this work makes the first step towards automating the inspection of\nFAST by integrating deep-learning techniques with drone technology. First, a\ndrone flies over the surface along a predetermined route. Since surface defects\nsignificantly vary in scale and show high inter-class similarity, directly\napplying existing deep detectors to detect defects on the drone imagery is\nhighly prone to missing and misidentifying defects. As a remedy, we introduce\ncross-fusion, a dedicated plug-in operation for deep detectors that enables the\nadaptive fusion of multi-level features in a point-wise selective fashion,\ndepending on local defect patterns. Consequently, strong semantics and\nfine-grained details are dynamically fused at different positions to support\nthe accurate detection of defects of various scales and types. Our AI-powered\ndrone-based automated inspection is time-efficient, reliable, and has good\naccessibility, which guarantees the long-term and stable operation of FAST.",
    "descriptor": "",
    "authors": [
      "Jianan Li",
      "Shenwang Jiang",
      "Liqiang Song",
      "Peiran Peng",
      "Feng Mu",
      "Hui Li",
      "Peng Jiang",
      "Tingfa Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09039"
  },
  {
    "id": "arXiv:2212.09040",
    "title": "The Underlying Correlated Dynamics in Neural Training",
    "abstract": "Training of neural networks is a computationally intensive task. The\nsignificance of understanding and modeling the training dynamics is growing as\nincreasingly larger networks are being trained. We propose in this work a model\nbased on the correlation of the parameters' dynamics, which dramatically\nreduces the dimensionality. We refer to our algorithm as \\emph{correlation mode\ndecomposition} (CMD). It splits the parameter space into groups of parameters\n(modes) which behave in a highly correlated manner through the epochs.\nWe achieve a remarkable dimensionality reduction with this approach, where\nnetworks like ResNet-18, transformers and GANs, containing millions of\nparameters, can be modeled well using just a few modes. We observe each typical\ntime profile of a mode is spread throughout the network in all layers.\nMoreover, our model induces regularization which yields better generalization\ncapacity on the test set. This representation enhances the understanding of the\nunderlying training dynamics and can pave the way for designing better\nacceleration techniques.",
    "descriptor": "",
    "authors": [
      "Rotem Turjeman",
      "Tom Berkov",
      "Ido Cohen",
      "Guy Gilboa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09040"
  },
  {
    "id": "arXiv:2212.09042",
    "title": "Gait Recognition Using 3-D Human Body Shape Inference",
    "abstract": "Gait recognition, which identifies individuals based on their walking\npatterns, is an important biometric technique since it can be observed from a\ndistance and does not require the subject's cooperation. Recognizing a person's\ngait is difficult because of the appearance variants in human silhouette\nsequences produced by varying viewing angles, carrying objects, and clothing.\nRecent research has produced a number of ways for coping with these variants.\nIn this paper, we present the usage of inferring 3-D body shapes distilled from\nlimited images, which are, in principle, invariant to the specified variants.\nInference of 3-D shape is a difficult task, especially when only silhouettes\nare provided in a dataset. We provide a method for learning 3-D body inference\nfrom silhouettes by transferring knowledge from 3-D shape prior from RGB\nphotos. We use our method on multiple existing state-of-the-art gait baselines\nand obtain consistent improvements for gait identification on two public\ndatasets, CASIA-B and OUMVLP, on several variants and settings, including a new\nsetting of novel views not seen during training.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Haidong Zhu",
      "Zhaoheng Zheng",
      "Ram Nevatia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09042"
  },
  {
    "id": "arXiv:2212.09044",
    "title": "Text2Struct: A Machine Learning Pipeline for Mining Structured Data from  Text",
    "abstract": "Many analysis and prediction tasks require the extraction of structured data\nfrom unstructured texts. To solve it, this paper presents an end-to-end machine\nlearning pipeline, Text2Struct, including a text annotation scheme, training\ndata processing, and machine learning implementation. We formulated the mining\nproblems as the extraction of metrics and units associated with numerals in the\ntext. Text2Struct was evaluated on an annotated text dataset collected from\nabstracts of medical publications regarding thrombectomy. In terms of\nprediction performance, a dice coefficient of 0.82 was achieved on the test\ndataset. By random sampling, most predicted relations between numerals and\nentities were well matched to the ground-truth annotations. These results\nshowed that the Text2Struct is viable for the mining of structured data from\ntext without special templates or patterns. It is anticipated to further\nimprove the pipeline by expanding the dataset and investigating other machine\nlearning models. A code demonstration can be found at:\nhttps://github.com/zcc861007/CourseProject",
    "descriptor": "",
    "authors": [
      "Chaochao Zhou",
      "Bo Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09044"
  },
  {
    "id": "arXiv:2212.09045",
    "title": "Task Preferences across Languages on Community Question Answering  Platforms",
    "abstract": "With the steady emergence of community question answering (CQA) platforms\nlike Quora, StackExchange, and WikiHow, users now have an unprecedented access\nto information on various kind of queries and tasks. Moreover, the rapid\nproliferation and localization of these platforms spanning geographic and\nlinguistic boundaries offer a unique opportunity to study the task requirements\nand preferences of users in different socio-linguistic groups. In this study,\nwe implement an entity-embedding model trained on a large longitudinal dataset\nof multi-lingual and task-oriented question-answer pairs to uncover and\nquantify the (i) prevalence and distribution of various online tasks across\nlinguistic communities, and (ii) emerging and receding trends in task\npopularity over time in these communities. Our results show that there exists\nsubstantial variance in task preference as well as popularity trends across\nlinguistic communities on the platform. Findings from this study will help Q&A\nplatforms better curate and personalize content for non-English users, while\nalso offering valuable insights to businesses looking to target non-English\nspeaking communities online.",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Sebastin Santy",
      "Prasanta Bhattacharya",
      "Rishabh Mehrotra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2212.09045"
  },
  {
    "id": "arXiv:2212.09049",
    "title": "Perfectly Covert Communication with a Reflective Panel",
    "abstract": "Covert communication, a sub-field of information security, is focused on\nhiding the mere existence of communication from unwanted listeners via the\nphysical layer, i.e., via signal and noise characteristics, rather than\nassuming coding or secure protocols at the higher layers.\nIn this work, we consider the problem of perfect covert communication in\nwireless networks. Specifically, harnessing an Intelligent Reflecting Surface\n(IRS), we turn our attention to schemes which allow the transmitter to\ncompletely hide the communication, with zero energy at the unwanted listener\n(Willie) and hence zero probability of detection. Applications of such schemes\ngo beyond simple covertness, as we prevent detectability or decoding even when\nthe codebook, timings and channel characteristics are known to Willie. That is,\nperfect covertness also ensures Willie is unable to decode, even assuming\ncommunication took place and knowing the codebook.\nWe define perfect covertness, give a necessary and sufficient condition for\nit in IRS-assisted communication and define the optimization problem. For $N=2$\nIRS elements we compute the probability of finding a solution and give the\nsolution analytically. For $N>2$, we also analytically compute the probability\nof such a zero-detection solution, and show that it tends to $1$ as the number\nof IRS elements increases. We provide a perfectly covert scheme achieving it\nand prove its convergence. The results are also supported by simulation\nresults, showing that a small amount of IRS elements allows for a positive rate\nat the legitimate user yet with zero probability of detection at an unwanted\nlistener.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Or Elimelech",
      "Itai Bitton",
      "Eran Nehushtan",
      "Asaf Cohen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.09049"
  },
  {
    "id": "arXiv:2212.09050",
    "title": "Determining Distributions of Security Means for Wireless Sensor Networks  based on the Model of a Neighbourhood Watch",
    "abstract": "Neighbourhood watch is a concept that allows a community to distribute a\ncomplex security task in between all members. Members of the community carry\nout individual security tasks to contribute to the overall security of it. It\nreduces the workload of a particular individual while securing all members and\nallowing them to carry out a multitude of security tasks. Wireless sensor\nnetworks (WSNs) are composed of resource-constraint independent battery driven\ncomputers as nodes communicating wirelessly. Security in WSNs is essential.\nWithout sufficient security, an attacker is able to eavesdrop the\ncommunication, tamper monitoring results or deny critical nodes providing their\nservice in a way to cut off larger network parts. The resource-constraint\nnature of sensor nodes prevents them from running full-fledged security\nprotocols. Instead, it is necessary to assess the most significant security\nthreats and implement specialised protocols. A neighbourhood-watch inspired\ndistributed security scheme for WSNs has been introduced by Langend\\\"orfer. Its\ngoal is to increase the variety of attacks a WSN can fend off. A framework of\nsuch complexity has to be designed in multiple steps. Here, we introduce an\napproach to determine distributions of security means on large-scale static\nhomogeneous WSNs. Therefore, we model WSNs as undirected graphs in which two\nnodes connected iff they are in transmission range. The framework aims to\npartition the graph into $n$ distinct security means resulting in the targeted\ndistribution. The underlying problems turn out to be NP hard and we attempt to\nsolve them using linear programs (LPs). To evaluate the computability of the\nLPs, we generate large numbers of random {\\lambda}-precision unit disk graphs\n(UDGs) as representation of WSNs. For this purpose, we introduce a novel\n{\\lambda}-precision UDG generator to model WSNs with a minimal distance in\nbetween nodes.",
    "descriptor": "\nComments: International Journal of Information Security\n",
    "authors": [
      "Benjamin F\u00f6rster",
      "Peter Langend\u00f6rfer",
      "Thomas Hinze"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.09050"
  },
  {
    "id": "arXiv:2212.09052",
    "title": "A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet  Extraction",
    "abstract": "Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,\nsentiment and opinion term triplets from sentences. Since the initial datasets\nused to evaluate models on ASTE had flaws, several studies later corrected the\ninitial datasets and released new versions of the datasets independently. As a\nresult, different studies select different versions of datasets to evaluate\ntheir methods, which makes ASTE-related works hard to follow. In this paper, we\nanalyze the relation between different versions of datasets and suggest that\nthe entire-space version should be used for ASTE. Besides the sentences\ncontaining triplets and the triplets in the sentences, the entire-space version\nadditionally includes the sentences without triplets and the aspect terms which\ndo not belong to any triplets. Hence, the entire-space version is consistent\nwith real-world scenarios and evaluating models on the entire-space version can\nbetter reflect the models' performance in real-world scenarios. In addition,\nexperimental results show that evaluating models on non-entire-space datasets\ninflates the performance of existing models and models trained on the\nentire-space version can obtain better performance.",
    "descriptor": "",
    "authors": [
      "Yuncong Li",
      "Fang Wang",
      "Sheng-Hua Zhong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09052"
  },
  {
    "id": "arXiv:2212.09056",
    "title": "Beyond Digital \"Echo Chambers\": The Role of Viewpoint Diversity in  Political Discussion",
    "abstract": "Increasingly taking place in online spaces, modern political conversations\nare typically perceived to be unproductively affirming -- siloed in so called\n``echo chambers'' of exclusively like-minded discussants. Yet, to date we lack\nsufficient means to measure viewpoint diversity in conversations. To this end,\nin this paper, we operationalize two viewpoint metrics proposed for recommender\nsystems and adapt them to the context of social media conversations. This is\nthe first study to apply these two metrics (Representation and Fragmentation)\nto real world data and to consider the implications for online conversations\nspecifically. We apply these measures to two topics -- daylight savings time\n(DST), which serves as a control, and the more politically polarized topic of\nimmigration. We find that the diversity scores for both Fragmentation and\nRepresentation are lower for immigration than for DST. Further, we find that\nwhile pro-immigrant views receive consistent pushback on the platform,\nanti-immigrant views largely operate within echo chambers. We observe less\nsevere yet similar patterns for DST. Taken together, Representation and\nFragmentation paint a meaningful and important new picture of viewpoint\ndiversity.",
    "descriptor": "\nComments: Camera-ready version in WSDM 2023\n",
    "authors": [
      "Rishav Hada",
      "Amir Ebrahimi Fard",
      "Sarah Shugars",
      "Federico Bianchi",
      "Patricia Rossini",
      "Dirk Hovy",
      "Rebekah Tromble",
      "Nava Tintarev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09056"
  },
  {
    "id": "arXiv:2212.09062",
    "title": "Bort: Towards Explainable Neural Networks with Bounded Orthogonal  Constraint",
    "abstract": "Deep learning has revolutionized human society, yet the black-box nature of\ndeep neural networks hinders further application to reliability-demanded\nindustries. In the attempt to unpack them, many works observe or impact\ninternal variables to improve the model's comprehensibility and transparency.\nHowever, existing methods rely on intuitive assumptions and lack mathematical\nguarantees. To bridge this gap, we introduce Bort, an optimizer for improving\nmodel explainability with boundedness and orthogonality constraints on model\nparameters, derived from the sufficient conditions of model comprehensibility\nand transparency. We perform reconstruction and backtracking on the model\nrepresentations optimized by Bort and observe an evident improvement in model\nexplainability. Based on Bort, we are able to synthesize explainable\nadversarial samples without additional parameters and training. Surprisingly,\nwe find Bort constantly improves the classification accuracy of various\narchitectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet.",
    "descriptor": "",
    "authors": [
      "Borui Zhang",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09062"
  },
  {
    "id": "arXiv:2212.09064",
    "title": "PlexiChain: A Secure Blockchain-based Flexibility Aggregator Framework",
    "abstract": "Flexible resources in built environments are seen as a low-cost opportunity\nfor delivering grid management services. Consequently, the centralised\naggregator model, where the aggregator is used to bundle demand flexibility\nfrom flexible resources and deliver it to flexibility customers such as\nDistributed/Transmission System Operator (DSO/TSO) in flexibility markets, has\nbeen adopted. However, the aggregator role introduces various security and\ntrust challenges. In this work, we propose a blockchain-based flexibility\ntrading framework dubbed PlexiChain to address the security and trust\nchallenges the aggregator poses in the centralised aggregator model. The\nsecurity evaluations performed using a real-world dataset show that PlexiChain\nis robust against known security attacks, such as MadIoT and False Data\nInjection attacks. Additionally, the performance evaluations show that\nPlexiChain has lower computation and communication costs than other\nblockchain-based applications in resource-constrained environments.",
    "descriptor": "\nComments: 10 pages, 8 figure\n",
    "authors": [
      "Samuel Karumba",
      "Salil S. Kanhere",
      "Raja Jurdak",
      "Subbu Sethuvenkatraman"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.09064"
  },
  {
    "id": "arXiv:2212.09067",
    "title": "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks",
    "abstract": "Backdoor attacks represent one of the major threats to machine learning\nmodels. Various efforts have been made to mitigate backdoors. However, existing\ndefenses have become increasingly complex and often require high computational\nresources or may also jeopardize models' utility. In this work, we show that\nfine-tuning, one of the most common and easy-to-adopt machine learning training\noperations, can effectively remove backdoors from machine learning models while\nmaintaining high model utility. Extensive experiments over three machine\nlearning paradigms show that fine-tuning and our newly proposed\nsuper-fine-tuning achieve strong defense performance. Furthermore, we coin a\nnew term, namely backdoor sequela, to measure the changes in model\nvulnerabilities to other attacks before and after the backdoor has been\nremoved. Empirical evaluation shows that, compared to other defense methods,\nsuper-fine-tuning leaves limited backdoor sequela. We hope our results can help\nmachine learning model owners better protect their models from backdoor\nthreats. Also, it calls for the design of more advanced attacks in order to\ncomprehensively assess machine learning models' backdoor vulnerabilities.",
    "descriptor": "\nComments: 17 pages, 17 figures\n",
    "authors": [
      "Zeyang Sha",
      "Xinlei He",
      "Pascal Berrang",
      "Mathias Humbert",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09067"
  },
  {
    "id": "arXiv:2212.09068",
    "title": "Style-Hallucinated Dual Consistency Learning: A Unified Framework for  Visual Domain Generalization",
    "abstract": "Domain shift widely exists in the visual world, while modern deep neural\nnetworks commonly suffer from severe performance degradation under domain shift\ndue to the poor generalization ability, which limits the real-world\napplications. The domain shift mainly lies in the limited source environmental\nvariations and the large distribution gap between source and unseen target\ndata. To this end, we propose a unified framework, Style-HAllucinated Dual\nconsistEncy learning (SHADE), to handle such domain shift in various visual\ntasks. Specifically, SHADE is constructed based on two consistency constraints,\nStyle Consistency (SC) and Retrospection Consistency (RC). SC enriches the\nsource situations and encourages the model to learn consistent representation\nacross style-diversified samples. RC leverages general visual knowledge to\nprevent the model from overfitting to source data and thus largely keeps the\nrepresentation consistent between the source and general visual models.\nFurthermore, we present a novel style hallucination module (SHM) to generate\nstyle-diversified samples that are essential to consistency learning. SHM\nselects basis styles from the source distribution, enabling the model to\ndynamically generate diverse and realistic samples during training. Extensive\nexperiments demonstrate that our versatile SHADE can significantly enhance the\ngeneralization in various visual recognition tasks, including image\nclassification, semantic segmentation and object detection, with different\nmodels, i.e., ConvNets and Transformer.",
    "descriptor": "\nComments: Journal extension of arXiv:2204.02548. Code is available at this https URL\n",
    "authors": [
      "Yuyang Zhao",
      "Zhun Zhong",
      "Na Zhao",
      "Nicu Sebe",
      "Gim Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09068"
  },
  {
    "id": "arXiv:2212.09069",
    "title": "Masked Wavelet Representation for Compact Neural Radiance Fields",
    "abstract": "Neural radiance fields (NeRF) have demonstrated the potential of\ncoordinate-based neural representation (neural fields or implicit neural\nrepresentation) in neural rendering. However, using a multi-layer perceptron\n(MLP) to represent a 3D scene or object requires enormous computational\nresources and time. There have been recent studies on how to reduce these\ncomputational inefficiencies by using additional data structures, such as grids\nor trees. Despite the promising performance, the explicit data structure\nnecessitates a substantial amount of memory. In this work, we present a method\nto reduce the size without compromising the advantages of having additional\ndata structures. In detail, we propose using the wavelet transform on\ngrid-based neural fields. Grid-based neural fields are for fast convergence,\nand the wavelet transform, whose efficiency has been demonstrated in\nhigh-performance standard codecs, is to improve the parameter efficiency of\ngrids. Furthermore, in order to achieve a higher sparsity of grid coefficients\nwhile maintaining reconstruction quality, we present a novel trainable masking\napproach. Experimental results demonstrate that non-spatial grid coefficients,\nsuch as wavelet coefficients, are capable of attaining a higher level of\nsparsity than spatial grid coefficients, resulting in a more compact\nrepresentation. With our proposed mask and compression pipeline, we achieved\nstate-of-the-art performance within a memory budget of 2 MB. Our code is\navailable at https://github.com/daniel03c1/masked_wavelet_nerf.",
    "descriptor": "",
    "authors": [
      "Daniel Rho",
      "Byeonghyeon Lee",
      "Seungtae Nam",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2212.09069"
  },
  {
    "id": "arXiv:2212.09071",
    "title": "Disentangling Learnable and Memorizable Data via Contrastive Learning  for Semantic Communications",
    "abstract": "Achieving artificially intelligent-native wireless networks is necessary for\nthe operation of future 6G applications such as the metaverse. Nonetheless,\ncurrent communication schemes are, at heart, a mere reconstruction process that\nlacks reasoning. One key solution that enables evolving wireless communication\nto a human-like conversation is semantic communications. In this paper, a novel\nmachine reasoning framework is proposed to pre-process and disentangle source\ndata so as to make it semantic-ready. In particular, a novel contrastive\nlearning framework is proposed, whereby instance and cluster discrimination are\nperformed on the data. These two tasks enable increasing the cohesiveness\nbetween data points mapping to semantically similar content elements and\ndisentangling data points of semantically different content elements.\nSubsequently, the semantic deep clusters formed are ranked according to their\nlevel of confidence. Deep semantic clusters of highest confidence are\nconsidered learnable, semantic-rich data, i.e., data that can be used to build\na language in a semantic communications system. The least confident ones are\nconsidered, random, semantic-poor, and memorizable data that must be\ntransmitted classically. Our simulation results showcase the superiority of our\ncontrastive learning approach in terms of semantic impact and minimalism. In\nfact, the length of the semantic representation achieved is minimized by 57.22%\ncompared to vanilla semantic communication systems, thus achieving minimalist\nsemantic representations.",
    "descriptor": "",
    "authors": [
      "Christina Chaccour",
      "Walid Saad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2212.09071"
  },
  {
    "id": "arXiv:2212.09072",
    "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems",
    "abstract": "Negotiation is one of the crucial abilities in human communication, and there\nhas been a resurgent research interest in negotiation dialogue systems\nrecently, which goal is to empower intelligent agents with such ability that\ncan efficiently help humans resolve conflicts or reach beneficial agreements.\nAlthough there have been many explorations in negotiation dialogue systems, a\nsystematic review of this task has to date remained notably absent. To this\nend, we aim to fill this gap by reviewing contemporary studies in the emerging\nfield of negotiation dialogue systems, covering benchmarks, evaluations, and\nmethodologies. Furthermore, we also discuss potential future directions,\nincluding multi-modal, multi-party, and cross-cultural negotiation scenarios.\nOur goal is to provide the community with a systematic overview of negotiation\ndialogue systems and to inspire future research.",
    "descriptor": "\nComments: An early version, work in progress\n",
    "authors": [
      "Haolan Zhan",
      "Yufei Wang",
      "Tao Feng",
      "Yuncheng Hua",
      "Suraj Sharma",
      "Zhuang Li",
      "Lizhen Qu",
      "Gholamreza Haffari"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09072"
  },
  {
    "id": "arXiv:2212.09077",
    "title": "Answer-Set Programming for Lexicographical Makespan Optimisation in  Parallel Machine Scheduling",
    "abstract": "We deal with a challenging scheduling problem on parallel machines with\nsequence-dependent setup times and release dates from a real-world application\nof semiconductor work-shop production. There, jobs can only be processed by\ndedicated machines, thus few machines can determine the makespan almost\nregardless of how jobs are scheduled on the remaining ones. This causes\nproblems when machines fail and jobs need to be rescheduled. Instead of\noptimising only the makespan, we put the individual machine spans in\nnon-ascending order and lexicographically minimise the resulting tuples. This\nachieves that all machines complete as early as possible and increases the\nrobustness of the schedule. We study the application of Answer-Set Programming\n(ASP) to solve this problem. While ASP eases modelling, the combination of\ntiming constraints and the considered objective function challenges current\nsolving technology. The former issue is addressed by using an extension of ASP\nby difference logic. For the latter, we devise different algorithms that use\nmulti-shot solving. To tackle industrial-sized instances, we study different\napproximations and heuristics. Our experimental results show that ASP is indeed\na promising KRR paradigm for this problem and is competitive with\nstate-of-the-art CP and MIP solvers. Under consideration in Theory and Practice\nof Logic Programming (TPLP).",
    "descriptor": "\nComments: Under consideration in Theory and Practice of Logic Programming (TPLP)\n",
    "authors": [
      "Thomas Eiter",
      "Tobias Geibinger",
      "Nysret Musliu",
      "Johannes Oetsch",
      "Peter Skocovsky",
      "Daria Stepanova"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09077"
  },
  {
    "id": "arXiv:2212.09078",
    "title": "Multi-embodiment Legged Robot Control as a Sequence Modeling Problem",
    "abstract": "Robots are traditionally bounded by a fixed embodiment during their\noperational lifetime, which limits their ability to adapt to their\nsurroundings. Co-optimizing control and morphology of a robot, however, is\noften inefficient due to the complex interplay between the controller and\nmorphology. In this paper, we propose a learning-based control method that can\ninherently take morphology into consideration such that once the control policy\nis trained in the simulator, it can be easily deployed to robots with different\nembodiments in the real world. In particular, we present the Embodiment-aware\nTransformer (EAT), an architecture that casts this control problem as\nconditional sequence modeling. EAT outputs the optimal actions by leveraging a\ncausally masked Transformer. By conditioning an autoregressive model on the\ndesired robot embodiment, past states, and actions, our EAT model can generate\nfuture actions that best fit the current robot embodiment. Experimental results\nshow that EAT can outperform all other alternatives in embodiment-varying\ntasks, and succeed in an example of real-world evolution tasks: stepping down a\nstair through updating the morphology alone. We hope that EAT will inspire a\nnew push toward real-world evolution across many domains, where algorithms like\nEAT can blaze a trail by bridging the field of evolutionary robotics and big\ndata sequence modeling.",
    "descriptor": "",
    "authors": [
      "Chen Yu",
      "Weinan Zhang",
      "Hang Lai",
      "Zheng Tian",
      "Laurent Kneip",
      "Jun Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09078"
  },
  {
    "id": "arXiv:2212.09080",
    "title": "Synthesis and Evaluation of a Domain-specific Large Data Set for  Dungeons & Dragons",
    "abstract": "This paper introduces the Forgotten Realms Wiki (FRW) data set and domain\nspecific natural language generation using FRW along with related analyses.\nForgotten Realms is the de-facto default setting of the popular open ended\ntabletop fantasy role playing game, Dungeons & Dragons. The data set was\nextracted from the Forgotten Realms Fandom wiki consisting of more than over\n45,200 articles. The FRW data set is constituted of 11 sub-data sets in a\nnumber of formats: raw plain text, plain text annotated by article title,\ndirected link graphs, wiki info-boxes annotated by the wiki article title,\nPoincar\\'e embedding of first link graph, multiple Word2Vec and Doc2Vec models\nof the corpus. This is the first data set of this size for the Dungeons &\nDragons domain. We then present a pairwise similarity comparison benchmark\nwhich utilizes similarity measures. In addition, we perform D&D domain specific\nnatural language generation using the corpus and evaluate the named entity\nclassification with respect to the lore of Forgotten Realms.",
    "descriptor": "",
    "authors": [
      "Akila Peiris",
      "Nisansa de Silva"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09080"
  },
  {
    "id": "arXiv:2212.09082",
    "title": "On the Connection between Invariant Learning and Adversarial Training  for Out-of-Distribution Generalization",
    "abstract": "Despite impressive success in many tasks, deep learning models are shown to\nrely on spurious features, which will catastrophically fail when generalized to\nout-of-distribution (OOD) data. Invariant Risk Minimization (IRM) is proposed\nto alleviate this issue by extracting domain-invariant features for OOD\ngeneralization. Nevertheless, recent work shows that IRM is only effective for\na certain type of distribution shift (e.g., correlation shift) while it fails\nfor other cases (e.g., diversity shift). Meanwhile, another thread of method,\nAdversarial Training (AT), has shown better domain transfer performance,\nsuggesting that it has the potential to be an effective candidate for\nextracting domain-invariant features. This paper investigates this possibility\nby exploring the similarity between the IRM and AT objectives. Inspired by this\nconnection, we propose Domainwise Adversarial Training (DAT), an AT-inspired\nmethod for alleviating distribution shift by domain-specific perturbations.\nExtensive experiments show that our proposed DAT can effectively remove\ndomain-varying features and improve OOD generalization under both correlation\nshift and diversity shift.",
    "descriptor": "\nComments: To appear in AAAI-23\n",
    "authors": [
      "Shiji Xin",
      "Yifei Wang",
      "Jingtong Su",
      "Yisen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09082"
  },
  {
    "id": "arXiv:2212.09083",
    "title": "Influence-Based Mini-Batching for Graph Neural Networks",
    "abstract": "Using graph neural networks for large graphs is challenging since there is no\nclear way of constructing mini-batches. To solve this, previous methods have\nrelied on sampling or graph clustering. While these approaches often lead to\ngood training convergence, they introduce significant overhead due to expensive\nrandom data accesses and perform poorly during inference. In this work we\ninstead focus on model behavior during inference. We theoretically model batch\nconstruction via maximizing the influence score of nodes on the outputs. This\nformulation leads to optimal approximation of the output when we do not have\nknowledge of the trained model. We call the resulting method influence-based\nmini-batching (IBMB). IBMB accelerates inference by up to 130x compared to\nprevious methods that reach similar accuracy. Remarkably, with adaptive\noptimization and the right training schedule IBMB can also substantially\naccelerate training, thanks to precomputed batches and consecutive memory\naccesses. This results in up to 18x faster training per epoch and up to 17x\nfaster convergence per runtime compared to previous methods.",
    "descriptor": "\nComments: Published as a proceedings paper at LoG 2022\n",
    "authors": [
      "Johannes Gasteiger",
      "Chendi Qian",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09083"
  },
  {
    "id": "arXiv:2212.09086",
    "title": "PVGRU: Generating Diverse and Relevant Dialogue Responses via  Pseudo-Variational Mechanism",
    "abstract": "We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Yongkang Liu",
      "Shi Feng",
      "Daling Wang",
      "Hinrich Sch\u00fctze",
      "Yifei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09086"
  },
  {
    "id": "arXiv:2212.09088",
    "title": "LR-CSNet: Low-Rank Deep Unfolding Network for Image Compressive Sensing",
    "abstract": "Deep unfolding networks (DUNs) have proven to be a viable approach to\ncompressive sensing (CS). In this work, we propose a DUN called low-rank CS\nnetwork (LR-CSNet) for natural image CS. Real-world image patches are often\nwell-represented by low-rank approximations. LR-CSNet exploits this property by\nadding a low-rank prior to the CS optimization task. We derive a corresponding\niterative optimization procedure using variable splitting, which is then\ntranslated to a new DUN architecture. The architecture uses low-rank generation\nmodules (LRGMs), which learn low-rank matrix factorizations, as well as\ngradient descent and proximal mappings (GDPMs), which are proposed to extract\nhigh-frequency features to refine image details. In addition, the deep features\ngenerated at each reconstruction stage in the DUN are transferred between\nstages to boost the performance. Our extensive experiments on three widely\nconsidered datasets demonstrate the promising performance of LR-CSNet compared\nto state-of-the-art methods in natural image CS.",
    "descriptor": "",
    "authors": [
      "Tianfang Zhang",
      "Lei Li",
      "Christian Igel",
      "Stefan Oehmcke",
      "Fabian Gieseke",
      "Zhenming Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.09088"
  },
  {
    "id": "arXiv:2212.09090",
    "title": "Exploring Workplace Behaviors through Speaking Patterns using  Large-scale Multimodal Wearable Recordings: A Study of Healthcare Providers",
    "abstract": "Interpersonal spoken communication is central to human interaction and the\nexchange of information. Such interactive processes involve not only speech and\nspoken language but also non-verbal cues such as hand gestures, facial\nexpressions, and nonverbal vocalization, that are used to express feelings and\nprovide feedback. These multimodal communication signals carry a variety of\ninformation about the people: traits like gender and age as well as about\nphysical and psychological states and behavior. This work uses wearable\nmultimodal sensors to investigate interpersonal communication behaviors\nfocusing on speaking patterns among healthcare providers with a focus on\nnurses. We analyze longitudinal data collected from $99$ nurses in a large\nhospital setting over ten weeks. The results indicate that speaking pattern\ndifferences across shift schedules and working units. Moreover, results show\nthat speaking patterns combined with physiological measures can be used to\npredict affect measures and life satisfaction scores. The implementation of\nthis work can be accessed at https://github.com/usc-sail/tiles-audio-arousal.",
    "descriptor": "",
    "authors": [
      "Tiantian Feng",
      "Shrikanth Narayanan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.09090"
  },
  {
    "id": "arXiv:2212.09091",
    "title": "A convergent numerical algorithm for the stochastic growth-fragmentation  problem",
    "abstract": "The stochastic growth-fragmentation model describes the temporal evolution of\na structured cell population through a discrete-time and continuous-state\nMarkov chain. The simulations of this stochastic process and its invariant\nmeasure are of interest. In this paper, we propose a numerical scheme for both\nthe simulation of the process and the computation of the invariant measure, and\nshow that under appropriate assumptions, the numerical chain converges to the\ncontinuous growth-fragmentation chain with an explicit error bound. With a\ntriangle inequality argument, we are also able to quantitatively estimate the\ndistance between the invariant measures of these two Markov chains.",
    "descriptor": "",
    "authors": [
      "Dawei Wu",
      "Zhennan Zhou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09091"
  },
  {
    "id": "arXiv:2212.09093",
    "title": "A Degree Based Approximation of an SIR Model with Contact Tracing and  Isolation",
    "abstract": "In this paper we study a susceptible infectious recovered (SIR) model with\nasymptomatic patients, contact tracing and isolation on a configuration\nnetwork. Using degree based approximation, we derive a system of differential\nequations for this model. This system can not be solved analytically. We\npresent an early-time analysis for the model. The early-time analysis produces\nan epidemic threshold. On one side of the threshold, the disease dies out\nquickly. On the other side, a significant fraction of the population are\ninfected. The threshold only depends on the parameters of the disease, the mean\naccess degree of the network, and the fraction of asymptomatic patients. The\nthreshold does not depend on the parameter of contact tracing and isolation\npolicy. We present an approximate analysis which greatly reduces computational\ncomplexity. The nonlinear system derived from the approximation is not almost\nlinear. We present a stability analysis for this system. We simulate the SIR\nmodel with contact tracing and isolation on five real-world networks.\nSimulation results show that contact tracing and isolation are useful to\ncontain epidemics.",
    "descriptor": "",
    "authors": [
      "Duan-Shin Lee",
      "Ting-Zhe Liu",
      "Ruhui Zhang",
      "Cheng-Shang Chang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2212.09093"
  },
  {
    "id": "arXiv:2212.09095",
    "title": "Rethinking the Role of Scale for In-Context Learning: An  Interpretability-based Case Study at 66 Billion Scale",
    "abstract": "Language models have been shown to perform better with an increase in scale\non a wide variety of tasks via the in-context learning paradigm. In this paper,\nwe investigate the hypothesis that the ability of a large language model to\nin-context learn-perform a task is not uniformly spread across all of its\nunderlying components. Using a 66 billion parameter language model (OPT-66B)\nacross a diverse set of 14 downstream tasks, we find this is indeed the case:\n$\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be\nremoved with minimal decline in task performance. We find substantial overlap\nin the set of attention heads (un)important for in-context learning across\ntasks and number of in-context examples. We also address our hypothesis through\na task-agnostic lens, finding that a small set of attention heads in OPT-66B\nscore highly on their ability to perform primitive induction operations\nassociated with in-context learning, namely, prefix matching and copying. These\ninduction heads overlap with task-specific important heads, suggesting that\ninduction heads are among the heads capable of more sophisticated behaviors\nassociated with in-context learning. Overall, our study provides several\ninsights that indicate large language models may be under-trained to perform\nin-context learning and opens up questions on how to pre-train language models\nto more effectively perform in-context learning.",
    "descriptor": "\nComments: 21 pages, 19 figures, 1 table, 2 algorithms\n",
    "authors": [
      "Hritik Bansal",
      "Karthik Gopalakrishnan",
      "Saket Dingliwal",
      "Sravan Bodapati",
      "Katrin Kirchhoff",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09095"
  },
  {
    "id": "arXiv:2212.09096",
    "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on  DAG-based Blockchain",
    "abstract": "Decentralized Storage Networks (DSNs) can gather storage resources from\nmutually untrusted providers and form worldwide decentralized file systems.\nCompared to traditional storage networks, DSNs are built on top of blockchains,\nwhich can incentivize service providers and ensure strong security. However,\nexisting DSNs face two major challenges. First, deduplication can only be\nachieved at the directory-level. Missing file-level deduplication leads to\nunavoidable extra storage and bandwidth cost. Second, current DSNs realize file\nindexing by storing extra metadata while blockchain ledgers are not fully\nexploited. To overcome these problems, we propose FileDAG, a DSN built on\nDAG-based blockchain to support file-level deduplication in storing\nmulti-versioned files. When updating files, we adopt an increment generation\nmethod to calculate and store only the increments instead of the entire updated\nfiles. Besides, we introduce a two-layer DAG-based blockchain ledger, by which\nFileDAG can provide flexible and storage-saving file indexing by directly using\nthe blockchain database without incurring extra storage overhead. We implement\nFileDAG and evaluate its performance with extensive experiments. The results\ndemonstrate that FileDAG outperforms the state-of-the-art industrial DSNs\nconsidering storage cost and latency.",
    "descriptor": "",
    "authors": [
      "Hechuan Guo",
      "Minghui Xu",
      "Jiahao Zhang",
      "Chunchi Liu",
      "Dongxiao Yu",
      "Schahram Dustdar",
      "Xiuzhen Cheng"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.09096"
  },
  {
    "id": "arXiv:2212.09097",
    "title": "Continually Learning from Existing Models: Knowledge Accumulation for  Neural Machine Translation",
    "abstract": "Although continually extending an existing NMT model to new domains or\nlanguages has attracted intensive interest in recent years, the equally\nvaluable problem of continually improving a given NMT model in its domain by\nleveraging knowledge from an unlimited number of existing NMT models is not\nexplored yet. To facilitate the study, we propose a formal definition for the\nproblem named knowledge accumulation for NMT (KA-NMT) with corresponding\ndatasets and evaluation metrics and develop a novel method for KA-NMT. We\ninvestigate a novel knowledge detection algorithm to identify beneficial\nknowledge from existing models at token level, and propose to learn from\nbeneficial knowledge and learn against other knowledge simultaneously to\nimprove learning efficiency. To alleviate catastrophic forgetting, we further\npropose to transfer knowledge from previous to current version of the given\nmodel. Extensive experiments show that our proposed method significantly and\nconsistently outperforms representative baselines under homogeneous,\nheterogeneous, and malicious model settings for different language pairs.",
    "descriptor": "\nComments: 18 pages, 3 figures\n",
    "authors": [
      "Yuanchi Zhang",
      "Peng Li",
      "Maosong Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09097"
  },
  {
    "id": "arXiv:2212.09098",
    "title": "Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion  and UV GAN",
    "abstract": "Fine-grained semantic segmentation of a person's face and head, including\nfacial parts and head components, has progressed a great deal in recent years.\nHowever, it remains a challenging task, whereby considering ambiguous\nocclusions and large pose variations are particularly difficult. To overcome\nthese difficulties, we propose a novel framework termed Mask-FPAN. It uses a\nde-occlusion module that learns to parse occluded faces in a semi-supervised\nway. In particular, face landmark localization, face occlusionstimations, and\ndetected head poses are taken into account. A 3D morphable face model combined\nwith the UV GAN improves the robustness of 2D face parsing. In addition, we\nintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face\nparing work. The proposed Mask-FPAN framework addresses the face parsing\nproblem in the wild and shows significant performance improvements with MIOU\nfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging face\ndatasets.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Lei Li",
      "Tianfang Zhang",
      "Stefan Oehmcke",
      "Fabian Gieseke",
      "Christian Igel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09098"
  },
  {
    "id": "arXiv:2212.09099",
    "title": "On the design of energy-decaying momentum-conserving integrator for  nonlinear dynamics using energy splitting and perturbation techniques",
    "abstract": "This work proposes a suite of numerical techniques to facilitate the design\nof structure-preserving integrators for nonlinear dynamics. The celebrated\nLaBudde-Greenspan integrator and various energy-momentum schemes adopt a\ndifference quotient formula in their algorithmic force definitions, which\nsuffers from numerical instability as the denominator gets close to zero. There\nis a need to develop structure-preserving integrators without invoking the\nquotient formula. In this work, the potential energy of a Hamiltonian system is\nsplit into two parts, and specially developed quadrature rules are applied\nseparately to them. The resulting integrators can be regarded as classical ones\nperturbed with first- or second-order terms, and the energy split guarantees\nthe dissipative nature in the numerical residual. In the meantime, the\nconservation of invariants is respected in the design. A complete analysis of\nthe proposed integrators is given, with representative numerical examples\nprovided to demonstrate their performance. They can be used either\nindependently as energy-decaying and momentum-conserving schemes for nonlinear\nproblems or as an alternate option with a conserving integrator, such as the\nLaBudde-Greenspan integrator, when the numerical instability in the difference\nquotient is detected.",
    "descriptor": "",
    "authors": [
      "Ju Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Chaotic Dynamics (nlin.CD)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2212.09099"
  },
  {
    "id": "arXiv:2212.09100",
    "title": "SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input  Images",
    "abstract": "Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset will be made public with the code and\nmodels on the project website https://abdullahamdi.com/sparf/ .",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Abdullah Hamdi",
      "Bernard Ghanem",
      "Matthias Nie\u00dfner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09100"
  },
  {
    "id": "arXiv:2212.09102",
    "title": "Face Generation and Editing with StyleGAN: A Survey",
    "abstract": "Our goal with this survey is to provide an overview of the state of the art\ndeep learning technologies for face generation and editing. We will cover\npopular latest architectures and discuss key ideas that make them work, such as\ninversion, latent representation, loss functions, training procedures, editing\nmethods, and cross domain style transfer. We particularly focus on GAN-based\narchitectures that have culminated in the StyleGAN approaches, which allow\ngeneration of high-quality face images and offer rich interfaces for\ncontrollable semantics editing and preserving photo quality. We aim to provide\nan entry point into the field for readers that have basic knowledge about the\nfield of deep learning and are looking for an accessible introduction and\noverview.",
    "descriptor": "",
    "authors": [
      "Andrew Melnik",
      "Maksim Miasayedzenkau",
      "Dzianis Makarovets",
      "Dzianis Pirshtuk",
      "Eren Akbulut",
      "Dennis Holzmann",
      "Tarek Renusch",
      "Gustav Reichert",
      "Helge Ritter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09102"
  },
  {
    "id": "arXiv:2212.09104",
    "title": "LaSQuE: Improved Zero-Shot Classification from Explanations Through  Quantifier Modeling and Curriculum Learning",
    "abstract": "A hallmark of human intelligence is the ability to learn new concepts purely\nfrom language. Several recent approaches have explored training machine\nlearning models via natural language supervision. However, these approaches\nfall short in leveraging linguistic quantifiers (such as 'always' or 'rarely')\nand mimicking humans in compositionally learning complex tasks. Here, we\npresent LaSQuE, a method that can learn zero-shot classifiers from language\nexplanations by using three new strategies - (1) modeling the semantics of\nlinguistic quantifiers in explanations (including exploiting ordinal strength\nrelationships, such as 'always' > 'likely'), (2) aggregating information from\nmultiple explanations using an attention-based mechanism, and (3) model\ntraining via curriculum learning. With these strategies, LaSQuE outperforms\nprior work, showing an absolute gain of up to 7% in generalizing to unseen\nreal-world classification tasks.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Sayan Ghosh",
      "Rakesh R Menon",
      "Shashank Srivastava"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09104"
  },
  {
    "id": "arXiv:2212.09107",
    "title": "A Generalized Framework for Critical Heat Flux Detection Using  Unsupervised Image-to-Image Translation",
    "abstract": "This work proposes a framework developed to generalize Critical Heat Flux\n(CHF) detection classification models using an Unsupervised Image-to-Image\n(UI2I) translation model. The framework enables a typical classification model\nthat was trained and tested on boiling images from domain A to predict boiling\nimages coming from domain B that was never seen by the classification model.\nThis is done by using the UI2I model to transform the domain B images to look\nlike domain A images that the classification model is familiar with. Although\nCNN was used as the classification model and Fixed-Point GAN (FP-GAN) was used\nas the UI2I model, the framework is model agnostic. Meaning, that the framework\ncan generalize any image classification model type, making it applicable to a\nvariety of similar applications and not limited to the boiling crisis detection\nproblem. It also means that the more the UI2I models advance, the better the\nperformance of the framework.",
    "descriptor": "\nComments: This work has been submitted to the Expert Systems With Applications Journal on Sep 25, 2022\n",
    "authors": [
      "Firas Al-Hindawi",
      "Tejaswi Soorib",
      "Han Hu",
      "Md Siddiquee",
      "Hyunsoo Yoon",
      "Teresa Wu",
      "Ying Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.09107"
  },
  {
    "id": "arXiv:2212.09114",
    "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
    "abstract": "The dual-encoder has become the de facto architecture for dense retrieval.\nTypically, it computes the latent representations of the query and document\nindependently, thus failing to fully capture the interactions between the query\nand document. To alleviate this, recent work expects to get query-informed\nrepresentations of documents. During training, it expands the document with a\nreal query, while replacing the real query with a generated pseudo query at\ninference. This discrepancy between training and inference makes the dense\nretrieval model pay more attention to the query information but ignore the\ndocument when computing the document representation. As a result, it even\nperforms worse than the vanilla dense retrieval model, since its performance\ndepends heavily on the relevance between the generated queries and the real\nquery. In this paper, we propose a curriculum sampling strategy, which also\nresorts to the pseudo query at training and gradually increases the relevance\nof the generated query to the real query. In this way, the retrieval model can\nlearn to extend its attention from the document only to both the document and\nquery, hence getting high-quality query-informed document representations.\nExperimental results on several passage retrieval datasets show that our\napproach outperforms the previous dense retrieval methods1.",
    "descriptor": "",
    "authors": [
      "Xingwei He",
      "Yeyun Gong",
      "A-Long Jin",
      "Hang Zhang",
      "Anlei Dong",
      "Jian Jiao",
      "Siu Ming Yiu",
      "Nan Duan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09114"
  },
  {
    "id": "arXiv:2212.09121",
    "title": "RIScatter: Unifying Backscatter Communication and Reconfigurable  Intelligent Surface",
    "abstract": "Backscatter Communication (BackCom) nodes harvest energy from and modulate\ninformation over an external electromagnetic wave. Reconfigurable Intelligent\nSurface (RIS) adapts its phase shift response to enhance or attenuate channel\nstrength in specific directions. In this paper, we show how those two seemingly\ndifferent technologies (and their derivatives) can be unified to leverage their\nbenefits simultaneously into a single architecture called RIScatter. RIScatter\nconsists of multiple dispersed or co-located scatter nodes, whose reflection\nstates can be adapted to partially engineer the wireless channel of the\nexisting link and partially modulate their own information onto the scattered\nwave. This contrasts with BackCom (resp. RIS) where the reflection pattern is\nexclusively a function of the information symbol (resp. Channel State\nInformation (CSI)). The key principle in RIScatter is to render the probability\ndistribution of reflection states (i.e., backscatter channel input) as a joint\nfunction of the information source, CSI, and Quality of Service (QoS) of the\ncoexisting active primary and passive backscatter links. This enables RIScatter\nto softly bridge, generalize, and outperform BackCom and RIS; boil down to\neither under specific input distribution; or evolve in a mixed form for\nheterogeneous traffic control and universal hardware design. For a single-user\nmulti-node RIScatter network, we characterize the achievable\nprimary-(total-)backscatter rate region by optimizing the input distribution at\nthe nodes, the active beamforming at the Access Point (AP), and the backscatter\ndetection regions at the user. Simulation results demonstrate RIScatter nodes\ncan exploit the additional propagation paths to smoothly transition between\nbackscatter modulation and passive beamforming.",
    "descriptor": "",
    "authors": [
      "Yang Zhao",
      "Bruno Clerckx"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.09121"
  },
  {
    "id": "arXiv:2212.09125",
    "title": "Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate  Ultra-Fine Entity Typing",
    "abstract": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,\npresident, politician) of a given entity mention (e.g., Joe Biden) in context.\nState-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture.\nCE concatenates the mention (and its context) with each type and feeds the\npairs into a pretrained language model (PLM) to score their relevance. It\nbrings deeper interaction between mention and types to reach better performance\nbut has to perform N (type set size) forward passes to infer types of a single\nmention. CE is therefore very slow in inference when the type set is large\n(e.g., N = 10k for UFET). To this end, we propose to perform entity typing in a\nrecall-expand-filter manner. The recall and expand stages prune the large type\nset and generate K (K is typically less than 256) most relevant type candidates\nfor each mention. At the filter stage, we use a novel model called MCCE to\nconcurrently encode and score these K candidates in only one forward pass to\nobtain the final type prediction. We investigate different variants of MCCE and\nextensive experiments show that MCCE under our paradigm reaches SOTA\nperformance on ultra-fine entity typing and is thousands of times faster than\nthe cross-encoder. We also found MCCE is very effective in fine-grained (130\ntypes) and coarse-grained (9 types) entity typing. Our code is available at\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MCCE}.",
    "descriptor": "",
    "authors": [
      "Chengyue Jiang",
      "Wenyang Hui",
      "Yong Jiang",
      "Xiaobin Wang",
      "Pengjun Xie",
      "Kewei Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09125"
  },
  {
    "id": "arXiv:2212.09129",
    "title": "SUCRe: Leveraging Scene Structure for Underwater Color Restoration",
    "abstract": "Underwater images are altered by the physical characteristics of the medium\nthrough which light rays pass before reaching the optical sensor. Scattering\nand strong wavelength-dependent absorption significantly modify the captured\ncolors depending on the distance of observed elements to the image plane. In\nthis paper, we aim to recover the original colors of the scene as if the water\nhad no effect on them. We propose two novel methods that rely on different sets\nof inputs. The first assumes that pixel intensities in the restored image are\nnormally distributed within each color channel, leading to an alternative\noptimization of the well-known \\textit{Sea-thru} method which acts on single\nimages and their distance maps. We additionally introduce SUCRe, a new method\nthat further exploits the scene's 3D Structure for Underwater Color\nRestoration. By following points in multiple images and tracking their\nintensities at different distances to the sensor we constrain the optimization\nof the image formation model parameters. When compared to similar existing\napproaches, SUCRe provides clear improvements in a variety of scenarios ranging\nfrom natural light to deep-sea environments. The code for both approaches is\npublicly available at https://github.com/clementinboittiaux/sucre .",
    "descriptor": "",
    "authors": [
      "Cl\u00e9mentin Boittiaux",
      "Ricard Marxer",
      "Claire Dune",
      "Aur\u00e9lien Arnaubec",
      "Maxime Ferrera",
      "Vincent Hugel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09129"
  },
  {
    "id": "arXiv:2212.09132",
    "title": "JEMMA: An Extensible Java Dataset for ML4Code Applications",
    "abstract": "Machine Learning for Source Code (ML4Code) is an active research field in\nwhich extensive experimentation is needed to discover how to best use source\ncode's richly structured information. With this in mind, we introduce JEMMA, an\nExtensible Java Dataset for ML4Code Applications, which is a large-scale,\ndiverse, and high-quality dataset targeted at ML4Code. Our goal with JEMMA is\nto lower the barrier to entry in ML4Code by providing the building blocks to\nexperiment with source code models and tasks. JEMMA comes with a considerable\namount of pre-processed information such as metadata, representations (e.g.,\ncode tokens, ASTs, graphs), and several properties (e.g., metrics, static\nanalysis results) for 50,000 Java projects from the 50KC dataset, with over 1.2\nmillion classes and over 8 million methods. JEMMA is also extensible allowing\nusers to add new properties and representations to the dataset, and evaluate\ntasks on them. Thus, JEMMA becomes a workbench that researchers can use to\nexperiment with novel representations and tasks operating on source code. To\ndemonstrate the utility of the dataset, we also report results from two\nempirical studies on our data, ultimately showing that significant work lies\nahead in the design of context-aware source code models that can reason over a\nbroader network of source code entities in a software project, the very task\nthat JEMMA is designed to help with.",
    "descriptor": "",
    "authors": [
      "Anjan Karmakar",
      "Miltiadis Allamanis",
      "Romain Robbes"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09132"
  },
  {
    "id": "arXiv:2212.09134",
    "title": "The impact of RDMA on the design of networked systems",
    "abstract": "Developers of networked systems often work with low-level RDMA libraries to\ntailor network modules to take full advantage of offload capabilities offered\nby RDMA-capable network controllers. Because of the huge design space of\nnetworked data access protocols and variability in capabilities of RDMA\ninfrastructure, developers tend to reinvent and reimplement common data\nexchange protocols, wasting months of development yet missing various\nperformance and system capabilities. In this work, we summarise and categorize\nRDMA data exchange protocols and elaborate on what features they can offer to\nnetworked systems and what implications they have on their memory and network\nmanagement.",
    "descriptor": "",
    "authors": [
      "Konstantin Taranov",
      "Fabian Fischer",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.09134"
  },
  {
    "id": "arXiv:2212.09135",
    "title": "Load Mitigation and Power Tracking Control for Multi-Rotor Turbines",
    "abstract": "A model-based feasible control strategy for multi-rotor systems is presented,\npursuing two control objectives simultaneously: Mechanical loads on the main\ntower are to be mitigated, and an externally determined power change is to be\nfollowed to obtain fast power reference response in power systems. For this\npurpose, a scalable control strategy consisting of two levels is proposed: The\nfirst level consists of the decentralized control of each rotor unit. By using\nan LPV formalism, it is shown how the nonlinearities of the controlled system\nare considered in the design using a decentralized wind speed observer of each\nrotor to improve the overall closed-loop performance. To mitigate the lateral\nloads on the multi-rotor main tower caused by asymmetric rotor thrust forces, a\nhigher-level controller is introduced. Finally, the applicability of the\ncontroller structure is demonstrated by simulation studies.",
    "descriptor": "\nComments: 8 pages, 7 figures, Preprint from CWD 2023 conference proceedings\n",
    "authors": [
      "Horst Schulte",
      "Urs Giger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.09135"
  },
  {
    "id": "arXiv:2212.09140",
    "title": "Unsupervised Discontinuous Constituency Parsing with Mildly  Context-Sensitive Grammars",
    "abstract": "We study grammar induction with mildly context-sensitive grammars for\nunsupervised discontinuous parsing. Using the probabilistic linear context-free\nrewriting system (LCFRS) formalism, our approach fixes the rule structure in\nadvance and focuses on parameter learning with maximum likelihood. To reduce\nthe computational complexity of both parsing and parameter estimation, we\nrestrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)\nand further discard rules that require O(n^6) time to parse, reducing inference\nto O(n^5). We find that using a large number of nonterminals is beneficial and\nthus make use of tensor decomposition-based rank-space dynamic programming with\nan embedding-based parameterization of rule probabilities to scale up the\nnumber of nonterminals. Experiments on German and Dutch show that our approach\nis able to induce linguistically meaningful trees with continuous and\ndiscontinuous structures",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Songlin Yang",
      "Roger P. Levy",
      "Yoon Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09140"
  },
  {
    "id": "arXiv:2212.09144",
    "title": "Performance Analysis of YOLO-based Architectures for Vehicle Detection  from Traffic Images in Bangladesh",
    "abstract": "The task of locating and classifying different types of vehicles has become a\nvital element in numerous applications of automation and intelligent systems\nranging from traffic surveillance to vehicle identification and many more. In\nrecent times, Deep Learning models have been dominating the field of vehicle\ndetection. Yet, Bangladeshi vehicle detection has remained a relatively\nunexplored area. One of the main goals of vehicle detection is its real-time\napplication, where `You Only Look Once' (YOLO) models have proven to be the\nmost effective architecture. In this work, intending to find the best-suited\nYOLO architecture for fast and accurate vehicle detection from traffic images\nin Bangladesh, we have conducted a performance analysis of different variants\nof the YOLO-based architectures such as YOLOV3, YOLOV5s, and YOLOV5x. The\nmodels were trained on a dataset containing 7390 images belonging to 21 types\nof vehicles comprising samples from the DhakaAI dataset, the Poribohon-BD\ndataset, and our self-collected images. After thorough quantitative and\nqualitative analysis, we found the YOLOV5x variant to be the best-suited model,\nperforming better than YOLOv3 and YOLOv5s models respectively by 7 & 4 percent\nin mAP, and 12 & 8.5 percent in terms of Accuracy.",
    "descriptor": "\nComments: Accepted in 25th ICCIT (6 pages, 5 figures, 1 table)\n",
    "authors": [
      "Refaat Mohammad Alamgir",
      "Ali Abir Shuvro",
      "Mueeze Al Mushabbir",
      "Mohammed Ashfaq Raiyan",
      "Nusrat Jahan Rani",
      "Md. Mushfiqur Rahman",
      "Md. Hasanul Kabir",
      "Sabbir Ahmed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09144"
  },
  {
    "id": "arXiv:2212.09146",
    "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between  the Retriever and the Language Model",
    "abstract": "The emergence of large pretrained models has enabled language models to\nachieve superior performance in common NLP tasks, including language modeling\nand question answering, compared to previous static word representation\nmethods. Augmenting these models with a retriever to retrieve the related text\nand documents as supporting information has shown promise in effectively\nsolving NLP problems in a more interpretable way given that the additional\nknowledge is injected explicitly rather than being captured in the models'\nparameters. In spite of the recent progress, our analysis on\nretriever-augmented language models shows that this class of language models\nstill lack reasoning over the retrieved documents. In this paper, we study the\nstrengths and weaknesses of different retriever-augmented language models such\nas REALM, kNN-LM, FiD, ATLAS, and Flan-T5 in reasoning over the selected\ndocuments in different tasks. In particular, we analyze the reasoning failures\nof each of these models and study how the models' failures in reasoning are\nrooted in the retriever module as well as the language model.",
    "descriptor": "\nComments: 14 pages, 8 figures\n",
    "authors": [
      "Parishad BehnamGhader",
      "Santiago Miret",
      "Siva Reddy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09146"
  },
  {
    "id": "arXiv:2212.09149",
    "title": "From NEA and NIA to NESAS and SCAS: Demystifying the 5G Security  Ecosystem",
    "abstract": "Despite the numerous pompous statements regarding 5G, it is indisputable that\n5G creates a radical shift in telecommunications. The main reason is that 5G is\nan enabler of numerous applications we have long envisioned and either\nsimulated or implemented in test environments, partially or on a smaller scale.\n5G will soon unlock the potential of smart cities, industry 4.0, and IoT, to\nname a few. However, a crucial question is how much we can trust this\ntechnology. Since this technology will soon become the core infrastructure for\nall of the above, it is critical to understand the fundamental security\nmechanisms that comprise this technology and the guarantees they provide to\nassess the potential risks we are exposed to. This work follows a non-technical\nyet bottom-up approach to introduce the reader to the core security mechanisms\nand establish a baseline for the security of 5G, to demystify the principal\nnotions and processes. Based on the above, we streamline future directions and\nhighlight possible threats.",
    "descriptor": "",
    "authors": [
      "Angelos Michalas",
      "Constantinos Patsakis",
      "Dimitrios D. Vergados",
      "Dimitrios J Vergados"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.09149"
  },
  {
    "id": "arXiv:2212.09154",
    "title": "Empirical Analysis of AI-based Energy Management in Electric Vehicles: A  Case Study on Reinforcement Learning",
    "abstract": "Reinforcement learning-based (RL-based) energy management strategy (EMS) is\nconsidered a promising solution for the energy management of electric vehicles\nwith multiple power sources. It has been shown to outperform conventional\nmethods in energy management problems regarding energy-saving and real-time\nperformance. However, previous studies have not systematically examined the\nessential elements of RL-based EMS. This paper presents an empirical analysis\nof RL-based EMS in a Plug-in Hybrid Electric Vehicle (PHEV) and Fuel Cell\nElectric Vehicle (FCEV). The empirical analysis is developed in four aspects:\nalgorithm, perception and decision granularity, hyperparameters, and reward\nfunction. The results show that the Off-policy algorithm effectively develops a\nmore fuel-efficient solution within the complete driving cycle compared with\nother algorithms. Improving the perception and decision granularity does not\nproduce a more desirable energy-saving solution but better balances battery\npower and fuel consumption. The equivalent energy optimization objective based\non the instantaneous state of charge (SOC) variation is parameter sensitive and\ncan help RL-EMSs to achieve more efficient energy-cost strategies.",
    "descriptor": "",
    "authors": [
      "Jincheng Hu",
      "Yang Lin",
      "Jihao Li",
      "Zhuoran Hou",
      "Dezong Zhao",
      "Quan Zhou",
      "Jingjing Jiang",
      "Yuanjian Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09154"
  },
  {
    "id": "arXiv:2212.09155",
    "title": "Estimating the Adversarial Robustness of Attributions in Text with  Transformers",
    "abstract": "Explanations are crucial parts of deep neural network (DNN) classifiers. In\nhigh stakes applications, faithful and robust explanations are important to\nunderstand and gain trust in DNN classifiers. However, recent work has shown\nthat state-of-the-art attribution methods in text classifiers are susceptible\nto imperceptible adversarial perturbations that alter explanations\nsignificantly while maintaining the correct prediction outcome. If undetected,\nthis can critically mislead the users of DNNs. Thus, it is crucial to\nunderstand the influence of such adversarial perturbations on the networks'\nexplanations and their perceptibility. In this work, we establish a novel\ndefinition of attribution robustness (AR) in text classification, based on\nLipschitz continuity. Crucially, it reflects both attribution change induced by\nadversarial input alterations and perceptibility of such alterations. Moreover,\nwe introduce a wide set of text similarity measures to effectively capture\nlocality between two text samples and imperceptibility of adversarial\nperturbations in text. We then propose our novel TransformerExplanationAttack\n(TEA), a strong adversary that provides a tight estimation for attribution\nrobustness in text classification. TEA uses state-of-the-art language models to\nextract word substitutions that result in fluent, contextual adversarial\nsamples. Finally, with experiments on several text classification\narchitectures, we show that TEA consistently outperforms current\nstate-of-the-art AR estimators, yielding perturbations that alter explanations\nto a greater extent while being more fluent and less perceptible.",
    "descriptor": "",
    "authors": [
      "Adam Ivankay",
      "Mattia Rigotti",
      "Ivan Girardi",
      "Chiara Marchiori",
      "Pascal Frossard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09155"
  },
  {
    "id": "arXiv:2212.09162",
    "title": "Medical Diagnosis with Large Scale Multimodal Transformers: Leveraging  Diverse Data for More Accurate Diagnosis",
    "abstract": "Multimodal deep learning has been used to predict clinical endpoints and\ndiagnoses from clinical routine data. However, these models suffer from scaling\nissues: they have to learn pairwise interactions between each piece of\ninformation in each data type, thereby escalating model complexity beyond\nmanageable scales. This has so far precluded a widespread use of multimodal\ndeep learning. Here, we present a new technical approach of \"learnable\nsynergies\", in which the model only selects relevant interactions between data\nmodalities and keeps an \"internal memory\" of relevant data. Our approach is\neasily scalable and naturally adapts to multimodal data inputs from clinical\nroutine. We demonstrate this approach on three large multimodal datasets from\nradiology and ophthalmology and show that it outperforms state-of-the-art\nmodels in clinically relevant diagnosis tasks. Our new approach is transferable\nand will allow the application of multimodal deep learning to a broad set of\nclinically relevant problems.",
    "descriptor": "",
    "authors": [
      "Firas Khader",
      "Gustav Mueller-Franzes",
      "Tianci Wang",
      "Tianyu Han",
      "Soroosh Tayebi Arasteh",
      "Christoph Haarburger",
      "Johannes Stegmaier",
      "Keno Bressem",
      "Christiane Kuhl",
      "Sven Nebelung",
      "Jakob Nikolas Kather",
      "Daniel Truhn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09162"
  },
  {
    "id": "arXiv:2212.09163",
    "title": "CEDCES: A Cost Effective Deadline Constrained Evolutionary Scheduler for  Task Graphs in Multi-Cloud System",
    "abstract": "Many scientific workflows can be modeled as a Directed Acyclic Graph\n(henceforth mentioned as DAG) where the nodes represent individual tasks and\nthe directed edges represent data and control flow dependency between two\ntasks. Due to large computational resource requirements, a single cloud cannot\nmeet the requirements of the workflow. Hence, a multi-cloud system, where\nmultiple cloud providers pool their resources together becomes a good solution.\nThe major objectives considered while scheduling the tasks present in a task\ngraph include execution cost and makespan. In this paper, we present Cost\nEffective Deadline Constrained Evolutionary Scheduler (henceforth mentioned as\nCEDCES) which aims to minimize the execution cost under a given deadline\nconstraint. CEDCES contains Particle Swarm Optimization-based (henceforth\nmentioned as PSO) method in its core, however includes novel initialization,\ncrossover, and mutation schemes. Extensive simulation experiments on real-world\nworkflows show that CEDCES outperforms the state-of-art algorithms, in\nparticular, 60.41% on average in terms of execution cost. In cases where the\ndeadline is violated, CEDCES gives the least overshoot in execution time and\noutperforming the others by 10.96% on average.",
    "descriptor": "\nComments: This paper has just been accepted as a poster paper at the 38th ACM/SIGAPP Symposium On Applied Computing. Related DOI will be update soon\n",
    "authors": [
      "Atharva Tekawade",
      "Suman Banerjee"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.09163"
  },
  {
    "id": "arXiv:2212.09165",
    "title": "Unconstrained Traveling Tournament Problem is APX-complete",
    "abstract": "We show that the Unconstrained Traveling Tournament Problem (UTTP) is\nAPX-complete by presenting an L-reduction from a version of metric (1,2)-TSP to\nUTTP.\nKeywords: Traveling Tournament Problem, APX-complete, Approximation\nalgorithms, Traveling Salesman Problem",
    "descriptor": "\nComments: The results of this paper appear in preliminary form in the thesis of the first author \"The Traveling Tournament Problem'', Master's thesis, University of Waterloo, 2022. See: this https URL\n",
    "authors": [
      "Salomon Bendayan",
      "Joseph Cheriyan",
      "Kevin K.H. Cheung"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2212.09165"
  },
  {
    "id": "arXiv:2212.09166",
    "title": "A Cost Effective Reliability Aware Scheduler for Task Graphs in  Multi-Cloud System",
    "abstract": "Many scientific workflows can be represented by a Directed Acyclic Graph\n(DAG) where each node represents a task, and there will be a directed edge\nbetween two tasks if and only if there is a dependency relationship between the\ntwo i.e. the second one can not be started unless the first one is finished.\nDue to the increasing computational requirements of these workflows, they are\ndeployed on cloud computing systems. Scheduling of workflows on such systems to\nachieve certain goals(e.g. minimization of makespan, cost, or maximization of\nreliability, etc.) remains an active area of research. In this paper, we\npropose a scheduling algorithm for allocating the nodes of our task graph in a\nheterogeneous multi-cloud system. The proposed scheduler considers many\npractical concerns such as pricing mechanisms, discounting schemes, and\nreliability analysis for task execution. This is a list-based heuristic that\nallocates tasks based on the expected times for which VMs need to be rented for\nthem. We have analyzed the proposed approach to understand its time\nrequirement. We perform a large number of experiments with real-world\nworkflows: FFT, Ligo, Epigenomics, and Random workflows and observe that the\nproposed scheduler outperforms the state-of-art approaches up to 12%, 11%, and\n1.1% in terms of cost, makespan, and reliability, respectively.",
    "descriptor": "\nComments: This paper has been recently accepted as a full paper at the 15th International Conference on COMmunication Systems & NETworkS (COMSNETS 2023)\n",
    "authors": [
      "Atharva Tekawade",
      "Suman Banerjee"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.09166"
  },
  {
    "id": "arXiv:2212.09170",
    "title": "On Isotropy and Learning Dynamics of Contrastive-based Sentence  Representation Learning",
    "abstract": "Incorporating contrastive learning objectives in sentence representation\nlearning (SRL) has yielded significant improvements on many sentence-level NLP\ntasks. However, It is not well understood why contrastive learning works for\nlearning sentence-level semantics. In this paper, we take a closer look at\ncontrastive sentence representation learning through the lens of isotropy and\nlearning dynamics. We interpret its success stories through the geometry of the\nrepresentation shifts. We show that contrastive learning brings isotropy, and\nsurprisingly learns to converge tokens to similar positions in the semantic\nspace if given the signal that they are in the same sentence. Also, what we\nformalize as \"spurious contextualization\" is mitigated for semantically\nmeaningful tokens, while augmented for functional ones. The embedding space is\npushed toward the origin during training, with more areas now better defined.\nWe ablate these findings by observing the learning dynamic with different\ntraining temperatures, batch sizes and pooling methods. With these findings, we\naim to shed light on future designs of sentence representation learning\nmethods.",
    "descriptor": "\nComments: 17 pages, 24 figures\n",
    "authors": [
      "Chenghao Xiao",
      "Yang Long",
      "Noura Al Moubayed"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09170"
  },
  {
    "id": "arXiv:2212.09171",
    "title": "Rainproof: An Umbrella To Shield Text Generators From  Out-Of-Distribution Data",
    "abstract": "As more and more conversational and translation systems are deployed in\nproduction, it is essential to implement and to develop effective control\nmechanisms guaranteeing their proper functioning and security. An essential\ncomponent to ensure safe system behavior is out-of-distribution (OOD)\ndetection, which aims at detecting whether an input sample is statistically far\nfrom the training distribution. Although OOD detection is a widely covered\ntopic in classification tasks, it has received much less attention in text\ngeneration. This paper addresses the problem of OOD detection for machine\ntranslation and dialog generation from an operational perspective. Our\ncontributions include: (i) RAINPROOF a Relative informAItioN Projection ODD\ndetection framework; and (ii) a more operational evaluation setting for OOD\ndetection. Surprisingly, we find that OOD detection is not necessarily aligned\nwith task-specific measures. The OOD detector may filter out samples that are\nwell processed by the model and keep samples that are not, leading to weaker\nperformance. Our results show that RAINPROOF breaks this curse and achieve good\nresults in OOD detection while increasing performance.",
    "descriptor": "",
    "authors": [
      "Maxime Darrin",
      "Pablo Piantanida",
      "Pierre Colombo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09171"
  },
  {
    "id": "arXiv:2212.09172",
    "title": "Knowledge Transfer and Reuse: A Case Study of AI-enabled Resource  Management in RAN Slicing",
    "abstract": "An efficient resource management scheme is critical to enable network slicing\nin 5G networks and in envisioned 6G networks, and artificial intelligence (AI)\ntechniques offer promising solutions. Considering the rapidly emerging new\nmachine learning techniques, such as graph learning, federated learning, and\ntransfer learning, a timely survey is needed to provide an overview of resource\nmanagement and network slicing techniques of AI-enabled wireless networks. This\narticle provides such a survey along with an application of knowledge transfer\nin radio access network (RAN) slicing. In particular, we firs provide some\nbackground on resource management and network slicing, and review relevant\nstate-of-the-art AI and machine learning (ML) techniques and their\napplications. Then, we introduce our AI-enabled knowledge transfer and\nreuse-based resource management (AKRM) scheme, where we apply transfer learning\nto improve system performance. Compared with most existing works, which focus\non the training of standalone agents from scratch, the main difference of AKRM\nlies in its knowledge transfer and reuse capability between different tasks.\nOur paper aims to be a roadmap for researchers to use knowledge transfer\nschemes in AI-enabled wireless networks, and we provide a case study over the\nresource allocation problem in RAN slicing.",
    "descriptor": "\nComments: This work has been accepted by IEEE Wireless Communications Magazine. All rights belong to IEEE\n",
    "authors": [
      "Hao Zhou",
      "Melike Erol-Kantarci",
      "Vincent Poor"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.09172"
  },
  {
    "id": "arXiv:2212.09175",
    "title": "Predicting Citi Bike Demand Evolution Using Dynamic Graphs",
    "abstract": "Bike sharing systems often suffer from poor capacity management as a result\nof variable demand. These bike sharing systems would benefit from models to\npredict demand in order to moderate the number of bikes stored at each station.\nIn this paper, we attempt to apply a graph neural network model to predict bike\ndemand in the New York City, Citi Bike dataset.",
    "descriptor": "",
    "authors": [
      "Alexander Saff",
      "Mayur Bhandary",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09175"
  },
  {
    "id": "arXiv:2212.09180",
    "title": "Don't Forget Your ABC's: Evaluating the State-of-the-Art in  Chat-Oriented Dialogue Systems",
    "abstract": "There has been great recent advancement in human-computer chat. However,\nproper evaluation currently requires human judgements that produce notoriously\nhigh-variance metrics due to their inherent subjectivity. Furthermore, there is\nlittle standardization in the methods and labels used for evaluation, with an\noverall lack of work to compare and assess the validity of various evaluation\napproaches. As a consequence, existing evaluation results likely leave an\nincomplete picture of the strengths and weaknesses of open-domain chatbots. We\naim towards a dimensional evaluation of human-computer chat that can reliably\nmeasure several distinct aspects of chat quality. To this end, we present our\nnovel human evaluation method that quantifies the rate of several\nquality-related chatbot behaviors. Our results demonstrate our method to be\nmore suitable for dimensional chat evaluation than alternative likert-style or\ncomparative methods. We then use our validated method and existing methods to\nevaluate four open-domain chat models from the recent literature.",
    "descriptor": "",
    "authors": [
      "Sarah E. Finch",
      "James D. Finch",
      "Jinho D. Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09180"
  },
  {
    "id": "arXiv:2212.09184",
    "title": "Faithful Heteroscedastic Regression with Neural Networks",
    "abstract": "Heteroscedastic regression models a Gaussian variable's mean and variance as\na function of covariates. Parametric methods that employ neural networks for\nthese parameter maps can capture complex relationships in the data. Yet,\noptimizing network parameters via log likelihood gradients can yield suboptimal\nmean and uncalibrated variance estimates. Current solutions side-step this\noptimization problem with surrogate objectives or Bayesian treatments. Instead,\nwe make two simple modifications to optimization. Notably, their combination\nproduces a heteroscedastic model with mean estimates that are provably as\naccurate as those from its homoscedastic counterpart (i.e.~fitting the mean\nunder squared error loss). For a wide variety of network and task complexities,\nwe find that mean estimates from existing heteroscedastic solutions can be\nsignificantly less accurate than those from an equivalently expressive\nmean-only model. Our approach provably retains the accuracy of an equally\nflexible mean-only model while also offering best-in-class variance\ncalibration. Lastly, we show how to leverage our method to recover the\nunderlying heteroscedastic noise variance.",
    "descriptor": "",
    "authors": [
      "Andrew Stirn",
      "Hans-Hermann Wessels",
      "Megan Schertzer",
      "Laura Pereira",
      "Neville E. Sanjana",
      "David A. Knowles"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09184"
  },
  {
    "id": "arXiv:2212.09191",
    "title": "Sufficient Statistics and Split Idempotents in Discrete Probability  Theory",
    "abstract": "A sufficient statistic is a deterministic function that captures an essential\nproperty of a probabilistic function (channel, kernel). Being a sufficient\nstatistic can be expressed nicely in terms of string diagrams, as Tobias Fritz\nshowed recently, in adjoint form. This reformulation highlights the role of\nsplit idempotents, in the Fisher-Neyman factorisation theorem. Examples of a\nsufficient statistic occur in the literature, but mostly in continuous\nprobability. This paper demonstrates that there are also several fundamental\nexamples of a sufficient statistic in discrete probability. They emerge after\nsome combinatorial groundwork that reveals the relevant dagger split\nidempotents and shows that a sufficient statistic is a deterministic dagger\nepi.",
    "descriptor": "",
    "authors": [
      "Bart Jacobs"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2212.09191"
  },
  {
    "id": "arXiv:2212.09196",
    "title": "Emergent Analogical Reasoning in Large Language Models",
    "abstract": "The recent advent of large language models - large neural networks trained on\na simple predictive objective over a massive corpus of natural language - has\nreinvigorated debate over whether human cognitive capacities might emerge in\nsuch generic models given sufficient training data. Of particular interest is\nthe ability of these models to reason about novel problems zero-shot, without\nany direct training on those problems. In human cognition, this capacity is\nclosely tied to an ability to reason by analogy. Here, we performed a direct\ncomparison between human reasoners and a large language model (GPT-3) on a\nrange of analogical tasks, including a novel text-based matrix reasoning task\nclosely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed\na surprisingly strong capacity for abstract pattern induction, matching or even\nsurpassing human capabilities in most settings. Our results indicate that large\nlanguage models such as GPT-3 have acquired an emergent ability to find\nzero-shot solutions to a broad range of analogy problems.",
    "descriptor": "",
    "authors": [
      "Taylor Webb",
      "Keith J. Holyoak",
      "Hongjing Lu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09196"
  },
  {
    "id": "arXiv:2212.09219",
    "title": "Modeling and Performance Analysis of Single-Server Database Over  Quasi-static Rayleigh Fading Channel",
    "abstract": "Cloud database is the key technology in cloud computing. The effective and\nefficient service quality of the cloud database is inseparable from\ncommunication technology, just as improving communication quality will reduce\nthe concurrency phenomenon in the ticketing system. In order to visually\nobserve the impact of communication on the cloud database, we propose a\nCommunication-Database (C-D) Model with a single-server database over the\nquasi-static Rayleigh fading channel, which consists of three parts: CLIENTS\nSOURCE, COMMUNICATION SYSTEM and DATABASE SYSTEM. This paper uses the queuing\nmodel, M/G/1//K, to model the whole system. The C-D Model is analyzed in two\ncases: nonlinearity and linearity, which correspond to some instances of SISO\nand MIMO. The simulation results of average staying time, average number of\ntransactions and other performance characteristics are basically consistent\nwith the theoretical results, which verifies the validity of the C-D Model. The\ncomparison of these experimental results also proves that poor communication\nquality does lead to the reduction in the quality of service.",
    "descriptor": "",
    "authors": [
      "Mengying Chen",
      "Wannian An",
      "Yang Liu",
      "Chen Dong",
      "Xiaodong Xu",
      "Boxiao Han",
      "Ping Zhang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2212.09219"
  },
  {
    "id": "arXiv:2212.09225",
    "title": "An Extension of Fisher's Criterion: Theoretical Results with a Neural  Network Realization",
    "abstract": "Fisher's criterion is a widely used tool in machine learning for feature\nselection. For large search spaces, Fisher's criterion can provide a scalable\nsolution to select features. A challenging limitation of Fisher's criterion,\nhowever, is that it performs poorly when mean values of class-conditional\ndistributions are close to each other. Motivated by this challenge, we propose\nan extension of Fisher's criterion to overcome this limitation. The proposed\nextension utilizes the available heteroscedasticity of class-conditional\ndistributions to distinguish one class from another. Additionally, we describe\nhow our theoretical results can be casted into a neural network framework, and\nconduct a proof-of-concept experiment to demonstrate the viability of our\napproach to solve classification problems.",
    "descriptor": "",
    "authors": [
      "Ibrahim Alsolami",
      "Tomoki Fukai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09225"
  },
  {
    "id": "arXiv:2212.09227",
    "title": "Blockchain Interoperability Landscape",
    "abstract": "Blockchain has become a popular emergent technology in many industries. It is\nsuitable for a broad range of applications, from its base role as an immutable\ndistributed ledger to the deployment of distributed applications. Many\norganizations are adopting the technology, but choosing a specific blockchain\nimplementation in an emerging field exposes them to significant technology\nrisk. Selecting the wrong implementation could expose an organization to\nsecurity vulnerabilities, reduce access to its target audience, or cause issues\nin the future when switching to a more mature protocol. Blockchain\ninteroperability aims to solve this adaptability problem by increasing the\nextensibility of blockchain, enabling the addition of new use cases and\nfeatures without sacrificing the performance of the original blockchain.\nHowever, most existing blockchain platforms need to be designed for\ninteroperability, and simple operations like sending assets across platforms\ncreate problems. Cryptographic protocols that are secure in isolation may\nbecome insecure when several different (individually secure) protocols are\ncomposed. Similarly, utilizing trusted custodians may undercut most of the\nbenefits of decentralization offered by blockchain-based systems. Even though\nthere is some research and development in the field of blockchain\ninteroperability, a characterization of the interoperability solutions for\nvarious infrastructure options is lacking. This paper presents a methodology\nfor characterizing blockchain interoperability solutions that will help focus\non new developments and evaluate existing and future solutions in this space.",
    "descriptor": "\nComments: 10 pages, 7 figures, 2 tables. Accepted at IEEE 10th International Workshop on Distributed Storage and Blockchain Technologies for Big Data\n",
    "authors": [
      "Inwon Kang",
      "Aparna Gupta",
      "Oshani Seneviratne"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2212.09227"
  },
  {
    "id": "arXiv:2212.09229",
    "title": "Mobile Edge Computing for the Metaverse",
    "abstract": "The Metaverse has emerged as the next generation of the Internet. It aims to\nprovide an immersive, persistent virtual space where people can live, learn,\nwork and interact with each other. However, the existing technology is\ninadequate to guarantee high visual quality and ultra-low latency service for\nthe Metaverse players. Mobile Edge Computing (MEC) is a paradigm where proximal\nedge servers are utilized to perform computation-intensive and\nlatency-sensitive tasks like image processing and video analysis. In MEC, the\nlarge amount of data is processed by edge servers closest to where it is\ncaptured, thus significantly reducing the latency and providing almost\nreal-time performance. In this paper, we integrate fundamental elements (5G and\n6G wireless communications, Blockchain, digital twin and artificial\nintelligence) into the MEC framework to facilitate the Metaverse. We also\nelaborate on the research problems and applications in the MEC-enabled\nMetaverse. Finally, we provide a case study to establish a thorough knowledge\nof the user utility maximization problem in a real-world scenario and gain some\ninsights about trends in potential research directions.",
    "descriptor": "",
    "authors": [
      "Chang Liu",
      "Yitong Wang",
      "Jun Zhao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2212.09229"
  },
  {
    "id": "arXiv:2212.09230",
    "title": "Quantum algebra in R: the weyl package",
    "abstract": "Weyl algebra is a simple noncommutative system used in quantum mechanics.\nHere I introduce the weyl package, written in the R computing language, which\nfurnishes functionality for working with univariate and multivariate Weyl\nalgebras. The package is available on CRAN at\nhttps://CRAN.R-project.org/package=weyl.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Robin K. S. Hankin"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2212.09230"
  },
  {
    "id": "arXiv:2212.09233",
    "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
    "abstract": "Aspect or query-based summarization has recently caught more attention, as it\ncan generate differentiated summaries based on users' interests. However, the\ncurrent dataset for aspect or query-based summarization either focuses on\nspecific domains, contains relatively small-scale instances, or includes only a\nfew aspect types. Such limitations hinder further explorations in this\ndirection. In this work, we take advantage of crowd-sourcing knowledge on\nWikipedia.org and automatically create a high-quality, large-scale open-domain\naspect-based summarization dataset named OASum, which contains more than 3.7\nmillion instances with around 1 million different aspects on 2 million\nWikipedia pages. We provide benchmark results on OAsum and demonstrate its\nability for diverse aspect-based summarization generation. To overcome the data\nscarcity problem on specific domains, we also perform zero-shot, few-shot, and\nfine-tuning on seven downstream datasets. Specifically, zero/few-shot and\nfine-tuning results show that the model pre-trained on our corpus demonstrates\na strong aspect or query-focused generation ability compared with the backbone\nmodel. Our dataset and pre-trained checkpoints are publicly available.",
    "descriptor": "",
    "authors": [
      "Xianjun Yang",
      "Kaiqiang Song",
      "Sangwoo Cho",
      "Xiaoyang Wang",
      "Xiaoman Pan",
      "Linda Petzold",
      "Dong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09233"
  },
  {
    "id": "arXiv:2212.09234",
    "title": "Real-Time Deformable-Contact-Aware Model Predictive Control for  Force-Modulated Manipulation",
    "abstract": "Force modulation of robotic manipulators has been extensively studied for\nseveral decades. However, it is not yet commonly used in safety-critical\napplications due to a lack of accurate interaction contact modeling and weak\nperformance guarantees - a large proportion of them concerning the modulation\nof interaction forces. This study presents a high-level framework for\nsimultaneous trajectory optimization and force control of the interaction\nbetween a manipulator and soft environments, which is prone to external\ndisturbances. Sliding friction and normal contact force are taken into account.\nThe dynamics of the soft contact model and the manipulator are simultaneously\nincorporated in a trajectory optimizer to generate desired motion and force\nprofiles. A constrained optimization framework based on Alternative Direction\nMethod of Multipliers (ADMM) has been employed to efficiently generate\nreal-time optimal control inputs and high-dimensional state trajectories in a\nModel Predictive Control fashion. Experimental validation of the model\nperformance is conducted on a soft substrate with known material properties\nusing a Cartesian space force control mode. Results show a comparison of ground\ntruth and real-time model-based contact force and motion tracking for multiple\nCartesian motions in the valid range of the friction model. It is shown that a\ncontact model-based motion planner can compensate for frictional forces and\nmotion disturbances and improve the overall motion and force tracking accuracy.\nThe proposed high-level planner has the potential to facilitate the automation\nof medical tasks involving the manipulation of compliant, delicate, and\ndeformable tissues.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2004.09734\n",
    "authors": [
      "Lasitha Wijayarathne",
      "Ziyi Zhou",
      "Ye Zhao",
      "Frank L. Hammond III"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09234"
  },
  {
    "id": "arXiv:2212.09235",
    "title": "PAL: Persona-Augmented Emotional Support Conversation Generation",
    "abstract": "Due to the lack of human resources for mental health support, there is an\nincreasing demand for employing conversational agents for support. Recent work\nhas demonstrated the effectiveness of dialogue models in providing emotional\nsupport. As previous studies have demonstrated that seekers' persona is an\nimportant factor for effective support, we investigate whether there are\nbenefits to modeling such information in dialogue models for support. In this\npaper, our empirical analysis verifies that persona has an important impact on\nemotional support. Therefore, we propose a framework for dynamically inferring\nand modeling seekers' persona. We first train a model for inferring the\nseeker's persona from the conversation history. Accordingly, we propose PAL, a\nmodel that leverages persona information and, in conjunction with our\nstrategy-based controllable generation method, provides personalized emotional\nsupport. Automatic and manual evaluations demonstrate that our proposed model,\nPAL, achieves state-of-the-art results, outperforming the baselines on the\nstudied benchmark. Our code and data are publicly available at\nhttps://github.com/chengjl19/PAL.",
    "descriptor": "",
    "authors": [
      "Jiale Cheng",
      "Sahand Sabour",
      "Hao Sun",
      "Zhuang Chen",
      "Minlie Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09235"
  },
  {
    "id": "arXiv:2212.09239",
    "title": "On Non-Interactive Source Simulation via Fourier Transform",
    "abstract": "The non-interactive source simulation (NISS) scenario is considered. In this\nscenario, a pair of distributed agents, Alice and Bob, observe a distributed\nbinary memoryless source $(X^d,Y^d)$ generated based on joint distribution\n$P_{X,Y}$. The agents wish to produce a pair of discrete random variables\n$(U_d,V_d)$ with joint distribution $P_{U_d,V_d}$, such that $P_{U_d,V_d}$\nconverges in total variation distance to a target distribution $Q_{U,V}$ as the\ninput blocklength $d$ is taken to be asymptotically large. Inner and outer\nbounds are obtained on the set of distributions $Q_{U,V}$ which can be produced\ngiven an input distribution $P_{X,Y}$. To this end, a bijective mapping from\nthe set of distributions $Q_{U,V}$ to a union of star-convex sets is provided.\nBy leveraging proof techniques from discrete Fourier analysis along with a\nnovel randomized rounding technique, inner and outer bounds are derived for\neach of these star-convex sets, and by inverting the aforementioned bijective\nmapping, necessary and sufficient conditions on $Q_{U,V}$ and $P_{X,Y}$ are\nprovided under which $Q_{U,V}$ can be produced from $P_{X,Y}$. The bounds are\napplicable in NISS scenarios where the output alphabets $\\mathcal{U}$ and\n$\\mathcal{V}$ have arbitrary finite size. In case of binary output alphabets,\nthe outer-bound recovers the previously best-known outer-bound.",
    "descriptor": "",
    "authors": [
      "Farhad Shirani",
      "Mohsen Heidari"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2212.09239"
  },
  {
    "id": "arXiv:2212.09242",
    "title": "Learning-from-Observation System Considering Hardware-Level Reusability",
    "abstract": "Robot developers develop various types of robots for satisfying users'\nvarious demands. Users' demands are related to their backgrounds and robots\nsuitable for users may vary. If a certain developer would offer a robot that is\ndifferent from the usual to a user, the robot-specific software has to be\nchanged. On the other hand, robot-software developers would like to reuse their\ndeveloped software as much as possible to reduce their efforts. We propose the\nsystem design considering hardware-level reusability. For this purpose, we\nbegin with the learning-from-observation framework. This framework represents a\ntarget task in robot-agnostic representation, and thus the represented task\ndescription can be shared with various robots. When executing the task, it is\nnecessary to convert the robot-agnostic description into commands of a target\nrobot. To increase the reusability, first, we implement the skill library,\nrobot motion primitives, only considering a robot hand and we regarded that a\nrobot was just a carrier to move the hand on the target trajectory. The skill\nlibrary is reusable if we would like to the same robot hand. Second, we employ\nthe generic IK solver to quickly swap a robot. We verify the hardware-level\nreusability by applying two task descriptions to two different robots, Nextage\nand Fetch.",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Jun Takamatsu",
      "Kazuhiro Sasabuchi",
      "Naoki Wake",
      "Atsushi Kanehira",
      "Katsushi Ikeuchi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09242"
  },
  {
    "id": "arXiv:2212.09246",
    "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and  Self-Imitation",
    "abstract": "Pre-trained language models, despite their rapid advancements powered by\nscale, still fall short of robust commonsense capabilities. And yet, scale\nappears to be the winning recipe; after all, the largest models seem to have\nacquired the largest amount of commonsense capabilities. Or is it?\nIn this paper, we investigate the possibility of a seemingly impossible\nmatch: can smaller language models with dismal commonsense capabilities (i.e.,\nGPT-2), ever win over models that are orders of magnitude larger and better\n(i.e., GPT-3), if the smaller models are powered with novel commonsense\ndistillation algorithms? The key intellectual question we ask here is whether\nit is possible, if at all, to design a learning algorithm that does not benefit\nfrom scale, yet leads to a competitive level of commonsense acquisition. In\nthis work, we study the generative models of commonsense knowledge, focusing on\nthe task of generating generics, statements of commonsense facts about everyday\nconcepts, e.g., birds can fly.\nWe introduce a novel commonsense distillation framework, I2D2, that loosely\nfollows the Symbolic Knowledge Distillation of West et al. but breaks the\ndependence on the extreme-scale models as the teacher model by two innovations:\n(1) the novel adaptation of NeuroLogic Decoding to enhance the generation\nquality of the weak, off-the-shelf language models, and (2) self-imitation\nlearning to iteratively learn from the model's own enhanced commonsense\nacquisition capabilities. Empirical results suggest that scale is not the only\nway, as novel algorithms can be a promising alternative. Moreover, our study\nleads to a new corpus of generics, Gen-A-Tomic, that is of the largest and\nhighest quality available to date.",
    "descriptor": "",
    "authors": [
      "Chandra Bhagavatula",
      "Jena D. Hwang",
      "Doug Downey",
      "Ronan Le Bras",
      "Ximing Lu",
      "Keisuke Sakaguchi",
      "Swabha Swayamdipta",
      "Peter West",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09246"
  },
  {
    "id": "arXiv:2212.09247",
    "title": "ColoristaNet for Photorealistic Video Style Transfer",
    "abstract": "Photorealistic style transfer aims to transfer the artistic style of an image\nonto an input image or video while keeping photorealism. In this paper, we\nthink it's the summary statistics matching scheme in existing algorithms that\nleads to unrealistic stylization. To avoid employing the popular Gram loss, we\npropose a self-supervised style transfer framework, which contains a style\nremoval part and a style restoration part. The style removal network removes\nthe original image styles, and the style restoration network recovers image\nstyles in a supervised manner. Meanwhile, to address the problems in current\nfeature transformation methods, we propose decoupled instance normalization to\ndecompose feature transformation into style whitening and restylization. It\nworks quite well in ColoristaNet and can transfer image styles efficiently\nwhile keeping photorealism. To ensure temporal coherency, we also incorporate\noptical flow methods and ConvLSTM to embed contextual information. Experiments\ndemonstrates that ColoristaNet can achieve better stylization effects when\ncompared with state-of-the-art algorithms.",
    "descriptor": "\nComments: 30 pages, 29 figures\n",
    "authors": [
      "Xiaowen Qiu",
      "Ruize Xu",
      "Boan He",
      "Yingtao Zhang",
      "Wenqiang Zhang",
      "Weifeng Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.09247"
  },
  {
    "id": "arXiv:2212.09248",
    "title": "Natural Language to Code Generation in Interactive Data Science  Notebooks",
    "abstract": "Computational notebooks, such as Jupyter notebooks, are interactive computing\nenvironments that are ubiquitous among data scientists to perform data\nwrangling and analytic tasks. To measure the performance of AI pair programmers\nthat automatically synthesize programs for those tasks given natural language\n(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation\nproblems using the pandas data analysis framework in data science notebooks.\nARCADE features multiple rounds of NL-to-code problems from the same notebook.\nIt requires a model to understand rich multi-modal contexts, such as existing\nnotebook cells and their execution states as well as previous turns of\ninteraction. To establish a strong baseline on this challenging task, we\ndevelop PaChiNCo, a 62B code language model (LM) for Python computational\nnotebooks, which significantly outperforms public code LMs. Finally, we explore\nfew-shot prompting strategies to elicit better code with step-by-step\ndecomposition and NL explanation, showing the potential to improve the\ndiversity and explainability of model predictions.",
    "descriptor": "\nComments: 46 pages. 32 figures\n",
    "authors": [
      "Pengcheng Yin",
      "Wen-Ding Li",
      "Kefan Xiao",
      "Abhishek Rao",
      "Yeming Wen",
      "Kensen Shi",
      "Joshua Howland",
      "Paige Bailey",
      "Michele Catasta",
      "Henryk Michalewski",
      "Alex Polozov",
      "Charles Sutton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09248"
  },
  {
    "id": "arXiv:2212.09251",
    "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
    "abstract": "As language models (LMs) scale, they develop many novel behaviors, good and\nbad, exacerbating the need to evaluate how they behave. Prior work creates\nevaluations with crowdwork (which is time-consuming and expensive) or existing\ndata sources (which are not always available). Here, we automatically generate\nevaluations with LMs. We explore approaches with varying amounts of human\neffort, from instructing LMs to write yes/no questions to making complex\nWinogender schemas with multiple stages of LM-based generation and filtering.\nCrowdworkers rate the examples as highly relevant and agree with 90-100% of\nlabels, sometimes more so than corresponding human-written datasets. We\ngenerate 154 datasets and discover new cases of inverse scaling where LMs get\nworse with size. Larger LMs repeat back a dialog user's preferred answer\n(\"sycophancy\") and express greater desire to pursue concerning goals like\nresource acquisition and goal preservation. We also find some of the first\nexamples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF\nmakes LMs worse. For example, RLHF makes LMs express stronger political views\n(on gun rights and immigration) and a greater desire to avoid shut down.\nOverall, LM-written evaluations are high-quality and let us quickly discover\nmany novel LM behaviors.",
    "descriptor": "\nComments: for associated data visualizations, see this https URL for full datasets, see this https URL\n",
    "authors": [
      "Ethan Perez",
      "Sam Ringer",
      "Kamil\u0117 Luko\u0161i\u016bt\u0117",
      "Karina Nguyen",
      "Edwin Chen",
      "Scott Heiner",
      "Craig Pettit",
      "Catherine Olsson",
      "Sandipan Kundu",
      "Saurav Kadavath",
      "Andy Jones",
      "Anna Chen",
      "Ben Mann",
      "Brian Israel",
      "Bryan Seethor",
      "Cameron McKinnon",
      "Christopher Olah",
      "Da Yan",
      "Daniela Amodei",
      "Dario Amodei",
      "Dawn Drain",
      "Dustin Li",
      "Eli Tran-Johnson",
      "Guro Khundadze",
      "Jackson Kernion",
      "James Landis",
      "Jamie Kerr",
      "Jared Mueller",
      "Jeeyoon Hyun",
      "Joshua Landau",
      "Kamal Ndousse",
      "Landon Goldberg",
      "Liane Lovitt",
      "Martin Lucas",
      "Michael Sellitto",
      "Miranda Zhang",
      "Neerav Kingsland",
      "Nelson Elhage",
      "Nicholas Joseph",
      "Noem\u00ed Mercado",
      "Nova DasSarma",
      "Oliver Rausch",
      "Robin Larson",
      "Sam McCandlish",
      "Scott Johnston",
      "Shauna Kravec",
      "Sheer El Showk",
      "Tamera Lanham",
      "Timothy Telleen-Lawton",
      "Tom Brown",
      "Tom Henighan",
      "Tristan Hume"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09251"
  },
  {
    "id": "arXiv:2212.09252",
    "title": "Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems",
    "abstract": "Many dialogue systems (DSs) lack characteristics humans have, such as emotion\nperception, factuality, and informativeness. Enhancing DSs with knowledge\nalleviates this problem, but, as many ways of doing so exist, keeping track of\nall proposed methods is difficult. Here, we present the first survey of\nknowledge-enhanced DSs. We define three categories of systems - internal,\nexternal, and hybrid - based on the knowledge they use. We survey the\nmotivation for enhancing DSs with knowledge, used datasets, and methods for\nknowledge search, knowledge encoding, and knowledge incorporation. Finally, we\npropose how to improve existing systems based on theories from linguistics and\ncognitive science.",
    "descriptor": "",
    "authors": [
      "Sagi Shaier",
      "Lawrence Hunter",
      "Katharina Kann"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09252"
  },
  {
    "id": "arXiv:2212.09254",
    "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven  Optimization",
    "abstract": "Robustness evaluation against adversarial examples has become increasingly\nimportant to unveil the trustworthiness of the prevailing deep models in\nnatural language processing (NLP). However, in contrast to the computer vision\ndomain where the first-order projected gradient descent (PGD) is used as the\nbenchmark approach to generate adversarial examples for robustness evaluation,\nthere lacks a principled first-order gradient-based robustness evaluation\nframework in NLP. The emerging optimization challenges lie in 1) the discrete\nnature of textual inputs together with the strong coupling between the\nperturbation location and the actual content, and 2) the additional constraint\nthat the perturbed text should be fluent and achieve a low perplexity under a\nlanguage model. These challenges make the development of PGD-like NLP attacks\ndifficult. To bridge the gap, we propose TextGrad, a new attack generator using\ngradient-driven optimization, supporting high-accuracy and high-quality\nassessment of adversarial robustness in NLP. Specifically, we address the\naforementioned challenges in a unified optimization framework. And we develop\nan effective convex relaxation method to co-optimize the continuously-relaxed\nsite selection and perturbation variables and leverage an effective sampling\nmethod to establish an accurate mapping from the continuous optimization\nvariables to the discrete textual perturbations. Moreover, as a first-order\nattack generation method, TextGrad can be baked into adversarial training to\nfurther improve the robustness of NLP models. Extensive experiments are\nprovided to demonstrate the effectiveness of TextGrad not only in attack\ngeneration for robustness evaluation but also in adversarial defense.",
    "descriptor": "\nComments: 18 pages, 2 figures\n",
    "authors": [
      "Bairu Hou",
      "Jinghan Jia",
      "Yihua Zhang",
      "Guanhua Zhang",
      "Yang Zhang",
      "Sijia Liu",
      "Shiyu Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09254"
  },
  {
    "id": "arXiv:2212.09255",
    "title": "Multi hash embeddings in spaCy",
    "abstract": "The distributed representation of symbols is one of the key technologies in\nmachine learning systems today, playing a pivotal role in modern natural\nlanguage processing. Traditional word embeddings associate a separate vector\nwith each word. While this approach is simple and leads to good performance, it\nrequires a lot of memory for representing a large vocabulary. To reduce the\nmemory footprint, the default embedding layer in spaCy is a hash embeddings\nlayer. It is a stochastic approximation of traditional embeddings that provides\nunique vectors for a large number of words without explicitly storing a\nseparate vector for each of them. To be able to compute meaningful\nrepresentations for both known and unknown words, hash embeddings represent\neach word as a summary of the normalized word form, subword information and\nword shape. Together, these features produce a multi-embedding of a word. In\nthis technical report we lay out a bit of history and introduce the embedding\nmethods in spaCy in detail. Second, we critically evaluate the hash embedding\narchitecture with multi-embeddings on Named Entity Recognition datasets from a\nvariety of domains and languages. The experiments validate most key design\nchoices behind spaCy's embedders, but we also uncover a few surprising results.",
    "descriptor": "",
    "authors": [
      "Lester James Miranda",
      "\u00c1kos K\u00e1d\u00e1r",
      "Adriane Boyd",
      "Sofie Van Landeghem",
      "Anders S\u00f8gaard",
      "Matthew Honnibal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09255"
  },
  {
    "id": "arXiv:2212.09257",
    "title": "PromptBoosting: Black-Box Text Classification with Ten Forward Passes",
    "abstract": "We describe PromptBoosting, a query-efficient procedure for building a text\nclassifier from a neural language model (LM) without access to the LM's\nparameters, gradients, or hidden representations. This form of \"black-box\"\nclassifier training has become increasingly important as the cost of training\nand inference in large-scale LMs grows. But existing black-box LM classifier\nlearning approaches are themselves computationally inefficient, typically\nspecializing LMs to the target task by searching in a large space of (discrete\nor continuous) prompts using zeroth-order optimization methods. Instead of\ndirectly optimizing in prompt space, PromptBoosting obtains a small pool of\nprompts via a gradient-free approach and then constructs a large pool of weak\nlearners by pairing these prompts with different elements of the LM's output\ndistribution. These weak learners are then ensembled using the AdaBoost\nalgorithm. The entire learning process requires only a small number of forward\npasses and no backward pass. Experiments show that PromptBoosting achieves\nstate-of-the-art performance in multiple black-box few-shot classification\ntasks, and matches or outperforms full fine-tuning in both few-shot and\nstandard learning paradigms, while training 10x faster than existing black-box\nmethods.",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Bairu Hou",
      "Joe O'Connor",
      "Jacob Andreas",
      "Shiyu Chang",
      "Yang Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09257"
  },
  {
    "id": "arXiv:2212.09258",
    "title": "CHAD: Charlotte Anomaly Dataset",
    "abstract": "In recent years, we have seen a significant interest in data-driven deep\nlearning approaches for video anomaly detection, where an algorithm must\ndetermine if specific frames of a video contain abnormal behaviors. However,\nvideo anomaly detection is particularly context-specific, and the availability\nof representative datasets heavily limits real-world accuracy. Additionally,\nthe metrics currently reported by most state-of-the-art methods often do not\nreflect how well the model will perform in real-world scenarios. In this\narticle, we present the Charlotte Anomaly Dataset (CHAD). CHAD is a\nhigh-resolution, multi-camera anomaly dataset in a commercial parking lot\nsetting. In addition to frame-level anomaly labels, CHAD is the first anomaly\ndataset to include bounding box, identity, and pose annotations for each actor.\nThis is especially beneficial for skeleton-based anomaly detection, which is\nuseful for its lower computational demand in real-world settings. CHAD is also\nthe first anomaly dataset to contain multiple views of the same scene. With\nfour camera views and over 1.15 million frames, CHAD is the largest fully\nannotated anomaly detection dataset including person annotations, collected\nfrom continuous video streams from stationary cameras for smart video\nsurveillance applications. To demonstrate the efficacy of CHAD for training and\nevaluation, we benchmark two state-of-the-art skeleton-based anomaly detection\nalgorithms on CHAD and provide comprehensive analysis, including both\nquantitative results and qualitative examination.",
    "descriptor": "",
    "authors": [
      "Armin Danesh Pazho",
      "Ghazal Alinezhad Noghre",
      "Babak Rahimi Ardabili",
      "Christopher Neff",
      "Hamed Tabkhi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09258"
  },
  {
    "id": "arXiv:2212.09260",
    "title": "CMA-ES with Margin for Single-and Multi-Objective Mixed-Integer  Black-Box Optimization",
    "abstract": "This study targets the mixed-integer black-box optimization (MI-BBO) problem\nwhere continuous and integer variables should be optimized simultaneously. The\nCMA-ES, our focus in this study, is a population-based stochastic search method\nthat samples solution candidates from a multivariate Gaussian distribution\n(MGD), which shows excellent performance in continuous BBO. The parameters of\nMGD, mean and (co)variance, are updated based on the evaluation value of\ncandidate solutions in the CMA-ES. If the CMA-ES is applied to the MI-BBO with\nstraightforward discretization, however, the variance corresponding to the\ninteger variables becomes much smaller than the granularity of the\ndiscretization before reaching the optimal solution, which leads to the\nstagnation of the optimization. In particular, when binary variables are\nincluded in the problem, this stagnation more likely occurs because the\ngranularity of the discretization becomes wider, and the existing modification\nto the CMA-ES does not address this stagnation. To overcome these limitations,\nwe propose a simple extension of the CMA-ES based on lower-bounding the\nmarginal probabilities associated with the generation of integer variables in\nthe MGD. The numerical experiments on the MI-BBO benchmark problems demonstrate\nthe efficiency and robustness of the proposed method. Furthermore, in order to\ndemonstrate the generality of the idea of the proposed method, in addition to\nthe single-objective optimization case, we incorporate it into multi-objective\nCMA-ES and verify its performance on bi-objective mixed-integer benchmark\nproblems.",
    "descriptor": "\nComments: This paper is an extended version of arXiv:2205.13482 and is submitted to the ACM Transactions on Evolutionary Learning and Optimization (TELO)\n",
    "authors": [
      "Ryoki Hamano",
      "Shota Saito",
      "Masahiro Nomura",
      "Shinichi Shirakawa"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2212.09260"
  },
  {
    "id": "arXiv:2212.09262",
    "title": "Photo-Realistic Out-of-domain GAN inversion via Invertibility  Decomposition",
    "abstract": "The fidelity of Generative Adversarial Networks (GAN) inversion is impeded by\nOut-Of-Domain (OOD) areas (e.g., background, accessories) in the image.\nDetecting the OOD areas beyond the generation ability of the pretrained model\nand blending these regions with the input image can enhance fidelity. The\n``invertibility mask\" figures out these OOD areas, and existing methods predict\nthe mask with the reconstruction error. However, the estimated mask is usually\ninaccurate due to the influence of the reconstruction error in the In-Domain\n(ID) area. In this paper, we propose a novel framework that enhances the\nfidelity of human face inversion by designing a new module to decompose the\ninput images to ID and OOD partitions with invertibility masks. Unlike previous\nworks, our invertibility detector is simultaneously learned with a spatial\nalignment module. We iteratively align the generated features to the input\ngeometry and reduce the reconstruction error in the ID regions. Thus, the OOD\nareas are more distinguishable and can be precisely predicted. Then, we improve\nthe fidelity of our results by blending the OOD areas from the input image with\nthe ID GAN inversion results. Our method produces photo-realistic results for\nreal-world human face image inversion and manipulation. Extensive experiments\ndemonstrate our method's superiority over existing methods in the quality of\nGAN inversion and attribute manipulation.",
    "descriptor": "",
    "authors": [
      "Xin Yang",
      "Xiaogang Xu",
      "Yingcong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09262"
  },
  {
    "id": "arXiv:2212.09265",
    "title": "Diversity Analysis of Multi-Aperture UWOC System over EGG Channel with  Pointing Errors",
    "abstract": "Single aperture reception for underwater wireless optical communication\n(UWOC) is insufficient to deal with oceanic turbulence caused by the combined\neffect of temperature gradient and air bubbles. This paper analyzes the\nperformance of multi-aperture reception for UWOC under channel irradiance\nfluctuations characterized by the mixture exponential generalized gamma (EGG)\ndistribution. We analyze the system performance by employing both selection\ncombining (SC) and maximum ratio combining (MRC) receivers. In particular, we\nderive the exact outage probability expression for the SC-based multi-aperture\nUWOC receiver and obtain an upper bound on the outage probability for the\nMRC-based multi-aperture UWOC receiver. With the help of the derived results,\nwe analytically obtain the diversity order of the considered multi-aperture\nUWOC system.",
    "descriptor": "\nComments: This paper has been submitted in IEEE for possible publication\n",
    "authors": [
      "Ziyaur Rahman",
      "Ankur Bansal",
      "S. M. Zafaruddin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.09265"
  },
  {
    "id": "arXiv:2212.09268",
    "title": "UAVCAN Dataset Description",
    "abstract": "We collected attack data from unmanned vehicles using the UAVCAN protocol,\nand public and described technical documents. A testbed was built with a drone\nusing PX4, and a total of three attacks, Flooding, Fuzzy, and Replay, were\nperformed. The attack was carried out in a total of 10 scenarios. We expect\nthat the attack data will help develop technologies such as anomaly detection\nto solve the security threat problem of drones.",
    "descriptor": "\nComments: in Korean language\n",
    "authors": [
      "Dongsung Kim",
      "Yuchan Song",
      "Soonhyeon Kwon",
      "Haerin Kim",
      "Jeong Do Yoo",
      "Huy Kang Kim"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2212.09268"
  },
  {
    "id": "arXiv:2212.09270",
    "title": "The One-Inclusion Graph Algorithm is not Always Optimal",
    "abstract": "The one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth\nachieves an optimal in-expectation risk bound in the standard PAC\nclassification setup. In one of the first COLT open problems, Warmuth\nconjectured that this prediction strategy always implies an optimal high\nprobability bound on the risk, and hence is also an optimal PAC algorithm. We\nrefute this conjecture in the strongest sense: for any practically interesting\nVapnik-Chervonenkis class, we provide an in-expectation optimal one-inclusion\ngraph algorithm whose high probability risk bound cannot go beyond that implied\nby Markov's inequality. Our construction of these poorly performing\none-inclusion graph algorithms uses Varshamov-Tenengolts error correcting\ncodes.\nOur negative result has several implications. First, it shows that the same\npoor high-probability performance is inherited by several recent prediction\nstrategies based on generalizations of the one-inclusion graph algorithm.\nSecond, our analysis shows yet another statistical problem that enjoys an\nestimator that is provably optimal in expectation via a leave-one-out argument,\nbut fails in the high-probability regime. This discrepancy occurs despite the\nboundedness of the binary loss for which arguments based on concentration\ninequalities often provide sharp high probability risk bounds.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Ishaq Aden-Ali",
      "Yeshwanth Cherapanamjeri",
      "Abhishek Shetty",
      "Nikita Zhivotovskiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2212.09270"
  },
  {
    "id": "arXiv:2212.09271",
    "title": "Very Large Language Model as a Unified Methodology of Text Mining",
    "abstract": "Text data mining is the process of deriving essential information from\nlanguage text. Typical text mining tasks include text categorization, text\nclustering, topic modeling, information extraction, and text summarization.\nVarious data sets are collected and various algorithms are designed for the\ndifferent types of tasks. In this paper, I present a blue sky idea that very\nlarge language model (VLLM) will become an effective unified methodology of\ntext mining. I discuss at least three advantages of this new methodology\nagainst conventional methods. Finally I discuss the challenges in the design\nand development of VLLM techniques for text mining.",
    "descriptor": "\nComments: 4 pages, 3 figures\n",
    "authors": [
      "Meng Jiang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09271"
  },
  {
    "id": "arXiv:2212.09272",
    "title": "Statistical Dataset Evaluation: Reliability, Difficulty, and Validity",
    "abstract": "Datasets serve as crucial training resources and model performance trackers.\nHowever, existing datasets have exposed a plethora of problems, inducing biased\nmodels and unreliable evaluation results. In this paper, we propose a\nmodel-agnostic dataset evaluation framework for automatic dataset quality\nevaluation. We seek the statistical properties of the datasets and address\nthree fundamental dimensions: reliability, difficulty, and validity, following\na classical testing theory. Taking the Named Entity Recognition (NER) datasets\nas a case study, we introduce $9$ statistical metrics for a statistical dataset\nevaluation framework. Experimental results and human evaluation validate that\nour evaluation framework effectively assesses various aspects of the dataset\nquality. Furthermore, we study how the dataset scores on our statistical\nmetrics affect the model performance, and appeal for dataset quality evaluation\nor targeted dataset improvement before training or testing models.",
    "descriptor": "",
    "authors": [
      "Chengwen Wang",
      "Qingxiu Dong",
      "Xiaochen Wang",
      "Haitao Wang",
      "Zhifang Sui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09272"
  },
  {
    "id": "arXiv:2212.09273",
    "title": "Learning Object-level Point Augmentor for Semi-supervised 3D Object  Detection",
    "abstract": "Semi-supervised object detection is important for 3D scene understanding\nbecause obtaining large-scale 3D bounding box annotations on point clouds is\ntime-consuming and labor-intensive. Existing semi-supervised methods usually\nemploy teacher-student knowledge distillation together with an augmentation\nstrategy to leverage unlabeled point clouds. However, these methods adopt\nglobal augmentation with scene-level transformations and hence are sub-optimal\nfor instance-level object detection. In this work, we propose an object-level\npoint augmentor (OPA) that performs local transformations for semi-supervised\n3D object detection. In this way, the resultant augmentor is derived to\nemphasize object instances rather than irrelevant backgrounds, making the\naugmented data more useful for object detector training. Extensive experiments\non the ScanNet and SUN RGB-D datasets show that the proposed OPA performs\nfavorably against the state-of-the-art methods under various experimental\nsettings. The source code will be available at https://github.com/nomiaro/OPA.",
    "descriptor": "\nComments: BMVC2022\n",
    "authors": [
      "Cheng-Ju Ho",
      "Chen-Hsuan Tai",
      "Yi-Hsuan Tsai",
      "Yen-Yu Lin",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09273"
  },
  {
    "id": "arXiv:2212.09274",
    "title": "A Makespan and Energy-Aware Scheduling Algorithm for Workflows under  Reliability Constraint on a Multiprocessor Platform",
    "abstract": "Many scientific workflows can be modeled as a Directed Acyclic Graph\n(henceforth mentioned as DAG) where the nodes represent individual tasks, and\nthe directed edges represent data and control flow dependency between two\ntasks. Due to the large volume of data, multiprocessor systems are often used\nto execute these workflows. Hence, scheduling the tasks of a workflow to\nachieve certain goals (such as minimizing the makespan, energy, or maximizing\nreliability, processor utilization, etc.) remains an active area of research in\nembedded systems. In this paper, we propose a workflow scheduling algorithm to\nminimize the makespan and energy for a given reliability constraint. If the\nreliability constraint is higher, we further propose Energy Aware Fault\nTolerant Scheduling (henceforth mentioned as EAFTS) based on active\nreplication. Additionally, given that the allocation of task nodes to\nprocessors is known, we develop a frequency allocation algorithm that assigns\nfrequencies to the processors. Mathematically we show that our algorithms can\nwork for any satisfiable reliability constraint. We analyze the proposed\nsolution approaches to understand their time requirements. Experiments with\nreal-world Workflows show that our algorithms, MERT and EAFTS, outperform the\nstate-of-art approaches. In particular, we observe that MERT gives 3.12% lesser\nenergy consumption and 14.14% lesser makespan on average. In the fault-tolerant\nsetting, our method EAFTS gives 11.11% lesser energy consumption on average\nwhen compared with the state-of-art approaches.",
    "descriptor": "\nComments: This paper has just been accepted at the 38th ACM/SIGAPP Symposium On Applied Computing\n",
    "authors": [
      "Atharva Tekawade",
      "Suman Banerjee"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2212.09274"
  },
  {
    "id": "arXiv:2212.09277",
    "title": "Building Height Prediction with Instance Segmentation",
    "abstract": "Extracting building heights from satellite images is an active research area\nused in many fields such as telecommunications, city planning, etc. Many\nstudies utilize DSM (Digital Surface Models) generated with lidars or stereo\nimages for this purpose. Predicting the height of the buildings using only RGB\nimages is challenging due to the insufficient amount of data, low data quality,\nvariations of building types, different angles of light and shadow, etc. In\nthis study, we present an instance segmentation-based building height\nextraction method to predict building masks with their respective heights from\na single RGB satellite image. We used satellite images with building height\nannotations of certain cities along with an open-source satellite dataset with\nthe transfer learning approach. We reached, the bounding box mAP 59, the mask\nmAP 52.6, and the average accuracy value of 70% for buildings belonging to each\nheight class in our test set.",
    "descriptor": "\nComments: Instance Segmentation, Satellite Images, Building Height Prediction\n",
    "authors": [
      "Furkan Burak Bagci",
      "Ahmet Alp Kindriroglu",
      "Metehan Yalcin",
      "Ufuk Uyan",
      "Mahiye Uluyagmur Ozturk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09277"
  },
  {
    "id": "arXiv:2212.09278",
    "title": "MIGA: A Unified Multi-task Generation Framework for Conversational  Text-to-SQL",
    "abstract": "Conversational text-to-SQL is designed to translate multi-turn natural\nlanguage questions into their corresponding SQL queries. Most state-of-the-art\nconversational text- to-SQL methods are incompatible with generative\npre-trained language models (PLMs), such as T5. In this paper, we present a\ntwo-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs'\nability to tackle conversational text-to-SQL. In the pre-training stage, MIGA\nfirst decomposes the main task into several related sub-tasks and then unifies\nthem into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific\nnatural language prompts to boost the main task from multi-task training. Later\nin the fine-tuning stage, we propose four SQL perturbations to alleviate the\nerror propagation problem. MIGA tends to achieve state-of-the-art performance\non two benchmarks (SparC and CoSQL). We also provide extensive analyses and\ndiscussions to shed light on some new perspectives for conversational\ntext-to-SQL.",
    "descriptor": "\nComments: Accepted by AAAI23\n",
    "authors": [
      "Yingwen Fu",
      "Wenjie Ou",
      "Zhou Yu",
      "Yue Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09278"
  },
  {
    "id": "arXiv:2212.09280",
    "title": "Superimposed Channel Estimation in OTFS Modulation Using Compressive  Sensing",
    "abstract": "Orthogonal time frequency space (OTFS) technique is a two-dimensional\nmodulation method that multiplexes information symbols in the delay-Doppler\n(DD) domain. OTFS combats high Doppler shift existing in high speed wireless\ncommunication. However, conventional channel estimation in OTFS suffers from\nhigh pilot overhead because guard symbols occupy a significant part of the DD\ndomain grids. In this paper, a superimposed channel estimation is proposed\nwhich can completely estimate channel parameters without considering pilot\noverhead and performance degradation. As the channel state information (CSI) in\nthe DD domain is sparse, a sparse recovery algorithm orthogonal matching\npursuit (OMP) is used. Besides, our proposed method does not suffer from high\npeak to average power ratio (PAPR). To detect information symbols, a message\npassing (MP) detector, which exploits the sparsity of DD channel\nrepresentation, is employed.",
    "descriptor": "",
    "authors": [
      "Omid Abbassi Aghda",
      "Mohammad Javad Omidi",
      "Hamid Saeedi-Sourck"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.09280"
  },
  {
    "id": "arXiv:2212.09282",
    "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models  for Logical Reasoning",
    "abstract": "Logical reasoning of text is an important ability that requires understanding\nthe information present in the text, their interconnections, and then reasoning\nthrough them to infer new conclusions. Prior works on improving the logical\nreasoning ability of language models require complex processing of training\ndata (e.g., aligning symbolic knowledge to text), yielding task-specific data\naugmentation solutions that restrict the learning of general logical reasoning\nskills. In this work, we propose APOLLO, an adaptively pretrained language\nmodel that has improved logical reasoning abilities. We select a subset of\nWikipedia, based on a set of logical inference keywords, for continued\npretraining of a language model. We use two self-supervised loss functions: a\nmodified masked language modeling loss where only specific parts-of-speech\nwords, that would likely require more reasoning than basic language\nunderstanding, are masked, and a sentence-level classification loss that\nteaches the model to distinguish between entailment and contradiction types of\nsentences. The proposed training paradigm is both simple and independent of\ntask formats. We demonstrate the effectiveness of APOLLO by comparing it with\nprior baselines on two logical reasoning datasets. APOLLO performs comparably\non ReClor and outperforms baselines on LogiQA.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Soumya Sanyal",
      "Yichong Xu",
      "Shuohang Wang",
      "Ziyi Yang",
      "Reid Pryzant",
      "Wenhao Yu",
      "Chenguang Zhu",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09282"
  },
  {
    "id": "arXiv:2212.09284",
    "title": "An Investigation of Indian Native Language Phonemic Influences on L2  English Pronunciations",
    "abstract": "Speech systems are sensitive to accent variations. This is especially\nchallenging in the Indian context, with an abundance of languages but a dearth\nof linguistic studies characterising pronunciation variations. The growing\nnumber of L2 English speakers in India reinforces the need to study accents and\nL1-L2 interactions. We investigate the accents of Indian English (IE) speakers\nand report in detail our observations, both specific and common to all regions.\nIn particular, we observe the phonemic variations and phonotactics occurring in\nthe speakers' native languages and apply this to their English pronunciations.\nWe demonstrate the influence of 18 Indian languages on IE by comparing the\nnative language pronunciations with IE pronunciations obtained jointly from\nexisting literature studies and phonetically annotated speech of 80 speakers.\nConsequently, we are able to validate the intuitions of Indian language\ninfluences on IE pronunciations by justifying pronunciation rules from the\nperspective of Indian language phonology. We obtain a comprehensive description\nin terms of universal and region-specific characteristics of IE, which\nfacilitates accent conversion and adaptation of existing ASR and TTS systems to\ndifferent Indian accents.",
    "descriptor": "\nComments: 9 pages, 1 figure\n",
    "authors": [
      "Shelly Jain",
      "Priyanshi Pal",
      "Anil Vuppala",
      "Prasanta Ghosh",
      "Chiranjeevi Yarra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09284"
  },
  {
    "id": "arXiv:2212.09285",
    "title": "Localizability of the approximation method",
    "abstract": "We use the approximation method of Razborov to analyze the locality barrier\nwhich arose from the investigation of the hardness magnification approach to\ncomplexity lower bounds. Adapting a limitation of the approximation method\nobtained by Razborov, we show that in many cases it is not possible to combine\nthe approximation method with typical (localizable) hardness magnification\ntheorems to derive strong circuit lower bounds. In particular, one cannot use\nthe approximation method to derive an extremely strong constant-depth circuit\nlower bound and then magnify it to an $NC^1$ lower bound for an explicit\nfunction.\nTo prove this we show that lower bounds obtained by the approximation method\nare in many cases localizable in the sense that they imply lower bounds for\ncircuits which are allowed to use arbitrarily powerful oracles with small\nfan-in.",
    "descriptor": "",
    "authors": [
      "Jan Pich"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2212.09285"
  },
  {
    "id": "arXiv:2212.09289",
    "title": "Detecting Features Concerning Privacy From App Reviews",
    "abstract": "Privacy requirements not only relate to legal compliance but also influence\nuser satisfaction. Massive rapidly increasing App reviews have been proved a\nvaluable requirements knowledge repository. Existing studies on App reviews\nmining have made much effort to automatically extract various requirements\nrelated information, e.g., feature request, bug report, and user opinions.\nHowever, less attention has been paid to privacy requirements refinement based\non user reviews mining, which is beneficial for addressing users' privacy\nconcern. In this work, we aim to detect privacy related features from App\nreviews to facilitate software maintenance activities. To that end, we design a\nsemi-automatic framework to identify privacy related reviews from which App\nfeatures are extracted and mapped to those listed in App descriptions. Firstly,\nwe combine information retrieval and supervised text classification to identify\nprivacy related reviews. Then, we design a dependency parsing method to extract\nApp features from those privacy related reviews. Finally, those automatically\nextracted features are matched with those manually annotated ones in App\ndescriptions based on phrase similarity. We quantitatively evaluate the three\ncomponents of our framework on the reviews of Apps from multiple categories.\nFor privacy related reviews identification, Gradient Boosting classifier\nachieves the highest F1 score of 93.77\\% among other competitive algorithms\nincluding deep learning ones. On App feature extraction, our dependency parsing\nbased method can achieve a recall of 85.63\\%, more than 20\\% higher than the\nbaseline. For feature matching, the phrase embedding cosine similarity shows\nthe best matching result among four types of similarity methods, obtaining an\naverage accuracy of 57\\%. We finally discuss the potential applications of our\nframework in detecting feature problems that may cause privacy threats.",
    "descriptor": "",
    "authors": [
      "Jianzhang Zhang",
      "Jinping Hua",
      "Yiyang Chen",
      "Nan Niu",
      "Chuang Liu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09289"
  },
  {
    "id": "arXiv:2212.09290",
    "title": "XEngine: Optimal Tensor Rematerialization for Neural Networks in  Heterogeneous Environments",
    "abstract": "Memory efficiency is crucial in training deep learning networks on\nresource-restricted devices. During backpropagation, forward tensors are used\nto calculate gradients. Despite the option of keeping those dependencies in\nmemory until they are reused in backpropagation, some forward tensors can be\ndiscarded and recomputed later from saved tensors, so-called checkpoints. This\nallows, in particular, for resource-constrained heterogeneous environments to\nmake use of all available compute devices. Unfortunately, the definition of\nthese checkpoints is a non-trivial problem and poses a challenge to the\nprogrammer - improper or excessive recomputations negate the benefit of\ncheckpointing.\nIn this article, we present XEngine, an approach that schedules network\noperators to heterogeneous devices in low memory environments by determining\ncheckpoints and recomputations of tensors. Our approach selects suitable\nresources per timestep and operator and optimizes the end-to-end time for\nneural networks taking the memory limitation of each device into account. For\nthis, we formulate a mixed-integer quadratic program (MIQP) to schedule\noperators of deep learning networks on heterogeneous systems. We compare our\nMIQP solver XEngine against Checkmate, a mixed-integer linear programming\n(MILP) approach that solves recomputation on a single device. Our solver finds\nsolutions that are up to 22.5 % faster than the fastest Checkmate schedule in\nwhich the network is computed exclusively on a single device. We also find\nvalid schedules for networks making use of both central processing units and\ngraphics processing units if memory limitations do not allow scheduling\nexclusively to the graphics processing unit.",
    "descriptor": "",
    "authors": [
      "Manuela Schuler",
      "Richard Membarth",
      "Philipp Slusallek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09290"
  },
  {
    "id": "arXiv:2212.09292",
    "title": "ChatGPT: The End of Online Exam Integrity?",
    "abstract": "This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.",
    "descriptor": "",
    "authors": [
      "Teo Susnjak"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09292"
  },
  {
    "id": "arXiv:2212.09295",
    "title": "Unified, User and Task (UUT) Centered Artificial Intelligence for  Metaverse Edge Computing",
    "abstract": "The Metaverse can be considered the extension of the present-day web, which\nintegrates the physical and virtual worlds, delivering hyper-realistic user\nexperiences. The inception of the Metaverse brings forth many ecosystem\nservices such as content creation, social entertainment, in-world value\ntransfer, intelligent traffic, healthcare. These services are compute-intensive\nand require computation offloading onto a Metaverse edge computing server\n(MECS). Existing Metaverse edge computing approaches do not efficiently and\neffectively handle resource allocation to ensure a fluid, seamless and\nhyper-realistic Metaverse experience required for Metaverse ecosystem services.\nTherefore, we introduce a new Metaverse-compatible, Unified, User and Task\n(UUT) centered artificial intelligence (AI)- based mobile edge computing (MEC)\nparadigm, which serves as a concept upon which future AI control algorithms\ncould be built to develop a more user and task-focused MEC.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Terence Jie Chua",
      "Wenhan Yu",
      "Jun Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09295"
  },
  {
    "id": "arXiv:2212.09297",
    "title": "Transferring General Multimodal Pretrained Models to Text Recognition",
    "abstract": "This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained\nmodels to text recognition. Specifically, we recast text recognition as image\ncaptioning and directly transfer a unified vision-language pretrained model to\nthe end task. Without pretraining on large-scale annotated or synthetic text\nrecognition data, OFA-OCR outperforms the baselines and achieves\nstate-of-the-art performance in the Chinese text recognition benchmark.\nAdditionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate\nthat it can achieve competitive performance with the product-level API. The\ncode (https://github.com/OFA-Sys/OFA) and demo\n(https://modelscope.cn/studios/damo/ofa_ocr_pipeline/summary) are publicly\navailable.",
    "descriptor": "",
    "authors": [
      "Junyang Lin",
      "Xuancheng Ren",
      "Yichang Zhang",
      "Gao Liu",
      "Peng Wang",
      "An Yang",
      "Chang Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09297"
  },
  {
    "id": "arXiv:2212.09298",
    "title": "From a Bird's Eye View to See: Joint Camera and Subject Registration  without the Camera Calibration",
    "abstract": "We tackle a new problem of multi-view camera and subject registration in the\nbird's eye view (BEV) without pre-given camera calibration. This is a very\nchallenging problem since its only input is several RGB images from different\nfirst-person views (FPVs) for a multi-person scene, without the BEV image and\nthe calibration of the FPVs, while the output is a unified plane with the\nlocalization and orientation of both the subjects and cameras in a BEV. We\npropose an end-to-end framework solving this problem, whose main idea can be\ndivided into following parts: i) creating a view-transform subject detection\nmodule to transform the FPV to a virtual BEV including localization and\norientation of each pedestrian, ii) deriving a geometric transformation based\nmethod to estimate camera localization and view direction, i.e., the camera\nregistration in a unified BEV, iii) making use of spatial and appearance\ninformation to aggregate the subjects into the unified BEV. We collect a new\nlarge-scale synthetic dataset with rich annotations for evaluation. The\nexperimental results show the remarkable effectiveness of our proposed method.",
    "descriptor": "",
    "authors": [
      "Zekun Qian",
      "Ruize Han",
      "Wei Feng",
      "Feifan Wang",
      "Song Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09298"
  },
  {
    "id": "arXiv:2212.09301",
    "title": "A modified splitting method for the cubic nonlinear Schr\u00f6dinger  equation",
    "abstract": "As a classical time-stepping method, it is well-known that the Strang\nsplitting method reaches the first-order accuracy by losing two spatial\nderivatives. In this paper, we propose a modified splitting method for the 1D\ncubic nonlinear Schr\\\"odinger equation: \\begin{align*}\nu^{n+1}=\\mathrm{e}^{i\\frac\\tau2\\partial_x^2}{\\mathcal N}_\\tau\n\\left[\\mathrm{e}^{i\\frac\\tau2\\partial_x^2}\\big(\\Pi_\\tau +\\mathrm{e}^{-2\\pi\ni\\lambda M_0\\tau}\\Pi^\\tau \\big)u^n\\right], \\end{align*} with ${\\mathcal\nN}_t(\\phi)=\\mathrm{e}^{-i\\lambda t|\\Pi_\\tau\\phi|^2}\\phi,$ and $M_0$ is the mass\nof the initial data. Suitably choosing the filters $\\Pi_\\tau$ and $\\Pi^\\tau$,\nit is shown rigorously that it reaches the first-order accuracy by only losing\n$\\frac32$-spatial derivatives. Moreover, if $\\gamma\\in (0,1)$, the new method\npresents the convergence rate of $\\tau^{\\frac{4\\gamma}{4+\\gamma}}$ in\n$L^2$-norm for the $H^\\gamma$-data; if $\\gamma\\in [1,2]$, it presents the\nconvergence rate of $\\tau^{\\frac25(1+\\gamma)-}$ in $L^2$-norm for the\n$H^\\gamma$-data. %In particular, the regularity requirement of the initial data\nfor the first-order convergence in $L^2$-norm is only $H^{\\frac32+}$. These\nresults are better than the expected ones for the standard (filtered) Strang\nsplitting methods. Moreover, the mass is conserved: $$\\frac1{2\\pi}\\int_{\\mathbb\nT} |u^n(x)|^2\\,d x\\equiv M_0, \\quad n=0,1,\\ldots, L . $$ The key idea is based\non the observation that the low frequency and high frequency components of\nsolutions are almost separated (up to some smooth components). Then the\nalgorithm is constructed by tracking the solution behavior at the low and high\nfrequency components separately.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Yifei Wu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2212.09301"
  },
  {
    "id": "arXiv:2212.09303",
    "title": "Finite Blocklength Performance Bound for the DNA Storage Channel",
    "abstract": "We present a finite blocklength performance bound for a DNA storage channel\nwith insertions, deletions, and substitutions. The considered bound -- the\ndependency testing (DT) bound, introduced by Polyanskiy et al. in 2010 --\nprovides an upper bound on the achievable frame error probability and can be\nused to benchmark coding schemes in the practical short-to-medium blocklength\nregime. In particular, we consider a concatenated coding scheme where an inner\nsynchronization code deals with insertions and deletions and the outer code\ncorrects remaining (mostly substitution) errors. The bound depends on the inner\nsynchronization code. Thus, it allows to guide its choice. We then consider\nlow-density parity-check codes for the outer code, which we optimize based on\nextrinsic information transfer charts. Our optimized coding schemes achieve a\nnormalized rate of $88\\%$ to $96\\%$ with respect to the DT bound for code\nlengths up to $2000$ DNA symbols for a frame error probability of $10^{-3}$ and\ncode rate 1/2.",
    "descriptor": "",
    "authors": [
      "Issam Maarouf",
      "Gianluigi Liva",
      "Eirik Rosnes",
      "Alexandre Graell i Amat"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09303"
  },
  {
    "id": "arXiv:2212.09304",
    "title": "Enabling Temporal-Spectral Decoding in Pre-movement Detection",
    "abstract": "Non-invasive brain-computer interfaces help the subjects to control external\ndevices by brain intentions. The multi-class classification of upper limb\nmovements can provide external devices with more control commands. The onsets\nof the upper limb movements are located by the external limb trajectory to\neliminate the delay and bias among the trials. However, the trajectories are\nnot recorded due to the limitation of experiments. The delay cannot be avoided\nin the analysis of signals. The delay negatively influences the classification\nperformance, which limits the further application of upper limb movements in\nthe brain-computer interface. This work focuses on multi-channel brain signals\nanalysis in the temporal-frequency approach. It proposes the two-stage-training\ntemporal-spectral neural network (TTSNet) to decode patterns from brain\nsignals. The TTSNet first divides the signals into various filter banks. In\neach filter bank, task-related component analysis is used to reduce the\ndimension and reject the noise of the brain. A convolutional neural network\n(CNN) is then used to optimize the temporal characteristic of signals and\nextract class-related features. Finally, these class-related features from all\nfilter banks are fused by concatenation and classified by the fully connected\nlayer of the CNN. The proposed method is evaluated in two public datasets. The\nresults show that TTSNet has an improved accuracy of 0.7456$\\pm$0.1205 compared\nto the EEGNet of 0.6506$\\pm$0.1275 ($p<0.05$) and FBTRCA of 0.6787$\\pm$0.1260\n($p<0.1$) in the movement detection task, which classifies the movement state\nand the resting state. The proposed method is expected to help detect limb\nmovements and assist in the rehabilitation of stroke patients.",
    "descriptor": "",
    "authors": [
      "Hao Jia",
      "Feng Duan",
      "Yu Zhang",
      "Zhe Sun",
      "Jordi Sole-Casals"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2212.09304"
  },
  {
    "id": "arXiv:2212.09305",
    "title": "SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation",
    "abstract": "Is it possible to leverage large scale raw and raw parallel corpora to build\na general learned metric? Existing learned metrics have gaps to human\njudgements, are model-dependent or are limited to the domains or tasks where\nhuman ratings are available. In this paper, we propose SEScore2, a model-based\nmetric pretrained over million-scale synthetic dataset constructed by our novel\nretrieval augmented data synthesis pipeline. SEScore2 achieves high correlation\nto human judgements without any human rating supervisions. Importantly, our\nunsupervised SEScore2 can outperform supervised metrics, which are trained on\nthe News human ratings, at the TED domain. We evaluate SEScore2 over four text\ngeneration tasks across three languages. SEScore2 outperforms all prior\nunsupervised evaluation metrics in machine translation, speech translation,\ndata-to-text and dialogue generation, with average Kendall improvements 0.158.\nSEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue\ngeneration and overall correlation.",
    "descriptor": "",
    "authors": [
      "Wenda Xu",
      "Xian Qian",
      "Mingxuan Wang",
      "Lei Li",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09305"
  },
  {
    "id": "arXiv:2212.09306",
    "title": "E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text",
    "abstract": "Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.",
    "descriptor": "\nComments: 5 pages, 3 figures, submitted to NLLP workshop in EMNLP 2022\n",
    "authors": [
      "Ting Wai Terence Au",
      "Ingemar J. Cox",
      "Vasileios Lampos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09306"
  },
  {
    "id": "arXiv:2212.09308",
    "title": "Diffusing Surrogate Dreams of Video Scenes to Predict Video Memorability",
    "abstract": "As part of the MediaEval 2022 Predicting Video Memorability task we explore\nthe relationship between visual memorability, the visual representation that\ncharacterises it, and the underlying concept portrayed by that visual\nrepresentation. We achieve state-of-the-art memorability prediction performance\nwith a model trained and tested exclusively on surrogate dream images,\nelevating concepts to the status of a cornerstone memorability feature, and\nfinding strong evidence to suggest that the intrinsic memorability of visual\ncontent can be distilled to its underlying concept or meaning irrespective of\nits specific visual representational.",
    "descriptor": "\nComments: 5 pages, 3 figures, 1 table, MediaEval-22: Multimedia Evaluation Workshop, 13-15 January 2023, Bergen, Norway and Online\n",
    "authors": [
      "Lorin Sweeney",
      "Graham Healy",
      "Alan F. Smeaton"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09308"
  },
  {
    "id": "arXiv:2212.09309",
    "title": "Levelwise construction of a single cylindrical algebraic cell",
    "abstract": "Satisfiability Modulo Theories (SMT) solvers check the satisfiability of\nquantifier-free first-order logic formulas. We consider the theory of\nnon-linear real arithmetic where the formulae are logical combinations of\npolynomial constraints. Here a commonly used tool is the Cylindrical Algebraic\nDecomposition (CAD) to decompose real space into cells where the constraints\nare truth-invariant through the use of projection polynomials.\nAn improved approach is to repackage the CAD theory into a search-based\nalgorithm: one that guesses sample points to satisfy the formula, and\ngeneralizes guesses that conflict constraints to cylindrical cells around\nsamples which are avoided in the continuing search. Such an approach can lead\nto a satisfying assignment more quickly, or conclude unsatisfiability with\nfewer cells. A notable example of this approach is Jovanovi\\'c and de Moura's\nNLSAT algorithm. Since these cells are produced locally to a sample we might\nneed fewer projection polynomials than the traditional CAD projection. The\noriginal NLSAT algorithm reduced the set a little; while Brown's single cell\nconstruction reduced it much further still. However, the shape and size of the\ncell produced depends on the order in which the polynomials are considered.\nThis paper proposes a method to construct such cells levelwise, i.e. built\nlevel-by-level according to a variable ordering. We still use a reduced number\nof projection polynomials, but can now consider a variety of different\nreductions and use heuristics to select the projection polynomials in order to\noptimise the shape of the cell under construction. We formulate all the\nnecessary theory as a proof system: while not a common presentation for work in\nthis field, it allows an elegant decoupling of heuristics from the algorithm\nand its proof of correctness.",
    "descriptor": "",
    "authors": [
      "Jasper Nalbach",
      "Erika \u00c1brah\u00e1m",
      "Philippe Specht",
      "Christopher W. Brown",
      "James H. Davenport",
      "Matthew England"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2212.09309"
  },
  {
    "id": "arXiv:2212.09313",
    "title": "When elephants nodded and dolls spoke: Bringing together robotics and  storytelling for environmental literacy",
    "abstract": "Inculcating principles of environmental stewardship among the children and\nyouth is needed urgently today for creating a sustainable future. This paper\npresents a model for promoting environment literacy in India using story\ntelling based workshops while focusing on STEM education including\ncomputational thinking, robotics and maker skills. During the workshop,\nparticipants build a robotic diorama with digital animations and animatronics\nto tell their story. Our initial observations from pilot studies conducted in\n2019 in six rural and semi-urban schools in India showed us that the children\nwere deeply engaged and enthusiastic throughout the workshop making the entire\nlearning experience a very meaningful and joyful one for all.",
    "descriptor": "\nComments: Published in this https URL\n",
    "authors": [
      "Mukil M.V.",
      "Gayathri Manikutty",
      "Divya Vijayan",
      "Aparna Rangudu",
      "Bhavani Rao R"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2212.09313"
  },
  {
    "id": "arXiv:2212.09314",
    "title": "Bounds on Mixed Codes with Finite Alphabets",
    "abstract": "Mixed codes, which are error-correcting codes in the Cartesian product of\ndifferent-sized spaces, model degrading storage systems well. While such codes\nhave previously been studied for their algebraic properties (e.g., existence of\nperfect codes) or in the case of unbounded alphabet sizes, we focus on the case\nof finite alphabets, and generalize the Gilbert-Varshamov, sphere-packing,\nElias-Bassalygo, and first linear programming bounds to that setting. In the\nlatter case, our proof is also the first for the non-symmetric mono-alphabetic\n$q$-ary case using Navon and Samorodnitsky's Fourier-analytic approach.",
    "descriptor": "",
    "authors": [
      "Yonatan Yehezkeally",
      "Haider Al Kim",
      "Sven Puchinger",
      "Antonia Wachter-Zeh"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09314"
  },
  {
    "id": "arXiv:2212.09315",
    "title": "Real-Time Rendering of Arbitrary Surface Geometries using Learnt  Transfer",
    "abstract": "Precomputed Radiance Transfer (PRT) is widely used for real-time\nphotorealistic effects. PRT disentangles the rendering equation into transfer\nand lighting, enabling their precomputation. Transfer accounts for the\ncosine-weighted visibility of points in the scene while lighting for emitted\nradiance from the environment. Prior art stored precomputed transfer in a\ntabulated manner, either in vertex or texture space. These values are fetched\nwith interpolation at each point for shading. Vertex space methods require\ndensely tessellated mesh vertices for high quality images. Texture space\nmethods require non-overlapping and area-preserving UV mapping to be available.\nThey also require a high-resolution texture to avoid rendering artifacts. In\nthis paper, we propose a compact transfer representation that is learnt\ndirectly on scene geometry points. Specifically, we train a small multi-layer\nperceptron (MLP) to predict the transfer at sampled surface points. Our\napproach is most beneficial where inherent mesh storage structure and natural\nUV mapping are not available, such as Implicit Surfaces as it learns the\ntransfer values directly on the surface. We demonstrate real-time,\nphotorealistic renderings of diffuse and glossy materials on SDF geometries\nwith PRT using our approach.",
    "descriptor": "\nComments: Accepted at ICVGIP 2022\n",
    "authors": [
      "Sirikonda Dhawal",
      "Aakash KT",
      "P.J. Narayanan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2212.09315"
  },
  {
    "id": "arXiv:2212.09317",
    "title": "Synthetic Data Augmentation Using GAN For Improved Automated Visual  Inspection",
    "abstract": "Quality control is a crucial activity performed by manufacturing companies to\nensure their products conform to the requirements and specifications. The\nintroduction of artificial intelligence models enables to automate the visual\nquality inspection, speeding up the inspection process and ensuring all\nproducts are evaluated under the same criteria. In this research, we compare\nsupervised and unsupervised defect detection techniques and explore data\naugmentation techniques to mitigate the data imbalance in the context of\nautomated visual inspection. Furthermore, we use Generative Adversarial\nNetworks for data augmentation to enhance the classifiers' discriminative\nperformance. Our results show that state-of-the-art unsupervised defect\ndetection does not match the performance of supervised models but can be used\nto reduce the labeling workload by more than 50%. Furthermore, the best\nclassification performance was achieved considering GAN-based data generation\nwith AUC ROC scores equal to or higher than 0,9898, even when increasing the\ndataset imbalance by leaving only 25\\% of the images denoting defective\nproducts. We performed the research with real-world data provided by Philips\nConsumer Lifestyle BV.",
    "descriptor": "",
    "authors": [
      "Jo\u017ee M. Ro\u017eanec",
      "Patrik Zajec",
      "Spyros Theodoropoulos",
      "Erik Koehorst",
      "Bla\u017e Fortuna",
      "Dunja Mladeni\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09317"
  },
  {
    "id": "arXiv:2212.09321",
    "title": "Learning from Training Dynamics: Identifying Mislabeled Data Beyond  Manually Designed Features",
    "abstract": "While mislabeled or ambiguously-labeled samples in the training set could\nnegatively affect the performance of deep models, diagnosing the dataset and\nidentifying mislabeled samples helps to improve the generalization power.\nTraining dynamics, i.e., the traces left by iterations of optimization\nalgorithms, have recently been proved to be effective to localize mislabeled\nsamples with hand-crafted features. In this paper, beyond manually designed\nfeatures, we introduce a novel learning-based solution, leveraging a noise\ndetector, instanced by an LSTM network, which learns to predict whether a\nsample was mislabeled using the raw training dynamics as input. Specifically,\nthe proposed method trains the noise detector in a supervised manner using the\ndataset with synthesized label noises and can adapt to various datasets (either\nnaturally or synthesized label-noised) without retraining. We conduct extensive\nexperiments to evaluate the proposed method. We train the noise detector based\non the synthesized label-noised CIFAR dataset and test such noise detector on\nTiny ImageNet, CUB-200, Caltech-256, WebVision and Clothing1M. Results show\nthat the proposed method precisely detects mislabeled samples on various\ndatasets without further adaptation, and outperforms state-of-the-art methods.\nBesides, more experiments demonstrate that the mislabel identification can\nguide a label correction, namely data debugging, providing orthogonal\nimprovements of algorithm-centric state-of-the-art techniques from the data\naspect.",
    "descriptor": "\nComments: AAAI23 accepted Conference Paper\n",
    "authors": [
      "Qingrui Jia",
      "Xuhong Li",
      "Lei Yu",
      "Jiang Bian",
      "Penghao Zhao",
      "Shupeng Li",
      "Haoyi Xiong",
      "Dejing Dou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09321"
  },
  {
    "id": "arXiv:2212.09322",
    "title": "A singularly perturbed convection-diffusion parabolic problem with  incompatible boundary/initial data",
    "abstract": "A singularly perturbed parabolic problem of convection-diffusion type with\nincompatible inflow boundary and initial conditions is examined. In the case of\nconstant coefficients, a set of singular functions are identified which match\ncertain incompatibilities in the data and also satisfy the associated\nhomogenous differential equation. When the convective coefficient only depends\non the time variable and the initial/boundary data is discontinuous, then a\nmixed analytical/numerical approach is taken. In the case of variable\ncoefficients and the zero level of compatibility being satisfied (i.e.\ncontinuous boundary/initial data), a numerical method is constructed whose\norder of convergence is shown to depend on the next level of compatibility\nbeing satisfied by the data. Numerical results are presented to support the\ntheoretical error bounds established for both of the approaches examined in the\npaper.",
    "descriptor": "\nComments: 29 pages, 4 figures\n",
    "authors": [
      "Jose Luis Gracia",
      "Eugene O'Riordan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09322"
  },
  {
    "id": "arXiv:2212.09324",
    "title": "Dealing with observability in interaction-based Offline Runtime  Verification of Distributed Systems",
    "abstract": "Interactions are formal models describing asynchronous communications within\na Distributed System (DS). They can be drawn in the fashion of sequence\ndiagrams and executed thanks to an operational semantics akin to that of\nprocess algebras. Executions of DS can be characterized by tuples of local\ntraces (one per subsystem) called multi-traces. For a given execution, those\nlocal traces can be collected via monitoring and the resulting multi-trace can\nbe analysed using offline Runtime Verification (RV). To that end, interactions\nmay serve as formal references. In practice, however, not all subsystems may be\nobserved and, without synchronising the end of monitoring on different\nsubsystems, some events may not be observed, e.g. the reception of a message\nmay be observed but not the corresponding emission. So as to be able to\nconsider all such cases of partial observation, we propose an offline RV\nalgorithm which uses removal operations to restrict the reference interaction\non-the-fly, disregarding the parts concerning no longer observed subsystems. We\nprove the correctness of the algorithm and assess the performance of an\nimplementation.",
    "descriptor": "\nComments: 37 pages (21p article, 2p references, 14p appendices)\n",
    "authors": [
      "Erwan Mahe",
      "Boutheina Bannour",
      "Christophe Gaston",
      "Arnault Lapitre",
      "Pascale Le Gall"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2212.09324"
  },
  {
    "id": "arXiv:2212.09329",
    "title": "SrTR: Self-reasoning Transformer with Visual-linguistic Knowledge for  Scene Graph Generation",
    "abstract": "Objects in a scene are not always related. The execution efficiency of the\none-stage scene graph generation approaches are quite high, which infer the\neffective relation between entity pairs using sparse proposal sets and a few\nqueries. However, they only focus on the relation between subject and object in\ntriplet set subject entity, predicate entity, object entity, ignoring the\nrelation between subject and predicate or predicate and object, and the model\nlacks self-reasoning ability. In addition, linguistic modality has been\nneglected in the one-stage method. It is necessary to mine linguistic modality\nknowledge to improve model reasoning ability. To address the above-mentioned\nshortcomings, a Self-reasoning Transformer with Visual-linguistic Knowledge\n(SrTR) is proposed to add flexible self-reasoning ability to the model. An\nencoder-decoder architecture is adopted in SrTR, and a self-reasoning decoder\nis developed to complete three inferences of the triplet set, s+o-p, s+p-o and\np+o-s. Inspired by the large-scale pre-training image-text foundation models,\nvisual-linguistic prior knowledge is introduced and a visual-linguistic\nalignment strategy is designed to project visual representations into semantic\nspaces with prior knowledge to aid relational reasoning. Experiments on the\nVisual Genome dataset demonstrate the superiority and fast inference ability of\nthe proposed method.",
    "descriptor": "",
    "authors": [
      "Yuxiang Zhang",
      "Zhenbo Liu",
      "Shuai Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09329"
  },
  {
    "id": "arXiv:2212.09330",
    "title": "StyleTRF: Stylizing Tensorial Radiance Fields",
    "abstract": "Stylized view generation of scenes captured casually using a camera has\nreceived much attention recently. The geometry and appearance of the scene are\ntypically captured as neural point sets or neural radiance fields in the\nprevious work. An image stylization method is used to stylize the captured\nappearance by training its network jointly or iteratively with the structure\ncapture network. The state-of-the-art SNeRF method trains the NeRF and\nstylization network in an alternating manner. These methods have high training\ntime and require joint optimization. In this work, we present StyleTRF, a\ncompact, quick-to-optimize strategy for stylized view generation using TensoRF.\nThe appearance part is fine-tuned using sparse stylized priors of a few views\nrendered using the TensoRF representation for a few iterations. Our method thus\neffectively decouples style-adaption from view capture and is much faster than\nthe previous methods. We show state-of-the-art results on several scenes used\nfor this purpose.",
    "descriptor": "\nComments: Accepted at ICVGIP-2022\n",
    "authors": [
      "Rahul Goel",
      "Sirikonda Dhawal",
      "Saurabh Saini",
      "P. J. Narayanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09330"
  },
  {
    "id": "arXiv:2212.09335",
    "title": "Distilling Vision-Language Pre-training to Collaborate with  Weakly-Supervised Temporal Action Localization",
    "abstract": "Weakly-supervised temporal action localization (WTAL) learns to detect and\nclassify action instances with only category labels. Most methods widely adopt\nthe off-the-shelf Classification-Based Pre-training (CBP) to generate video\nfeatures for action localization. However, the different optimization\nobjectives between classification and localization, make temporally localized\nresults suffer from the serious incomplete issue. To tackle this issue without\nadditional annotations, this paper considers to distill free action knowledge\nfrom Vision-Language Pre-training (VLP), since we surprisingly observe that the\nlocalization results of vanilla VLP have an over-complete issue, which is just\ncomplementary to the CBP results. To fuse such complementarity, we propose a\nnovel distillation-collaboration framework with two branches acting as CBP and\nVLP respectively. The framework is optimized through a dual-branch alternate\ntraining strategy. Specifically, during the B step, we distill the confident\nbackground pseudo-labels from the CBP branch; while during the F step, the\nconfident foreground pseudo-labels are distilled from the VLP branch. And as a\nresult, the dual-branch complementarity is effectively fused to promote a\nstrong alliance. Extensive experiments and ablation studies on THUMOS14 and\nActivityNet1.2 reveal that our method significantly outperforms\nstate-of-the-art methods.",
    "descriptor": "\nComments: The first two authors share the same contribution\n",
    "authors": [
      "Chen Ju",
      "Kunhao Zheng",
      "Jinxiang Liu",
      "Peisen Zhao",
      "Ya Zhang",
      "Jianlong Chang",
      "Yanfeng Wang",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09335"
  },
  {
    "id": "arXiv:2212.09342",
    "title": "Explaining Classifications to Non Experts: An XAI User Study of Post Hoc  Explanations for a Classifier When People Lack Expertise",
    "abstract": "Very few eXplainable AI (XAI) studies consider how users understanding of\nexplanations might change depending on whether they know more or less about the\nto be explained domain (i.e., whether they differ in their expertise). Yet,\nexpertise is a critical facet of most high stakes, human decision making (e.g.,\nunderstanding how a trainee doctor differs from an experienced consultant).\nAccordingly, this paper reports a novel, user study (N=96) on how peoples\nexpertise in a domain affects their understanding of post-hoc explanations by\nexample for a deep-learning, black box classifier. The results show that\npeoples understanding of explanations for correct and incorrect classifications\nchanges dramatically, on several dimensions (e.g., response times, perceptions\nof correctness and helpfulness), when the image-based domain considered is\nfamiliar (i.e., MNIST) as opposed to unfamiliar (i.e., Kannada MNIST). The\nwider implications of these new findings for XAI strategies are discussed.",
    "descriptor": "\nComments: 16 pages, 6 figures\n",
    "authors": [
      "Courtney Ford",
      "Mark T Keane"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2212.09342"
  },
  {
    "id": "arXiv:2212.09347",
    "title": "Review of security techniques for memristor computing systems",
    "abstract": "Neural network (NN) algorithms have become the dominant tool in visual object\nrecognition, natural language processing, and robotics. To enhance the\ncomputational efficiency of these algorithms, in comparison to the traditional\nvon Neuman computing architectures, researchers have been focusing on memristor\ncomputing systems. A major drawback when using memristor computing systems\ntoday is that, in the artificial intelligence (AI) era, well-trained NN models\nare intellectual property and, when loaded in the memristor computing systems,\nface theft threats, especially when running in edge devices. An adversary may\nsteal the well-trained NN models through advanced attacks such as learning\nattacks and side-channel analysis. In this paper, we review different security\ntechniques for protecting memristor computing systems. Two threat models are\ndescribed based on their assumptions regarding the adversary's capabilities: a\nblack-box (BB) model and a white-box (WB) model. We categorize the existing\nsecurity techniques into five classes in the context of these threat models:\nthwarting learning attacks (BB), thwarting side-channel attacks (BB), NN model\nencryption (WB), NN weight transformation (WB), and fingerprint embedding (WB).\nWe also present a cross-comparison of the limitations of the security\ntechniques. This paper could serve as an aid when designing secure memristor\ncomputing systems.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Minhui Zou",
      "Nan Du",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2212.09347"
  },
  {
    "id": "arXiv:2212.09352",
    "title": "Robust Anomaly Map Assisted Multiple Defect Detection with Supervised  Classification Techniques",
    "abstract": "Industry 4.0 aims to optimize the manufacturing environment by leveraging new\ntechnological advances, such as new sensing capabilities and artificial\nintelligence. The DRAEM technique has shown state-of-the-art performance for\nunsupervised classification. The ability to create anomaly maps highlighting\nareas where defects probably lie can be leveraged to provide cues to supervised\nclassification models and enhance their performance. Our research shows that\nthe best performance is achieved when training a defect detection model by\nproviding an image and the corresponding anomaly map as input. Furthermore,\nsuch a setting provides consistent performance when framing the defect\ndetection as a binary or multiclass classification problem and is not affected\nby class balancing policies. We performed the experiments on three datasets\nwith real-world data provided by Philips Consumer Lifestyle BV.",
    "descriptor": "",
    "authors": [
      "Jo\u017ee M. Ro\u017eanec",
      "Patrik Zajec",
      "Spyros Theodoropoulos",
      "Erik Koehorst",
      "Bla\u017e Fortuna",
      "Dunja Mladeni\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09352"
  },
  {
    "id": "arXiv:2212.09353",
    "title": "Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational  Machine Reading Comprehension",
    "abstract": "Open-retrieval conversational machine reading comprehension (OCMRC) simulates\nreal-life conversational interaction scenes. Machines are required to make a\ndecision of \"Yes/No/Inquire\" or generate a follow-up question when the decision\nis \"Inquire\" based on retrieved rule texts, user scenario, user question, and\ndialogue history. Recent studies explored the methods to reduce the information\ngap between decision-making and question generation and thus improve the\nperformance of generation. However, the information gap still exists because\nthese pipeline structures are still limited in decision-making, span\nextraction, and question rephrasing three stages. Decision-making and\ngeneration are reasoning separately, and the entailment reasoning utilized in\ndecision-making is hard to share through all stages. To tackle the above\nproblem, we proposed a novel one-stage end-to-end framework, called Entailment\nFused-T5 (EFT), to bridge the information gap between decision-making and\ngeneration in a global understanding manner. The extensive experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance on the OR-ShARC benchmark.",
    "descriptor": "",
    "authors": [
      "Xiao Zhang",
      "Heyan Huang",
      "Zewen Chi",
      "Xian-Ling Mao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09353"
  },
  {
    "id": "arXiv:2212.09354",
    "title": "Lessons from Robot-Assisted Disaster Response Deployments by the German  Rescue Robotics Center Task Force",
    "abstract": "Earthquakes, fire, and floods often cause structural collapses of buildings.\nThe inspection of damaged buildings poses a high risk for emergency forces or\nis even impossible, though. We present three recent selected missions of the\nRobotics Task Force of the German Rescue Robotics Center, where both ground and\naerial robots were used to explore destroyed buildings. We describe and reflect\nthe missions as well as the lessons learned that have resulted from them. In\norder to make robots from research laboratories fit for real operations,\nrealistic test environments were set up for outdoor and indoor use and tested\nin regular exercises by researchers and emergency forces. Based on this\nexperience, the robots and their control software were significantly improved.\nFurthermore, top teams of researchers and first responders were formed, each\nwith realistic assessments of the operational and practical suitability of\nrobotic systems.",
    "descriptor": "\nComments: 8 pages, 9 figures, PrePrint, Technical Report, German Rescue Robotic Center\n",
    "authors": [
      "Hartmut Surmann",
      "Ivana Kruijff-Korbayova",
      "Kevin Daun",
      "Marius Schnaubelt",
      "Oskar von Stryk",
      "Manuel Patchou",
      "Stefan Boecker",
      "Christian Wietfeld",
      "Jan Quenzel",
      "Daniel Schleich",
      "Sven Behnke",
      "Robert Grafe",
      "Nils Heidemann",
      "Dominik Slomma"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09354"
  },
  {
    "id": "arXiv:2212.09356",
    "title": "On BTI Aging Rejuvenation in Memory Address Decoders",
    "abstract": "Memory designs require timing margins to compensate for aging and fabrication\nprocess variations. With technology downscaling, aging mechanisms became more\napparent, and larger margins are considered necessary. This, in return, means a\nlarger area requirement and lower performance for the memory. Bias Temperature\nInstability (BTI) is one of the main contributors to aging, which slows down\ntransistors and ultimately causes permanent faults. In this paper, first, we\npropose a low-cost aging mitigation scheme, which can be applied to existing\nhardware to mitigate aging on memory address decoder logic. We mitigate the BTI\neffect on critical transistors by applying a rejuvenation workload to the\nmemory. Such an auxiliary workload is executed periodically to rejuvenate\ntransistors that are located on critical paths of the address decoder. Second,\nwe analyze workloads' efficiency to optimize the mitigation scheme.\nExperimental results performed with realistic benchmarks demonstrate\nseveral-times lifetime extension with a negligible execution overhead.",
    "descriptor": "\nComments: 2022 IEEE 23rd Latin American Test Symposium (LATS)\n",
    "authors": [
      "Cemil Cem Gursoy",
      "Daniel Kraak",
      "Foisal Ahmed",
      "Mottaqiallah Taouil",
      "Maksim Jenihhin",
      "Said Hamdioui"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2212.09356"
  },
  {
    "id": "arXiv:2212.09358",
    "title": "A Soft SIMD Based Energy Efficient Computing Microarchitecture",
    "abstract": "The ever-increasing size and computational complexity of today's\nmachine-learning algorithms pose an increasing strain on the underlying\nhardware. In this light, novel and dedicated architectural solutions are\nrequired to optimize energy efficiency by leveraging opportunities (such as\nintrinsic parallelism and robustness to quantization errors) exposed by\nalgorithms. We herein address this challenge by introducing a flexible\ntwo-stages computing pipeline. The pipeline can support fine-grained operand\nquantization through software-supported Single Instruction Multiple Data (SIMD)\noperations. Moreover, it can efficiently execute sequential multiplications\nover SIMD sub-words thanks to zero-skipping and Canonical Signed Digit (CSD)\ncoding. Finally, a lightweight repacking unit allows changing the bitwidth of\nsub-words at run-time dynamically. These features are implemented within a\ntight energy and area budget. Indeed, experimental results showcase that our\napproach greatly outperforms traditional hardware SIMD ones both in terms of\narea and energy requirements. In particular, our pipeline occupies up to 53.1%\nsmaller than a hardware SIMD one supporting the same sub-word widths, while\nperforming multiplication up to 88.8% more efficiently.",
    "descriptor": "\nComments: 6 pages, 10 figures\n",
    "authors": [
      "Pengbo Yu",
      "Alexandre Levisse",
      "Mohit Gupta",
      "Evenblij Timon",
      "Giovanni Ansaloni",
      "Francky Catthoor",
      "David Atienza"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2212.09358"
  },
  {
    "id": "arXiv:2212.09359",
    "title": "WACO: Word-Aligned Contrastive Learning for Speech Translation",
    "abstract": "End-to-end Speech Translation (E2E ST) aims to translate source speech into\ntarget translation without generating the intermediate transcript. However,\nexisting approaches for E2E ST degrade considerably when only limited ST data\nare available. We observe that an ST model's performance strongly correlates\nwith its embedding similarity from speech and transcript. In this paper, we\npropose Word-Aligned COntrastive learning (WACO), a novel method for few-shot\nspeech-to-text translation. Our key idea is bridging word-level representations\nfor both modalities via contrastive learning. We evaluate WACO and other\nmethods on the MuST-C dataset, a widely used ST benchmark. Our experiments\ndemonstrate that WACO outperforms the best baseline methods by 0.7-8.5 BLEU\npoints with only 1-hour parallel data. Code is available at\nhttps://anonymous.4open.science/r/WACO .",
    "descriptor": "",
    "authors": [
      "Siqi Ouyang",
      "Rong Ye",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.09359"
  },
  {
    "id": "arXiv:2212.09360",
    "title": "AI Security for Geoscience and Remote Sensing: Challenges and Future  Trends",
    "abstract": "Recent advances in artificial intelligence (AI) have significantly\nintensified research in the geoscience and remote sensing (RS) field. AI\nalgorithms, especially deep learning-based ones, have been developed and\napplied widely to RS data analysis. The successful application of AI covers\nalmost all aspects of Earth observation (EO) missions, from low-level vision\ntasks like super-resolution, denoising, and inpainting, to high-level vision\ntasks like scene classification, object detection, and semantic segmentation.\nWhile AI techniques enable researchers to observe and understand the Earth more\naccurately, the vulnerability and uncertainty of AI models deserve further\nattention, considering that many geoscience and RS tasks are highly\nsafety-critical. This paper reviews the current development of AI security in\nthe geoscience and RS field, covering the following five important aspects:\nadversarial attack, backdoor attack, federated learning, uncertainty, and\nexplainability. Moreover, the potential opportunities and trends are discussed\nto provide insights for future research. To the best of the authors' knowledge,\nthis paper is the first attempt to provide a systematic review of AI\nsecurity-related research in the geoscience and RS community. Available code\nand datasets are also listed in the paper to move this vibrant field of\nresearch forward.",
    "descriptor": "",
    "authors": [
      "Yonghao Xu",
      "Tao Bai",
      "Weikang Yu",
      "Shizhen Chang",
      "Peter M. Atkinson",
      "Pedram Ghamisi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09360"
  },
  {
    "id": "arXiv:2212.09361",
    "title": "Stochastic stability analysis of legged locomotion using unscented  transformation",
    "abstract": "In this manuscript, we present a novel method for estimating the stochastic\nstability characteristics of metastable legged systems using the unscented\ntransformation. Prior methods for stability analysis in such systems often\nrequired high-dimensional state space discretization and a broad set of initial\nconditions, resulting in significant computational complexity. Our approach\naims to alleviate this issue by reducing the dimensionality of the system and\nutilizing the unscented transformation to estimate the output distribution.\nThis technique allows us to account for multiple sources of uncertainty and\nhigh-dimensional system dynamics, while leveraging prior knowledge of noise\nstatistics to inform the selection of initial conditions for experiments. As a\nresult, our method enables the efficient assessment of controller performance\nand analysis of parametric dependencies with fewer experiments. To demonstrate\nthe efficacy of our proposed method, we apply it to the analysis of a\none-dimensional hopper and an underactuated bipedal walking simulation with a\nhybrid zero dynamics controller.",
    "descriptor": "",
    "authors": [
      "Guner Dilsad ER",
      "Mustafa Mert Ankarali"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09361"
  },
  {
    "id": "arXiv:2212.09368",
    "title": "TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in  Unstructured Outdoor Environments",
    "abstract": "Vegetation Indices based on paired images of the visible color spectrum (VIS)\nand near infrared spectrum (NIR) have been widely used in remote sensing\napplications. These vegetation indices are extended for their application in\nautonomous driving in unstructured outdoor environments. In this domain we can\ncombine traditional vegetation indices like the Normalized Difference\nVegetation Index (NDVI) and Enhanced Vegetation Index (EVI) with Convolutional\nNeural Networks (CNNs) pre-trained on available VIS datasets. By laying a focus\non learning calibrated CNN outputs, we can provide an approach to fuse known\nhand-crafted image features with CNN predictions for different domains as well.\nThe method is evaluated on a VIS+NIR dataset of semantically annotated images\nin unstructured outdoor environments. The dataset is available at\nmucar3.de/iros2022-ppniv-tas-nir.",
    "descriptor": "",
    "authors": [
      "Peter Mortimer",
      "Hans-Joachim Wuensche"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09368"
  },
  {
    "id": "arXiv:2212.09369",
    "title": "Imaging an acoustic obstacle and its excitation sources from phaseless  near-field data",
    "abstract": "This paper is concerned with reconstructing an acoustic obstacle and its\nexcitation sources from the phaseless near-field measurements. By supplementing\nsome artificial sources to the inverse scattering system, this co-inversion\nproblem can be decoupled into two inverse problems: an inverse obstacle\nscattering problem and an inverse source problem, and the corresponding\nuniqueness can be established. This novel decoupling technique requires some\nextra data but brings in several salient benefits. First, our method is fast\nand easy to implement. Second, the boundary condition of the obstacle is not\nneeded. Finally, this approximate decoupling method can be applied to other\nco-inversion problems, such as determining the medium and its excitation\nsources. Several numerical examples are presented to demonstrate the\nfeasibility and effectiveness of the proposed method.",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Deyue Zhang",
      "Yue Wu",
      "Yukun Guo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2212.09369"
  },
  {
    "id": "arXiv:2212.09376",
    "title": "Enriching Relation Extraction with OpenIE",
    "abstract": "Relation extraction (RE) is a sub-discipline of information extraction (IE)\nwhich focuses on the prediction of a relational predicate from a\nnatural-language input unit (such as a sentence, a clause, or even a short\nparagraph consisting of multiple sentences and/or clauses). Together with\nnamed-entity recognition (NER) and disambiguation (NED), RE forms the basis for\nmany advanced IE tasks such as knowledge-base (KB) population and verification.\nIn this work, we explore how recent approaches for open information extraction\n(OpenIE) may help to improve the task of RE by encoding structured information\nabout the sentences' principal units, such as subjects, objects, verbal\nphrases, and adverbials, into various forms of vectorized (and hence\nunstructured) representations of the sentences. Our main conjecture is that the\ndecomposition of long and possibly convoluted sentences into multiple smaller\nclauses via OpenIE even helps to fine-tune context-sensitive language models\nsuch as BERT (and its plethora of variants) for RE. Our experiments over two\nannotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy\nof our enriched models compared to existing RE approaches. Our best results\nreach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,\nproving the effectiveness of our approach on competitive benchmarks.",
    "descriptor": "",
    "authors": [
      "Alessandro Temperoni",
      "Maria Biryukov",
      "Martin Theobald"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09376"
  },
  {
    "id": "arXiv:2212.09377",
    "title": "Flowstorm: Open-Source Platform with Hybrid Dialogue Architecture",
    "abstract": "This paper presents a conversational AI platform called Flowstorm. Flowstorm\nis an open-source SaaS project suitable for creating, running, and analyzing\nconversational applications. Thanks to the fast and fully automated build\nprocess, the dialogues created within the platform can be executed in seconds.\nFurthermore, we propose a novel dialogue architecture that uses a combination\nof tree structures with generative models. The tree structures are also used\nfor training NLU models suitable for specific dialogue scenarios. However, the\ngenerative models are globally used across applications and extend the\nfunctionality of the dialogue trees. Moreover, the platform functionality\nbenefits from out-of-the-box components, such as the one responsible for\nextracting data from utterances or working with crawled data. Additionally, it\ncan be extended using a custom code directly in the platform. One of the\nessential features of the platform is the possibility to reuse the created\nassets across applications. There is a library of prepared assets where each\ndeveloper can contribute. All of the features are available through a\nuser-friendly visual editor.",
    "descriptor": "",
    "authors": [
      "Jan Pichl",
      "Petr Marek",
      "Jakub Konr\u00e1d",
      "Petr Lorenc",
      "Ond\u0159ej Kobza",
      "Tom\u00e1\u0161 Zaj\u00ed\u010dek",
      "Jan \u0160ediv\u00fd"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09377"
  },
  {
    "id": "arXiv:2212.09381",
    "title": "Cognitive Accident Prediction in Driving Scenes: A Multimodality  Benchmark",
    "abstract": "Traffic accident prediction in driving videos aims to provide an early\nwarning of the accident occurrence, and supports the decision making of safe\ndriving systems. Previous works usually concentrate on the spatial-temporal\ncorrelation of object-level context, while they do not fit the inherent\nlong-tailed data distribution well and are vulnerable to severe environmental\nchange. In this work, we propose a Cognitive Accident Prediction (CAP) method\nthat explicitly leverages human-inspired cognition of text description on the\nvisual observation and the driver attention to facilitate model training. In\nparticular, the text description provides a dense semantic description guidance\nfor the primary context of the traffic scene, while the driver attention\nprovides a traction to focus on the critical region closely correlating with\nsafe driving. CAP is formulated by an attentive text-to-vision shift fusion\nmodule, an attentive scene context transfer module, and the driver attention\nguided accident prediction module. We leverage the attention mechanism in these\nmodules to explore the core semantic cues for accident prediction. In order to\ntrain CAP, we extend an existing self-collected DADA-2000 dataset (with\nannotated driver attention for each frame) with further factual text\ndescriptions for the visual observations before the accidents. Besides, we\nconstruct a new large-scale benchmark consisting of 11,727 in-the-wild accident\nvideos with over 2.19 million frames (named as CAP-DATA) together with labeled\nfact-effect-reason-introspection description and temporal accident frame label.\nBased on extensive experiments, the superiority of CAP is validated compared\nwith state-of-the-art approaches. The code, CAP-DATA, and all results will be\nreleased in \\url{https://github.com/JWFanggit/LOTVS-CAP}.",
    "descriptor": "\nComments: Submitted to IEEE Transactions journal for possible publication\n",
    "authors": [
      "Jianwu Fang",
      "Lei-Lei Li",
      "Kuan Yang",
      "Zhedong Zheng",
      "Jianru Xue",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09381"
  },
  {
    "id": "arXiv:2212.09385",
    "title": "Prediction of Auto Insurance Risk Based on t-SNE Dimensionality  Reduction",
    "abstract": "Correct scoring of a driver's risk is of great significance to auto insurance\ncompanies. While the current tools used in this field have been proven in\npractice to be quite efficient and beneficial, we argue that there is still a\nlot of room for development and improvement in the auto insurance risk\nestimation process. To this end, we develop a framework based on a combination\nof a neural network together with a dimensionality reduction technique t-SNE\n(t-distributed stochastic neighbour embedding). This enables us to visually\nrepresent the complex structure of the risk as a two-dimensional surface, while\nstill preserving the properties of the local region in the features space. The\nobtained results, which are based on real insurance data, reveal a clear\ncontrast between the high and low risk policy holders, and indeed improve upon\nthe actual risk estimation performed by the insurer. Due to the visual\naccessibility of the portfolio in this approach, we argue that this framework\ncould be advantageous to the auto insurer, both as a main risk prediction tool\nand as an additional validation stage in other approaches.",
    "descriptor": "\nComments: 13 pages, 9 figures\n",
    "authors": [
      "Joseph Levitas",
      "Konstantin Yavilberg",
      "Oleg Korol",
      "Genadi Man"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Risk Management (q-fin.RM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09385"
  },
  {
    "id": "arXiv:2212.09387",
    "title": "Prompt Gating: A Parameter Efficient Tuning Method for Zero-Shot  Multi-Source Translation",
    "abstract": "Multi-source translation (MST), which typically receives multiple source\nsentences of the same meaning in different languages, has been shown superior\nto single-source translation. As the quantity of multi-source parallel data is\nlimited, taking full advantage of single-source data and limited multi-source\ndata to make models perform well when receiving as many as possible sources\nremains a challenge. Unlike previous work mostly devoted to supervised\nscenarios, we focus on zero-shot MST: expecting models to be able to process\nunseen combinations of multiple sources, e.g., unseen language combinations,\nduring inference. We propose a simple yet effective parameter efficient method,\nnamed Prompt Gating, which appends prompts to the model inputs and attaches\ngates on the extended hidden states for each encoder layer. It shows strong\nzero-shot transferability (+9.0 BLEU points maximally) and remarkable\ncompositionality (+15.6 BLEU points maximally) on MST, and also shows its\nsuperiorities over baselines on lexically constrained translation.",
    "descriptor": "",
    "authors": [
      "Xuancheng Huang",
      "Zijun Liu",
      "Peng Li",
      "Maosong Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09387"
  },
  {
    "id": "arXiv:2212.09390",
    "title": "Fast Converging Anytime Model Counting",
    "abstract": "Model counting is a fundamental problem which has been influential in many\napplications, from artificial intelligence to formal verification. Due to the\nintrinsic hardness of model counting, approximate techniques have been\ndeveloped to solve real-world instances of model counting. This paper designs a\nnew anytime approach called PartialKC for approximate model counting. The idea\nis a form of partial knowledge compilation to provide an unbiased estimate of\nthe model count which can converge to the exact count. Our empirical analysis\ndemonstrates that PartialKC achieves significant scalability and accuracy over\nprior state-of-the-art approximate counters, including satss and STS.\nInterestingly, the empirical results show that PartialKC reaches convergence\nfor many instances and therefore provides exact model counting performance\ncomparable to state-of-the-art exact counters.",
    "descriptor": "",
    "authors": [
      "Yong Lai",
      "Kuldeep S. Meel",
      "Roland H. C. Yap"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2212.09390"
  },
  {
    "id": "arXiv:2212.09394",
    "title": "Exploring a multi_stage feedback teaching mode for graduate students of  software engineering discipline based on project_driven competition",
    "abstract": "Aiming at the current problems of theory-oriented,practice-light,and lack of\ninnovation ability in the teaching of postgraduate software engineering\ncourses,a multi-stage feedback teaching mode for software engineering\npostgraduates based on competition project_driven is proposed. The model is\ndriven by the competition project,and implementing suggestions are given in\nterms of stage allocation of software engineering course tasks and ability\ncultivation,competition case design and process evaluation improvement,etc.\nThrough the implementation of this teaching mode,students enthusiasm and\ninitiative are expected to be stimulated,and the overall development of\nstudents professional skills and comprehension ability would be improved to\nmeet the demand of society for software engineering technical talents.",
    "descriptor": "\nComments: conference\n",
    "authors": [
      "Xiangdong Pei",
      "Rui Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09394"
  },
  {
    "id": "arXiv:2212.09399",
    "title": "AI Art in Architecture",
    "abstract": "Recent diffusion-based AI art platforms are able to create impressive images\nfrom simple text descriptions. This makes them powerful tools for concept\ndesign in any discipline that requires creativity in visual design tasks. This\nis also true for early stages of architectural design with multiple stages of\nideation, sketching and modelling. In this paper, we investigate how applicable\ndiffusion-based models already are to these tasks. We research the\napplicability of the platforms Midjourney, DALL-E 2 and StableDiffusion to a\nseries of common use cases in architectural design to determine which are\nalready solvable or might soon be. We also analyze how they are already being\nused by analyzing a data set of 40 million Midjourney queries with NLP methods\nto extract common usage patterns. With this insights we derived a workflow to\ninterior and exterior design that combines the strengths of the individual\nplatforms.",
    "descriptor": "",
    "authors": [
      "Joern Ploennigs",
      "Markus Berger"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09399"
  },
  {
    "id": "arXiv:2212.09400",
    "title": "An Efficient Drug-Drug Interactions Prediction Technology for  Molecularly Intelligent Manufacturing",
    "abstract": "Drug-Drug Interactions (DDIs) prediction is an essential issue in the\nmolecular field. Traditional methods of observing DDIs in medical experiments\nrequire plenty of resources and labor. In this paper, we present a\ncomputational model dubbed MedKGQA based on Graph Neural Networks to\nautomatically predict the DDIs after reading multiple medical documents in the\nform of multi-hop machine reading comprehension. We introduced a knowledge\nfusion system to obtain the complete nature of drugs and proteins and exploited\na graph reasoning system to infer the drugs and proteins contained in the\ndocuments. Our model significantly improves the performance compared to\nprevious state-of-the-art models on the QANGAROO MedHop dataset, which obtained\na 4.5% improvement in terms of DDIs prediction accuracy.",
    "descriptor": "",
    "authors": [
      "Peng Gao",
      "Feng Gao",
      "Jian-Cheng Ni",
      "Hamido Fujita"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09400"
  },
  {
    "id": "arXiv:2212.09408",
    "title": "Million-scale Object Detection with Large Vision Model",
    "abstract": "Over the past few years, developing a broad, universal, and general-purpose\ncomputer vision system has become a hot topic. A powerful universal system\nwould be capable of solving diverse vision tasks simultaneously without being\nrestricted to a specific problem or a specific data domain, which is of great\nimportance in practical real-world computer vision applications. This study\npushes the direction forward by concentrating on the million-scale multi-domain\nuniversal object detection problem. The problem is not trivial due to its\ncomplicated nature in terms of cross-dataset category label duplication, label\nconflicts, and the hierarchical taxonomy handling. Moreover, what is the\nresource-efficient way to utilize emerging large pre-trained vision models for\nmillion-scale cross-dataset object detection remains an open challenge. This\npaper tries to address these challenges by introducing our practices in label\nhandling, hierarchy-aware loss design and resource-efficient model training\nwith a pre-trained large model. Our method is ranked second in the object\ndetection track of Robust Vision Challenge 2022 (RVC 2022). We hope our\ndetailed study would serve as an alternative practice paradigm for similar\nproblems in the community. The code is available at\nhttps://github.com/linfeng93/Large-UniDet.",
    "descriptor": "",
    "authors": [
      "Feng Lin",
      "Wenze Hu",
      "Yaowei Wang",
      "Yonghong Tian",
      "Guangming Lu",
      "Fanglin Chen",
      "Yong Xu",
      "Xiaoyu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09408"
  },
  {
    "id": "arXiv:2212.09409",
    "title": "Multi-View Knowledge Distillation from Crowd Annotations for  Out-of-Domain Generalization",
    "abstract": "Selecting an effective training signal for tasks in natural language\nprocessing is difficult: collecting expert annotations is expensive, and\ncrowd-sourced annotations may not be reliable. At the same time, recent work in\nmachine learning has demonstrated that learning from soft-labels acquired from\ncrowd annotations can be effective, especially when there is distribution shift\nin the test set. However, the best method for acquiring these soft labels is\ninconsistent across tasks. This paper proposes new methods for acquiring\nsoft-labels from crowd-annotations by aggregating the distributions produced by\nexisting methods. In particular, we propose to find a distribution over classes\nby learning from multiple-views of crowd annotations via temperature scaling\nand finding the Jensen-Shannon centroid of their distributions. We demonstrate\nthat using these aggregation methods leads to best or near-best performance\nacross four NLP tasks on out-of-domain test sets, mitigating fluctuations in\nperformance when using the constituent methods on their own. Additionally,\nthese methods result in best or near-best uncertainty estimation across tasks.\nWe argue that aggregating different views of crowd-annotations as soft-labels\nis an effective way to ensure performance which is as good or better than the\nbest individual view, which is useful given the inconsistency in performance of\nthe individual methods.",
    "descriptor": "\nComments: 14 pages, 4 figures, 1 table\n",
    "authors": [
      "Dustin Wright",
      "Isabelle Augenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09409"
  },
  {
    "id": "arXiv:2212.09410",
    "title": "Less is More: Parameter-Free Text Classification with Gzip",
    "abstract": "Deep neural networks (DNNs) are often used for text classification tasks as\nthey usually achieve high levels of accuracy. However, DNNs can be\ncomputationally intensive with billions of parameters and large amounts of\nlabeled data, which can make them expensive to use, to optimize and to transfer\nto out-of-distribution (OOD) cases in practice. In this paper, we propose a\nnon-parametric alternative to DNNs that's easy, light-weight and universal in\ntext classification: a combination of a simple compressor like gzip with a\n$k$-nearest-neighbor classifier. Without any training, pre-training or\nfine-tuning, our method achieves results that are competitive with\nnon-pretrained deep learning methods on six in-distributed datasets. It even\noutperforms BERT on all five OOD datasets, including four low-resource\nlanguages. Our method also performs particularly well in few-shot settings\nwhere labeled data are too scarce for DNNs to achieve a satisfying accuracy.",
    "descriptor": "",
    "authors": [
      "Zhiying Jiang",
      "Matthew Y.R. Yang",
      "Mikhail Tsirlin",
      "Raphael Tang",
      "Jimmy Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09410"
  },
  {
    "id": "arXiv:2212.09412",
    "title": "Difformer: Empowering Diffusion Model on Embedding Space for Text  Generation",
    "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on visual\nand audio tasks, and recent works adapt them to textual data by diffusing on\nthe embedding space. But the difference between the continuous data space and\nthe embedding space raises challenges to the diffusion model, which have not\nbeen carefully explored. In this paper, we conduct systematic studies and\nanalyze the challenges threefold. Firstly, the data distribution is learnable\nfor embeddings, which may lead to the collapse of the loss function. Secondly,\nas the norm of embedding varies between popular and rare words, adding the same\nnoise scale will lead to sub-optimal results. In addition, we find that noises\nsampled from a standard Gaussian distribution may distract the diffusion\nprocess. To solve the above challenges, we propose Difformer, a denoising\ndiffusion probabilistic model based on Transformer, which consists of three\ntechniques including utilizing an anchor loss function, a layer normalization\nmodule for embeddings, and a norm factor to the Gaussian noise. All techniques\nare complementary to each other and critical to boosting the model performance\ntogether. Experiments are conducted on benchmark datasets over two seminal text\ngeneration tasks including machine translation and text summarization. The\nresults show that Difformer significantly outperforms the embedding diffusion\nbaselines, while achieving competitive results with strong autoregressive\nbaselines.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Zhujin Gao",
      "Junliang Guo",
      "Xu Tan",
      "Yongxin Zhu",
      "Fang Zhang",
      "Jiang Bian",
      "Linli Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09412"
  },
  {
    "id": "arXiv:2212.09415",
    "title": "Training Lightweight Graph Convolutional Networks with Phase-field  Models",
    "abstract": "In this paper, we design lightweight graph convolutional networks (GCNs)\nusing a particular class of regularizers, dubbed as phase-field models (PFMs).\nPFMs exhibit a bi-phase behavior using a particular ultra-local term that\nallows training both the topology and the weight parameters of GCNs as a part\nof a single \"end-to-end\" optimization problem. Our proposed solution also\nrelies on a reparametrization that pushes the mask of the topology towards\nbinary values leading to effective topology selection and high generalization\nwhile implementing any targeted pruning rate. Both masks and weights share the\nsame set of latent variables and this further enhances the generalization power\nof the resulting lightweight GCNs. Extensive experiments conducted on the\nchallenging task of skeleton-based recognition show the outperformance of PFMs\nagainst other staple regularizers as well as related lightweight design\nmethods.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.13616\n",
    "authors": [
      "Hichem Sahbi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09415"
  },
  {
    "id": "arXiv:2212.09420",
    "title": "When Neural Model Meets NL2Code: A Survey",
    "abstract": "Given a natural language that describes the user's demands, the NL2Code task\naims to generate code that addresses the demands. This is a critical but\nchallenging task that mirrors the capabilities of AI-powered programming. The\nNL2Code task is inherently versatile, diverse and complex. For example, a\ndemand can be described in different languages, in different formats, and at\ndifferent levels of granularity. This inspired us to do this survey for\nNL2Code. In this survey, we focus on how does neural network (NN) solves\nNL2Code. We first propose a comprehensive framework, which is able to cover all\nstudies in this field. Then, we in-depth parse the existing studies into this\nframework. We create an online website to record the parsing results, which\ntracks existing and recent NL2Code progress. In addition, we summarize the\ncurrent challenges of NL2Code as well as its future directions. We hope that\nthis survey can foster the evolution of this field.",
    "descriptor": "",
    "authors": [
      "Daoguang Zan",
      "Bei Chen",
      "Fengji Zhang",
      "Dianjie Lu",
      "Bingchao Wu",
      "Bei Guan",
      "Yongji Wang",
      "Jian-Guang Lou"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2212.09420"
  },
  {
    "id": "arXiv:2212.09422",
    "title": "Human in the loop: How to effectively create coherent topics by manually  labeling only a few documents per class",
    "abstract": "Few-shot methods for accurate modeling under sparse label-settings have\nimproved significantly. However, the applications of few-shot modeling in\nnatural language processing remain solely in the field of document\nclassification. With recent performance improvements, supervised few-shot\nmethods, combined with a simple topic extraction method pose a significant\nchallenge to unsupervised topic modeling methods. Our research shows that\nsupervised few-shot learning, combined with a simple topic extraction method,\ncan outperform unsupervised topic modeling techniques in terms of generating\ncoherent topics, even when only a few labeled documents per class are used.",
    "descriptor": "",
    "authors": [
      "Anton Thielmann",
      "Christoph Weisser",
      "Benjamin S\u00e4fken"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09422"
  },
  {
    "id": "arXiv:2212.09423",
    "title": "Trial-Based Dominance Enables Non-Parametric Tests to Compare both the  Speed and Accuracy of Stochastic Optimizers",
    "abstract": "Non-parametric tests can determine the better of two stochastic optimization\nalgorithms when benchmarking results are ordinal, like the final fitness values\nof multiple trials. For many benchmarks, however, a trial can also terminate\nonce it reaches a pre-specified target value. When only some trials reach the\ntarget value, two variables characterize a trial's outcome: the time it takes\nto reach the target value (or not) and its final fitness value. This paper\ndescribes a simple way to impose linear order on this two-variable trial data\nset so that traditional non-parametric methods can determine the better\nalgorithm when neither dominates. We illustrate the method with the\nMann-Whitney U-test. A simulation demonstrates that U-scores are much more\neffective than dominance when tasked with identifying the better of two\nalgorithms. We test U-scores by having them determine the winners of the CEC\n2022 Special Session and Competition on Real-Parameter Numerical Optimization.",
    "descriptor": "\nComments: 25 pages, 7 figures, 8 tables\n",
    "authors": [
      "Kenneth V. Price",
      "Abhishek Kumar",
      "Ponnuthurai N Suganthan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2212.09423"
  },
  {
    "id": "arXiv:2212.09426",
    "title": "Multistep Multiappliance Load Prediction",
    "abstract": "A well-performing prediction model is vital for a recommendation system\nsuggesting actions for energy-efficient consumer behavior. However, reliable\nand accurate predictions depend on informative features and a suitable model\ndesign to perform well and robustly across different households and appliances.\nMoreover, customers' unjustifiably high expectations of accurate predictions\nmay discourage them from using the system in the long term. In this paper, we\ndesign a three-step forecasting framework to assess predictability, engineering\nfeatures, and deep learning architectures to forecast 24 hourly load values.\nFirst, our predictability analysis provides a tool for expectation management\nto cushion customers' anticipations. Second, we design several new weather-,\ntime- and appliance-related parameters for the modeling procedure and test\ntheir contribution to the model's prediction performance. Third, we examine six\ndeep learning techniques and compare them to tree- and support vector\nregression benchmarks. We develop a robust and accurate model for the\nappliance-level load prediction based on four datasets from four different\nregions (US, UK, Austria, and Canada) with an equal set of appliances. The\nempirical results show that cyclical encoding of time features and weather\nindicators alongside a long-short term memory (LSTM) model offer the optimal\nperformance.",
    "descriptor": "",
    "authors": [
      "Alona Zharova",
      "Antonia Scherz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09426"
  },
  {
    "id": "arXiv:2212.09429",
    "title": "On the Complexity of Representation Learning in Contextual Linear  Bandits",
    "abstract": "In contextual linear bandits, the reward function is assumed to be a linear\ncombination of an unknown reward vector and a given embedding of context-arm\npairs. In practice, the embedding is often learned at the same time as the\nreward vector, thus leading to an online representation learning problem.\nExisting approaches to representation learning in contextual bandits are either\nvery generic (e.g., model-selection techniques or algorithms for learning with\narbitrary function classes) or specialized to particular structures (e.g.,\nnested features or representations with certain spectral properties). As a\nresult, the understanding of the cost of representation learning in contextual\nlinear bandit is still limited. In this paper, we take a systematic approach to\nthe problem and provide a comprehensive study through an instance-dependent\nperspective. We show that representation learning is fundamentally more complex\nthan linear bandits (i.e., learning with a given representation). In\nparticular, learning with a given set of representations is never simpler than\nlearning with the worst realizable representation in the set, while we show\ncases where it can be arbitrarily harder. We complement this result with an\nextensive discussion of how it relates to existing literature and we illustrate\npositive instances where representation learning is as complex as learning with\na fixed representation and where sub-logarithmic regret is achievable.",
    "descriptor": "",
    "authors": [
      "Andrea Tirinzoni",
      "Matteo Pirotta",
      "Alessandro Lazaric"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09429"
  },
  {
    "id": "arXiv:2212.09436",
    "title": "FTX Collapse: A Ponzi Story",
    "abstract": "FTX used to be the third-largest centralised exchange (CEX) in crypto\nmarkets, managing over \\$10B in daily trading volume before its downfall. Such\na giant, however, failed to avoid the fate of mania, panic, and crash. In this\nwork, we revisit the FTX's crash by telling it as a Ponzi story. In regards to\nwhy FTX could not sustain this Ponzi game, we extract and demonstrate the three\nfacilitators of the FTX collapse, namely, \\textit{FTT}, \\textit{leverage}, and\n\\textit{diversion}. The unfunctionality in each factor can iteratively magnify\nthe impact of damages when the panic is triggered. Rooted in the unstable\nground, FTX eventually suffered insolvency and rapidly crashed in Nov. 2022.\nThe crisis of FTX is not an isolated event; it consequently results in the\ncollapse of a chain of associated companies in the entire crypto market. Recall\nthis painful experience, we discuss possible paths for a way forward for both\nCeFi and DeFi services.",
    "descriptor": "",
    "authors": [
      "Shange Fu",
      "Qin Wang",
      "Jiangshan Yu",
      "Shiping Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2212.09436"
  },
  {
    "id": "arXiv:2212.09437",
    "title": "Machine Learning Containers are Bloated and Vulnerable",
    "abstract": "Today's software is bloated leading to significant resource wastage. This\nbloat is prevalent across the entire software stack, from the operating system,\nall the way to software backends, frontends, and web-pages. In this paper, we\nstudy how prevalent bloat is in machine learning containers. We develop MMLB, a\nframework to analyze bloat in machine learning containers, measuring the amount\nof bloat that exists on the container and package levels. Our tool quantifies\nthe sources of bloat and removes them. We integrate our tool with vulnerability\nanalysis tools to measure how bloat affects container vulnerabilities. We\nexperimentally study 15 machine learning containers from the official\nTensorflow, Pytorch, and NVIDIA container registries under different tasks,\n(i.e., training, tuning, and serving). Our findings show that machine learning\ncontainers contain bloat encompassing up to 80\\% of the container size. We find\nthat debloating machine learning containers speeds provisioning times by up to\n$3.7\\times$ and removes up to 98\\% of all vulnerabilities detected by\nvulnerability analysis tools such as Grype. Finally, we relate our results to\nthe larger discussion about technical debt in machine learning systems.",
    "descriptor": "",
    "authors": [
      "Huaifeng Zhang",
      "Fahmi Abdulqadir Ahmed",
      "Dyako Fatih",
      "Akayou Kitessa",
      "Mohannad Alhanahnah",
      "Philipp Leitner",
      "Ahmed Ali-Eldin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09437"
  },
  {
    "id": "arXiv:2212.09438",
    "title": "Leveraging Road Area Semantic Segmentation with Auxiliary Steering Task",
    "abstract": "Robustness of different pattern recognition methods is one of the key\nchallenges in autonomous driving, especially when driving in the high variety\nof road environments and weather conditions, such as gravel roads and snowfall.\nAlthough one can collect data from these adverse conditions using cars equipped\nwith sensors, it is quite tedious to annotate the data for training. In this\nwork, we address this limitation and propose a CNN-based method that can\nleverage the steering wheel angle information to improve the road area semantic\nsegmentation. As the steering wheel angle data can be easily acquired with the\nassociated images, one could improve the accuracy of road area semantic\nsegmentation by collecting data in new road environments without manual data\nannotation. We demonstrate the effectiveness of the proposed approach on two\nchallenging data sets for autonomous driving and show that when the steering\ntask is used in our segmentation model training, it leads to a 0.1-2.9% gain in\nthe road area mIoU (mean Intersection over Union) compared to the corresponding\nreference transfer learning model.",
    "descriptor": "\nComments: 11 pages, 4 figures (Supplementary material 6 pages, 3 figures). Author's accepted version of the contribution included in proceedings of the 21st International Conference on Image Analysis and Processing (ICIAP), 2022\n",
    "authors": [
      "Jyri Maanp\u00e4\u00e4",
      "Iaroslav Melekhov",
      "Josef Taher",
      "Petri Manninen",
      "Juha Hyypp\u00e4"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09438"
  },
  {
    "id": "arXiv:2212.09440",
    "title": "Synaptic Dynamics Realize First-order Adaptive Learning and Weight  Symmetry",
    "abstract": "Gradient-based first-order adaptive optimization methods such as the Adam\noptimizer are prevalent in training artificial networks, achieving the\nstate-of-the-art results. This work attempts to answer the question whether it\nis viable for biological neural systems to adopt such optimization methods. To\nthis end, we demonstrate a realization of the Adam optimizer using\nbiologically-plausible mechanisms in synapses. The proposed learning rule has\nclear biological correspondence, runs continuously in time, and achieves\nperformance to comparable Adam's. In addition, we present a new approach,\ninspired by the predisposition property of synapses observed in neuroscience,\nto circumvent the biological implausibility of the weight transport problem in\nbackpropagation (BP). With only local information and no separate training\nphases, this method establishes and maintains weight symmetry in the forward\nand backward signaling paths, and is applicable to the proposed biologically\nplausible Adam learning rule. These mechanisms may shed light on the way in\nwhich biological synaptic dynamics facilitate learning.",
    "descriptor": "",
    "authors": [
      "Yukun Yang",
      "Peng Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09440"
  },
  {
    "id": "arXiv:2212.09447",
    "title": "Improving Pre-Trained Weights Through Meta-Heuristics Fine-Tuning",
    "abstract": "Machine Learning algorithms have been extensively researched throughout the\nlast decade, leading to unprecedented advances in a broad range of\napplications, such as image classification and reconstruction, object\nrecognition, and text categorization. Nonetheless, most Machine Learning\nalgorithms are trained via derivative-based optimizers, such as the Stochastic\nGradient Descent, leading to possible local optimum entrapments and inhibiting\nthem from achieving proper performances. A bio-inspired alternative to\ntraditional optimization techniques, denoted as meta-heuristic, has received\nsignificant attention due to its simplicity and ability to avoid local optimums\nimprisonment. In this work, we propose to use meta-heuristic techniques to\nfine-tune pre-trained weights, exploring additional regions of the search\nspace, and improving their effectiveness. The experimental evaluation comprises\ntwo classification tasks (image and text) and is assessed under four literature\ndatasets. Experimental results show nature-inspired algorithms' capacity in\nexploring the neighborhood of pre-trained weights, achieving superior results\nthan their counterpart pre-trained architectures. Additionally, a thorough\nanalysis of distinct architectures, such as Multi-Layer Perceptron and\nRecurrent Neural Networks, attempts to visualize and provide more precise\ninsights into the most critical weights to be fine-tuned in the learning\nprocess.",
    "descriptor": "",
    "authors": [
      "Gustavo H. de Rosa",
      "Mateus Roder",
      "Jo\u00e3o Paulo Papa",
      "Claudio F. G. dos Santos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09447"
  },
  {
    "id": "arXiv:2212.09448",
    "title": "Smart Journey in Istanbul: A Mobile Application in Smart Cities for  Traffic Estimation by Harnessing Time Series",
    "abstract": "In recent decades, mobile applications (apps) have gained enormous\npopularity. Smart services for smart cities increasingly gain attention. The\nmain goal of the proposed research is to present a new AI-powered mobile\napplication on Istanbul's traffic congestion forecast by using traffic density\ndata. It addresses the research question by using time series approaches (LSTM,\nTransformer, and XGBoost) based on past data over the traffic load dataset\ncombined with meteorological conditions. Analysis of simulation results on\npredicted models will be discussed according to performance indicators such as\nMAPE, MAE, and RMSE. And then, it was observed that the Transformer model made\nthe most accurate traffic prediction. The developed traffic forecasting\nprototype is expected to be a starting point on future products for a mobile\napplication suitable for citizens' daily use.",
    "descriptor": "",
    "authors": [
      "Senem Tanberk",
      "Mustafa Can"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.09448"
  },
  {
    "id": "arXiv:2212.09453",
    "title": "Energy-Aware Packet Schedulers for Battery-Less LoRaWAN Nodes",
    "abstract": "The Internet of Things (IoT) enables a wide variety of applications where\nlarge sensor networks are deployed in remote areas without power grid access.\nThus, the sensor nodes often run on batteries, whose replacement and disposal\nrepresent important economical and environmental costs. To realize more\nsustainable IoT solutions, it is therefore desirable to adopt battery-less\nenergy-neutral devices that can harvest energy from renewable sources and store\nit in super-capacitors, whose environmental impact is much lower than that of\nbatteries. To achieve the energetic self-sustainability of such nodes, however,\ncommunication and computational processes must be optimized to make the best\nuse of the scarce and volatile energy available. In this work, we propose\ndifferent energy-aware packet scheduling algorithms for battery-less LoRaWAN\nnodes, and compare them in various simulated scenarios, using actual\nenergy-harvesting measurements taken from a testbed. We show that an\nenergy-aware design can significantly increase the number of transmitted\npackets, also lowering the mean time between packet transmissions, though (as\npredictable) the gain strongly depends on the harvesting capabilities of the\nnodes.",
    "descriptor": "",
    "authors": [
      "Martina Capuzzo",
      "Carmen Delgado",
      "Jeroen Famaey",
      "Andrea Zanella"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2212.09453"
  },
  {
    "id": "arXiv:2212.09458",
    "title": "Exploring Optimal Substructure for Out-of-distribution Generalization  via Feature-targeted Model Pruning",
    "abstract": "Recent studies show that even highly biased dense networks contain an\nunbiased substructure that can achieve better out-of-distribution (OOD)\ngeneralization than the original model. Existing works usually search the\ninvariant subnetwork using modular risk minimization (MRM) with out-domain\ndata. Such a paradigm may bring about two potential weaknesses: 1) Unfairness,\ndue to the insufficient observation of out-domain data during training; and 2)\nSub-optimal OOD generalization, due to the feature-untargeted model pruning on\nthe whole data distribution. In this paper, we propose a novel Spurious\nFeature-targeted model Pruning framework, dubbed SFP, to automatically explore\ninvariant substructures without referring to the above weaknesses.\nSpecifically, SFP identifies in-distribution (ID) features during training\nusing our theoretically verified task loss, upon which, SFP can perform ID\ntargeted-model pruning that removes branches with strong dependencies on ID\nfeatures. Notably, by attenuating the projections of spurious features into\nmodel space, SFP can push the model learning toward invariant features and pull\nthat out of environmental features, devising optimal OOD generalization.\nMoreover, we also conduct detailed theoretical analysis to provide the\nrationality guarantee and a proof framework for OOD structures via model\nsparsity, and for the first time, reveal how a highly biased data distribution\naffects the model's OOD generalization. Extensive experiments on various OOD\ndatasets show that SFP can significantly outperform both structure-based and\nnon-structure OOD generalization SOTAs, with accuracy improvement up to 4.72%\nand 23.35%, respectively.",
    "descriptor": "\nComments: 9 pages;2 figures\n",
    "authors": [
      "Yingchun Wang",
      "Jingcai Guo",
      "Song Guo",
      "Weizhan Zhang",
      "Jie Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09458"
  },
  {
    "id": "arXiv:2212.09460",
    "title": "Hardware Acceleration of Lane Detection Algorithm: A GPU Versus FPGA  Comparison",
    "abstract": "A Complete Computer vision system can be divided into two main categories:\ndetection and classification. The Lane detection algorithm is a part of the\ncomputer vision detection category and has been applied in autonomous driving\nand smart vehicle systems. The lane detection system is responsible for lane\nmarking in a complex road environment. At the same time, lane detection plays a\ncrucial role in the warning system for a car when departs the lane. The\nimplemented lane detection algorithm is mainly divided into two steps: edge\ndetection and line detection. In this paper, we will compare the\nstate-of-the-art implementation performance obtained with both FPGA and GPU to\nevaluate the trade-off for latency, power consumption, and utilization. Our\ncomparison emphasises the advantages and disadvantages of the two systems.",
    "descriptor": "",
    "authors": [
      "Mohamed Alshemi",
      "Sherif Saif",
      "Mohamed Taher"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.09460"
  },
  {
    "id": "arXiv:2212.09462",
    "title": "Latent Diffusion for Language Generation",
    "abstract": "Diffusion models have achieved great success in modeling continuous data\nmodalities such as images, audio, and video, but have seen limited use in\ndiscrete domains such as language. Recent attempts to adapt diffusion to\nlanguage have presented diffusion as an alternative to autoregressive language\ngeneration. We instead view diffusion as a complementary method that can\naugment the generative capabilities of existing pre-trained language models. We\ndemonstrate that continuous diffusion models can be learned in the latent space\nof a pre-trained encoder-decoder model, enabling us to sample continuous latent\nrepresentations that can be decoded into natural language with the pre-trained\ndecoder. We show that our latent diffusion models are more effective at\nsampling novel text from data distributions than a strong autoregressive\nbaseline and also enable controllable generation.",
    "descriptor": "",
    "authors": [
      "Justin Lovelace",
      "Varsha Kishore",
      "Chao Wan",
      "Eliot Shekhtman",
      "Kilian Weinberger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09462"
  },
  {
    "id": "arXiv:2212.09465",
    "title": "Improving the Generalizability of Text-Based Emotion Detection by  Leveraging Transformers with Psycholinguistic Features",
    "abstract": "In recent years, there has been increased interest in building predictive\nmodels that harness natural language processing and machine learning techniques\nto detect emotions from various text sources, including social media posts,\nmicro-blogs or news articles. Yet, deployment of such models in real-world\nsentiment and emotion applications faces challenges, in particular poor\nout-of-domain generalizability. This is likely due to domain-specific\ndifferences (e.g., topics, communicative goals, and annotation schemes) that\nmake transfer between different models of emotion recognition difficult. In\nthis work we propose approaches for text-based emotion detection that leverage\ntransformer models (BERT and RoBERTa) in combination with Bidirectional Long\nShort-Term Memory (BiLSTM) networks trained on a comprehensive set of\npsycholinguistic features. First, we evaluate the performance of our models\nwithin-domain on two benchmark datasets: GoEmotion and ISEAR. Second, we\nconduct transfer learning experiments on six datasets from the Unified Emotion\nDataset to evaluate their out-of-domain robustness. We find that the proposed\nhybrid models improve the ability to generalize to out-of-distribution data\ncompared to a standard transformer-based approach. Moreover, we observe that\nthese models perform competitively on in-domain data.",
    "descriptor": "\nComments: accepted at EMNLP2022\n",
    "authors": [
      "Sourabh Zanwar",
      "Daniel Wiechmann",
      "Yu Qiao",
      "Elma Kerz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09465"
  },
  {
    "id": "arXiv:2212.09473",
    "title": "Graph theoretical models and algorithms of portfolio compression",
    "abstract": "In portfolio compression, market participants (banks, organizations,\ncompanies, financial agents) sign contracts, creating liabilities between each\nother, which increases the systemic risk. Large, dense markets commonly can be\ncompressed by reducing obligations without lowering the net notional of each\nparticipant (an example is if liabilities make a cycle between agents, then it\nis possible to reduce each of them without any net notional changing), and our\ntarget is to eliminate as much excess notional as possible in practice (excess\nis defined as the difference between gross and net notional). A limiting factor\nthat may reduce the effectiveness of the compression can be the preferences and\npriorities of compression participants, who may individually define conditions\nfor the compression, which must be considered when designing the clearing\nprocess, otherwise, a participant may bail out, resulting in the designed\nclearing process to be impossible to execute. These markets can be\nwell-represented with edge-weighted graphs. In this paper, I examine cases when\npreferences of participants on behalf of clearing are given, e.g., in what\norder would they pay back their liabilities (a key factor can be the rate of\ninterest) and I show a clearing algorithm for these problems. On top of that,\nsince it is a common goal for the compression coordinating authority to\nmaximize the compressed amount, I also show a method to compute the maximum\nvolume conservative compression in a network. I further evaluate the\npossibility of combining the two models. Examples and program code of the model\nare also shown, also a0 pseudo-code of the clearing algorithms.",
    "descriptor": "\nComments: From Bachelors thesis\n",
    "authors": [
      "Mih\u00e1ly P\u00e9ter Hanics"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Discrete Mathematics (cs.DM)",
      "Portfolio Management (q-fin.PM)"
    ],
    "url": "https://arxiv.org/abs/2212.09473"
  },
  {
    "id": "arXiv:2212.09474",
    "title": "MICOSE4aPS: Industrially Applicable Maturity Metric to Improve  Systematic Reuse of Control Software",
    "abstract": "automated Production Systems (aPS) are highly complex, mechatronic systems\nthat usually have to operate reliably for many decades. Standardization and\nreuse of control software modules is a core prerequisite to achieve the\nrequired system quality in increasingly shorter development cycles. However,\nindustrial case studies in the field of aPS show that many aPS companies still\nstruggle with strategically reusing software. This paper proposes a\nmetric-based approach to objectively measure the maturity of industrial IEC\n61131-based control software in aPS (MICOSE4aPS) to identify potential\nweaknesses and quality issues hampering systematic reuse. Module developers in\nthe machine and plant manufacturing industry can directly benefit as the metric\ncalculation is integrated into the software engineering workflow. An in-depth\nindustrial evaluation in a top-ranked machine manufacturing company in food\npackaging and an expert evaluation with different companies confirmed the\nbenefit to efficiently manage the quality of control software.",
    "descriptor": "\nComments: 19 pages, this https URL\n",
    "authors": [
      "Birgit Vogel-Heuser",
      "Eva-Maria Neumann",
      "Juliane Fischer"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09474"
  },
  {
    "id": "arXiv:2212.09475",
    "title": "A Model Driven Approach on Object Oriented PLC Programming for  Manufacturing Systems with regard to Usability",
    "abstract": "This paper presents the modular automation for reuse in manufacturing systems\n(modAT4rMS) approach to support the model-driven engineering (MDE) of object\noriented manufacturing automation software with regard to its usability and\nsoftware modularity. With usability we refer to the aspects effectiveness,\nefficiency and user acceptance, as defined by ISO 9241-11. The modAT4rMS\nnotations are based on selected features from the Unified Modeling Language\n(UML) and the Systems Modeling language (SysML) and iteratively further\ndeveloped by a series of empirical studies with industrial practitioners as\nwell as mechatronics trainees. With modAT4rMS a MDE approach for Programmable\nLogic Controller (PLC) programming was developed with the goal to facilitate\nmodular object oriented programming of PLC software by improving the\nrepresentation of the relationships between the structure and behavior diagram\ntypes and by reducing the level of abstraction in the structure model.\nmodAT4rMS notations for PLC software structure and software behavior modeling\nare presented and illustrated with a modeling example using a modAT4rMS editor\nprototype. For the evaluation of the developed notations the results from a\nstudy with 168 participants is presented, showing the benefits of this new\napproach in comparison to the classic procedural paradigm (IEC 61131-3) and the\ndomain specific UML profile plcML in regard to programming performance and\nusability aspects. Finally the advantages and limitations of the approach are\ndiscussed and an outlook for further development is given.",
    "descriptor": "\nComments: 14 pages, this https URL\n",
    "authors": [
      "Martin Obermeier",
      "Steven Braun",
      "Birgit Vogel-Heuser"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09475"
  },
  {
    "id": "arXiv:2212.09476",
    "title": "Boosting Extra-functional Code Reusability in Cyber-physical Production  Systems: The Error Handling Case Study",
    "abstract": "Cyber-Physical Production Systems (CPPS) are long-living and mechatronic\nsystems, which include mechanics, electrics/electronics and software. The\ninterdisciplinary nature combined with challenges and trends in the context of\nIndustry 4.0 such as a high degree of customization, small lot sizes and\nevolution cause a high amount of variability. Mastering the variability of\nfunctional control software, e.g., different control variants of an actuator\ntype, is itself a challenge in developing and reusing CPPS software. This task\nbecomes even more complex when considering extra-functional software such as\noperating modes, diagnosis and error handling. These software parts have high\ninterdependencies with functional software, often involving the human-machine\ninterface (HMI) to enable the intervention of operators. This paper illustrates\nthe challenges in documenting the dependencies of these software parts\nincluding their variability using family models. A procedural and an\nobject-oriented concept for implementing error handling, which represents an\nextra-functional task with high dependencies to functional software and the\nHMI, are proposed. The suitability of both concepts to increase the software's\nreusability and, thus, its flexibility in the context of Industry 4.0 is\ndiscussed. Their comparison confirms the high potential of the object-oriented\nextension of IEC 61131-3 to handle planned reuse of extra-functional CPPS\nsoftware successfully.",
    "descriptor": "\nComments: 13 pages, this https URL\n",
    "authors": [
      "Birgit Vogel-Heuser",
      "Juliane Fischer",
      "Dieter Hess",
      "Eva-Maria Neumann",
      "Marcus Wuerr"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09476"
  },
  {
    "id": "arXiv:2212.09478",
    "title": "MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and  Video Generation",
    "abstract": "We propose the first joint audio-video generation framework that brings\nengaging watching and listening experiences simultaneously, towards\nhigh-quality realistic videos. To generate joint audio-video pairs, we propose\na novel Multi-Modal Diffusion model (i.e., MM-Diffusion), with two-coupled\ndenoising autoencoders. In contrast to existing single-modal diffusion models,\nMM-Diffusion consists of a sequential multi-modal U-Net for a joint denoising\nprocess by design. Two subnets for audio and video learn to gradually generate\naligned audio-video pairs from Gaussian noises. To ensure semantic consistency\nacross modalities, we propose a novel random-shift based attention block\nbridging over the two subnets, which enables efficient cross-modal alignment,\nand thus reinforces the audio-video fidelity for each other. Extensive\nexperiments show superior results in unconditional audio-video generation, and\nzero-shot conditional tasks (e.g., video-to-audio). In particular, we achieve\nthe best FVD and FAD on Landscape and AIST++ dancing datasets. Turing tests of\n10k votes further demonstrate dominant preferences for our model. The code and\npre-trained models can be downloaded at\nhttps://github.com/researchmm/MM-Diffusion.",
    "descriptor": "",
    "authors": [
      "Ludan Ruan",
      "Yiyang Ma",
      "Huan Yang",
      "Huiguo He",
      "Bei Liu",
      "Jianlong Fu",
      "Nicholas Jing Yuan",
      "Qin Jin",
      "Baining Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09478"
  },
  {
    "id": "arXiv:2212.09479",
    "title": "Performance assessment and exhaustive listing of 500+ nature inspired  metaheuristic algorithms",
    "abstract": "Metaheuristics are popularly used in various fields, and they have attracted\nmuch attention in the scientific and industrial communities. In recent years,\nthe number of new metaheuristic names has been continuously growing. Generally,\nthe inventors attribute the novelties of these new algorithms to inspirations\nfrom either biology, human behaviors, physics, or other phenomena. In addition,\nthese new algorithms, compared against basic versions of other metaheuristics\nusing classical benchmark problems without shift/rotation, show competitive\nperformances. In this study, we exhaustively tabulate more than 500\nmetaheuristics. To comparatively evaluate the performance of the recent\ncompetitive variants and newly proposed metaheuristics, 11 newly proposed\nmetaheuristics and 4 variants of established metaheuristics are comprehensively\ncompared on the CEC2017 benchmark suite. In addition, whether these algorithms\nhave a search bias to the center of the search space is investigated. The\nresults show that the performance of the newly proposed EBCM (effective\nbutterfly optimizer with covariance matrix adaptation) algorithm performs\ncomparably to the 4 well performing variants of the established metaheuristics\nand possesses similar properties and behaviors, such as convergence, diversity,\nexploration and exploitation trade-offs, in many aspects. The performance of\nall 15 of the algorithms is likely to deteriorate due to certain\ntransformations, while the 4 state-of-the-art metaheuristics are less affected\nby transformations such as the shifting of the global optimal point away from\nthe center of the search space. It should be noted that, except EBCM, the other\n10 new algorithms proposed mostly during 2019-2020 are inferior to the well\nperforming 2017 variants of differential evolution and evolution strategy in\nterms of convergence speed and global search ability on CEC 2017 functions.",
    "descriptor": "",
    "authors": [
      "Zhongqiang Ma",
      "Guohua Wu",
      "Ponnuthurai N. Suganthan",
      "Aijuan Song",
      "Qizhang Luo"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09479"
  },
  {
    "id": "arXiv:2212.09482",
    "title": "Rainbow Cycle Number and EFX Allocations: (Almost) Closing the Gap",
    "abstract": "Recently, some studies on the fair allocation of indivisible goods notice a\nconnection between a purely combinatorial problem called the Rainbow Cycle\nproblem and a notion of fairness known as EFX: assuming that the rainbow cycle\nnumber for parameter $d$ (i.e. $R(d)$) is $O(d^\\beta \\log^\\gamma d)$, we can\nfind a $(1-\\epsilon)$-EFX allocation with\n$O_{\\epsilon}(n^{\\frac{\\beta}{\\beta+1}}\\log^{\\frac{\\gamma}{\\beta +1}} n)$\nnumber of discarded goods [7]. The best upper bound on $R(d)$ improved in\nseries of works to $O(d^4)$ [7], $O(d^{2+o(1)})$ [2], and finally to $O(d^2)$\n[1]. Also, via a simple observation, we have $R(d) \\in \\Omega(d)$ [7]. In this\npaper, we almost close the gap between the upper bound and the lower bound and\nshow that $R(d) \\in O(d \\log d)$. This in turn proves the existence of\n$(1-\\epsilon)$-EFX allocation with $\\widetilde{O}_{\\epsilon}(\\sqrt n)$ number\nof discarded goods.",
    "descriptor": "",
    "authors": [
      "Shayan Chashm Jahan",
      "Masoud Seddighin",
      "Seyed-Mohammad Seyed-Javadi",
      "Mohammad Sharifi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2212.09482"
  },
  {
    "id": "arXiv:2212.09483",
    "title": "Adaptive Control of Client Selection and Gradient Compression for  Efficient Federated Learning",
    "abstract": "Federated learning (FL) allows multiple clients cooperatively train models\nwithout disclosing local data. However, the existing works fail to address all\nthese practical concerns in FL: limited communication resources, dynamic\nnetwork conditions and heterogeneous client properties, which slow down the\nconvergence of FL. To tackle the above challenges, we propose a\nheterogeneity-aware FL framework, called FedCG, with adaptive client selection\nand gradient compression. Specifically, the parameter server (PS) selects a\nrepresentative client subset considering statistical heterogeneity and sends\nthe global model to them. After local training, these selected clients upload\ncompressed model updates matching their capabilities to the PS for aggregation,\nwhich significantly alleviates the communication load and mitigates the\nstraggler effect. We theoretically analyze the impact of both client selection\nand gradient compression on convergence performance. Guided by the derived\nconvergence rate, we develop an iteration-based algorithm to jointly optimize\nclient selection and compression ratio decision using submodular maximization\nand linear programming. Extensive experiments on both real-world prototypes and\nsimulations show that FedCG can provide up to 5.3$\\times$ speedup compared to\nother methods.",
    "descriptor": "",
    "authors": [
      "Zhida Jiang",
      "Yang Xu",
      "Hongli Xu",
      "Zhiyuan Wang",
      "Chen Qian"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09483"
  },
  {
    "id": "arXiv:2212.09491",
    "title": "A defect-correction algorithm for quadratic matrix equations, with  applications to quasi-Toeplitz matrices",
    "abstract": "A defect correction formula for quadratic matrix equations of the kind\n$A_1X^2+A_0X+A_{-1}=0$ is presented. This formula, expressed by means of an\ninvariant subspace of a suitable pencil, allows us to introduce a modification\nof the Structure-preserving Doubling Algorithm (SDA), that enables refining an\ninitial approximation to the sought solution. This modification provides\nsubstantial advantages, in terms of convergence acceleration, in the solution\nof equations coming from stochastic models, by choosing a stochastic matrix as\nthe initial approximation. An application to solving random walks in the\nquarter plane is shown, where the coefficients $A_{-1},A_0,A_1$ are\nquasi-Toeplitz matrices of infinite size.",
    "descriptor": "",
    "authors": [
      "Dario Bini",
      "Beatrice Meini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09491"
  },
  {
    "id": "arXiv:2212.09498",
    "title": "Feature Disentanglement Learning with Switching and Aggregation for  Video-based Person Re-Identification",
    "abstract": "In video person re-identification (Re-ID), the network must consistently\nextract features of the target person from successive frames. Existing methods\ntend to focus only on how to use temporal information, which often leads to\nnetworks being fooled by similar appearances and same backgrounds. In this\npaper, we propose a Disentanglement and Switching and Aggregation Network\n(DSANet), which segregates the features representing identity and features\nbased on camera characteristics, and pays more attention to ID information. We\nalso introduce an auxiliary task that utilizes a new pair of features created\nthrough switching and aggregation to increase the network's capability for\nvarious camera scenarios. Furthermore, we devise a Target Localization Module\n(TLM) that extracts robust features against a change in the position of the\ntarget according to the frame flow and a Frame Weight Generation (FWG) that\nreflects temporal information in the final representation. Various loss\nfunctions for disentanglement learning are designed so that each component of\nthe network can cooperate while satisfactorily performing its own role.\nQuantitative and qualitative results from extensive experiments demonstrate the\nsuperiority of DSANet over state-of-the-art methods on three benchmark\ndatasets.",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Minjung Kim",
      "MyeongAh Cho",
      "Sangyoun Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09498"
  },
  {
    "id": "arXiv:2212.09500",
    "title": "Exact Error Backpropagation Through Spikes for Precise Training of  Spiking Neural Networks",
    "abstract": "Event-based simulations of Spiking Neural Networks (SNNs) are fast and\naccurate. However, they are rarely used in the context of event-based gradient\ndescent because their implementations on GPUs are difficult. Discretization\nwith the forward Euler method is instead often used with gradient descent\ntechniques but has the disadvantage of being computationally expensive.\nMoreover, the lack of precision of discretized simulations can create\nmismatches between the simulated models and analog neuromorphic hardware. In\nthis work, we propose a new exact error-backpropagation through spikes method\nfor SNNs, extending Fast \\& Deep to multiple spikes per neuron. We show that\nour method can be efficiently implemented on GPUs in a fully event-based\nmanner, making it fast to compute and precise enough for analog neuromorphic\nhardware. Compared to the original Fast \\& Deep and the current\nstate-of-the-art event-based gradient-descent algorithms, we demonstrate\nincreased performance on several benchmark datasets with both feedforward and\nconvolutional SNNs. In particular, we show that multi-spike SNNs can have\nadvantages over single-spike networks in terms of convergence, sparsity,\nclassification latency and sensitivity to the dead neuron problem.",
    "descriptor": "",
    "authors": [
      "Florian Bacho",
      "Dominique Chu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09500"
  },
  {
    "id": "arXiv:2212.09501",
    "title": "NAWQ-SR: A Hybrid-Precision NPU Engine for Efficient On-Device  Super-Resolution",
    "abstract": "In recent years, image and video delivery systems have begun integrating deep\nlearning super-resolution (SR) approaches, leveraging their unprecedented\nvisual enhancement capabilities while reducing reliance on networking\nconditions. Nevertheless, deploying these solutions on mobile devices still\nremains an active challenge as SR models are excessively demanding with respect\nto workload and memory footprint. Despite recent progress on on-device SR\nframeworks, existing systems either penalize visual quality, lead to excessive\nenergy consumption or make inefficient use of the available resources. This\nwork presents NAWQ-SR, a novel framework for the efficient on-device execution\nof SR models. Through a novel hybrid-precision quantization technique and a\nruntime neural image codec, NAWQ-SR exploits the multi-precision capabilities\nof modern mobile NPUs in order to minimize latency, while meeting\nuser-specified quality constraints. Moreover, NAWQ-SR selectively adapts the\narithmetic precision at run time to equip the SR DNN's layers with wider\nrepresentational power, improving visual quality beyond what was previously\npossible on NPUs. Altogether, NAWQ-SR achieves an average speedup of 7.9x, 3x\nand 1.91x over the state-of-the-art on-device SR systems that use heterogeneous\nprocessors (MobiSR), CPU (SplitSR) and NPU (XLSR), respectively. Furthermore,\nNAWQ-SR delivers an average of 3.2x speedup and 0.39 dB higher PSNR over\nstatus-quo INT8 NPU designs, but most importantly mitigates the negative\neffects of quantization on visual quality, setting a new state-of-the-art in\nthe attainable quality of NPU-based SR.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Stylianos I. Venieris",
      "Mario Almeida",
      "Royson Lee",
      "Nicholas D. Lane"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09501"
  },
  {
    "id": "arXiv:2212.09503",
    "title": "Measuring Annotator Agreement Generally across Complex Structured,  Multi-object, and Free-text Annotation Tasks",
    "abstract": "When annotators label data, a key metric for quality assurance is\ninter-annotator agreement (IAA): the extent to which annotators agree on their\nlabels. Though many IAA measures exist for simple categorical and ordinal\nlabeling tasks, relatively little work has considered more complex labeling\ntasks, such as structured, multi-object, and free-text annotations.\nKrippendorff's alpha, best known for use with simpler labeling tasks, does have\na distance-based formulation with broader applicability, but little work has\nstudied its efficacy and consistency across complex annotation tasks.\nWe investigate the design and evaluation of IAA measures for complex\nannotation tasks, with evaluation spanning seven diverse tasks: image bounding\nboxes, image keypoints, text sequence tagging, ranked lists, free text\ntranslations, numeric vectors, and syntax trees. We identify the difficulty of\ninterpretability and the complexity of choosing a distance function as key\nobstacles in applying Krippendorff's alpha generally across these tasks. We\npropose two novel, more interpretable measures, showing they yield more\nconsistent IAA measures across tasks and annotation distance functions.",
    "descriptor": "",
    "authors": [
      "Alexander Braylan",
      "Omar Alonso",
      "Matthew Lease"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09503"
  },
  {
    "id": "arXiv:2212.09506",
    "title": "CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly  Supervised Semantic Segmentation",
    "abstract": "Weakly supervised semantic segmentation (WSSS) with image-level labels is a\nchallenging task in computer vision. Mainstream approaches follow a multi-stage\nframework and suffer from high training costs. In this paper, we explore the\npotential of Contrastive Language-Image Pre-training models (CLIP) to localize\ndifferent categories with only image-level labels and without any further\ntraining. To efficiently generate high-quality segmentation masks from CLIP, we\npropose a novel framework called CLIP-ES for WSSS. Our framework improves all\nthree stages of WSSS with special designs for CLIP: 1) We introduce the softmax\nfunction into GradCAM and exploit the zero-shot ability of CLIP to suppress the\nconfusion caused by non-target classes and backgrounds. Meanwhile, to take full\nadvantage of CLIP, we re-explore text inputs under the WSSS setting and\ncustomize two text-driven strategies: sharpness-based prompt selection and\nsynonym fusion. 2) To simplify the stage of CAM refinement, we propose a\nreal-time class-aware attention-based affinity (CAA) module based on the\ninherent multi-head self-attention (MHSA) in CLIP-ViTs. 3) When training the\nfinal segmentation model with the masks generated by CLIP, we introduced a\nconfidence-guided loss (CGL) to mitigate noise and focus on confident regions.\nOur proposed framework dramatically reduces the cost of training for WSSS and\nshows the capability of localizing objects in CLIP. Our CLIP-ES achieves SOTA\nperformance on Pascal VOC 2012 and MS COCO 2014 while only taking 10% time of\nprevious methods for the pseudo mask generation. Code is available at\nhttps://github.com/linyq2117/CLIP-ES.",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Yuqi Lin",
      "Minghao Chen",
      "Wenxiao Wang",
      "Boxi Wu",
      "Ke Li",
      "Binbin Lin",
      "Haifeng Liu",
      "Xiaofei He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09506"
  },
  {
    "id": "arXiv:2212.09507",
    "title": "VC dimensions of group convolutional neural networks",
    "abstract": "We study the generalization capacity of group convolutional neural networks.\nWe identify precise estimates for the VC dimensions of simple sets of group\nconvolutional neural networks. In particular, we find that for infinite groups\nand appropriately chosen convolutional kernels, already two-parameter families\nof convolutional neural networks have an infinite VC dimension, despite being\ninvariant to the action of an infinite group.",
    "descriptor": "",
    "authors": [
      "Philipp Christian Petersen",
      "Anna Sepliarskaia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09507"
  },
  {
    "id": "arXiv:2212.09508",
    "title": "A note on the smallest eigenvalue of the empirical covariance of causal  Gaussian processes",
    "abstract": "We present a simple proof for bounding the smallest eigenvalue of the\nempirical covariance in a causal Gaussian process. Along the way, we establish\na one-sided tail inequality for Gaussian quadratic forms using a causal\ndecomposition. Our proof only uses elementary facts about the Gaussian\ndistribution and the union bound.",
    "descriptor": "",
    "authors": [
      "Ingvar Ziemann"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2212.09508"
  },
  {
    "id": "arXiv:2212.09512",
    "title": "Rethinking Label Smoothing on Multi-hop Question Answering",
    "abstract": "Label smoothing is a regularization technique widely used in supervised\nlearning to improve the generalization of models on various tasks, such as\nimage classification and machine translation. However, the effectiveness of\nlabel smoothing in multi-hop question answering (MHQA) has yet to be well\nstudied. In this paper, we systematically analyze the role of label smoothing\non various modules of MHQA and propose F1 smoothing, a novel label smoothing\ntechnique specifically designed for machine reading comprehension (MRC) tasks.\nWe evaluate our method on the HotpotQA dataset and demonstrate its superiority\nover several strong baselines, including models that utilize complex attention\nmechanisms. Our results suggest that label smoothing can be effective in MHQA,\nbut the choice of smoothing strategy can significantly affect performance.",
    "descriptor": "\nComments: 8 pages, 2 figures\n",
    "authors": [
      "Zhangyue Yin",
      "Yuxin Wang",
      "Yiguang Wu",
      "Hang Yan",
      "Xiannian Hu",
      "Xinyu Zhang",
      "Zhao Cao",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09512"
  },
  {
    "id": "arXiv:2212.09515",
    "title": "Using Microbenchmark Suites to Detect Application Performance Changes",
    "abstract": "Software performance changes are costly and often hard to detect pre-release.\nSimilar to software testing frameworks, either application benchmarks or\nmicrobenchmarks can be integrated into quality assurance pipelines to detect\nperformance changes before releasing a new application version. Unfortunately,\nextensive benchmarking studies usually take several hours which is problematic\nwhen examining dozens of daily code changes in detail; hence, trade-offs have\nto be made. Optimized microbenchmark suites, which only include a small subset\nof the full suite, are a potential solution for this problem, given that they\nstill reliably detect the majority of the application performance changes such\nas an increased request latency. It is, however, unclear whether\nmicrobenchmarks and application benchmarks detect the same performance problems\nand one can be a proxy for the other.\nIn this paper, we explore whether microbenchmark suites can detect the same\napplication performance changes as an application benchmark. For this, we run\nextensive benchmark experiments with both the complete and the optimized\nmicrobenchmark suites of the two time-series database systems InuxDB and\nVictoriaMetrics and compare their results to the results of corresponding\napplication benchmarks. We do this for 70 and 110 commits, respectively. Our\nresults show that it is possible to detect application performance changes\nusing an optimized microbenchmark suite if frequent false-positive alarms can\nbe tolerated.",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Cloud Computing\n",
    "authors": [
      "Martin Grambow",
      "Denis Kovalev",
      "Christoph Laaber",
      "Philipp Leitner",
      "David Bermbach"
    ],
    "subjectives": [
      "Performance (cs.PF)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09515"
  },
  {
    "id": "arXiv:2212.09517",
    "title": "Fake it, Mix it, Segment it: Bridging the Domain Gap Between Lidar  Sensors",
    "abstract": "Segmentation of lidar data is a task that provides rich, point-wise\ninformation about the environment of robots or autonomous vehicles. Currently\nbest performing neural networks for lidar segmentation are fine-tuned to\nspecific datasets. Switching the lidar sensor without retraining on a big set\nof annotated data from the new sensor creates a domain shift, which causes the\nnetwork performance to drop drastically. In this work we propose a new method\nfor lidar domain adaption, in which we use annotated panoptic lidar datasets\nand recreate the recorded scenes in the structure of a different lidar sensor.\nWe narrow the domain gap to the target data by recreating panoptic data from\none domain in another and mixing the generated data with parts of (pseudo)\nlabeled target domain data. Our method improves the nuScenes to SemanticKITTI\nunsupervised domain adaptation performance by 15.2 mean Intersection over Union\npoints (mIoU) and by 48.3 mIoU in our semi-supervised approach. We demonstrate\na similar improvement for the SemanticKITTI to nuScenes domain adaptation by\n21.8 mIoU and 51.5 mIoU, respectively. We compare our method with two state of\nthe art approaches for semantic lidar segmentation domain adaptation with a\nsignificant improvement for unsupervised and semi-supervised domain adaptation.\nFurthermore we successfully apply our proposed method to two entirely unlabeled\ndatasets of two state of the art lidar sensors Velodyne Alpha Prime and\nInnovizTwo, and train well performing semantic segmentation networks for both.",
    "descriptor": "\nComments: 10 pages, 7 figures, to be published in proceedings of \"International Conference on Pattern Recognition Applications and Methods 2023\"\n",
    "authors": [
      "Frederik Hasecke",
      "Pascal Colling",
      "Anton Kummert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09517"
  },
  {
    "id": "arXiv:2212.09518",
    "title": "FedTADBench: Federated Time-Series Anomaly Detection Benchmark",
    "abstract": "Time series anomaly detection strives to uncover potential abnormal behaviors\nand patterns from temporal data, and has fundamental significance in diverse\napplication scenarios. Constructing an effective detection model usually\nrequires adequate training data stored in a centralized manner, however, this\nrequirement sometimes could not be satisfied in realistic scenarios. As a\nprevailing approach to address the above problem, federated learning has\ndemonstrated its power to cooperate with the distributed data available while\nprotecting the privacy of data providers. However, it is still unclear that how\nexisting time series anomaly detection algorithms perform with decentralized\ndata storage and privacy protection through federated learning. To study this,\nwe conduct a federated time series anomaly detection benchmark, named\nFedTADBench, which involves five representative time series anomaly detection\nalgorithms and four popular federated learning methods. We would like to answer\nthe following questions: (1)How is the performance of time series anomaly\ndetection algorithms when meeting federated learning? (2) Which federated\nlearning method is the most appropriate one for time series anomaly detection?\n(3) How do federated time series anomaly detection approaches perform on\ndifferent partitions of data in clients? Numbers of results as well as\ncorresponding analysis are provided from extensive experiments with various\nsettings. The source code of our benchmark is publicly available at\nhttps://github.com/fanxingliu2020/FedTADBench.",
    "descriptor": "\nComments: 8 pages, 6 figures, published by IEEE HPCC 2022\n",
    "authors": [
      "Fanxing Liu",
      "Cheng Zeng",
      "Le Zhang",
      "Yingjie Zhou",
      "Qing Mu",
      "Yanru Zhang",
      "Ling Zhang",
      "Ce Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09518"
  },
  {
    "id": "arXiv:2212.09519",
    "title": "Explainable Fuzzer Evaluation",
    "abstract": "While the aim of fuzzer evaluation is to establish fuzzer performance in\ngeneral, an evaluation is always conducted on a specific benchmark. In this\npaper, we investigate the degree to which the benchmarking result depends on\nthe properties of the benchmark and propose a methodology to quantify the\nimpact of benchmark properties on the benchmarking result in relation to the\nimpact of the choice of fuzzer. We found that the measured performance and\nranking of a fuzzer substantially depends on properties of the programs and the\nseed corpora used during evaluation. For instance, if the benchmark contained\nlarger programs or seed corpora with a higher initial coverage, AFL's ranking\nwould improve while LibFuzzer's ranking would worsen. We describe our\nmethodology as explainable fuzzer evaluation because it explains why the\nspecific evaluation setup yields the observed superiority or ranking of the\nfuzzers and how it might change for different benchmarks. We envision that our\nanalysis can be used to assess the degree to which evaluation results are\noverfitted to the benchmark and to identify the specific conditions under which\ndifferent fuzzers performs better than others.",
    "descriptor": "",
    "authors": [
      "Dylan Wolff",
      "Marcel B\u00f6hme",
      "Abhik Roychoudhury"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09519"
  },
  {
    "id": "arXiv:2212.09521",
    "title": "Mechanism Design With Predictions for Obnoxious Facility Location",
    "abstract": "We study mechanism design with predictions for the obnoxious facility\nlocation problem. We present deterministic strategyproof mechanisms that\ndisplay tradeoffs between robustness and consistency on segments, squares,\ncircles and trees. All these mechanisms are actually group strategyproof, with\nthe exception of the case of squares, where manipulations from coalitions of\ntwo agents exist. We prove that these tradeoffs are optimal in the\n1-dimensional case.",
    "descriptor": "",
    "authors": [
      "Gabriel Istrate",
      "Cosmin Bonchis"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2212.09521"
  },
  {
    "id": "arXiv:2212.09522",
    "title": "MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form  Video Question Answering",
    "abstract": "To build Video Question Answering (VideoQA) systems capable of assisting\nhumans in daily activities, seeking answers from long-form videos with diverse\nand complex events is a must. Existing multi-modal VQA models achieve promising\nperformance on images or short video clips, especially with the recent success\nof large-scale multi-modal pre-training. However, when extending these methods\nto long-form videos, new challenges arise. On the one hand, using a dense video\nsampling strategy is computationally prohibitive. On the other hand, methods\nrelying on sparse sampling struggle in scenarios where multi-event and\nmulti-granularity visual reasoning are required. In this work, we introduce a\nnew model named Multi-modal Iterative Spatial-temporal Transformer (MIST) to\nbetter adapt pre-trained models for long-form VideoQA. Specifically, MIST\ndecomposes traditional dense spatial-temporal self-attention into cascaded\nsegment and region selection modules that adaptively select frames and image\nregions that are closely relevant to the question itself. Visual concepts at\ndifferent granularities are then processed efficiently through an attention\nmodule. In addition, MIST iteratively conducts selection and attention over\nmultiple layers to support reasoning over multiple events. The experimental\nresults on four VideoQA datasets, including AGQA, NExT-QA, STAR, and Env-QA,\nshow that MIST achieves state-of-the-art performance and is superior at\ncomputation efficiency and interpretability.",
    "descriptor": "",
    "authors": [
      "Difei Gao",
      "Luowei Zhou",
      "Lei Ji",
      "Linchao Zhu",
      "Yi Yang",
      "Mike Zheng Shou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09522"
  },
  {
    "id": "arXiv:2212.09523",
    "title": "Natural Language Processing in Customer Service: A Systematic Review",
    "abstract": "Artificial intelligence and natural language processing (NLP) are\nincreasingly being used in customer service to interact with users and answer\ntheir questions. The goal of this systematic review is to examine existing\nresearch on the use of NLP technology in customer service, including the\nresearch domain, applications, datasets used, and evaluation methods. The\nreview also looks at the future direction of the field and any significant\nlimitations. The review covers the time period from 2015 to 2022 and includes\npapers from five major scientific databases. Chatbots and question-answering\nsystems were found to be used in 10 main fields, with the most common use in\ngeneral, social networking, and e-commerce areas. Twitter was the second most\ncommonly used dataset, with most research also using their own original\ndatasets. Accuracy, precision, recall, and F1 were the most common evaluation\nmethods. Future work aims to improve the performance and understanding of user\nbehavior and emotions, and address limitations such as the volume, diversity,\nand quality of datasets. This review includes research on different spoken\nlanguages and models and techniques.",
    "descriptor": "",
    "authors": [
      "Malak Mashaabi",
      "Areej Alotaibi",
      "Hala Qudaih",
      "Raghad Alnashwan",
      "Hend Al-Khalifa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09523"
  },
  {
    "id": "arXiv:2212.09525",
    "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost",
    "abstract": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.",
    "descriptor": "\nComments: AAAI 2023\n",
    "authors": [
      "Yangyu Huang",
      "Xi Chen",
      "Jongyoo Kim",
      "Hao Yang",
      "Chong Li",
      "Jiaolong Yang",
      "Dong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09525"
  },
  {
    "id": "arXiv:2212.09530",
    "title": "HARP: Personalized Hand Reconstruction from a Monocular RGB Video",
    "abstract": "We present HARP (HAnd Reconstruction and Personalization), a personalized\nhand avatar creation approach that takes a short monocular RGB video of a human\nhand as input and reconstructs a faithful hand avatar exhibiting a\nhigh-fidelity appearance and geometry. In contrast to the major trend of neural\nimplicit representations, HARP models a hand with a mesh-based parametric hand\nmodel, a vertex displacement map, a normal map, and an albedo without any\nneural components. As validated by our experiments, the explicit nature of our\nrepresentation enables a truly scalable, robust, and efficient approach to hand\navatar creation. HARP is optimized via gradient descent from a short sequence\ncaptured by a hand-held mobile phone and can be directly used in AR/VR\napplications with real-time rendering capability. To enable this, we carefully\ndesign and implement a shadow-aware differentiable rendering scheme that is\nrobust to high degree articulations and self-shadowing regularly present in\nhand motion sequences, as well as challenging lighting conditions. It also\ngeneralizes to unseen poses and novel viewpoints, producing photo-realistic\nrenderings of hand animations performing highly-articulated motions.\nFurthermore, the learned HARP representation can be used for improving 3D hand\npose estimation quality in challenging viewpoints. The key advantages of HARP\nare validated by the in-depth analyses on appearance reconstruction, novel-view\nand novel pose synthesis, and 3D hand pose refinement. It is an AR/VR-ready\npersonalized hand representation that shows superior fidelity and scalability.",
    "descriptor": "",
    "authors": [
      "Korrawe Karunratanakul",
      "Sergey Prokudin",
      "Otmar Hilliges",
      "Siyu Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09530"
  },
  {
    "id": "arXiv:2212.09535",
    "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
    "abstract": "The BLOOM model is a large open-source multilingual language model capable of\nzero-shot learning, but its pretraining was limited to 46 languages. To improve\nits zero-shot performance on unseen languages, it is desirable to adapt BLOOM,\nbut previous works have only explored adapting small language models. In this\nwork, we apply existing language adaptation strategies to BLOOM and benchmark\nits zero-shot prompting performance on eight new languages. We find language\nadaptation to be effective at improving zero-shot performance in new languages.\nSurprisingly, adapter-based finetuning is more effective than continued\npretraining for large models. In addition, we discover that prompting\nperformance is not significantly affected by language specifics, such as the\nwriting system. It is primarily determined by the size of the language\nadaptation data. We also add new languages to BLOOMZ, which is a multitask\nfinetuned version of BLOOM capable of following task instructions zero-shot. We\nfind including a new language in the multitask fine-tuning mixture to be the\nmost effective method to teach BLOOMZ a new language. We conclude that with\nsufficient training data language adaptation can generalize well to diverse\nlanguages. Our code is available at\n\\url{https://github.com/bigscience-workshop/multilingual-modeling/}.",
    "descriptor": "",
    "authors": [
      "Zheng-Xin Yong",
      "Hailey Schoelkopf",
      "Niklas Muennighoff",
      "Alham Fikri Aji",
      "David Ifeoluwa Adelani",
      "Khalid Almubarak",
      "M Saiful Bari",
      "Lintang Sutawika",
      "Jungo Kasai",
      "Ahmed Baruwa",
      "Genta Indra Winata",
      "Stella Biderman",
      "Dragomir Radev",
      "Vassilina Nikoulina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09535"
  },
  {
    "id": "arXiv:2212.09541",
    "title": "Positive-incentive Noise",
    "abstract": "Noise is conventionally viewed as a severe problem in diverse fields, e.g.,\nengineering, learning systems. However, this paper aims to investigate whether\nthe conventional proposition always holds. It begins with the definition of\ntask entropy, which extends from the information entropy and measures the\ncomplexity of the task. After introducing the task entropy, the noise can be\nclassified into two kinds, Positive-incentive noise (Pi-noise or $\\pi$-noise)\nand pure noise, according to whether the noise can reduce the complexity of the\ntask. Interestingly, as shown theoretically and empirically, even the simple\nrandom noise can be the $\\pi$-noise that simplifies the task. $\\pi$-noise\noffers new explanations for some models and provides a new principle for some\nfields, such as multi-task learning, adversarial training, etc. Moreover, it\nreminds us to rethink the investigation of noises.",
    "descriptor": "",
    "authors": [
      "Xuelong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09541"
  },
  {
    "id": "arXiv:2212.09553",
    "title": "Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models",
    "abstract": "We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model\npre-trained jointly on unlabeled speech, unlabeled text and supervised data\nspanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST)\nand Machine Translation (MT), in over 100 languages. By leveraging a quantized\nrepresentation of speech as a target, Mu$^{2}$SLAM trains the speech-text\nmodels with a sequence-to-sequence masked denoising objective similar to T5 on\nthe decoder and a masked language modeling (MLM) objective on the encoder, for\nboth unlabeled speech and text, while utilizing the supervised tasks to improve\ncross-lingual and cross-modal representation alignment within the model. On\nCoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained\non public datasets, improving on xx-en translation over the previous best by\n1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR,\nour model matches the performance of an mSLAM model fine-tuned with an RNN-T\ndecoder, despite using a relatively weaker sequence-to-sequence architecture.\nOn text understanding tasks, our model improves by more than 6\\% over mSLAM on\nXNLI, getting closer to the performance of mT5 models of comparable capacity on\nXNLI and TydiQA, paving the way towards a single model for all speech and text\nunderstanding tasks.",
    "descriptor": "",
    "authors": [
      "Yong Cheng",
      "Yu Zhang",
      "Melvin Johnson",
      "Wolfgang Macherey",
      "Ankur Bapna"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.09553"
  },
  {
    "id": "arXiv:2212.09555",
    "title": "Interactive Cartoonization with Controllable Perceptual Factors",
    "abstract": "Cartoonization is a task that renders natural photos into cartoon styles.\nPrevious deep cartoonization methods only have focused on end-to-end\ntranslation, which may hinder editability. Instead, we propose a novel solution\nwith editing features of texture and color based on the cartoon creation\nprocess. To do that, we design a model architecture to have separate decoders,\ntexture and color, to decouple these attributes. In the texture decoder, we\npropose a texture controller, which enables a user to control stroke style and\nabstraction to generate diverse cartoon textures. We also introduce an HSV\ncolor augmentation to induce the networks to generate diverse and controllable\ncolor translation. To the best of our knowledge, our work is the first deep\napproach to control the cartoonization at inference while showing profound\nquality improvement over to baselines.",
    "descriptor": "",
    "authors": [
      "Namhyuk Ahn",
      "Patrick Kwon",
      "Jihye Back",
      "Kibeom Hong",
      "Seungkwon Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09555"
  },
  {
    "id": "arXiv:2212.09560",
    "title": "A modified equation analysis for immersed boundary methods based on  volume penalization: applications to linear advection-diffusion and  high-order discontinuous Galerkin schemes",
    "abstract": "The Immersed Boundary Method (IBM) is a popular numerical approach to impose\nboundary conditions without relying on body-fitted grids, thus reducing the\ncostly effort of mesh generation. To obtain enhanced accuracy, IBM can be\ncombined with high-order methods (e.g., discontinuous Galerkin). For this\ncombination to be effective, an analysis of the numerical errors is essential.\nIn this work, we apply, for the first time, a modified equation analysis to the\ncombination of IBM (based on volume penalization) and high-order methods (based\non nodal discontinuous Galerkin methods) to analyze a priori numerical errors\nand obtain practical guidelines on the selection of IBM parameters. The\nanalysis is performed on a linear advection-diffusion equation with Dirichlet\nboundary conditions. Three ways to penalize the immerse boundary are\nconsidered, the first penalizes the solution inside the IBM region (classic\napproach), whilst the second and third penalize the first and second\nderivatives of the solution. We find optimal combinations of the penalization\nparameters, including the first and second penalizing derivatives, resulting in\nminimum errors. We validate the theoretical analysis with numerical experiments\nfor one- and two-dimensional advection-diffusion equations.",
    "descriptor": "",
    "authors": [
      "Victor J. Llorente",
      "Jiaqing Kou",
      "Eusebio Valero",
      "Esteban Ferrer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2212.09560"
  },
  {
    "id": "arXiv:2212.09561",
    "title": "Large Language Models are reasoners with Self-Verification",
    "abstract": "When a large language model (LLM) performs complex reasoning by chain of\nthought (CoT), it can be highly sensitive to individual mistakes. We have had\nto train verifiers to address this issue. As we all know, after human inferring\na conclusion, they often check it by re-verifying it, which can avoid some\nmistakes. We propose a new method called self-verification that uses the\nconclusion of the CoT as a condition to build a new sample and asks the LLM to\nre-predict the original conditions which be masked. We calculate an explainable\nverification score based on the accuracy. This method can improve the accuracy\nof multiple arithmetics and logical reasoning datasets when using few-shot\nlearning. we have demonstrated that LLMs can conduct explainable\nself-verification of their own conclusions and achieve competitive reasoning\nperformance. Extensive experimentals have demonstrated that our method can help\nmultiple large language models with self-verification can avoid interference\nfrom incorrect CoT. Code is available at\n\\url{https://github.com/WENGSYX/Self-Verification}",
    "descriptor": "",
    "authors": [
      "Yixuan Weng",
      "Minjun Zhu",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09561"
  },
  {
    "id": "arXiv:2212.09562",
    "title": "High Performance Construction of RecSplit Based Minimal Perfect Hash  Functions",
    "abstract": "A minimal perfect hash function (MPHF) is a bijection from a set of objects S\nto the first |S| integers. It can be used as a building block in databases and\ndata compression. RecSplit [Esposito et al., ALENEX20] is currently the most\nspace efficient practical minimal perfect hash function. Its main building\nblocks are splittings and bijections. Using a tree-like data structure,\nRecSplit first splits the input set into small sets of constant size l and then\ncomputes a bijection on each leaf. Both splittings and bijections heavily rely\non trying multiple hash functions in a brute-force way. We greatly improve the\nconstruction time of RecSplit using two orthogonal approaches. On the one hand,\nwe explore the trade-off between (exponential time) brute force and more\ninformed (polynomial time) search heuristics. Rotation fitting hashes the\nobjects in each leaf to two sets and tries to combine them to a bijection by\ncyclically shifting one set to fill the holes in the other. ShockHash\nconstructs a small cuckoo hash table in each leaf, which is overloaded to hold\nmore objects than the asymptotic maximum. On the other hand, we harness\nparallelism on the level of bits, vectors, cores, and GPUs. In combination, the\nresulting improvements yield speedups up to 241 on a CPU and up to 2072 using a\nGPU. The original RecSplit implementation needs 19 minutes to construct an MPHF\nfor 1 Million objects with 1.56 bits per object. On the GPU, we achieve the\nsame space usage in 1.5 seconds. Given that the speedups are larger than the\nincrease in energy consumption, our implementation is more energy efficient\nthan the original implementation. As a result, our improved RecSplit\nimplementation is now the approach to perfect hashing with the fastest\nconstruction time over a wide range of space budgets. Surprisingly, this even\nholds for rather high space budgets where asymptotically faster methods are\navailable.",
    "descriptor": "",
    "authors": [
      "Dominik Bez",
      "Florian Kurpicz",
      "Hans-Peter Lehmann",
      "Peter Sanders"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2212.09562"
  },
  {
    "id": "arXiv:2212.09563",
    "title": "Source-Free Domain Adaptation for Question Answering with Masked  Self-training",
    "abstract": "Most previous unsupervised domain adaptation (UDA) methods for question\nanswering(QA) require access to source domain data while fine-tuning the model\nfor the target domain. Source domain data may, however, contain sensitive\ninformation and may be restricted. In this study, we investigate a more\nchallenging setting, source-free UDA, in which we have only the pretrained\nsource model and target domain data, without access to source domain data. We\npropose a novel self-training approach to QA models that integrates a unique\nmask module for domain adaptation. The mask is auto-adjusted to extract key\ndomain knowledge while trained on the source domain. To maintain previously\nlearned domain knowledge, certain mask weights are frozen during adaptation,\nwhile other weights are adjusted to mitigate domain shifts with pseudo-labeled\nsamples generated in the target domain. %As part of the self-training process,\nwe generate pseudo-labeled samples in the target domain based on models trained\nin the source domain. Our empirical results on four benchmark datasets suggest\nthat our approach significantly enhances the performance of pretrained QA\nmodels on the target domain, and even outperforms models that have access to\nthe source data during adaptation.",
    "descriptor": "",
    "authors": [
      "M. Yin",
      "B. Wang",
      "Y. Dong",
      "C. Ling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09563"
  },
  {
    "id": "arXiv:2212.09567",
    "title": "Answering Complex Logical Queries on Knowledge Graphs via Query Tree  Optimization",
    "abstract": "Answering complex logical queries on incomplete knowledge graphs is a\nchallenging task, and has been widely studied. Embedding-based methods require\ntraining on complex queries, and cannot generalize well to out-of-distribution\nquery structures. Recent work frames this task as an end-to-end optimization\nproblem, and it only requires a pretrained link predictor. However, due to the\nexponentially large combinatorial search space, the optimal solution can only\nbe approximated, limiting the final accuracy. In this work, we propose QTO\n(Query Tree Optimization) that can efficiently find the exact optimal solution.\nQTO finds the optimal solution by a forward-backward propagation on the\ntree-like computation graph, i.e., query tree. In particular, QTO utilizes the\nindependence encoded in the query tree to reduce the search space, where only\nlocal computations are involved during the optimization procedure. Experiments\non 3 datasets show that QTO obtains state-of-the-art performance on complex\nquery answering, outperforming previous best results by an average of 22%.\nMoreover, QTO can interpret the intermediate solutions for each of the one-hop\natoms in the query with over 90% accuracy.",
    "descriptor": "\nComments: Code is available at this https URL\n",
    "authors": [
      "Yushi Bai",
      "Xin Lv",
      "Juanzi Li",
      "Lei Hou"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2212.09567"
  },
  {
    "id": "arXiv:2212.09568",
    "title": "On the Density of Codes over Finite Chain Rings",
    "abstract": "We determine the asymptotic proportion of free modules over finite chain\nrings with good distance properties and treat the asymptotics in the code\nlength n and the residue field size q separately. We then specialize and apply\nour technique to rank metric codes and to Hamming metric codes.",
    "descriptor": "",
    "authors": [
      "Anna-Lena Horlemann",
      "Violetta Weger",
      "Nadja Willenborg"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09568"
  },
  {
    "id": "arXiv:2212.09570",
    "title": "Solving QMLTP Problems by Translation to Higher-order Logic",
    "abstract": "This paper describes an evaluation of Automated Theorem Proving (ATP) systems\non problems taken from the QMLTP library of first-order modal logic problems.\nPrincipally, the problems are translated to higher-order logic in the TPTP\nlanguages using an embedding approach, and solved using higher-order logic ATP\nsystems. Additionally, the results from native modal logic ATP systems are\nconsidered, and compared with those from the embedding approach. The\nconclusions are that (i) The embedding process is reliable and successful. (ii)\nThe choice of backend ATP system can significantly impact the performance of\nthe embedding approach. (iii) Native modal logic ATP systems outperform the\nembedding approach. (iv) The embedding approach can cope with a wider range\nmodal logics than the native modal systems considered.",
    "descriptor": "",
    "authors": [
      "Alexander Steen",
      "Geoff Sutcliffe",
      "Tobias Glei\u00dfner",
      "Christoph Benzm\u00fcller"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09570"
  },
  {
    "id": "arXiv:2212.09573",
    "title": "Privacy Adhering Machine Un-learning in NLP",
    "abstract": "Regulations introduced by General Data Protection Regulation (GDPR) in the EU\nor California Consumer Privacy Act (CCPA) in the US have included provisions on\nthe \\textit{right to be forgotten} that mandates industry applications to\nremove data related to an individual from their systems. In several real world\nindustry applications that use Machine Learning to build models on user data,\nsuch mandates require significant effort both in terms of data cleansing as\nwell as model retraining while ensuring the models do not deteriorate in\nprediction quality due to removal of data. As a result, continuous removal of\ndata and model retraining steps do not scale if these applications receive such\nrequests at a very high frequency. Recently, a few researchers proposed the\nidea of \\textit{Machine Unlearning} to tackle this challenge. Despite the\nsignificant importance of this task, the area of Machine Unlearning is\nunder-explored in Natural Language Processing (NLP) tasks. In this paper, we\nexplore the Unlearning framework on various GLUE tasks \\cite{Wang:18}, such as,\nQQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC and\nSISA-A) to perform \\textit{guaranteed} Unlearning that provides significant\nreduction in terms of both memory (90-95\\%), time (100x) and space consumption\n(99\\%) in comparison to the baselines while keeping model performance constant.",
    "descriptor": "",
    "authors": [
      "Vinayshekhar Bannihatti Kumar",
      "Rashmi Gangadharaiah",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09573"
  },
  {
    "id": "arXiv:2212.09577",
    "title": "CiteBench: A benchmark for Scientific Citation Text Generation",
    "abstract": "The publication rates are skyrocketing across many fields of science, and it\nis difficult to stay up to date with the latest research. This makes\nautomatically summarizing the latest findings and helping scholars to\nsynthesize related work in a given area an attractive research objective. In\nthis paper we study the problem of citation text generation, where given a set\nof cited papers and citing context the model should generate a citation text.\nWhile citation text generation has been tackled in prior work, existing studies\nuse different datasets and task definitions, which makes it hard to study\ncitation text generation systematically. To address this, we propose CiteBench:\na benchmark for citation text generation that unifies the previous datasets and\nenables standardized evaluation of citation text generation models across task\nsettings and domains. Using the new benchmark, we investigate the performance\nof multiple strong baselines, test their transferability between the datasets,\nand deliver new insights into task definition and evaluation to guide the\nfuture research in citation text generation. We make CiteBench publicly\navailable at https://github.com/UKPLab/citebench.",
    "descriptor": "",
    "authors": [
      "Martin Funkquist",
      "Ilia Kuznetsov",
      "Yufang Hou",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09577"
  },
  {
    "id": "arXiv:2212.09579",
    "title": "Observability-aware online multi-lidar extrinsic calibration",
    "abstract": "Accurate and robust extrinsic calibration is necessary for deploying\nautonomous systems which need multiple sensors for perception. In this paper,\nwe present a robust system for real-time extrinsic calibration of multiple\nlidars in vehicle base frame without the need for any fiducial markers or\nfeatures. We base our approach on matching absolute GNSS and estimated lidar\nposes in real-time. Comparing rotation components allows us to improve the\nrobustness of the solution than traditional least-square approach comparing\ntranslation components only. Additionally, instead of comparing all\ncorresponding poses, we select poses comprising maximum mutual information\nbased on our novel observability criteria. This allows us to identify a subset\nof the poses helpful for real-time calibration. We also provide stopping\ncriteria for ensuring calibration completion. To validate our approach\nextensive tests were carried out on data collected using Scania test vehicles\n(7 sequences for a total of ~ 6.5 Km). The results presented in this paper show\nthat our approach is able to accurately determine the extrinsic calibration for\nvarious combinations of sensor setups.",
    "descriptor": "\nComments: For associated video file, see this https URL\n",
    "authors": [
      "Sandipan Das",
      "Ludvig af Klinteberg",
      "Maurice Fallon",
      "Saikat Chatterjee"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09579"
  },
  {
    "id": "arXiv:2212.09580",
    "title": "Independent Components of Word Embeddings Represent Semantic Features",
    "abstract": "Independent Component Analysis (ICA) is an algorithm originally developed for\nfinding separate sources in a mixed signal, such as a recording of multiple\npeople in the same room speaking at the same time. It has also been used to\nfind linguistic features in distributional representations. In this paper, we\nused ICA to analyze words embeddings. We have found that ICA can be used to\nfind semantic features of the words and these features can easily be combined\nto search for words that satisfy the combination. We show that only some of the\nindependent components represent such features, but those that do are stable\nwith regard to random initialization of the algorithm.",
    "descriptor": "",
    "authors": [
      "Tom\u00e1\u0161 Musil",
      "David Mare\u010dek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09580"
  },
  {
    "id": "arXiv:2212.09581",
    "title": "Reference-based Image and Video Super-Resolution via C2-Matching",
    "abstract": "Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising\nparadigm to enhance a low-resolution (LR) input image or video by introducing\nan additional high-resolution (HR) reference image. Existing Ref-SR methods\nmostly rely on implicit correspondence matching to borrow HR textures from\nreference images to compensate for the information loss in input images.\nHowever, performing local transfer is difficult because of two gaps between\ninput and reference images: the transformation gap (e.g., scale and rotation)\nand the resolution gap (e.g., HR and LR). To tackle these challenges, we\npropose C2-Matching in this work, which performs explicit robust matching\ncrossing transformation and resolution. 1) To bridge the transformation gap, we\npropose a contrastive correspondence network, which learns\ntransformation-robust correspondences using augmented views of the input image.\n2) To address the resolution gap, we adopt teacher-student correlation\ndistillation, which distills knowledge from the easier HR-HR matching to guide\nthe more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation\nmodule to address the potential misalignment issue between input images and\nreference images. In addition, to faithfully evaluate the performance of\nReference-based Image Super-Resolution under a realistic setting, we contribute\nthe Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage\nscenario. We also extend C2-Matching to Reference-based Video Super-Resolution\ntask, where an image taken in a similar scene serves as the HR reference image.\nExtensive experiments demonstrate that our proposed C2-Matching significantly\noutperforms state of the arts on the standard CUFED5 benchmark and also boosts\nthe performance of video SR by incorporating the C2-Matching component into\nVideo SR pipelines.",
    "descriptor": "\nComments: To appear in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). arXiv admin note: substantial text overlap with arXiv:2106.01863\n",
    "authors": [
      "Yuming Jiang",
      "Kelvin C.K. Chan",
      "Xintao Wang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09581"
  },
  {
    "id": "arXiv:2212.09586",
    "title": "Learning Latent Representations to Co-Adapt to Humans",
    "abstract": "When robots interact with humans in homes, roads, or factories the human's\nbehavior often changes in response to the robot. Non-stationary humans are\nchallenging for robot learners: actions the robot has learned to coordinate\nwith the original human may fail after the human adapts to the robot. In this\npaper we introduce an algorithmic formalism that enables robots (i.e., ego\nagents) to co-adapt alongside dynamic humans (i.e., other agents) using only\nthe robot's low-level states, actions, and rewards. A core challenge is that\nhumans not only react to the robot's behavior, but the way in which humans\nreact inevitably changes both over time and between users. To deal with this\nchallenge, our insight is that -- instead of building an exact model of the\nhuman -- robots can learn and reason over high-level representations of the\nhuman's policy and policy dynamics. Applying this insight we develop RILI:\nRobustly Influencing Latent Intent. RILI first embeds low-level robot\nobservations into predictions of the human's latent strategy and strategy\ndynamics. Next, RILI harnesses these predictions to select actions that\ninfluence the adaptive human towards advantageous, high reward behaviors over\nrepeated interactions. We demonstrate that -- given RILI's measured performance\nwith users sampled from an underlying distribution -- we can probabilistically\nbound RILI's expected performance across new humans sampled from the same\ndistribution. Our simulated experiments compare RILI to state-of-the-art\nrepresentation and reinforcement learning baselines, and show that RILI better\nlearns to coordinate with imperfect, noisy, and time-varying agents. Finally,\nwe conduct two user studies where RILI co-adapts alongside actual humans in a\ngame of tag and a tower-building task. See videos of our user studies here:\nhttps://youtu.be/WYGO5amDXbQ",
    "descriptor": "",
    "authors": [
      "Sagar Parekh",
      "Dylan P. Losey"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09586"
  },
  {
    "id": "arXiv:2212.09588",
    "title": "Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint  Modeling",
    "abstract": "The quality of knowledge retrieval is crucial in knowledge-intensive\nconversations. Two common strategies to improve the retrieval quality are\nfinetuning the retriever or generating a self-contained query, while they\nencounter heavy burdens on expensive computation and elaborate annotations. In\nthis paper, we propose an unsupervised query enhanced approach for\nknowledge-intensive conversations, namely QKConv. There are three modules in\nQKConv: a query generator, an off-the-shelf knowledge selector, and a response\ngenerator. Without extra supervision, the end-to-end joint training of QKConv\nexplores multiple candidate queries and utilizes corresponding selected\nknowledge to yield the target response. To evaluate the effectiveness of the\nproposed method, we conducted comprehensive experiments on conversational\nquestion-answering, task-oriented dialogue, and knowledge-grounded\nconversation. Experimental results demonstrate that QKConv achieves\nstate-of-the-art performance compared to unsupervised methods and competitive\nperformance compared to supervised methods.",
    "descriptor": "",
    "authors": [
      "Mingzhu Cai",
      "Siqi Bao",
      "Xin Tian",
      "Huang He",
      "Fan Wang",
      "Hua Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09588"
  },
  {
    "id": "arXiv:2212.09589",
    "title": "Learning to Detect Good Keypoints to Match Non-Rigid Objects in RGB  Images",
    "abstract": "We present a novel learned keypoint detection method designed to maximize the\nnumber of correct matches for the task of non-rigid image correspondence. Our\ntraining framework uses true correspondences, obtained by matching annotated\nimage pairs with a predefined descriptor extractor, as a ground-truth to train\na convolutional neural network (CNN). We optimize the model architecture by\napplying known geometric transformations to images as the supervisory signal.\nExperiments show that our method outperforms the state-of-the-art keypoint\ndetector on real images of non-rigid objects by 20 p.p. on Mean Matching\nAccuracy and also improves the matching performance of several descriptors when\ncoupled with our detection method. We also employ the proposed method in one\nchallenging realworld application: object retrieval, where our detector\nexhibits performance on par with the best available keypoint detectors. The\nsource code and trained model are publicly available at\nhttps://github.com/verlab/LearningToDetect SIBGRAPI 2022",
    "descriptor": "",
    "authors": [
      "Welerson Melo",
      "Guilherme Potje",
      "Felipe Cadar",
      "Renato Martins",
      "Erickson R. Nascimento"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09589"
  },
  {
    "id": "arXiv:2212.09593",
    "title": "Unsupervised Summarization Re-ranking",
    "abstract": "With the rise of task-specific pre-training objectives, abstractive\nsummarization models like PEGASUS offer appealing zero-shot performance on\ndownstream summarization tasks. However, the performance of such unsupervised\nmodels still lags significantly behind their supervised counterparts. Similarly\nto the supervised setup, we notice a very high variance in quality among\nsummary candidates from these models whereas only one candidate is kept as the\nsummary output. In this paper, we propose to re-rank summary candidates in an\nunsupervised manner, aiming to close the performance gap between unsupervised\nand supervised models. Our approach improves the pre-trained unsupervised\nPEGASUS by 4.37% to 7.27% relative mean ROUGE across four widely-adopted\nsummarization benchmarks, and achieves relative gains of 7.51% (up to 23.73%)\naveraged over 30 transfer setups.",
    "descriptor": "",
    "authors": [
      "Mathieu Ravaut",
      "Shafiq Joty",
      "Nancy Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09593"
  },
  {
    "id": "arXiv:2212.09597",
    "title": "Reasoning with Language Model Prompting: A Survey",
    "abstract": "Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions.",
    "descriptor": "\nComments: Work in progress and resources are available at this https URL (updated periodically)\n",
    "authors": [
      "Shuofei Qiao",
      "Yixin Ou",
      "Ningyu Zhang",
      "Xiang Chen",
      "Yunzhi Yao",
      "Shumin Deng",
      "Chuanqi Tan",
      "Fei Huang",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09597"
  },
  {
    "id": "arXiv:2212.09598",
    "title": "Query-as-context Pre-training for Dense Passage Retrieval",
    "abstract": "This paper presents a pre-training technique called query-as-context that\nuses query prediction to improve dense retrieval. Previous research has applied\nquery prediction to document expansion in order to alleviate the problem of\nlexical mismatch in sparse retrieval. However, query prediction has not yet\nbeen studied in the context of dense retrieval. Query-as-context pre-training\nassumes that the predicted query is a special context for the document and uses\ncontrastive learning or contextual masked auto-encoding learning to compress\nthe document and query into dense vectors. The technique is evaluated on\nlarge-scale passage retrieval benchmarks and shows considerable improvements\ncompared to existing strong baselines such as coCondenser and CoT-MAE,\ndemonstrating its effectiveness. Our code will be available at\nhttps://github.com/caskcsg/ir/tree/main/cotmae-qc .",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Xing Wu",
      "Guangyuan Ma",
      "Songlin Hu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09598"
  },
  {
    "id": "arXiv:2212.09603",
    "title": "Explanation Regeneration via Information Bottleneck",
    "abstract": "Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.",
    "descriptor": "",
    "authors": [
      "Qintong Li",
      "Zhiyong Wu",
      "Lingpeng Kong",
      "Wei Bi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09603"
  },
  {
    "id": "arXiv:2212.09606",
    "title": "Discrimination, calibration, and point estimate accuracy of  GRU-D-Weibull architecture for real-time individualized endpoint prediction",
    "abstract": "Real-time individual endpoint prediction has always been a challenging task\nbut of great clinic utility for both patients and healthcare providers. With\n6,879 chronic kidney disease stage 4 (CKD4) patients as a use case, we explored\nthe feasibility and performance of gated recurrent units with decay that models\nWeibull probability density function (GRU-D-Weibull) as a semi-parametric\nlongitudinal model for real-time individual endpoint prediction. GRU-D-Weibull\nhas a maximum C-index of 0.77 at 4.3 years of follow-up, compared to 0.68\nachieved by competing models. The L1-loss of GRU-D-Weibull is ~66% of XGB(AFT),\n~60% of MTLR, and ~30% of AFT model at CKD4 index date. The average absolute\nL1-loss of GRU-D-Weibull is around one year, with a minimum of 40% Parkes\nserious error after index date. GRU-D-Weibull is not calibrated and\nsignificantly underestimates true survival probability. Feature importance\ntests indicate blood pressure becomes increasingly important during follow-up,\nwhile eGFR and blood albumin are less important. Most continuous features have\nnon-linear/parabola impact on predicted survival time, and the results are\ngenerally consistent with existing knowledge. GRU-D-Weibull as a\nsemi-parametric temporal model shows advantages in built-in parameterization of\nmissing, native support for asynchronously arrived measurement, capability of\noutput both probability and point estimates at arbitrary time point for\narbitrary prediction horizon, improved discrimination and point estimate\naccuracy after incorporating newly arrived data. Further research on its\nperformance with more comprehensive input features, in-process or post-process\ncalibration are warranted to benefit CKD4 or alike terminally-ill patients.",
    "descriptor": "\nComments: 30 pages, 3 tables, 1 supplementary table, 9 figures, 8 supplementary figures, 52 references\n",
    "authors": [
      "Xiaoyang Ruan",
      "Liwei Wang",
      "Michelle Mai",
      "Charat Thongprayoon",
      "Wisit Cheungpasitporn",
      "Hongfang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2212.09606"
  },
  {
    "id": "arXiv:2212.09611",
    "title": "Optimizing Prompts for Text-to-Image Generation",
    "abstract": "Well-designed prompts can guide text-to-image models to generate amazing\nimages. However, the performant prompts are often model-specific and misaligned\nwith user input. Instead of laborious human engineering, we propose prompt\nadaptation, a general framework that automatically adapts original user input\nto model-preferred prompts. Specifically, we first perform supervised\nfine-tuning with a pretrained language model on a small collection of manually\nengineered prompts. Then we use reinforcement learning to explore better\nprompts. We define a reward function that encourages the policy to generate\nmore aesthetically pleasing images while preserving the original user\nintentions. Experimental results on Stable Diffusion show that our method\noutperforms manual prompt engineering in terms of both automatic metrics and\nhuman preference ratings. Moreover, reinforcement learning further boosts\nperformance, especially on out-of-domain prompts. The pretrained checkpoints\nare available at https://aka.ms/promptist. The demo can be found at\nhttps://aka.ms/promptist-demo.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Yaru Hao",
      "Zewen Chi",
      "Li Dong",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09611"
  },
  {
    "id": "arXiv:2212.09613",
    "title": "Model Predictive Spherical Image-Based Visual Servoing On $SO(3)$ for  Aggressive Aerial Tracking",
    "abstract": "This paper presents an image-based visual servo control (IBVS) method for a\nfirst-person-view (FPV) quadrotor to conduct aggressive aerial tracking. There\nare three major challenges to maneuvering an underactuated vehicle using IBVS:\n(i) finding a visual feature representation that is robust to large rotations\nand is suited to be an optimization variable; (ii) keeping the target visible\nwithout sacrificing the robot's agility; and (iii) compensating for the\nrotational effects in the detected features. We propose a complete design\nframework to address these problems. First, we employ a rotation on $SO(3)$ to\nrepresent a spherical image feature on $S^{2}$ to gain singularity-free and\nsecond-order differentiable properties. To ensure target visibility, we\nformulate the IBVS as a nonlinear model predictive control (NMPC) problem with\nthree constraints taken into account: the robot's physical limits, target\nvisibility, and time-to-collision (TTC). Furthermore, we propose a novel\nattitude-compensation scheme to enable formulating the visibility constraint in\nthe actual image plane instead of a virtual fix-orientation image plane. It\nguarantees that the visibility constraint is valid under large rotations.\nExtensive experimental results show that our method can track a fast-moving\ntarget stably and aggressively without the aid of a localization system.",
    "descriptor": "",
    "authors": [
      "Chao Qin",
      "Qiuyu Yu",
      "Hugh H.T. Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09613"
  },
  {
    "id": "arXiv:2212.09616",
    "title": "Pseudonymization at Scale: OLCF's Summit Usage Data Case Study",
    "abstract": "The analysis of vast amounts of data and the processing of complex\ncomputational jobs have traditionally relied upon high performance computing\n(HPC) systems. Understanding these analyses' needs is paramount for designing\nsolutions that can lead to better science, and similarly, understanding the\ncharacteristics of the user behavior on those systems is important for\nimproving user experiences on HPC systems. A common approach to gathering data\nabout user behavior is to analyze system log data available only to system\nadministrators. Recently at Oak Ridge Leadership Computing Facility (OLCF),\nhowever, we unveiled user behavior about the Summit supercomputer by collecting\ndata from a user's point of view with ordinary Unix commands.\nHere, we discuss the process, challenges, and lessons learned while preparing\nthis dataset for publication and submission to an open data challenge. The\noriginal dataset contains personal identifiable information (PII) about OLCF\nusers which needed be masked prior to publication, and we determined that\nanonymization, which scrubs PII completely, destroyed too much of the structure\nof the data to be interesting for the data challenge. We instead chose to\npseudonymize the dataset to reduce its linkability to users' identities.\nPseudonymization is significantly more computationally expensive than\nanonymization, and the size of our dataset, approximately 175 million lines of\nraw text, necessitated the development of a parallelized workflow that could be\nreused on different HPC machines. We demonstrate the scaling behavior of the\nworkflow on two leadership class HPC systems at OLCF, and we show that we were\nable to bring the overall makespan time from an impractical 20+ hours on a\nsingle node down to around 2 hours. As a result of this work, we release the\nentire pseudonymized dataset and make the workflows and source code publicly\navailable.",
    "descriptor": "\nComments: 9 pages, 5 figures, accepted to BTSD 2022 workshop (see this https URL for more information), to be published in the proceedings of IEEE Big Data 2022\n",
    "authors": [
      "Ketan Maheshwari",
      "Sean R. Wilkinson",
      "Alex May",
      "Tyler Skluzacek",
      "Olga A. Kuchar",
      "Rafael Ferreira da Silva"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2212.09616"
  },
  {
    "id": "arXiv:2212.09621",
    "title": "Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document  Understanding",
    "abstract": "Unsupervised pre-training on millions of digital-born or scanned documents\nhas shown promising advances in visual document understanding~(VDU). While\nvarious vision-language pre-training objectives are studied in existing\nsolutions, the document textline, as an intrinsic granularity in VDU, has\nseldom been explored so far. A document textline usually contains words that\nare spatially and semantically correlated, which can be easily obtained from\nOCR engines. In this paper, we propose Wukong-Reader, trained with new\npre-training objectives to leverage the structural knowledge nested in document\ntextlines. We introduce textline-region contrastive learning to achieve\nfine-grained alignment between the visual regions and texts of document\ntextlines. Furthermore, masked region modeling and textline-grid matching are\nalso designed to enhance the visual and layout representations of textlines.\nExperiments show that our Wukong-Reader has superior performance on various VDU\ntasks such as information extraction. The fine-grained alignment over textlines\nalso empowers Wukong-Reader with promising localization ability.",
    "descriptor": "",
    "authors": [
      "Haoli Bai",
      "Zhiguang Liu",
      "Xiaojun Meng",
      "Wentao Li",
      "Shuang Liu",
      "Nian Xie",
      "Rongfu Zheng",
      "Liangwei Wang",
      "Lu Hou",
      "Jiansheng Wei",
      "Xin Jiang",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09621"
  },
  {
    "id": "arXiv:2212.09631",
    "title": "Optimal Transport for Unsupervised Hallucination Detection in Neural  Machine Translation",
    "abstract": "Neural machine translation (NMT) has become the de-facto standard in\nreal-world machine translation applications. However, NMT models can\nunpredictably produce severely pathological translations, known as\nhallucinations, that seriously undermine user trust. It becomes thus crucial to\nimplement effective preventive strategies to guarantee their proper\nfunctioning. In this paper, we address the problem of hallucination detection\nin NMT by following a simple intuition: as hallucinations are detached from the\nsource content, they exhibit encoder-decoder attention patterns that are\nstatistically different from those of good quality translations. We frame this\nproblem with an optimal transport formulation and propose a fully unsupervised,\nplug-in detector that can be used with any attention-based NMT model.\nExperimental results show that our detector not only outperforms all previous\nmodel-based detectors, but is also competitive with detectors that employ large\nmodels trained on millions of samples.",
    "descriptor": "",
    "authors": [
      "Nuno M. Guerreiro",
      "Pierre Colombo",
      "Pablo Piantanida",
      "Andr\u00e9 F. T. Martins"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09631"
  },
  {
    "id": "arXiv:2212.09633",
    "title": "Towards Assessing Data Bias in Clinical Trials",
    "abstract": "Algorithms and technologies are essential tools that pervade all aspects of\nour daily lives. In the last decades, health care research benefited from new\ncomputer-based recruiting methods, the use of federated architectures for data\nstorage, the introduction of innovative analyses of datasets, and so on.\nNevertheless, health care datasets can still be affected by data bias. Due to\ndata bias, they provide a distorted view of reality, leading to wrong analysis\nresults and, consequently, decisions. For example, in a clinical trial that\nstudied the risk of cardiovascular diseases, predictions were wrong due to the\nlack of data on ethnic minorities. It is, therefore, of paramount importance\nfor researchers to acknowledge data bias that may be present in the datasets\nthey use, eventually adopt techniques to mitigate them and control if and how\nanalyses results are impacted. This paper proposes a method to address bias in\ndatasets that: (i) defines the types of data bias that may be present in the\ndataset, (ii) characterizes and quantifies data bias with adequate metrics,\n(iii) provides guidelines to identify, measure, and mitigate data bias for\ndifferent data sources. The method we propose is applicable both for\nprospective and retrospective clinical trials. We evaluate our proposal both\nthrough theoretical considerations and through interviews with researchers in\nthe health care environment.",
    "descriptor": "\nComments: 13 pages, 6 figures, accepted for publication at The Eighth International Workshop on Data Management and Analytics for Medicine and Healthcare (VLDB DMAH 2022)\n",
    "authors": [
      "Chiara Criscuolo",
      "Tommaso Dolci",
      "Mattia Salnitri"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2212.09633"
  },
  {
    "id": "arXiv:2212.09637",
    "title": "A Sequential Concept Drift Detection Method for On-Device Learning on  Low-End Edge Devices",
    "abstract": "A practical issue of edge AI systems is that data distributions of trained\ndataset and deployed environment may differ due to noise and environmental\nchanges over time. Such a phenomenon is known as a concept drift, and this gap\ndegrades the performance of edge AI systems and may introduce system failures.\nTo address this gap, a retraining of neural network models triggered by concept\ndrift detection is a practical approach. However, since available compute\nresources are strictly limited in edge devices, in this paper we propose a\nlightweight concept drift detection method in cooperation with a recently\nproposed on-device learning technique of neural networks. In this case, both\nthe neural network retraining and the proposed concept drift detection are done\nby sequential computation only to reduce computation cost and memory\nutilization. Evaluation results of the proposed approach shows that while the\naccuracy is decreased by 3.8%-4.3% compared to existing batch-based detection\nmethods, it decreases the memory size by 88.9%-96.4% and the execution time by\n1.3%-83.8%. As a result, the combination of the neural network retraining and\nthe proposed concept drift detection method is demonstrated on Raspberry Pi\nPico that has 264kB memory.",
    "descriptor": "",
    "authors": [
      "Takeya Yamada",
      "Hiroki Matsutani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09637"
  },
  {
    "id": "arXiv:2212.09638",
    "title": "Co-designing for a Hybrid Workplace Experience in Software Development",
    "abstract": "With increasing demands for flexible work models, many IT organizations have\nadapted to hybrid work that promises enhanced team productivity as well as work\nsatisfaction. To achieve productive engineering practice, collaborative product\ninnovation, and effective mentorship in the ensuing hybrid work, we introduce a\nworkshop approach on co-designing for a hybrid workplace experience and provide\nimplications for continuously improving collaborative software development at\nscale.",
    "descriptor": "\nComments: Accepted at IEEE Software\n",
    "authors": [
      "Zhendong Wang",
      "Yi-Hung Chou",
      "Kayla Fathi",
      "Tobias Schimmer",
      "Peter Colligan",
      "David Redmiles",
      "Rafael Prikladnicki"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2212.09638"
  },
  {
    "id": "arXiv:2212.09641",
    "title": "Uncovering the Origins of Instability in Dynamical Systems: How  Attention Mechanism Can Help?",
    "abstract": "The behavior of the network and its stability are governed by both dynamics\nof individual nodes as well as their topological interconnections. Attention\nmechanism as an integral part of neural network models was initially designed\nfor natural language processing (NLP), and so far, has shown excellent\nperformance in combining dynamics of individual nodes and the coupling\nstrengths between them within a network. Despite undoubted impact of attention\nmechanism, it is not yet clear why some nodes of a network get higher attention\nweights. To come up with more explainable solutions, we tried to look at the\nproblem from stability perspective. Based on stability theory, negative\nconnections in a network can create feedback loops or other complex structures\nby allowing information to flow in the opposite direction. These structures\nplay a critical role in the dynamics of a complex system and can contribute to\nabnormal synchronization, amplification, or suppression. We hypothesized that\nthose nodes that are involved in organizing such structures can push the entire\nnetwork into instability modes and therefore need higher attention during\nanalysis. To test this hypothesis, attention mechanism along with spectral and\ntopological stability analyses was performed on a real-world numerical problem,\ni.e., a linear Multi Input Multi Output state-space model of a piezoelectric\ntube actuator. The findings of our study suggest that the attention should be\ndirected toward the collective behaviour of imbalanced structures and\npolarity-driven structural instabilities within the network. The results\ndemonstrated that the nodes receiving more attention cause more instability in\nthe system. Our study provides a proof of concept to understand why perturbing\nsome nodes of a network may cause dramatic changes in the network dynamics.",
    "descriptor": "",
    "authors": [
      "Nooshin Bahador",
      "Milad Lankarany"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09641"
  },
  {
    "id": "arXiv:2212.09642",
    "title": "Computation of the von Neumann entropy of large matrices via trace  estimators and rational Krylov methods",
    "abstract": "We consider the problem of approximating the von Neumann entropy of a large,\nsparse, symmetric positive semidefinite matrix $A$, defined as\n$\\operatorname{tr}(f(A))$ where $f(x)=-x\\log x$. After establishing some useful\nproperties of this matrix function, we consider the use of both polynomial and\nrational Krylov subspace algorithms within two types of approximations methods,\nnamely, randomized trace estimators and probing techniques based on graph\ncolorings. We develop error bounds and heuristics which are employed in the\nimplementation of the algorithms. Numerical experiments on density matrices of\ndifferent types of networks illustrate the performance of the methods.",
    "descriptor": "\nComments: 32 pages, 9 figures\n",
    "authors": [
      "Michele Benzi",
      "Michele Rinelli",
      "Igor Simunec"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09642"
  },
  {
    "id": "arXiv:2212.09645",
    "title": "Designing Culturally Aware Learning Analytics: A Value Sensitive  Perspective",
    "abstract": "This chapter aims to stress the importance of addressing culture when\ndesigning and implementing learning analytics services. Learning analytics have\nbeen implemented in different countries with the purpose of improving learning\nand supporting teaching; yet, largely at a limited scale and so far with\nlimited evidence of achieving their purpose. Even though some solutions seem\npromising, their transfer from one country to another might prove challenging\nand sometimes impossible due to various technical, social, contextual and\ncultural factors. In this chapter, we argue for a need to carefully consider\none of these factors, namely cultural values when designing and implementing\nlearning analytics systems. Viewing culture from a value-sensitive perspective,\nin this chapter, we: 1)exemplify two selected values (i.e. privacy and\nautonomy) that might play a significant role in the design of learning\nanalytics systems, and 2)discuss opportunities for applying culture-and\nvalue-sensitive design methods that can guide the design of culturally aware\nlearning analytics systems. Finally, a set of design implications for\nculturally aware and value-sensitive learning analytics services is offered.",
    "descriptor": "\nComments: 20\n",
    "authors": [
      "Olga Viberg",
      "Ioana Jivet",
      "Maren Scheffel"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2212.09645"
  },
  {
    "id": "arXiv:2212.09648",
    "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources",
    "abstract": "We present NusaCrowd, a collaborative initiative to collect and unite\nexisting resources for Indonesian languages, including opening access to\npreviously non-public resources. Through this initiative, we have has brought\ntogether 137 datasets and 117 standardized data loaders. The quality of the\ndatasets has been assessed manually and automatically, and their effectiveness\nhas been demonstrated in multiple experiments. NusaCrowd's data collection\nenables the creation of the first zero-shot benchmarks for natural language\nunderstanding and generation in Indonesian and its local languages.\nFurthermore, NusaCrowd brings the creation of the first multilingual automatic\nspeech recognition benchmark in Indonesian and its local languages. Our work is\nintended to help advance natural language processing research in\nunder-represented languages.",
    "descriptor": "",
    "authors": [
      "Samuel Cahyawijaya",
      "Holy Lovenia",
      "Alham Fikri Aji",
      "Genta Indra Winata",
      "Bryan Wilie",
      "Rahmad Mahendra",
      "Christian Wibisono",
      "Ade Romadhony",
      "Karissa Vincentio",
      "Fajri Koto",
      "Jennifer Santoso",
      "David Moeljadi",
      "Cahya Wirawan",
      "Frederikus Hudi",
      "Ivan Halim Parmonangan",
      "Ika Alfina",
      "Muhammad Satrio Wicaksono",
      "Ilham Firdausi Putra",
      "Samsul Rahmadani",
      "Yulianti Oenang",
      "Ali Akbar Septiandri",
      "James Jaya",
      "Kaustubh D. Dhole",
      "Arie Ardiyanti Suryani",
      "Rifki Afina Putri",
      "Dan Su",
      "Keith Stevens",
      "Made Nindyatama Nityasya",
      "Muhammad Farid Adilazuarda",
      "Ryan Ignatius",
      "Ryandito Diandaru",
      "Tiezheng Yu",
      "Vito Ghifari",
      "Wenliang Dai",
      "Yan Xu",
      "Dyah Damapuspita",
      "Cuk Tho",
      "Ichwanul Muslim Karo Karo",
      "Tirana Noor Fatyanosa",
      "Ziwei Ji",
      "Pascale Fung",
      "Graham Neubig",
      "Timothy Baldwin",
      "Sebastian Ruder",
      "Herry Sujaini",
      "Sakriani Sakti",
      "Ayu Purwarianti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09648"
  },
  {
    "id": "arXiv:2212.09651",
    "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
    "abstract": "Multilingual Pretrained Language Models (MPLMs) have shown their strong\nmultilinguality in recent empirical cross-lingual transfer studies. In this\npaper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)\npipeline to improve the zero-shot performance on low-resource languages (LRLs)\nby augmenting the context with semantically similar sentences retrieved from a\nhigh-resource language (HRL) as prompts. PARC improves the zero-shot\nperformance on three downstream tasks (binary sentiment classification, topic\ncategorization and natural language inference) with multilingual parallel test\nsets across 10 LRLs covering 6 language families in both unlabeled settings\n(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the\nfinetuning baseline by 3.7%. We find a significant positive correlation between\ncross-lingual transfer performance on one side, and the similarity between the\nhigh- and low-resource languages as well as the amount of low-resource\npretraining data on the other side. A robustness analysis suggests that PARC\nhas the potential to achieve even stronger performance with more powerful\nMPLMs.",
    "descriptor": "",
    "authors": [
      "Ercong Nie",
      "Sheng Liang",
      "Helmut Schmid",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09651"
  },
  {
    "id": "arXiv:2212.09656",
    "title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
    "abstract": "This paper proposes a question-answering system that can answer questions\nwhose supporting evidence is spread over multiple (potentially long) documents.\nThe system, called Visconde, uses a three-step pipeline to perform the task:\ndecompose, retrieve, and aggregate. The first step decomposes the question into\nsimpler questions using a few-shot large language model (LLM). Then, a\nstate-of-the-art search engine is used to retrieve candidate passages from a\nlarge collection for each decomposed question. In the final step, we use the\nLLM in a few-shot setting to aggregate the contents of the passages into the\nfinal answer. The system is evaluated on three datasets: IIRC, Qasper, and\nStrategyQA. Results suggest that current retrievers are the main bottleneck and\nthat readers are already performing at the human level as long as relevant\npassages are provided. The system is also shown to be more effective when the\nmodel is induced to give explanations before answering a question. Code is\navailable at \\url{https://github.com/neuralmind-ai/visconde}.",
    "descriptor": "",
    "authors": [
      "Jayr Pereira",
      "Robson Fidalgo",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.09656"
  },
  {
    "id": "arXiv:2212.09657",
    "title": "Grafting Laplace and Gaussian distributions: A new noise mechanism for  differential privacy",
    "abstract": "The framework of Differential privacy protects an individual's privacy while\npublishing query responses on congregated data. In this work, a new noise\naddition mechanism for differential privacy is introduced where the noise added\nis sampled from a hybrid density that resembles Laplace in the centre and\nGaussian in the tail. With a sharper centre and light, sub-Gaussian tail, this\ndensity has the best characteristics of both distributions. We theoretically\nanalyse the proposed mechanism and we derive the necessary and sufficient\ncondition in one dimension and a sufficient condition in high dimensions for\nthe mechanism to guarantee (${\\epsilon}$,${\\delta}$)-differential privacy.\nNumerical simulations corroborate the efficacy of the proposed mechanism\ncompared to other existing mechanisms in achieving better trade-off for privacy\nand accuracy.",
    "descriptor": "",
    "authors": [
      "Gokularam Muthukrishnan",
      "Sheetal Kalyani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09657"
  },
  {
    "id": "arXiv:2212.09660",
    "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic  Survey on Trends and Challenges",
    "abstract": "Code-Switching, a common phenomenon in written text and conversation, has\nbeen studied over decades by the natural language processing (NLP) research\ncommunity. Initially, code-switching is intensively explored by leveraging\nlinguistic theories and, currently, more machine-learning oriented approaches\nto develop models. We introduce a comprehensive systematic survey on\ncode-switching research in natural language processing to understand the\nprogress of the past decades and conceptualize the challenges and tasks on the\ncode-switching topic. Finally, we summarize the trends and findings and\nconclude with a discussion for future direction and open questions for further\ninvestigation.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Genta Indra Winata",
      "Alham Fikri Aji",
      "Zheng-Xin Yong",
      "Thamar Solorio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09660"
  },
  {
    "id": "arXiv:2212.09662",
    "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and  Chart Derendering",
    "abstract": "Visual language data such as plots, charts, and infographics are ubiquitous\nin the human world. However, state-of-the-art vision-language models do not\nperform well on these data. We propose MatCha (Math reasoning and Chart\nderendering pretraining) to enhance visual language models' capabilities in\njointly modeling charts/plots and language data. Specifically, we propose\nseveral pretraining tasks that cover plot deconstruction and numerical\nreasoning which are the key capabilities in visual language modeling.\nWe perform the MatCha pretraining starting from Pix2Struct, a recently\nproposed image-to-text visual language model. On standard benchmarks such as\nPlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as\nmuch as nearly 20%. We also examine how well MatCha pretraining transfers to\ndomains such as screenshots, textbook diagrams, and document figures and\nobserve overall improvement, verifying the usefulness of MatCha pretraining on\nbroader visual language tasks.",
    "descriptor": "",
    "authors": [
      "Fangyu Liu",
      "Francesco Piccinno",
      "Syrine Krichene",
      "Chenxi Pang",
      "Kenton Lee",
      "Mandar Joshi",
      "Yasemin Altun",
      "Nigel Collier",
      "Julian Martin Eisenschlos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09662"
  },
  {
    "id": "arXiv:2212.09663",
    "title": "Norm of word embedding encodes information gain",
    "abstract": "Distributed representations of words encode lexical semantic information, but\nhow is that information encoded in word embeddings? Focusing on the skip-gram\nwith negative-sampling method, we show theoretically and experimentally that\nthe squared norm of word embedding encodes the information gain defined by the\nKullback-Leibler divergence of the co-occurrence distribution of a word to the\nunigram distribution of the corpus. Furthermore, through experiments on tasks\nof keyword extraction, hypernym prediction, and part-of-speech discrimination,\nwe confirmed that the KL divergence and the squared norm of embedding work as a\nmeasure of the informativeness of a word provided that the bias caused by word\nfrequency is adequately corrected.",
    "descriptor": "",
    "authors": [
      "Momose Oyama",
      "Sho Yokoi",
      "Hidetoshi Shimodaira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09663"
  },
  {
    "id": "arXiv:2212.09666",
    "title": "MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code  Completion",
    "abstract": "Code completion is a valuable topic in both academia and industry. Recently,\nlarge-scale mono-programming-lingual (MonoPL) pre-training models have been\nproposed to boost the performance of code completion. However, the code\ncompletion on low-resource programming languages (PL) is difficult for the\ndata-driven paradigm, while there are plenty of developers using low-resource\nPLs. On the other hand, there are few studies exploring the effects of\nmulti-programming-lingual (MultiPL) pre-training for the code completion,\nespecially the impact on low-resource programming languages. To this end, we\npropose the MultiCoder to enhance the low-resource code completion via MultiPL\npre-training and MultiPL Mixture-of-Experts (MoE) layers. We further propose a\nnovel PL-level MoE routing strategy (PL-MoE) for improving the code completion\non all PLs. Experimental results on CodeXGLUE and MultiCC demonstrate that 1)\nthe proposed MultiCoder significantly outperforms the MonoPL baselines on\nlow-resource programming languages, and 2) the PL-MoE module further boosts the\nperformance on six programming languages. In addition, we analyze the effects\nof the proposed method in details and explore the effectiveness of our method\nin a variety of scenarios.",
    "descriptor": "",
    "authors": [
      "Zi Gong",
      "Yinpeng Guo",
      "Pingyi Zhou",
      "Cuiyun Gao",
      "Yasheng Wang",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09666"
  },
  {
    "id": "arXiv:2212.09667",
    "title": "Foveate, Attribute, and Rationalize: Towards Safe and Trustworthy AI",
    "abstract": "Users' physical safety is an increasing concern as the market for intelligent\nsystems continues to grow, where unconstrained systems may recommend users\ndangerous actions that can lead to serious injury. Covertly unsafe text,\nlanguage that contains actionable physical harm, but requires further reasoning\nto identify such harm, is an area of particular interest, as such texts may\narise from everyday scenarios and are challenging to detect as harmful.\nQualifying the knowledge required to reason about the safety of various texts\nand providing human-interpretable rationales can shed light on the risk of\nsystems to specific user groups, helping both stakeholders manage the risks of\ntheir systems and policymakers to provide concrete safeguards for consumer\nsafety. We propose FARM, a novel framework that leverages external knowledge\nfor trustworthy rationale generation in the context of safety. In particular,\nFARM foveates on missing knowledge in specific scenarios, retrieves this\nknowledge with attribution to trustworthy sources, and uses this to both\nclassify the safety of the original text and generate human-interpretable\nrationales, combining critically important qualities for sensitive domains such\nas user safety. Furthermore, FARM obtains state-of-the-art results on the\nSafeText dataset, improving safety classification accuracy by 5.29 points.",
    "descriptor": "\nComments: 9 pages, 3 figures, 5 tables\n",
    "authors": [
      "Alex Mei",
      "Sharon Levy",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09667"
  },
  {
    "id": "arXiv:2212.09668",
    "title": "Task-Oriented Communications for NextG: End-to-End Deep Learning and AI  Security Aspects",
    "abstract": "Communications systems to date are primarily designed with the goal of\nreliable (error-free) transfer of digital sequences (bits). Next generation\n(NextG) communication systems are beginning to explore shifting this design\nparadigm of reliably decoding bits to reliably executing a given task.\nTask-oriented communications system design is likely to find impactful\napplications, for example, considering the relative importance of messages. In\nthis paper, a wireless signal classification is considered as the task to be\nperformed in the NextG Radio Access Network (RAN) for signal intelligence and\nspectrum awareness applications such as user equipment (UE) identification and\nauthentication, and incumbent signal detection for spectrum co-existence. For\nthat purpose, edge devices collect wireless signals and communicate with the\nNextG base station (gNodeB) that needs to know the signal class. Edge devices\nmay not have sufficient processing power and may not be trusted to perform the\nsignal classification task, whereas the transfer of the captured signals from\nthe edge devices to the gNodeB may not be efficient or even feasible subject to\nstringent delay, rate, and energy restrictions. We present a task-oriented\ncommunications approach, where all the transmitter, receiver and classifier\nfunctionalities are jointly trained as two deep neural networks (DNNs), one for\nthe edge device and another for the gNodeB. We show that this approach achieves\nbetter accuracy with smaller DNNs compared to the baselines that treat\ncommunications and signal classification as two separate tasks. Finally, we\ndiscuss how adversarial machine learning poses a major security threat for the\nuse of DNNs for task-oriented communications. We demonstrate the major\nperformance loss under backdoor (Trojan) attacks and adversarial (evasion)\nattacks that target the training and test processes of task-oriented\ncommunications.",
    "descriptor": "",
    "authors": [
      "Yalin E. Sagduyu",
      "Sennur Ulukus",
      "Aylin Yener"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.09668"
  },
  {
    "id": "arXiv:2212.09670",
    "title": "StyleFlow: Disentangle Latent Representations via Normalizing Flow for  Unsupervised Text Style Transfer",
    "abstract": "Text style transfer aims to alter the style of a sentence while preserving\nits content. Due to the lack of parallel corpora, most recent work focuses on\nunsupervised methods and often uses cycle construction to train models. Since\ncycle construction helps to improve the style transfer ability of the model by\nrebuilding transferred sentences back to original-style sentences, it brings\nabout a content loss in unsupervised text style transfer tasks. In this paper,\nwe propose a novel disentanglement-based style transfer model StyleFlow to\nenhance content preservation. Instead of the typical encoder-decoder scheme,\nStyleFlow can not only conduct the forward process to obtain the output, but\nalso infer to the input through the output. We design an attention-aware\ncoupling layers to disentangle the content representations and the style\nrepresentations of a sentence. Besides, we propose a data augmentation method\nbased on Normalizing Flow to improve the robustness of the model. Experiment\nresults demonstrate that our model preserves content effectively and achieves\nthe state-of-the-art performance on the most metrics.",
    "descriptor": "",
    "authors": [
      "Kangchen Zhu",
      "Zhiliang Tian",
      "Ruifeng Luo",
      "Xiaoguang Mao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09670"
  },
  {
    "id": "arXiv:2212.09672",
    "title": "An Adaptive Robotics Framework for Chemistry Lab Automation",
    "abstract": "In the process of materials discovery, chemists currently need to perform\nmany laborious, time-consuming, and often dangerous lab experiments. To\naccelerate this process, we propose a framework for robots to assist chemists\nby performing lab experiments autonomously. The solution allows a\ngeneral-purpose robot to perform diverse chemistry experiments and efficiently\nmake use of available lab tools. Our system can load high-level descriptions of\nchemistry experiments, perceive a dynamic workspace, and autonomously plan the\nrequired actions and motions to perform the given chemistry experiments with\ncommon tools found in the existing lab environment. Our architecture uses a\nmodified PDDLStream solver for integrated task and constrained motion planning,\nwhich generates plans and motions that are guaranteed to be safe by preventing\ncollisions and spillage. We present a modular framework that can scale to many\ndifferent experiments, actions, and lab tools. In this work, we demonstrate the\nutility of our framework on three pouring skills and two foundational chemical\nexperiments for materials synthesis: solubility and recrystallization. More\nexperiments and updated evaluations can be found at\nhttps://ac-rad.github.io/arc-icra2023.",
    "descriptor": "\nComments: Equal author contribution from Naruki Yoshikawa, Andrew Zou Li, Kourosh Darvish, Yuchi Zhao and Haoping Xu\n",
    "authors": [
      "Naruki Yoshikawa",
      "Andrew Zou Li",
      "Kourosh Darvish",
      "Yuchi Zhao",
      "Haoping Xu",
      "Alan Aspuru-Guzik",
      "Animesh Garg",
      "Florian Shkurti"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.09672"
  },
  {
    "id": "arXiv:2212.09673",
    "title": "The pressure-wired Stokes element: a mesh-robust version of the  Scott-Vogelius element",
    "abstract": "The conforming Scott-Vogelius pair for the stationary Stokes equation in 2D\nis a popular finite element which is inf-sup stable for any fixed regular\ntriangulation.\nHowever, the inf-sup constant deteriorates if the \"singular distance\"\n(measured by some geometric mesh quantity $\\Theta_{\\min}>0$) of the finite\nelement mesh to certain \"singular\" mesh configurations is small. In this paper\nwe present a modification of the classical Scott-Vogelius element of arbitrary\npolynomial order $k\\geq4$ for the velocity where a constraint on the pressure\nspace is imposed if locally the singular distance is smaller than some control\nparameter $\\eta>0$. We establish a lower bound on the inf-sup constant in terms\nof $\\Theta_{\\mathrm{\\min}}+\\eta>0$ independent of the maximal mesh width and\nthe polynomial degree that does not deteriorate for small $\\Theta_{\\min} \\ll\n1$. The divergence of the discrete velocity is at most of size\n$\\mathcal{O}(\\eta)$ and very small in practical examples. In the limit\n$\\eta\\rightarrow0$ we recover and improve estimates for the classical\nScott-Vogelius Stokes element.",
    "descriptor": "",
    "authors": [
      "Benedikt Gr\u00e4\u00dfle",
      "Nis-Erik Bohne",
      "Stefan A. Sauter"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09673"
  },
  {
    "id": "arXiv:2212.09674",
    "title": "LR-Sum: Summarization for Less-Resourced Languages",
    "abstract": "This preprint describes work in progress on LR-Sum, a new\npermissively-licensed dataset created with the goal of enabling further\nresearch in automatic summarization for less-resourced languages. LR-Sum\ncontains human-written summaries for 40 languages, many of which are\nless-resourced. We describe our process for extracting and filtering the\ndataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022). The\nsource data is public domain newswire collected from from Voice of America\nwebsites, and LR-Sum is released under a Creative Commons license (CC BY 4.0),\nmaking it one of the most openly-licensed multilingual summarization datasets.\nWe describe how we plan to use the data for modeling experiments and discuss\nlimitations of the dataset.",
    "descriptor": "",
    "authors": [
      "Chester Palen-Michel",
      "Constantine Lignos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09674"
  },
  {
    "id": "arXiv:2212.09676",
    "title": "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings  in Scholarly Publications",
    "abstract": "Scholarly text is often laden with jargon, or specialized language that\ndivides disciplines. We extend past work that characterizes science at the\nlevel of word types, by using BERT-based word sense induction to find\nadditional words that are widespread but overloaded with different uses across\nfields. We define scholarly jargon as discipline-specific word types and\nsenses, and estimate its prevalence across hundreds of fields using\ninterpretable, information-theoretic metrics. We demonstrate the utility of our\napproach for science of science and computational sociolinguistics by\nhighlighting two key social implications. First, we measure audience design,\nand find that most fields reduce jargon when publishing in general-purpose\njournals, but some do so more than others. Second, though jargon has varying\ncorrelation with articles' citation rates within fields, it nearly always\nimpedes interdisciplinary impact. Broadly, our measurements can inform ways in\nwhich language could be revised to serve as a bridge rather than a barrier in\nscience.",
    "descriptor": "\nComments: 16 pages, 10 figures\n",
    "authors": [
      "Li Lucy",
      "Jesse Dodge",
      "David Bamman",
      "Katherine A. Keith"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2212.09676"
  },
  {
    "id": "arXiv:2212.09682",
    "title": "Multilingual Sequence-to-Sequence Models for Hebrew NLP",
    "abstract": "Recent work attributes progress in NLP to large language models (LMs) with\nincreased model size and large quantities of pretraining data. Despite this,\ncurrent state-of-the-art LMs for Hebrew are both under-parameterized and\nunder-trained compared to LMs in other languages. Additionally, previous work\non pretrained Hebrew LMs focused on encoder-only models. While the encoder-only\narchitecture is beneficial for classification tasks, it does not cater well for\nsub-word prediction tasks, such as Named Entity Recognition, when considering\nthe morphologically rich nature of Hebrew. In this paper we argue that\nsequence-to-sequence generative architectures are more suitable for LLMs in the\ncase of morphologically rich languages (MRLs) such as Hebrew. We demonstrate\nthat by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, we can\nleverage powerful multilingual, pretrained sequence-to-sequence models as mT5,\neliminating the need for a specialized, morpheme-based, separately fine-tuned\ndecoder. Using this approach, our experiments show substantial improvements\nover previously published results on existing Hebrew NLP benchmarks. These\nresults suggest that multilingual sequence-to-sequence models present a\npromising building block for NLP for MRLs.",
    "descriptor": "",
    "authors": [
      "Matan Eyal",
      "Hila Noga",
      "Roee Aharoni",
      "Idan Szpektor",
      "Reut Tsarfaty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09682"
  },
  {
    "id": "arXiv:2212.09683",
    "title": "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case  Study of COVID-19 Treatments",
    "abstract": "We present a human-in-the-loop evaluation framework for fact-checking novel\nmisinformation claims and identifying social media messages that violate\nrelevant policies. Our approach extracts structured representations of\ncheck-worthy claims, which are aggregated and ranked for review. Stance\nclassifiers are then used to identify tweets supporting novel misinformation\nclaims, which are further reviewed to determine whether they violate relevant\npolicies. To demonstrate the feasibility of our approach, we develop a baseline\nsystem based on modern NLP methods for human-in-the-loop fact-checking in the\ndomain of COVID-19 treatments. Using our baseline system, we show that human\nfact-checkers can identify 124 tweets per hour that violate Twitter's policies\non COVID-19 misinformation. We will make our code, data, and detailed\nannotation guidelines available to support the evaluation of human-in-the-loop\nsystems that identify novel misinformation directly from raw user-generated\ncontent.",
    "descriptor": "",
    "authors": [
      "Ethan Mendes",
      "Yang Chen",
      "Alan Ritter",
      "Wei Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09683"
  },
  {
    "id": "arXiv:2212.09686",
    "title": "A Natural Bias for Language Generation Models",
    "abstract": "After just a few hundred training updates, a standard probabilistic model for\nlanguage generation has likely not yet learnt many semantic or syntactic rules\nof natural language, which inherently makes it difficult to estimate the right\nprobability distribution over next tokens. Yet around this point, these models\nhave identified a simple, loss-minimising behaviour: to output the unigram\ndistribution of the target training corpus. The use of such a crude heuristic\nraises the question: Rather than wasting precious compute resources and model\ncapacity for learning this strategy at early training stages, can we initialise\nour models with this behaviour? Here, we show that we can effectively endow our\nmodel with a separate module that reflects unigram frequency statistics as\nprior knowledge. Standard neural language generation architectures offer a\nnatural opportunity for implementing this idea: by initialising the bias term\nin a model's final linear layer with the log-unigram distribution. Experiments\nin neural machine translation demonstrate that this simple technique: (i)\nimproves learning efficiency; (ii) achieves better overall performance; and\n(iii) appears to disentangle strong frequency effects, encouraging the model to\nspecialise in non-frequency-related aspects of language.",
    "descriptor": "",
    "authors": [
      "Clara Meister",
      "Wojciech Stokowiec",
      "Tiago Pimentel",
      "Lei Yu",
      "Laura Rimell",
      "Adhiguna Kuncoro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09686"
  },
  {
    "id": "arXiv:2212.09689",
    "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human  Labor",
    "abstract": "Instruction tuning enables pretrained language models to perform new tasks\nfrom inference-time natural language descriptions. These approaches rely on\nvast amounts of human supervision in the form of crowdsourced datasets or user\ninteractions. In this work, we introduce Unnatural Instructions: a large\ndataset of creative and diverse instructions, collected with virtually no human\nlabor. We collect 64,000 examples by prompting a language model with three seed\nexamples of instructions and eliciting a fourth. This set is then expanded by\nprompting the model to rephrase each instruction, creating a total of\napproximately 240,000 examples of instructions, inputs, and outputs.\nExperiments show that despite containing a fair amount of noise, training on\nUnnatural Instructions rivals the effectiveness of training on open-source\nmanually-curated datasets, surpassing the performance of models such as T0++\nand Tk-Instruct across various benchmarks. These results demonstrate the\npotential of model-generated data as a cost-effective alternative to\ncrowdsourcing for dataset expansion and diversification.",
    "descriptor": "\nComments: 18 pages, 7 figures\n",
    "authors": [
      "Or Honovich",
      "Thomas Scialom",
      "Omer Levy",
      "Timo Schick"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09689"
  },
  {
    "id": "arXiv:2212.09692",
    "title": "Analysis and Compilation of Normal Map Generation Techniques for Pixel  Art",
    "abstract": "Pixel art is a popular artistic style adopted in the gaming industry, and\nnowadays, it is often accompanied by modern rendering techniques. One example\nis dynamic lighting for the game sprites, for which normal mapping defines how\nthe light interacts with the material represented by each pixel. Although there\nare different methods to generate normal maps for 3D games, applying them for\npixel art may not yield correct results due to the style specificities.\nTherefore, this work compiles different normal map generation methods and study\ntheir applicability for pixel art, reducing the scarcity of existing material\non the techniques and contributing to a qualitative analysis of the behavior of\nthese methods in different case studies.",
    "descriptor": "\nComments: Published in SBGames 2022\n",
    "authors": [
      "Rodrigo D. Moreira",
      "Fl\u00e1vio Coutinho",
      "Luiz Chaimowicz"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2212.09692"
  },
  {
    "id": "arXiv:2212.09699",
    "title": "SegAugment: Maximizing the Utility of Speech Translation Data with  Segmentation-based Augmentations",
    "abstract": "Data scarcity is one of the main issues with the end-to-end approach for\nSpeech Translation, as compared to the cascaded one. Although most data\nresources for Speech Translation are originally document-level, they offer a\nsentence-level view, which can be directly used during training. But this\nsentence-level view is single and static, potentially limiting the utility of\nthe data. Our proposed data augmentation method SegAugment challenges this idea\nand aims to increase data availability by providing multiple alternative\nsentence-level views of a dataset. Our method heavily relies on an Audio\nSegmentation system to re-segment the speech of each document, after which we\nobtain the target text with alignment methods. The Audio Segmentation system\ncan be parameterized with different length constraints, thus giving us access\nto multiple and diverse sentence-level views for each document. Experiments in\nMuST-C show consistent gains across 8 language pairs, with an average increase\nof 2.2 BLEU points, and up to 4.7 BLEU for lower-resource scenarios in mTEDx.\nAdditionally, we find that SegAugment is also applicable to purely\nsentence-level data, as in CoVoST, and that it enables Speech Translation\nmodels to completely close the gap between the gold and automatic segmentation\nat inference time.",
    "descriptor": "\nComments: Work in progress, 10 pages + appendix\n",
    "authors": [
      "Ioannis Tsiamas",
      "Jos\u00e9 A. R. Fonollosa",
      "Marta R. Costa-juss\u00e0"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.09699"
  },
  {
    "id": "arXiv:2212.09700",
    "title": "Resoling Open-textured Rules with Templated Interpretive Arguments",
    "abstract": "Open-textured terms in written rules are typically settled through\ninterpretive argumentation. Ongoing work has attempted to catalogue the schemes\nused in such interpretive argumentation. But how can the use of these schemes\naffect the way in which people actually use and reason over the proper\ninterpretations of open-textured terms? Using the interpretive\nargument-eliciting game Aporia as our framework, we carried out an empirical\nstudy to answer this question. Differing from previous work, we did not allow\nparticipants to argue for interpretations arbitrarily, but to only use\narguments that fit with a given set of interpretive argument templates.\nFinally, we analyze the results captured by this new dataset, specifically\nfocusing on practical implications for the development of\ninterpretation-capable artificial reasoners.",
    "descriptor": "\nComments: Presented at the 2022 European Conference on Argumentation (ECA)\n",
    "authors": [
      "John Licato",
      "Logan Fields",
      "Zaid Marji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09700"
  },
  {
    "id": "arXiv:2212.09701",
    "title": "Graph-based Semantical Extractive Text Analysis",
    "abstract": "In the past few decades, there has been an explosion in the amount of\navailable data produced from various sources with different topics. The\navailability of this enormous data necessitates us to adopt effective\ncomputational tools to explore the data. This leads to an intense growing\ninterest in the research community to develop computational methods focused on\nprocessing this text data. A line of study focused on condensing the text so\nthat we are able to get a higher level of understanding in a shorter time. The\ntwo important tasks to do this are keyword extraction and text summarization.\nIn keyword extraction, we are interested in finding the key important words\nfrom a text. This makes us familiar with the general topic of a text. In text\nsummarization, we are interested in producing a short-length text which\nincludes important information about the document. The TextRank algorithm, an\nunsupervised learning method that is an extension of the PageRank (algorithm\nwhich is the base algorithm of Google search engine for searching pages and\nranking them) has shown its efficacy in large-scale text mining, especially for\ntext summarization and keyword extraction. this algorithm can automatically\nextract the important parts of a text (keywords or sentences) and declare them\nas the result. However, this algorithm neglects the semantic similarity between\nthe different parts. In this work, we improved the results of the TextRank\nalgorithm by incorporating the semantic similarity between parts of the text.\nAside from keyword extraction and text summarization, we develop a topic\nclustering algorithm based on our framework which can be used individually or\nas a part of generating the summary to overcome coverage problems.",
    "descriptor": "",
    "authors": [
      "Mina Samizadeh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09701"
  },
  {
    "id": "arXiv:2212.09702",
    "title": "On Event Individuation for Document-Level Information Extraction",
    "abstract": "As information extraction (IE) systems have grown more capable at\nwhole-document extraction, the classic task of \\emph{template filling} has seen\nrenewed interest as a benchmark for evaluating them. In this position paper, we\ncall into question the suitability of template filling for this purpose. We\nargue that the task demands definitive answers to thorny questions of\n\\emph{event individuation} -- the problem of distinguishing distinct events --\nabout which even human experts disagree. We show through annotation studies and\nerror analysis that this raises concerns about the usefulness of template\nfilling evaluation metrics, the quality of datasets for the task, and the\nability of models to learn it. Finally, we consider possible solutions.",
    "descriptor": "",
    "authors": [
      "William Gantt",
      "Reno Kriz",
      "Yunmo Chen",
      "Siddharth Vashishtha",
      "Aaron Steven White"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09702"
  },
  {
    "id": "arXiv:2212.09704",
    "title": "Rate-Privacy-Storage Tradeoff in Federated Learning with Top $r$  Sparsification",
    "abstract": "We investigate the trade-off between rate, privacy and storage in federated\nlearning (FL) with top $r$ sparsification, where the users and the servers in\nthe FL system only share the most significant $r$ and $r'$ fractions,\nrespectively, of updates and parameters in the FL process, to reduce the\ncommunication cost. We present schemes that guarantee information theoretic\nprivacy of the values and indices of the sparse updates sent by the users at\nthe expense of a larger storage cost. To this end, we generalize the scheme to\nreduce the storage cost by allowing a certain amount of information leakage.\nThus, we provide the general trade-off between the communication cost, storage\ncost, and information leakage in private FL with top $r$ sparsification, along\nthe lines of two proposed schemes.",
    "descriptor": "",
    "authors": [
      "Sajani Vithana",
      "Sennur Ulukus"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.09704"
  },
  {
    "id": "arXiv:2212.09710",
    "title": "Continual Learning for Instruction Following from Realtime Feedback",
    "abstract": "We study the problem of continually training an instruction-following agent\nthrough feedback provided by users during collaborative interactions. During\ninteraction, human users instruct an agent using natural language, and provide\nrealtime binary feedback as they observe the agent's instruction execution. We\ncast learning as a contextual bandit problem, converting the user feedback to\nimmediate reward. We evaluate through multiple rounds of human-agent\ninteractions, demonstrating 15.4% absolute improvement in instruction execution\nover time. We also show our approach is robust to several design variations,\nand that the feedback signal is roughly equivalent to the learning signal of\nsupervised demonstration data.",
    "descriptor": "",
    "authors": [
      "Alane Suhr",
      "Yoav Artzi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09710"
  },
  {
    "id": "arXiv:2212.09713",
    "title": "A Probabilistic Framework for Lifelong Test-Time Adaptation",
    "abstract": "Test-time adaptation is the problem of adapting a source pre-trained model\nusing test inputs from a target domain without access to source domain data.\nMost of the existing approaches address the setting in which the target domain\nis stationary. Moreover, these approaches are prone to making erroneous\npredictions with unreliable uncertainty estimates when distribution shifts\noccur. Hence, test-time adaptation in the face of non-stationary target domain\nshift becomes a problem of significant interest. To address these issues, we\npropose a principled approach, PETAL (Probabilistic lifElong Test-time\nAdaptation with seLf-training prior), which looks into this problem from a\nprobabilistic perspective using a partly data-dependent prior. A\nstudent-teacher framework, where the teacher model is an exponential moving\naverage of the student model naturally emerges from this probabilistic\nperspective. In addition, the knowledge from the posterior distribution\nobtained for the source task acts as a regularizer. To handle catastrophic\nforgetting in the long term, we also propose a data-driven model parameter\nresetting mechanism based on the Fisher information matrix (FIM). Moreover,\nimprovements in experimental results suggest that FIM based data-driven\nparameter restoration contributes to reducing the error accumulation and\nmaintaining the knowledge of recent domain by restoring only the irrelevant\nparameters. In terms of predictive error rate as well as uncertainty based\nmetrics such as Brier score and negative log-likelihood, our method achieves\nbetter results than the current state-of-the-art for online lifelong test time\nadaptation across various benchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC,\nand ImageNet3DCC datasets.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Dhanajit Brahma",
      "Piyush Rai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09713"
  },
  {
    "id": "arXiv:2212.09720",
    "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
    "abstract": "Quantization methods reduce the number of bits required to represent each\nparameter in a model, trading accuracy for smaller memory footprints and\ninference latencies. However, the final model size depends on both the number\nof parameters of the original model and the rate of compression. For example, a\n30B 8-bit model and a 60B 4-bit model have the same number of bits but may have\nvery different zero-shot accuracies. In this work, we study this trade-off by\ndeveloping inference scaling laws of zero-shot performance in Large Language\nModels (LLMs) to determine the bit-precision and model size that maximizes\nzero-shot performance. We run more than 35,000 zero-shot experiments with\n16-bit inputs and k-bit parameters to examine which quantization methods\nimprove scaling for 3 to 8-bit precision at scales of 19M to 66B parameters\nacross the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is\nchallenging to improve the bit-level scaling trade-off, with the only\nimprovements being the use of a small block size -- splitting the parameters\ninto small independently quantized blocks -- and the quantization data type\nbeing used (e.g., Int vs Float). Overall, our findings show that 4-bit\nprecision is almost universally optimal for total model bits and zero-shot\naccuracy.",
    "descriptor": "",
    "authors": [
      "Tim Dettmers",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.09720"
  },
  {
    "id": "arXiv:2212.09721",
    "title": "KNIFE: Knowledge Distillation with Free-Text Rationales",
    "abstract": "Free-text rationales (FTRs) follow how humans communicate by explaining\nreasoning processes via natural language. A number of recent works have studied\nhow to improve language model (LM) generalization by using FTRs to teach LMs\nthe correct reasoning processes behind correct task outputs. These prior works\naim to learn from FTRs by appending them to the LM input or target output, but\nthis may introduce an input distribution shift or conflict with the task\nobjective, respectively. We propose KNIFE, which distills FTR knowledge from an\nFTR-augmented teacher LM (takes both task input and FTR) to a student LM (takes\nonly task input), which is used for inference. Crucially, the teacher LM's\nforward computation has a bottleneck stage in which all of its FTR states are\nmasked out, which pushes knowledge from the FTR states into the task\ninput/output states. Then, FTR knowledge is distilled to the student LM by\ntraining its task input/output states to align with the teacher LM's. On two\nquestion answering datasets, we show that KNIFE significantly outperforms\nexisting FTR learning methods, in both fully-supervised and low-resource\nsettings.",
    "descriptor": "\nComments: 14 pages, 7 figures\n",
    "authors": [
      "Aaron Chan",
      "Zhiyuan Zeng",
      "Wyatt Lake",
      "Brihi Joshi",
      "Hanjie Chen",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09721"
  },
  {
    "id": "arXiv:2212.09723",
    "title": "MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource  Languages",
    "abstract": "This paper investigates the problem of Named Entity Recognition (NER) for\nextreme low-resource languages with only a few hundred tagged data samples. NER\nis a fundamental task in Natural Language Processing (NLP). A critical driver\naccelerating NER systems' progress is the existence of large-scale language\ncorpora that enable NER systems to achieve outstanding performance in languages\nsuch as English and French with abundant training data. However, NER for\nlow-resource languages remains relatively unexplored. In this paper, we\nintroduce Mask Augmented Named Entity Recognition (MANER), a new methodology\nthat leverages the distributional hypothesis of pre-trained masked language\nmodels (MLMs) for NER. The <mask> token in pre-trained MLMs encodes valuable\nsemantic contextual information. MANER re-purposes the <mask> token for NER\nprediction. Specifically, we prepend the <mask> token to every word in a\nsentence for which we would like to predict the named entity tag. During\ntraining, we jointly fine-tune the MLM and a new NER prediction head attached\nto each <mask> token. We demonstrate that MANER is well-suited for NER in\nlow-resource languages; our experiments show that for 100 languages with as few\nas 100 training examples, it improves on state-of-the-art methods by up to 48%\nand by 12% on average on F1 score. We also perform detailed analyses and\nablation studies to understand the scenarios that are best-suited to MANER.",
    "descriptor": "",
    "authors": [
      "Shashank Sonkar",
      "Zichao Wang",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09723"
  },
  {
    "id": "arXiv:2212.09724",
    "title": "A Retrieve-and-Read Framework for Knowledge Graph Link Prediction",
    "abstract": "Knowledge graph (KG) link prediction aims to infer new facts based on\nexisting facts in the KG. Recent studies have shown that using the graph\nneighborhood of a node via graph neural networks (GNNs) provides more useful\ninformation compared to just using the query information. Conventional GNNs for\nKG link prediction follow the standard message-passing paradigm on the entire\nKG, which leads to over-smoothing of representations and also limits their\nscalability. On a large scale, it becomes computationally expensive to\naggregate useful information from the entire KG for inference. To address the\nlimitations of existing KG link prediction frameworks, we propose a novel\nretrieve-and-read framework, which first retrieves a relevant subgraph context\nfor the query and then jointly reasons over the context and the query with a\nhigh-capacity reader. As part of our exemplar instantiation for the new\nframework, we propose a novel Transformer-based GNN as the reader, which\nincorporates graph-based attention structure and cross-attention between query\nand context for deep fusion. This design enables the model to focus on salient\ncontext information relevant to the query. Empirical results on two standard KG\nlink prediction datasets demonstrate the competitive performance of the\nproposed method.",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Vardaan Pahuja",
      "Boshi Wang",
      "Hugo Latapie",
      "Jayanth Srinivasa",
      "Yu Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09724"
  },
  {
    "id": "arXiv:2212.09726",
    "title": "Improving Faithfulness of Abstractive Summarization by Controlling  Confounding Effect of Irrelevant Sentences",
    "abstract": "Lack of factual correctness is an issue that still plagues state-of-the-art\nsummarization systems despite their impressive progress on generating seemingly\nfluent summaries. In this paper, we show that factual inconsistency can be\ncaused by irrelevant parts of the input text, which act as confounders. To that\nend, we leverage information-theoretic measures of causal effects to quantify\nthe amount of confounding and precisely quantify how they affect the\nsummarization performance. Based on insights derived from our theoretical\nresults, we design a simple multi-task model to control such confounding by\nleveraging human-annotated relevant sentences when available. Crucially, we\ngive a principled characterization of data distributions where such confounding\ncan be large thereby necessitating the use of human annotated relevant\nsentences to generate factual summaries. Our approach improves faithfulness\nscores by 20\\% over strong baselines on AnswerSumm\n\\citep{fabbri2021answersumm}, a conversation summarization dataset where lack\nof faithfulness is a significant issue due to the subjective nature of the\ntask. Our best method achieves the highest faithfulness score while also\nachieving state-of-the-art results on standard metrics like ROUGE and METEOR.\nWe corroborate these improvements through human evaluation.",
    "descriptor": "",
    "authors": [
      "Asish Ghoshal",
      "Arash Einolghozati",
      "Ankit Arun",
      "Haoran Li",
      "Lili Yu",
      "Yashar Mehdad",
      "Scott Wen-tau Yih",
      "Asli Celikyilmaz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09726"
  },
  {
    "id": "arXiv:2212.09730",
    "title": "Speaking Style Conversion With Discrete Self-Supervised Units",
    "abstract": "Voice Conversion (VC) is the task of making a spoken utterance by one speaker\nsound as if uttered by a different speaker, while keeping other aspects like\ncontent unchanged. Current VC methods, focus primarily on spectral features\nlike timbre, while ignoring the unique speaking style of people which often\nimpacts prosody. In this study, we introduce a method for converting not only\nthe timbre, but also prosodic information (i.e., rhythm and pitch changes) to\nthose of the target speaker. The proposed approach is based on a pretrained,\nself-supervised, model for encoding speech to discrete units, which make it\nsimple, effective, and easy to optimise. We consider the many-to-many setting\nwith no paired data. We introduce a suite of quantitative and qualitative\nevaluation metrics for this setup, and empirically demonstrate the proposed\napproach is significantly superior to the evaluated baselines. Code and samples\ncan be found under https://pages.cs.huji.ac.il/adiyoss-lab/dissc/ .",
    "descriptor": "",
    "authors": [
      "Gallil Maimon",
      "Yossi Adi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2212.09730"
  },
  {
    "id": "arXiv:2212.09735",
    "title": "Correspondence Distillation from NeRF-based GAN",
    "abstract": "The neural radiance field (NeRF) has shown promising results in preserving\nthe fine details of objects and scenes. However, unlike mesh-based\nrepresentations, it remains an open problem to build dense correspondences\nacross different NeRFs of the same category, which is essential in many\ndownstream tasks. The main difficulties of this problem lie in the implicit\nnature of NeRF and the lack of ground-truth correspondence annotations. In this\npaper, we show it is possible to bypass these challenges by leveraging the rich\nsemantics and structural priors encapsulated in a pre-trained NeRF-based GAN.\nSpecifically, we exploit such priors from three aspects, namely 1) a dual\ndeformation field that takes latent codes as global structural indicators, 2) a\nlearning objective that regards generator features as geometric-aware local\ndescriptors, and 3) a source of infinite object-specific NeRF samples. Our\nexperiments demonstrate that such priors lead to 3D dense correspondence that\nis accurate, smooth, and robust. We also show that established dense\ncorrespondence across NeRFs can effectively enable many NeRF-based downstream\napplications such as texture transfer.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Yushi Lan",
      "Chen Change Loy",
      "Bo Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09735"
  },
  {
    "id": "arXiv:2212.09736",
    "title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models  to Real-World Environments",
    "abstract": "A key missing ability of current language models (LMs) is grounding to\nreal-world environments. Most existing work for grounded language understanding\nuses LMs to directly generate plans that can be executed in the environment to\nachieve the desired effects. It casts the burden of ensuring grammaticality,\nfaithfulness, and controllability all on the LMs. We propose Pangu, a generic\nframework for grounded language understanding that capitalizes on the\ndiscriminative ability of LMs instead of their generative ability. Pangu\nconsists of a symbolic agent and a neural LM working in a concerted fashion:\nthe agent explores the environment to incrementally construct valid candidate\nplans, and the LM evaluates the plausibility of the candidate plans to guide\nthe search process. A case study on the challenging problem of knowledge base\nquestion answering (KBQA), which features a massive environment, demonstrates\nthe remarkable effectiveness and flexibility of Pangu: A BERT-base LM is\nsufficient for achieving a new state of the art on standard KBQA datasets, and\nlarger LMs further improve the performance by a large margin. Pangu also\nenables, for the first time, effective few-shot in-context learning for KBQA\nwith large LMs such as Codex.",
    "descriptor": "\nComments: 17 pages, 5 figures\n",
    "authors": [
      "Yu Gu",
      "Xiang Deng",
      "Yu Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.09736"
  },
  {
    "id": "arXiv:2212.09737",
    "title": "Position-guided Text Prompt for Vision-Language Pre-training",
    "abstract": "Vision-Language Pre-Training (VLP) has shown promising capabilities to align\nimage and text pairs, facilitating a broad variety of cross-modal learning\ntasks. However, we observe that VLP models often lack the visual\ngrounding/localization capability which is critical for many downstream tasks\nsuch as visual reasoning. In this work, we propose a novel Position-guided Text\nPrompt (PTP) paradigm to enhance the visual grounding ability of cross-modal\nmodels trained with VLP. Specifically, in the VLP phase, PTP divides the image\ninto $N\\times N$ blocks, and identifies the objects in each block through the\nwidely used object detector in VLP. It then reformulates the visual grounding\ntask into a fill-in-the-blank problem given a PTP by encouraging the model to\npredict the objects in the given blocks or regress the blocks of a given\nobject, e.g. filling `P\" or ``O\" in aPTP ``The block P has a O\". This mechanism\nimproves the visual grounding capability of VLP models and thus helps them\nbetter handle various downstream tasks. By introducing PTP into several\nstate-of-the-art VLP frameworks, we observe consistently significant\nimprovements across representative cross-modal learning model architectures and\nseveral benchmarks, e.g. zero-shot Flickr30K Retrieval (+4.8 in average\nrecall@1) for ViLT \\cite{vilt} baseline, and COCO Captioning (+5.3 in CIDEr)\nfor SOTA BLIP \\cite{blip} baseline. Moreover, PTP achieves comparable results\nwith object-detector based methods, and much faster inference speed since PTP\ndiscards its object detector for inference while the later cannot. Our code and\npre-trained weight will be released at \\url{https://github.com/sail-sg/ptp}.",
    "descriptor": "\nComments: Work in progress, code is in this https URL\n",
    "authors": [
      "Alex Jinpeng Wang",
      "Pan Zhou",
      "Mike Zheng Shou",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09737"
  },
  {
    "id": "arXiv:2212.09739",
    "title": "LENS: A Learnable Evaluation Metric for Text Simplification",
    "abstract": "Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets in text simplification are limited\nby a lack of annotations, unitary simplification types, and outdated models,\nmaking them unsuitable for this approach. To address these issues, we introduce\nthe SIMPEVAL corpus that contains: SIMPEVAL_ASSET, comprising 12K human ratings\non 2.4K simplifications of 24 systems, and SIMPEVAL_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including generations from GPT-3.5. Training on SIMPEVAL_ASSET,\nwe present LENS, a Learnable Evaluation Metric for Text Simplification.\nExtensive empirical results show that LENS correlates better with human\njudgment than existing metrics, paving the way for future progress in the\nevaluation of text simplification. To create the SIMPEVAL datasets, we\nintroduce RANK & RATE, a human evaluation framework that rates simplifications\nfrom several models in a list-wise manner by leveraging an interactive\ninterface, which ensures both consistency and accuracy in the evaluation\nprocess. Our metric, dataset, and annotation toolkit are available at\nhttps://github.com/Yao-Dou/LENS.",
    "descriptor": "",
    "authors": [
      "Mounica Maddela",
      "Yao Dou",
      "David Heineman",
      "Wei Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09739"
  },
  {
    "id": "arXiv:2212.09741",
    "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
    "abstract": "We introduce INSTRUCTOR, a new method for computing text embeddings given\ntask instructions: every text input is embedded together with instructions\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\ncan generate text embeddings tailored to different downstream tasks and\ndomains, without any further training. We first annotate instructions for 330\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\nunseen during training), ranging from classification and information retrieval\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\nwhile having an order of magnitude fewer parameters than the previous best\nmodel, achieves state-of-the-art performance, with an average improvement of\n3.4% compared to the previous best results on the 70 diverse datasets. Our\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\nthat instruction finetuning mitigates the challenge of training a single model\non diverse datasets. Our model, code, and data are available at\nhttps://instructor-embedding.github.io.",
    "descriptor": "",
    "authors": [
      "Hongjin Su",
      "Weijia Shi",
      "Jungo Kasai",
      "Yizhong Wang",
      "Yushi Hu",
      "Mari Ostendorf",
      "Wen-tau Yih",
      "Noah A. Smith",
      "Luke Zettlemoyer",
      "Tao Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09741"
  },
  {
    "id": "arXiv:2212.09744",
    "title": "DSI++: Updating Transformer Memory with New Documents",
    "abstract": "Differentiable Search Indices (DSIs) encode a corpus of documents in the\nparameters of a model and use the same model to map queries directly to\nrelevant document identifiers. Despite the strong performance of DSI models,\ndeploying them in situations where the corpus changes over time is\ncomputationally expensive because reindexing the corpus requires re-training\nthe model. In this work, we introduce DSI++, a continual learning challenge for\nDSI to incrementally index new documents while being able to answer queries\nrelated to both previously and newly indexed documents. Across different model\nscales and document identifier representations, we show that continual indexing\nof new documents leads to considerable forgetting of previously indexed\ndocuments. We also hypothesize and verify that the model experiences forgetting\nevents during training, leading to unstable learning. To mitigate these issues,\nwe investigate two approaches. The first focuses on modifying the training\ndynamics. Flatter minima implicitly alleviate forgetting, so we optimize for\nflatter loss basins and show that the model stably memorizes more documents\n(+12\\%). Next, we introduce a generative memory to sample pseudo-queries for\ndocuments and supplement them during continual indexing to prevent forgetting\nfor the retrieval task. Extensive experiments on novel continual indexing\nbenchmarks based on Natural Questions (NQ) and MS MARCO demonstrate that our\nproposed solution mitigates forgetting by a significant margin. Concretely, it\nimproves the average Hits@10 by $+21.1\\%$ over competitive baselines for NQ and\nrequires $6$ times fewer model updates compared to re-training the DSI model\nfor incrementally indexing five corpora in a sequence.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Sanket Vaibhav Mehta",
      "Jai Gupta",
      "Yi Tay",
      "Mostafa Dehghani",
      "Vinh Q. Tran",
      "Jinfeng Rao",
      "Marc Najork",
      "Emma Strubell",
      "Donald Metzler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09744"
  },
  {
    "id": "arXiv:2212.09746",
    "title": "Evaluating Human-Language Model Interaction",
    "abstract": "Many real-world applications of language models (LMs), such as code\nautocomplete and writing assistance, involve human-LM interaction. However, the\nmain LM benchmarks are non-interactive in that a system produces output without\nhuman involvement. To evaluate human-LM interaction, we develop a new\nframework, Human-AI Language-based Interaction Evaluation (HALIE), that expands\nnon-interactive evaluation along three dimensions, capturing (i) the\ninteractive process, not only the final output; (ii) the first-person\nsubjective experience, not just a third-party assessment; and (iii) notions of\npreference beyond quality. We then design five tasks ranging from goal-oriented\nto open-ended to capture different forms of interaction. On four\nstate-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21's J1-Jumbo), we\nfind that non-interactive performance does not always result in better human-LM\ninteraction and that first-person and third-party metrics can diverge,\nsuggesting the importance of examining the nuances of human-LM interaction.",
    "descriptor": "",
    "authors": [
      "Mina Lee",
      "Megha Srivastava",
      "Amelia Hardy",
      "John Thickstun",
      "Esin Durmus",
      "Ashwin Paranjape",
      "Ines Gerard-Ursin",
      "Xiang Lisa Li",
      "Faisal Ladhak",
      "Frieda Rong",
      "Rose E. Wang",
      "Minae Kwon",
      "Joon Sung Park",
      "Hancheng Cao",
      "Tony Lee",
      "Rishi Bommasani",
      "Michael Bernstein",
      "Percy Liang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09746"
  },
  {
    "id": "arXiv:2212.09747",
    "title": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?",
    "abstract": "Named Entity Recognition (NER) is an important and well-studied task in\nnatural language processing. The classic CoNLL-2003 English dataset, published\nalmost 20 years ago, is commonly used to train and evaluate named entity\ntaggers. The age of this dataset raises the question of how well these models\nperform when applied to modern data. In this paper, we present CoNLL++, a new\nannotated test set that mimics the process used to create the original\nCoNLL-2003 test set as closely as possible, except with data collected from\n2020. Using CoNLL++, we evaluate the generalization of 20+ different models to\nmodern data. We observe that different models have very different\ngeneralization behavior. F\\textsubscript{1} scores of large transformer-based\nmodels which are pre-trained on recent data dropped much less than models using\nstatic word embeddings, and RoBERTa-based and T5 models achieve comparable\nF\\textsubscript{1} scores on both CoNLL-2003 and CoNLL++. Our experiments show\nthat achieving good generalizability requires a combined effort of developing\nlarger models and continuing pre-training with in-domain and recent data. These\nresults suggest standard evaluation methodology may have under-estimated\nprogress on named entity recognition over the past 20 years; in addition to\nimproving performance on the original CoNLL-2003 dataset, we have also improved\nthe ability of our models to generalize to modern data.",
    "descriptor": "",
    "authors": [
      "Shuheng Liu",
      "Alan Ritter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.09747"
  },
  {
    "id": "arXiv:2212.09748",
    "title": "Scalable Diffusion Models with Transformers",
    "abstract": "We explore a new class of diffusion models based on the transformer\narchitecture. We train latent diffusion models of images, replacing the\ncommonly-used U-Net backbone with a transformer that operates on latent\npatches. We analyze the scalability of our Diffusion Transformers (DiTs)\nthrough the lens of forward pass complexity as measured by Gflops. We find that\nDiTs with higher Gflops -- through increased transformer depth/width or\nincreased number of input tokens -- consistently have lower FID. In addition to\npossessing good scalability properties, our largest DiT-XL/2 models outperform\nall prior diffusion models on the class-conditional ImageNet 512x512 and\n256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
    "descriptor": "\nComments: Code, project page and videos available at this https URL\n",
    "authors": [
      "William Peebles",
      "Saining Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09748"
  },
  {
    "id": "arXiv:2211.03101",
    "title": "The effect of the processing and measurement operators on the expressive  power of quantum models",
    "abstract": "There is an increasing interest in Quantum Machine Learning (QML) models, how\nthey work and for which applications they could be useful. There have been many\ndifferent proposals on how classical data can be encoded and what circuit\nans\\\"atze and measurement operators should be used to process the encoded data\nand measure the output state of an ansatz. The choice of the aforementioned\noperators plays a determinant role in the expressive power of the QML model. In\nthis work we investigate how certain changes in the circuit structure change\nthis expressivity. We introduce both numerical and analytical tools to explore\nthe effect that these operators have in the overall performance of the QML\nmodel. These tools are based on previous work on the teacher-student scheme,\nthe partial Fourier series and the averaged operator size. We focus our\nanalysis on simple QML models with two and three qubits and observe that\nincreasing the number of parameterized and entangling gates leads to a more\nexpressive model for certain circuit structures. Also, on which qubit the\nmeasurement is performed affects the type of functions that QML models could\nlearn. This work sketches the determinant role that the processing and\nmeasurement operators have on the expressive power of simple quantum circuits.",
    "descriptor": "",
    "authors": [
      "Aikaterini",
      "Gratsea",
      "Patrick Huembeli"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2211.03101"
  },
  {
    "id": "arXiv:2212.07079",
    "title": "A novel state connection strategy for quantum computing to represent and  compress digital images",
    "abstract": "Quantum image processing draws a lot of attention due to faster data\ncomputation and storage compared to classical data processing systems.\nConverting classical image data into the quantum domain and state label\npreparation complexity is still a challenging issue. The existing techniques\nnormally connect the pixel values and the state position directly. Recently,\nthe EFRQI (efficient flexible representation of the quantum image) approach\nuses an auxiliary qubit that connects the pixel-representing qubits to the\nstate position qubits via Toffoli gates to reduce state connection. Due to the\ntwice use of Toffoli gates for each pixel connection still it requires a\nsignificant number of bits to connect each pixel value. In this paper, we\npropose a new SCMFRQI (state connection modification FRQI) approach for further\nreducing the required bits by modifying the state connection using a reset gate\nrather than repeating the use of the same Toffoli gate connection as a reset\ngate. Moreover, unlike other existing methods, we compress images using\nblock-level for further reduction of required qubits. The experimental results\nconfirm that the proposed method outperforms the existing methods in terms of\nboth image representation and compression points of view.",
    "descriptor": "\nComments: 8 pages, conference\n",
    "authors": [
      "Md Ershadul Haque",
      "Manoranjan Paul",
      "Tanmoy Debnath"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2212.07079"
  },
  {
    "id": "arXiv:2212.08674",
    "title": "An unfolding method based on conditional Invertible Neural Networks  (cINN) using iterative training",
    "abstract": "The unfolding of detector effects is crucial for the comparison of data to\ntheory predictions. While traditional methods are limited to representing the\ndata in a low number of dimensions, machine learning has enabled new unfolding\ntechniques while retaining the full dimensionality. Generative networks like\ninvertible neural networks~(INN) enable a probabilistic unfolding, which map\nindividual events to their corresponding unfolded probability distribution. The\naccuracy of such methods is however limited by how well simulated training\nsamples model the actual data that is unfolded. We introduce the iterative\nconditional INN~(IcINN) for unfolding that adjusts for deviations between\nsimulated training samples and data. The IcINN unfolding is first validated on\ntoy data and then applied to pseudo-data for the $pp \\to Z \\gamma \\gamma$\nprocess.",
    "descriptor": "\nComments: 21 pages, 13 figures\n",
    "authors": [
      "Mathias Backes",
      "Anja Butter",
      "Monica Dunford",
      "Bogdan Malaescu"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2212.08674"
  },
  {
    "id": "arXiv:2212.08678",
    "title": "A super-polynomial quantum advantage for combinatorial optimization  problems",
    "abstract": "Combinatorial optimization - a field of research addressing problems that\nfeature strongly in a wealth of practical and industrial contexts - has been\nidentified as one of the core potential fields of applicability of near-term\nquantum computers. It is still unclear, however, to what extent variational\nquantum algorithms can actually outperform classical algorithms for this type\nof problems. In this work, by resorting to computational learning theory and\ncryptographic notions, we prove that fault-tolerant quantum computers feature a\nsuper-polynomial advantage over classical computers in approximating solutions\nto combinatorial optimization problems. Specifically, building on seminal work\nof Kearns and Valiant, we construct special instances of the integer\nprogramming problem (which in its most general form is NP-complete) that we\nprove to be hard-to-approximate classically but give an efficient quantum\nalgorithm to approximate the optimal solution of those instances, hence showing\na super-polynomial quantum advantage. This result shows that quantum devices\nhave the power to approximate combinatorial optimization solutions beyond the\nreach of classical efficient algorithms.",
    "descriptor": "\nComments: 22 pages, 5 figures\n",
    "authors": [
      "Niklas Pirnay",
      "Vincent Ulitzsch",
      "Frederik Wilde",
      "Jens Eisert",
      "Jean-Pierre Seifert"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Other Condensed Matter (cond-mat.other)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2212.08678"
  },
  {
    "id": "arXiv:2212.08703",
    "title": "Fast Entropy-Based Methods of Word-Level Confidence Estimation for  End-To-End Automatic Speech Recognition",
    "abstract": "This paper presents a class of new fast non-trainable entropy-based\nconfidence estimation methods for automatic speech recognition. We show how\nper-frame entropy values can be normalized and aggregated to obtain a\nconfidence measure per unit and per word for Connectionist Temporal\nClassification (CTC) and Recurrent Neural Network Transducer (RNN-T) models.\nProposed methods have similar computational complexity to the traditional\nmethod based on the maximum per-frame probability, but they are more\nadjustable, have a wider effective threshold range, and better push apart the\nconfidence distributions of correct and incorrect words. We evaluate the\nproposed confidence measures on LibriSpeech test sets, and show that they are\nup to 2 and 4 times better than confidence estimation based on the maximum\nper-frame probability at detecting incorrect words for Conformer-CTC and\nConformer-RNN-T models, respectively.",
    "descriptor": "\nComments: To appear in Proc. SLT 2022, Jan 09-12, 2023, Doha, Qatar. 8 pages, 4 figures, 4 tables\n",
    "authors": [
      "Aleksandr Laptev",
      "Boris Ginsburg"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08703"
  },
  {
    "id": "arXiv:2212.08740",
    "title": "Lateral Strain Imaging using Self-supervised and Physically Inspired  Constraints in Unsupervised Regularized Elastography",
    "abstract": "Convolutional Neural Networks (CNN) have shown promising results for\ndisplacement estimation in UltraSound Elastography (USE). Many modifications\nhave been proposed to improve the displacement estimation of CNNs for USE in\nthe axial direction. However, the lateral strain, which is essential in several\ndownstream tasks such as the inverse problem of elasticity imaging, remains a\nchallenge. The lateral strain estimation is complicated since the motion and\nthe sampling frequency in this direction are substantially lower than the axial\none, and a lack of carrier signal in this direction. In computer vision\napplications, the axial and the lateral motions are independent. In contrast,\nthe tissue motion pattern in USE is governed by laws of physics which link the\naxial and lateral displacements. In this paper, inspired by Hooke's law, we\nfirst propose Physically Inspired ConsTraint for Unsupervised Regularized\nElastography (PICTURE), where we impose a constraint on the Effective Poisson's\nratio (EPR) to improve the lateral strain estimation. In the next step, we\npropose self-supervised PICTURE (sPICTURE) to further enhance the strain image\nestimation. Extensive experiments on simulation, experimental phantom and in\nvivo data demonstrate that the proposed methods estimate accurate axial and\nlateral strain maps.",
    "descriptor": "\nComments: Accepted in IEEE Transactions on Medical Imaging (TMI)\n",
    "authors": [
      "Ali K. Z. Tehrani",
      "Md Ashikuzzaman",
      "Hassan Rivaz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08740"
  },
  {
    "id": "arXiv:2212.08771",
    "title": "Assign Experiment Variants at Scale in Online Controlled Experiments",
    "abstract": "Online controlled experiments (A/B tests) have become the gold standard for\nlearning the impact of new product features in technology companies.\nRandomization enables the inference of causality from an A/B test. The\nrandomized assignment maps end users to experiment buckets and balances user\ncharacteristics between the groups. Therefore, experiments can attribute any\noutcome differences between the experiment groups to the product feature under\nexperiment. Technology companies run A/B tests at scale -- hundreds if not\nthousands of A/B tests concurrently, each with millions of users. The large\nscale poses unique challenges to randomization. First, the randomized\nassignment must be fast since the experiment service receives hundreds of\nthousands of queries per second. Second, the variant assignments must be\nindependent between experiments. Third, the assignment must be consistent when\nusers revisit or an experiment enrolls more users. We present a novel\nassignment algorithm and statistical tests to validate the randomized\nassignments. Our results demonstrate that not only is this algorithm\ncomputationally fast but also satisfies the statistical requirements --\nunbiased and independent.",
    "descriptor": "",
    "authors": [
      "Qike Li",
      "Samir Jamkhande",
      "Pavel Kochetkov",
      "Pai Liu"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08771"
  },
  {
    "id": "arXiv:2212.08791",
    "title": "Two-Scale Gradient Descent Ascent Dynamics Finds Mixed Nash Equilibria  of Continuous Games: A Mean-Field Perspective",
    "abstract": "Finding the mixed Nash equilibria (MNE) of a two-player zero sum continuous\ngame is an important and challenging problem in machine learning. A canonical\nalgorithm to finding the MNE is the noisy gradient descent ascent method which\nin the infinite particle limit gives rise to the {\\em Mean-Field Gradient\nDescent Ascent} (GDA) dynamics on the space of probability measures. In this\npaper, we first study the convergence of a two-scale Mean-Field GDA dynamics\nfor finding the MNE of the entropy-regularized objective. More precisely we\nshow that for any fixed positive temperature (or regularization parameter), the\ntwo-scale Mean-Field GDA with a {\\em finite} scale ratio converges to\nexponentially to the unique MNE without assuming the convexity or concavity of\nthe interaction potential. The key ingredient of our proof lies in the\nconstruction of new Lyapunov functions that dissipate exponentially along the\nMean-Field GDA. We further study the simulated annealing of the Mean-Field GDA\ndynamics. We show that with a temperature schedule that decays logarithmically\nin time the annealed Mean-Field GDA converges to the MNE of the original\nunregularized objective function.",
    "descriptor": "",
    "authors": [
      "Yulong Lu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.08791"
  },
  {
    "id": "arXiv:2212.08810",
    "title": "Shape Aware Automatic Region-of-Interest Subdivisions",
    "abstract": "In a wide variety of fields, analysis of images involves defining a region\nand measuring its inherent properties. Such measurements include a region's\nsurface area, curvature, volume, average gray and/or color scale, and so on.\nFurthermore, the subsequent subdivision of these regions is sometimes\nperformed. These subdivisions are then used to measure local information, at\neven finer scales. However, simple griding or manual editing methods are\ntypically used to subdivide a region into smaller units. The resulting\nsubdivisions can therefore either not relate well to the actual shape or\nproperty of the region being studied (i.e., gridding methods), or be time\nconsuming and based on user subjectivity (i.e., manual methods). The method\ndiscussed in this work extracts subdivisional units based on a region's general\nshape information. We present the results of applying our method to the medical\nimage analysis of nested regions-of-interest of myocardial wall, where the\nsubdivisions are used to study temporal and/or spatial heterogeneity of\nmyocardial perfusion. This method is of particular interest for creating\nsubdivision regions-of-interest (SROIs) when no variable intensity or other\ncriteria within a region need be used to separate a particular region into\nsubunits.",
    "descriptor": "",
    "authors": [
      "Timothy L. Kline"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08810"
  },
  {
    "id": "arXiv:2212.08811",
    "title": "Toward BCI-enabled Metaverse: A Joint Radio and Computing Resource  Allocation Approach",
    "abstract": "Toward user-driven Metaverse applications with fast wireless connectivity and\ntremendous computing demand through future 6G infrastructures, we propose a\nBrain-Computer Interface (BCI) enabled framework that paves the way for the\ncreation of intelligent human-like avatars. Our approach takes a first step\ntoward the Metaverse systems in which the digital avatars are envisioned to be\nmore intelligent by collecting and analyzing brain signals through cellular\nnetworks. In our proposed system, Metaverse users experience Metaverse\napplications while sending their brain signals via uplink wireless channels in\norder to create intelligent human-like avatars at the base station. As such,\nthe digital avatars can not only give useful recommendations for the users but\nalso enable the system to create user-driven applications. Our proposed\nframework involves a mixed decision-making and classification problem in which\nthe base station has to allocate its computing and radio resources to the users\nand classify the brain signals of users in an efficient manner. To this end, we\npropose a hybrid training algorithm that utilizes recent advances in deep\nreinforcement learning to address the problem. Specifically, our hybrid\ntraining algorithm contains three deep neural networks cooperating with each\nother to enable better realization of the mixed decision-making and\nclassification problem. Simulation results show that our proposed framework can\njointly address resource allocation for the system and classify brain signals\nof the users with highly accurate predictions.",
    "descriptor": "",
    "authors": [
      "Nguyen Quang Hieu",
      "Dinh Thai Hoang",
      "Diep N. Nguyen",
      "Eryk Dutkiewicz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2212.08811"
  },
  {
    "id": "arXiv:2212.08826",
    "title": "Molecule optimization via multi-objective evolutionary in implicit  chemical space",
    "abstract": "Machine learning methods have been used to accelerate the molecule\noptimization process. However, efficient search for optimized molecules\nsatisfying several properties with scarce labeled data remains a challenge for\nmachine learning molecule optimization. In this study, we propose MOMO, a\nmulti-objective molecule optimization framework to address the challenge by\ncombining learning of chemical knowledge with Pareto-based multi-objective\nevolutionary search. To learn chemistry, it employs a self-supervised codec to\nconstruct an implicit chemical space and acquire the continues representation\nof molecules. To explore the established chemical space, MOMO uses\nmulti-objective evolution to comprehensively and efficiently search for similar\nmolecules with multiple desirable properties. We demonstrate the high\nperformance of MOMO on four multi-objective property and similarity\noptimization tasks, and illustrate the search capability of MOMO through case\nstudies. Remarkably, our approach significantly outperforms previous approaches\nin optimizing three objectives simultaneously. The results show the\noptimization capability of MOMO, suggesting to improve the success rate of lead\nmolecule optimization.",
    "descriptor": "\nComments: 38 pages, 6 figures, 74 conferences\n",
    "authors": [
      "Xin Xia",
      "Yansen Su",
      "Chunhou Zheng",
      "Xiangxiang Zeng"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.08826"
  },
  {
    "id": "arXiv:2212.08842",
    "title": "Modeling, scientific computing and optimal control for renewable energy  systems with storage",
    "abstract": "This paper presents models for renewable energy systems with storage, and\nconsiders its optimal operation. We model and simulate wind and solar power\nproduction using stochastic differential equations as well as storage of the\nproduced power using batteries, thermal storage, and water electrolysis. We\nformulate an economic optimal control problem, with the scope of controlling\nthe system in the most efficient way, while satisfying the power demand from\nthe electric grid. Deploying multiple storage systems allows flexibility and\nhigher reliability of the renewable energy system.",
    "descriptor": "\nComments: Submitted to European Control Conference 2023\n",
    "authors": [
      "Nicola Cantisani",
      "Tobias K. S. Ritschel",
      "Christian A. Thilker",
      "Henrik Madsen",
      "John Bagterp J\u00f8rgensen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.08842"
  },
  {
    "id": "arXiv:2212.08852",
    "title": "Unrolling SVT to obtain computationally efficient SVT for n-qubit  quantum state tomography",
    "abstract": "Quantum state tomography aims to estimate the state of a quantum mechanical\nsystem which is described by a trace one, Hermitian positive semidefinite\ncomplex matrix, given a set of measurements of the state. Existing works focus\non estimating the density matrix that represents the state, using a compressive\nsensing approach, with only fewer measurements than that required for a\ntomographically complete set, with the assumption that the true state has a low\nrank. One very popular method to estimate the state is the use of the Singular\nValue Thresholding (SVT) algorithm. In this work, we present a machine learning\napproach to estimate the quantum state of n-qubit systems by unrolling the\niterations of SVT which we call Learned Quantum State Tomography (LQST). As\nmerely unrolling SVT may not ensure that the output of the network meets the\nconstraints required for a quantum state, we design and train a custom neural\nnetwork whose architecture is inspired from the iterations of SVT with\nadditional layers to meet the required constraints. We show that our proposed\nLQST with very few layers reconstructs the density matrix with much better\nfidelity than the SVT algorithm which takes many hundreds of iterations to\nconverge. We also demonstrate the reconstruction of the quantum Bell state from\nan informationally incomplete set of noisy measurements.",
    "descriptor": "\nComments: A preliminary version of this paper, [1] is uploaded to arXiv which is not tailored to quantum state estimation. The paper [1] deals with the estimation of a general low-rank real matrix whereas the current paper is tailored for estimating the quantum state of n-qubit systems and the current paper is applied to estimating the Bell state from its noisy measurements. [1] arXiv:2105.06934\n",
    "authors": [
      "Siva Shanmugam",
      "Sheetal Kalyani"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2212.08852"
  },
  {
    "id": "arXiv:2212.08877",
    "title": "Multilayer structure enhances the optimal outcome of coordination games",
    "abstract": "We study mechanisms of synchronisation, coordination, and equilibrium\nselection in two-player coordination games on multilayer networks. We apply the\napproach from evolutionary game theory with three possible update rules: the\nreplicator dynamics (RD), the best response (BR), and the unconditional\nimitation (UI). Players interact on a two-layer random regular network. The\npopulation on each layer plays a different game, with layer I preferring the\nopposite strategy to layer II. We measure the difference between the two games\nplayed on the layers by a difference in payoffs $\\Delta S$ while the\ninter-connectedness is measured by a node overlap parameter $q$. We discover a\ncritical value $q_c(\\Delta S)$ below which layers do not synchronise. For\n$q>q_c$ in general both layers coordinate on the same strategy. Surprisingly,\nthere is a symmetry breaking in the selection of equilibrium -- for RD and UI\nthere is a phase where only the payoff-dominant equilibrium is selected. Our\nwork is an example of previously observed differences between the update rules\non a single network. However, we took a novel approach with the game being\nplayed on two inter-connected layers. As we show, the multilayer structure\nenhances the abundance of the Pareto-optimal equilibrium in coordination games\nwith imitative update rules.",
    "descriptor": "",
    "authors": [
      "Tomasz Raducha",
      "Maxi San Miguel"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2212.08877"
  },
  {
    "id": "arXiv:2212.08901",
    "title": "Learning with linear mixed model for group recommendation systems",
    "abstract": "Accurate prediction of users' responses to items is one of the main aims of\nmany computational advising applications. Examples include recommending movies,\nnews articles, songs, jobs, clothes, books and so forth. Accurate prediction of\ninactive users' responses still remains a challenging problem for many\napplications. In this paper, we explore the linear mixed model in\nrecommendation system. The recommendation process is naturally modelled as the\nmixed process between objective effects (fixed effects) and subjective effects\n(random effects). The latent association between the subjective effects and the\nusers' responses can be mined through the restricted maximum likelihood method.\nIt turns out the linear mixed models can collaborate items' attributes and\nusers' characteristics naturally and effectively. While this model cannot\nproduce the most precisely individual level personalized recommendation, it is\nrelative fast and accurate for group (users)/class (items) recommendation.\nNumerical examples on GroupLens benchmark problems are presented to show the\neffectiveness of this method.",
    "descriptor": "\nComments: 5 pages, 9 figures, published\n",
    "authors": [
      "Baode Gao",
      "Guangpeng Zhan",
      "Hanzhang Wang",
      "Yiming Wang",
      "Shengxin Zhu"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.08901"
  },
  {
    "id": "arXiv:2212.08941",
    "title": "The Calder\u00f3n's problem via DeepONets",
    "abstract": "We consider the Dirichlet-to-Neumann operator and the Calder\\'on's mapping\nappearing in the Inverse Problem of recovering a smooth bounded and positive\nisotropic conductivity of a material filling a smooth bounded domain in space.\nUsing deep learning techniques, we prove that this map is rigorously\napproximated by DeepONets, infinite-dimensional counterparts of standard\nartificial neural networks.",
    "descriptor": "\nComments: 24 pp.; contribution to the special issue dedicated to Carlos Kenig's 70th birthday\n",
    "authors": [
      "Javier Castro",
      "Claudio Mu\u00f1oz",
      "Nicol\u00e1s Valenzuela"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.08941"
  },
  {
    "id": "arXiv:2212.08959",
    "title": "Natural bijections for contiguous pattern avoidance in words",
    "abstract": "Two words $p$ and $q$ are avoided by the same number of length-$n$ words, for\nall $n$, precisely when $p$ and $q$ have the same set of border lengths.\nHowever, known proofs of this result use generating functions and do not\nprovide explicit bijections. We establish a natural bijection from the set of\nwords avoiding $p$ to the set of words avoiding $q$ in the case that $p$ and\n$q$ have the same set of proper borders.",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Julia Carrigan",
      "Isaiah Hollars",
      "Eric Rowland"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2212.08959"
  },
  {
    "id": "arXiv:2212.09058",
    "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
    "abstract": "The massive growth of self-supervised learning (SSL) has been witnessed in\nlanguage, vision, speech, and audio domains over the past few years. While\ndiscrete label prediction is widely adopted for other modalities, the\nstate-of-the-art audio SSL models still employ reconstruction loss for\npre-training. Compared with reconstruction loss, semantic-rich discrete label\nprediction encourages the SSL model to abstract the high-level audio semantics\nand discard the redundant details as in human perception. However, a\nsemantic-rich acoustic tokenizer for general audio pre-training is usually not\nstraightforward to obtain, due to the continuous property of audio and\nunavailable phoneme sequences like speech. To tackle this challenge, we propose\nBEATs, an iterative audio pre-training framework to learn Bidirectional Encoder\nrepresentation from Audio Transformers, where an acoustic tokenizer and an\naudio SSL model are optimized by iterations. In the first iteration, we use\nrandom projection as the acoustic tokenizer to train an audio SSL model in a\nmask and label prediction manner. Then, we train an acoustic tokenizer for the\nnext iteration by distilling the semantic knowledge from the pre-trained or\nfine-tuned audio SSL model. The iteration is repeated with the hope of mutual\npromotion of the acoustic tokenizer and audio SSL model. The experimental\nresults demonstrate our acoustic tokenizers can generate discrete labels with\nrich audio semantics and our audio SSL models achieve state-of-the-art results\nacross various audio classification benchmarks, even outperforming previous\nmodels that use more training data and model parameters significantly.\nSpecifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for\naudio-only models without using any external data, and 98.1% accuracy on\nESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
    "descriptor": "",
    "authors": [
      "Sanyuan Chen",
      "Yu Wu",
      "Chengyi Wang",
      "Shujie Liu",
      "Daniel Tompkins",
      "Zhuo Chen",
      "Furu Wei"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2212.09058"
  },
  {
    "id": "arXiv:2212.09060",
    "title": "Parsing as a lifting problem and the Chomsky-Sch\u00fctzenberger  representation theorem",
    "abstract": "We begin by explaining how any context-free grammar encodes a functor of\noperads from a freely generated operad into a certain \"operad of spliced\nwords\". This motivates a more general notion of CFG over any category $C$,\ndefined as a finite species $S$ equipped with a color denoting the start symbol\nand a functor of operads $p : Free[S] \\to W[C]$ into the operad of spliced\narrows in $C$. We show that many standard properties of CFGs can be formulated\nwithin this framework, and that usual closure properties of CF languages\ngeneralize to CF languages of arrows. We also discuss a dual fibrational\nperspective on the functor $p$ via the notion of \"displayed\" operad,\ncorresponding to a lax functor of operads $W[C] \\to Span(Set)$.\nWe then turn to the Chomsky-Sch\\\"utzenberger Representation Theorem. We\ndescribe how a non-deterministic finite state automaton can be seen as a\ncategory $Q$ equipped with a pair of objects denoting initial and accepting\nstates and a functor of categories $Q \\to C$ satisfying the unique lifting of\nfactorizations property and the finite fiber property. Then, we explain how to\nextend this notion of automaton to functors of operads, which generalize tree\nautomata, allowing us to lift an automaton over a category to an automaton over\nits operad of spliced arrows. We show that every CFG over a category can be\npulled back along a ND finite state automaton over the same category, and hence\nthat CF languages are closed under intersection with regular languages. The\nlast important ingredient is the identification of a left adjoint $C[-] :\nOperad \\to Cat$ to the operad of spliced arrows functor, building the \"contour\ncategory\" of an operad. Using this, we generalize the C-S representation\ntheorem, proving that any context-free language of arrows over a category $C$\nis the functorial image of the intersection of a $C$-chromatic tree contour\nlanguage and a regular language.",
    "descriptor": "\nComments: proceedings of MFPS 2022\n",
    "authors": [
      "Paul-Andr\u00e9 Melli\u00e8s",
      "Noam Zeilberger"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2212.09060"
  },
  {
    "id": "arXiv:2212.09066",
    "title": "Property of upper bounds on the number of rich words",
    "abstract": "A finite word $w$ is called \\emph{rich} if it contains $\\vert w\\vert+1$\ndistinct palindromic factors including the empty word. Let $q\\geq 2$ be the\nsize of the alphabet. Let $R(n)$ be the number of rich words of length $n$. Let\n$d>1$ be a real constant and let $\\phi, \\psi$ be real functions such that\n\\begin{itemize}\\item there is $n_0$ such that $2\\psi(2^{-1}\\phi(n))\\geq\nd\\psi(n)$ for all $n>n_0$, \\item $\\frac{n}{\\phi(n)}$ is an upper bound on the\npalindromic length of rich words of length $n$, and \\item\n$\\frac{x}{\\psi(x)}+\\frac{x\\ln{(\\phi(x))}}{\\phi(x)}$ is a strictly increasing\nconcave function. \\end{itemize} We show that if $c_1,c_2$ are real constants\nand $R(n)\\leq q^{c_1\\frac{n}{\\psi(n)}+c_2\\frac{n\\ln(\\phi(n))}{\\phi(n)}}$ then\nfor every real constant $c_3>0$ there is a positive integer $n_0$ such that for\nall $n>n_0$ we have that \\[R(n)\\leq\nq^{(c_1+c_3)\\frac{n}{d\\psi(n)}+c_2\\frac{n\\ln(\\phi(n))}{\\phi(n)}(1+\\frac{1}{c_2\\ln{q}}+c_3)}\\mbox{.}\\]",
    "descriptor": "",
    "authors": [
      "Josef Rukavicka"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2212.09066"
  },
  {
    "id": "arXiv:2212.09073",
    "title": "Upper Bounds on the Distillable Randomness of Bipartite Quantum States",
    "abstract": "The distillable randomness of a bipartite quantum state is an\ninformation-theoretic quantity equal to the largest net rate at which shared\nrandomness can be distilled from the state by means of local operations and\nclassical communication. This quantity has been widely used as a measure of\nclassical correlations, and one version of it is equal to the regularized\nHolevo information of the ensemble that results from measuring one share of the\nstate. However, due to the regularization, the distillable randomness is\ndifficult to compute in general. To address this problem, we define measures of\nclassical correlations and prove a number of their properties, most importantly\nthat they serve as upper bounds on the distillable randomness of an arbitrary\nbipartite state. We then further bound these measures from above by some that\nare efficiently computable by means of semi-definite programming, we evaluate\none of them for the example of an isotropic state, and we remark on the\nrelation to quantities previously proposed in the literature.",
    "descriptor": "\nComments: 11 pages, 1 figure, submission to the 2023 IEEE Information Theory Workshop, to take place in the walled city of Saint-Malo, France\n",
    "authors": [
      "Ludovico Lami",
      "Bartosz Regula",
      "Xin Wang",
      "Mark M. Wilde"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09073"
  },
  {
    "id": "arXiv:2212.09081",
    "title": "Riemannian Optimization for Variance Estimation in Linear Mixed Models",
    "abstract": "Variance parameter estimation in linear mixed models is a challenge for many\nclassical nonlinear optimization algorithms due to the positive-definiteness\nconstraint of the random effects covariance matrix. We take a completely novel\nview on parameter estimation in linear mixed models by exploiting the intrinsic\ngeometry of the parameter space. We formulate the problem of residual maximum\nlikelihood estimation as an optimization problem on a Riemannian manifold.\nBased on the introduced formulation, we give geometric higher-order information\non the problem via the Riemannian gradient and the Riemannian Hessian. Based on\nthat, we test our approach with Riemannian optimization algorithms numerically.\nOur approach yields a higher quality of the variance parameter estimates\ncompared to existing approaches.",
    "descriptor": "",
    "authors": [
      "Lena Sembach",
      "Jan Pablo Burgard",
      "Volker H. Schulz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2212.09081"
  },
  {
    "id": "arXiv:2212.09108",
    "title": "A Permutation-Free Kernel Independence Test",
    "abstract": "In nonparametric independence testing, we observe i.i.d.\\ data\n$\\{(X_i,Y_i)\\}_{i=1}^n$, where $X \\in \\mathcal{X}, Y \\in \\mathcal{Y}$ lie in\nany general spaces, and we wish to test the null that $X$ is independent of\n$Y$. Modern test statistics such as the kernel Hilbert-Schmidt Independence\nCriterion (HSIC) and Distance Covariance (dCov) have intractable null\ndistributions due to the degeneracy of the underlying U-statistics. Thus, in\npractice, one often resorts to using permutation testing, which provides a\nnonasymptotic guarantee at the expense of recalculating the quadratic-time\nstatistics (say) a few hundred times. This paper provides a simple but\nnontrivial modification of HSIC and dCov (called xHSIC and xdCov, pronounced\n``cross'' HSIC/dCov) so that they have a limiting Gaussian distribution under\nthe null, and thus do not require permutations. This requires building on the\nnewly developed theory of cross U-statistics by Kim and Ramdas (2020), and in\nparticular developing several nontrivial extensions of the theory in Shekhar et\nal. (2022), which developed an analogous permutation-free kernel two-sample\ntest. We show that our new tests, like the originals, are consistent against\nfixed alternatives, and minimax rate optimal against smooth local alternatives.\nNumerical simulations demonstrate that compared to the full dCov or HSIC, our\nvariants have the same power up to a $\\sqrt 2$ factor, giving practitioners a\nnew option for large problems or data-analysis pipelines where computation, not\nsample size, could be the bottleneck.",
    "descriptor": "\nComments: 52 pages, 4 figures\n",
    "authors": [
      "Shubhanshu Shekhar",
      "Ilmun Kim",
      "Aaditya Ramdas"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09108"
  },
  {
    "id": "arXiv:2212.09115",
    "title": "Entropy-variance inequalities for discrete log-concave random variables  via degree of freedom",
    "abstract": "We utilize a discrete version of the notion of degree of freedom to prove a\nsharp min-entropy-variance inequality for integer valued log-concave random\nvariables. More specifically, we show that the geometric distribution minimizes\nthe min-entropy within the class of log-concave probability sequences with\nfixed variance. As an application, we obtain a discrete R\\'enyi entropy power\ninequality in the log-concave case, which improves a result of Bobkov,\nMarsiglietti and Melbourne (2022).",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Heshan Aravinda"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09115"
  },
  {
    "id": "arXiv:2212.09138",
    "title": "Manin conjecture for statistical pre-Frobenius manifolds, hypercube  relations and motivic Galois group in coding",
    "abstract": "This article develops, via the perspective of (arithmetic) algebraic geometry\nand category theory, different aspects of geometry of information. First, we\ndescribe in the terms of Eilenberg--Moore algebras over a Giry monad, the\ncollection $Cap_n$ of all probability distributions on the measurable space\n$(\\Omega_n, \\mathcal{A})$ (where $\\Omega$ is discrete with $n$ issues) and it\nturns out that there exists an embedding relation of Segre type among the\nproduct of $Cap_n$'s. We unravel hidden symmetries of these type of embeddings\nand show that there exists a hypercubic relation. Secondly, we show that the\nManin conjecture -- initially defined concerning the diophantine geometry of\nFano varieties -- is true in the case of exponential statistical manifolds,\ndefined over a discrete sample space. Thirdly, we introduce a modified version\nof the parenthesised braids ($\\mathbf{mPaB}$), which forms a key tool in\ncode-correction. This modified version $\\mathbf{mPaB}$ presents all types of\nmistakes that could occur during a transmission process. We show that the\nstandard parenthesised braids $\\mathbf{PaB}$ form a full subcategory of\n$\\mathbf{mPaB}$. We discuss the role of the Grothendieck--Teichm\\\"uller group\nin relation to the modified parenthesised braids. Finally, we prove that the\nmotivic Galois group is contained in the automorphism\n$Aut(\\widehat{\\mathbf{mPaB}}).$ We conclude by presenting an open question\nconcerning rational points, Commutative Moufang Loops and information geometry.",
    "descriptor": "",
    "authors": [
      "N. C. Combe"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Information Theory (cs.IT)",
      "Algebraic Topology (math.AT)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2212.09138"
  },
  {
    "id": "arXiv:2212.09178",
    "title": "Support Vector Regression: Risk Quadrangle Framework",
    "abstract": "This paper investigates Support Vector Regression (SVR) in the context of the\nfundamental risk quadrangle paradigm. It is shown that both formulations of\nSVR, $\\varepsilon$-SVR and $\\nu$-SVR, correspond to the minimization of\nequivalent regular error measures (Vapnik error and superquantile (CVaR) norm,\nrespectively) with a regularization penalty. These error measures, in turn,\ngive rise to corresponding risk quadrangles. Additionally, the technique used\nfor the construction of quadrangles serves as a powerful tool in proving the\nequivalence between $\\varepsilon$-SVR and $\\nu$-SVR.\nBy constructing the fundamental risk quadrangle, which corresponds to SVR, we\nshow that SVR is the asymptotically unbiased estimator of the average of two\nsymmetric conditional quantiles. Additionally, SVR is formulated as a regular\ndeviation minimization problem with a regularization penalty by invoking Error\nShaping Decomposition of Regression. Finally, the dual formulation of SVR in\nthe risk quadrangle framework is derived.",
    "descriptor": "",
    "authors": [
      "Anton Malandii",
      "Stan Uryasev"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2212.09178"
  },
  {
    "id": "arXiv:2212.09188",
    "title": "Problems, proofs, and disproofs on the inversion number",
    "abstract": "The {\\it inversion} of a set $X$ of vertices in a digraph $D$ consists in\nreversing the direction of all arcs of $D\\langle X\\rangle$. The {\\it inversion\nnumber} of an oriented graph $D$, denoted by ${\\rm inv}(D)$, is the minimum\nnumber of inversions needed to transform $D$ into an acyclic oriented graph. In\nthis paper, we study a number of problems involving the inversion number of\noriented graphs. Firstly, we give bounds on ${\\rm inv}(n)$, the maximum of the\ninversion numbers of the oriented graphs of order $n$. We show $n -\n\\mathcal{O}(\\sqrt{n\\log n}) \\ \\leq \\ {\\rm inv}(n) \\ \\leq \\ n - \\lceil \\log\n(n+1) \\rceil$. Secondly, we disprove a conjecture of Bang-Jensen et al.\nasserting that, for every pair of oriented graphs $L$ and $R$, we have ${\\rm\ninv}(L\\Rightarrow R) ={\\rm inv}(L) + {\\rm inv}(R)$, where $L\\Rightarrow R$ is\nthe oriented graph obtained from the disjoint union of $L$ and $R$ by adding\nall arcs from $L$ to $R$. Finally, we investigate whether, for all pairs of\npositive integers $k_1,k_2$, there exists an integer $f(k_1,k_2)$ such that if\n$D$ is an oriented graph with ${\\rm inv}(D) \\geq f(k_1,k_2)$ then there is a\npartition $(V_1, V_2)$ of $V(D)$ such that ${\\rm inv}(D\\langle V_i\\rangle) \\geq\nk_i$ for $i=1,2$. We show that $f(1,k)$ exists and $f(1,k)\\leq k+10$ for all\npositive integers $k$. Further, we show that $f(k_1,k_2)$ exists for all pairs\nof positive integers $k_1,k_2$ when the oriented graphs in consideration are\nrestricted to be tournaments.",
    "descriptor": "",
    "authors": [
      "Guillaume Aubian",
      "Fr\u00e9d\u00e9ric Havet",
      "Florian H\u00f6rsch",
      "Felix Klingelhoefer",
      "Nicolas Nisse",
      "Cl\u00e9ment Rambaud",
      "Quentin Vermande"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2212.09188"
  },
  {
    "id": "arXiv:2212.09201",
    "title": "Spectral Regularized Kernel Two-Sample Tests",
    "abstract": "Over the last decade, an approach that has gained a lot of popularity to\ntackle non-parametric testing problems on general (i.e., non-Euclidean) domains\nis based on the notion of reproducing kernel Hilbert space (RKHS) embedding of\nprobability distributions. The main goal of our work is to understand the\noptimality of two-sample tests constructed based on this approach. First, we\nshow that the popular MMD (maximum mean discrepancy) two-sample test is not\noptimal in terms of the separation boundary measured in Hellinger distance.\nSecond, we propose a modification to the MMD test based on spectral\nregularization by taking into account the covariance information (which is not\ncaptured by the MMD test) and prove the proposed test to be minimax optimal\nwith a smaller separation boundary than that achieved by the MMD test. Third,\nwe propose an adaptive version of the above test which involves a data-driven\nstrategy to choose the regularization parameter and show the adaptive test to\nbe almost minimax optimal up to a logarithmic factor. Moreover, our results\nhold for the permutation variant of the test where the test threshold is chosen\nelegantly through the permutation of the samples. Through numerical experiments\non synthetic and real-world data, we demonstrate the superior performance of\nthe proposed test in comparison to the MMD test.",
    "descriptor": "\nComments: 63 pages\n",
    "authors": [
      "Omar Hagrass",
      "Bharath K. Sriperumbudur",
      "Bing Li"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09201"
  },
  {
    "id": "arXiv:2212.09206",
    "title": "Segmentation Ability Map: Interpret deep features for medical image  segmentation",
    "abstract": "Deep convolutional neural networks (CNNs) have been widely used for medical\nimage segmentation. In most studies, only the output layer is exploited to\ncompute the final segmentation results and the hidden representations of the\ndeep learned features have not been well understood. In this paper, we propose\na prototype segmentation (ProtoSeg) method to compute a binary segmentation map\nbased on deep features. We measure the segmentation abilities of the features\nby computing the Dice between the feature segmentation map and ground-truth,\nnamed as the segmentation ability score (SA score for short). The corresponding\nSA score can quantify the segmentation abilities of deep features in different\nlayers and units to understand the deep neural networks for segmentation. In\naddition, our method can provide a mean SA score which can give a performance\nestimation of the output on the test images without ground-truth. Finally, we\nuse the proposed ProtoSeg method to compute the segmentation map directly on\ninput images to further understand the segmentation ability of each input\nimage. Results are presented on segmenting tumors in brain MRI, lesions in skin\nimages, COVID-related abnormality in CT images, prostate segmentation in\nabdominal MRI, and pancreatic mass segmentation in CT images. Our method can\nprovide new insights for interpreting and explainable AI systems for medical\nimage segmentation.\nOur code is available on: \\url{https://github.com/shengfly/ProtoSeg}.",
    "descriptor": "",
    "authors": [
      "Sheng He",
      "Yanfang Feng",
      "P. Ellen Grant",
      "Yangming Ou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09206"
  },
  {
    "id": "arXiv:2212.09209",
    "title": "CDIO-CT collaborative strategy for project-driven STEM education: an  illustration of solving the period of mathematical pendulum",
    "abstract": "The project-driven STEM education plays a significant role in training\nstudents' ability of innovation. It is a key issue that educators adopt\nappropriate methodology to work on the objective multi-discipline project of\ninterest. A novel approach based on the combination of\nconceive-design-implement-operate (CDIO) and computational thinking (CT) is\nproposed for the STEM education of students in colleges and universities, in\nwhich the CDIO concerns ``how to do\", CT concerns ``how to think\", and the\nproject means ``what to do\". As an illustration, the project of solving the\nperiod of mathematical pendulum (MP) is discussed in detail with the CDIO-CT\ncollaborative strategy. The most important work of this project is to solve the\ncomplete elliptic integral of the first kind (CEI-1). In the philosophy of STEM\neducation, all problems have more than one solutions. For computing the CEI-1,\nfour methods are discussed with a top-down strategy, which includes the\ninfinite series method, arithmetic-geometric mean (AGM) method, Gauss-Chebyshev\nmethod and Gauss-Legendre method. The algorithms involved can be utilized for R\n\\& D projects of interest and be reused according to the requirements\nencountered. The concepts and tools arising in developing the software for\ncalculating CEI-1 are valuable for teaching computer programming. The\nmethodology embedded in the project of exploring the expression and solution to\nthe period of MP is enlightening, which is worth popularizing to students and\ninstructors in colleges and universities.",
    "descriptor": "",
    "authors": [
      "Hong-Yan Zhang",
      "Yu Zhou",
      "Yu-Tao Li",
      "Fu-Yun Li",
      "Yong-Hui Jiang"
    ],
    "subjectives": [
      "Physics Education (physics.ed-ph)",
      "Computers and Society (cs.CY)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09209"
  },
  {
    "id": "arXiv:2212.09240",
    "title": "Probabilistic machine learning based predictive and interpretable  digital twin for dynamical systems",
    "abstract": "A framework for creating and updating digital twins for dynamical systems\nfrom a library of physics-based functions is proposed. The sparse Bayesian\nmachine learning is used to update and derive an interpretable expression for\nthe digital twin. Two approaches for updating the digital twin are proposed.\nThe first approach makes use of both the input and output information from a\ndynamical system, whereas the second approach utilizes output-only observations\nto update the digital twin. Both methods use a library of candidate functions\nrepresenting certain physics to infer new perturbation terms in the existing\ndigital twin model. In both cases, the resulting expressions of updated digital\ntwins are identical, and in addition, the epistemic uncertainties are\nquantified. In the first approach, the regression problem is derived from a\nstate-space model, whereas in the latter case, the output-only information is\ntreated as a stochastic process. The concepts of It\\^o calculus and\nKramers-Moyal expansion are being utilized to derive the regression equation.\nThe performance of the proposed approaches is demonstrated using highly\nnonlinear dynamical systems such as the crack-degradation problem. Numerical\nresults demonstrated in this paper almost exactly identify the correct\nperturbation terms along with their associated parameters in the dynamical\nsystem. The probabilistic nature of the proposed approach also helps in\nquantifying the uncertainties associated with updated models. The proposed\napproaches provide an exact and explainable description of the perturbations in\ndigital twin models, which can be directly used for better cyber-physical\nintegration, long-term future predictions, degradation monitoring, and\nmodel-agnostic control.",
    "descriptor": "",
    "authors": [
      "Tapas Tripura",
      "Aarya Sheetal Desai",
      "Sondipon Adhikari",
      "Souvik Chakraborty"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09240"
  },
  {
    "id": "arXiv:2212.09263",
    "title": "Focal-UNet: UNet-like Focal Modulation for Medical Image Segmentation",
    "abstract": "Recently, many attempts have been made to construct a transformer base\nU-shaped architecture, and new methods have been proposed that outperformed\nCNN-based rivals. However, serious problems such as blockiness and cropped\nedges in predicted masks remain because of transformers' patch partitioning\noperations. In this work, we propose a new U-shaped architecture for medical\nimage segmentation with the help of the newly introduced focal modulation\nmechanism. The proposed architecture has asymmetric depths for the encoder and\ndecoder. Due to the ability of the focal module to aggregate local and global\nfeatures, our model could simultaneously benefit the wide receptive field of\ntransformers and local viewing of CNNs. This helps the proposed method balance\nthe local and global feature usage to outperform one of the most powerful\ntransformer-based U-shaped models called Swin-UNet. We achieved a 1.68% higher\nDICE score and a 0.89 better HD metric on the Synapse dataset. Also, with\nextremely limited data, we had a 4.25% higher DICE score on the NeoPolyp\ndataset. Our implementations are available at:\nhttps://github.com/givkashi/Focal-UNet",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "MohammadReza Naderi",
      "MohammadHossein Givkashi",
      "Fatemeh Piri",
      "Nader Karimi",
      "Shadrokh Samavi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09263"
  },
  {
    "id": "arXiv:2212.09276",
    "title": "COVID-19 Detection Based on Self-Supervised Transfer Learning Using  Chest X-Ray Images",
    "abstract": "Purpose: Considering several patients screened due to COVID-19 pandemic,\ncomputer-aided detection has strong potential in assisting clinical workflow\nefficiency and reducing the incidence of infections among radiologists and\nhealthcare providers. Since many confirmed COVID-19 cases present radiological\nfindings of pneumonia, radiologic examinations can be useful for fast\ndetection. Therefore, chest radiography can be used to fast screen COVID-19\nduring the patient triage, thereby determining the priority of patient's care\nto help saturated medical facilities in a pandemic situation. Methods: In this\npaper, we propose a new learning scheme called self-supervised transfer\nlearning for detecting COVID-19 from chest X-ray (CXR) images. We compared six\nself-supervised learning (SSL) methods (Cross, BYOL, SimSiam, SimCLR,\nPIRL-jigsaw, and PIRL-rotation) with the proposed method. Additionally, we\ncompared six pretrained DCNNs (ResNet18, ResNet50, ResNet101, CheXNet,\nDenseNet201, and InceptionV3) with the proposed method. We provide quantitative\nevaluation on the largest open COVID-19 CXR dataset and qualitative results for\nvisual inspection. Results: Our method achieved a harmonic mean (HM) score of\n0.985, AUC of 0.999, and four-class accuracy of 0.953. We also used the\nvisualization technique Grad-CAM++ to generate visual explanations of different\nclasses of CXR images with the proposed method to increase the\ninterpretability. Conclusions: Our method shows that the knowledge learned from\nnatural images using transfer learning is beneficial for SSL of the CXR images\nand boosts the performance of representation learning for COVID-19 detection.\nOur method promises to reduce the incidence of infections among radiologists\nand healthcare providers.",
    "descriptor": "\nComments: Published as a journal paper at Springer IJCARS\n",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09276"
  },
  {
    "id": "arXiv:2212.09281",
    "title": "Boosting Automatic COVID-19 Detection Performance with Self-Supervised  Learning and Batch Knowledge Ensembling",
    "abstract": "Background and objective: COVID-19 and its variants have caused significant\ndisruptions in over 200 countries and regions worldwide, affecting the health\nand lives of billions of people. Detecting COVID-19 from chest X-Ray (CXR)\nimages has become one of the fastest and easiest methods for detecting COVID-19\nsince the common occurrence of radiological pneumonia findings in COVID-19\npatients. We present a novel high-accuracy COVID-19 detection method that uses\nCXR images. Methods: Our method consists of two phases. One is self-supervised\nlearning-based pertaining; the other is batch knowledge ensembling-based\nfine-tuning. Self-supervised learning-based pretraining can learn distinguished\nrepresentations from CXR images without manually annotated labels. On the other\nhand, batch knowledge ensembling-based fine-tuning can utilize category\nknowledge of images in a batch according to their visual feature similarities\nto improve detection performance. Unlike our previous implementation, we\nintroduce batch knowledge ensembling into the fine-tuning phase, reducing the\nmemory used in self-supervised learning and improving COVID-19 detection\naccuracy. Results: On two public COVID-19 CXR datasets, namely, a large dataset\nand an unbalanced dataset, our method exhibited promising COVID-19 detection\nperformance. Our method maintains high detection accuracy even when annotated\nCXR training images are reduced significantly (e.g., using only 10% of the\noriginal dataset). In addition, our method is insensitive to changes in\nhyperparameters. Conclusions: The proposed method outperforms other\nstate-of-the-art COVID-19 detection methods in different settings. Our method\ncan reduce the workloads of healthcare providers and radiologists.",
    "descriptor": "\nComments: Submitted as a journal paper at Elsevier CBM\n",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.09281"
  },
  {
    "id": "arXiv:2212.09310",
    "title": "Multimodal CNN Networks for Brain Tumor Segmentation in MRI: A BraTS  2022 Challenge Solution",
    "abstract": "Automatic segmentation is essential for the brain tumor diagnosis, disease\nprognosis, and follow-up therapy of patients with gliomas. Still, accurate\ndetection of gliomas and their sub-regions in multimodal MRI is very\nchallenging due to the variety of scanners and imaging protocols. Over the last\nyears, the BraTS Challenge has provided a large number of multi-institutional\nMRI scans as a benchmark for glioma segmentation algorithms. This paper\ndescribes our contribution to the BraTS 2022 Continuous Evaluation challenge.\nWe propose a new ensemble of multiple deep learning frameworks namely, DeepSeg,\nnnU-Net, and DeepSCAN for automatic glioma boundaries detection in\npre-operative MRI. It is worth noting that our ensemble models took first place\nin the final evaluation on the BraTS testing dataset with Dice scores of\n0.9294, 0.8788, and 0.8803, and Hausdorf distance of 5.23, 13.54, and 12.05,\nfor the whole tumor, tumor core, and enhancing tumor, respectively.\nFurthermore, the proposed ensemble method ranked first in the final ranking on\nanother unseen test dataset, namely Sub-Saharan Africa dataset, achieving mean\nDice scores of 0.9737, 0.9593, and 0.9022, and HD95 of 2.66, 1.72, 3.32 for the\nwhole tumor, tumor core, and enhancing tumor, respectively. The docker image\nfor the winning submission is publicly available at\n(https://hub.docker.com/r/razeineldin/camed22).",
    "descriptor": "\nComments: Accepted in BraTS 2022 (as part of the BrainLes workshop proceedings distributed by Springer LNCS). arXiv admin note: text overlap with arXiv:2112.06554\n",
    "authors": [
      "Ramy A. Zeineldin",
      "Mohamed E. Karar",
      "Oliver Burgert",
      "Franziska Mathis-Ullrich"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09310"
  },
  {
    "id": "arXiv:2212.09316",
    "title": "Lower bound on the running time of Pop-Stack Sorting on a random  permutation",
    "abstract": "Pop-Stack Sorting is an algorithm that takes a permutation as an input and\nsorts its elements. It consists of several steps. At one step, the algorithm\nreads the permutation it has to process from left to right and reverses each of\nits maximal decreasing subsequences of consecutive elements. It terminates at\nthe first step that outputs the identity permutation.\nIn this note, we answer a question of Defant on the running time of Pop-Stack\nSorting on the uniform random permutation $\\sigma_n$. More precisely, we show\nthat there is a constant $c > 0.5$ such that asymptotically almost surely, the\nalgorithm needs at least $cn$ steps to terminate on $\\sigma_n$.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Lyuben Lichev"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2212.09316"
  },
  {
    "id": "arXiv:2212.09328",
    "title": "Quantum policy gradient algorithms",
    "abstract": "Understanding the power and limitations of quantum access to data in machine\nlearning tasks is primordial to assess the potential of quantum computing in\nartificial intelligence. Previous works have already shown that speed-ups in\nlearning are possible when given quantum access to reinforcement learning\nenvironments. Yet, the applicability of quantum algorithms in this setting\nremains very limited, notably in environments with large state and action\nspaces. In this work, we design quantum algorithms to train state-of-the-art\nreinforcement learning policies by exploiting quantum interactions with an\nenvironment. However, these algorithms only offer full quadratic speed-ups in\nsample complexity over their classical analogs when the trained policies\nsatisfy some regularity conditions. Interestingly, we find that reinforcement\nlearning policies derived from parametrized quantum circuits are well-behaved\nwith respect to these conditions, which showcases the benefit of a\nfully-quantum reinforcement learning framework.",
    "descriptor": "\nComments: 22 pages, 1 figure\n",
    "authors": [
      "Sofiene Jerbi",
      "Arjan Cornelissen",
      "M\u0101ris Ozols",
      "Vedran Dunjko"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.09328"
  },
  {
    "id": "arXiv:2212.09337",
    "title": "Semantics-Aware Remote Estimation via Information Bottleneck-Inspired  Type Based Multiple Access",
    "abstract": "Type-based multiple access (TBMA) is a semantics-aware multiple access\nprotocol for remote inference. In TBMA, codewords are reused across\ntransmitting sensors, with each codeword being assigned to a different\nobservation value. Existing TBMA protocols are based on fixed shared codebooks\nand on conventional maximum-likelihood or Bayesian decoders, which require\nknowledge of the distributions of observations and channels. In this letter, we\npropose a novel design principle for TBMA based on the information bottleneck\n(IB). In the proposed IB-TBMA protocol, the shared codebook is jointly\noptimized with a decoder based on artificial neural networks (ANNs), so as to\nadapt to source, observations, and channel statistics based on data only. We\nalso introduce the Compressed IB-TBMA (CB-TBMA) protocol, which improves\nIB-TBMA by enabling a reduction in the number of codewords via an IB-inspired\nclustering phase. Numerical results demonstrate the importance of a joint\ndesign of codebook and neural decoder, and validate the benefits of codebook\ncompression.",
    "descriptor": "\nComments: 5 pages, 3 figures, submitted\n",
    "authors": [
      "Meiyi Zhu",
      "Chunyan Feng",
      "Caili Guo",
      "Zhe Liu",
      "Nan Jiang",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.09337"
  },
  {
    "id": "arXiv:2212.09348",
    "title": "Excluding Single-Crossing Matching Minors in Bipartite Graphs",
    "abstract": "\\noindent By a seminal result of Valiant, computing the permanent of\n$(0,1)$-matrices is, in general, $\\#\\mathsf{P}$-hard. In 1913 P\\'olya asked for\nwhich $(0,1)$-matrices $A$ it is possible to change some signs such that the\npermanent of $A$ equals the determinant of the resulting matrix. In 1975,\nLittle showed these matrices to be exactly the biadjacency matrices of\nbipartite graphs excluding $K_{3,3}$ as a \\{matching minor}. This was turned\ninto a polynomial time algorithm by McCuaig, Robertson, Seymour, and Thomas in\n1999. However, the relation between the exclusion of some matching minor in a\nbipartite graph and the tractability of the permanent extends beyond $K_{3,3}.$\nRecently it was shown that the exclusion of any planar bipartite graph as a\nmatching minor yields a class of bipartite graphs on which the {permanent} of\nthe corresponding $(0,1)$-matrices can be computed efficiently. In this paper\nwe unify the two results above into a single, more general result in the style\nof the celebrated structure theorem for single-crossing-minor-free graphs. We\nidentify a class of bipartite graphs strictly generalising planar bipartite\ngraphs and $K_{3,3}$ which includes infinitely many non-Pfaffian graphs. The\nexclusion of any member of this class as a matching minor yields a structure\nthat allows for the efficient evaluation of the permanent. Moreover, we show\nthat the evaluation of the permanent remains $\\#\\mathsf{P}$-hard on bipartite\ngraphs which exclude $K_{5,5}$ as a matching minor. This establishes a first\ncomputational lower bound for the problem of counting perfect matchings on\nmatching minor closed classes.",
    "descriptor": "\nComments: Accepted in SODA 2023\n",
    "authors": [
      "Archontia C. Giannopoulou",
      "Dimitrios M. Thilikos",
      "Sebastian Wiederrecht"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2212.09348"
  },
  {
    "id": "arXiv:2212.09396",
    "title": "Rank-1 Matrix Completion with Gradient Descent and Small Random  Initialization",
    "abstract": "The nonconvex formulation of matrix completion problem has received\nsignificant attention in recent years due to its affordable complexity compared\nto the convex formulation. Gradient descent (GD) is the simplest yet efficient\nbaseline algorithm for solving nonconvex optimization problems. The success of\nGD has been witnessed in many different problems in both theory and practice\nwhen it is combined with random initialization. However, previous works on\nmatrix completion require either careful initialization or regularizers to\nprove the convergence of GD. In this work, we study the rank-1 symmetric matrix\ncompletion and prove that GD converges to the ground truth when small random\ninitialization is used. We show that in logarithmic amount of iterations, the\ntrajectory enters the region where local convergence occurs. We provide an\nupper bound on the initialization size that is sufficient to guarantee the\nconvergence and show that a larger initialization can be used as more samples\nare available. We observe that implicit regularization effect of GD plays a\ncritical role in the analysis, and for the entire trajectory, it prevents each\nentry from becoming much larger than the others.",
    "descriptor": "",
    "authors": [
      "Daesung Kim",
      "Hye Won Chung"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09396"
  },
  {
    "id": "arXiv:2212.09444",
    "title": "Voltage Gated Domain Wall Magnetic Tunnel Junction-based Spiking  Convolutional Neural Network",
    "abstract": "We propose a novel spin-orbit torque (SOT) driven and voltage-gated domain\nwall motion (DWM)-based MTJ device and its application in neuromorphic\ncomputing. We show that by utilizing the voltage-controlled gating effect on\nthe DWM, the access transistor can be eliminated. The device provides more\ncontrol over individual synapse writing and shows highly linear synaptic\nbehavior. The linearity dependence on material parameters such as DMI and\ntemperature is evaluated for real-environment performance analysis.\nFurthermore, using skyrmion-based leaky integrate and fire neuron model, we\nimplement the spiking convolutional neural network for pattern recognition\napplications on the CIFAR-10 data set. The accuracy of the device is above 85%,\nproving its applicability in SNN.",
    "descriptor": "\nComments: 6 pages and 5 figures\n",
    "authors": [
      "Aijaz H Lone",
      "Hanrui Li",
      "Nazek El-Atab",
      "Xiaohang Li",
      "Hossein Fariborzi"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2212.09444"
  },
  {
    "id": "arXiv:2212.09450",
    "title": "Accelerating Antimicrobial Peptide Discovery with Latent  Sequence-Structure Model",
    "abstract": "Antimicrobial peptide (AMP) is a promising therapy in the treatment of\nbroad-spectrum antibiotics and drug-resistant infections. Recently, an\nincreasing number of researchers have been introducing deep generative models\nto accelerate AMP discovery. However, current studies mainly focus on sequence\nattributes and ignore structure information, which is important in AMP\nbiological functions. In this paper, we propose a latent sequence-structure\nmodel for AMPs (LSSAMP) with multi-scale VQ-VAE to incorporate secondary\nstructures. By sampling in the latent space, LSSAMP can simultaneously generate\npeptides with ideal sequence attributes and secondary structures. Experimental\nresults show that the peptides generated by LSSAMP have a high probability of\nAMP, and two of the 21 candidates have been verified to have good antimicrobial\nactivity. Our model will be released to help create high-quality AMP candidates\nfor follow-up biological experiments and accelerate the whole AMP discovery.",
    "descriptor": "",
    "authors": [
      "Danqing Wang",
      "Zeyu Wen",
      "Fei Ye",
      "Hao Zhou",
      "Lei Li"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09450"
  },
  {
    "id": "arXiv:2212.09470",
    "title": "Automated Search for Conjectures on Mathematical Constants using  Analysis of Integer Sequences",
    "abstract": "Formulas involving fundamental mathematical constants had a great impact on\nvarious fields of science and mathematics, for example aiding in proofs of\nirrationality of constants. However, the discovery of such formulas has\nhistorically remained scarce, often perceived as an act of mathematical genius\nby great mathematicians such as Ramanujan, Euler, and Gauss. Recent efforts to\nautomate the discovery of formulas for mathematical constants, such as the\nRamanujan Machine project, relied on exhaustive search. Despite several\nsuccessful discoveries, exhaustive search remains limited by the space of\noptions that can be covered and by the need for vast amounts of computational\nresources. Here we propose a fundamentally different method to search for\nconjectures on mathematical constants: through analysis of integer sequences.\nWe introduce the Enumerated Signed-continued-fraction Massey Approve (ESMA)\nalgorithm, which builds on the Berlekamp-Massey algorithm to identify patterns\nin integer sequences that represent mathematical constants. The ESMA algorithm\nfound various known formulas for $e, e^2, tan(1)$, and ratios of values of\nBessel functions. The algorithm further discovered a large number of new\nconjectures for these constants, some providing simpler representations and\nsome providing faster numerical convergence than the corresponding simple\ncontinued fractions. Along with the algorithm, we present mathematical tools\nfor manipulating continued fractions. These connections enable us to\ncharacterize what space of constants can be found by ESMA and quantify its\nalgorithmic advantage in certain scenarios. Altogether, this work continues in\nthe development of augmenting mathematical intuition by computer algorithms, to\nhelp reveal mathematical structures and accelerate mathematical research.",
    "descriptor": "\nComments: 5 figures, 1 flow chart, 31 pages including supplementary information\n",
    "authors": [
      "Ofir Razon",
      "Yoav Harris",
      "Shahar Gottlieb",
      "Dan Carmon",
      "Ofir David",
      "Ido Kaminer"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09470"
  },
  {
    "id": "arXiv:2212.09472",
    "title": "Distributed Unconstrained Optimization with Time-varying Cost Functions",
    "abstract": "In this paper, we propose a novel solution for the distributed unconstrained\noptimization problem where the total cost is the summation of time-varying\nlocal cost functions of a group networked agents. The objective is to track the\noptimal trajectory that minimizes the total cost at each time instant. Our\napproach consists of a two-stage dynamics, where the first one samples the\nfirst and second derivatives of the local costs periodically to construct an\nestimate of the descent direction towards the optimal trajectory, and the\nsecond one uses this estimate and a consensus term to drive local states\ntowards the time-varying solution while reaching consensus. The first part is\ncarried out by the implementation of a weighted average consensus algorithm in\nthe discrete-time framework and the second part is performed with a\ncontinuous-time dynamics. Using the Lyapunov stability analysis, an upper bound\non the gradient of the total cost is obtained which is asymptotically reached.\nThis bound is characterized by the properties of the local costs. To\ndemonstrate the performance of the proposed method, a numerical example is\nconducted that studies tuning the algorithm's parameters and their effects on\nthe convergence of local states to the optimal trajectory.",
    "descriptor": "",
    "authors": [
      "Amir-Salar Esteki",
      "Solmaz S. Kia"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09472"
  },
  {
    "id": "arXiv:2212.09497",
    "title": "Target selection for Near-Earth Asteroids in-orbit sample collection  missions",
    "abstract": "This work presents a mission concept for in-orbit particle collection for\nsampling and exploration missions towards Near-Earth asteroids. Ejecta is\ngenerated via a small kinetic impactor and two possible collection strategies\nare investigated: collecting the particle along the anti-solar direction,\nexploiting the dynamical features of the L$_2$ Lagrangian point or collecting\nthem while the spacecraft orbits the asteroid and before they re-impact onto\nthe asteroid surface. Combining the dynamics of the particles in the Circular\nRestricted Three-Body Problem perturbed by Solar Radiation Pressure with models\nfor the ejecta generation, we identify possible target asteroids as a function\nof their physical properties, by evaluating the potential for particle\ncollection.",
    "descriptor": "",
    "authors": [
      "Mirko Trisolini",
      "Camilla Colombo",
      "Yuichi Tsuda"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09497"
  },
  {
    "id": "arXiv:2212.09505",
    "title": "Variational Quantum Search with Exponential Speedup",
    "abstract": "With powerful quantum computers already built, we need more efficient quantum\nalgorithms to achieve quantum supremacy over classical computers in the noisy\nintermediate-scale quantum (NISQ) era. Grover's search algorithm and its\ngeneralization, quantum amplitude amplification, provide quadratic speedup in\nsolving many important scientific problems. However, they still have\nexponential time complexity as the depths of their quantum circuits increase\nexponentially with the number of qubits. To address this problem, we propose a\nnew algorithm, Variational Quantum Search (VQS), which is based on the\ncelebrated variational quantum algorithms and includes a parameterized quantum\ncircuit, known as Ansatz. We show that a depth-10 Ansatz can amplify the total\nprobability of $k$ ($k \\geq 1$) good elements, out of $2^n$ elements\nrepresented by $n$+1 qubits, from $k/2^n$ to nearly 1, as verified for $n$ up\nto 26, and that the maximum depth of quantum circuits in the VQS increases\nlinearly with the number of qubits. We demonstrate that a depth-56 circuit in\nVQS can replace a depth-270,989 circuit in Grover's algorithm, and thus VQS is\nmore suitable for NISQ computers. We envisage our VQS could exponentially speed\nup the solutions to many important problems, including the NP-complete\nproblems, which is widely considered impossible.",
    "descriptor": "\nComments: 13 pages, 7 figures\n",
    "authors": [
      "Junpeng Zhan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Quantum Algebra (math.QA)"
    ],
    "url": "https://arxiv.org/abs/2212.09505"
  },
  {
    "id": "arXiv:2212.09510",
    "title": "Near-optimal Policy Identification in Active Reinforcement Learning",
    "abstract": "Many real-world reinforcement learning tasks require control of complex\ndynamical systems that involve both costly data acquisition processes and large\nstate spaces. In cases where the transition dynamics can be readily evaluated\nat specified states (e.g., via a simulator), agents can operate in what is\noften referred to as planning with a \\emph{generative model}. We propose the\nAE-LSVI algorithm for best-policy identification, a novel variant of the\nkernelized least-squares value iteration (LSVI) algorithm that combines\noptimism with pessimism for active exploration (AE). AE-LSVI provably\nidentifies a near-optimal policy \\emph{uniformly} over an entire state space\nand achieves polynomial sample complexity guarantees that are independent of\nthe number of states. When specialized to the recently introduced offline\ncontextual Bayesian optimization setting, our algorithm achieves improved\nsample complexity bounds. Experimentally, we demonstrate that AE-LSVI\noutperforms other RL algorithms in a variety of environments when robustness to\nthe initial state is required.",
    "descriptor": "",
    "authors": [
      "Xiang Li",
      "Viraj Mehta",
      "Johannes Kirschner",
      "Ian Char",
      "Willie Neiswanger",
      "Jeff Schneider",
      "Andreas Krause",
      "Ilija Bogunovic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09510"
  },
  {
    "id": "arXiv:2212.09513",
    "title": "Stochastic Inexact Augmented Lagrangian Method for Nonconvex Expectation  Constrained Optimization",
    "abstract": "Many real-world problems not only have complicated nonconvex functional\nconstraints but also use a large number of data points. This motivates the\ndesign of efficient stochastic methods on finite-sum or expectation constrained\nproblems. In this paper, we design and analyze stochastic inexact augmented\nLagrangian methods (Stoc-iALM) to solve problems involving a nonconvex\ncomposite (i.e. smooth+nonsmooth) objective and nonconvex smooth functional\nconstraints. We adopt the standard iALM framework and design a subroutine by\nusing the momentum-based variance-reduced proximal stochastic gradient method\n(PStorm) and a postprocessing step. Under certain regularity conditions\n(assumed also in existing works), to reach an $\\varepsilon$-KKT point in\nexpectation, we establish an oracle complexity result of $O(\\varepsilon^{-5})$,\nwhich is better than the best-known $O(\\varepsilon^{-6})$ result. Numerical\nexperiments on the fairness constrained problem and the Neyman-Pearson\nclassification problem with real data demonstrate that our proposed method\noutperforms an existing method with the previously best-known complexity\nresult.",
    "descriptor": "",
    "authors": [
      "Zichong Li",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Songtao Lu",
      "Yangyang Xu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.09513"
  },
  {
    "id": "arXiv:2212.09612",
    "title": "Taming Lagrangian Chaos with Multi-Objective Reinforcement Learning",
    "abstract": "We consider the problem of two active particles in 2D complex flows with the\nmulti-objective goals of minimizing both the dispersion rate and the energy\nconsumption of the pair. We approach the problem by means of Multi Objective\nReinforcement Learning (MORL), combining scalarization techniques together with\na Q-learning algorithm, for Lagrangian drifters that have variable swimming\nvelocity. We show that MORL is able to find a set of trade-off solutions\nforming an optimal Pareto frontier. As a benchmark, we show that a set of\nheuristic strategies are dominated by the MORL solutions. We consider the\nsituation in which the agents cannot update their control variables\ncontinuously, but only after a discrete (decision) time, $\\tau$. We show that\nthere is a range of decision times, in between the Lyapunov time and the\ncontinuous updating limit, where Reinforcement Learning finds strategies that\nsignificantly improve over heuristics. In particular, we discuss how large\ndecision times require enhanced knowledge of the flow, whereas for smaller\n$\\tau$ all a priori heuristic strategies become Pareto optimal.",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Chiara Calascibetta",
      "Luca Biferale",
      "Francesco Borra",
      "Antonio Celani",
      "Massimo Cencini"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2212.09612"
  },
  {
    "id": "arXiv:2212.09624",
    "title": "Holder Recommendations using Graph Representation Learning & Link  Prediction",
    "abstract": "Lead recommendations for financial products such as funds or ETF is\npotentially challenging in investment space due to changing market scenarios,\nand difficulty in capturing financial holder's mindset and their philosophy.\nCurrent methods surface leads based on certain product categorization and\nattributes like returns, fees, category etc. to suggest similar product to\ninvestors which may not capture the holder's investment behavior holistically.\nOther reported works does subjective analysis of institutional holder's\nideology. This paper proposes a comprehensive data driven framework for\ndeveloping a lead recommendations system in holder's space for financial\nproducts like funds by using transactional history, asset flows and product\nspecific attributes. The system assumes holder's interest implicitly by\nconsidering all investment transactions made and collects possible meta\ninformation to detect holder's investment profile/persona like investment\nanticipation and investment behavior. This paper focusses on holder\nrecommendation component of framework which employs a bi-partite graph\nrepresentation of financial holders and funds using variety of attributes and\nfurther employs GraphSage model for learning representations followed by link\nprediction model for ranking recommendation for future period. The performance\nof the proposed approach is compared with baseline model i.e., content-based\nfiltering approach on metric hits at Top-k (50, 100, 200) recommendations. We\nfound that the proposed graph ML solution outperform baseline by absolute 42%,\n22% and 14% with a look ahead bias and by absolute 18%, 19% and 18% on\ncompletely unseen holders in terms of hit rate for top-k recommendations: 50,\n100 and 200 respectively.",
    "descriptor": "\nComments: 6 pages, 6 figures, 2 tables Presented at a workshop in ACM AI in Finance conference\n",
    "authors": [
      "Rachna Saxena",
      "Abhijeet Kumar",
      "Mridul Mishra"
    ],
    "subjectives": [
      "General Finance (q-fin.GN)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2212.09624"
  },
  {
    "id": "arXiv:2212.09681",
    "title": "Annual field-scale maps of tall and short crops at the global scale  using GEDI and Sentinel-2",
    "abstract": "Crop type maps are critical for tracking agricultural land use and estimating\ncrop production. Remote sensing has proven an efficient and reliable tool for\ncreating these maps in regions with abundant ground labels for model training,\nyet these labels remain difficult to obtain in many regions and years. NASA's\nGlobal Ecosystem Dynamics Investigation (GEDI) spaceborne lidar instrument,\noriginally designed for forest monitoring, has shown promise for distinguishing\ntall and short crops. In the current study, we leverage GEDI to develop\nwall-to-wall maps of short vs tall crops on a global scale at 10 m resolution\nfor 2019-2021. Specifically, we show that (1) GEDI returns can reliably be\nclassified into tall and short crops after removing shots with extreme view\nangles or topographic slope, (2) the frequency of tall crops over time can be\nused to identify months when tall crops are at their peak height, and (3) GEDI\nshots in these months can then be used to train random forest models that use\nSentinel-2 time series to accurately predict short vs. tall crops. Independent\nreference data from around the world are then used to evaluate these GEDI-S2\nmaps. We find that GEDI-S2 performed nearly as well as models trained on\nthousands of local reference training points, with accuracies of at least 87%\nand often above 90% throughout the Americas, Europe, and East Asia. Systematic\nunderestimation of tall crop area was observed in regions where crops\nfrequently exhibit low biomass, namely Africa and South Asia, and further work\nis needed in these systems. Although the GEDI-S2 approach only differentiates\ntall from short crops, in many landscapes this distinction goes a long way\ntoward mapping the main individual crop types. The combination of GEDI and\nSentinel-2 thus presents a very promising path towards global crop mapping with\nminimal reliance on ground data.",
    "descriptor": "",
    "authors": [
      "Stefania Di Tommaso",
      "Sherrie Wang",
      "Vivek Vajipey",
      "Noel Gorelick",
      "Rob Strey",
      "David B. Lobell"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.09681"
  },
  {
    "id": "arXiv:1711.04211",
    "title": "Convergence of Hierarchical Clustering and Persistent Homology Methods  on Directed Networks",
    "abstract": "Comments: This paper has been withdrawn by the authors. This paper has been superseded by v3 of arXiv:1708.04727 (merged from arXiv:1708.04727 (v2), arXiv:1804.02820, and arXiv:1711.04211), which in turn has appeared in the Journal of Applied and Computational Topology",
    "descriptor": "\nComments: This paper has been withdrawn by the authors. This paper has been superseded by v3 of arXiv:1708.04727 (merged from arXiv:1708.04727 (v2), arXiv:1804.02820, and arXiv:1711.04211), which in turn has appeared in the Journal of Applied and Computational Topology\n",
    "authors": [
      "Samir Chowdhury",
      "Facundo M\u00e9moli"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/1711.04211"
  },
  {
    "id": "arXiv:1803.01792",
    "title": "Multiagent Learning for Competitive Opinion Optimization",
    "abstract": "Multiagent Learning for Competitive Opinion Optimization",
    "descriptor": "",
    "authors": [
      "Po-An Chen",
      "Chi-Jen Lu",
      "Chuang-Chieh Lin",
      "Ke-Wei Fu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/1803.01792"
  },
  {
    "id": "arXiv:1804.02820",
    "title": "The Metric Space of Networks",
    "abstract": "Comments: This paper has been withdrawn by the authors. This paper has been superseded by v3 of arXiv:1708.04727 (merged from arXiv:1708.04727 (v2), arXiv:1804.02820, and arXiv:1711.04211), which in turn has appeared in the Journal of Applied and Computational Topology",
    "descriptor": "\nComments: This paper has been withdrawn by the authors. This paper has been superseded by v3 of arXiv:1708.04727 (merged from arXiv:1708.04727 (v2), arXiv:1804.02820, and arXiv:1711.04211), which in turn has appeared in the Journal of Applied and Computational Topology\n",
    "authors": [
      "Samir Chowdhury",
      "Facundo M\u00e9moli"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/1804.02820"
  },
  {
    "id": "arXiv:1903.06262",
    "title": "Overlap Removal of Dimensionality Reduction Scatterplot Layouts",
    "abstract": "Comments: 11 pages and 10 figures",
    "descriptor": "\nComments: 11 pages and 10 figures\n",
    "authors": [
      "Gladys M. Hilasaca",
      "Wilson E. Marc\u00edlio-Jr",
      "Danilo M. Eler",
      "Rafael M. Martins",
      "Fernando V. Paulovich"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1903.06262"
  },
  {
    "id": "arXiv:1906.10019",
    "title": "Machine Learning Construction: implications to cybersecurity",
    "abstract": "Machine Learning Construction: implications to cybersecurity",
    "descriptor": "",
    "authors": [
      "Waleed A. Yousef"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1906.10019"
  },
  {
    "id": "arXiv:1907.12851",
    "title": "Machine Learning Assessment: implications to cybersecurity",
    "abstract": "Machine Learning Assessment: implications to cybersecurity",
    "descriptor": "",
    "authors": [
      "Waleed A. Yousef"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1907.12851"
  },
  {
    "id": "arXiv:2002.00817",
    "title": "Private Summation in the Multi-Message Shuffle Model",
    "abstract": "Comments: Published at CCS'20",
    "descriptor": "\nComments: Published at CCS'20\n",
    "authors": [
      "Borja Balle",
      "James Bell",
      "Adria Gascon",
      "Kobbi Nissim"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2002.00817"
  },
  {
    "id": "arXiv:2002.08410",
    "title": "Gaussian Mixture Reduction with Composite Transportation Divergence",
    "abstract": "Gaussian Mixture Reduction with Composite Transportation Divergence",
    "descriptor": "",
    "authors": [
      "Qiong Zhang",
      "Archer Gong Zhang",
      "Jiahua Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2002.08410"
  },
  {
    "id": "arXiv:2003.03303",
    "title": "Deep Learning-based CSI Feedback and Cooperative Recovery in Massive  MIMO",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Jiajia Guo",
      "Xi Yang",
      "Chao-Kai Wen",
      "Shi Jin",
      "Geoffrey Ye Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2003.03303"
  },
  {
    "id": "arXiv:2003.13605",
    "title": "On different Versions of the Exact Subgraph Hierarchy for the Stable Set  Problem",
    "abstract": "On different Versions of the Exact Subgraph Hierarchy for the Stable Set  Problem",
    "descriptor": "",
    "authors": [
      "Elisabeth Gaar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2003.13605"
  },
  {
    "id": "arXiv:2005.03982",
    "title": "Distributed Stochastic Constrained Composite Optimization over  Time-Varying Network with a Class of Communication Noise",
    "abstract": "Distributed Stochastic Constrained Composite Optimization over  Time-Varying Network with a Class of Communication Noise",
    "descriptor": "",
    "authors": [
      "Zhan Yu",
      "Daniel W. C. Ho",
      "Deming Yuan",
      "Jie Liu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2005.03982"
  },
  {
    "id": "arXiv:2006.02032",
    "title": "A Unified Single-loop Alternating Gradient Projection Algorithm for  Nonconvex-Concave and Convex-Nonconcave Minimax Problems",
    "abstract": "A Unified Single-loop Alternating Gradient Projection Algorithm for  Nonconvex-Concave and Convex-Nonconcave Minimax Problems",
    "descriptor": "",
    "authors": [
      "Zi Xu",
      "Huiling Zhang",
      "Yang Xu",
      "Guanghui Lan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.02032"
  },
  {
    "id": "arXiv:2007.03418",
    "title": "Impasse Surface of Differential-Algebraic Power System Models: An  Interpretation Based on Admittance Matrices",
    "abstract": "Comments: 8 pages, 5 figures",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Yue Song",
      "David J. Hill",
      "Tao Liu",
      "Xinran Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2007.03418"
  },
  {
    "id": "arXiv:2007.12350",
    "title": "Improving the dilation of a metric graph by adding edges",
    "abstract": "Comments: Journal version, TALG 2022",
    "descriptor": "\nComments: Journal version, TALG 2022\n",
    "authors": [
      "Joachim Gudmundsson",
      "Sampson Wong"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2007.12350"
  },
  {
    "id": "arXiv:2008.13670",
    "title": "A Square Equal-area Map Projection with Low Angular Distortion, Minimal  Cusps, and Closed-form Solutions",
    "abstract": "Comments: 17 pages, 6 figures, 1 table; corrections to Appendix A",
    "descriptor": "\nComments: 17 pages, 6 figures, 1 table; corrections to Appendix A\n",
    "authors": [
      "Matthew A. Petroff"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2008.13670"
  },
  {
    "id": "arXiv:2009.09083",
    "title": "What is an intelligent system?",
    "abstract": "Comments: New edition with new and updated content",
    "descriptor": "\nComments: New edition with new and updated content\n",
    "authors": [
      "Martin Molina"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2009.09083"
  },
  {
    "id": "arXiv:2011.09735",
    "title": "Decentralized Design and Plug-and-Play Distributed Control for Linear  Multi-Channel Systems",
    "abstract": "Comments: Submitted to IEEE Transactions on Automatic Control",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Automatic Control\n",
    "authors": [
      "Taekyoo Kim",
      "Donggil Lee",
      "Hyungbo Shim"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.09735"
  },
  {
    "id": "arXiv:2011.12216",
    "title": "Energy-Based Models for Continual Learning",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Shuang Li",
      "Yilun Du",
      "Gido M. van de Ven",
      "Igor Mordatch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.12216"
  },
  {
    "id": "arXiv:2101.08700",
    "title": "Multi-sense embeddings through a word sense disambiguation process",
    "abstract": "Multi-sense embeddings through a word sense disambiguation process",
    "descriptor": "",
    "authors": [
      "Terry Ruas",
      "William Grosky",
      "Akiko Aizawa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.08700"
  },
  {
    "id": "arXiv:2101.09023",
    "title": "Enhanced word embeddings using multi-semantic representation through  lexical chains",
    "abstract": "Enhanced word embeddings using multi-semantic representation through  lexical chains",
    "descriptor": "",
    "authors": [
      "Terry Ruas",
      "Charles Henrique Porto Ferreira",
      "William Grosky",
      "Fabr\u00edcio Olivetti de Fran\u00e7a",
      "D\u00e9bora Maria Rossi Medeiros"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.09023"
  },
  {
    "id": "arXiv:2101.12430",
    "title": "Subgraph nomination: Query by Example Subgraph Retrieval in Networks",
    "abstract": "Comments: 37 pages, 11 figures",
    "descriptor": "\nComments: 37 pages, 11 figures\n",
    "authors": [
      "Al-Fahad M. Al-Qadhi",
      "Carey E. Priebe",
      "Hayden S. Helm",
      "Vince Lyzinski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.12430"
  },
  {
    "id": "arXiv:2102.06847",
    "title": "Disease2Vec: Representing Alzheimer's Progression via Disease Embedding  Tree",
    "abstract": "Comments: Submitted to Information Processing in Medical Imaging (IPMI) 2023",
    "descriptor": "\nComments: Submitted to Information Processing in Medical Imaging (IPMI) 2023\n",
    "authors": [
      "Lu Zhang",
      "Li Wang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06847"
  },
  {
    "id": "arXiv:2103.04287",
    "title": "Reduction Free Normalisation for a proof irrelevant type of propositions",
    "abstract": "Comments: Corrected typos; changed the base category and added new results",
    "descriptor": "\nComments: Corrected typos; changed the base category and added new results\n",
    "authors": [
      "Thierry Coquand"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2103.04287"
  },
  {
    "id": "arXiv:2103.05529",
    "title": "Deep and Statistical Learning in Biomedical Imaging: State of the Art in  3D MRI Brain Tumor Segmentation",
    "abstract": "Comments: 21 pages, 7 figures",
    "descriptor": "\nComments: 21 pages, 7 figures\n",
    "authors": [
      "K. Ruwani M. Fernando",
      "Chris P. Tsokos"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.05529"
  },
  {
    "id": "arXiv:2103.12122",
    "title": "Moving from Linear to Conic Markets for Electricity",
    "abstract": "Comments: Final accepted manuscript with supplementary material (revised 15.12.2022)",
    "descriptor": "\nComments: Final accepted manuscript with supplementary material (revised 15.12.2022)\n",
    "authors": [
      "Anubhav Ratha",
      "Pierre Pinson",
      "H\u00e9l\u00e8ne Le Cadre",
      "Ana Virag",
      "Jalal Kazempour"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2103.12122"
  },
  {
    "id": "arXiv:2103.15890",
    "title": "Learning Domain Invariant Representations for Generalizable Person  Re-Identification",
    "abstract": "Learning Domain Invariant Representations for Generalizable Person  Re-Identification",
    "descriptor": "",
    "authors": [
      "Yi-Fan Zhang",
      "Zhang Zhang",
      "Da Li",
      "Zhen Jia",
      "Liang Wang",
      "Tieniu Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.15890"
  },
  {
    "id": "arXiv:2104.00970",
    "title": "From banks to DeFi: the evolution of the lending market",
    "abstract": "From banks to DeFi: the evolution of the lending market",
    "descriptor": "",
    "authors": [
      "Jiahua Xu",
      "Nikhil Vadgama"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2104.00970"
  },
  {
    "id": "arXiv:2104.06728",
    "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World",
    "abstract": "Comments: accepted by TPAMI 2022",
    "descriptor": "\nComments: accepted by TPAMI 2022\n",
    "authors": [
      "Xingxing Wei",
      "Ying Guo",
      "Jie Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.06728"
  },
  {
    "id": "arXiv:2104.13336",
    "title": "Stochastic partial differential equations arising in self-organized  criticality",
    "abstract": "Comments: An entirely new section containing numerical simulations was added",
    "descriptor": "\nComments: An entirely new section containing numerical simulations was added\n",
    "authors": [
      "\u013dubom\u00edr Ba\u0148as",
      "Benjamin Gess",
      "Marius Neu\u00df"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2104.13336"
  },
  {
    "id": "arXiv:2105.03939",
    "title": "Differentiable Neural Architecture Search for Extremely Lightweight  Image Super-Resolution",
    "abstract": "Comments: Accepted to IEEE Transactions on Circuits and Systems for Video Technology",
    "descriptor": "\nComments: Accepted to IEEE Transactions on Circuits and Systems for Video Technology\n",
    "authors": [
      "Han Huang",
      "Li Shen",
      "Chaoyang He",
      "Weisheng Dong",
      "Wei Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03939"
  },
  {
    "id": "arXiv:2105.04137",
    "title": "On the inversion number of oriented graphs",
    "abstract": "On the inversion number of oriented graphs",
    "descriptor": "",
    "authors": [
      "J\u00f8rgen Bang-Jensen",
      "Jonas Costa Ferreira da Silva",
      "Fr\u00e9d\u00e9ric Havet"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2105.04137"
  },
  {
    "id": "arXiv:2105.08383",
    "title": "I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text  Recognition",
    "abstract": "Comments: Accepted by special issue Transformer Models in Vision of the Transactions on Pattern Analysis and Machine Intelligence",
    "descriptor": "\nComments: Accepted by special issue Transformer Models in Vision of the Transactions on Pattern Analysis and Machine Intelligence\n",
    "authors": [
      "Chuhui Xue",
      "Jiaxing Huang",
      "Wenqing Zhang",
      "Shijian Lu",
      "Changhu Wang",
      "Song Bai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.08383"
  },
  {
    "id": "arXiv:2105.08585",
    "title": "Revisiting Additive Compositionality: AND, OR and NOT Operations with  Word Embeddings",
    "abstract": "Comments: 13pages; v1: accepted at ACL-IJCNLP 2021 Student Research Workshop; v2: minor revision",
    "descriptor": "\nComments: 13pages; v1: accepted at ACL-IJCNLP 2021 Student Research Workshop; v2: minor revision\n",
    "authors": [
      "Masahiro Naito",
      "Sho Yokoi",
      "Geewook Kim",
      "Hidetoshi Shimodaira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.08585"
  },
  {
    "id": "arXiv:2106.00116",
    "title": "Effect of Pre-Training Scale on Intra- and Inter-Domain Full and  Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images",
    "abstract": "Comments: Short version published in MedNeurIPS 2021. Long version published in IJCNN 2022. Code: this https URL",
    "descriptor": "\nComments: Short version published in MedNeurIPS 2021. Long version published in IJCNN 2022. Code: this https URL\n",
    "authors": [
      "Mehdi Cherti",
      "Jenia Jitsev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.00116"
  },
  {
    "id": "arXiv:2106.04265",
    "title": "Towards Social Role-Based Interruptibility Management",
    "abstract": "Comments: 10 pages, 6 figures, submitted on December 2022, to appear in IEEE Pervasive Computing, Special Issue - Human-Centered AI",
    "descriptor": "\nComments: 10 pages, 6 figures, submitted on December 2022, to appear in IEEE Pervasive Computing, Special Issue - Human-Centered AI\n",
    "authors": [
      "Christoph Anderson",
      "Judith Simone Heinisch",
      "Shohreh Deldari",
      "Flora D. Salim",
      "Sandra Ohly",
      "Klaus David",
      "Veljko Pejovic"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.04265"
  },
  {
    "id": "arXiv:2106.15098",
    "title": "Molecule Generation by Principal Subgraph Mining and Assembling",
    "abstract": "Comments: Accepted by NeurIPS 2022. Oral presentation",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Oral presentation\n",
    "authors": [
      "Xiangzhe Kong",
      "Wenbing Huang",
      "Zhixing Tan",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.15098"
  },
  {
    "id": "arXiv:2107.00841",
    "title": "A Heterogeneous Graph Attention Network for Multi-hop Machine Reading  Comprehension",
    "abstract": "A Heterogeneous Graph Attention Network for Multi-hop Machine Reading  Comprehension",
    "descriptor": "",
    "authors": [
      "Peng Gao",
      "Feng Gao",
      "Jian-Cheng Ni",
      "Hamido Fujita"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2107.00841"
  },
  {
    "id": "arXiv:2107.02363",
    "title": "Asymptotics of Network Embeddings Learned via Subsampling",
    "abstract": "Comments: Under review at JMLR. 120 pages, 3 figures, 1 table",
    "descriptor": "\nComments: Under review at JMLR. 120 pages, 3 figures, 1 table\n",
    "authors": [
      "Andrew Davison",
      "Morgane Austern"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2107.02363"
  },
  {
    "id": "arXiv:2107.05613",
    "title": "Parallel Element-based Algebraic Multigrid for H(curl) and H(div)  Problems Using the ParELAG Library",
    "abstract": "Parallel Element-based Algebraic Multigrid for H(curl) and H(div)  Problems Using the ParELAG Library",
    "descriptor": "",
    "authors": [
      "Delyan Z. Kalchev",
      "Panayot S. Vassilevski",
      "Umberto Villa"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2107.05613"
  },
  {
    "id": "arXiv:2107.06111",
    "title": "Towards exact structural thresholds for parameterized complexity",
    "abstract": "Comments: 52 pages, 16 figures, shortened abstract due to character limit",
    "descriptor": "\nComments: 52 pages, 16 figures, shortened abstract due to character limit\n",
    "authors": [
      "Falko Hegerfeld",
      "Stefan Kratsch"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2107.06111"
  },
  {
    "id": "arXiv:2107.06607",
    "title": "Uncertainty Quantification of Inclusion Boundaries in the Context of  X-ray Tomography",
    "abstract": "Uncertainty Quantification of Inclusion Boundaries in the Context of  X-ray Tomography",
    "descriptor": "",
    "authors": [
      "Babak Maboudi Afkham",
      "Yiqiu Dong",
      "Per Christian Hansen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.06607"
  },
  {
    "id": "arXiv:2107.06916",
    "title": "Training Compact CNNs for Image Classification using Dynamic-coded  Filter Fusion",
    "abstract": "Training Compact CNNs for Image Classification using Dynamic-coded  Filter Fusion",
    "descriptor": "",
    "authors": [
      "Mingbao Lin",
      "Bohong Chen",
      "Fei Chao",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.06916"
  },
  {
    "id": "arXiv:2107.09245",
    "title": "Revisiting Residue Codes for Modern Memories",
    "abstract": "Revisiting Residue Codes for Modern Memories",
    "descriptor": "",
    "authors": [
      "Evgeny Manzhosov",
      "Adam Hastings",
      "Meghna Pancholi",
      "Ryan Piersma",
      "Mohamed Tarek Ibn Ziad",
      "Simha Sethumadhavan"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2107.09245"
  },
  {
    "id": "arXiv:2107.14126",
    "title": "The Complexity of Growing a Graph",
    "abstract": "Comments: 30 pages",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "George B. Mertzios",
      "Othon Michail",
      "George Skretas",
      "Paul G. Spirakis",
      "Michail Theofilatos"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2107.14126"
  },
  {
    "id": "arXiv:2108.06706",
    "title": "Temporal Action Segmentation with High-level Complex Activity Labels",
    "abstract": "Comments: 12 pages, 6 figures",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Guodong Ding",
      "Angela Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.06706"
  },
  {
    "id": "arXiv:2108.07232",
    "title": "Better GPU Hash Tables",
    "abstract": "Comments: Our implementation is available at this https URL",
    "descriptor": "\nComments: Our implementation is available at this https URL\n",
    "authors": [
      "Muhammad A. Awad",
      "Saman Ashkiani",
      "Serban D. Porumbescu",
      "Mart\u00edn Farach-Colton",
      "John D. Owens"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.07232"
  },
  {
    "id": "arXiv:2109.04228",
    "title": "Coordinate Descent Methods for DC Minimization: Optimality Conditions  and Global Convergence",
    "abstract": "Coordinate Descent Methods for DC Minimization: Optimality Conditions  and Global Convergence",
    "descriptor": "",
    "authors": [
      "Ganzhao Yuan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04228"
  },
  {
    "id": "arXiv:2109.05721",
    "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment",
    "abstract": "Comments: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021 (ICCV 2021)",
    "descriptor": "\nComments: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021 (ICCV 2021)\n",
    "authors": [
      "Yangyu Huang",
      "Hao Yang",
      "Chong Li",
      "Jongyoo Kim",
      "Fangyun Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.05721"
  },
  {
    "id": "arXiv:2109.07902",
    "title": "Token-based Insurance Solutions on Blockchain",
    "abstract": "Token-based Insurance Solutions on Blockchain",
    "descriptor": "",
    "authors": [
      "Simon Cousaert",
      "Nikhil Vadgama",
      "Jiahua Xu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2109.07902"
  },
  {
    "id": "arXiv:2109.15296",
    "title": "Electronic Observables for Relaxed Bilayer 2D Heterostructures in  Momentum Space",
    "abstract": "Comments: 38 pages, 10 figures",
    "descriptor": "\nComments: 38 pages, 10 figures\n",
    "authors": [
      "Daniel Massatt",
      "Stephen Carr",
      "Mitchell Luskin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.15296"
  },
  {
    "id": "arXiv:2110.01010",
    "title": "High Capacity Reversible Data Hiding in Encrypted 3D Mesh Models Based  on Multi-MSB Prediction",
    "abstract": "Comments: Published in Signal Processing",
    "descriptor": "\nComments: Published in Signal Processing\n",
    "authors": [
      "Wanli Lv",
      "Lulu Cheng",
      "Zhaoxia Yin"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.01010"
  },
  {
    "id": "arXiv:2110.02355",
    "title": "Robustness and sample complexity of model-based MARL for general-sum  Markov games",
    "abstract": "Robustness and sample complexity of model-based MARL for general-sum  Markov games",
    "descriptor": "",
    "authors": [
      "Jayakumar Subramanian",
      "Amit Sinha",
      "Aditya Mahajan"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.02355"
  },
  {
    "id": "arXiv:2110.03479",
    "title": "Camera Calibration through Camera Projection Loss",
    "abstract": "Comments: 5 pages, ICASSP 2022",
    "descriptor": "\nComments: 5 pages, ICASSP 2022\n",
    "authors": [
      "Talha Hanif Butt",
      "Murtaza Taj"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03479"
  },
  {
    "id": "arXiv:2110.04447",
    "title": "EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals  Measurement",
    "abstract": "EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals  Measurement",
    "descriptor": "",
    "authors": [
      "Xin Liu",
      "Brian L. Hill",
      "Ziheng Jiang",
      "Shwetak Patel",
      "Daniel McDuff"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.04447"
  },
  {
    "id": "arXiv:2110.04996",
    "title": "Learning criteria going beyond the usual risk",
    "abstract": "Comments: Substantial revision of initial draft",
    "descriptor": "\nComments: Substantial revision of initial draft\n",
    "authors": [
      "Matthew J. Holland",
      "Kazuki Tanabe"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.04996"
  },
  {
    "id": "arXiv:2110.07150",
    "title": "Cross-Lingual Open-Domain Question Answering with Answer Sentence  Generation",
    "abstract": "Comments: AACL 2022 Long Paper",
    "descriptor": "\nComments: AACL 2022 Long Paper\n",
    "authors": [
      "Benjamin Muller",
      "Luca Soldaini",
      "Rik Koncel-Kedziorski",
      "Eric Lind",
      "Alessandro Moschitti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07150"
  },
  {
    "id": "arXiv:2110.07510",
    "title": "Omni-Training: Bridging Pre-Training and Meta-Training for Few-Shot  Learning",
    "abstract": "Omni-Training: Bridging Pre-Training and Meta-Training for Few-Shot  Learning",
    "descriptor": "",
    "authors": [
      "Yang Shu",
      "Zhangjie Cao",
      "Jinghan Gao",
      "Jianmin Wang",
      "Philip S. Yu",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07510"
  },
  {
    "id": "arXiv:2110.11707",
    "title": "Variational Wasserstein Barycenters with c-Cyclical Monotonicity",
    "abstract": "Variational Wasserstein Barycenters with c-Cyclical Monotonicity",
    "descriptor": "",
    "authors": [
      "Jinjin Chi",
      "Zhiyao Yang",
      "Jihong Ouyang",
      "Ximing Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.11707"
  },
  {
    "id": "arXiv:2110.12010",
    "title": "ClimateBert: A Pretrained Language Model for Climate-Related Text",
    "abstract": "ClimateBert: A Pretrained Language Model for Climate-Related Text",
    "descriptor": "",
    "authors": [
      "Nicolas Webersinke",
      "Mathias Kraus",
      "Julia Anna Bingler",
      "Markus Leippold"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.12010"
  },
  {
    "id": "arXiv:2110.13632",
    "title": "Generative Networks for Precision Enthusiasts",
    "abstract": "Comments: 28 pages, 14 figures",
    "descriptor": "\nComments: 28 pages, 14 figures\n",
    "authors": [
      "Anja Butter",
      "Theo Heimel",
      "Sander Hummerich",
      "Tobias Krebs",
      "Tilman Plehn",
      "Armand Rousselot",
      "Sophia Vent"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13632"
  },
  {
    "id": "arXiv:2111.04850",
    "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences",
    "abstract": "Comments: Aldo Pacchiano and Aadirupa Saha contributed equally",
    "descriptor": "\nComments: Aldo Pacchiano and Aadirupa Saha contributed equally\n",
    "authors": [
      "Aldo Pacchiano",
      "Aadirupa Saha",
      "Jonathan Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.04850"
  },
  {
    "id": "arXiv:2111.06708",
    "title": "Finite Element Analysis of Time Fractional Integro-differential  Equations of Kirchhoff type for Non-homogeneous Materials",
    "abstract": "Finite Element Analysis of Time Fractional Integro-differential  Equations of Kirchhoff type for Non-homogeneous Materials",
    "descriptor": "",
    "authors": [
      "Lalit Kumar",
      "Sivaji Ganesh Sista",
      "Konijeti Sreenadh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.06708"
  },
  {
    "id": "arXiv:2111.08486",
    "title": "Neural Class Expression Synthesis",
    "abstract": "Comments: 11 pages, 4 figures, 7 tables",
    "descriptor": "\nComments: 11 pages, 4 figures, 7 tables\n",
    "authors": [
      "N'Dah Jean Kouagou",
      "Stefan Heindorf",
      "Caglar Demir",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.08486"
  },
  {
    "id": "arXiv:2111.10589",
    "title": "Garbage Collection or Serialization? Between a Rock and a Hard Place!",
    "abstract": "Comments: 17 pages, 12 figures, asplos23 submission revision",
    "descriptor": "\nComments: 17 pages, 12 figures, asplos23 submission revision\n",
    "authors": [
      "Iacovos G. Kolokasis",
      "Giannos Evdorou",
      "Anastasios Papagiannis",
      "Foivos Zakkak",
      "Christos Kozanitis",
      "Shoaib Akram",
      "Polyvios Pratikakis",
      "Angelos Bilas"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2111.10589"
  },
  {
    "id": "arXiv:2112.00933",
    "title": "PartImageNet: A Large, High-Quality Dataset of Parts",
    "abstract": "PartImageNet: A Large, High-Quality Dataset of Parts",
    "descriptor": "",
    "authors": [
      "Ju He",
      "Shuo Yang",
      "Shaokang Yang",
      "Adam Kortylewski",
      "Xiaoding Yuan",
      "Jie-Neng Chen",
      "Shuai Liu",
      "Cheng Yang",
      "Qihang Yu",
      "Alan Yuille"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00933"
  },
  {
    "id": "arXiv:2112.03277",
    "title": "Automatic quality control framework for more reliable integration of  machine learning-based image segmentation into medical workflows",
    "abstract": "Comments: 19 pages",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Elena Williams",
      "Sebastian Niehaus",
      "Janis Reinelt",
      "Alberto Merola",
      "Paul Glad Mihai",
      "Kersten Villringer",
      "Konstantin Thierbach",
      "Evelyn Medawar",
      "Daniel Lichterfeld",
      "Ingo Roeder",
      "Nico Scherf",
      "Maria del C. Vald\u00e9s Hern\u00e1ndez"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.03277"
  },
  {
    "id": "arXiv:2112.11165",
    "title": "Scalable High-Rate Twin-Field Quantum Key Distribution Networks without  Constraint of Probability and Intensity",
    "abstract": "Comments: 16 pages, 6 figures, 3 tables",
    "descriptor": "\nComments: 16 pages, 6 figures, 3 tables\n",
    "authors": [
      "Yuan-Mei Xie",
      "Chen-Xun Weng",
      "Yu-Shuo Lu",
      "Yao Fu",
      "Yang Wang",
      "Hua-Lei Yin",
      "Zeng-Bing Chen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2112.11165"
  },
  {
    "id": "arXiv:2112.12310",
    "title": "Adversarial Attacks against Windows PE Malware Detection: A Survey of  the State-of-the-Art",
    "abstract": "Adversarial Attacks against Windows PE Malware Detection: A Survey of  the State-of-the-Art",
    "descriptor": "",
    "authors": [
      "Xiang Ling",
      "Lingfei Wu",
      "Jiangyu Zhang",
      "Zhenqing Qu",
      "Wei Deng",
      "Xiang Chen",
      "Yaguan Qian",
      "Chunming Wu",
      "Shouling Ji",
      "Tianyue Luo",
      "Jingzheng Wu",
      "Yanjun Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.12310"
  },
  {
    "id": "arXiv:2112.14513",
    "title": "Spatial Distribution Patterns of Clownfish in Recirculating Aquaculture  Systems",
    "abstract": "Comments: 14 pages, 15 figures",
    "descriptor": "\nComments: 14 pages, 15 figures\n",
    "authors": [
      "Fahad Aljehani",
      "Ibrahima N'Doye",
      "Micaela S. Justo",
      "John E. Majoris",
      "Michael L. Berumen",
      "Taous-Meriem Laleg-Kirati"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.14513"
  },
  {
    "id": "arXiv:2112.14768",
    "title": "Video Reconstruction from a Single Motion Blurred Image using Learned  Dynamic Phase Coding",
    "abstract": "Video Reconstruction from a Single Motion Blurred Image using Learned  Dynamic Phase Coding",
    "descriptor": "",
    "authors": [
      "Erez Yosef",
      "Shay Elmalem",
      "Raja Giryes"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.14768"
  },
  {
    "id": "arXiv:2201.01689",
    "title": "Asymptotics of $\\ell_2$ Regularized Network Embeddings",
    "abstract": "Comments: Accepted in Neural Information Processing Systems 2022. 44 pages, 2 figures, 2 tables",
    "descriptor": "\nComments: Accepted in Neural Information Processing Systems 2022. 44 pages, 2 figures, 2 tables\n",
    "authors": [
      "Andrew Davison"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2201.01689"
  },
  {
    "id": "arXiv:2201.04437",
    "title": "Multi-task Joint Strategies of Self-supervised Representation Learning  on Biomedical Networks for Drug Discovery",
    "abstract": "Comments: 44 pages, 11 figures",
    "descriptor": "\nComments: 44 pages, 11 figures\n",
    "authors": [
      "Xiaoqi Wang",
      "Yingjie Cheng",
      "Yaning Yang",
      "Yue Yu",
      "Fei Li",
      "Shaoliang Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2201.04437"
  },
  {
    "id": "arXiv:2201.05337",
    "title": "A Survey of Controllable Text Generation using Transformer-based  Pre-trained Language Models",
    "abstract": "A Survey of Controllable Text Generation using Transformer-based  Pre-trained Language Models",
    "descriptor": "",
    "authors": [
      "Hanqing Zhang",
      "Haolin Song",
      "Shaoyu Li",
      "Ming Zhou",
      "Dawei Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.05337"
  },
  {
    "id": "arXiv:2201.05401",
    "title": "Agile Effort Estimation: Have We Solved the Problem Yet? Insights From A  Replication Study",
    "abstract": "Comments: Accepted for publication in IEEE Transactions on Software Engineering (TSE, 2022)",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Software Engineering (TSE, 2022)\n",
    "authors": [
      "Vali Tawosi",
      "Rebecca Moussa",
      "Federica Sarro"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.05401"
  },
  {
    "id": "arXiv:2201.13008",
    "title": "Communication-Efficient Distributed Multiple Testing for Large-Scale  Inference",
    "abstract": "Comments: Accepted to the 2022 IEEE International Symposium on Information Theory (ISIT)",
    "descriptor": "\nComments: Accepted to the 2022 IEEE International Symposium on Information Theory (ISIT)\n",
    "authors": [
      "Mehrdad Pournaderi",
      "Yu Xiang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.13008"
  },
  {
    "id": "arXiv:2202.03195",
    "title": "More is Better (Mostly): On the Backdoor Attacks in Federated Graph  Neural Networks",
    "abstract": "Comments: 15 pages, 14 figures",
    "descriptor": "\nComments: 15 pages, 14 figures\n",
    "authors": [
      "Jing Xu",
      "Rui Wang",
      "Stefanos Koffas",
      "Kaitai Liang",
      "Stjepan Picek"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03195"
  },
  {
    "id": "arXiv:2202.04744",
    "title": "Robust Bayesian Inference for Simulator-based Models via the MMD  Posterior Bootstrap",
    "abstract": "Comments: Accepted for publication (with an oral presentation) at AISTATS 2022. A preliminary version of this paper was accepted in the NeurIPS 2021 workshop \"Your Model is Wrong: Robustness and misspecification in probabilistic modeling\". v2: added some references. v3: corrected small error in theorem 3",
    "descriptor": "\nComments: Accepted for publication (with an oral presentation) at AISTATS 2022. A preliminary version of this paper was accepted in the NeurIPS 2021 workshop \"Your Model is Wrong: Robustness and misspecification in probabilistic modeling\". v2: added some references. v3: corrected small error in theorem 3\n",
    "authors": [
      "Charita Dellaporta",
      "Jeremias Knoblauch",
      "Theodoros Damoulas",
      "Fran\u00e7ois-Xavier Briol"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04744"
  },
  {
    "id": "arXiv:2202.05786",
    "title": "Multi-Modal Knowledge Graph Construction and Application: A Survey",
    "abstract": "Comments: 20 pages, 8 figures, 6 tables. Accepted by TKDE 2022",
    "descriptor": "\nComments: 20 pages, 8 figures, 6 tables. Accepted by TKDE 2022\n",
    "authors": [
      "Xiangru Zhu",
      "Zhixu Li",
      "Xiaodan Wang",
      "Xueyao Jiang",
      "Penglei Sun",
      "Xuwu Wang",
      "Yanghua Xiao",
      "Nicholas Jing Yuan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.05786"
  },
  {
    "id": "arXiv:2202.06247",
    "title": "ORBGRAND Is Almost Capacity-Achieving",
    "abstract": "ORBGRAND Is Almost Capacity-Achieving",
    "descriptor": "",
    "authors": [
      "Mengxiao Liu",
      "Yuejun Wei",
      "Zhenyuan Chen",
      "Wenyi Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2202.06247"
  },
  {
    "id": "arXiv:2202.12232",
    "title": "Bounding Membership Inference",
    "abstract": "Bounding Membership Inference",
    "descriptor": "",
    "authors": [
      "Anvith Thudi",
      "Ilia Shumailov",
      "Franziska Boenisch",
      "Nicolas Papernot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12232"
  },
  {
    "id": "arXiv:2203.02351",
    "title": "Uncertainty Estimation for Heatmap-based Landmark Localization",
    "abstract": "Comments: 14 pages, in IEEE Transactions on Medical Imaging, 2022",
    "descriptor": "\nComments: 14 pages, in IEEE Transactions on Medical Imaging, 2022\n",
    "authors": [
      "Lawrence Schobs",
      "Andrew J. Swift",
      "Haiping Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2203.02351"
  },
  {
    "id": "arXiv:2203.03540",
    "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information  from Unstructured Electronic Health Records",
    "abstract": "Comments: 24 pages, 2 figures, 3 tables",
    "descriptor": "\nComments: 24 pages, 2 figures, 3 tables\n",
    "authors": [
      "Xi Yang",
      "Aokun Chen",
      "Nima PourNejatian",
      "Hoo Chang Shin",
      "Kaleb E Smith",
      "Christopher Parisien",
      "Colin Compas",
      "Cheryl Martin",
      "Mona G Flores",
      "Ying Zhang",
      "Tanja Magoc",
      "Christopher A Harle",
      "Gloria Lipori",
      "Duane A Mitchell",
      "William R Hogan",
      "Elizabeth A Shenkman",
      "Jiang Bian",
      "Yonghui Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.03540"
  },
  {
    "id": "arXiv:2203.03990",
    "title": "Skating-Mixer: Long-Term Sport Audio-Visual Modeling with MLPs",
    "abstract": "Comments: Our code is available at this https URL",
    "descriptor": "\nComments: Our code is available at this https URL\n",
    "authors": [
      "Jingfei Xia",
      "Mingchen Zhuge",
      "Tiantian Geng",
      "Shun Fan",
      "Yuantai Wei",
      "Zhenyu He",
      "Feng Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.03990"
  },
  {
    "id": "arXiv:2203.04236",
    "title": "A Complete Characterization of Linear Estimators for Offline Policy  Evaluation",
    "abstract": "Comments: added extensions to misspecified case, comparisons to Bellman residual minimization, 41 pages",
    "descriptor": "\nComments: added extensions to misspecified case, comparisons to Bellman residual minimization, 41 pages\n",
    "authors": [
      "Juan C. Perdomo",
      "Akshay Krishnamurthy",
      "Peter Bartlett",
      "Sham Kakade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.04236"
  },
  {
    "id": "arXiv:2203.05281",
    "title": "Multi-Agent Task Assignment in Vehicular Edge Computing: A  Regret-Matching Learning-Based Approach",
    "abstract": "Comments: 10 pages, 12 figures, and 1 table",
    "descriptor": "\nComments: 10 pages, 12 figures, and 1 table\n",
    "authors": [
      "Bach Long Nguyen",
      "Duong D. Nguyen",
      "Hung X. Nguyen",
      "Duy T. Ngo",
      "Markus Wagner"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.05281"
  },
  {
    "id": "arXiv:2203.08496",
    "title": "Dynamic Grass Color Scale Display Technique Based on Grass Length for  Green Landscape-Friendly Animation Display",
    "abstract": "Comments: 17 pages",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Kojiro Tanaka",
      "Yuichi Kato",
      "Akito Mizuno",
      "Masahiko Mikawa",
      "Makoto Fujisawa"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2203.08496"
  },
  {
    "id": "arXiv:2203.10902",
    "title": "PublicCheck: Public Integrity Verification for Services of Run-time Deep  Models",
    "abstract": "Comments: 18 pages, 9 figures. Accepted to S&P 2023",
    "descriptor": "\nComments: 18 pages, 9 figures. Accepted to S&P 2023\n",
    "authors": [
      "Shuo Wang",
      "Sharif Abuadbba",
      "Sidharth Agarwal",
      "Kristen Moore",
      "Ruoxi Sun",
      "Minhui Xue",
      "Surya Nepal",
      "Seyit Camtepe",
      "Salil Kanhere"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.10902"
  },
  {
    "id": "arXiv:2203.15193",
    "title": "Mismatched Rate-Distortion Theory: Ensembles, Bounds, and General  Alphabets",
    "abstract": "Mismatched Rate-Distortion Theory: Ensembles, Bounds, and General  Alphabets",
    "descriptor": "",
    "authors": [
      "Millen Kanabar",
      "Jonathan Scarlett"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2203.15193"
  },
  {
    "id": "arXiv:2203.16462",
    "title": "Convergence of gradient descent for deep neural networks",
    "abstract": "Comments: 23 pages. The article has been majorly reorganized, by deleting some unnecessary materials. Some minor errors have been fixed. (Theorem numbers have changed in this revision.)",
    "descriptor": "\nComments: 23 pages. The article has been majorly reorganized, by deleting some unnecessary materials. Some minor errors have been fixed. (Theorem numbers have changed in this revision.)\n",
    "authors": [
      "Sourav Chatterjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.16462"
  },
  {
    "id": "arXiv:2204.05422",
    "title": "SATA: Sparsity-Aware Training Accelerator for Spiking Neural Networks",
    "abstract": "Comments: 13 pages. Accepted to IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2022)",
    "descriptor": "\nComments: 13 pages. Accepted to IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2022)\n",
    "authors": [
      "Ruokai Yin",
      "Abhishek Moitra",
      "Abhiroop Bhattacharjee",
      "Youngeun Kim",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2204.05422"
  },
  {
    "id": "arXiv:2204.05909",
    "title": "Learning Performance Graphs from Demonstrations via Task-Based  Evaluations",
    "abstract": "Comments: Published in IEEE Robotics and Automation Letters (RA-L) Vol. 8 Issue 1",
    "descriptor": "\nComments: Published in IEEE Robotics and Automation Letters (RA-L) Vol. 8 Issue 1\n",
    "authors": [
      "Aniruddh G. Puranic",
      "Jyotirmoy V. Deshmukh",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.05909"
  },
  {
    "id": "arXiv:2204.07265",
    "title": "The Rise of Intelligent Reflecting Surfaces in Integrated Sensing and  Communications Paradigms",
    "abstract": "Comments: Accepted paper in IEEE Network Magazine",
    "descriptor": "\nComments: Accepted paper in IEEE Network Magazine\n",
    "authors": [
      "Ahmet M. Elbir",
      "Kumar Vijay Mishra",
      "M. R. Bhavani Shankar",
      "Symeon Chatzinotas"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2204.07265"
  },
  {
    "id": "arXiv:2204.12676",
    "title": "The Multimarginal Optimal Transport Formulation of Adversarial  Multiclass Classification",
    "abstract": "The Multimarginal Optimal Transport Formulation of Adversarial  Multiclass Classification",
    "descriptor": "",
    "authors": [
      "Nicolas Garcia Trillos",
      "Matt Jacobs",
      "Jakwang Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.12676"
  },
  {
    "id": "arXiv:2204.13740",
    "title": "Black-Scholes Option Pricing on Intel CPUs and GPUs: Implementation on  SYCL and Optimization Techniques",
    "abstract": "Comments: 15 pages, 2 figures",
    "descriptor": "\nComments: 15 pages, 2 figures\n",
    "authors": [
      "Elena Panova",
      "Valentin Volokitin",
      "Anton Gorshkov",
      "Iosif Meyerov"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Mathematical Software (cs.MS)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2204.13740"
  },
  {
    "id": "arXiv:2205.00179",
    "title": "Towards Feature Distribution Alignment and Diversity Enhancement for  Data-Free Quantization",
    "abstract": "Comments: Please cite this work as: Yangcheng Gao, Zhao Zhang, Richang Hong, Haijun Zhang, Jicong Fan and Shuicheng Yan, \"Towards Feature Distribution Alignment and Diversity Enhancement for Data-Free Quantization,\" In: Proceedings of the 22nd IEEE International Conference on Data Mining (ICDM), Orlando, FL, USA, pp.1-10, Aug 2022",
    "descriptor": "\nComments: Please cite this work as: Yangcheng Gao, Zhao Zhang, Richang Hong, Haijun Zhang, Jicong Fan and Shuicheng Yan, \"Towards Feature Distribution Alignment and Diversity Enhancement for Data-Free Quantization,\" In: Proceedings of the 22nd IEEE International Conference on Data Mining (ICDM), Orlando, FL, USA, pp.1-10, Aug 2022\n",
    "authors": [
      "Yangcheng Gao",
      "Zhao Zhang",
      "Richang Hong",
      "Haijun Zhang",
      "Jicong Fan",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00179"
  },
  {
    "id": "arXiv:2205.00734",
    "title": "Deterministic pushdown automata can compress some normal sequences",
    "abstract": "Deterministic pushdown automata can compress some normal sequences",
    "descriptor": "",
    "authors": [
      "Olivier Carton",
      "Sylvain Perifel"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.00734"
  },
  {
    "id": "arXiv:2205.01392",
    "title": "A Unified Framework for Verification of Observational Properties for  Partially-Observed Discrete-Event Systems",
    "abstract": "A Unified Framework for Verification of Observational Properties for  Partially-Observed Discrete-Event Systems",
    "descriptor": "",
    "authors": [
      "Jianing Zhao",
      "Xiang Yin",
      "Shaoyuan Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.01392"
  },
  {
    "id": "arXiv:2205.01535",
    "title": "A review of the Separation Theorem of Chebyshev-Markov-Stieltjes for  polynomial and some rational Krylov subspaces",
    "abstract": "A review of the Separation Theorem of Chebyshev-Markov-Stieltjes for  polynomial and some rational Krylov subspaces",
    "descriptor": "",
    "authors": [
      "Tobias Jawecki"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.01535"
  },
  {
    "id": "arXiv:2205.02954",
    "title": "Leveraging Application Data Constraints to Optimize Database-Backed Web  Applications",
    "abstract": "Leveraging Application Data Constraints to Optimize Database-Backed Web  Applications",
    "descriptor": "",
    "authors": [
      "Xiaoxuan Liu",
      "Shuxian Wang",
      "Mengzhu Sun",
      "Sicheng Pan",
      "Ge Li",
      "Siddharth Jha",
      "Cong Yan",
      "Junwen Yang",
      "Shan Lu",
      "Alvin Cheung"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.02954"
  },
  {
    "id": "arXiv:2205.03437",
    "title": "Finding Points in Convex Position in Density-Restricted Sets",
    "abstract": "Comments: 20 pages, 6 figures",
    "descriptor": "\nComments: 20 pages, 6 figures\n",
    "authors": [
      "Adrian Dumitrescu",
      "Csaba D. T\u00f3th"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.03437"
  },
  {
    "id": "arXiv:2205.04358",
    "title": "Towards Implementing Responsible AI",
    "abstract": "Comments: to be published in IEEE International Conference on Big Data, 2022",
    "descriptor": "\nComments: to be published in IEEE International Conference on Big Data, 2022\n",
    "authors": [
      "Conrad Sanderson",
      "Qinghua Lu",
      "David Douglas",
      "Xiwei Xu",
      "Liming Zhu",
      "Jon Whittle"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.04358"
  },
  {
    "id": "arXiv:2205.06970",
    "title": "Learning to Reorient Objects with Stable Placements Afforded by  Extrinsic Supports",
    "abstract": "Learning to Reorient Objects with Stable Placements Afforded by  Extrinsic Supports",
    "descriptor": "",
    "authors": [
      "Peng Xu",
      "Hu Cheng",
      "Jiankun Wang",
      "Max Q.-H. Meng"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.06970"
  },
  {
    "id": "arXiv:2205.13496",
    "title": "Censored Quantile Regression Neural Networks for Distribution-Free  Survival Analysis",
    "abstract": "Comments: Published in NeurIPS 2022",
    "descriptor": "\nComments: Published in NeurIPS 2022\n",
    "authors": [
      "Tim Pearce",
      "Jong-Hyeon Jeong",
      "Yichen Jia",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13496"
  },
  {
    "id": "arXiv:2205.13733",
    "title": "Towards Faithful and Consistent Explanations for Graph Neural Networks",
    "abstract": "Comments: Accepted by WSDM2023",
    "descriptor": "\nComments: Accepted by WSDM2023\n",
    "authors": [
      "Tianxiang Zhao",
      "Dongsheng Luo",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.13733"
  },
  {
    "id": "arXiv:2205.13817",
    "title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in  World Models",
    "abstract": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in  World Models",
    "descriptor": "",
    "authors": [
      "Minting Pan",
      "Xiangming Zhu",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13817"
  },
  {
    "id": "arXiv:2205.13958",
    "title": "Machine Learning-Based User Scheduling in Integrated  Satellite-HAPS-Ground Networks",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2204.13257",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2204.13257\n",
    "authors": [
      "Hayssam Dahrouj",
      "Shasha Liu",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13958"
  },
  {
    "id": "arXiv:2205.15772",
    "title": "The hybrid approach -- Convolutional Neural Networks and Expectation  Maximization Algorithm -- for Tomographic Reconstruction of Hyperspectral  Images",
    "abstract": "Comments: 36 pages, 13 figures and 2 tables. Supplemental material: 21 pages and 14 figures. v2: Clarifications added, analyses and argumentation updated",
    "descriptor": "\nComments: 36 pages, 13 figures and 2 tables. Supplemental material: 21 pages and 14 figures. v2: Clarifications added, analyses and argumentation updated\n",
    "authors": [
      "Mads J. Ahleb\u00e6k",
      "Mads S. Peters",
      "Wei-Chih Huang",
      "Mads T. Frandsen",
      "Ren\u00e9 L. Eriksen",
      "Bjarke J\u00f8rgensen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15772"
  },
  {
    "id": "arXiv:2206.00050",
    "title": "FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear  Modulation",
    "abstract": "Comments: accepted at NeurIPS 2022",
    "descriptor": "\nComments: accepted at NeurIPS 2022\n",
    "authors": [
      "Mehmet Ozgur Turkoglu",
      "Alexander Becker",
      "H\u00fcseyin Anil G\u00fcnd\u00fcz",
      "Mina Rezaei",
      "Bernd Bischl",
      "Rodrigo Caye Daudt",
      "Stefano D'Aronco",
      "Jan Dirk Wegner",
      "Konrad Schindler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00050"
  },
  {
    "id": "arXiv:2206.00145",
    "title": "CASSOCK: Viable Backdoor Attacks against DNN in The Wall of  Source-Specific Backdoor Defences",
    "abstract": "Comments: 13 pages,14 figures",
    "descriptor": "\nComments: 13 pages,14 figures\n",
    "authors": [
      "Shang Wang",
      "Yansong Gao",
      "Anmin Fu",
      "Zhi Zhang",
      "Yuqing Zhang",
      "Willy Susilo",
      "Dongxi Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00145"
  },
  {
    "id": "arXiv:2206.01152",
    "title": "Causal Structure Learning: a Combinatorial Perspective",
    "abstract": "Causal Structure Learning: a Combinatorial Perspective",
    "descriptor": "",
    "authors": [
      "Chandler Squires",
      "Caroline Uhler"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01152"
  },
  {
    "id": "arXiv:2206.01526",
    "title": "Erd\u0151s Matching Conjecture for almost perfect matchings",
    "abstract": "Erd\u0151s Matching Conjecture for almost perfect matchings",
    "descriptor": "",
    "authors": [
      "Dmitriy Kolupaev",
      "Andrey Kupavskii"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.01526"
  },
  {
    "id": "arXiv:2206.04770",
    "title": "A Continuous-Time Perspective on Optimal Methods for Monotone Equation  Problems",
    "abstract": "Comments: 23 Pages; Fixing some confusing points in the proof and simplifying the main context",
    "descriptor": "\nComments: 23 Pages; Fixing some confusing points in the proof and simplifying the main context\n",
    "authors": [
      "Tianyi Lin",
      "Michael. I. Jordan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.04770"
  },
  {
    "id": "arXiv:2206.05314",
    "title": "Large-Scale Retrieval for Reinforcement Learning",
    "abstract": "Comments: Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS 2022), 16 pages",
    "descriptor": "\nComments: Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS 2022), 16 pages\n",
    "authors": [
      "Peter C. Humphreys",
      "Arthur Guez",
      "Olivier Tieleman",
      "Laurent Sifre",
      "Th\u00e9ophane Weber",
      "Timothy Lillicrap"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05314"
  },
  {
    "id": "arXiv:2206.05398",
    "title": "E2PN: Efficient SE(3)-Equivariant Point Network",
    "abstract": "E2PN: Efficient SE(3)-Equivariant Point Network",
    "descriptor": "",
    "authors": [
      "Minghan Zhu",
      "Maani Ghaffari",
      "William A. Clark",
      "Huei Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05398"
  },
  {
    "id": "arXiv:2206.06942",
    "title": "Boolean dimension and dim-boundedness: Planar cover graph with a zero",
    "abstract": "Boolean dimension and dim-boundedness: Planar cover graph with a zero",
    "descriptor": "",
    "authors": [
      "Heather Smith Blake",
      "Piotr Micek",
      "William T. Trotter"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.06942"
  },
  {
    "id": "arXiv:2206.07520",
    "title": "Principal Trade-off Analysis",
    "abstract": "Comments: 17 pages, 8 figures",
    "descriptor": "\nComments: 17 pages, 8 figures\n",
    "authors": [
      "Alexander Strang",
      "David SeWell",
      "Alexander Kim",
      "Kevin Alcedo",
      "David Rosenbluth"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07520"
  },
  {
    "id": "arXiv:2206.08010",
    "title": "MoDi: Unconditional Motion Synthesis from Diverse Data",
    "abstract": "Comments: Video: this https URL, Project page: this https URL, Code: this https URL",
    "descriptor": "\nComments: Video: this https URL, Project page: this https URL, Code: this https URL\n",
    "authors": [
      "Sigal Raab",
      "Inbal Leibovitch",
      "Peizhuo Li",
      "Kfir Aberman",
      "Olga Sorkine-Hornung",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08010"
  },
  {
    "id": "arXiv:2206.08413",
    "title": "Recursion does not always help",
    "abstract": "Comments: Corrected two typos",
    "descriptor": "\nComments: Corrected two typos\n",
    "authors": [
      "Gordon Plotkin"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08413"
  },
  {
    "id": "arXiv:2206.10990",
    "title": "Bridging the gap: Generating a design space model of Socially Assistive  Robots (SARs) for Older Adults using Participatory Design (PD)",
    "abstract": "Comments: 23 pages, 12 figures, 11 tables",
    "descriptor": "\nComments: 23 pages, 12 figures, 11 tables\n",
    "authors": [
      "Adi Bulgaro",
      "Ela Liberman-Pincu",
      "Tal Oron-Gilad"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.10990"
  },
  {
    "id": "arXiv:2206.11072",
    "title": "AlphaMLDigger: A Novel Machine Learning Solution to Explore Excess  Return on Investment",
    "abstract": "AlphaMLDigger: A Novel Machine Learning Solution to Explore Excess  Return on Investment",
    "descriptor": "",
    "authors": [
      "Jimei Shen",
      "Zhehu Yuan",
      "Yifan Jin"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.11072"
  },
  {
    "id": "arXiv:2206.12131",
    "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation",
    "abstract": "MVP: Multi-task Supervised Pre-training for Natural Language Generation",
    "descriptor": "",
    "authors": [
      "Tianyi Tang",
      "Junyi Li",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.12131"
  },
  {
    "id": "arXiv:2206.13444",
    "title": "Decomposing and Executing Serverless Applications as Resource Graphs",
    "abstract": "Decomposing and Executing Serverless Applications as Resource Graphs",
    "descriptor": "",
    "authors": [
      "Zhiyuan Guo",
      "Zachary Blanco",
      "Mohammad Shahrad",
      "Zerui Wei",
      "Bili Dong",
      "Jinmou Li",
      "Ishaan Pota",
      "Harry Xu",
      "Yiying Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.13444"
  },
  {
    "id": "arXiv:2207.00856",
    "title": "Cell-Free Massive MIMO for URLLC: A Finite-Blocklength Analysis",
    "abstract": "Comments: 30 pages, 8 figures, 1 table",
    "descriptor": "\nComments: 30 pages, 8 figures, 1 table\n",
    "authors": [
      "Alejandro Lancho",
      "Giuseppe Durisi",
      "Luca Sanguinetti"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2207.00856"
  },
  {
    "id": "arXiv:2207.03677",
    "title": "SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via  Jointly Architecture Searching and Parameter Pruning",
    "abstract": "Comments: Accepted by ECCV 2022",
    "descriptor": "\nComments: Accepted by ECCV 2022\n",
    "authors": [
      "Haoran You",
      "Baopu Li",
      "Zhanyi Sun",
      "Xu Ouyang",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.03677"
  },
  {
    "id": "arXiv:2207.07338",
    "title": "Context-sensitive neocortical neurons transform the effectiveness and  efficiency of neural information processing",
    "abstract": "Context-sensitive neocortical neurons transform the effectiveness and  efficiency of neural information processing",
    "descriptor": "",
    "authors": [
      "Ahsan Adeel",
      "Mario Franco",
      "Mohsin Raza",
      "Khubaib Ahmed"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.07338"
  },
  {
    "id": "arXiv:2207.08268",
    "title": "Online Lewis Weight Sampling",
    "abstract": "Comments: To appear in SODA 2023; Removed result on adversarial streaming",
    "descriptor": "\nComments: To appear in SODA 2023; Removed result on adversarial streaming\n",
    "authors": [
      "David P. Woodruff",
      "Taisuke Yasuda"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.08268"
  },
  {
    "id": "arXiv:2207.08519",
    "title": "Non-Gaussian Bayesian Filtering by Density Parametrization Using Power  Moments",
    "abstract": "Comments: 14 pages, 5 figures",
    "descriptor": "\nComments: 14 pages, 5 figures\n",
    "authors": [
      "Guangyu Wu",
      "Anders Lindquist"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2207.08519"
  },
  {
    "id": "arXiv:2207.08540",
    "title": "Multi-block-Single-probe Variance Reduced Estimator for Coupled  Compositional Optimization",
    "abstract": "Multi-block-Single-probe Variance Reduced Estimator for Coupled  Compositional Optimization",
    "descriptor": "",
    "authors": [
      "Wei Jiang",
      "Gang Li",
      "Yibo Wang",
      "Lijun Zhang",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2207.08540"
  },
  {
    "id": "arXiv:2207.10409",
    "title": "Sequence Models for Drone vs Bird Classification",
    "abstract": "Sequence Models for Drone vs Bird Classification",
    "descriptor": "",
    "authors": [
      "Fatih Cagatay Akyon",
      "Erdem Akagunduz",
      "Sinan Onur Altinuc",
      "Alptekin Temizel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.10409"
  },
  {
    "id": "arXiv:2207.10796",
    "title": "Multiple Robust Learning for Recommendation",
    "abstract": "Comments: Accepted by AAAI'23",
    "descriptor": "\nComments: Accepted by AAAI'23\n",
    "authors": [
      "Haoxuan Li",
      "Quanyu Dai",
      "Yuru Li",
      "Yan Lyu",
      "Zhenhua Dong",
      "Xiao-Hua Zhou",
      "Peng Wu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.10796"
  },
  {
    "id": "arXiv:2207.12362",
    "title": "OpenRAN Gym: AI/ML Development, Data Collection, and Testing for O-RAN  on PAWR Platforms",
    "abstract": "Comments: 12 pages, 8 figures, 4 tables",
    "descriptor": "\nComments: 12 pages, 8 figures, 4 tables\n",
    "authors": [
      "Leonardo Bonati",
      "Michele Polese",
      "Salvatore D'Oro",
      "Stefano Basagni",
      "Tommaso Melodia"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2207.12362"
  },
  {
    "id": "arXiv:2208.00349",
    "title": "What Do Deep Neural Networks Find in Disordered Structures of Glasses?",
    "abstract": "Comments: 16+12 pages, 8+10 figures",
    "descriptor": "\nComments: 16+12 pages, 8+10 figures\n",
    "authors": [
      "Norihiro Oyama",
      "Shihori Koyama",
      "Takeshi Kawasaki"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.00349"
  },
  {
    "id": "arXiv:2208.01200",
    "title": "Towards Efficient Communications in Federated Learning: A Contemporary  Survey",
    "abstract": "Towards Efficient Communications in Federated Learning: A Contemporary  Survey",
    "descriptor": "",
    "authors": [
      "Zihao Zhao",
      "Yuzhu Mao",
      "Yang Liu",
      "Linqi Song",
      "Ye Ouyang",
      "Xinlei Chen",
      "Wenbo Ding"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2208.01200"
  },
  {
    "id": "arXiv:2208.01709",
    "title": "Adapting Triplet Importance of Implicit Feedback for Personalized  Recommendation",
    "abstract": "Adapting Triplet Importance of Implicit Feedback for Personalized  Recommendation",
    "descriptor": "",
    "authors": [
      "Haolun Wu",
      "Chen Ma",
      "Yingxue Zhang",
      "Xue Liu",
      "Ruiming Tang",
      "Mark Coates"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.01709"
  },
  {
    "id": "arXiv:2208.05232",
    "title": "Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for  Patients with Cerebral Palsy",
    "abstract": "Comments: 7 pages, 4 figures; supplemental material 9 pages, 8 figures",
    "descriptor": "\nComments: 7 pages, 4 figures; supplemental material 9 pages, 8 figures\n",
    "authors": [
      "Alexander Rind",
      "Djordje Slijep\u010devi\u0107",
      "Matthias Zeppelzauer",
      "Fabian Unglaube",
      "Andreas Kranzl",
      "Brian Horsak"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.05232"
  },
  {
    "id": "arXiv:2208.05693",
    "title": "Dynamics of cold random hyperbolic graphs with link persistence",
    "abstract": "Comments: 14 pages, 9 figures. Generalizes the model in arXiv:1907.00073",
    "descriptor": "\nComments: 14 pages, 9 figures. Generalizes the model in arXiv:1907.00073\n",
    "authors": [
      "Sofoclis Zambirinis",
      "Harrison Hartle",
      "Fragkiskos Papadopoulos"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2208.05693"
  },
  {
    "id": "arXiv:2208.06815",
    "title": "An Improved Greedy Algorithm for Stochastic Online Scheduling on  Unrelated Machines",
    "abstract": "Comments: 32 pages, 5 figures",
    "descriptor": "\nComments: 32 pages, 5 figures\n",
    "authors": [
      "Sven J\u00e4ger"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2208.06815"
  },
  {
    "id": "arXiv:2208.08227",
    "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural  Code Generation",
    "abstract": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural  Code Generation",
    "descriptor": "",
    "authors": [
      "Federico Cassano",
      "John Gouwar",
      "Daniel Nguyen",
      "Sydney Nguyen",
      "Luna Phipps-Costin",
      "Donald Pinckney",
      "Ming-Ho Yee",
      "Yangtian Zi",
      "Carolyn Jane Anderson",
      "Molly Q Feldman",
      "Arjun Guha",
      "Michael Greenberg",
      "Abhinav Jangda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2208.08227"
  },
  {
    "id": "arXiv:2208.08376",
    "title": "Image Varifolds on Meshes for Mapping Spatial Transcriptomics",
    "abstract": "Image Varifolds on Meshes for Mapping Spatial Transcriptomics",
    "descriptor": "",
    "authors": [
      "Michael I Miller",
      "Alain Trouv\u00e9",
      "Laurent Younes"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2208.08376"
  },
  {
    "id": "arXiv:2208.09478",
    "title": "Communication Size Reduction of Federated Learning based on Neural ODE  Model",
    "abstract": "Comments: PDF format error corrected",
    "descriptor": "\nComments: PDF format error corrected\n",
    "authors": [
      "Yuto Hoshino",
      "Hiroki Kawakami",
      "Hiroki Matsutani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2208.09478"
  },
  {
    "id": "arXiv:2208.10295",
    "title": "Physical LiDAR Simulation in Real-Time Engine",
    "abstract": "Comments: IEEE Sensors 2022 Conference, Dallas, TX, USA",
    "descriptor": "\nComments: IEEE Sensors 2022 Conference, Dallas, TX, USA\n",
    "authors": [
      "Wouter Jansen",
      "Nico Huebel",
      "Jan Steckel"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2208.10295"
  },
  {
    "id": "arXiv:2208.11475",
    "title": "Droplet-Local Line Integration for Multiphase Flow",
    "abstract": "Droplet-Local Line Integration for Multiphase Flow",
    "descriptor": "",
    "authors": [
      "Alexander Straub",
      "Sebastian Boblest",
      "Grzegorz K. Karch",
      "Filip Sadlo",
      "Thomas Ertl"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2208.11475"
  },
  {
    "id": "arXiv:2208.11530",
    "title": "Collaborative Algorithms for Online Personalized Mean Estimation",
    "abstract": "Comments: Accepted to Transactions on Machine Learning Research (TMLR)",
    "descriptor": "\nComments: Accepted to Transactions on Machine Learning Research (TMLR)\n",
    "authors": [
      "Mahsa Asadi",
      "Aur\u00e9lien Bellet",
      "Odalric-Ambrym Maillard",
      "Marc Tommasi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.11530"
  },
  {
    "id": "arXiv:2209.00613",
    "title": "ID and OOD Performance Are Sometimes Inversely Correlated on Real-world  Datasets",
    "abstract": "ID and OOD Performance Are Sometimes Inversely Correlated on Real-world  Datasets",
    "descriptor": "",
    "authors": [
      "Damien Teney",
      "Yong Lin",
      "Seong Joon Oh",
      "Ehsan Abbasnejad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.00613"
  },
  {
    "id": "arXiv:2209.00780",
    "title": "Index Tracking via Learning to Predict Market Sensitivities",
    "abstract": "Index Tracking via Learning to Predict Market Sensitivities",
    "descriptor": "",
    "authors": [
      "Yoonsik Hong",
      "Yanghoon Kim",
      "Jeonghun Kim",
      "Yongmin Choi"
    ],
    "subjectives": [
      "Portfolio Management (q-fin.PM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.00780"
  },
  {
    "id": "arXiv:2209.00892",
    "title": "Scalable Adversarial Attack Algorithms on Influence Maximization",
    "abstract": "Comments: 11 pages, 2 figures",
    "descriptor": "\nComments: 11 pages, 2 figures\n",
    "authors": [
      "Lichao Sun",
      "Xiaobin Rui",
      "Wei Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.00892"
  },
  {
    "id": "arXiv:2209.01693",
    "title": "Variational Inference for Model-Free and Model-Based Reinforcement  Learning",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2012.13962",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2012.13962\n",
    "authors": [
      "Felix Leibfried"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.01693"
  },
  {
    "id": "arXiv:2209.02965",
    "title": "Risk of Bias in Chest X-ray Foundation Models",
    "abstract": "Comments: Code available under this https URL",
    "descriptor": "\nComments: Code available under this https URL\n",
    "authors": [
      "Ben Glocker",
      "Charles Jones",
      "Melanie Bernhardt",
      "Stefan Winzeck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.02965"
  },
  {
    "id": "arXiv:2209.03648",
    "title": "FETA: Towards Specializing Foundation Models for Expert Task  Applications",
    "abstract": "FETA: Towards Specializing Foundation Models for Expert Task  Applications",
    "descriptor": "",
    "authors": [
      "Amit Alfassy",
      "Assaf Arbelle",
      "Oshri Halimi",
      "Sivan Harary",
      "Roei Herzig",
      "Eli Schwartz",
      "Rameswar Panda",
      "Michele Dolfi",
      "Christoph Auer",
      "Kate Saenko",
      "PeterW. J. Staar",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.03648"
  },
  {
    "id": "arXiv:2209.04899",
    "title": "Instruction-driven history-aware policies for robotic manipulations",
    "abstract": "Comments: Accepted in CoRL 2022 (oral); project page at this https URL",
    "descriptor": "\nComments: Accepted in CoRL 2022 (oral); project page at this https URL\n",
    "authors": [
      "Pierre-Louis Guhur",
      "Shizhe Chen",
      "Ricardo Garcia",
      "Makarand Tapaswi",
      "Ivan Laptev",
      "Cordelia Schmid"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.04899"
  },
  {
    "id": "arXiv:2209.04937",
    "title": "Toward a Framework for Adaptive Impedance Control of an Upper-limb  Prosthesis",
    "abstract": "Toward a Framework for Adaptive Impedance Control of an Upper-limb  Prosthesis",
    "descriptor": "",
    "authors": [
      "Laura Ferrante",
      "Mohan Sridharan",
      "Claudio Zito",
      "Dario Farina"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.04937"
  },
  {
    "id": "arXiv:2209.05644",
    "title": "Proprioceptive State Estimation of Legged Robots with Kinematic Chain  Modeling",
    "abstract": "Comments: Published in Humanoids 2022",
    "descriptor": "\nComments: Published in Humanoids 2022\n",
    "authors": [
      "Varun Agrawal",
      "Sylvain Bertrand",
      "Robert Griffin",
      "Frank Dellaert"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.05644"
  },
  {
    "id": "arXiv:2209.06461",
    "title": "Electric Vehicle Battery Sharing Game for Mobile Energy Storage  Provision in Power Networks",
    "abstract": "Comments: 10 pages, IEEE CDC 2022",
    "descriptor": "\nComments: 10 pages, IEEE CDC 2022\n",
    "authors": [
      "Utkarsha Agwan",
      "Junjie Qin",
      "Kameshwar Poolla",
      "Pravin Varaiya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.06461"
  },
  {
    "id": "arXiv:2209.06767",
    "title": "Parameter-Efficient Finetuning for Robust Continual Multilingual  Learning",
    "abstract": "Parameter-Efficient Finetuning for Robust Continual Multilingual  Learning",
    "descriptor": "",
    "authors": [
      "Kartikeya Badola",
      "Shachi Dave",
      "Partha Talukdar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.06767"
  },
  {
    "id": "arXiv:2209.07042",
    "title": "Efficient Perception, Planning, and Control Algorithms for Vision-Based  Automated Vehicles",
    "abstract": "Comments: 7 figures, 12 pages",
    "descriptor": "\nComments: 7 figures, 12 pages\n",
    "authors": [
      "Der-Hau Lee"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.07042"
  },
  {
    "id": "arXiv:2209.08525",
    "title": "Two-field mixed hp-finite elements for time-dependent problems in the  refined theories of thermodynamics",
    "abstract": "Two-field mixed hp-finite elements for time-dependent problems in the  refined theories of thermodynamics",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs T\u00f3th",
      "Zsombor Moln\u00e1r",
      "R\u00f3bert Kov\u00e1cs"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.08525"
  },
  {
    "id": "arXiv:2209.08807",
    "title": "A Deep Learning Approach for Parallel Imaging and Compressed Sensing MRI  Reconstruction",
    "abstract": "Comments: 13 pages, 11 figures",
    "descriptor": "\nComments: 13 pages, 11 figures\n",
    "authors": [
      "Farhan Sadik",
      "Md. Kamrul Hasan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.08807"
  },
  {
    "id": "arXiv:2209.09371",
    "title": "Towards Quantum Advantage on Noisy Quantum Computers",
    "abstract": "Comments: This paper is a follow up to arXiv:2108.02811 with improved theoretical results and other additional results",
    "descriptor": "\nComments: This paper is a follow up to arXiv:2108.02811 with improved theoretical results and other additional results\n",
    "authors": [
      "Ismail Yunus Akhalwaya",
      "Shashanka Ubaru",
      "Kenneth L. Clarkson",
      "Mark S. Squillante",
      "Vishnu Jejjala",
      "Yang-Hui He",
      "Kugendran Naidoo",
      "Vasileios Kalantzis",
      "Lior Horesh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.09371"
  },
  {
    "id": "arXiv:2209.09464",
    "title": "Rethinking Dimensionality Reduction in Grid-based 3D Object Detection",
    "abstract": "Comments: Submitted to ICRA 2023",
    "descriptor": "\nComments: Submitted to ICRA 2023\n",
    "authors": [
      "Dihe Huang",
      "Ying Chen",
      "Yikang Ding",
      "Jinli Liao",
      "Jianlin Liu",
      "Kai Wu",
      "Qiang Nie",
      "Yong Liu",
      "Chengjie Wang",
      "Zhiheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.09464"
  },
  {
    "id": "arXiv:2209.09716",
    "title": "Recurrence times, waiting times and universal entropy production  estimators",
    "abstract": "Recurrence times, waiting times and universal entropy production  estimators",
    "descriptor": "",
    "authors": [
      "Giampaolo Cristadoro",
      "Mirko Degli Esposti",
      "Vojkan Jak\u0161i\u0107",
      "Renaud Raqu\u00e9pas"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Information Theory (cs.IT)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2209.09716"
  },
  {
    "id": "arXiv:2209.10046",
    "title": "Contractivity of the Method of Successive Approximations for Optimal  Control",
    "abstract": "Contractivity of the Method of Successive Approximations for Optimal  Control",
    "descriptor": "",
    "authors": [
      "Kevin D. Smith",
      "Francesco Bullo"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.10046"
  },
  {
    "id": "arXiv:2209.10285",
    "title": "AirFi: Empowering WiFi-based Passive Human Gesture Recognition to Unseen  Environment via Domain Generalization",
    "abstract": "Comments: The paper has been accepted by IEEE Transactions on Mobile Computing",
    "descriptor": "\nComments: The paper has been accepted by IEEE Transactions on Mobile Computing\n",
    "authors": [
      "Dazhuo Wang",
      "Jianfei Yang",
      "Wei Cui",
      "Lihua Xie",
      "Sumei Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.10285"
  },
  {
    "id": "arXiv:2209.11715",
    "title": "The \"Beatrix'' Resurrections: Robust Backdoor Detection via Gram  Matrices",
    "abstract": "Comments: 18 pages, 23 figures. Accepted to NDSS 2023. Camera-ready version. Code availability: this https URL",
    "descriptor": "\nComments: 18 pages, 23 figures. Accepted to NDSS 2023. Camera-ready version. Code availability: this https URL\n",
    "authors": [
      "Wanlun Ma",
      "Derui Wang",
      "Ruoxi Sun",
      "Minhui Xue",
      "Sheng Wen",
      "Yang Xiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.11715"
  },
  {
    "id": "arXiv:2209.13827",
    "title": "Compressing network populations with modal networks reveals structural  diversity",
    "abstract": "Compressing network populations with modal networks reveals structural  diversity",
    "descriptor": "",
    "authors": [
      "Alec Kirkley",
      "Alexis Rojas",
      "Martin Rosvall",
      "Jean-Gabriel Young"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.13827"
  },
  {
    "id": "arXiv:2209.14169",
    "title": "CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention",
    "abstract": "Comments: Accepted by AAAI 2023, 12 pages, 6 figures",
    "descriptor": "\nComments: Accepted by AAAI 2023, 12 pages, 6 figures\n",
    "authors": [
      "Ziyu Guo",
      "Renrui Zhang",
      "Longtian Qiu",
      "Xianzheng Ma",
      "Xupeng Miao",
      "Xuming He",
      "Bin Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2209.14169"
  },
  {
    "id": "arXiv:2210.00379",
    "title": "NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review",
    "abstract": "NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review",
    "descriptor": "",
    "authors": [
      "Kyle Gao",
      "Yina Gao",
      "Hongjie He",
      "Dening Lu",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.00379"
  },
  {
    "id": "arXiv:2210.00762",
    "title": "Meta-Learning Priors for Safe Bayesian Optimization",
    "abstract": "Comments: Accepted for the Conference on Robot Learning (CoRL) 2022",
    "descriptor": "\nComments: Accepted for the Conference on Robot Learning (CoRL) 2022\n",
    "authors": [
      "Jonas Rothfuss",
      "Christopher Koenig",
      "Alisa Rupenyan",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.00762"
  },
  {
    "id": "arXiv:2210.00911",
    "title": "Learning Equivariant Segmentation with Instance-Unique Querying",
    "abstract": "Comments: Accepted to NeurIPS 2022; Code: this https URL",
    "descriptor": "\nComments: Accepted to NeurIPS 2022; Code: this https URL\n",
    "authors": [
      "Wenguan Wang",
      "James Liang",
      "Dongfang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.00911"
  },
  {
    "id": "arXiv:2210.01126",
    "title": "Wheel Impact Test by Deep Learning: Prediction of Location and Magnitude  of Maximum Stress",
    "abstract": "Wheel Impact Test by Deep Learning: Prediction of Location and Magnitude  of Maximum Stress",
    "descriptor": "",
    "authors": [
      "Seungyeon Shin",
      "Ah-hyeon Jin",
      "Soyoung Yoo",
      "Sunghee Lee",
      "ChangGon Kim",
      "Sungpil Heo",
      "Namwoo Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01126"
  },
  {
    "id": "arXiv:2210.01504",
    "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
    "abstract": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
    "descriptor": "",
    "authors": [
      "Joel Jang",
      "Dongkeun Yoon",
      "Sohee Yang",
      "Sungmin Cha",
      "Moontae Lee",
      "Lajanugen Logeswaran",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.01504"
  },
  {
    "id": "arXiv:2210.01656",
    "title": "Improving Quantum Classifier Performance in NISQ Computers by Voting  Strategy from Ensemble Learning",
    "abstract": "Comments: 6 pages, 5 figures",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Ruiyang Qin",
      "Zhiding Liang",
      "Jinglei Cheng",
      "Peter Kogge",
      "Yiyu Shi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.01656"
  },
  {
    "id": "arXiv:2210.01751",
    "title": "Proportional algebras",
    "abstract": "Proportional algebras",
    "descriptor": "",
    "authors": [
      "Christian Anti\u0107"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.01751"
  },
  {
    "id": "arXiv:2210.01954",
    "title": "Rectangular Ruler Wrapping",
    "abstract": "Rectangular Ruler Wrapping",
    "descriptor": "",
    "authors": [
      "Xing Lyu",
      "Travis Gagie",
      "Meng He"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.01954"
  },
  {
    "id": "arXiv:2210.04156",
    "title": "Optimal Fault-Tolerant Data Fusion in Sensor Networks: Fundamental  Limits and Efficient Algorithms",
    "abstract": "Optimal Fault-Tolerant Data Fusion in Sensor Networks: Fundamental  Limits and Efficient Algorithms",
    "descriptor": "",
    "authors": [
      "Marian Temprana Alonso",
      "Farhad Shirani",
      "S. Sitharama Iyengar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.04156"
  },
  {
    "id": "arXiv:2210.04277",
    "title": "Boost Event-Driven Tactile Learning with Location Spiking Neurons",
    "abstract": "Comments: Under review. Please note that this paper is a journal extension of our previous conference paper: arXiv:2209.01080. Please check what we added in the introduction part",
    "descriptor": "\nComments: Under review. Please note that this paper is a journal extension of our previous conference paper: arXiv:2209.01080. Please check what we added in the introduction part\n",
    "authors": [
      "Peng Kang",
      "Srutarshi Banerjee",
      "Henry Chopp",
      "Aggelos Katsaggelos",
      "Oliver Cossairt"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.04277"
  },
  {
    "id": "arXiv:2210.04371",
    "title": "A Detailed Study of Interpretability of Deep Neural Network based Top  Taggers",
    "abstract": "Comments: Repository: this https URL",
    "descriptor": "\nComments: Repository: this https URL\n",
    "authors": [
      "Ayush Khot",
      "Mark S. Neubauer",
      "Avik Roy"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04371"
  },
  {
    "id": "arXiv:2210.04866",
    "title": "PoGaIN: Poisson-Gaussian Image Noise Modeling from Paired Samples",
    "abstract": "Comments: 5 pages, 4 figures, and 3 tables. Code is available at this https URL",
    "descriptor": "\nComments: 5 pages, 4 figures, and 3 tables. Code is available at this https URL\n",
    "authors": [
      "Nicolas B\u00e4hler",
      "Majed El Helou",
      "\u00c9tienne Objois",
      "Kaan Okumu\u015f",
      "Sabine S\u00fcsstrunk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.04866"
  },
  {
    "id": "arXiv:2210.06543",
    "title": "A General Stochastic Optimization Framework for Convergence Bidding",
    "abstract": "A General Stochastic Optimization Framework for Convergence Bidding",
    "descriptor": "",
    "authors": [
      "Letif Mones",
      "Sean Lovett"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06543"
  },
  {
    "id": "arXiv:2210.07588",
    "title": "Distributed Distributionally Robust Optimization with Non-Convex  Objectives",
    "abstract": "Comments: Accepted to NeurIPS2022",
    "descriptor": "\nComments: Accepted to NeurIPS2022\n",
    "authors": [
      "Yang Jiao",
      "Kai Yang",
      "Dongjin Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07588"
  },
  {
    "id": "arXiv:2210.09083",
    "title": "Nish: A Novel Negative Stimulated Hybrid Activation Function",
    "abstract": "Comments: 10 pages, 2 figures, 2 tables",
    "descriptor": "\nComments: 10 pages, 2 figures, 2 tables\n",
    "authors": [
      "Yildiray Anagun",
      "Sahin Isik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.09083"
  },
  {
    "id": "arXiv:2210.10631",
    "title": "Simulated Contextual Bandits for Personalization Tasks from  Recommendation Datasets",
    "abstract": "Simulated Contextual Bandits for Personalization Tasks from  Recommendation Datasets",
    "descriptor": "",
    "authors": [
      "Anton Dereventsov",
      "Anton Bibin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.10631"
  },
  {
    "id": "arXiv:2210.11489",
    "title": "Machine-Learning Compression for Particle Physics Discoveries",
    "abstract": "Comments: 9 pages, 3 figures",
    "descriptor": "\nComments: 9 pages, 3 figures\n",
    "authors": [
      "Jack H. Collins",
      "Yifeng Huang",
      "Simon Knapen",
      "Benjamin Nachman",
      "Daniel Whiteson"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.11489"
  },
  {
    "id": "arXiv:2210.12698",
    "title": "A class of models for random hypergraphs",
    "abstract": "Comments: 11 pages, 11 figures (final version with additional results)",
    "descriptor": "\nComments: 11 pages, 11 figures (final version with additional results)\n",
    "authors": [
      "Marc Barthelemy"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12698"
  },
  {
    "id": "arXiv:2210.13361",
    "title": "NASA: Neural Architecture Search and Acceleration for Hardware Inspired  Hybrid Networks",
    "abstract": "Comments: Accepted to ICCAD2022",
    "descriptor": "\nComments: Accepted to ICCAD2022\n",
    "authors": [
      "Huihong Shi",
      "Haoran You",
      "Yang Zhao",
      "Zhongfeng Wang",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13361"
  },
  {
    "id": "arXiv:2210.14871",
    "title": "Impact of the 2022 OSTP Memo: A Bibliometric Analysis of U.S. Federally  Funded Publications, 2017-2021",
    "abstract": "Comments: 30 pages, including Appendix. 11 figures, 6 tables. Accepted for publication in Quantitative Science Studies",
    "descriptor": "\nComments: 30 pages, including Appendix. 11 figures, 6 tables. Accepted for publication in Quantitative Science Studies\n",
    "authors": [
      "Eric Schares"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.14871"
  },
  {
    "id": "arXiv:2210.15655",
    "title": "GILP: An Interactive Tool for Visualizing the Simplex Algorithm",
    "abstract": "Comments: ACM SIGCSE 2023 Manuscript, 12 pages, 6 figures",
    "descriptor": "\nComments: ACM SIGCSE 2023 Manuscript, 12 pages, 6 figures\n",
    "authors": [
      "Henry W. Robbins",
      "Samuel C. Gutekunst",
      "David B. Shmoys",
      "David P. Williamson"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.15655"
  },
  {
    "id": "arXiv:2211.01837",
    "title": "Latent Prompt Tuning for Text Summarization",
    "abstract": "Latent Prompt Tuning for Text Summarization",
    "descriptor": "",
    "authors": [
      "Yubo Zhang",
      "Xingxing Zhang",
      "Xun Wang",
      "Si-qing Chen",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2211.01837"
  },
  {
    "id": "arXiv:2211.02069",
    "title": "LMentry: A Language Model Benchmark of Elementary Language Tasks",
    "abstract": "Comments: minor results updates",
    "descriptor": "\nComments: minor results updates\n",
    "authors": [
      "Avia Efrat",
      "Or Honovich",
      "Omer Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2211.02069"
  },
  {
    "id": "arXiv:2211.03622",
    "title": "Do Users Write More Insecure Code with AI Assistants?",
    "abstract": "Comments: 18 pages, 16 figures, update adds names of statistical tests and survey questions",
    "descriptor": "\nComments: 18 pages, 16 figures, update adds names of statistical tests and survey questions\n",
    "authors": [
      "Neil Perry",
      "Megha Srivastava",
      "Deepak Kumar",
      "Dan Boneh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2211.03622"
  },
  {
    "id": "arXiv:2211.03648",
    "title": "Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue  Systems",
    "abstract": "Comments: 23 pages, 10 figures",
    "descriptor": "\nComments: 23 pages, 10 figures\n",
    "authors": [
      "Songbo Hu",
      "Ivan Vuli\u0107",
      "Fangyu Liu",
      "Anna Korhonen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2211.03648"
  },
  {
    "id": "arXiv:2211.04815",
    "title": "The hull of two classical propagation rules and their applications",
    "abstract": "Comments: 31 pages, 6 tables",
    "descriptor": "\nComments: 31 pages, 6 tables\n",
    "authors": [
      "Yang Li",
      "Shixin Zhu",
      "Edgar Mart\u00ednez-Moro"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2211.04815"
  },
  {
    "id": "arXiv:2211.04871",
    "title": "Graph classes equivalent to 12-representable graphs",
    "abstract": "Comments: 12 pages, 6 figures, Corrected typos, Corrected Reference [22]",
    "descriptor": "\nComments: 12 pages, 6 figures, Corrected typos, Corrected Reference [22]\n",
    "authors": [
      "Asahi Takaoka"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2211.04871"
  },
  {
    "id": "arXiv:2211.05205",
    "title": "Maximum Entropy on the Mean and the Cram\u00e9r Rate Function in  Statistical Estimation and Inverse Problems: Properties, Models, and  Algorithms",
    "abstract": "Maximum Entropy on the Mean and the Cram\u00e9r Rate Function in  Statistical Estimation and Inverse Problems: Properties, Models, and  Algorithms",
    "descriptor": "",
    "authors": [
      "Yakov Vaisbourd",
      "Rustum Choksi",
      "Ariel Goodwin",
      "Tim Hoheisel",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2211.05205"
  },
  {
    "id": "arXiv:2211.05855",
    "title": "Robust N-1 secure HV Grid Flexibility Estimation for TSO-DSO coordinated  Congestion Management with Deep Reinforcement Learning",
    "abstract": "Comments: Conference: NEIS 2022, Hamburg",
    "descriptor": "\nComments: Conference: NEIS 2022, Hamburg\n",
    "authors": [
      "Zhenqi Wang",
      "Sebastian Wende-von Berg",
      "Martin Braun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.05855"
  },
  {
    "id": "arXiv:2211.05939",
    "title": "pyRDDLGym: From RDDL to Gym Environments",
    "abstract": "pyRDDLGym: From RDDL to Gym Environments",
    "descriptor": "",
    "authors": [
      "Ayal Taitler",
      "Michael Gimelfarb",
      "Jihwan Jeong",
      "Sriram Gopalakrishnan",
      "Martin Mladenov",
      "Xiaotian Liu",
      "Scott Sanner"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.05939"
  },
  {
    "id": "arXiv:2211.06869",
    "title": "What would Harry say? Building Dialogue Agents for Characters in a Story",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Nuo Chen",
      "Yan Wang",
      "Haiyun Jiang",
      "Deng Cai",
      "Ziyang Chen",
      "Longyue Wang",
      "Jia Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.06869"
  },
  {
    "id": "arXiv:2211.08604",
    "title": "PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention  Networks with Imbalanced PU Labels",
    "abstract": "Comments: Under Review, Industry Track",
    "descriptor": "\nComments: Under Review, Industry Track\n",
    "authors": [
      "Jiho Choi",
      "Junghoon Park",
      "Woocheol Kim",
      "Jin-Hyeok Park",
      "Yumin Suh",
      "Minchang Sung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2211.08604"
  },
  {
    "id": "arXiv:2211.08615",
    "title": "GLFF: Global and Local Feature Fusion for Face Forgery Detection",
    "abstract": "GLFF: Global and Local Feature Fusion for Face Forgery Detection",
    "descriptor": "",
    "authors": [
      "Yan Ju",
      "Shan Jia",
      "Jialing Cai",
      "Haiying Guan",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2211.08615"
  },
  {
    "id": "arXiv:2211.08804",
    "title": "Analysis and Detectability of Offline Data Poisoning Attacks on Linear  Systems",
    "abstract": "Analysis and Detectability of Offline Data Poisoning Attacks on Linear  Systems",
    "descriptor": "",
    "authors": [
      "Alessio Russo",
      "Alexandre Proutiere"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2211.08804"
  },
  {
    "id": "arXiv:2211.09102",
    "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
    "abstract": "Prompting PaLM for Translation: Assessing Strategies and Performance",
    "descriptor": "",
    "authors": [
      "David Vilar",
      "Markus Freitag",
      "Colin Cherry",
      "Jiaming Luo",
      "Viresh Ratnakar",
      "George Foster"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2211.09102"
  },
  {
    "id": "arXiv:2211.09609",
    "title": "Efficiency of Learning from Proof Blocks Versus Writing Proofs",
    "abstract": "Comments: To be presented at SIGCSE 2023",
    "descriptor": "\nComments: To be presented at SIGCSE 2023\n",
    "authors": [
      "Seth Poulsen",
      "Yael Gertner",
      "Benjamin Cosman",
      "Matthew West",
      "Geoffrey L. Herman"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2211.09609"
  },
  {
    "id": "arXiv:2211.09783",
    "title": "UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and  Prefix-Tuning",
    "abstract": "UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and  Prefix-Tuning",
    "descriptor": "",
    "authors": [
      "Yulong Chen",
      "Yang Liu",
      "Ruochen Xu",
      "Ziyi Yang",
      "Chenguang Zhu",
      "Michael Zeng",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2211.09783"
  },
  {
    "id": "arXiv:2211.09848",
    "title": "Fully Digital Second-order Level-crossing Sampling ADC for Data Saving  in Sensing Sparse Signals",
    "abstract": "Comments: Manuscript revised according to reviewers' suggestions, will upload a new version",
    "descriptor": "\nComments: Manuscript revised according to reviewers' suggestions, will upload a new version\n",
    "authors": [
      "Mario Renteria-Pinon",
      "Xiaochen Tang",
      "Jaime Ramirez-Angulo",
      "Wei Tang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2211.09848"
  },
  {
    "id": "arXiv:2211.10265",
    "title": "Context Variance Evaluation of Pretrained Language Models for  Prompt-based Biomedical Knowledge Probing",
    "abstract": "Comments: Presented at the AMIA 2023 Informatics Summit as an oral paper",
    "descriptor": "\nComments: Presented at the AMIA 2023 Informatics Summit as an oral paper\n",
    "authors": [
      "Zonghai Yao",
      "Yi Cao",
      "Zhichao Yang",
      "Hong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.10265"
  },
  {
    "id": "arXiv:2211.11214",
    "title": "DiffBP: Generative Diffusion of 3D Molecules for Target Protein Binding",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Haitao Lin",
      "Yufei Huang",
      "Meng Liu",
      "Xuanjing Li",
      "Shuiwang Ji",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2211.11214"
  },
  {
    "id": "arXiv:2211.11403",
    "title": "Time-reversal equivariant neural network potential and Hamiltonian for  magnetic materials",
    "abstract": "Comments: 17 pages,2 figures and 2 tables",
    "descriptor": "\nComments: 17 pages,2 figures and 2 tables\n",
    "authors": [
      "Hongyu Yu",
      "Yang Zhong",
      "Junyi Ji",
      "Xingao Gong",
      "Hongjun Xiang"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2211.11403"
  },
  {
    "id": "arXiv:2211.11482",
    "title": "Applications of statistical causal inference in software engineering",
    "abstract": "Comments: 38 pages, 12 tables, 9 figures, submitted to Information and Software Technology",
    "descriptor": "\nComments: 38 pages, 12 tables, 9 figures, submitted to Information and Software Technology\n",
    "authors": [
      "Julien Siebert"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.11482"
  },
  {
    "id": "arXiv:2211.11495",
    "title": "Global misinformation spillovers in the online vaccination debate before  and during COVID-19",
    "abstract": "Global misinformation spillovers in the online vaccination debate before  and during COVID-19",
    "descriptor": "",
    "authors": [
      "Jacopo Lenti",
      "Kyriaki Kalimeri",
      "Andr\u00e9 Panisson",
      "Daniela Paolotti",
      "Michele Tizzani",
      "Yelena Mejova",
      "Michele Starnini"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2211.11495"
  },
  {
    "id": "arXiv:2211.11690",
    "title": "Learn to explain yourself, when you can: Equipping Concept Bottleneck  Models with the ability to abstain on their concept predictions",
    "abstract": "Comments: Changed LaTeX template",
    "descriptor": "\nComments: Changed LaTeX template\n",
    "authors": [
      "Joshua Lockhart",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2211.11690"
  },
  {
    "id": "arXiv:2211.11711",
    "title": "CLAWSAT: Towards Both Robust and Accurate Code Models",
    "abstract": "Comments: Accepted by SANER2023",
    "descriptor": "\nComments: Accepted by SANER2023\n",
    "authors": [
      "Jinghan Jia",
      "Shashank Srikant",
      "Tamara Mitrovska",
      "Chuang Gan",
      "Shiyu Chang",
      "Sijia Liu",
      "Una-May O'Reilly"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2211.11711"
  },
  {
    "id": "arXiv:2211.11753",
    "title": "SplitNet: Learnable Clean-Noisy Label Splitting for Learning with Noisy  Labels",
    "abstract": "Comments: project page link: this https URL",
    "descriptor": "\nComments: project page link: this https URL\n",
    "authors": [
      "Daehwan Kim",
      "Kwangrok Ryoo",
      "Hansang Cho",
      "Seungryong Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2211.11753"
  },
  {
    "id": "arXiv:2211.11801",
    "title": "Self-Supervised Pre-training of 3D Point Cloud Networks with Image Data",
    "abstract": "Comments: In Proceedings of the Conference on Robot Learning (CoRL'22) Workshop on Pre-Training Robot Learning, Auckland, New Zealand, December 15, 2022",
    "descriptor": "\nComments: In Proceedings of the Conference on Robot Learning (CoRL'22) Workshop on Pre-Training Robot Learning, Auckland, New Zealand, December 15, 2022\n",
    "authors": [
      "Andrej Janda",
      "Brandon Wagstaff",
      "Edwin G. Ng",
      "Jonathan Kelly"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2211.11801"
  },
  {
    "id": "arXiv:2211.11822",
    "title": "CONFIG: Constrained Efficient Global Optimization for Closed-Loop  Control System Optimization with Unmodeled Constraints",
    "abstract": "CONFIG: Constrained Efficient Global Optimization for Closed-Loop  Control System Optimization with Unmodeled Constraints",
    "descriptor": "",
    "authors": [
      "Wenjie Xu",
      "Yuning Jiang",
      "Bratislav Svetozarevic",
      "Colin N. Jones"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2211.11822"
  },
  {
    "id": "arXiv:2211.12337",
    "title": "Quality-diversity in dissimilarity spaces",
    "abstract": "Comments: Minor bug fix: see new appendix J for details. Only small quantitative effects; no significant changes to results (but all redone)",
    "descriptor": "\nComments: Minor bug fix: see new appendix J for details. Only small quantitative effects; no significant changes to results (but all redone)\n",
    "authors": [
      "Steve Huntsman"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2211.12337"
  },
  {
    "id": "arXiv:2211.13649",
    "title": "End-to-end Wind Turbine Wake Modelling with Deep Graph Representation  Learning",
    "abstract": "Comments: 20 pages, 12 figures",
    "descriptor": "\nComments: 20 pages, 12 figures\n",
    "authors": [
      "Siyi Li",
      "Mingrui Zhang",
      "Matthew D. Piggott"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2211.13649"
  },
  {
    "id": "arXiv:2211.13779",
    "title": "Learning to Play Trajectory Games Against Opponents with Unknown  Objectives",
    "abstract": "Learning to Play Trajectory Games Against Opponents with Unknown  Objectives",
    "descriptor": "",
    "authors": [
      "Xinjie Liu",
      "Lasse Peters",
      "Javier Alonso-Mora"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2211.13779"
  },
  {
    "id": "arXiv:2211.15076",
    "title": "Refined Semantic Enhancement towards Frequency Diffusion for Video  Captioning",
    "abstract": "Refined Semantic Enhancement towards Frequency Diffusion for Video  Captioning",
    "descriptor": "",
    "authors": [
      "Xian Zhong",
      "Zipeng Li",
      "Shuqin Chen",
      "Kui Jiang",
      "Chen Chen",
      "Mang Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2211.15076"
  },
  {
    "id": "arXiv:2211.15081",
    "title": "Flip Initial Features: Generalization of Neural Networks for  Semi-supervised Node Classification",
    "abstract": "Flip Initial Features: Generalization of Neural Networks for  Semi-supervised Node Classification",
    "descriptor": "",
    "authors": [
      "Yoonhyuk Choi",
      "Jiho Choi",
      "Taewook Ko",
      "Chong-Kwon Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.15081"
  },
  {
    "id": "arXiv:2211.15413",
    "title": "Towards Developing Safety Assurance Cases for Learning-Enabled Medical  Cyber-Physical Systems",
    "abstract": "Towards Developing Safety Assurance Cases for Learning-Enabled Medical  Cyber-Physical Systems",
    "descriptor": "",
    "authors": [
      "Maryam Bagheri",
      "Josephine Lamp",
      "Xugui Zhou",
      "Lu Feng",
      "Homa Alemzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2211.15413"
  },
  {
    "id": "arXiv:2211.16059",
    "title": "On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach",
    "abstract": "On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach",
    "descriptor": "",
    "authors": [
      "Mehrdad Pournaderi",
      "Yu Xiang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2211.16059"
  },
  {
    "id": "arXiv:2211.16098",
    "title": "Three-stage binarization of color document images based on discrete  wavelet transform and generative adversarial networks",
    "abstract": "Three-stage binarization of color document images based on discrete  wavelet transform and generative adversarial networks",
    "descriptor": "",
    "authors": [
      "Yu-Shian Lin",
      "Rui-Yang Ju",
      "Chih-Chia Chen",
      "Ting-Yu Lin",
      "Jen-Shiun Chiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2211.16098"
  },
  {
    "id": "arXiv:2211.16509",
    "title": "Multimodal Learning for Multi-Omics: A Survey",
    "abstract": "Comments: 52 pages, 3 figures; Revised matrix factorization fusion section",
    "descriptor": "\nComments: 52 pages, 3 figures; Revised matrix factorization fusion section\n",
    "authors": [
      "Sina Tabakhi",
      "Mohammod Naimul Islam Suvon",
      "Pegah Ahadian",
      "Haiping Lu"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2211.16509"
  },
  {
    "id": "arXiv:2211.16944",
    "title": "AIONER: All-in-one scheme-based biomedical named entity recognition  using deep learning",
    "abstract": "AIONER: All-in-one scheme-based biomedical named entity recognition  using deep learning",
    "descriptor": "",
    "authors": [
      "Ling Luo",
      "Chih-Hsuan Wei",
      "Po-Ting Lai",
      "Robert Leaman",
      "Qingyu Chen",
      "Zhiyong Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2211.16944"
  },
  {
    "id": "arXiv:2212.00112",
    "title": "Approximation of the non-linear water hammer problem by a Lax-Wendroff  finite difference scheme",
    "abstract": "Comments: 25 pages, 4 figures",
    "descriptor": "\nComments: 25 pages, 4 figures\n",
    "authors": [
      "Hugo Carrillo-Lincopi",
      "Alden Waters",
      "Teke Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2212.00112"
  },
  {
    "id": "arXiv:2212.01529",
    "title": "Laplacian Convolutional Representation for Traffic Time Series  Imputation",
    "abstract": "Comments: 13 pages, 8 figures",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Xinyu Chen",
      "Zhanhong Cheng",
      "Nicolas Saunier",
      "Lijun Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.01529"
  },
  {
    "id": "arXiv:2212.02340",
    "title": "CBNet: A Plug-and-Play Network for Segmentation-based Scene Text  Detection",
    "abstract": "CBNet: A Plug-and-Play Network for Segmentation-based Scene Text  Detection",
    "descriptor": "",
    "authors": [
      "Xi Zhao",
      "Wei Feng",
      "Zheng Zhang",
      "Jingjing Lv",
      "Xin Zhu",
      "Zhangang Lin",
      "Jinghe Hu",
      "Jingping Shao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.02340"
  },
  {
    "id": "arXiv:2212.02977",
    "title": "Denoising diffusion probabilistic models for probabilistic energy  forecasting",
    "abstract": "Comments: Version submitted to Powertech 2023. arXiv admin note: text overlap with arXiv:2106.09370, arXiv:2107.01034",
    "descriptor": "\nComments: Version submitted to Powertech 2023. arXiv admin note: text overlap with arXiv:2106.09370, arXiv:2107.01034\n",
    "authors": [
      "Esteban Hernandez Capel",
      "Jonathan Dumas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.02977"
  },
  {
    "id": "arXiv:2212.04064",
    "title": "On CRC-Aided, Dual-Trellis, List Decoding for High-Rate Convolutional  Codes with Short Blocklengths",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2111.07929",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2111.07929\n",
    "authors": [
      "Wenhui Sui",
      "Brendan Towell",
      "Ava Asmani",
      "Hengjie Yang",
      "Richard D. Wesel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2212.04064"
  },
  {
    "id": "arXiv:2212.04068",
    "title": "Investigating Glyph Phonetic Information for Chinese Spell Checking:  What Works and What's Next",
    "abstract": "Investigating Glyph Phonetic Information for Chinese Spell Checking:  What Works and What's Next",
    "descriptor": "",
    "authors": [
      "Xiaotian Zhang",
      "Yanjun Zheng",
      "Hang Yan",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.04068"
  },
  {
    "id": "arXiv:2212.04875",
    "title": "Expeditious Saliency-guided Mix-up through Random Gradient Thresholding",
    "abstract": "Comments: Accepted Long paper at 2nd Practical-DL Workshop at AAAI 2023. V2 fix typo",
    "descriptor": "\nComments: Accepted Long paper at 2nd Practical-DL Workshop at AAAI 2023. V2 fix typo\n",
    "authors": [
      "Minh-Long Luu",
      "Zeyi Huang",
      "Eric P. Xing",
      "Yong Jae Lee",
      "Haohan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.04875"
  },
  {
    "id": "arXiv:2212.04902",
    "title": "Self-Supervised PPG Representation Learning Shows High Inter-Subject  Variability",
    "abstract": "Comments: The current version has been accepted to be presented at ICMLT 2023; Typo corrected in the second author's name",
    "descriptor": "\nComments: The current version has been accepted to be presented at ICMLT 2023; Typo corrected in the second author's name\n",
    "authors": [
      "Ramin Ghorbani",
      "Marcel J.T. Reinders",
      "David M.J. Tax"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.04902"
  },
  {
    "id": "arXiv:2212.05236",
    "title": "Neuromorphic Computing and Sensing in Space",
    "abstract": "Neuromorphic Computing and Sensing in Space",
    "descriptor": "",
    "authors": [
      "Dario Izzo",
      "Alexander Hadjiivanov",
      "Dominik Dold",
      "Gabriele Meoni",
      "Emmanuel Blazquez"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.05236"
  },
  {
    "id": "arXiv:2212.05245",
    "title": "Joint Spatio-Temporal Modeling for the Semantic Change Detection in  Remote Sensing Images",
    "abstract": "Joint Spatio-Temporal Modeling for the Semantic Change Detection in  Remote Sensing Images",
    "descriptor": "",
    "authors": [
      "Lei Ding",
      "Jing Zhang",
      "Kai Zhang",
      "Haitao Guo",
      "Bing Liu",
      "Lorenzo Bruzzone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.05245"
  },
  {
    "id": "arXiv:2212.05421",
    "title": "Feature-Level Debiased Natural Language Understanding",
    "abstract": "Comments: Accepted by AAAI2023",
    "descriptor": "\nComments: Accepted by AAAI2023\n",
    "authors": [
      "Yougang Lyu",
      "Piji Li",
      "Yechang Yang",
      "Maarten de Rijke",
      "Pengjie Ren",
      "Yukun Zhao",
      "Dawei Yin",
      "Zhaochun Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2212.05421"
  },
  {
    "id": "arXiv:2212.05743",
    "title": "A Light-Weight LiDAR-Inertial SLAM System with Loop Closing",
    "abstract": "Comments: ICARM 2022",
    "descriptor": "\nComments: ICARM 2022\n",
    "authors": [
      "Kangcheng Liu",
      "Huosen Ou"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2212.05743"
  },
  {
    "id": "arXiv:2212.05853",
    "title": "DeepCut: Unsupervised Segmentation using Graph Neural Networks  Clustering",
    "abstract": "DeepCut: Unsupervised Segmentation using Graph Neural Networks  Clustering",
    "descriptor": "",
    "authors": [
      "Amit Aflalo",
      "Shai Bagon",
      "Tamar Kashti",
      "Yonina Eldar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.05853"
  },
  {
    "id": "arXiv:2212.05956",
    "title": "Improving Generalization of Pre-trained Language Models via Stochastic  Weight Averaging",
    "abstract": "Comments: Published at EMNLP 2022 (Findings)",
    "descriptor": "\nComments: Published at EMNLP 2022 (Findings)\n",
    "authors": [
      "Peng Lu",
      "Ivan Kobyzev",
      "Mehdi Rezagholizadeh",
      "Ahmad Rashid",
      "Ali Ghodsi",
      "Philippe Langlais"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.05956"
  },
  {
    "id": "arXiv:2212.06109",
    "title": "Optimal thresholds for Latin squares, Steiner Triple Systems, and edge  colorings",
    "abstract": "Comments: 10 pages. Simplified proof; results unchanged",
    "descriptor": "\nComments: 10 pages. Simplified proof; results unchanged\n",
    "authors": [
      "Vishesh Jain",
      "Huy Tuan Pham"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2212.06109"
  },
  {
    "id": "arXiv:2212.06143",
    "title": "Improving Mutual Information based Feature Selection by Boosting Unique  Relevance",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Shiyu Liu",
      "Mehul Motani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.06143"
  },
  {
    "id": "arXiv:2212.06206",
    "title": "Contextual Explainable Video Representation: Human Perception-based  Understanding",
    "abstract": "Comments: Accepted in Asilomar Conference 2022",
    "descriptor": "\nComments: Accepted in Asilomar Conference 2022\n",
    "authors": [
      "Khoa Vo",
      "Kashu Yamazaki",
      "Phong X. Nguyen",
      "Phat Nguyen",
      "Khoa Luu",
      "Ngan Le"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.06206"
  },
  {
    "id": "arXiv:2212.06321",
    "title": "Data Layout from a Type-Theoretic Perspective (extended version)",
    "abstract": "Comments: Extended version of paper to appear in MFPS 2022 special issue of ENTICS; contains additional section and proofs",
    "descriptor": "\nComments: Extended version of paper to appear in MFPS 2022 special issue of ENTICS; contains additional section and proofs\n",
    "authors": [
      "Henry DeYoung",
      "Frank Pfenning"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2212.06321"
  },
  {
    "id": "arXiv:2212.06468",
    "title": "Lisan: Yemeni, Iraqi, Libyan, and Sudanese Arabic Dialect Copora with  Morphological Annotations",
    "abstract": "Lisan: Yemeni, Iraqi, Libyan, and Sudanese Arabic Dialect Copora with  Morphological Annotations",
    "descriptor": "",
    "authors": [
      "Mustafa Jarrar",
      "Fadi A Zaraket",
      "Tymaa Hammouda",
      "Daanish Masood Alavi",
      "Martin Waahlisch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2212.06468"
  },
  {
    "id": "arXiv:2212.06655",
    "title": "The Hateful Memes Challenge Next Move",
    "abstract": "The Hateful Memes Challenge Next Move",
    "descriptor": "",
    "authors": [
      "Weijun Jin",
      "Lance Wilhelm"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.06655"
  },
  {
    "id": "arXiv:2212.06662",
    "title": "Selected Trends in Artificial Intelligence for Space Applications",
    "abstract": "Selected Trends in Artificial Intelligence for Space Applications",
    "descriptor": "",
    "authors": [
      "Dario Izzo",
      "Gabriele Meoni",
      "Pablo G\u00f3mez",
      "Dominik Dold",
      "Alexander Zoechbauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.06662"
  },
  {
    "id": "arXiv:2212.06757",
    "title": "Gradient flow in the gaussian covariate model: exact solution of  learning curves and multiple descent structures",
    "abstract": "Gradient flow in the gaussian covariate model: exact solution of  learning curves and multiple descent structures",
    "descriptor": "",
    "authors": [
      "Antoine Bodin",
      "Nicolas Macris"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.06757"
  },
  {
    "id": "arXiv:2212.07560",
    "title": "Multi-level and multi-modal feature fusion for accurate 3D object  detection in Connected and Automated Vehicles",
    "abstract": "Multi-level and multi-modal feature fusion for accurate 3D object  detection in Connected and Automated Vehicles",
    "descriptor": "",
    "authors": [
      "Yiming Hou",
      "Mahdi Rezaei",
      "Richard Romano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.07560"
  },
  {
    "id": "arXiv:2212.07613",
    "title": "DCS-RISR: Dynamic Channel Splitting for Efficient Real-world Image  Super-Resolution",
    "abstract": "DCS-RISR: Dynamic Channel Splitting for Efficient Real-world Image  Super-Resolution",
    "descriptor": "",
    "authors": [
      "Junbo Qiao",
      "Shaohui Lin",
      "Yunlun Zhang",
      "Wei Li",
      "Jie Hu",
      "Gaoqi He",
      "Changbo Wang",
      "Zhuangli Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.07613"
  },
  {
    "id": "arXiv:2212.07666",
    "title": "Surrogate-assisted level-based learning evolutionary search for heat  extraction optimization of enhanced geothermal system",
    "abstract": "Surrogate-assisted level-based learning evolutionary search for heat  extraction optimization of enhanced geothermal system",
    "descriptor": "",
    "authors": [
      "Guodong Chen",
      "Xin Luo",
      "Chuanyin Jiang",
      "Jiu Jimmy Jiao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2212.07666"
  },
  {
    "id": "arXiv:2212.07671",
    "title": "Multi-task Fusion for Efficient Panoptic-Part Segmentation",
    "abstract": "Comments: Accepted in ICPRAM 2023",
    "descriptor": "\nComments: Accepted in ICPRAM 2023\n",
    "authors": [
      "Sravan Kumar Jagadeesh",
      "Ren\u00e9 Schuster",
      "Didier Stricker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.07671"
  },
  {
    "id": "arXiv:2212.07684",
    "title": "Multi-Agent Reinforcement Learning with Shared Resources for Inventory  Management",
    "abstract": "Comments: Appeared in RL4RealLife@NeurIPS 2022",
    "descriptor": "\nComments: Appeared in RL4RealLife@NeurIPS 2022\n",
    "authors": [
      "Yuandong Ding",
      "Mingxiao Feng",
      "Guozi Liu",
      "Wei Jiang",
      "Chuheng Zhang",
      "Li Zhao",
      "Lei Song",
      "Houqiang Li",
      "Yan Jin",
      "Jiang Bian"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2212.07684"
  },
  {
    "id": "arXiv:2212.07784",
    "title": "RTMDet: An Empirical Study of Designing Real-Time Object Detectors",
    "abstract": "Comments: 15 pages, 4 figures",
    "descriptor": "\nComments: 15 pages, 4 figures\n",
    "authors": [
      "Chengqi Lyu",
      "Wenwei Zhang",
      "Haian Huang",
      "Yue Zhou",
      "Yudong Wang",
      "Yanyi Liu",
      "Shilong Zhang",
      "Kai Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.07784"
  },
  {
    "id": "arXiv:2212.07898",
    "title": "An Effective Methodology for Short-Circuit Calculation of Power Systems  Dominated by Power Electronics Converters Considering Unbalanced Voltage  Conditions and Converter Limits",
    "abstract": "An Effective Methodology for Short-Circuit Calculation of Power Systems  Dominated by Power Electronics Converters Considering Unbalanced Voltage  Conditions and Converter Limits",
    "descriptor": "",
    "authors": [
      "Jie Song",
      "Marc Cheah-Mane",
      "Eduardo Prieto-Araujo",
      "Oriol Gomis-Bellmunt"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2212.07898"
  },
  {
    "id": "arXiv:2212.08008",
    "title": "A New Deep Boosted CNN and Ensemble Learning based IoT Malware Detection",
    "abstract": "Comments: 20 pages, 10 figures, 6 tables; Corresponding saddamhkhan@ueas.edu.pk",
    "descriptor": "\nComments: 20 pages, 10 figures, 6 tables; Corresponding saddamhkhan@ueas.edu.pk\n",
    "authors": [
      "Saddam Hussain Khan",
      "Wasi Ullah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2212.08008"
  },
  {
    "id": "arXiv:2212.08049",
    "title": "Sliced Optimal Partial Transport",
    "abstract": "Sliced Optimal Partial Transport",
    "descriptor": "",
    "authors": [
      "Yikun Bai",
      "Bernard Schmitzer",
      "Mathew Thorpe",
      "Soheil Kolouri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2212.08049"
  },
  {
    "id": "arXiv:2212.08062",
    "title": "MetaPortrait: Identity-Preserving Talking Head Generation with Fast  Personalized Adaptation",
    "abstract": "Comments: Project Page: this https URL",
    "descriptor": "\nComments: Project Page: this https URL\n",
    "authors": [
      "Bowen Zhang",
      "Chenyang Qi",
      "Pan Zhang",
      "Bo Zhang",
      "HsiangTao Wu",
      "Dong Chen",
      "Qifeng Chen",
      "Yong Wang",
      "Fang Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2212.08062"
  },
  {
    "id": "arXiv:2212.08109",
    "title": "An Empirical Study of Deep Learning Models for Vulnerability Detection",
    "abstract": "Comments: 11 pages, 14 figures. Accepted at ICSE 2023 (not camera-ready version). Corrected typos in Listing 2",
    "descriptor": "\nComments: 11 pages, 14 figures. Accepted at ICSE 2023 (not camera-ready version). Corrected typos in Listing 2\n",
    "authors": [
      "Benjamin Steenhoek",
      "Md Mahbubur Rahman",
      "Richard Jiles",
      "Wei Le"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08109"
  },
  {
    "id": "arXiv:2212.08212",
    "title": "The $\\mathbb{DL}(P)$ vector space of pencils for singular matrix  polynomials",
    "abstract": "The $\\mathbb{DL}(P)$ vector space of pencils for singular matrix  polynomials",
    "descriptor": "",
    "authors": [
      "Froil\u00e1n Dopico",
      "Vanni Noferini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2212.08212"
  },
  {
    "id": "arXiv:2212.08216",
    "title": "Azimuth: Systematic Error Analysis for Text Classification",
    "abstract": "Comments: To be published in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 13 pages and 14 figures",
    "descriptor": "\nComments: To be published in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 13 pages and 14 figures\n",
    "authors": [
      "Gabrielle Gauthier-Melan\u00e7on",
      "Orlando Marquez Ayala",
      "Lindsay Brin",
      "Chris Tyler",
      "Fr\u00e9d\u00e9ric Branchaud-Charron",
      "Joseph Marinier",
      "Karine Grande",
      "Di Le"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2212.08216"
  },
  {
    "id": "arXiv:2212.08327",
    "title": "WavEnhancer: Unifying Wavelet and Transformer for Image Enhancement",
    "abstract": "WavEnhancer: Unifying Wavelet and Transformer for Image Enhancement",
    "descriptor": "",
    "authors": [
      "Zinuo Li",
      "Xuhang Chen",
      "Chi-Man Pun",
      "Shuqiang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2212.08327"
  },
  {
    "id": "arXiv:2212.08410",
    "title": "Teaching Small Language Models to Reason",
    "abstract": "Teaching Small Language Models to Reason",
    "descriptor": "",
    "authors": [
      "Lucie Charlotte Magister",
      "Jonathan Mallinson",
      "Jakub Adamek",
      "Eric Malmi",
      "Aliaksei Severyn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08410"
  },
  {
    "id": "arXiv:2212.08624",
    "title": "Development of A Real-time POCUS Image Quality Assessment and  Acquisition Guidance System",
    "abstract": "Development of A Real-time POCUS Image Quality Assessment and  Acquisition Guidance System",
    "descriptor": "",
    "authors": [
      "Zhenge Jia",
      "Yiyu Shi",
      "Jingtong Hu",
      "Lei Yang",
      "Benjamin Nti"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2212.08624"
  }
]