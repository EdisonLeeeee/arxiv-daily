[
  {
    "id": "arXiv:2205.14148",
    "title": "Enhanced physics-informed neural networks for hyperelasticity",
    "abstract": "Physics-informed neural networks have gained growing interest. Specifically,\nthey are used to solve partial differential equations governing several\nphysical phenomena. However, physics-informed neural network models suffer from\nseveral issues and can fail to provide accurate solutions in many scenarios. We\ndiscuss a few of these challenges and the techniques, such as the use of\nFourier transform, that can be used to resolve these issues. This paper\nproposes and develops a physics-informed neural network model that combines the\nresiduals of the strong form and the potential energy, yielding many loss terms\ncontributing to the definition of the loss function to be minimized. Hence, we\npropose using the coefficient of variation weighting scheme to dynamically and\nadaptively assign the weight for each loss term in the loss function. The\ndeveloped PINN model is standalone and meshfree. In other words, it can\naccurately capture the mechanical response without requiring any labeled data.\nAlthough the framework can be used for many solid mechanics problems, we focus\non three-dimensional (3D) hyperelasticity, where we consider two hyperelastic\nmodels. Once the model is trained, the response can be obtained almost\ninstantly at any point in the physical domain, given its spatial coordinates.\nWe demonstrate the framework's performance by solving different problems with\nvarious boundary conditions.",
    "descriptor": "",
    "authors": [
      "Diab W. Abueidda",
      "Seid Koric",
      "Erman Guleryuz",
      "Nahil A. Sobh"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.14148"
  },
  {
    "id": "arXiv:2205.14151",
    "title": "Interactive and Robust Mesh Booleans",
    "abstract": "Boolean operations are among the most used paradigms to create and edit\ndigital shapes. Despite being conceptually simple, the computation of mesh\nBooleans is notoriously challenging. Main issues come from numerical\napproximations that make the detection and processing of intersection points\ninconsistent and unreliable, exposing implementations based on floating point\narithmetic to many kinds of degeneracy and failure. Numerical methods based on\nrational numbers or exact geometric predicates have the needed robustness\nguarantees, that are achieved at the cost of increased computation times that,\nas of today, has always restricted the use of robust mesh Booleans to offline\napplications. We introduce the first algorithm for Boolean operations with\nrobustness guarantees that is capable of operating at interactive frame rates\non meshes with up to 200K triangles. We evaluate our tool thoroughly,\nconsidering not only interactive applications but also batch processing of\nlarge collections of meshes, processing of huge meshes containing millions of\nelements and variadic Booleans of hundreds of shapes altogether. In all these\nexperiments, we consistently outperform prior art by at least one order of\nmagnitude.",
    "descriptor": "",
    "authors": [
      "Gianmarco Cherchi",
      "Fabio Pellacini",
      "Marco Attene",
      "Marco Livesu"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.14151"
  },
  {
    "id": "arXiv:2205.14156",
    "title": "The Internet of People (IoP): A New Wave in Pervasive Mobile Computing",
    "abstract": "Cyber-Physical convergence, the fast expansion of the Internet at its edge,\nand tighter interactions between human users and their personal mobile devices\npush towards an Internet where the human user becomes more central than ever,\nand where their personal devices become their proxies in the cyber world, in\naddition to acting as a fundamental tool to sense the physical world. The\ncurrent Internet paradigm, which is infrastructure-centric, is not the right\none to cope with such emerging scenario with a wider range of applications.\nThis calls for a radically new Internet paradigm, that we name the Internet of\nPeople (IoP), where the humans and their personal devices are not seen merely\nas end users of applications, but become active elements of the Internet. Note\nthat IoP is not a replacement of the current Internet infrastructure, but it\nexploits legacy Internet services as (reliable) primitives to achieve\nend-to-end connectivity on a global-scale. In this visionary paper, we first\ndiscuss the key features of the IoP paradigm along with the underlying research\nissues and challenges. Then we present emerging networking and computing\nparadigms that are anticipating IoP",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.13970\n",
    "authors": [
      "Marco Conti",
      "Andrea Passarella",
      "Sajal K. Das"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14156"
  },
  {
    "id": "arXiv:2205.14173",
    "title": "Momentum Stiefel Optimizer, with Applications to Suitably-Orthogonal  Attention, and Optimal Transport",
    "abstract": "The problem of optimization on Stiefel manifold, i.e., minimizing functions\nof (not necessarily square) matrices that satisfy orthogonality constraints,\nhas been extensively studied, partly due to rich machine learning applications.\nYet, a new approach is proposed based on, for the first time, an interplay\nbetween thoughtfully designed continuous and discrete dynamics. It leads to a\ngradient-based optimizer with intrinsically added momentum. This method exactly\npreserves the manifold structure but does not require commonly used projection\nor retraction, and thus having low computational costs when compared to\nexisting algorithms. Its generalization to adaptive learning rates is also\ndemonstrated. Pleasant performances are observed in various practical tasks.\nFor instance, we discover that placing orthogonal constraints on attention\nheads of trained-from-scratch Vision Transformer [Dosovitskiy et al. 2022]\ncould remarkably improve its performance, when our optimizer is used, and it is\nbetter that each head is made orthogonal within itself but not necessarily to\nother heads. This optimizer also makes the useful notion of Projection Robust\nWasserstein Distance [Paty & Cuturi 2019][Lin et al. 2020] for high-dim.\noptimal transport even more effective.",
    "descriptor": "\nComments: Comments are welcome\n",
    "authors": [
      "Lingkai Kong",
      "Yuqing Wang",
      "Molei Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14173"
  },
  {
    "id": "arXiv:2205.14177",
    "title": "Multiscale Voxel Based Decoding For Enhanced Natural Image  Reconstruction From Brain Activity",
    "abstract": "Reconstructing perceived images from human brain activity monitored by\nfunctional magnetic resonance imaging (fMRI) is hard, especially for natural\nimages. Existing methods often result in blurry and unintelligible\nreconstructions with low fidelity. In this study, we present a novel approach\nfor enhanced image reconstruction, in which existing methods for object\ndecoding and image reconstruction are merged together. This is achieved by\nconditioning the reconstructed image to its decoded image category using a\nclass-conditional generative adversarial network and neural style transfer. The\nresults indicate that our approach improves the semantic similarity of the\nreconstructed images and can be used as a general framework for enhanced image\nreconstruction.",
    "descriptor": "\nComments: Accepted at 2022 International Joint Conference on Neural Networks\n",
    "authors": [
      "Mali Halac",
      "Murat Isik",
      "Hasan Ayaz",
      "Anup Das"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.14177"
  },
  {
    "id": "arXiv:2205.14181",
    "title": "Direction and Trajectory Tracking Control for Nonholonomic Spherical  Robot by Combining Sliding Mode Controller and Model Prediction Controller",
    "abstract": "Spherical robot is a nonlinear, nonholonomic and unstable system which\nincreases the difficulty of the direction and trajectory tracking problem. In\nthis study, we propose a new direction controller HTSMC, an instruction\nplanning controller MPC, and a trajectory tracking framework MHH. The HTSMC is\ndesigned by integrating a fast terminal algorithm, a hierarchical method, the\nmotion features of a spherical robot, and its dynamics. In addition, the new\ndirection controller has an excellent control effect with a quick response\nspeed and strong stability. MPC can obtain optimal commands that are then\ntransmitted to the velocity and direction controller. Since the two torque\ncontrollers in MHH are all Lyapunov-based sliding mode controllers, the MHH\nframework may achieve optimal control performance while assuring stability.\nFinally, the two controllers eliminate the requirement for MPC's stability and\ndynamic constraints. Finally, hardware experiments demonstrate the efficacy of\nthe HTSMC, MPC, and MHH.",
    "descriptor": "\nComments: This work has been submitted to the IEEE Robotics and Automation Letters RA-L for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Yifan Liu",
      "Yixu Wang",
      "Xiaoqing Guan",
      "Tao Hu",
      "Ziang Zhang",
      "Song Jin",
      "You Wang",
      "Jie Hao",
      "Guang Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14181"
  },
  {
    "id": "arXiv:2205.14182",
    "title": "Who is we? Disambiguating the referents of first person plural pronouns  in parliamentary debates",
    "abstract": "This paper investigates the use of first person plural pronouns as a\nrhetorical device in political speeches. We present an annotation schema for\ndisambiguating pronoun references and use our schema to create an annotated\ncorpus of debates from the German Bundestag. We then use our corpus to learn to\nautomatically resolve pronoun referents in parliamentary debates. We explore\nthe use of data augmentation with weak supervision to further expand our corpus\nand report preliminary results.",
    "descriptor": "\nComments: Ines Rehbein, Josef Ruppenhofer, and Julian Bernauer. 2021. Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates. In Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021), pages 147--158, D\\\"usseldorf, Germany\n",
    "authors": [
      "Ines Rehbein",
      "Josef Ruppenhofer",
      "Julian Bernauer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14182"
  },
  {
    "id": "arXiv:2205.14188",
    "title": "Introducing k4.0s: a Model for Mixed-Criticality Container Orchestration  in Industry 4.0",
    "abstract": "Time predictable edge cloud is seen as the answer for many arising needs in\nIndustry 4.0 environments, since it is able to provide flexible, modular, and\nreconfigurable services with low latency and reduced costs. Orchestration\nsystems are becoming the core component of clouds since they take decisions on\nthe placement and lifecycle of software components. Current solutions start\nintroducing real-time containers support for time predictability; however,\nthese approaches lack of determinism as well as support for workloads requiring\nmultiple levels of assurance/criticality.\nIn this paper, we present k4.0s, an orchestration model for real-time and\nmixed-criticality environments, which includes timeliness, criticality and\nnetwork requirements. The model leverages new abstractions for both node and\njobs, e.g., node assurance, and requires novel monitoring strategies. We sketch\nan implementation of the proposal based on Kubernetes, and present an\nexperimentation motivating the need for node assurance levels and adequate\nmonitoring.",
    "descriptor": "\nComments: This paper has been accepted for RT-Cloud workshop @ (Euromicro Conference on Real-Time Systems) ECRTS 2022\n",
    "authors": [
      "Marco Barletta",
      "Marcello Cinque",
      "Luigi De Simone",
      "Raffaele Della Corte"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14188"
  },
  {
    "id": "arXiv:2205.14191",
    "title": "Sensing Eating Events in Context: A Smartphone-Only Approach",
    "abstract": "While the task of automatically detecting eating events has been examined in\nprior work using various wearable devices, the use of smartphones as standalone\ndevices to infer eating events remains an open issue. This paper proposes a\nframework that infers eating vs. non-eating events from passive smartphone\nsensing and evaluates it on a dataset of 58 college students. First, we show\nthat time of the day and features from modalities such as screen usage,\naccelerometer, app usage, and location are indicative of eating and non-eating\nevents. Then, we show that eating events can be inferred with an AUROC (area\nunder the receiver operating characteristics curve) of 0.65 using\nsubject-independent machine learning models, which can be further improved up\nto 0.81 for subject-dependent and 0.81 for hybrid models using personalization\ntechniques. Moreover, we show that users have different behavioral and\ncontextual routines around eating episodes requiring specific feature groups to\ntrain fully personalized models. These findings are of potential value for\nfuture mobile food diary apps that are context-aware by enabling scalable\nsensing-based eating studies using only smartphones; detecting under-reported\neating events, thus increasing data quality in self report-based studies;\nproviding functionality to track food consumption and generate reminders for\non-time collection of food diaries; and supporting mobile interventions towards\nhealthy eating practices.",
    "descriptor": "\nComments: Accepted for publication at IEEE Access\n",
    "authors": [
      "Wageesha Bangamuarachchi",
      "Anju Chamantha",
      "Lakmal Meegahapola",
      "Salvador Ruiz-Correa",
      "Indika Perera",
      "Daniel Gatica-Perez"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.14191"
  },
  {
    "id": "arXiv:2205.14192",
    "title": "Constrained Langevin Algorithms with L-mixing External Random Variables",
    "abstract": "Langevin algorithms are gradient descent methods augmented with additive\nnoise, and are widely used in Markov Chain Monte Carlo (MCMC) sampling,\noptimization, and learning. In recent years, the non-asymptotic analysis of\nLangevin algorithms for non-convex optimization learning has been extensively\nexplored. For constrained problems with non-convex losses over compact convex\ndomain in the case of IID data variables, Langevin algorithm achieves a\ndeviation of $O(T^{-1/4} (\\log T)^{1/2})$ from its target distribution [22]. In\nthis paper, we obtain a deviation of $O(T^{-1/2} \\log T)$ in $1$-Wasserstein\ndistance for non-convex losses with $L$-mixing data variables and polyhedral\nconstraints (which are not necessarily bounded). This deviation indicates that\nour convergence rate is faster than those in the previous works on constrained\nLangevin algorithms for non-convex optimization.",
    "descriptor": "\nComments: 34 pages. Under Review for NeurIPS 2022\n",
    "authors": [
      "Yuping Zheng",
      "Andrew Lamperski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.14192"
  },
  {
    "id": "arXiv:2205.14195",
    "title": "Unsupervised learning of features and object boundaries from local  prediction",
    "abstract": "A visual system has to learn both which features to extract from images and\nhow to group locations into (proto-)objects. Those two aspects are usually\ndealt with separately, although predictability is discussed as a cue for both.\nTo incorporate features and boundaries into the same model, we model a layer of\nfeature maps with a pairwise Markov random field model in which each factor is\npaired with an additional binary variable, which switches the factor on or off.\nUsing one of two contrastive learning objectives, we can learn both the\nfeatures and the parameters of the Markov random field factors from images\nwithout further supervision signals. The features learned by shallow neural\nnetworks based on this loss are local averages, opponent colors, and Gabor-like\nstripe patterns. Furthermore, we can infer connectivity between locations by\ninferring the switch variables. Contours inferred from this connectivity\nperform quite well on the Berkeley segmentation database (BSDS500) without any\ntraining on contours. Thus, computing predictions across space aids both\nsegmentation and feature learning, and models trained to optimize these\npredictions show similarities to the human visual system. We speculate that\nretinotopic visual cortex might implement such predictions over space through\nlateral connections.",
    "descriptor": "\nComments: Submitted to NeurIPS 2022\n",
    "authors": [
      "Heiko H. Sch\u00fctt",
      "Wei Ji Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.14195"
  },
  {
    "id": "arXiv:2205.14196",
    "title": "FadMan: Federated Anomaly Detection across Multiple Attributed Networks",
    "abstract": "Anomaly subgraph detection has been widely used in various applications,\nranging from cyber attack in computer networks to malicious activities in\nsocial networks. Despite an increasing need for federated anomaly detection\nacross multiple attributed networks, only a limited number of approaches are\navailable for this problem. Federated anomaly detection faces two major\nchallenges. One is that isolated data in most industries are restricted share\nwith others for data privacy and security. The other is most of the centralized\napproaches training based on data integration. The main idea of federated\nanomaly detection is aligning private anomalies from local data owners on the\npublic anomalies from the attributed network in the server through public\nanomalies to federate local anomalies. In each private attributed network, the\ndetected anomaly subgraph is aligned with an anomaly subgraph in the public\nattributed network. The significant public anomaly subgraphs are selected for\nfederated private anomalies while preventing local private data leakage. The\nproposed algorithm FadMan is a vertical federated learning framework for public\nnode aligned with many private nodes of different features, and is validated on\ntwo tasks correlated anomaly detection on multiple attributed networks and\nanomaly detection on an attributeless network using five real-world datasets.\nIn the first scenario, FadMan outperforms competitive methods by at least 12%\naccuracy at 10% noise level. In the second scenario, by analyzing the\ndistribution of abnormal nodes, we find that the nodes of traffic anomalies are\nassociated with the event of postgraduate entrance examination on the same day.",
    "descriptor": "",
    "authors": [
      "Nannan Wu",
      "Ning Zhang",
      "Wenjun Wang",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14196"
  },
  {
    "id": "arXiv:2205.14198",
    "title": "Generalized Reductions: Making any Hierarchical Clustering Fair and  Balanced with Low Cost",
    "abstract": "Clustering is a fundamental building block of modern statistical analysis\npipelines. Fair clustering has seen much attention from the machine learning\ncommunity in recent years. We are some of the first to study fairness in the\ncontext of hierarchical clustering, after the results of Ahmadian et al. from\nNeurIPS in 2020. We evaluate our results using Dasgupta's cost function,\nperhaps one of the most prevalent theoretical metrics for hierarchical\nclustering evaluation. Our work vastly improves the previous\n$O(n^{5/6}poly\\log(n))$ fair approximation for cost to a near polylogarithmic\n$O(n^\\delta poly\\log(n))$ fair approximation for any constant $\\delta\\in(0,1)$.\nThis result establishes a cost-fairness tradeoff and extends to broader\nfairness constraints than the previous work. We also show how to alter existing\nhierarchical clusterings to guarantee fairness and cluster balance across any\nlevel in the hierarchy.",
    "descriptor": "",
    "authors": [
      "Marina Knittel",
      "John P. Dickerson",
      "MohammadTaghi Hajiaghayi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14198"
  },
  {
    "id": "arXiv:2205.14204",
    "title": "Multimodal Masked Autoencoders Learn Transferable Representations",
    "abstract": "Building scalable models to learn from diverse, multimodal data remains an\nopen challenge. For vision-language data, the dominant approaches are based on\ncontrastive learning objectives that train a separate encoder for each\nmodality. While effective, contrastive learning approaches introduce sampling\nbias depending on the data augmentations used, which can degrade performance on\ndownstream tasks. Moreover, these methods are limited to paired image-text\ndata, and cannot leverage widely-available unpaired data. In this paper, we\ninvestigate whether a large multimodal model trained purely via masked token\nprediction, without using modality-specific encoders or contrastive learning,\ncan learn transferable representations for downstream tasks. We propose a\nsimple and scalable network architecture, the Multimodal Masked Autoencoder\n(M3AE), which learns a unified encoder for both vision and language data via\nmasked token prediction. We provide an empirical study of M3AE trained on a\nlarge-scale image-text dataset, and find that M3AE is able to learn\ngeneralizable representations that transfer well to downstream tasks.\nSurprisingly, we find that M3AE benefits from a higher text mask ratio\n(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the\njoint training of two data modalities. We also provide qualitative analysis\nshowing that the learned representation incorporates meaningful information\nfrom both image and language. Lastly, we demonstrate the scalability of M3AE\nwith larger model size and training time, and its flexibility to train on both\npaired image-text data as well as unpaired data.",
    "descriptor": "",
    "authors": [
      "Xinyang Geng",
      "Hao Liu",
      "Lisa Lee",
      "Dale Schuurams",
      "Sergey Levine",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14204"
  },
  {
    "id": "arXiv:2205.14205",
    "title": "ALMA: Hierarchical Learning for Composite Multi-Agent Tasks",
    "abstract": "Despite significant progress on multi-agent reinforcement learning (MARL) in\nrecent years, coordination in complex domains remains a challenge. Work in MARL\noften focuses on solving tasks where agents interact with all other agents and\nentities in the environment; however, we observe that real-world tasks are\noften composed of several isolated instances of local agent interactions\n(subtasks), and each agent can meaningfully focus on one subtask to the\nexclusion of all else in the environment. In these composite tasks, successful\npolicies can often be decomposed into two levels of decision-making: agents are\nallocated to specific subtasks and each agent acts productively towards their\nassigned subtask alone. This decomposed decision making provides a strong\nstructural inductive bias, significantly reduces agent observation spaces, and\nencourages subtask-specific policies to be reused and composed during training,\nas opposed to treating each new composition of subtasks as unique. We introduce\nALMA, a general learning method for taking advantage of these structured tasks.\nALMA simultaneously learns a high-level subtask allocation policy and low-level\nagent policies. We demonstrate that ALMA learns sophisticated coordination\nbehavior in a number of challenging environments, outperforming strong\nbaselines. ALMA's modularity also enables it to better generalize to new\nenvironment configurations. Finally, we find that while ALMA can integrate\nseparately trained allocation and action policies, the best performance is\nobtained only by training all components jointly.",
    "descriptor": "",
    "authors": [
      "Shariq Iqbal",
      "Robby Costales",
      "Fei Sha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14205"
  },
  {
    "id": "arXiv:2205.14206",
    "title": "Network Digital Twin: Context, Enabling Technologies and Opportunities",
    "abstract": "The proliferation of emergent network applications (e.g., telesurgery,\nmetaverse) is increasing the difficulty of managing modern communication\nnetworks. These applications entail stringent network requirements (e.g.,\nultra-low deterministic latency), which hinders network operators to manage\ntheir resources efficiently. In this article, we introduce the network digital\ntwin (NDT), a renovated concept of classical network modeling tools whose goal\nis to build accurate data-driven network models that can operate in real-time.\nWe describe the general architecture of the NDT and argue that modern machine\nlearning (ML) technologies enable building some of its core components. Then,\nwe present a case study that leverages a ML-based NDT for network performance\nevaluation and apply it to routing optimization in a QoS-aware use case.\nLastly, we describe some key open challenges and research opportunities yet to\nbe explored to achieve effective deployment of NDTs in real-world networks.",
    "descriptor": "\nComments: 7 pages, 4 figures. arXiv admin note: text overlap with arXiv:2201.01144\n",
    "authors": [
      "Paul Almasan",
      "Miquel Ferriol-Galm\u00e9s",
      "Jordi Paillisse",
      "Jos\u00e9 Su\u00e1rez-Varela",
      "Diego Perino",
      "Diego L\u00f3pez",
      "Antonio Agustin Pastor Perales",
      "Paul Harvey",
      "Laurent Ciavaglia",
      "Leon Wong",
      "Vishnu Ram",
      "Shihan Xiao",
      "Xiang Shi",
      "Xiangle Cheng",
      "Albert Cabellos-Aparicio",
      "Pere Barlet-Ros"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14206"
  },
  {
    "id": "arXiv:2205.14208",
    "title": "Targeted Adaptive Design",
    "abstract": "Modern advanced manufacturing and advanced materials design often require\nsearches of relatively high-dimensional process control parameter spaces for\nsettings that result in optimal structure, property, and performance\nparameters. The mapping from the former to the latter must be determined from\nnoisy experiments or from expensive simulations. We abstract this problem to a\nmathematical framework in which an unknown function from a control space to a\ndesign space must be ascertained by means of expensive noisy measurements,\nwhich locate optimal control settings generating desired design features within\nspecified tolerances, with quantified uncertainty. We describe targeted\nadaptive design (TAD), a new algorithm that performs this optimal sampling\ntask. TAD creates a Gaussian process surrogate model of the unknown mapping at\neach iterative stage, proposing a new batch of control settings to sample\nexperimentally and optimizing the updated log-predictive likelihood of the\ntarget design. TAD either stops upon locating a solution with uncertainties\nthat fit inside the tolerance box or uses a measure of expected future\ninformation to determine that the search space has been exhausted with no\nsolution. TAD thus embodies the exploration-exploitation tension in a manner\nthat recalls, but is essentially different from, Bayesian optimization and\noptimal experimental design.",
    "descriptor": "\nComments: Submitted to SIAM/ASA Journal on Uncertainty Quantification\n",
    "authors": [
      "Carlo Graziani",
      "Marieme Ngom"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14208"
  },
  {
    "id": "arXiv:2205.14209",
    "title": "StarGraph: A Coarse-to-Fine Representation Method for Large-Scale  Knowledge Graph",
    "abstract": "Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector, ignoring the rich information\ncontained in neighbor entities. We propose a method named StarGraph, which\ngives a novel way to utilize the neighborhood information for large-scale\nknowledge graphs to get better entity representations. The core idea is to\ndivide the neighborhood information into different levels for sampling and\nprocessing, where the generalized coarse-grained information and unique\nfine-grained information are combined to generate an efficient subgraph for\neach node. In addition, a self-attention network is proposed to process the\nsubgraphs and get the entity representations, which are used to replace the\nentity embeddings in conventional methods. The proposed method achieves the\nbest results on the ogbl-wikikg2 dataset, which validates the effectiveness of\nit. The code is now available at https://github.com/hzli-ucas/StarGraph",
    "descriptor": "",
    "authors": [
      "Hongzhu Li",
      "Xiangrui Gao",
      "Yafeng Deng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14209"
  },
  {
    "id": "arXiv:2205.14210",
    "title": "MIP-GNN: A Data-Driven Framework for Guiding Combinatorial Solvers",
    "abstract": "Mixed-integer programming (MIP) technology offers a generic way of\nformulating and solving combinatorial optimization problems. While generally\nreliable, state-of-the-art MIP solvers base many crucial decisions on\nhand-crafted heuristics, largely ignoring common patterns within a given\ninstance distribution of the problem of interest. Here, we propose MIP-GNN, a\ngeneral framework for enhancing such solvers with data-driven insights. By\nencoding the variable-constraint interactions of a given mixed-integer linear\nprogram (MILP) as a bipartite graph, we leverage state-of-the-art graph neural\nnetwork architectures to predict variable biases, i.e., component-wise averages\nof (near) optimal solutions, indicating how likely a variable will be set to 0\nor 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases\nstemming from a single, once-trained model are used to guide the solver,\nreplacing heuristic components. We integrate MIP-GNN into a state-of-the-art\nMIP solver, applying it to tasks such as node selection and warm-starting,\nshowing significant improvements compared to the default setting of the solver\non two classes of challenging binary MILPs.",
    "descriptor": "\nComments: AAAI 2022\n",
    "authors": [
      "Elias B. Khalil",
      "Christopher Morris",
      "Andrea Lodi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14210"
  },
  {
    "id": "arXiv:2205.14211",
    "title": "KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal",
    "abstract": "In this work, we consider and analyze the sample complexity of model-free\nreinforcement learning with a generative model. Particularly, we analyze mirror\ndescent value iteration (MDVI) by Geist et al. (2019) and Vieillard et al.\n(2020a), which uses the Kullback-Leibler divergence and entropy regularization\nin its value and policy updates. Our analysis shows that it is nearly\nminimax-optimal for finding an $\\varepsilon$-optimal policy when $\\varepsilon$\nis sufficiently small. This is the first theoretical result that demonstrates\nthat a simple model-free algorithm without variance-reduction can be nearly\nminimax-optimal under the considered setting.",
    "descriptor": "\nComments: 29 pages, 6 figures\n",
    "authors": [
      "Tadashi Kozuno",
      "Wenhao Yang",
      "Nino Vieillard",
      "Toshinori Kitamura",
      "Yunhao Tang",
      "Jincheng Mei",
      "Pierre M\u00e9nard",
      "Mohammad Gheshlaghi Azar",
      "Michal Valko",
      "R\u00e9mi Munos",
      "Olivier Pietquin",
      "Matthieu Geist",
      "Csaba Szepesv\u00e1ri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14211"
  },
  {
    "id": "arXiv:2205.14212",
    "title": "Exemplar Free Class Agnostic Counting",
    "abstract": "We tackle the task of Class Agnostic Counting, which aims to count objects in\na novel object category at test time without any access to labeled training\ndata for that category. All previous class agnostic counting methods cannot\nwork in a fully automated setting, and require computationally expensive test\ntime adaptation. To address these challenges, we propose a visual counter which\noperates in a fully automated setting and does not require any test time\nadaptation. Our proposed approach first identifies exemplars from repeating\nobjects in an image, and then counts the repeating objects. We propose a novel\nregion proposal network for identifying the exemplars. After identifying the\nexemplars, we obtain the corresponding count by using a density estimation\nbased Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and\nshow that it achieves superior performance compared to the existing approaches.",
    "descriptor": "",
    "authors": [
      "Viresh Ranjan",
      "Minh Hoai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14212"
  },
  {
    "id": "arXiv:2205.14213",
    "title": "When Is Recoverable Consensus Harder Than Consensus?",
    "abstract": "We study the ability of different shared object types to solve recoverable\nconsensus using non-volatile shared memory in a system with crashes and\nrecoveries. In particular, we compare the difficulty of solving recoverable\nconsensus to the difficulty of solving the standard wait-free consensus problem\nin a system with halting failures. We focus on the model where individual\nprocesses may crash and recover and the large class of object types that are\nequipped with a read operation. We characterize the readable object types that\ncan solve recoverable consensus among a given number of processes. Using this\ncharacterization, we show that the number of processes that can solve consensus\nusing a readable type can be larger than the number of processes that can solve\nrecoverable consensus using that type, but only slightly larger.",
    "descriptor": "\nComments: A shorter version of this paper will appear in Proceedings of the 2022 ACM Symposium on Principles of Distributed Computing (PODC '22), July 25-29, 2022, Salerno, Italy, this https URL\n",
    "authors": [
      "Carole Delporte-Gallet",
      "Panagiota Fatourou",
      "Hugues Fauconnier",
      "Eric Ruppert"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14213"
  },
  {
    "id": "arXiv:2205.14217",
    "title": "Diffusion-LM Improves Controllable Text Generation",
    "abstract": "Controlling the behavior of language models (LMs) without re-training is a\nmajor open problem in natural language generation. While recent works have\ndemonstrated successes on controlling simple sentence attributes (e.g.,\nsentiment), there has been little progress on complex, fine-grained controls\n(e.g., syntactic structure). To address this challenge, we develop a new\nnon-autoregressive language model based on continuous diffusions that we call\nDiffusion-LM. Building upon the recent successes of diffusion models in\ncontinuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian\nvectors into word vectors, yielding a sequence of intermediate latent\nvariables. The continuous, hierarchical nature of these intermediate variables\nenables a simple gradient-based algorithm to perform complex, controllable\ngeneration tasks. We demonstrate successful control of Diffusion-LM for six\nchallenging fine-grained control tasks, significantly outperforming prior work.",
    "descriptor": "",
    "authors": [
      "Xiang Lisa Li",
      "John Thickstun",
      "Ishaan Gulrajani",
      "Percy Liang",
      "Tatsunori B. Hashimoto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14217"
  },
  {
    "id": "arXiv:2205.14219",
    "title": "Controllable Text Generation with Neurally-Decomposed Oracle",
    "abstract": "We propose a general and efficient framework to control auto-regressive\ngeneration models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained\nbase language model and a sequence-level boolean oracle function, we propose to\ndecompose the oracle function into token-level guidance to steer the base model\nin text generation. Specifically, the token-level guidance is approximated by a\nneural model trained with examples sampled from the base model, demanding no\nadditional auxiliary labeled data. We present the closed-form optimal solution\nto incorporate the token-level guidance into the base model for controllable\ngeneration. We further provide a theoretical analysis of how the approximation\nquality of NADO affects the controllable generation results. Experiments\nconducted on two applications: (1) text generation with lexical constraints and\n(2) machine translation with formality control demonstrate that our framework\nefficiently guides the base model towards the given oracle while maintaining\nhigh generation quality.",
    "descriptor": "\nComments: Submitted to Neurips 2022\n",
    "authors": [
      "Tao Meng",
      "Sidi Lu",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14219"
  },
  {
    "id": "arXiv:2205.14223",
    "title": "A short note on inf-sup conditions for the Taylor-Hood family  $Q_k$-$Q_{k-1}$",
    "abstract": "We discuss two types of discrete inf-sup conditions for the Taylor-Hood\nfamily $Q_k$-$Q_{k-1}$ for all $k\\in \\mathbb{N}$ with $k\\ge 2$ in 2D and 3D.\nWhile in 2D all results hold for a general class of hexahedral meshes, the\nresults in 3D are restricted to meshes of parallelepipeds. The analysis is\nbased on an element-wise technique as opposed to the widely used macroelement\ntechnique. This leads to inf-sup conditions on each element of the subdivision\nas well as to inf-sup conditions on the whole computational domain.",
    "descriptor": "",
    "authors": [
      "Walter Zulehner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14223"
  },
  {
    "id": "arXiv:2205.14224",
    "title": "Will Bilevel Optimizers Benefit from Loops",
    "abstract": "Bilevel optimization has arisen as a powerful tool for solving a variety of\nmachine learning problems. Two current popular bilevel optimizers AID-BiO and\nITD-BiO naturally involve solving one or two sub-problems, and consequently,\nwhether we solve these problems with loops (that take many iterations) or\nwithout loops (that take only a few iterations) can significantly affect the\noverall computational efficiency. Existing studies in the literature cover only\nsome of those implementation choices, and the complexity bounds available are\nnot refined enough to enable rigorous comparison among different\nimplementations. In this paper, we first establish unified convergence analysis\nfor both AID-BiO and ITD-BiO that are applicable to all implementation choices\nof loops. We then specialize our results to characterize the computational\ncomplexity for all implementations, which enable an explicit comparison among\nthem. Our result indicates that for AID-BiO, the loop for estimating the\noptimal point of the inner function is beneficial for overall efficiency,\nalthough it causes higher complexity for each update step, and the loop for\napproximating the outer-level Hessian-inverse-vector product reduces the\ngradient complexity. For ITD-BiO, the two loops always coexist, and our\nconvergence upper and lower bounds show that such loops are necessary to\nguarantee a vanishing convergence error, whereas the no-loop scheme suffers\nfrom an unavoidable non-vanishing convergence error. Our numerical experiments\nfurther corroborate our theoretical results.",
    "descriptor": "\nComments: 32 pages, 2 figures, 3 tables\n",
    "authors": [
      "Kaiyi Ji",
      "Mingrui Liu",
      "Yingbin Liang",
      "Lei Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14224"
  },
  {
    "id": "arXiv:2205.14226",
    "title": "Fast and Light-Weight Answer Text Retrieval in Dialogue Systems",
    "abstract": "Dialogue systems can benefit from being able to search through a corpus of\ntext to find information relevant to user requests, especially when\nencountering a request for which no manually curated response is available. The\nstate-of-the-art technology for neural dense retrieval or re-ranking involves\ndeep learning models with hundreds of millions of parameters. However, it is\ndifficult and expensive to get such models to operate at an industrial scale,\nespecially for cloud services that often need to support a big number of\nindividually customized dialogue systems, each with its own text corpus. We\nreport our work on enabling advanced neural dense retrieval systems to operate\neffectively at scale on relatively inexpensive hardware. We compare with\nleading alternative industrial solutions and show that we can provide a\nsolution that is effective, fast, and cost-efficient.",
    "descriptor": "\nComments: Accepted to appear in NAACL-HLT 2022 Industry Track\n",
    "authors": [
      "Hui Wan",
      "Siva Sankalp Patel",
      "J. William Murdock",
      "Saloni Potdar",
      "Sachindra Joshi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14226"
  },
  {
    "id": "arXiv:2205.14228",
    "title": "Sparse Conditional Hidden Markov Model for Weakly Supervised Named  Entity Recognition",
    "abstract": "Weakly supervised named entity recognition methods train label models to\naggregate the token annotations of multiple noisy labeling functions (LFs)\nwithout seeing any manually annotated labels. To work well, the label model\nneeds to contextually identify and emphasize well-performed LFs while\ndown-weighting the under-performers. However, evaluating the LFs is challenging\ndue to the lack of ground truths. To address this issue, we propose the sparse\nconditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire\nemission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating\nits diagonal elements, which are considered as the reliability scores of the\nLFs. The sparse scores are then expanded to the full-fledged emission matrix\nwith pre-defined expansion functions. We also augment the emission with\nweighted XOR scores, which track the probabilities of an LF observing incorrect\nentities. Sparse-CHMM is optimized through unsupervised learning with a\nthree-stage training pipeline that reduces the training difficulty and prevents\nthe model from falling into local optima. Compared with the baselines in the\nWrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on\nfive comprehensive datasets. Experiments show that each component of\nSparse-CHMM is effective, and the estimated LF reliabilities strongly correlate\nwith true LF F1 scores.",
    "descriptor": "\nComments: 11 pages, 8 figures, 11 tables\n",
    "authors": [
      "Yinghao Li",
      "Le Song",
      "Chao Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14228"
  },
  {
    "id": "arXiv:2205.14229",
    "title": "Learning to Find Proofs and Theorems by Learning to Refine Search  Strategies",
    "abstract": "We propose a new approach to automated theorem proving and deductive program\nsynthesis where an AlphaZero-style agent is self-training to refine a\nhigh-level expert strategy expressed as a nondeterministic program. An\nanalogous teacher agent is self-training to generate tasks of suitable\nrelevance and difficulty for the learner. This allows leveraging minimal\namounts of domain knowledge to tackle problems for which training data is\nunavailable or hard to synthesize. We illustrate our approach on the problem of\nloop invariant synthesis for imperative programs and using neural networks to\nrefine both the teacher and solver strategies.",
    "descriptor": "",
    "authors": [
      "Jonathan Laurent",
      "Andr\u00e9 Platzer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14229"
  },
  {
    "id": "arXiv:2205.14230",
    "title": "Semi-supervised Semantics-guided Adversarial Training for Trajectory  Prediction",
    "abstract": "Predicting the trajectories of surrounding objects is a critical task in\nself-driving and many other autonomous systems. Recent works demonstrate that\nadversarial attacks on trajectory prediction, where small crafted perturbations\nare introduced to history trajectories, may significantly mislead the\nprediction of future trajectories and ultimately induce unsafe planning.\nHowever, few works have addressed enhancing the robustness of this important\nsafety-critical task. In this paper, we present the first adversarial training\nmethod for trajectory prediction. Compared with typical adversarial training on\nimage tasks, our work is challenged by more random inputs with rich context,\nand a lack of class labels. To address these challenges, we propose a method\nbased on a semi-supervised adversarial autoencoder that models disentangled\nsemantic features with domain knowledge and provides additional latent labels\nfor the adversarial training. Extensive experiments with different types of\nattacks demonstrate that our semi-supervised semantics-guided adversarial\ntraining method can effectively mitigate the impact of adversarial attacks and\ngenerally improve the system's adversarial robustness to a variety of attacks,\nincluding unseen ones. We believe that such semantics-guided architecture and\nadvancement in robust generalization is an important step for developing robust\nprediction models and enabling safe decision making.",
    "descriptor": "\nComments: 11 pages, adversarial training for trajectory prediction\n",
    "authors": [
      "Ruochen Jiao",
      "Xiangguo Liu",
      "Takami Sato",
      "Qi Alfred Chen",
      "Qi Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14230"
  },
  {
    "id": "arXiv:2205.14236",
    "title": "FedControl: When Control Theory Meets Federated Learning",
    "abstract": "To date, the most popular federated learning algorithms use coordinate-wise\naveraging of the model parameters. We depart from this approach by\ndifferentiating client contributions according to the performance of local\nlearning and its evolution. The technique is inspired from control theory and\nits classification performance is evaluated extensively in IID framework and\ncompared with FedAvg.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.10864\n",
    "authors": [
      "Adnan Ben Mansour",
      "Gaia Carenini",
      "Alexandre Duplessis",
      "David Naccache"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14236"
  },
  {
    "id": "arXiv:2205.14237",
    "title": "Provably Sample-Efficient RL with Side Information about Latent Dynamics",
    "abstract": "We study reinforcement learning (RL) in settings where observations are\nhigh-dimensional, but where an RL agent has access to abstract knowledge about\nthe structure of the state space, as is the case, for example, when a robot is\ntasked to go to a specific room in a building using observations from its own\ncamera, while having access to the floor plan. We formalize this setting as\ntransfer reinforcement learning from an abstract simulator, which we assume is\ndeterministic (such as a simple model of moving around the floor plan), but\nwhich is only required to capture the target domain's latent-state dynamics\napproximately up to unknown (bounded) perturbations (to account for environment\nstochasticity). Crucially, we assume no prior knowledge about the structure of\nobservations in the target domain except that they can be used to identify the\nlatent states (but the decoding map is unknown). Under these assumptions, we\npresent an algorithm, called TASID, that learns a robust policy in the target\ndomain, with sample complexity that is polynomial in the horizon, and\nindependent of the number of states, which is not possible without access to\nsome prior knowledge. In synthetic experiments, we verify various properties of\nour algorithm and show that it empirically outperforms transfer RL algorithms\nthat require access to \"full simulators\" (i.e., those that also simulate\nobservations).",
    "descriptor": "\nComments: 35 pages, 4 figures\n",
    "authors": [
      "Yao Liu",
      "Dipendra Misra",
      "Miro Dud\u00edk",
      "Robert E. Schapire"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14237"
  },
  {
    "id": "arXiv:2205.14244",
    "title": "A Framework for Simulating Real-world Stream Data of the Internet of  Things",
    "abstract": "With the rapid growth in the number of devices of the Internet of Things\n(IoT), the volume and types of stream data are rapidly increasing in the real\nworld. Unfortunately, the stream data has the characteristics of infinite and\nperiodic volatility in the real world, which cause problems with the\ninefficient stream processing tasks. In this study, we report our recent\nefforts on this issue, with a focus on simulating stream data. Firstly, we\nexplore the characteristics of the real-world stream data of the IoT, which\nhelps us to understand the stream data in the real world. Secondly, the\npipeline of simulating stream data is proposed, which can accurately and\nefficiently simulate the characteristics of the stream data to improve\nefficiency for specific tasks. Finally, we design and implement a novel\nframework that can simulate various stream data for related stream processing\ntasks. To verify the validity of the proposed framework, we apply this\nframework to stream processing task running in the stream processing system.\nThe experimental results reveal that the related stream processing task is\naccelerated by at least 24 times using our proposed simulation framework with\nthe premise of ensuring volatility and trends of stream data.",
    "descriptor": "\nComments: 11 pages, 7 figures\n",
    "authors": [
      "Zheng Chu",
      "Xusheng Du",
      "Jiong Yu"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.14244"
  },
  {
    "id": "arXiv:2205.14246",
    "title": "Defending Against Stealthy Backdoor Attacks",
    "abstract": "Defenses against security threats have been an interest of recent studies.\nRecent works have shown that it is not difficult to attack a natural language\nprocessing (NLP) model while defending against them is still a cat-mouse game.\nBackdoor attacks are one such attack where a neural network is made to perform\nin a certain way on specific samples containing some triggers while achieving\nnormal results on other samples. In this work, we present a few defense\nstrategies that can be useful to counter against such an attack. We show that\nour defense methodologies significantly decrease the performance on the\nattacked inputs while maintaining similar performance on benign inputs. We also\nshow that some of our defenses have very less runtime and also maintain\nsimilarity with the original inputs.",
    "descriptor": "",
    "authors": [
      "Sangeet Sagar",
      "Abhinav Bhatt",
      "Abhijith Srinivas Bidaralli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14246"
  },
  {
    "id": "arXiv:2205.14247",
    "title": "Ainur: A Framework for Repeatable End-to-End Wireless Edge Computing  Testbed Research",
    "abstract": "Experimental research on wireless networking in combination with edge and\ncloud computing has been the subject of explosive interest in the last decade.\nThis development has been driven by the increasing complexity of modern\nwireless technologies and the extensive softwarization of these through\nprojects such as a Open Radio Access Network (O-RAN). In this context, a number\nof small- to mid-scale testbeds have emerged, employing a variety of\ntechnologies to target a wide array of use-cases and scenarios in the context\nof novel mobile communication technologies such as 5G and beyond-5G. Little\nwork, however, has yet been devoted to developing a standard framework for\nwireless testbed automation which is hardware-agnostic and compatible with\nedge- and cloud-native technologies. Such a solution would simplify the\ndevelopment of new testbeds by completely or partially removing the requirement\nfor custom management and orchestration software.\nIn this paper, we present the first such mostly hardware-agnostic wireless\ntestbed automation framework, Ainur. It is designed to configure, manage,\norchestrate, and deploy workloads from an end-to-end perspective. Ainur is\nbuilt on top of cloud-native technologies such as Docker, and is provided as\nFOSS to the community through the KTH-EXPECA/Ainur repository on GitHub. We\ndemonstrate the utility of the platform with a series of scenarios, showcasing\nin particular its flexibility with respect to physical link definition,\ncomputation placement, and automation of arbitrarily complex experimental\nscenarios.",
    "descriptor": "\nComments: 6 pages, 6 figures, demo session paper\n",
    "authors": [
      "Manuel {Olgu\u00edn Mu\u00f1oz}",
      "Seyed Samie Mostafavi",
      "Vishnu N. Moothedath",
      "James Gross"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14247"
  },
  {
    "id": "arXiv:2205.14248",
    "title": "Towards a Design Framework for TNN-Based Neuromorphic Sensory Processing  Units",
    "abstract": "Temporal Neural Networks (TNNs) are spiking neural networks that exhibit\nbrain-like sensory processing with high energy efficiency. This work presents\nthe ongoing research towards developing a custom design framework for designing\nefficient application-specific TNN-based Neuromorphic Sensory Processing Units\n(NSPUs). This paper examines previous works on NSPU designs for UCR time-series\nclustering and MNIST image classification applications. Current ideas for a\ncustom design framework and tools that enable efficient software-to-hardware\ndesign flow for rapid design space exploration of application-specific NSPUs\nwhile leveraging EDA tools to obtain post-layout netlist and\npower-performance-area (PPA) metrics are described. Future research directions\nare also outlined.",
    "descriptor": "",
    "authors": [
      "Prabhu Vellaisamy",
      "John Paul Shen"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.14248"
  },
  {
    "id": "arXiv:2205.14251",
    "title": "Is it Worth to Reason about Uncertainty in Occupancy Grid Maps during  Path Planning?",
    "abstract": "This paper investigates the usefulness of reasoning about the uncertain\npresence of obstacles during path planning, which typically stems from the\nusage of probabilistic occupancy grid maps for representing the environment\nwhen mapping via a noisy sensor like a stereo camera. The traditional planning\nparadigm prescribes using a hard threshold on the occupancy probability to\ndeclare that a cell is an obstacle, and to plan a single path accordingly while\ntreating unknown space as free. We compare this approach against a new\nuncertainty-aware planner, which plans two different path hypotheses and then\nmerges their initial trajectory segments into a single one ending in a\n\"next-best view\" pose. After this informative view is taken, the planner\ncommits to one of the hypotheses, or to a completely new one if a collision is\nimminent. Simulations were conducted comparing the proposed and traditional\nplanner. Results show the existence of planning scenarios -- like when the\nenvironment contains a dead-end, or when the goal is placed close to an\nobstacle -- in which reasoning about uncertainty can significantly decrease the\nrobot's traveled distance and increase the chances of reaching the goal. The\nnew planner was also validated on a real Clearpath Jackal robot equipped with a\nZED 2 stereo camera.",
    "descriptor": "\nComments: To appear in the Proceedings of ICRA 2022\n",
    "authors": [
      "Jacopo Banfi",
      "Lindsey Woo",
      "Mark Campbell"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14251"
  },
  {
    "id": "arXiv:2205.14252",
    "title": "Self-supervised models of audio effectively explain human cortical  responses to speech",
    "abstract": "Self-supervised language models are very effective at predicting high-level\ncortical responses during language comprehension. However, the best current\nmodels of lower-level auditory processing in the human brain rely on either\nhand-constructed acoustic filters or representations from supervised audio\nneural networks. In this work, we capitalize on the progress of self-supervised\nspeech representation learning (SSL) to create new state-of-the-art models of\nthe human auditory system. Compared against acoustic baselines, phonemic\nfeatures, and supervised models, representations from the middle layers of\nself-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently\nyield the best prediction performance for fMRI recordings within the auditory\ncortex (AC). Brain areas involved in low-level auditory processing exhibit a\npreference for earlier SSL model layers, whereas higher-level semantic areas\nprefer later layers. We show that these trends are due to the models' ability\nto encode information at multiple linguistic levels (acoustic, phonetic, and\nlexical) along their representation depth. Overall, these results show that\nself-supervised models effectively capture the hierarchy of information\nrelevant to different stages of speech processing in human cortex.",
    "descriptor": "\nComments: Accepted to the International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Aditya R. Vaidya",
      "Shailee Jain",
      "Alexander G. Huth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14252"
  },
  {
    "id": "arXiv:2205.14258",
    "title": "On the Symmetries of Deep Learning Models and their Internal  Representations",
    "abstract": "Symmetry has been a fundamental tool in the exploration of a broad range of\ncomplex systems. In machine learning, symmetry has been explored in both models\nand data. In this paper we seek to connect the symmetries arising from the\narchitecture of a family of models with the symmetries of that family's\ninternal representation of data. We do this by calculating a set of fundamental\nsymmetry groups, which we call the \\emph{intertwiner groups} of the model. Each\nof these arises from a particular nonlinear layer of the model and different\nnonlinearities result in different symmetry groups. These groups change the\nweights of a model in such a way that the underlying function that the model\nrepresents remains constant but the internal representations of data inside the\nmodel may change. We connect intertwiner groups to a model's internal\nrepresentations of data through a range of experiments that probe similarities\nbetween hidden states across models with the same architecture. Our work\nsuggests that the symmetries of a network are propagated into the symmetries in\nthat network's representation of data, providing us with a better understanding\nof how architecture affects the learning and prediction process. Finally, we\nspeculate that for ReLU networks, the intertwiner groups may provide a\njustification for the common practice of concentrating model interpretability\nexploration on the activation basis in hidden layers rather than arbitrary\nlinear combinations thereof.",
    "descriptor": "\nComments: CG and DB contributed equally\n",
    "authors": [
      "Charles Godfrey",
      "Davis Brown",
      "Tegan Emerson",
      "Henry Kvinge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14258"
  },
  {
    "id": "arXiv:2205.14259",
    "title": "Personalized PageRank Graph Attention Networks",
    "abstract": "There has been a rising interest in graph neural networks (GNNs) for\nrepresentation learning over the past few years. GNNs provide a general and\nefficient framework to learn from graph-structured data. However, GNNs\ntypically only use the information of a very limited neighborhood for each node\nto avoid over-smoothing. A larger neighborhood would be desirable to provide\nthe model with more information. In this work, we incorporate the limit\ndistribution of Personalized PageRank (PPR) into graph attention networks\n(GATs) to reflect the larger neighbor information without introducing\nover-smoothing. Intuitively, message aggregation based on Personalized PageRank\ncorresponds to infinitely many neighborhood aggregation layers. We show that\nour models outperform a variety of baseline models for four widely used\nbenchmark datasets. Our implementation is publicly available online.",
    "descriptor": "\nComments: Published as a conference paper at ICASSP 2022\n",
    "authors": [
      "Julie Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14259"
  },
  {
    "id": "arXiv:2205.14263",
    "title": "Optimal Multi-robot Formations for Relative Pose Estimation Using Range  Measurements",
    "abstract": "In multi-robot missions, relative position and attitude information between\nagents is valuable for a variety of tasks such as mapping, planning, and\nformation control. In this paper, the problem of estimating relative poses from\na set of inter-agent range measurements is investigated. Specifically, it is\nshown that the estimation accuracy is highly dependent on the true relative\nposes themselves, which prompts the desire to find multi-agent formations that\nprovide the best estimation performance. By direct maximization of Fischer\ninformation, it is shown in simulation and experiment that large improvements\nin estimation accuracy can be obtained by optimizing the formation geometry of\na team of robots.",
    "descriptor": "\nComments: 7 pages, 8 figures, submitted to International Conference on Intelligent Robots and Systems\n",
    "authors": [
      "Charles Champagne Cossette",
      "Mohammed Ayman Shalaby",
      "David Saussie",
      "Jerome Le Ny",
      "James Richard Forbes"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14263"
  },
  {
    "id": "arXiv:2205.14268",
    "title": "NeuPSL: Neural Probabilistic Soft Logic",
    "abstract": "We present Neural Probabilistic Soft Logic (NeuPSL), a novel neuro-symbolic\n(NeSy) framework that unites state-of-the-art symbolic reasoning with the\nlow-level perception of deep neural networks. To explicitly model the boundary\nbetween neural and symbolic representations, we introduce NeSy Energy-Based\nModels, a general family of energy-based models that combine neural and\nsymbolic reasoning. Using this framework, we show how to seamlessly integrate\nneural and symbolic parameter learning and inference. We perform an extensive\nempirical evaluation and show that NeuPSL outperforms existing methods on joint\ninference and has significantly lower variance in almost all settings.",
    "descriptor": "",
    "authors": [
      "Connor Pryor",
      "Charles Dickens",
      "Eriq Augustine",
      "Alon Albalak",
      "William Wang",
      "Lise Getoor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14268"
  },
  {
    "id": "arXiv:2205.14269",
    "title": "Temporal graph patterns by timed automata",
    "abstract": "Temporal graphs represent graph evolution over time, and have been receiving\nconsiderable research attention. Work on expressing temporal graph patterns or\ndiscovering temporal motifs typically assumes relatively simple temporal\nconstraints, such as journeys or, more generally, existential constraints,\npossibly with finite delays. In this paper we propose to use timed automata to\nexpress temporal constraints, leading to a general and powerful notion of\ntemporal basic graph pattern (BGP). The new difficulty is the evaluation of the\ntemporal constraint on a large set of matchings. An important benefit of timed\nautomata is that they support an iterative state assignment, which can be\nuseful for early detection of matches and pruning of non-matches. We introduce\nalgorithms to retrieve all instances of a temporal BGP match in a graph, and\npresent results of an extensive experimental evaluation, demonstrating\ninteresting performance trade-offs. We show that an on-demand algorithm that\nprocesses total matchings incrementally over time is preferable when dealing\nwith cyclic patterns on sparse graphs. On acyclic patterns or dense graphs, and\nwhen connectivity of partial matchings can be guaranteed, the best performance\nis achieved by maintaining partial matchings over time and allowing automaton\nevaluation to be fully incremental.",
    "descriptor": "",
    "authors": [
      "Amir Pouya Aghasadeghi",
      "Jan Van den Bussche",
      "Julia Stoyanovich"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.14269"
  },
  {
    "id": "arXiv:2205.14271",
    "title": "Towards Communication-Learning Trade-off for Federated Learning at the  Network Edge",
    "abstract": "In this letter, we study a wireless federated learning (FL) system where\nnetwork pruning is applied to local users with limited resources. Although\npruning is beneficial to reduce FL latency, it also deteriorates learning\nperformance due to the information loss. Thus, a trade-off problem between\ncommunication and learning is raised. To address this challenge, we quantify\nthe effects of network pruning and packet error on the learning performance by\nderiving the convergence rate of FL with a non-convex loss function. Then,\nclosed-form solutions for pruning control and bandwidth allocation are proposed\nto minimize the weighted sum of FL latency and FL performance. Finally,\nnumerical results demonstrate that 1) our proposed solution can outperform\nbenchmarks in terms of cost reduction and accuracy guarantee, and 2) a higher\npruning rate would bring less communication overhead but also worsen FL\naccuracy, which is consistent with our theoretical analysis.",
    "descriptor": "\nComments: This paper has been accepted by IEEE Communications Letters\n",
    "authors": [
      "Jianyang Ren",
      "Wanli Ni",
      "Hui Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14271"
  },
  {
    "id": "arXiv:2205.14273",
    "title": "A New Semi-Structured Algebraic Multigrid Method",
    "abstract": "Multigrid methods are well suited to large massively parallel computer\narchitectures because they are mathematically optimal and display excellent\nparallelization properties. Since current architecture trends are favoring\nregular compute patterns to achieve high performance, the ability to express\nstructure has become much more important. The hypre software library provides\nhigh-performance multigrid preconditioners and solvers through conceptual\ninterfaces, including a semi-structured interface that describes matrices\nprimarily in terms of stencils and logically structured grids. This paper\npresents a new semi-structured algebraic multigrid (SSAMG) method built on this\ninterface. The numerical convergence and performance of a CPU implementation of\nthis method are evaluated for a set of semi-structured problems. SSAMG achieves\nsignificantly better setup times than hypre's unstructured AMG solvers and\ncomparable convergence. In addition, the new method is capable of solving more\ncomplex problems than hypre's structured solvers.",
    "descriptor": "",
    "authors": [
      "Victor A. Paludetto Magri",
      "Robert D. Falgout",
      "Ulrike M. Yang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14273"
  },
  {
    "id": "arXiv:2205.14275",
    "title": "Image Keypoint Matching using Graph Neural Networks",
    "abstract": "Image matching is a key component of many tasks in computer vision and its\nmain objective is to find correspondences between features extracted from\ndifferent natural images. When images are represented as graphs, image matching\nboils down to the problem of graph matching which has been studied intensively\nin the past. In recent years, graph neural networks have shown great potential\nin the graph matching task, and have also been applied to image matching. In\nthis paper, we propose a graph neural network for the problem of image\nmatching. The proposed method first generates initial soft correspondences\nbetween keypoints using localized node embeddings and then iteratively refines\nthe initial correspondences using a series of graph neural network layers. We\nevaluate our method on natural image datasets with keypoint annotations and\nshow that, in comparison to a state-of-the-art model, our method speeds up\ninference times without sacrificing prediction accuracy.",
    "descriptor": "\nComments: Complex Networks\n",
    "authors": [
      "Nancy Xu",
      "Giannis Nikolentzos",
      "Michalis Vazirgiannis",
      "Henrik Bostr\u00f6m"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14275"
  },
  {
    "id": "arXiv:2205.14276",
    "title": "So3krates -- Self-attention for higher-order geometric interactions on  arbitrary length-scales",
    "abstract": "The application of machine learning methods in quantum chemistry has enabled\nthe study of numerous chemical phenomena, which are computationally intractable\nwith traditional ab-initio methods. However, some quantum mechanical properties\nof molecules and materials depend on non-local electronic effects, which are\noften neglected due to the difficulty of modeling them efficiently. This work\nproposes a modified attention mechanism adapted to the underlying physics,\nwhich allows to recover the relevant non-local effects. Namely, we introduce\nspherical harmonic coordinates (SPHCs) to reflect higher-order geometric\ninformation for each atom in a molecule, enabling a non-local formulation of\nattention in the SPHC space. Our proposed model So3krates -- a self-attention\nbased message passing neural network -- uncouples geometric information from\natomic features, making them independently amenable to attention mechanisms. We\nshow that in contrast to other published methods, So3krates is able to describe\nnon-local quantum mechanical effects over arbitrary length scales. Further, we\nfind evidence that the inclusion of higher-order geometric correlations\nincreases data efficiency and improves generalization. So3krates matches or\nexceeds state-of-the-art performance on popular benchmarks, notably, requiring\na significantly lower number of parameters (0.25--0.4x) while at the same time\ngiving a substantial speedup (6--14x for training and 2--11x for inference)\ncompared to other models.",
    "descriptor": "",
    "authors": [
      "J. Thorben Frank",
      "Oliver T. Unke",
      "Klaus-Robert M\u00fcller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14276"
  },
  {
    "id": "arXiv:2205.14277",
    "title": "Monte Carlo Tree Search Gait Planner for Non-Gaited Legged System  Control",
    "abstract": "In this work, a non-gaited framework for legged system locomotion is\npresented. The approach decouples the gait sequence optimization by considering\nthe problem as a decision-making process. The redefined contact sequence\nproblem is solved by utilizing a Monte Carlo Tree Search (MCTS) algorithm that\nexploits optimization-based simulations to evaluate the best search direction.\nThe proposed scheme has proven to have a good trade-off between exploration and\nexploitation of the search space compared to the state-of-the-art Mixed-Integer\nQuadratic Programming (MIQP). The model predictive control (MPC) utilizes the\ngait generated by the MCTS to optimize the ground reaction forces and future\nfootholds position. The simulation results, performed on a quadruped robot,\nshowed that the proposed framework could generate known periodic gait and adapt\nthe contact sequence to the encountered conditions, including external forces\nand terrain with unknown and variable properties. When tested on robots with\ndifferent layouts, the system has also shown its reliability.",
    "descriptor": "",
    "authors": [
      "Lorenzo Amatucci",
      "Joon-Ha Kim",
      "Jemin Hwangbo",
      "Hae-Won Park"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14277"
  },
  {
    "id": "arXiv:2205.14280",
    "title": "Fast Object Placement Assessment",
    "abstract": "Object placement assessment (OPA) aims to predict the rationality score of a\ncomposite image in terms of the placement (e.g., scale, location) of inserted\nforeground object. However, given a pair of scaled foreground and background,\nto enumerate all the reasonable locations, existing OPA model needs to place\nthe foreground at each location on the background and pass the obtained\ncomposite image through the model one at a time, which is very time-consuming.\nIn this work, we investigate a new task named as fast OPA. Specifically,\nprovided with a scaled foreground and a background, we only pass them through\nthe model once and predict the rationality scores for all locations. To\naccomplish this task, we propose a pioneering fast OPA model with several\ninnovations (i.e., foreground dynamic filter, background prior transfer, and\ncomposite feature mimicking) to bridge the performance gap between slow OPA\nmodel and fast OPA model. Extensive experiments on OPA dataset show that our\nproposed fast OPA model performs on par with slow OPA model but runs\nsignificantly faster.",
    "descriptor": "",
    "authors": [
      "Li Niu",
      "Qingyang Liu",
      "Zhenchen Liu",
      "Jiangtong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14280"
  },
  {
    "id": "arXiv:2205.14281",
    "title": "SHORTSTACK: Distributed, Fault-tolerant, Oblivious Data Access",
    "abstract": "Many applications that benefit from data offload to cloud services operate on\nprivate data. A now-long line of work has shown that, even when data is\noffloaded in an encrypted form, an adversary can learn sensitive information by\nanalyzing data access patterns. Existing techniques for oblivious data\naccess--that protect against access pattern attacks--require a centralized,\nstateful and trusted, proxy to orchestrate data accesses from applications to\ncloud services. We show that, in failure-prone deployments, such a centralized\nand stateful proxy results in violation of oblivious data access security\nguarantees and/or system unavailability. Thus, we initiate the study of\ndistributed, fault-tolerant, oblivious data access.\nWe present SHORTSTACK, a distributed proxy architecture for oblivious data\naccess in failure-prone deployments. SHORTSTACK achieves the classical\nobliviousness guarantee--access patterns observed by the adversary being\nindependent of the input--even under a powerful passive persistent adversary\nthat can force failure of arbitrary (bounded-sized) subset of proxy servers at\narbitrary times. We also introduce a security model that enables studying\noblivious data access with distributed, failure-prone, servers. We provide a\nformal proof that SHORTSTACK enables oblivious data access under this model,\nand show empirically that SHORTSTACK performance scales near-linearly with\nnumber of distributed proxy servers.",
    "descriptor": "\nComments: Full version of USENIX OSDI'22 paper\n",
    "authors": [
      "Midhul Vuppalapati",
      "Kushal Babel",
      "Anurag Khandelwal",
      "Rachit Agarwal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14281"
  },
  {
    "id": "arXiv:2205.14282",
    "title": "Investigating End-user Acceptance of Last-mile Delivery by Autonomous  Vehicles in the United States",
    "abstract": "This paper investigates the end-user acceptance of last-mile delivery carried\nout by autonomous vehicles within the United States. A total of 296\nparticipants were presented with information on this technology and then asked\nto complete a questionnaire on their perceptions to gauge their behavioral\nintention concerning acceptance. Structural equation modeling of the partial\nleast squares flavor (PLS-SEM) was employed to analyze the collected data. The\nresults indicated that the perceived usefulness of the technology played the\ngreatest role in end-user acceptance decisions, followed by the influence of\nothers, and then the enjoyment received by interacting with the technology.\nFurthermore, the perception of risk associated with using autonomous delivery\nvehicles for last-mile delivery led to a decrease in acceptance. However, most\nparticipants did not perceive the use of this technology to be risky. The paper\nconcludes by summarizing the implications our findings have on the respective\nstakeholders and proposing the next steps in this area of research.",
    "descriptor": "",
    "authors": [
      "Antonios Saravanos",
      "Olivia Verni",
      "Ian Moore",
      "Sall Aboubacar",
      "Jen Arriaza",
      "Sabrina Jivani",
      "Audrey Bennett",
      "Siqi Li",
      "Dongnanzi Zheng",
      "Stavros Zervoudakis"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14282"
  },
  {
    "id": "arXiv:2205.14287",
    "title": "Triple-Band Scheduling with Millimeter Wave and Terahertz Bands for  Wireless Backhaul",
    "abstract": "With the explosive growth of mobile traffic demand, densely deployed small\ncells underlying macrocells have great potential for 5G and beyond wireless\nnetworks. In this paper, we consider the problem of supporting traffic flows\nwith diverse QoS requirements by exploiting three high frequency bands, i.e.,\nthe 28GHz band, the E-band, and the Terahertz (THz) band. The cooperation of\nthe three bands is helpful for maximizing the number of flows with their QoS\nrequirements satisfied. To solve the formulated nonlinear integer programming\nproblem, we propose a triple-band scheduling scheme which can select the\noptimum scheduling band for each flow among three different frequency bands.\nThe proposed scheme also efficiently utilizes the resource to schedule flow\ntransmissions in time slots. Extensive simulations demonstrate the superior\nperformance of the proposed scheme over three baseline schemes with respect to\nthe number of completed flows and the system throughput.",
    "descriptor": "\nComments: 12 pages, 10 figures, Journal of Communications and Networks\n",
    "authors": [
      "Yibing Wang",
      "Hao Wu",
      "Yong Niu",
      "Jianwen Ding",
      "Shiwen Mao",
      "Bo Ai",
      "Zhangdui Zhong",
      "Ning Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14287"
  },
  {
    "id": "arXiv:2205.14288",
    "title": "Few-shot Subgoal Planning with Language Models",
    "abstract": "Pre-trained large language models have shown successful progress in many\nlanguage understanding benchmarks. This work explores the capability of these\nmodels to predict actionable plans in real-world environments. Given a text\ninstruction, we show that language priors encoded in pre-trained language\nmodels allow us to infer fine-grained subgoal sequences. In contrast to recent\nmethods which make strong assumptions about subgoal supervision, our\nexperiments show that language models can infer detailed subgoal sequences from\nfew training sequences without any fine-tuning. We further propose a simple\nstrategy to re-rank language model predictions based on interaction and\nfeedback from the environment. Combined with pre-trained navigation and visual\nreasoning components, our approach demonstrates competitive performance on\nsubgoal prediction and task completion in the ALFRED benchmark compared to\nprior methods that assume more subgoal supervision.",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Lajanugen Logeswaran",
      "Yao Fu",
      "Moontae Lee",
      "Honglak Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14288"
  },
  {
    "id": "arXiv:2205.14289",
    "title": "Contrastive Learning for Multi-Modal Automatic Code Review",
    "abstract": "Automatic code review (ACR), aiming to relieve manual inspection costs, is an\nindispensable and essential task in software engineering. The existing works\nonly use the source code fragments to predict the results, missing the\nexploitation of developer's comments. Thus, we present a Multi-Modal Apache\nAutomatic Code Review dataset (MACR) for the Multi-Modal ACR task. The release\nof this dataset would push forward the research in this field. Based on it, we\npropose a Contrastive Learning based Multi-Modal Network (CLMN) to deal with\nthe Multi-Modal ACR task. Concretely, our model consists of a code encoding\nmodule and a text encoding module. For each module, we use the dropout\noperation as minimal data augmentation. Then, the contrastive learning method\nis adopted to pre-train the module parameters. Finally, we combine the two\nencoders to fine-tune the CLMN to decide the results of Multi-Modal ACR.\nExperimental results on the MACR dataset illustrate that our proposed model\noutperforms the state-of-the-art methods.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Bingting Wu",
      "Xiaofang Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.14289"
  },
  {
    "id": "arXiv:2205.14290",
    "title": "Building net-native agreement systems",
    "abstract": "Agreements and contracts are everywhere, but they are built on layers and\nlayers of legal and social institutions. Software is slowly entering into this\nstack. In this article, we introduce agreement paths, a general model for\nunderstanding and decomposing digital agreement systems, and Agreement Engine,\nan open-source software service for building net-native agreement systems. We\ndemonstrate Agreement Engine by building two example agreement systems: Scarce\nKnowledge, an app for crowdfunding essays, and Twitter Social Capital, a bot\nthat allows users to form and enforce Twitter agreements.",
    "descriptor": "",
    "authors": [
      "Joshua Z. Tan",
      "Luke V. Miller"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.14290"
  },
  {
    "id": "arXiv:2205.14292",
    "title": "BulletArm: An Open-Source Robotic Manipulation Benchmark and Learning  Framework",
    "abstract": "We present BulletArm, a novel benchmark and learning-environment for robotic\nmanipulation. BulletArm is designed around two key principles: reproducibility\nand extensibility. We aim to encourage more direct comparisons between robotic\nlearning methods by providing a set of standardized benchmark tasks in\nsimulation alongside a collection of baseline algorithms. The framework\nconsists of 31 different manipulation tasks of varying difficulty, ranging from\nsimple reaching and picking tasks to more realistic tasks such as bin packing\nand pallet stacking. In addition to the provided tasks, BulletArm has been\nbuilt to facilitate easy expansion and provides a suite of tools to assist\nusers when adding new tasks to the framework. Moreover, we introduce a set of\nfive benchmarks and evaluate them using a series of state-of-the-art baseline\nalgorithms. By including these algorithms as part of our framework, we hope to\nencourage users to benchmark their work on any new tasks against these\nbaselines.",
    "descriptor": "",
    "authors": [
      "Dian Wang",
      "Colin Kohler",
      "Xupeng Zhu",
      "Mingxi Jia",
      "Robert Platt"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14292"
  },
  {
    "id": "arXiv:2205.14295",
    "title": "Is Lip Region-of-Interest Sufficient for Lipreading?",
    "abstract": "Lip region-of-interest (ROI) is conventionally used for visual input in the\nlipreading task. Few works have adopted the entire face as visual input because\nlip-excluded parts of the face are usually considered to be redundant and\nirrelevant to visual speech recognition. However, faces contain much more\ndetailed information than lips, such as speakers' head pose, emotion, identity\netc. We argue that such information might benefit visual speech recognition if\na powerful feature extractor employing the entire face is trained. In this\nwork, we propose to adopt the entire face for lipreading with self-supervised\nlearning. AV-HuBERT, an audio-visual multi-modal self-supervised learning\nframework, was adopted in our experiments. Our experimental results showed that\nadopting the entire face achieved 16% relative word error rate (WER) reduction\non the lipreading task, compared with the baseline method using lip as visual\ninput. Without self-supervised pretraining, the model with face input achieved\na higher WER than that using lip input in the case of limited training data (30\nhours), while a slightly lower WER when using large amount of training data\n(433 hours).",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Jing-Xuan Zhang",
      "Gen-Shun Wan",
      "Jia Pan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14295"
  },
  {
    "id": "arXiv:2205.14296",
    "title": "On the Complexity of Maximizing Social Welfare within Fair Allocations  of Indivisible Goods",
    "abstract": "Fair division is a classical topic studied in various disciplines and\ncaptures many real applications. One important issue in fair division is to\ncope with (economic) efficiency and fairness. A natural question along this\ndirection that receives considerable attention is: How to obtain the most\nefficient allocations among all the fair allocations? In this paper, we study\nthe complexity of maximizing social welfare within envy-free up to one item\n(EF1) allocations of indivisible goods for both normalized and unnormalized\nvaluations. With two agents, we show a fully polynomial time approximation\nscheme (FPTAS) and complement this positive result with the NP-hardness result\nwhere the latter resolves an open problem raised by the previous work. Further,\nwhen the number of agents $n$ is a constant, we provide a bi-criteria algorithm\nthat finds the optimal social welfare while relaxing EF1 by a factor\narbitrarily close to 1. We complement this by providing several strong\ninapproximability results if EF1 is not allowed to relax. In particular, we\ndemonstrate that the inapproximability becomes stronger as $n$ increases. Last,\nwe consider the case with general number of agents. In this case, we give a\nvariant of the round-robin algorithm with an approximation ratio of $1/n$ for\nunnormalized valuations and provide inapproximability results of\n$n^{1/3-\\varepsilon}$ and $m^{1/2-\\varepsilon}$ for normalized valuations. In\naddition, we show that our results of bi-criteria optimization for constant $n$\ncannot be extended to the setting here, unless P=NP.",
    "descriptor": "\nComments: 21 pages, 1 figure, 2 tables\n",
    "authors": [
      "Xiaolin Bu",
      "Zihao Li",
      "Shengxin Liu",
      "Jiaxin Song",
      "Biaoshuai Tao"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.14296"
  },
  {
    "id": "arXiv:2205.14297",
    "title": "Fake It Till You Make It: Near-Distribution Novelty Detection by  Score-Based Generative Models",
    "abstract": "We aim for image-based novelty detection. Despite considerable progress,\nexisting models either fail or face a dramatic drop under the so-called\n``near-distribution\" setting, where the differences between normal and\nanomalous samples are subtle. We first demonstrate existing methods experience\nup to 20\\% decrease in performance in the near-distribution setting. Next, we\npropose to exploit a score-based generative model to produce synthetic\nnear-distribution anomalous data. Our model is then fine-tuned to distinguish\nsuch data from the normal samples. We provide a quantitative as well as\nqualitative evaluation of this strategy, and compare the results with a variety\nof GAN-based models. Effectiveness of our method for both the near-distribution\nand standard novelty detection is assessed through extensive experiments on\ndatasets in diverse applications such as medical images, object classification,\nand quality control. This reveals that our method considerably improves over\nexisting models, and consistently decreases the gap between the\nnear-distribution and standard novelty detection performance. Overall, our\nmethod improves the near-distribution novelty detection by 6% and passes the\nstate-of-the-art by 1% to 5% across nine novelty detection benchmarks. The code\nrepository is available at https://github.com/rohban-lab/FITYMI",
    "descriptor": "",
    "authors": [
      "Hossein Mirzaei",
      "Mohammadreza Salehi",
      "Sajjad Shahabi",
      "Efstratios Gavves",
      "Cees G. M. Snoek",
      "Mohammad Sabokrou",
      "Mohammad Hossein Rohban"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14297"
  },
  {
    "id": "arXiv:2205.14298",
    "title": "MC-GEN:Multi-level Clustering for Private Synthetic Data Generation",
    "abstract": "Nowadays, machine learning is one of the most common technology to turn raw\ndata into useful information in scientific and industrial processes. The\nperformance of the machine learning model often depends on the size of dataset.\nCompanies and research institutes usually share or exchange their data to avoid\ndata scarcity. However, sharing original datasets that contain private\ninformation can cause privacy leakage. Utilizing synthetic datasets which have\nsimilar characteristics as a substitute is one of the solutions to avoid the\nprivacy issue. Differential privacy provides a strong privacy guarantee to\nprotect the individual data records which contain sensitive information. We\npropose MC-GEN, a privacy-preserving synthetic data generation method under\ndifferential privacy guarantee for multiple classification tasks. MC-GEN builds\ndifferentially private generative models on the multi-level clustered data to\ngenerate synthetic datasets. Our method also reduced the noise introduced from\ndifferential privacy to improve the utility. In experimental evaluation, we\nevaluated the parameter effect of MC-GEN and compared MC-GEN with three\nexisting methods. Our results showed that MC-GEN can achieve significant\neffectiveness under certain privacy guarantees on multiple classification\ntasks.",
    "descriptor": "",
    "authors": [
      "Mingchen Li",
      "Di Zhuang",
      "J. Morris Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14298"
  },
  {
    "id": "arXiv:2205.14299",
    "title": "Deep Learning with Label Noise: A Hierarchical Approach",
    "abstract": "Deep neural networks are susceptible to label noise. Existing methods to\nimprove robustness, such as meta-learning and regularization, usually require\nsignificant change to the network architecture or careful tuning of the\noptimization procedure. In this work, we propose a simple hierarchical approach\nthat incorporates a label hierarchy when training the deep learning models. Our\napproach requires no change of the network architecture or the optimization\nprocedure. We investigate our hierarchical network through a wide range of\nsimulated and real datasets and various label noise types. Our hierarchical\napproach improves upon regular deep neural networks in learning with label\nnoise. Combining our hierarchical approach with pre-trained models achieves\nstate-of-the-art performance in real-world noisy datasets.",
    "descriptor": "\nComments: 8 pages, 7 figures\n",
    "authors": [
      "Li Chen",
      "Ningyuan Huang",
      "Cong Mu",
      "Hayden S. Helm",
      "Kate Lytvynets",
      "Weiwei Yang",
      "Carey E. Priebe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14299"
  },
  {
    "id": "arXiv:2205.14300",
    "title": "A Quadrature Perspective on Frequency Bias in Neural Network Training  with Nonuniform Data",
    "abstract": "Small generalization errors of over-parameterized neural networks (NNs) can\nbe partially explained by the frequency biasing phenomenon, where\ngradient-based algorithms minimize the low-frequency misfit before reducing the\nhigh-frequency residuals. Using the Neural Tangent Kernel (NTK), one can\nprovide a theoretically rigorous analysis for training where data are drawn\nfrom constant or piecewise-constant probability densities. Since most training\ndata sets are not drawn from such distributions, we use the NTK model and a\ndata-dependent quadrature rule to theoretically quantify the frequency biasing\nof NN training given fully nonuniform data. By replacing the loss function with\na carefully selected Sobolev norm, we can further amplify, dampen,\ncounterbalance, or reverse the intrinsic frequency biasing in NN training.",
    "descriptor": "",
    "authors": [
      "Annan Yu",
      "Yunan Yang",
      "Alex Townsend"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14300"
  },
  {
    "id": "arXiv:2205.14303",
    "title": "Deep Embedded Clustering with Distribution Consistency Preservation for  Attributed Networks",
    "abstract": "Many complex systems in the real world can be characterized by attributed\nnetworks. To mine the potential information in these networks, deep embedded\nclustering, which obtains node representations and clusters simultaneously, has\nbeen paid much attention in recent years. Under the assumption of consistency\nfor data in different views, the cluster structure of network topology and that\nof node attributes should be consistent for an attributed network. However,\nmany existing methods ignore this property, even though they separately encode\nnode representations from network topology and node attributes meanwhile\nclustering nodes on representation vectors learnt from one of the views.\nTherefore, in this study, we propose an end-to-end deep embedded clustering\nmodel for attributed networks. It utilizes graph autoencoder and node attribute\nautoencoder to respectively learn node representations and cluster assignments.\nIn addition, a distribution consistency constraint is introduced to maintain\nthe latent consistency of cluster distributions of two views. Extensive\nexperiments on several datasets demonstrate that the proposed model achieves\nsignificantly better or competitive performance compared with the\nstate-of-the-art methods. The source code can be found at\nhttps://github.com/Zhengymm/DCP.",
    "descriptor": "\nComments: 28 pages, 5 figures\n",
    "authors": [
      "Yimei Zheng",
      "Caiyan Jia",
      "Jian Yu",
      "Xuanya Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14303"
  },
  {
    "id": "arXiv:2205.14304",
    "title": "Multimodal Fake News Detection via CLIP-Guided Learning",
    "abstract": "Multimodal fake news detection has attracted many research interests in\nsocial forensics. Many existing approaches introduce tailored attention\nmechanisms to guide the fusion of unimodal features. However, how the\nsimilarity of these features is calculated and how it will affect the\ndecision-making process in FND are still open questions. Besides, the potential\nof pretrained multi-modal feature learning models in fake news detection has\nnot been well exploited. This paper proposes a FND-CLIP framework, i.e., a\nmultimodal Fake News Detection network based on Contrastive Language-Image\nPretraining (CLIP). Given a targeted multimodal news, we extract the deep\nrepresentations from the image and text using a ResNet-based encoder, a\nBERT-based encoder and two pair-wise CLIP encoders. The multimodal feature is a\nconcatenation of the CLIP-generated features weighted by the standardized\ncross-modal similarity of the two modalities. The extracted features are\nfurther processed for redundancy reduction before feeding them into the final\nclassifier. We introduce a modality-wise attention module to adaptively\nreweight and aggregate the features. We have conducted extensive experiments on\ntypical fake news datasets. The results indicate that the proposed framework\nhas a better capability in mining crucial features for fake news detection. The\nproposed FND-CLIP can achieve better performances than previous works, i.e.,\n0.7\\%, 6.8\\% and 1.3\\% improvements in overall accuracy on Weibo, Politifact\nand Gossipcop, respectively. Besides, we justify that CLIP-based learning can\nallow better flexibility on multimodal feature selection.",
    "descriptor": "\nComments: Submitted to CIKM 2022\n",
    "authors": [
      "Yangming Zhou",
      "Qichao Ying",
      "Zhenxing Qian",
      "Sheng Li",
      "Xinpeng Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14304"
  },
  {
    "id": "arXiv:2205.14305",
    "title": "Ensemble2: Anomaly Detection via EVT-Ensemble Framework for Seasonal  KPIs in Communication Network",
    "abstract": "KPI anomaly detection is one important function of network management system.\nTraditional methods either require prior knowledge or manually set thresholds.\nTo overcome these shortcomings, we propose the Ensemble2 framework, which\napplies ensemble learning to improve exogenous capabilities. Meanwhile,\nautomatically adjusts thresholds based on extreme value theory. The model is\ntested on production datasets to verify its effectiveness. We further optimize\nthe model using online learning, and finally running at a speed of ~10 pts/s on\nan Intel i5 platform.",
    "descriptor": "",
    "authors": [
      "Shi-Yang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14305"
  },
  {
    "id": "arXiv:2205.14307",
    "title": "TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning  over Temporal Knowledge Graph",
    "abstract": "Multi-hop logical reasoning over knowledge graph (KG) plays a fundamental\nrole in many artificial intelligence tasks. Recent complex query embedding\n(CQE) methods for reasoning focus on static KGs, while temporal knowledge\ngraphs (TKGs) have not been fully explored. Reasoning over TKGs has two\nchallenges: 1. The query should answer entities or timestamps; 2. The operators\nshould consider both set logic on entity set and temporal logic on timestamp\nset. To bridge this gap, we define the multi-hop logical reasoning problem on\nTKGs. With generated three datasets, we propose the first temporal CQE named\nTemporal Feature-Logic Embedding framework (TFLEX) to answer the temporal\ncomplex queries. We utilize vector logic to compute the logic part of Temporal\nFeature-Logic embeddings, thus naturally modeling all First-Order Logic (FOL)\noperations on entity set. In addition, our framework extends vector logic on\ntimestamp set to cope with three extra temporal operators (After, Before and\nBetween). Experiments on numerous query patterns demonstrate the effectiveness\nof our method.",
    "descriptor": "",
    "authors": [
      "Xueyuan Lin",
      "Chengjin Xu",
      "Haihong E",
      "Fenglong Su",
      "Gengxian Zhou",
      "Tianyi Hu",
      "Ningyuan Li",
      "Mingzhi Sun",
      "Haoran Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14307"
  },
  {
    "id": "arXiv:2205.14309",
    "title": "Federated Neural Bandit",
    "abstract": "Recent works on neural contextual bandit have achieved compelling\nperformances thanks to their ability to leverage the strong representation\npower of neural networks (NNs) for reward prediction. Many applications of\ncontextual bandit involve multiple agents who collaborate without sharing raw\nobservations, giving rise to the setting of federated contextual bandit.\nExisting works on federated contextual bandit rely on linear or kernelized\nbandit, which may fall short when modeling complicated real-world reward\nfunctions. In this regard, we introduce the federated neural-upper confidence\nbound (FN-UCB) algorithm. To better exploit the federated setting, we adopt a\nweighted combination of two UCBs: $\\text{UCB}^{a}$ allows every agent to\nadditionally use the observations from the other agents to accelerate\nexploration (without sharing raw observations); $\\text{UCB}^{b}$ uses an NN\nwith aggregated parameters for reward prediction in a similar way as federated\naveraging for supervised learning. Notably, the weight between the two UCBs\nrequired by our theoretical analysis is amenable to an interesting\ninterpretation, which emphasizes $\\text{UCB}^{a}$ initially for accelerated\nexploration and relies more on $\\text{UCB}^{b}$ later after enough observations\nhave been collected to train the NNs for accurate reward prediction (i.e.,\nreliable exploitation). We prove sub-linear upper bounds on both the cumulative\nregret and the number of communication rounds of FN-UCB, and use empirical\nexperiments to demonstrate its competitive performances.",
    "descriptor": "\nComments: Preprint. Under review\n",
    "authors": [
      "Zhongxiang Dai",
      "Yao Shu",
      "Arun Verma",
      "Flint Xiaofeng Fan",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14309"
  },
  {
    "id": "arXiv:2205.14310",
    "title": "Approximate Conditional Coverage via Neural Model Approximations",
    "abstract": "Constructing reliable prediction sets is an obstacle for applications of\nneural models: Distribution-free conditional coverage is theoretically\nimpossible, and the exchangeability assumption underpinning the coverage\nguarantees of standard split-conformal approaches is violated on domain shifts.\nGiven these challenges, we propose and analyze a data-driven procedure for\nobtaining empirically reliable approximate conditional coverage, calculating\nunique quantile thresholds for each label for each test point. We achieve this\nvia the strong signals for prediction reliability from KNN-based model\napproximations over the training set and approximations over constrained\nsamples from the held-out calibration set. We demonstrate the potential for\nsubstantial (and otherwise unknowable) under-coverage with split-conformal\nalternatives with marginal coverage guarantees when not taking these distances\nand constraints into account with protein secondary structure prediction,\ngrammatical error detection, sentiment classification, and fact verification,\ncovering supervised sequence labeling, zero-shot sequence labeling (i.e.,\nfeature detection), document classification (with sparsity/interpretability\nconstraints), and retrieval-classification, including class-imbalanced and\ndomain-shifted settings.",
    "descriptor": "\nComments: 25 pages, 4 figures\n",
    "authors": [
      "Allen Schmaltz",
      "Danielle Rasooly"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14310"
  },
  {
    "id": "arXiv:2205.14311",
    "title": "Robust Molecular Image Recognition: A Graph Generation Approach",
    "abstract": "Molecular image recognition is a fundamental task in information extraction\nfrom chemistry literature. Previous data-driven models formulate it as an\nimage-to-sequence task, to generate a sequential representation of the molecule\n(e.g. SMILES string) from its graphical representation. Although they perform\nadequately on certain benchmarks, these models are not robust in real-world\nsituations, where molecular images differ in style, quality, and chemical\npatterns. In this paper, we propose a novel graph generation approach that\nexplicitly predicts atoms and bonds, along with their geometric layouts, to\nconstruct the molecular graph. We develop data augmentation strategies for\nmolecules and images to increase the robustness of our model against domain\nshifts. Our model is flexible to incorporate chemistry constraints, and\nproduces more interpretable predictions than SMILES. In experiments on both\nsynthetic and realistic molecular images, our model significantly outperforms\nprevious models, achieving 84-93% accuracy on five benchmarks. We also conduct\nhuman evaluation and show that our model reduces the time for a chemist to\nextract molecular structures from images by roughly 50%.",
    "descriptor": "\nComments: 16 pages, 8 figures\n",
    "authors": [
      "Yujie Qian",
      "Zhengkai Tu",
      "Jiang Guo",
      "Connor W. Coley",
      "Regina Barzilay"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14311"
  },
  {
    "id": "arXiv:2205.14312",
    "title": "Fine-Grained Buy-Many Mechanisms Are Not Much Better Than Bundling",
    "abstract": "Multi-item optimal mechanisms are known to be extremely complex, often\noffering buyers randomized lotteries of goods. In the standard buy-one model it\nis known that optimal mechanisms can yield revenue infinitely higher than that\nof any \"simple\" mechanism, even for the case of just two items and a single\nbuyer.\nWe introduce a new class of mechanisms, buy-$k$ mechanisms, which smoothly\ninterpolates between the classical buy-one mechanisms and buy-many mechanisms.\nBuy-$k$ mechanisms allow the buyer to (non-adaptively) buy up to $k$ many menu\noptions. We show that restricting the seller to the class of buy-$n$ mechanisms\nsuffices to overcome the bizarre, infinite revenue properties of the buy-one\nmodel for the case of a single, additive buyer. The revenue gap with respect to\nbundling, an extremely simple mechanism, is bounded by $O(n^3)$ for any\narbitrarily correlated distribution $\\mathcal{D}$ over $n$ items. For the\nspecial case of $n=2$, we show that the revenue-optimal buy-2 mechanism gets no\nbetter than 40 times the revenue from bundling. Our upper bounds also hold for\nthe case of adaptive buyers.\nFinally, we show that allowing the buyer to purchase a small number of menu\noptions does not suffice to guarantee sub-exponential approximations. If the\nbuyer is only allowed to buy $k = \\Theta(n^{1/2-\\varepsilon})$ many menu\noptions, the gap between the revenue-optimal buy-$k$ mechanism and bundling may\nbe exponential in $n$. This implies that no \"simple\" mechanism can get a\nsub-exponential approximation in this regime. Moreover, our lower bound\ninstance, based on combinatorial designs and cover-free sets, uses a buy-$k$\ndeterministic mechanism. This allows us to extend our lower bound to the case\nof adaptive buyers.",
    "descriptor": "",
    "authors": [
      "Sepehr Assadi",
      "Ariel Schvartzman"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.14312"
  },
  {
    "id": "arXiv:2205.14313",
    "title": "Learning to Use Chopsticks in Diverse Styles",
    "abstract": "Learning dexterous manipulation skills is a long-standing challenge in\ncomputer graphics and robotics, especially when the task involves complex and\ndelicate interactions between the hands, tools and objects. In this paper, we\nfocus on chopsticks-based object relocation tasks, which are common yet\ndemanding. The key to successful chopsticks skills is steady gripping of the\nsticks that also supports delicate maneuvers. We automatically discover\nphysically valid chopsticks holding poses by Bayesian Optimization (BO) and\nDeep Reinforcement Learning (DRL), which works for multiple gripping styles and\nhand morphologies without the need of example data. Given as input the\ndiscovered gripping poses and desired objects to be moved, we build\nphysics-based hand controllers to accomplish relocation tasks in two stages.\nFirst, kinematic trajectories are synthesized for the chopsticks and hand in a\nmotion planning stage. The key components of our motion planner include a\ngrasping model to select suitable chopsticks configurations for grasping the\nobject, and a trajectory optimization module to generate collision-free\nchopsticks trajectories. Then we train physics-based hand controllers through\nDRL again to track the desired kinematic trajectories produced by the motion\nplanner. We demonstrate the capabilities of our framework by relocating objects\nof various shapes and sizes, in diverse gripping styles and holding positions\nfor multiple hand morphologies. Our system achieves faster learning speed and\nbetter control robustness, when compared to vanilla systems that attempt to\nlearn chopstick-based skills without a gripping pose optimization module and/or\nwithout a kinematic motion planner.",
    "descriptor": "",
    "authors": [
      "Zeshi Yang",
      "KangKang Yin",
      "Libin Liu"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14313"
  },
  {
    "id": "arXiv:2205.14315",
    "title": "Efficient Federated Learning with Spike Neural Networks for Traffic Sign  Recognition",
    "abstract": "With the gradual popularization of self-driving, it is becoming increasingly\nimportant for vehicles to smartly make the right driving decisions and\nautonomously obey traffic rules by correctly recognizing traffic signs.\nHowever, for machine learning-based traffic sign recognition on the Internet of\nVehicles (IoV), a large amount of traffic sign data from distributed vehicles\nis needed to be gathered in a centralized server for model training, which\nbrings serious privacy leakage risk because of traffic sign data containing\nlots of location privacy information. To address this issue, we first exploit\nprivacy-preserving federated learning to perform collaborative training for\naccurate recognition models without sharing raw traffic sign data.\nNevertheless, due to the limited computing and energy resources of most\ndevices, it is hard for vehicles to continuously undertake complex artificial\nintelligence tasks. Therefore, we introduce powerful Spike Neural Networks\n(SNNs) into traffic sign recognition for energy-efficient and fast model\ntraining, which is the next generation of neural networks and is practical and\nwell-fitted to IoV scenarios. Furthermore, we design a novel encoding scheme\nfor SNNs based on neuron receptive fields to extract information from the pixel\nand spatial dimensions of traffic signs to achieve high-accuracy training.\nNumerical results indicate that the proposed federated SNN outperforms\ntraditional federated convolutional neural networks in terms of accuracy, noise\nimmunity, and energy efficiency as well.",
    "descriptor": "\nComments: Submitted by IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "Kan Xie",
      "Zhe Zhang",
      "Bo Li",
      "Jiawen Kang",
      "Dusit Niyato",
      "Shengli Xie",
      "Yi Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14315"
  },
  {
    "id": "arXiv:2205.14318",
    "title": "Learning from Self-Sampled Correct and Partially-Correct Programs",
    "abstract": "Program synthesis aims to generate executable programs that are consistent\nwith the user specification. While there are often multiple programs that\nsatisfy the same user specification, existing neural program synthesis models\nare often only learned from one reference program by maximizing its\nlog-likelihood. This causes the model to be overly confident in its predictions\nas it sees the single solution repeatedly during training. This leads to poor\ngeneralization on unseen examples, even when multiple attempts are allowed. To\nmitigate this issue, we propose to let the model perform sampling during\ntraining and learn from both self-sampled fully-correct programs, which yield\nthe gold execution results, as well as partially-correct programs, whose\nintermediate execution state matches another correct program. We show that our\nuse of self-sampled correct and partially-correct programs can benefit learning\nand help guide the sampling process, leading to more efficient exploration of\nthe program space. Additionally, we explore various training objectives to\nsupport learning from multiple programs per example and find they greatly\naffect the performance. Experiments on the MathQA and GSM8K datasets show that\nour proposed method improves the pass@k performance by 3.1% to 12.3% compared\nto learning from a single reference program with MLE.",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Ansong Ni",
      "Jeevana Priya Inala",
      "Chenglong Wang",
      "Oleksandr Polozov",
      "Christopher Meek",
      "Dragomir Radev",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.14318"
  },
  {
    "id": "arXiv:2205.14319",
    "title": "WT-MVSNet: Window-based Transformers for Multi-view Stereo",
    "abstract": "Recently, Transformers were shown to enhance the performance of multi-view\nstereo by enabling long-range feature interaction. In this work, we propose\nWindow-based Transformers (WT) for local feature matching and global feature\naggregation in multi-view stereo. We introduce a Window-based Epipolar\nTransformer (WET) which reduces matching redundancy by using epipolar\nconstraints. Since point-to-line matching is sensitive to erroneous camera pose\nand calibration, we match windows near the epipolar lines. A second Shifted WT\nis employed for aggregating global information within cost volume. We present a\nnovel Cost Transformer (CT) to replace 3D convolutions for cost volume\nregularization. In order to better constrain the estimated depth maps from\nmultiple views, we further design a novel geometric consistency loss (Geo Loss)\nwhich punishes unreliable areas where multi-view consistency is not satisfied.\nOur WT multi-view stereo method (WT-MVSNet) achieves state-of-the-art\nperformance across multiple datasets and ranks $1^{st}$ on Tanks and Temples\nbenchmark.",
    "descriptor": "",
    "authors": [
      "Jinli Liao",
      "Yikang Ding",
      "Yoli Shavit",
      "Dihe Huang",
      "Shihao Ren",
      "Jia Guo",
      "Wensen Feng",
      "Kai Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14319"
  },
  {
    "id": "arXiv:2205.14320",
    "title": "RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo",
    "abstract": "In this paper, we present a learning-based approach for multi-view stereo\n(MVS), i.e., estimate the depth map of a reference frame using posed multi-view\nimages. Our core idea lies in leveraging a \"learning-to-optimize\" paradigm to\niteratively index a plane-sweeping cost volume and regress the depth map via a\nconvolutional Gated Recurrent Unit (GRU). Since the cost volume plays a\nparamount role in encoding the multi-view geometry, we aim to improve its\nconstruction both in pixel- and frame- levels. In the pixel level, we propose\nto break the symmetry of the Siamese network (which is typically used in MVS to\nextract image features) by introducing a transformer block to the reference\nimage (but not to the source images). Such an asymmetric volume allows the\nnetwork to extract global features from the reference image to predict its\ndepth map. In view of the inaccuracy of poses between reference and source\nimages, we propose to incorporate a residual pose network to make corrections\nto the relative poses, which essentially rectifies the cost volume in the\nframe-level. We conduct extensive experiments on real-world MVS datasets and\nshow that our method achieves state-of-the-art performance in terms of both\nwithin-dataset evaluation and cross-dataset generalization.",
    "descriptor": "",
    "authors": [
      "Changjiang Cai",
      "Pan Ji",
      "Yi Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14320"
  },
  {
    "id": "arXiv:2205.14321",
    "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
    "abstract": "Multi-scenario learning (MSL) enables a service provider to cater for users'\nfine-grained demands by separating services for different user sectors, e.g.,\nby user's geographical region. Under each scenario there is a need to optimize\nmultiple task-specific targets e.g., click through rate and conversion rate,\nknown as multi-task learning (MTL). Recent solutions for MSL and MTL are mostly\nbased on the multi-gate mixture-of-experts (MMoE) architecture. MMoE structure\nis typically static and its design requires domain-specific knowledge, making\nit less effective in handling both MSL and MTL. In this paper, we propose a\nnovel Automatic Expert Selection framework for Multi-scenario and Multi-task\nsearch, named AESM^{2}. AESM^{2} integrates both MSL and MTL into a unified\nframework with an automatic structure learning. Specifically, AESM^{2} stacks\nmulti-task layers over multi-scenario layers. This hierarchical design enables\nus to flexibly establish intrinsic connections between different scenarios, and\nat the same time also supports high-level feature extraction for different\ntasks. At each multi-scenario/multi-task layer, a novel expert selection\nalgorithm is proposed to automatically identify scenario-/task-specific and\nshared experts for each input. Experiments over two real-world large-scale\ndatasets demonstrate the effectiveness of AESM^{2} over a battery of strong\nbaselines. Online A/B test also shows substantial performance gain on multiple\nmetrics. Currently, AESM^{2} has been deployed online for serving major\ntraffic.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Xinyu Zou",
      "Zhi Hu",
      "Yiming Zhao",
      "Xuchu Ding",
      "Zhongyi Liu",
      "Chenliang Li",
      "Aixin Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.14321"
  },
  {
    "id": "arXiv:2205.14323",
    "title": "Multi-agent Databases via Independent Learning",
    "abstract": "Machine learning is rapidly being used in database research to improve the\neffectiveness of numerous tasks included but not limited to query optimization,\nworkload scheduling, physical design, etc. essential database components, such\nas the optimizer, scheduler, and physical designer. Currently, the research\nfocus has been on replacing a single database component responsible for one\ntask by its learning-based counterpart. However, query performance is not\nsimply determined by the performance of a single component, but by the\ncooperation of multiple ones. As such, learned based database components need\nto collaborate during both training and execution in order to develop policies\nthat meet end performance goals. Thus, the paper attempts to address the\nquestion \"Is it possible to design a database consisting of various learned\ncomponents that cooperatively work to improve end-to-end query latency?\".\nTo answer this question, we introduce MADB (Multi-Agent DB), a\nproof-of-concept system that incorporates a learned query scheduler and a\nlearned query optimizer. MADB leverages a cooperative multi-agent reinforcement\nlearning approach that allows the two components to exchange the context of\ntheir decisions with each other and collaboratively work towards reducing the\nquery latency. Preliminary results demonstrate that MADB can outperform the\nnon-cooperative integration of learned components.",
    "descriptor": "",
    "authors": [
      "Chi Zhang",
      "Olga Papaemmanouil",
      "Josiah Hanna"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14323"
  },
  {
    "id": "arXiv:2205.14324",
    "title": "Differentially Private Covariance Revisited",
    "abstract": "In this paper, we present three new error bounds, in terms of the Frobenius\nnorm, for covariance estimation under differential privacy: (1) a worst-case\nbound of $\\tilde{O}(d^{1/4}/\\sqrt{n})$, which improves the standard Gaussian\nmechanism $\\tilde{O}(d/n)$ for the regime $d>\\widetilde{\\Omega}(n^{2/3})$; (2)\na trace-sensitive bound that improves the state of the art by a\n$\\sqrt{d}$-factor, and (3) a tail-sensitive bound that gives a more\ninstance-specific result. The corresponding algorithms are also simple and\nefficient. Experimental results show that they offer significant improvements\nover prior work.",
    "descriptor": "",
    "authors": [
      "Wei Dong",
      "Yuting Liang",
      "Ke Yi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14324"
  },
  {
    "id": "arXiv:2205.14325",
    "title": "Feature subset selection for kernel SVM classification via mixed-integer  optimization",
    "abstract": "We study the mixed-integer optimization (MIO) approach to feature subset\nselection in nonlinear kernel support vector machines (SVMs) for binary\nclassification. First proposed for linear regression in the 1970s, this\napproach has recently moved into the spotlight with advances in optimization\nalgorithms and computer hardware. The goal of this paper is to establish an MIO\napproach for selecting the best subset of features for kernel SVM\nclassification. To measure the performance of subset selection, we use the\nkernel-target alignment, which is the distance between the centroids of two\nresponse classes in a high-dimensional feature space. We propose a\nmixed-integer linear optimization (MILO) formulation based on the kernel-target\nalignment for feature subset selection, and this MILO problem can be solved to\noptimality using optimization software. We also derive a reduced version of the\nMILO problem to accelerate our MILO computations. Experimental results show\ngood computational efficiency for our MILO formulation with the reduced\nproblem. Moreover, our method can often outperform the linear-SVM-based MILO\nformulation and recursive feature elimination in prediction performance,\nespecially when there are relatively few data instances.",
    "descriptor": "",
    "authors": [
      "Ryuta Tamura",
      "Yuichi Takano",
      "Ryuhei Miyashiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14325"
  },
  {
    "id": "arXiv:2205.14326",
    "title": "Adaptive Activation Network For Low Resource Multilingual Speech  Recognition",
    "abstract": "Low resource automatic speech recognition (ASR) is a useful but thorny task,\nsince deep learning ASR models usually need huge amounts of training data. The\nexisting models mostly established a bottleneck (BN) layer by pre-training on a\nlarge source language, and transferring to the low resource target language. In\nthis work, we introduced an adaptive activation network to the upper layers of\nASR model, and applied different activation functions to different languages.\nWe also proposed two approaches to train the model: (1) cross-lingual learning,\nreplacing the activation function from source language to target language, (2)\nmultilingual learning, jointly training the Connectionist Temporal\nClassification (CTC) loss of each language and the relevance of different\nlanguages. Our experiments on IARPA Babel datasets demonstrated that our\napproaches outperform the from-scratch training and traditional bottleneck\nfeature based methods. In addition, combining the cross-lingual learning and\nmultilingual learning together could further improve the performance of\nmultilingual speech recognition.",
    "descriptor": "\nComments: accepted by WCCI 2022\n",
    "authors": [
      "Jian Luo",
      "Jianzong Wang",
      "Ning Cheng",
      "Zhenpeng Zheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14326"
  },
  {
    "id": "arXiv:2205.14327",
    "title": "Efficient Policy Iteration for Robust Markov Decision Processes via  Regularization",
    "abstract": "Robust Markov decision processes (MDPs) provide a general framework to model\ndecision problems where the system dynamics are changing or only partially\nknown. Recent work established the equivalence between \\texttt{s} rectangular\n$L_p$ robust MDPs and regularized MDPs, and derived a regularized policy\niteration scheme that enjoys the same level of efficiency as standard MDPs.\nHowever, there lacks a clear understanding of the policy improvement step. For\nexample, we know the greedy policy can be stochastic but have little clue how\neach action affects this greedy policy. In this work, we focus on the policy\nimprovement step and derive concrete forms for the greedy policy and the\noptimal robust Bellman operators. We find that the greedy policy is closely\nrelated to some combination of the top $k$ actions, which provides a novel\ncharacterization of its stochasticity. The exact nature of the combination\ndepends on the shape of the uncertainty set. Furthermore, our results allow us\nto efficiently compute the policy improvement step by a simple binary search,\nwithout turning to an external optimization subroutine. Moreover, for $L_1,\nL_2$, and $L_\\infty$ robust MDPs, we can even get rid of the binary search and\nevaluate the optimal robust Bellman operators exactly. Our work greatly extends\nexisting results on solving \\texttt{s}-rectangular $L_p$ robust MDPs via\nregularized policy iteration and can be readily adapted to sample-based\nmodel-free algorithms.",
    "descriptor": "",
    "authors": [
      "Navdeep Kumar",
      "Kfir Levy",
      "Kaixin Wang",
      "Shie Mannor"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14327"
  },
  {
    "id": "arXiv:2205.14328",
    "title": "Point RCNN: An Angle-Free Framework for Rotated Object Detection",
    "abstract": "Rotated object detection in aerial images is still challenging due to\narbitrary orientations, large scale and aspect ratio variations, and extreme\ndensity of objects. Existing state-of-the-art rotated object detection methods\nmainly rely on angle-based detectors. However, angle regression can easily\nsuffer from the long-standing boundary problem. To tackle this problem, we\npropose a purely angle-free framework for rotated object detection, called\nPoint RCNN, which mainly consists of PointRPN and PointReg. In particular,\nPointRPN generates accurate rotated RoIs (RRoIs) by converting the learned\nrepresentative points with a coarse-to-fine manner, which is motivated by\nRepPoints. Based on the learned RRoIs, PointReg performs corner points\nrefinement for more accurate detection. In addition, aerial images are often\nseverely unbalanced in categories, and existing methods almost ignore this\nissue. In this paper, we also experimentally verify that re-sampling the images\nof the rare categories will stabilize training and further improve the\ndetection performance. Experiments demonstrate that our Point RCNN achieves the\nnew state-of-the-art detection performance on commonly used aerial datasets,\nincluding DOTA-v1.0, DOTA-v1.5, and HRSC2016.",
    "descriptor": "",
    "authors": [
      "Qiang Zhou",
      "Chaohui Yu",
      "Zhibin Wang",
      "Hao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14328"
  },
  {
    "id": "arXiv:2205.14329",
    "title": "Speech Augmentation Based Unsupervised Learning for Keyword Spotting",
    "abstract": "In this paper, we investigated a speech augmentation based unsupervised\nlearning approach for keyword spotting (KWS) task. KWS is a useful speech\napplication, yet also heavily depends on the labeled data. We designed a\nCNN-Attention architecture to conduct the KWS task. CNN layers focus on the\nlocal acoustic features, and attention layers model the long-time dependency.\nTo improve the robustness of KWS model, we also proposed an unsupervised\nlearning method. The unsupervised loss is based on the similarity between the\noriginal and augmented speech features, as well as the audio reconstructing\ninformation. Two speech augmentation methods are explored in the unsupervised\nlearning: speed and intensity. The experiments on Google Speech Commands V2\nDataset demonstrated that our CNN-Attention model has competitive results.\nMoreover, the augmentation based unsupervised learning could further improve\nthe classification accuracy of KWS task. In our experiments, with augmentation\nbased unsupervised learning, our KWS model achieves better performance than\nother unsupervised methods, such as CPC, APC, and MPC.",
    "descriptor": "\nComments: accepted by WCCI 2022\n",
    "authors": [
      "Jian Luo",
      "Jianzong Wang",
      "Ning Cheng",
      "Haobin Tang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14329"
  },
  {
    "id": "arXiv:2205.14330",
    "title": "Differentiable Point-Based Radiance Fields for Efficient View Synthesis",
    "abstract": "We propose a differentiable rendering algorithm for efficient novel view\nsynthesis. By departing from volume-based representations in favor of a learned\npoint representation, we improve on existing methods more than an order of\nmagnitude in memory and runtime, both in training and inference. The method\nbegins with a uniformly-sampled random point cloud and learns per-point\nposition and view-dependent appearance, using a differentiable splat-based\nrenderer to evolve the model to match a set of input images. Our method is up\nto 300x faster than NeRF in both training and inference, with only a marginal\nsacrifice in quality, while using less than 10~MB of memory for a static scene.\nFor dynamic scenes, our method trains two orders of magnitude faster than\nSTNeRF and renders at near interactive rate, while maintaining high image\nquality and temporal coherence even without imposing any temporal-coherency\nregularizers.",
    "descriptor": "",
    "authors": [
      "Qiang Zhang",
      "Seung-Hwan Baek",
      "Szymon Rusinkiewicz",
      "Felix Heide"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.14330"
  },
  {
    "id": "arXiv:2205.14331",
    "title": "Survival Analysis on Structured Data using Deep Reinforcement Learning",
    "abstract": "Survival analysis is playing a major role in manufacturing sector by\nanalyzing occurrence of any unwanted event based on the input data. Predictive\nmaintenance, which is a part of survival analysis, helps to find any device\nfailure based on the current incoming data from different sensor or any\nequipment. Deep learning techniques were used to automate the predictive\nmaintenance problem to some extent, but they are not very helpful in predicting\nthe device failure for the input data which the algorithm had not learned.\nSince neural network predicts the output based on previous learned input\nfeatures, it cannot perform well when there is more variation in input\nfeatures. Performance of the model is degraded with the occurrence of changes\nin input data and finally the algorithm fails in predicting the device failure.\nThis problem can be solved by our proposed method where the algorithm can\npredict the device failure more precisely than the existing deep learning\nalgorithms. The proposed solution involves implementation of Deep Reinforcement\nLearning algorithm called Double Deep Q Network (DDQN) for classifying the\ndevice failure based on the input features. The algorithm is capable of\nlearning different variation of the input feature and is robust in predicting\nwhether the device will fail or not based on the input data. The proposed DDQN\nmodel is trained with limited or lesser amount of input data. The trained model\npredicted larger amount of test data efficiently and performed well compared to\nother deep learning and machine learning models.",
    "descriptor": "",
    "authors": [
      "Renith G",
      "Harikrishna Warrier",
      "Yogesh Gupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14331"
  },
  {
    "id": "arXiv:2205.14332",
    "title": "V4D: Voxel for 4D Novel View Synthesis",
    "abstract": "Neural radiance fields have made a remarkable breakthrough in the novel view\nsynthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,\ndynamic scene), the performance of the existing method is still limited by the\ncapacity of the neural network, typically in a multilayer perceptron network\n(MLP). In this paper, we present the method to model the 4D neural radiance\nfield by the 3D voxel, short as V4D, where the 3D voxel has two formats. The\nfirst one is to regularly model the bounded 3D space and then use the sampled\nlocal 3D feature with the time index to model the density field and the texture\nfield. The second one is in look-up tables (LUTs) format that is for the\npixel-level refinement, where the pseudo-surface produced by the volume\nrendering is utilized as the guidance information to learn a 2D pixel-level\nrefinement mapping. The proposed LUTs-based refinement module achieves the\nperformance gain with a little computational cost and could serve as the\nplug-and-play module in the novel view synthesis task. Moreover, we propose a\nmore effective conditional positional encoding toward the 4D data that achieves\nperformance gain with negligible computational burdens. Extensive experiments\ndemonstrate that the proposed method achieves state-of-the-art performance by a\nlarge margin. At last, the proposed V4D is also a computational-friendly method\nin both the training and testing phase, where we achieve 2 times faster in the\ntraining phase and 10 times faster in the inference phase compared with the\nstate-of-the-art method.",
    "descriptor": "",
    "authors": [
      "Wanshui Gan",
      "Hongbin Xu",
      "Yi Huang",
      "Shifeng Chen",
      "Naoto Yokoya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14332"
  },
  {
    "id": "arXiv:2205.14333",
    "title": "One Reference Is Not Enough: Diverse Distillation with Reference  Selection for Non-Autoregressive Translation",
    "abstract": "Non-autoregressive neural machine translation (NAT) suffers from the\nmulti-modality problem: the source sentence may have multiple correct\ntranslations, but the loss function is calculated only according to the\nreference sentence. Sequence-level knowledge distillation makes the target more\ndeterministic by replacing the target with the output from an autoregressive\nmodel. However, the multi-modality problem in the distilled dataset is still\nnonnegligible. Furthermore, learning from a specific teacher limits the upper\nbound of the model capability, restricting the potential of NAT models. In this\npaper, we argue that one reference is not enough and propose diverse\ndistillation with reference selection (DDRS) for NAT. Specifically, we first\npropose a method called SeedDiv for diverse machine translation, which enables\nus to generate a dataset containing multiple high-quality reference\ntranslations for each source sentence. During the training, we compare the NAT\noutput with all references and select the one that best fits the NAT output to\ntrain the model. Experiments on widely-used machine translation benchmarks\ndemonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one\ndecoding pass on WMT14 En-De, improving the state-of-the-art performance for\nNAT by over 1 BLEU. Source code: https://github.com/ictnlp/DDRS-NAT",
    "descriptor": "\nComments: NAACL 2022 main conference\n",
    "authors": [
      "Chenze Shao",
      "Xuanfu Wu",
      "Yang Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14333"
  },
  {
    "id": "arXiv:2205.14334",
    "title": "Teaching Models to Express Their Uncertainty in Words",
    "abstract": "We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n\"90% confidence\" or \"high confidence\"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples. To our knowledge, this is the first time a model\nhas been shown to express calibrated uncertainty about its own answers in\nnatural language. For testing calibration, we introduce the CalibratedMath\nsuite of tasks. We compare the calibration of uncertainty expressed in words\n(\"verbalized probability\") to uncertainty extracted from model logits. Both\nkinds of uncertainty are capable of generalizing calibration under distribution\nshift. We also provide evidence that GPT-3's ability to generalize calibration\ndepends on pre-trained latent representations that correlate with epistemic\nuncertainty over its answers.",
    "descriptor": "\nComments: CalibratedMath tasks and evaluation code are available at this https URL\n",
    "authors": [
      "Stephanie Lin",
      "Jacob Hilton",
      "Owain Evans"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14334"
  },
  {
    "id": "arXiv:2205.14336",
    "title": "Gating Dropout: Communication-efficient Regularization for Sparsely  Activated Transformers",
    "abstract": "Sparsely activated transformers, such as Mixture of Experts (MoE), have\nreceived great interest due to their outrageous scaling capability which\nenables dramatical increases in model size without significant increases in\ncomputational cost. To achieve this, MoE models replace the feedforward\nsub-layer with Mixture-of-Experts sub-layer in transformers and use a gating\nnetwork to route each token to its assigned experts. Since the common practice\nfor efficient training of such models requires distributing experts and tokens\nacross different machines, this routing strategy often incurs huge\ncross-machine communication cost because tokens and their assigned experts\nlikely reside in different machines. In this paper, we propose \\emph{Gating\nDropout}, which allows tokens to ignore the gating network and stay at their\nlocal machines, thus reducing the cross-machine communication. Similar to\ntraditional dropout, we also show that Gating Dropout has a regularization\neffect during training, resulting in improved generalization performance. We\nvalidate the effectiveness of Gating Dropout on multilingual machine\ntranslation tasks. Our results demonstrate that Gating Dropout improves a\nstate-of-the-art MoE model with faster wall-clock time convergence rates and\nbetter BLEU scores for a variety of model sizes and datasets.",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Rui Liu",
      "Young Jin Kim",
      "Alexandre Muzio",
      "Barzan Mozafari",
      "Hany Hassan Awadalla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14336"
  },
  {
    "id": "arXiv:2205.14337",
    "title": "List-Decodable Sparse Mean Estimation",
    "abstract": "Robust mean estimation is one of the most important problems in statistics:\ngiven a set of samples $\\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^d$ where an\n$\\alpha$ fraction are drawn from some distribution $D$ and the rest are\nadversarially corrupted, it aims to estimate the mean of $D$. A surge of recent\nresearch interest has been focusing on the list-decodable setting where $\\alpha\n\\in (0, \\frac12]$, and the goal is to output a finite number of estimates among\nwhich at least one approximates the target mean. In this paper, we consider\nthat the underlying distribution is Gaussian and the target mean is $k$-sparse.\nOur main contribution is the first polynomial-time algorithm that enjoys sample\ncomplexity $O\\big(\\mathrm{poly}(k, \\log d)\\big)$, i.e. poly-logarithmic in the\ndimension. One of the main algorithmic ingredients is using low-degree sparse\npolynomials to filter outliers, which may be of independent interest.",
    "descriptor": "",
    "authors": [
      "Shiwei Zeng",
      "Jie Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14337"
  },
  {
    "id": "arXiv:2205.14338",
    "title": "Object-wise Masked Autoencoders for Fast Pre-training",
    "abstract": "Self-supervised pre-training for images without labels has recently achieved\npromising performance in image classification. The success of transformer-based\nmethods, ViT and MAE, draws the community's attention to the design of backbone\narchitecture and self-supervised task. In this work, we show that current\nmasked image encoding models learn the underlying relationship between all\nobjects in the whole scene, instead of a single object representation.\nTherefore, those methods bring a lot of compute time for self-supervised\npre-training. To solve this issue, we introduce a novel object selection and\ndivision strategy to drop non-object patches for learning object-wise\nrepresentations by selective reconstruction with interested region masks. We\nrefer to this method ObjMAE. Extensive experiments on four commonly-used\ndatasets demonstrate the effectiveness of our model in reducing the compute\ncost by 72% while achieving competitive performance. Furthermore, we\ninvestigate the inter-object and intra-object relationship and find that the\nlatter is crucial for self-supervised pre-training.",
    "descriptor": "",
    "authors": [
      "Jiantao Wu",
      "Shentong Mo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14338"
  },
  {
    "id": "arXiv:2205.14340",
    "title": "Insights from an Industrial Collaborative Assembly Project: Lessons in  Research and Collaboration",
    "abstract": "Significant progress in robotics reveals new opportunities to advance\nmanufacturing. Next-generation industrial automation will require both\nintegration of distinct robotic technologies and their application to\nchallenging industrial environments. This paper presents lessons from a\ncollaborative assembly project between three academic research groups and an\nindustry partner. The goal of the project is to develop a flexible, safe, and\nproductive manufacturing cell for sub-centimeter precision assembly. Solving\nthis problem in a high-mix, low-volume production line motivates multiple\nresearch thrusts in robotics. This work identifies new directions in\ncollaborative robotics for industrial applications and offers insight toward\nstrengthening collaborations between institutions in academia and industry on\nthe development of new technologies.",
    "descriptor": "\nComments: Spotlight presentation at ICRA 2022 Workshop on Collaborative Robots and the Work of the Future (ICRA 2022 CoR-WotF); see the spotlight presentation at this https URL\n",
    "authors": [
      "Tan Chen",
      "Zhe Huang",
      "James Motes",
      "Junyi Geng",
      "Quang Minh Ta",
      "Holly Dinkel",
      "Hameed Abdul-Rashid",
      "Jessica Myers",
      "Ye-Ji Mun",
      "Wei-che Lin",
      "Yuan-yung Huang",
      "Sizhe Liu",
      "Marco Morales",
      "Nancy M. Amato",
      "Katherine Driggs-Campbell",
      "Timothy Bretl"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14340"
  },
  {
    "id": "arXiv:2205.14344",
    "title": "Data-Driven Evolutionary Multi-Objective Optimization Based on  Multiple-Gradient Descent for Disconnected Pareto Fronts",
    "abstract": "Data-driven evolutionary multi-objective optimization (EMO) has been\nrecognized as an effective approach for multi-objective optimization problems\nwith expensive objective functions. The current research is mainly developed\nfor problems with a 'regular' triangle-like Pareto-optimal front (PF), whereas\nthe performance can significantly deteriorate when the PF consists of\ndisconnected segments. Furthermore, the offspring reproduction in the current\ndata-driven EMO does not fully leverage the latent information of the surrogate\nmodel. Bearing these considerations in mind, this paper proposes a data-driven\nEMO algorithm based on multiple-gradient descent. By leveraging the regularity\ninformation provided by the up-to-date surrogate model, it is able to\nprogressively probe a set of well distributed candidate solutions with a\nconvergence guarantee. In addition, its infill criterion recommends a batch of\npromising candidate solutions to conduct expensive objective function\nevaluations. Experiments on $33$ benchmark test problem instances with\ndisconnected PFs fully demonstrate the effectiveness of our proposed method\nagainst four selected peer algorithms.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2109.05639\n",
    "authors": [
      "Renzhi Chen",
      "Ke Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.14344"
  },
  {
    "id": "arXiv:2205.14345",
    "title": "Reinforcement Learning for Branch-and-Bound Optimisation using  Retrospective Trajectories",
    "abstract": "Combinatorial optimisation problems framed as mixed integer linear programmes\n(MILPs) are ubiquitous across a range of real-world applications. The canonical\nbranch-and-bound (B&B) algorithm seeks to exactly solve MILPs by constructing a\nsearch tree of increasingly constrained sub-problems. In practice, its solving\ntime performance is dependent on heuristics, such as the choice of the next\nvariable to constrain ('branching'). Recently, machine learning (ML) has\nemerged as a promising paradigm for branching. However, prior works have\nstruggled to apply reinforcement learning (RL), citing sparse rewards,\ndifficult exploration, and partial observability as significant challenges.\nInstead, leading ML methodologies resort to approximating high quality\nhandcrafted heuristics with imitation learning (IL), which precludes the\ndiscovery of novel policies and requires expensive data labelling. In this\nwork, we propose retro branching; a simple yet effective approach to RL for\nbranching. By retrospectively deconstructing the search tree into multiple\npaths each contained within a sub-tree, we enable the agent to learn from\nshorter trajectories with more predictable next states. In experiments on four\ncombinatorial tasks, our approach enables learning-to-branch without any expert\nguidance or pre-training. We outperform the current state-of-the-art RL\nbranching algorithm by 3-5x and come within 20% of the best IL method's\nperformance on MILPs with 500 constraints and 1000 variables, with ablations\nverifying that our retrospectively constructed trajectories are essential to\nachieving these results.",
    "descriptor": "\nComments: 10 pages, 4 figures, 1 table, and supplementary material\n",
    "authors": [
      "Christopher W. F. Parsonson",
      "Alexandre Laterre",
      "Thomas D. Barrett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14345"
  },
  {
    "id": "arXiv:2205.14347",
    "title": "Estimation of 3D Body Shape and Clothing Measurements from Frontal- and  Side-view Images",
    "abstract": "The estimation of 3D human body shape and clothing measurements is crucial\nfor virtual try-on and size recommendation problems in the fashion industry but\nhas always been a challenging problem due to several conditions, such as lack\nof publicly available realistic datasets, ambiguity in multiple camera\nresolutions, and the undefinable human shape space. Existing works proposed\nvarious solutions to these problems but could not succeed in the industry\nadaptation because of complexity and restrictions. To solve the complexity and\nchallenges, in this paper, we propose a simple yet effective architecture to\nestimate both shape and measures from frontal- and side-view images. We utilize\nsilhouette segmentation from the two multi-view images and implement an\nauto-encoder network to learn low-dimensional features from segmented\nsilhouettes. Then, we adopt a kernel-based regularized regression module to\nestimate the body shape and measurements. The experimental results show that\nthe proposed method provides competitive results on the synthetic dataset,\nNOMO-3d-400-scans Dataset, and RGB Images of humans captured in different\ncameras.",
    "descriptor": "\nComments: 5pages, 3 figures, Submitted to ICIP 2022\n",
    "authors": [
      "Kundan Sai Prabhu Thota",
      "Sungho Suh",
      "Bo Zhou",
      "Paul Lukowicz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14347"
  },
  {
    "id": "arXiv:2205.14349",
    "title": "Do We Really Need to Use Constraint Violation in Constrained  Evolutionary Multi-Objective Optimization?",
    "abstract": "Constraint violation has been a building block to design evolutionary\nmulti-objective optimization algorithms for solving constrained multi-objective\noptimization problems. However, it is not uncommon that the constraint\nviolation is hardly approachable in real-world black-box optimization\nscenarios. It is unclear that whether the existing constrained evolutionary\nmulti-objective optimization algorithms, whose environmental selection\nmechanism are built upon the constraint violation, can still work or not when\nthe formulations of the constraint functions are unknown. Bearing this\nconsideration in mind, this paper picks up four widely used constrained\nevolutionary multi-objective optimization algorithms as the baseline and\ndevelop the corresponding variants that replace the constraint violation by a\ncrisp value. From our experiments on both synthetic and real-world benchmark\ntest problems, we find that the performance of the selected algorithms have not\nbeen significantly influenced when the constraint violation is not used to\nguide the environmental selection.",
    "descriptor": "",
    "authors": [
      "Shuang Li",
      "Ke Li",
      "Wei Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.14349"
  },
  {
    "id": "arXiv:2205.14352",
    "title": "Travelling Salesman Problem: Parallel Implementations & Analysis",
    "abstract": "The Traveling Salesman Problem (often called TSP) is a classic algorithmic\nproblem in the field of computer science and operations research. It is an\nNP-Hard problem focused on optimization. TSP has several applications even in\nits purest formulation, such as planning, logistics, and the manufacture of\nmicrochips; and can be slightly modified to appear as a sub-problem in many\nareas, such as DNA sequencing. In this paper, a study on parallelization of the\nBrute Force approach (under several paradigms) of the Travelling Salesman\nProblem is presented. Detailed timing studies for the serial and various\nparallel implementations of the Travelling Salesman Problem have also been\nillustrated.",
    "descriptor": "",
    "authors": [
      "Amey Gohil",
      "Manan Tayal",
      "Tezan Sahu",
      "Vyankatesh Sawalpurkar"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14352"
  },
  {
    "id": "arXiv:2205.14354",
    "title": "Multi-Task Learning with Multi-query Transformer for Dense Prediction",
    "abstract": "Previous multi-task dense prediction studies developed complex pipelines such\nas multi-modal distillations in multiple stages or searching for task\nrelational contexts for each task. The core insight beyond these methods is to\nmaximize the mutual effects between each task. Inspired by the recent\nquery-based Transformers, we propose a simpler pipeline named Multi-Query\nTransformer (MQTransformer) that is equipped with multiple queries from\ndifferent tasks to facilitate the reasoning among multiple tasks and simplify\nthe cross task pipeline. Instead of modeling the dense per-pixel context among\ndifferent tasks, we seek a task-specific proxy to perform cross-task reasoning\nvia multiple queries where each query encodes the task-related context. The\nMQTransformer is composed of three key components: shared encoder, cross task\nattention and shared decoder. We first model each task with a task-relevant and\nscale-aware query, and then both the image feature output by the feature\nextractor and the task-relevant query feature are fed into the shared encoder,\nthus encoding the query feature from the image feature. Secondly, we design a\ncross task attention module to reason the dependencies among multiple tasks and\nfeature scales from two perspectives including different tasks of the same\nscale and different scales of the same task. Then we use a shared decoder to\ngradually refine the image features with the reasoned query features from\ndifferent tasks. Extensive experiment results on two dense prediction datasets\n(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective\napproach and achieves the state-of-the-art result. Code will be available.",
    "descriptor": "",
    "authors": [
      "Yangyang Xu",
      "Xiangtai Li",
      "Haobo Yuan",
      "Yibo Yang",
      "Jing Zhang",
      "Yunhai Tong",
      "Lefei Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14354"
  },
  {
    "id": "arXiv:2205.14358",
    "title": "Fair Labeled Clustering",
    "abstract": "Numerous algorithms have been produced for the fundamental problem of\nclustering under many different notions of fairness. Perhaps the most common\nfamily of notions currently studied is group fairness, in which proportional\ngroup representation is ensured in every cluster. We extend this direction by\nconsidering the downstream application of clustering and how group fairness\nshould be ensured for such a setting. Specifically, we consider a common\nsetting in which a decision-maker runs a clustering algorithm, inspects the\ncenter of each cluster, and decides an appropriate outcome (label) for its\ncorresponding cluster. In hiring for example, there could be two outcomes,\npositive (hire) or negative (reject), and each cluster would be assigned one of\nthese two outcomes. To ensure group fairness in such a setting, we would desire\nproportional group representation in every label but not necessarily in every\ncluster as is done in group fair clustering. We provide algorithms for such\nproblems and show that in contrast to their NP-hard counterparts in group fair\nclustering, they permit efficient solutions. We also consider a well-motivated\nalternative setting where the decision-maker is free to assign labels to the\nclusters regardless of the centers' positions in the metric space. We show that\nthis setting exhibits interesting transitions from computationally hard to easy\naccording to additional constraints on the problem. Moreover, when the\nconstraint parameters take on natural values we show a randomized algorithm for\nthis setting that always achieves an optimal clustering and satisfies the\nfairness constraints in expectation. Finally, we run experiments on real world\ndatasets that validate the effectiveness of our algorithms.",
    "descriptor": "\nComments: Accepted to KDD 2022\n",
    "authors": [
      "Seyed A. Esmaeili",
      "Sharmila Duppala",
      "John P. Dickerson",
      "Brian Brubach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14358"
  },
  {
    "id": "arXiv:2205.14361",
    "title": "Boosting Facial Expression Recognition by A Semi-Supervised Progressive  Teacher",
    "abstract": "In this paper, we aim to improve the performance of in-the-wild Facial\nExpression Recognition (FER) by exploiting semi-supervised learning.\nLarge-scale labeled data and deep learning methods have greatly improved the\nperformance of image recognition. However, the performance of FER is still not\nideal due to the lack of training data and incorrect annotations (e.g., label\nnoises). Among existing in-the-wild FER datasets, reliable ones contain\ninsufficient data to train robust deep models while large-scale ones are\nannotated in lower quality. To address this problem, we propose a\nsemi-supervised learning algorithm named Progressive Teacher (PT) to utilize\nreliable FER datasets as well as large-scale unlabeled expression images for\neffective training. On the one hand, PT introduces semi-supervised learning\nmethod to relieve the shortage of data in FER. On the other hand, it selects\nuseful labeled training samples automatically and progressively to alleviate\nlabel noise. PT uses selected clean labeled data for computing the supervised\nclassification loss and unlabeled data for unsupervised consistency loss.\nExperiments on widely-used databases RAF-DB and FERPlus validate the\neffectiveness of our method, which achieves state-of-the-art performance with\naccuracy of 89.57% on RAF-DB. Additionally, when the synthetic noise rate\nreaches even 30%, the performance of our PT algorithm only degrades by 4.37%.",
    "descriptor": "",
    "authors": [
      "Jing Jiang",
      "Weihong Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14361"
  },
  {
    "id": "arXiv:2205.14365",
    "title": "Granular Generalized Variable Precision Rough Sets and Rational  Approximations",
    "abstract": "Rational approximations are introduced and studied in granular graded sets\nand generalizations thereof by the first author in recent research papers. The\nconcept of rationality is determined by related ontologies and coherence\nbetween granularity, parthood perspective and approximations used in the\ncontext. In addition, a framework is introduced by her in the mentioned\npaper(s). Granular approximations constructed as per the procedures of VPRS are\nlikely to be more rational than those constructed from a classical perspective\nunder certain conditions. This may continue to hold for some generalizations of\nthe former; however, a formal characterization of such conditions is not\navailable in the previously published literature. In this research, theoretical\naspects of the problem are critically examined, uniform generalizations of\ngranular VPRS are introduced, new connections with granular graded rough sets\nare proved, appropriate concepts of substantial parthood are introduced, and\ntheir extent of compatibility with the framework is accessed. Furthermore, meta\napplications to cluster validation, image segmentation and dynamic sorting are\ninvented. Basic assumptions made are explained, and additional examples are\nconstructed for readability.",
    "descriptor": "\nComments: 52 Pages\n",
    "authors": [
      "Mani A",
      "Sushmita Mitra"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14365"
  },
  {
    "id": "arXiv:2205.14368",
    "title": "Going Deeper into Permutation-Sensitive Graph Neural Networks",
    "abstract": "The invariance to permutations of the adjacency matrix, i.e., graph\nisomorphism, is an overarching requirement for Graph Neural Networks (GNNs).\nConventionally, this prerequisite can be satisfied by the invariant operations\nover node permutations when aggregating messages. However, such an invariant\nmanner may ignore the relationships among neighboring nodes, thereby hindering\nthe expressivity of GNNs. In this work, we devise an efficient\npermutation-sensitive aggregation mechanism via permutation groups, capturing\npairwise correlations between neighboring nodes. We prove that our approach is\nstrictly more powerful than the 2-dimensional Weisfeiler-Lehman (2-WL) graph\nisomorphism test and not less powerful than the 3-WL test. Moreover, we prove\nthat our approach achieves the linear sampling complexity. Comprehensive\nexperiments on multiple synthetic and real-world datasets demonstrate the\nsuperiority of our model.",
    "descriptor": "\nComments: Accepted by ICML 2022. Code is publicly available at this https URL\n",
    "authors": [
      "Zhongyu Huang",
      "Yingheng Wang",
      "Chaozhuo Li",
      "Huiguang He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14368"
  },
  {
    "id": "arXiv:2205.14371",
    "title": "GLITCH: an Intermediate-Representation-Based Security Analysis for  Infrastructure as Code Scripts",
    "abstract": "Infrastructure as Code (IaC) is the process of managing IT infrastructure via\nprogrammable configuration files (also called IaC scripts). Like other software\nartifacts, IaC scripts may contain security smells, which are coding patterns\nthat can result in security weaknesses. Automated analysis tools to detect\nsecurity smells in IaC scripts exist, but they focus on specific technologies\nsuch as Puppet, Ansible, or Chef. This means that when the detection of a new\nsmell is implemented in one of the tools, it is not immediately available for\nthe technologies supported by the other tools -- the only option is to\nduplicate the effort.\nThis paper presents GLITCH, a new technology-agnostic framework that enables\nautomated polyglot smell detection by transforming IaC scripts into an\nintermediate representation, on which different security smell detectors can be\ndefined. GLITCH currently supports the detection of nine different security\nsmells in scripts written in Puppet, Ansible, or Chef. We compare GLITCH with\nstate-of-the-art security smell detectors. The results obtained not only show\nthat GLITCH can reduce the effort of writing security smell analyses for\nmultiple IaC technologies, but also that it has higher precision and recall\nthan the current state-of-the-art tools.",
    "descriptor": "",
    "authors": [
      "Nuno Saavedra",
      "Jo\u00e3o F. Ferreira"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.14371"
  },
  {
    "id": "arXiv:2205.14374",
    "title": "Syntax-Guided Program Reduction for Understanding Neural Code  Intelligence Models",
    "abstract": "Neural code intelligence (CI) models are opaque black-boxes and offer little\ninsight on the features they use in making predictions. This opacity may lead\nto distrust in their prediction and hamper their wider adoption in\nsafety-critical applications. Recently, input program reduction techniques have\nbeen proposed to identify key features in the input programs to improve the\ntransparency of CI models. However, this approach is syntax-unaware and does\nnot consider the grammar of the programming language. In this paper, we apply a\nsyntax-guided program reduction technique that considers the grammar of the\ninput programs during reduction. Our experiments on multiple models across\ndifferent types of input programs show that the syntax-guided program reduction\ntechnique is faster and provides smaller sets of key tokens in reduced\nprograms. We also show that the key tokens could be used in generating\nadversarial examples for up to 65% of the input programs.",
    "descriptor": "\nComments: The 6th ACM SIGPLAN International Symposium on Machine Programming (MAPS'22). extension of arXiv:2202.06474\n",
    "authors": [
      "Md Rafiqul Islam Rabin",
      "Aftab Hussain",
      "Mohammad Amin Alipour"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.14374"
  },
  {
    "id": "arXiv:2205.14375",
    "title": "WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis",
    "abstract": "Gains in the ability to generalize on image analysis tasks for neural\nnetworks have come at the cost of increased number of parameters and layers,\ndataset sizes, training and test computations, and GPU RAM. We introduce a new\narchitecture -- WaveMix-Lite -- that can generalize on par with contemporary\ntransformers and convolutional neural networks (CNNs) while needing fewer\nresources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix\nspatial information from pixels. WaveMix-Lite seems to be a versatile and\nscalable architectural framework that can be used for multiple vision tasks,\nsuch as image classification and semantic segmentation, without requiring\nsignificant architectural changes, unlike transformers and CNNs. It is able to\nmeet or exceed several accuracy benchmarks while training on a single GPU. For\ninstance, it achieves state-of-the-art accuracy on five EMNIST datasets,\noutperforms CNNs and transformers in ImageNet-1K (64$\\times$64 images), and\nachieves an mIoU of 75.32 % on Cityscapes validation set, while using less than\none-fifth the number parameters and half the GPU RAM of comparable CNNs or\ntransformers. Our experiments show that while the convolutional elements of\nneural architectures exploit the shift-invariance property of images, new types\nof layers (e.g., wavelet transform) can exploit additional properties of\nimages, such as scale-invariance and finite spatial extents of objects.",
    "descriptor": "\nComments: 17 pages, 5 figures. arXiv admin note: text overlap with arXiv:2203.03689\n",
    "authors": [
      "Pranav Jeevan",
      "Kavitha Viswanathan",
      "Amit Sethi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14375"
  },
  {
    "id": "arXiv:2205.14376",
    "title": "One-Sided Repeated-Root Two-Dimensional Cyclic and Constacyclic Codes",
    "abstract": "In this paper, we study some repeated-root two-dimensional cyclic and\nconstacyclic codes over a finite field $F=\\mathbb{F}_q$. We obtain the\ngenerator matrices and generator polynomials of these codes and their duals. We\nalso investigate when such codes are self-dual. Moreover, we prove that if\nthere exists an asymptotically good family of one-sided repeated-root\ntwo-dimensional cyclic or constacyclic codes, then there exists an\nasymptotically good family of simple root two-dimensional cyclic or\nconstacyclic codes with parameters at least as good as the first family.\nFurthermore, we show that several of the main results of the papers Rajabi and\nKhashyarmanesh (2018) and Sepasdar and Khashyarmanesh (2016) are not accurate\nand find other conditions needed for them to hold.",
    "descriptor": "",
    "authors": [
      "Marziyeh Beygi Khormaei",
      "Ashkan Nikseresht",
      "Shohreh Namazi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Commutative Algebra (math.AC)"
    ],
    "url": "https://arxiv.org/abs/2205.14376"
  },
  {
    "id": "arXiv:2205.14377",
    "title": "Enhancing Quality of Pose-varied Face Restoration with Local Weak  Feature Sensing and GAN Prior",
    "abstract": "Facial semantic guidance (facial landmarks, facial parsing maps, facial\nheatmaps, etc.) and facial generative adversarial networks (GAN) prior have\nbeen widely used in blind face restoration (BFR) in recent years. Although\nexisting BFR methods have achieved good performance in ordinary cases, these\nsolutions have limited resilience when applied to face images with serious\ndegradation and pose-varied (look up, look down, laugh, etc.) in real-world\nscenarios. In this work, we propose a well-designed blind face restoration\nnetwork with generative facial prior. The proposed network is mainly comprised\nof an asymmetric codec and StyleGAN2 prior network. In the asymmetric codec, we\nadopt a mixed multi-path residual block (MMRB) to gradually extract weak\ntexture features of input images, which can improve the texture integrity and\nauthenticity of our networks. Furthermore, the MMRB block can also be\nplug-and-play in any other network. Besides, a novel self-supervised training\nstrategy is specially designed for face restoration tasks to fit the\ndistribution closer to the target and maintain training stability. Extensive\nexperiments over synthetic and real-world datasets demonstrate that our model\nachieves superior performance to the prior art for face restoration and face\nsuper-resolution tasks and can tackle seriously degraded face images in diverse\nposes and expressions.",
    "descriptor": "\nComments: pdfLaTeX 2021, 11 pages with 15 figures\n",
    "authors": [
      "Kai Hu",
      "Yu Liu",
      "Renhe Liu",
      "Wei Lu",
      "Gang Yu",
      "Bin Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14377"
  },
  {
    "id": "arXiv:2205.14380",
    "title": "Deep Deconfounded Content-based Tag Recommendation for UGC with Causal  Intervention",
    "abstract": "Traditional content-based tag recommender systems directly learn the\nassociation between user-generated content (UGC) and tags based on collected\nUGC-tag pairs. However, since a UGC uploader simultaneously creates the UGC and\nselects the corresponding tags, her personal preference inevitably biases the\ntag selections, which prevents these recommenders from learning the causal\ninfluence of UGCs' content features on tags. In this paper, we propose a deep\ndeconfounded content-based tag recommender system, namely, DecTag, to address\nthe above issues. We first establish a causal graph to represent the relations\namong uploader, UGC, and tag, where the uploaders are identified as confounders\nthat spuriously correlate UGC and tag selections. Specifically, to eliminate\nthe confounding bias, causal intervention is conducted on the UGC node in the\ngraph via backdoor adjustment, where uploaders' influence on tags leaked\nthrough backdoor paths can be eliminated for causal effect estimation.\nObserving that adjusting the causal graph with do-calculus requires integrating\nthe entire uploader space, which is infeasible, we design a novel Monte Carlo\n(MC)-based estimator with bootstrap, which can achieve asymptotic unbiasedness\nprovided that uploaders for the collected UGCs are i.i.d. samples from the\npopulation. In addition, the MC estimator has the intuition of substituting the\nbiased uploaders with a hypothetical random uploader from the population in the\ntraining phase, where deconfounding can be achieved in an interpretable manner.\nFinally, we establish a YT-8M-Causal dataset based on the widely used\nYouTube-8M dataset with causal intervention and propose an evaluation strategy\naccordingly to unbiasedly evaluate causal tag recommenders. Extensive\nexperiments show that DecTag is more robust to confounding bias than\nstate-of-the-art causal recommenders.",
    "descriptor": "",
    "authors": [
      "Yaochen Zhu",
      "Xubin Ren",
      "Jing Yi",
      "Zhenzhong Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.14380"
  },
  {
    "id": "arXiv:2205.14387",
    "title": "Regulating Matching Markets with Constraints: Data-driven Taxation",
    "abstract": "This paper develops a framework to conduct a counterfactual analysis to\nregulate matching markets with regional constraints that impose lower and upper\nbounds on the number of matches in each region. Our work is motivated by the\nJapan Residency Matching Program, in which the policymaker wants to guarantee\nthe least number of doctors working in rural regions to achieve the minimum\nstandard of service. Among the multiple possible policies that satisfy such\nconstraints, a policymaker wants to choose the best. To this end, we develop a\ndiscrete choice model approach that estimates the utility functions of agents\nfrom observed data and predicts agents' behavior under different counterfactual\npolicies. Our framework also allows the policymaker to design the\nwelfare-maximizing tax scheme, which outperforms the policy currently used in\npractice. Furthermore, a numerical experiment illustrates how our method works.",
    "descriptor": "",
    "authors": [
      "Akira Matsushita",
      "Kei Ikegami",
      "Kyohei Okumura",
      "Yoji Tomita",
      "Atsushi Iwasaki"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2205.14387"
  },
  {
    "id": "arXiv:2205.14390",
    "title": "Topological phase estimation method for reparameterized periodic  functions",
    "abstract": "We consider a signal composed of several periods of a periodic function, of\nwhich we observe a noisy reparametrisation. The phase estimation problem\nconsists of finding that reparametrisation, and, in particular, the number of\nobserved periods. Existing methods are well-suited to the setting where the\nperiodic function is known, or at least, simple. We consider the case when it\nis unknown and we propose an estimation method based on the shape of the\nsignal. We use the persistent homology of sublevel sets of the signal to\ncapture the temporal structure of its local extrema. We infer the number of\nperiods in the signal by counting points in the persistence diagram and their\nmultiplicities. Using the estimated number of periods, we construct an\nestimator of the reparametrisation. It is based on counting the number of\nsufficiently prominent local minima in the signal. This work is motivated by a\nvehicle positioning problem, on which we evaluated the proposed method.",
    "descriptor": "\nComments: 31 pages, 14 figures\n",
    "authors": [
      "Thomas Bonis",
      "Fr\u00e9d\u00e9ric Chazal",
      "Bertrand Michel",
      "Wojciech Reise"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Signal Processing (eess.SP)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2205.14390"
  },
  {
    "id": "arXiv:2205.14393",
    "title": "Relation-Specific Attentions over Entity Mentions for Enhanced  Document-Level Relation Extraction",
    "abstract": "Compared with traditional sentence-level relation extraction, document-level\nrelation extraction is a more challenging task where an entity in a document\nmay be mentioned multiple times and associated with multiple relations.\nHowever, most methods of document-level relation extraction do not distinguish\nbetween mention-level features and entity-level features, and just apply simple\npooling operation for aggregating mention-level features into entity-level\nfeatures. As a result, the distinct semantics between the different mentions of\nan entity are overlooked. To address this problem, we propose RSMAN in this\npaper which performs selective attentions over different entity mentions with\nrespect to candidate relations. In this manner, the flexible and\nrelation-specific representations of entities are obtained which indeed benefit\nrelation classification. Our extensive experiments upon two benchmark datasets\nshow that our RSMAN can bring significant improvements for some backbone models\nto achieve state-of-the-art performance, especially when an entity have\nmultiple mentions in the document.",
    "descriptor": "\nComments: Accepted by NAACL 2022\n",
    "authors": [
      "Jiaxin Yu",
      "Deqing Yang",
      "Shuyu Tian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14393"
  },
  {
    "id": "arXiv:2205.14395",
    "title": "Characterizing Tourist Daily Trip Chains Using Mobile Phone Big Data",
    "abstract": "Tourists tend to visit multiple destinations out of their variety-seeking\nmotivations in their trips. Thus, it is critical to discover travel patterns\ninvolving multi-destinations in tourism research. Existing relevant research\nmost relied on survey data or focused on citizens due to the lack of\nlarge-scale, fine-grained tourism datasets. Several scholars have mentioned the\nnotion of trip chains, but few works have been done towards quantitatively\nidentifying the structures of trip chains. In this paper, we propose a model\nfor quantitatively characterizing tourist daily trip chains. After applying\nthis model to tourist mobile phone big data, underlying tourist travel patterns\nare discovered. Through the framework, we find that: (1) Most \"hybrid\"\n(inter-city and intra-city) and \"intra-city\" (only intra-city) patterns can be\ncaptured by only 13 key trip chains relatively; (2) For two continuous days,\nalmost all kinds of original chains have a rather high probability to transfer\nto either the first two transferred chains, or other infrequent chains in our\nstudy areas; (3) The principle of least efforts (PLE) affects tourists'\nstructures of trip chains. We can use average degree and average travel\ndistance to interpret tourist travel behavior (achieving tasks in PLE). This\nstudy not only demonstrate the complex daily travel trip chains from tourism\nbig data, but also fill the gap in tourism literature on multi-destination\ntrips by discovering significant and underlying patterns based on mobile\ndatasets.",
    "descriptor": "",
    "authors": [
      "Yan Luo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14395"
  },
  {
    "id": "arXiv:2205.14396",
    "title": "Deep Learning-based Spatially Explicit Emulation of an Agent-Based  Simulator for Pandemic in a City",
    "abstract": "Agent-Based Models are very useful for simulation of physical or social\nprocesses, such as the spreading of a pandemic in a city. Such models proceed\nby specifying the behavior of individuals (agents) and their interactions, and\nparameterizing the process of infection based on such interactions based on the\ngeography and demography of the city. However, such models are computationally\nvery expensive, and the complexity is often linear in the total number of\nagents. This seriously limits the usage of such models for simulations, which\noften have to be run hundreds of times for policy planning and even model\nparameter estimation. An alternative is to develop an emulator, a surrogate\nmodel that can predict the Agent-Based Simulator's output based on its initial\nconditions and parameters. In this paper, we discuss a Deep Learning model\nbased on Dilated Convolutional Neural Network that can emulate such an agent\nbased model with high accuracy. We show that use of this model instead of the\noriginal Agent-Based Model provides us major gains in the speed of simulations,\nallowing much quicker calibration to observations, and more extensive scenario\nanalysis. The models we consider are spatially explicit, as the locations of\nthe infected individuals are simulated instead of the gross counts. Another\naspect of our emulation framework is its divide-and-conquer approach that\ndivides the city into several small overlapping blocks and carries out the\nemulation in them parallelly, after which these results are merged together.\nThis ensures that the same emulator can work for a city of any size, and also\nprovides significant improvement of time complexity of the emulator, compared\nto the original simulator.",
    "descriptor": "",
    "authors": [
      "Varun Madhavan",
      "Adway Mitra",
      "Partha Pratim Chakrabarti"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14396"
  },
  {
    "id": "arXiv:2205.14398",
    "title": "Deep neural networks overcome the curse of dimensionality in the  numerical approximation of semilinear partial differential equations",
    "abstract": "We prove that deep neural networks are capable of approximating solutions of\nsemilinear Kolmogorov PDE in the case of gradient-independent,\nLipschitz-continuous nonlinearities, while the required number of parameters in\nthe networks grow at most polynomially in both dimension $d \\in \\mathbb{N}$ and\nprescribed reciprocal accuracy $\\varepsilon$. Previously, this has only been\nproven in the case of semilinear heat equations.",
    "descriptor": "\nComments: 34 pages\n",
    "authors": [
      "Petru A. Cioica-Licht",
      "Martin Hutzenthaler",
      "P. Tobias Werner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.14398"
  },
  {
    "id": "arXiv:2205.14399",
    "title": "Incentive Mechanism Design for Emergency Frequency Control in  Multi-Infeed Hybrid AC-DC System",
    "abstract": "In multi-infeed hybrid AC-DC (MIDC) systems, the emergency frequency control\n(EFC) with LCC-HVDC systems participating is of vital importance for system\nfrequency stability. Nevertheless, when regional power systems are operated by\ndifferent decision-makers, the LCC-HVDC systems and their connected AC systems\nmight be unwilling to participate in the EFC due to the costs and losses. In\nthis paper, to incentivize the LCC-HVDC systems and their connected adjacent AC\nsystems to participate in the droop-based EFC, a novel control-parameter-based\nincentive mechanism is proposed, which can deal with various possible emergency\nfrequency faults. Then, a non-cooperative-based incentive game model is\nformulated to implement the incentive mechanism in the MIDC system. An\nalgorithm for seeking the Nash equilibrium is designed, and the uniqueness of\nNash equilibrium is proven. Moreover, the individual rationality, incentive\ncompatibility and social optimality of the proposed mechanism are analyzed and\nproven. The effectiveness of the proposed incentive mechanism is verified\nthrough a case study.",
    "descriptor": "",
    "authors": [
      "Ye Liu",
      "Chen Shen",
      "Zhaojian Wang",
      "Feng Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14399"
  },
  {
    "id": "arXiv:2205.14400",
    "title": "Agent-based Simulation of District-based Elections",
    "abstract": "In district-based elections, electors cast votes in their respective\ndistricts. In each district, the party with maximum votes wins the\ncorresponding seat in the governing body. The election result is based on the\nnumber of seats won by different parties. In this system, locations of electors\nacross the districts may severely affect the election result even if the total\nnumber of votes obtained by different parties remains unchanged. A less popular\nparty may end up winning more seats if their supporters are suitably\ndistributed spatially. This happens due to various regional and social\ninfluences on individual voters which modulate their voting choice. In this\npaper, we explore agent-based models for district-based elections, where we\nconsider each elector as an agent, and try to represent their social and\ngeographical attributes and political inclinations using probability\ndistributions. This model can be used to simulate election results by Monte\nCarlo sampling. The models allow us to explore the full space of possible\noutcomes of an electoral setting, though they can also be calibrated to actual\nelection results for suitable values of parameters. We use Approximate Bayesian\nComputation (ABC) framework to estimate model parameters. We show that our\nmodel can reproduce the results of elections held in India and USA, and can\nalso produce counterfactual scenarios.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2006.11865\n",
    "authors": [
      "Adway Mitra"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.14400"
  },
  {
    "id": "arXiv:2205.14401",
    "title": "Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud  Pre-training",
    "abstract": "Masked Autoencoders (MAE) have shown great potentials in self-supervised\npre-training for language and 2D image transformers. However, it still remains\nan open question on how to exploit masked autoencoding for learning 3D\nrepresentations of irregular point clouds. In this paper, we propose\nPoint-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical\nself-supervised learning of 3D point clouds. Unlike the standard transformer in\nMAE, we modify the encoder and decoder into pyramid architectures to\nprogressively model spatial geometries and capture both fine-grained and\nhigh-level semantics of 3D shapes. For the encoder that downsamples point\ntokens by stages, we design a multi-scale masking strategy to generate\nconsistent visible regions across scales, and adopt a local spatial\nself-attention mechanism to focus on neighboring patterns. By multi-scale token\npropagation, the lightweight decoder gradually upsamples point tokens with\ncomplementary skip connections from the encoder, which further promotes the\nreconstruction from a global-to-local perspective. Extensive experiments\ndemonstrate the state-of-the-art performance of Point-M2AE for 3D\nrepresentation learning. With a frozen encoder after pre-training, Point-M2AE\nachieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some\nfully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves\n86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely\nbenefits the few-shot classification, part segmentation and 3D object detection\nwith the hierarchical pre-training scheme. Code will be available at\nhttps://github.com/ZrrSkywalker/Point-M2AE.",
    "descriptor": "",
    "authors": [
      "Renrui Zhang",
      "Ziyu Guo",
      "Peng Gao",
      "Rongyao Fang",
      "Bin Zhao",
      "Dong Wang",
      "Yu Qiao",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14401"
  },
  {
    "id": "arXiv:2205.14403",
    "title": "Rethinking the Setting of Semi-supervised Learning on Graphs",
    "abstract": "We argue that the present setting of semisupervised learning on graphs may\nresult in unfair comparisons, due to its potential risk of over-tuning\nhyper-parameters for models. In this paper, we highlight the significant\ninfluence of tuning hyper-parameters, which leverages the label information in\nthe validation set to improve the performance. To explore the limit of\nover-tuning hyperparameters, we propose ValidUtil, an approach to fully utilize\nthe label information in the validation set through an extra group of\nhyper-parameters. With ValidUtil, even GCN can easily get high accuracy of\n85.8% on Cora.\nTo avoid over-tuning, we merge the training set and the validation set and\nconstruct an i.i.d. graph benchmark (IGB) consisting of 4 datasets. Each\ndataset contains 100 i.i.d. graphs sampled from a large graph to reduce the\nevaluation variance. Our experiments suggest that IGB is a more stable\nbenchmark than previous datasets for semisupervised learning on graphs.",
    "descriptor": "\nComments: To appear in IJCAI 2022\n",
    "authors": [
      "Ziang Li",
      "Ming Ding",
      "Weikai Li",
      "Zihan Wang",
      "Ziyu Zeng",
      "Yukuo Cen",
      "Jie Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14403"
  },
  {
    "id": "arXiv:2205.14405",
    "title": "Strengthening Skeletal Action Recognizers via Leveraging Temporal  Patterns",
    "abstract": "Skeleton sequences are compact and lightweight. Numerous skeleton-based\naction recognizers have been proposed to classify human behaviors. In this\nwork, we aim to incorporate components that are compatible with existing models\nand further improve their accuracy. To this end, we design two temporal\naccessories: discrete cosine encoding (DCE) and chronological loss (CRL). DCE\nfacilitates models to analyze motion patterns from the frequency domain and\nmeanwhile alleviates the influence of signal noise. CRL guides networks to\nexplicitly capture the sequence's chronological order. These two components\nconsistently endow many recently-proposed action recognizers with accuracy\nboosts, achieving new state-of-the-art (SOTA) accuracy on two large benchmark\ndatasets (NTU60 and NTU120).",
    "descriptor": "",
    "authors": [
      "Zhenyue Qin",
      "Dongwoo Kim",
      "Yang Liu",
      "Saeed Anwar",
      "Tom Gedeon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14405"
  },
  {
    "id": "arXiv:2205.14407",
    "title": "An efficient polynomial-time approximation scheme for parallel  multi-stage open shops",
    "abstract": "Various new scheduling problems have been arising from practical production\nprocesses and spawning new research areas in the scheduling field. We study the\nparallel multi-stage open shops problem, which generalizes the classic open\nshop scheduling and parallel machine scheduling problems. Given m identical\nk-stage open shops and a set of n jobs, we aim to process all jobs on these\nopen shops with the minimum makespan, i.e., the completion time of the last\njob, under the constraint that job preemption is not allowed. We present an\nefficient polynomial-time approximation scheme (EPTAS) for the case when both m\nand k are constant. The main idea for our EPTAS is the combination of several\ncategorization, scaling, and linear programming rounding techniques. Jobs\nand/or operations are first scaled and then categorized carefully into multiple\ntypes so that different types of jobs and/or operations are scheduled\nappropriately without increasing the makespan too much.",
    "descriptor": "",
    "authors": [
      "Jianming Dong",
      "Ruyan Jin",
      "Guohui Lin",
      "Bing Su",
      "Weitian Tong",
      "Yao Xu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14407"
  },
  {
    "id": "arXiv:2205.14409",
    "title": "Find Your ASMR: A Perceptual Retrieval Interface for Autonomous Sensory  Meridian Response Videos",
    "abstract": "Autonomous sensory meridian response (ASMR) is a type of video contents\ndesigned to help people relax and feel comfortable. Users usually retrieve ASMR\ncontents from various video websites using only keywords. However, it is\nchallenging to examine satisfactory contents to reflect users' needs for ASMR\nvideos using keywords or content-based retrieval. To solve this issue, we\npropose a perceptual video retrieval system for ASMR videos and provide a novel\nretrieval user interface that allows users to retrieve content according to\nwatching purpose and anticipated expectations, such as excitement, calmness,\nstress and sadness. An ASMR video perception dataset is constructed with\nannotations on affective responses after watching the videos. To verify the\nproposed video retrieval system, a user study is conducted showing that users\ncan retrieve satisfactory ASMR contents easily and efficiently compared to\nconventional keywords-based retrieval systems.",
    "descriptor": "\nComments: 12 pages, 8 figures, in proceedings of HCII2022\n",
    "authors": [
      "Qi Zhou",
      "Jiahao Weng",
      "Haoran Xie"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.14409"
  },
  {
    "id": "arXiv:2205.14410",
    "title": "Multi-Source Transfer Learning for Deep Model-Based Reinforcement  Learning",
    "abstract": "Recent progress in deep model-based reinforcement learning allows agents to\nbe significantly more sample efficient by constructing world models of\nhigh-dimensional environments from visual observations, which enables agents to\nlearn complex behaviours in summarized lower-dimensional spaces. Reusing\nknowledge from relevant previous tasks is another approach for achieving better\ndata-efficiency, which becomes especially more likely when information of\nmultiple previously learned tasks is accessible. We show that the simplified\nrepresentations of environments resulting from world models provide for\npromising transfer learning opportunities, by introducing several methods that\nfacilitate world model agents to benefit from multi-source transfer learning.\nMethods are proposed for autonomously extracting relevant knowledge from both\nmulti-task and multi-agent settings as multi-source origins, resulting in\nsubstantial performance improvements compared to learning from scratch. We\nintroduce two additional novel techniques that enable and enhance the proposed\napproaches respectively: fractional transfer learning and universal feature\nspaces from a universal autoencoder. We demonstrate that our methods enable\ntransfer learning from different domains with different state, reward, and\naction spaces by performing extensive and challenging multi-domain experiments\non Dreamer, the state-of-the-art world model based algorithm for visual\ncontinuous control tasks.",
    "descriptor": "\nComments: 15 pages, 6 figures, 8 tables. arXiv admin note: text overlap with arXiv:2108.06526\n",
    "authors": [
      "Remo Sasso",
      "Matthia Sabatelli",
      "Marco A. Wiering"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14410"
  },
  {
    "id": "arXiv:2205.14411",
    "title": "Feature Pyramid Attention based Residual Neural Network for  Environmental Sound Classification",
    "abstract": "Environmental sound classification (ESC) is a challenging problem due to the\nunstructured spatial-temporal relations that exist in the sound signals.\nRecently, many studies have focused on abstracting features from convolutional\nneural networks while the learning of semantically relevant frames of sound\nsignals has been overlooked. To this end, we present an end-to-end framework,\nnamely feature pyramid attention network (FPAM), focusing on abstracting the\nsemantically relevant features for ESC. We first extract the feature maps of\nthe preprocessed spectrogram of the sound waveform by a backbone network. Then,\nto build multi-scale hierarchical features of sound spectrograms, we construct\na feature pyramid representation of the sound spectrograms by aggregating the\nfeature maps from multi-scale layers, where the temporal frames and spatial\nlocations of semantically relevant frames are localized by FPAM. Specifically,\nthe multiple features are first processed by a dimension alignment module.\nAfterward, the pyramid spatial attention module (PSA) is attached to localize\nthe important frequency regions spatially with a spatial attention module\n(SAM). Last, the processed feature maps are refined by a pyramid channel\nattention (PCA) to localize the important temporal frames. To justify the\neffectiveness of the proposed FPAM, visualization of attention maps on the\nspectrograms has been presented. The visualization results show that FPAM can\nfocus more on the semantic relevant regions while neglecting the noises. The\neffectiveness of the proposed methods is validated on two widely used ESC\ndatasets: the ESC-50 and ESC-10 datasets. The experimental results show that\nthe FPAM yields comparable performance to state-of-the-art methods. A\nsubstantial performance increase has been achieved by FPAM compared with the\nbaseline methods.",
    "descriptor": "",
    "authors": [
      "Liguang Zhou",
      "Yuhongze Zhou",
      "Xiaonan Qi",
      "Junjie Hu",
      "Tin Lun Lam",
      "Yangsheng Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Human-Computer Interaction (cs.HC)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14411"
  },
  {
    "id": "arXiv:2205.14412",
    "title": "Design, Modelling, and Control of a Reconfigurable Rotary Series Elastic  Actuator with Nonlinear Stiffness for Assistive Robots",
    "abstract": "In assistive robots, compliant actuator is a key component in establishing\nsafe and satisfactory physical human-robot interaction (pHRI). The performance\nof compliant actuators largely depends on the stiffness of the elastic element.\nGenerally, low stiffness is desirable to achieve low impedance, high fidelity\nof force control and safe pHRI, while high stiffness is required to ensure\nsufficient force bandwidth and output force. These requirements, however, are\ncontradictory and often vary according to different tasks and conditions. In\norder to address the contradiction of stiffness selection and improve\nadaptability to different applications, we develop a reconfigurable rotary\nseries elastic actuator with nonlinear stiffness (RRSEAns) for assistive\nrobots. In this paper, an accurate model of the reconfigurable rotary series\nelastic element (RSEE) is presented and the adjusting principles are\ninvestigated, followed by detailed analysis and experimental validation. The\nRRSEAns can provide a wide range of stiffness from 0.095 Nm/deg to 2.33 Nm/deg,\nand different stiffness profiles can be yielded with respect to different\nconfiguration of the reconfigurable RSEE. The overall performance of the\nRRSEAns is verified by experiments on frequency response, torque control and\npHRI, which is adequate for most applications in assistive robots.\nSpecifically, the root-mean-square (RMS) error of the interaction torque\nresults as low as 0.07 Nm in transparent/human-in-charge mode, demonstrating\nthe advantages of the RRSEAns in pHRI.",
    "descriptor": "",
    "authors": [
      "Yuepeng Qian",
      "Shuaishuai Han",
      "Gabriel Aguirre-Ollinger",
      "Chenglong Fu",
      "Haoyong Yu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14412"
  },
  {
    "id": "arXiv:2205.14413",
    "title": "Discrimination-Based Double Auction for Maximizing Social Welfare in the  Electricity and Heating Market Considering Privacy Preservation",
    "abstract": "This paper proposes a doubled-sided auction mechanism with price\ndiscrimination for social welfare (SW) maximization in the electricity and\nheating market. In this mechanism, energy service providers (ESPs) submit\noffers and load aggregators (LAs) submit bids to an energy trading center (ETC)\nto maximize their utility; in turn, the selfless ETC as an auctioneer leverages\ndis-criminatory price weights to regulate the behaviors of ESPs and LAs, which\ncombines the individual benefits of each stakeholder with the overall social\nwelfare to achieve the global optimum. Nash games are employed to describe the\ninteractions between players with the same market role. Theoretically, we first\nprove the existence and uniqueness of the Nash equilibrium; then, considering\nthe requirement of game players to preserve privacy, a distributed algorithm\nbased on the alternating direction method of multipliers is developed to\nimplement distributed bidding and analytical target cascading algorithm is\napplied to reach the balance of demand and supply. We validated the proposed\nmechanism using case studies on a city-level distribution system. The results\nindicated that the achieved SW improved by 4%-15% compared with other\nmechanisms, and also verified the effectiveness of the distributed algorithm.",
    "descriptor": "",
    "authors": [
      "Lu Wang",
      "Wei Gu",
      "Shuai Lu",
      "Haifeng Qiu",
      "Zhi Wu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14413"
  },
  {
    "id": "arXiv:2205.14414",
    "title": "Theoretical Foundation of the Stretch Energy Minimization for  Area-Preserving Mappings",
    "abstract": "The stretch energy is a fully nonlinear energy functional that has been\napplied to the numerical computation of area-preserving mappings. However, this\napproach lacks theoretical support and the analysis is complicated due to the\nfull nonlinearity of the functional. In this paper, we provide a theoretical\nfoundation of the stretch energy minimization for the computation of\narea-preserving mappings, including a neat formulation of the gradient of the\nfunctional, and the proof of the minimizers of the functional being\narea-preserving mappings. In addition, the geometric interpretation of the\nstretch energy is also provided to better understand this energy functional.\nFurthermore, numerical experiments are demonstrated to validate the\neffectiveness and accuracy of the stretch energy minimization for the\ncomputation of square-shaped area-preserving mappings of simplicial surfaces.",
    "descriptor": "",
    "authors": [
      "Mei-Heng Yueh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.14414"
  },
  {
    "id": "arXiv:2205.14415",
    "title": "Non-stationary Transformers: Rethinking the Stationarity in Time Series  Forecasting",
    "abstract": "Transformers have shown great power in time series forecasting due to their\nglobal-range modeling ability. However, their performance can degenerate\nterribly on non-stationary real-world data in which the joint distribution\nchanges over time. Previous studies primarily adopt stationarization to reduce\nthe non-stationarity of original series for better predictability. But the\nstationarized series deprived of inherent non-stationarity can be less\ninstructive for real-world bursty events forecasting. This problem, termed\nover-stationarization in this paper, leads Transformers to generate\nindistinguishable temporal attentions for different series and impedes the\npredictive capability of deep models. To tackle the dilemma between series\npredictability and model capability, we propose Non-stationary Transformers as\na generic framework with two interdependent modules: Series Stationarization\nand De-stationary Attention. Concretely, Series Stationarization unifies the\nstatistics of each input and converts the output with restored statistics for\nbetter predictability. To address over-stationarization, De-stationary\nAttention is devised to recover the intrinsic non-stationary information into\ntemporal dependencies by approximating distinguishable attentions learned from\nunstationarized series. Our Non-stationary Transformers framework consistently\nboosts mainstream Transformers by a large margin, which reduces 49.43% MSE on\nTransformer, 47.34% on Informer, and 46.89% on Reformer, making them the\nstate-of-the-art in time series forecasting.",
    "descriptor": "",
    "authors": [
      "Yong Liu",
      "Haixu Wu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14415"
  },
  {
    "id": "arXiv:2205.14418",
    "title": "Data Generation for Satellite Image Classification Using Self-Supervised  Representation Learning",
    "abstract": "Supervised deep neural networks are the-state-of-the-art for many tasks in\nthe remote sensing domain, against the fact that such techniques require the\ndataset consisting of pairs of input and label, which are rare and expensive to\ncollect in term of both manpower and resources. On the other hand, there are\nabundance of raw satellite images available both for commercial and academic\npurposes. Hence, in this work, we tackle the insufficient labeled data problem\nin satellite image classification task by introducing the process based on the\nself-supervised learning technique to create the synthetic labels for satellite\nimage patches. These synthetic labels can be used as the training dataset for\nthe existing supervised learning techniques. In our experiments, we show that\nthe models trained on the synthetic labels give similar performance to the\nmodels trained on the real labels. And in the process of creating the synthetic\nlabels, we also obtain the visual representation vectors that are versatile and\nknowledge transferable.",
    "descriptor": "\nComments: 11 pages, 6 figures, 5 tables. Submitted to Science & Technology Asia\n",
    "authors": [
      "Sarun Gulyanon",
      "Wasit Limprasert",
      "Pokpong Songmuang",
      "Rachada Kongkachandra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14418"
  },
  {
    "id": "arXiv:2205.14420",
    "title": "Fault-Aware Design and Training to Enhance DNNs Reliability with  Zero-Overhead",
    "abstract": "Deep Neural Networks (DNNs) enable a wide series of technological\nadvancements, ranging from clinical imaging, to predictive industrial\nmaintenance and autonomous driving. However, recent findings indicate that\ntransient hardware faults may corrupt the models prediction dramatically. For\ninstance, the radiation-induced misprediction probability can be so high to\nimpede a safe deployment of DNNs models at scale, urging the need for efficient\nand effective hardening solutions. In this work, we propose to tackle the\nreliability issue both at training and model design time. First, we show that\nvanilla models are highly affected by transient faults, that can induce a\nperformances drop up to 37%. Hence, we provide three zero-overhead solutions,\nbased on DNN re-design and re-train, that can improve DNNs reliability to\ntransient faults up to one order of magnitude. We complement our work with\nextensive ablation studies to quantify the gain in performances of each\nhardening component.",
    "descriptor": "\nComments: 7 pages, 6 figures\n",
    "authors": [
      "Niccol\u00f2 Cavagnero",
      "Fernando Dos Santos",
      "Marco Ciccone",
      "Giuseppe Averta",
      "Tatiana Tommasi",
      "Paolo Rech"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.14420"
  },
  {
    "id": "arXiv:2205.14421",
    "title": "Approximation of Functionals by Neural Network without Curse of  Dimensionality",
    "abstract": "In this paper, we establish a neural network to approximate functionals,\nwhich are maps from infinite dimensional spaces to finite dimensional spaces.\nThe approximation error of the neural network is $O(1/\\sqrt{m})$ where $m$ is\nthe size of networks, which overcomes the curse of dimensionality. The key idea\nof the approximation is to define a Barron space of functionals.",
    "descriptor": "",
    "authors": [
      "Yahong Yang",
      "Yang Xiang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14421"
  },
  {
    "id": "arXiv:2205.14423",
    "title": "On the maximal solution of the conjugate discrete-time algebraic Riccati  equation",
    "abstract": "In this paper we consider a class of conjugate discrete-time Riccati\nequations, arising originally from the linear quadratic regulation problem for\ndiscrete-time antilinear systems. Under some mild assumptions and the framework\nof the fixed-point iteration, a constructive proof is given for the existence\nof the maximal solution to the conjugate discrete-time Riccati equation, in\nwhich the control weighting matrix is nonsingular and its constant term is\nHermitian. Moreover, starting with a suitable initial matrix, we also show that\nthe nonincreasing sequence generated by the fixed-point iteration converges at\nleast linearly to the maximal solution of the Riccati equation. An example is\ngiven to demonstrate the correctness of our main theorem and provide\nconsiderable insights into the study of another meaningful solutions.",
    "descriptor": "",
    "authors": [
      "Hung-Yuan Fan",
      "Chun-Yueh Chiang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14423"
  },
  {
    "id": "arXiv:2205.14428",
    "title": "Go Beyond Multiple Instance Neural Networks: Deep-learning Models based  on Local Pattern Aggregation",
    "abstract": "Deep convolutional neural networks (CNNs) have brought breakthroughs in\nprocessing clinical electrocardiograms (ECGs), speaker-independent speech and\ncomplex images. However, typical CNNs require a fixed input size while it is\ncommon to process variable-size data in practical use. Recurrent networks such\nas long short-term memory (LSTM) are capable of eliminating the restriction,\nbut suffer from high computational complexity. In this paper, we propose local\npattern aggregation-based deep-learning models to effectively deal with both\nproblems. The novel network structure, called LPANet, has cropping and\naggregation operations embedded into it. With these new features, LPANet can\nreduce the difficulty of tuning model parameters and thus tend to improve\ngeneralization performance. To demonstrate the effectiveness, we applied it to\nthe problem of premature ventricular contraction detection and the experimental\nresults shows that our proposed method has certain advantages compared to\nclassical network models, such as CNN and LSTM.",
    "descriptor": "",
    "authors": [
      "Linpeng Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14428"
  },
  {
    "id": "arXiv:2205.14430",
    "title": "Angle-Uniform Parallel Coordinates",
    "abstract": "We present angle-uniform parallel coordinates, a data-independent technique\nthat deforms the image plane of parallel coordinates so that the angles of\nlinear relationships between two variables are linearly mapped along the\nhorizontal axis of the parallel coordinates plot. Despite being a common method\nfor visualizing multidimensional data, parallel coordinates are ineffective for\nrevealing positive correlations since the associated parallel coordinates\npoints of such structures may be located at infinity in the image plane and the\nasymmetric encoding of negative and positive correlations may lead to\nunreliable estimations. To address this issue, we introduce a transformation\nthat bounds all points horizontally using an angle-uniform mapping and shrinks\nthem vertically in a structure-preserving fashion; polygonal lines become\nsmooth curves and symmetric representation of data correlations is achieved. We\nfurther propose a combined subsampling and density visualization approach to\nreduce visual clutter caused by overdrawing. Our method enables accurate visual\npattern interpretation of data correlations, and its data-independent nature\nmakes it applicable to all multidimensional datasets. The usefulness of our\nmethod is demonstrated using examples of synthetic and real-world datasets.",
    "descriptor": "\nComments: To be published in Computational Visual Media\n",
    "authors": [
      "Kaiyi Zhang",
      "Liang Zhou",
      "Lu Chen",
      "Shitong He",
      "Daniel Weiskopf",
      "Yunhai Wang"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.14430"
  },
  {
    "id": "arXiv:2205.14433",
    "title": "Discovery and capabilities of guard proxies for CoRE networks",
    "abstract": "Constrained RESTful Environments tolerate and even benefit from proxy\nservices. We explore the concept of proxies installed at entry points to\nconstrained networks without any unified management. We sketch proxies of\ndifferent levels of intrusiveness into applications, their announcement and\ndiscovery, and compare their theoretical capabilities in mitigating the effects\nof undesired traffic that can otherwise exhaust the environment's constrained\nresources.",
    "descriptor": "",
    "authors": [
      "Christian Ams\u00fcss"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14433"
  },
  {
    "id": "arXiv:2205.14434",
    "title": "A Theory of L-shaped Floor-plans",
    "abstract": "Existing graph theoretic approaches are mainly restricted to floor-plans with\nrectangular boundary. In this paper, we introduce floor-plans with L-shaped\nboundary (boundary with only one concave corner). To ensure the L-shaped\nboundary, we introduce the concept of non-triviality of a floor-plan. A\nfloor-plan with a rectilinear boundary with at least one concave corner is\nnon-trivial if the number of concave corners can not be reduced, without\naffecting the modules adjacencies within it. Further, we present necessary and\nsufficient conditions for the existence of a non-trivial L-shaped floor-plan\ncorresponding to a properly triangulated planar graph (PTPG) G. Also, we\ndevelop an O(n^2) algorithm for its construction, if it exists.",
    "descriptor": "\nComments: 35 pages, 61 figures\n",
    "authors": [
      "Raveena",
      "Krishnendra Shekhawat"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.14434"
  },
  {
    "id": "arXiv:2205.14439",
    "title": "Laplace HypoPINN: Physics-Informed Neural Network for hypocenter  localization and its predictive uncertainty",
    "abstract": "Several techniques have been proposed over the years for automatic hypocenter\nlocalization. While those techniques have pros and cons that trade-off\ncomputational efficiency and the susceptibility of getting trapped in local\nminima, an alternate approach is needed that allows robust localization\nperformance and holds the potential to make the elusive goal of real-time\nmicroseismic monitoring possible. Physics-informed neural networks (PINNs) have\nappeared on the scene as a flexible and versatile framework for solving partial\ndifferential equations (PDEs) along with the associated initial or boundary\nconditions. We develop HypoPINN -- a PINN-based inversion framework for\nhypocenter localization and introduce an approximate Bayesian framework for\nestimating its predictive uncertainties. This work focuses on predicting the\nhypocenter locations using HypoPINN and investigates the propagation of\nuncertainties from the random realizations of HypoPINN's weights and biases\nusing the Laplace approximation. We train HypoPINN to obtain the optimized\nweights for predicting hypocenter location. Next, we approximate the covariance\nmatrix at the optimized HypoPINN's weights for posterior sampling with the\nLaplace approximation. The posterior samples represent various realizations of\nHypoPINN's weights. Finally, we predict the locations of the hypocenter\nassociated with those weights' realizations to investigate the uncertainty\npropagation that comes from those realisations. We demonstrate the features of\nthis methodology through several numerical examples, including using the Otway\nvelocity model based on the Otway project in Australia.",
    "descriptor": "\nComments: 13 pages, 9 figures\n",
    "authors": [
      "Muhammad Izzatullah",
      "Isa Eren Yildirim",
      "Umair Bin Waheed",
      "Tariq Alkhalifah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14439"
  },
  {
    "id": "arXiv:2205.14440",
    "title": "Large-Scale Privacy-Preserving Network Embedding against Private Link  Inference Attacks",
    "abstract": "Network embedding represents network nodes by a low-dimensional informative\nvector. While it is generally effective for various downstream tasks, it may\nleak some private information of networks, such as hidden private links. In\nthis work, we address a novel problem of privacy-preserving network embedding\nagainst private link inference attacks. Basically, we propose to perturb the\noriginal network by adding or removing links, and expect the embedding\ngenerated on the perturbed network can leak little information about private\nlinks but hold high utility for various downstream tasks. Towards this goal, we\nfirst propose general measurements to quantify privacy gain and utility loss\nincurred by candidate network perturbations; we then design a PPNE framework to\nidentify the optimal perturbation solution with the best privacy-utility\ntrade-off in an iterative way. Furthermore, we propose many techniques to\naccelerate PPNE and ensure its scalability. For instance, as the skip-gram\nembedding methods including DeepWalk and LINE can be seen as matrix\nfactorization with closed form embedding results, we devise efficient privacy\ngain and utility loss approximation methods to avoid the repetitive\ntime-consuming embedding training for every candidate network perturbation in\neach iteration. Experiments on real-life network datasets (with up to millions\nof nodes) verify that PPNE outperforms baselines by sacrificing less utility\nand obtaining higher privacy protection.",
    "descriptor": "",
    "authors": [
      "Xiao Han",
      "Leye Wang",
      "Junjie Wu",
      "Yuncong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14440"
  },
  {
    "id": "arXiv:2205.14442",
    "title": "Looks Like Magic: Transfer Learning in GANs to Generate New Card  Illustrations",
    "abstract": "In this paper, we propose MAGICSTYLEGAN and MAGICSTYLEGAN-ADA - both\nincarnations of the state-of-the-art models StyleGan2 and StyleGan2 ADA - to\nexperiment with their capacity of transfer learning into a rather different\ndomain: creating new illustrations for the vast universe of the game \"Magic:\nThe Gathering\" cards. This is a challenging task especially due to the variety\nof elements present in these illustrations, such as humans, creatures,\nartifacts, and landscapes - not to mention the plethora of art styles of the\nimages made by various artists throughout the years. To solve the task at hand,\nwe introduced a novel dataset, named MTG, with thousands of illustration from\ndiverse card types and rich in metadata. The resulting set is a dataset\ncomposed by a myriad of both realistic and fantasy-like illustrations.\nAlthough, to investigate effects of diversity we also introduced subsets that\ncontain specific types of concepts, such as forests, islands, faces, and\nhumans. We show that simpler models, such as DCGANs, are not able to learn to\ngenerate proper illustrations in any setting. On the other side, we train\ninstances of MAGICSTYLEGAN using all proposed subsets, being able to generate\nhigh quality illustrations. We perform experiments to understand how well\npre-trained features from StyleGan2 can be transferred towards the target\ndomain. We show that in well trained models we can find particular instances of\nnoise vector that realistically represent real images from the dataset.\nMoreover, we provide both quantitative and qualitative studies to support our\nclaims, and that demonstrate that MAGICSTYLEGAN is the state-of-the-art\napproach for generating Magic illustrations. Finally, this paper highlights\nsome emerging properties regarding transfer learning in GANs, which is still a\nsomehow under-explored field in generative learning research.",
    "descriptor": "",
    "authors": [
      "Matheus K. Venturelli",
      "Pedro H. Gomes",
      "J\u00f4natas Wehrmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14442"
  },
  {
    "id": "arXiv:2205.14443",
    "title": "A Closer Look at Self-supervised Lightweight Vision Transformers",
    "abstract": "Self-supervised learning on large-scale Vision Transformers (ViTs) as\npre-training methods has achieved promising downstream performance. Yet, how\nsuch pre-training paradigms promote lightweight ViTs' performance is\nconsiderably less studied. In this work, we mainly produce recipes for\npre-training high-performance lightweight ViTs using\nmasked-image-modeling-based MAE, namely MAE-lite, which achieves 78.4% top-1\naccuracy on ImageNet with ViT-Tiny (5.7M). Furthermore, we develop and\nbenchmark other fully-supervised and self-supervised pre-training counterparts,\ne.g., contrastive-learning-based MoCo-v3, on both ImageNet and other\nclassification tasks. We analyze and clearly show the effect of such\npre-training, and reveal that properly-learned lower layers of the pre-trained\nmodels matter more than higher ones in data-sufficient downstream tasks.\nFinally, by further comparing with the pre-trained representations of the\nup-scaled models, a distillation strategy during pre-training is developed to\nimprove the pre-trained representations as well, leading to further downstream\nperformance improvement. The code and models will be made publicly available.",
    "descriptor": "",
    "authors": [
      "Shaoru Wang",
      "Jin Gao",
      "Zeming Li",
      "Jian Sun",
      "Weiming Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14443"
  },
  {
    "id": "arXiv:2205.14444",
    "title": "Visual Superordinate Abstraction for Robust Concept Learning",
    "abstract": "Concept learning constructs visual representations that are connected to\nlinguistic semantics, which is fundamental to vision-language tasks. Although\npromising progress has been made, existing concept learners are still\nvulnerable to attribute perturbations and out-of-distribution compositions\nduring inference. We ascribe the bottleneck to a failure of exploring the\nintrinsic semantic hierarchy of visual concepts, e.g. \\{red, blue,...\\} $\\in$\n`color' subspace yet cube $\\in$ `shape'. In this paper, we propose a visual\nsuperordinate abstraction framework for explicitly modeling semantic-aware\nvisual subspaces (i.e. visual superordinates). With only natural visual\nquestion answering data, our model first acquires the semantic hierarchy from a\nlinguistic view, and then explores mutually exclusive visual superordinates\nunder the guidance of linguistic hierarchy. In addition, a quasi-center visual\nconcept clustering and a superordinate shortcut learning schemes are proposed\nto enhance the discrimination and independence of concepts within each visual\nsuperordinate. Experiments demonstrate the superiority of the proposed\nframework under diverse settings, which increases the overall answering\naccuracy relatively by 7.5\\% on reasoning with perturbations and 15.6\\% on\ncompositional generalization tests.",
    "descriptor": "\nComments: 14 pages, 11 figures\n",
    "authors": [
      "Qi Zheng",
      "Chaoyue Wang",
      "Dadong Wang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14444"
  },
  {
    "id": "arXiv:2205.14449",
    "title": "Controller-Aware Dynamic Network Management for Industry 4.0",
    "abstract": "In this paper, we consider a cyber-physical manufacturing system (CPMS)\nscenario containing physical components (robots, sensors, and actuators),\noperating in a digitally connected, constrained environment to perform\nindustrial tasks. The CPMS has a centralized control plane with digital twins\n(DTs) of the physical resources, computational resources, and a network manager\nthat allocates network resources. Existing approaches for allocation of network\nresources are typically fixed with respect to controller-dependent run-time\nspecifications, which may impact the performance of physical processes. We\npropose a dynamic network management framework, where the network resource\nallocation schemes are controller-aware. The information about the controllers\nof the physical resources is implemented at the DT level, and metrics, such as\nregret bounds, take the process performance measures into account. The proposed\nnetwork management schemes optimize physical system performance by balancing\nthe shared resources between the physical assets on the plant floor, and by\nconsidering their control requirements, providing a new perspective for dynamic\nresource allocation. A simulation study is provided to illustrate the\nperformance of the proposed network management approaches and compare their\nefficiencies.",
    "descriptor": "",
    "authors": [
      "Efe C. Balta",
      "Mohammad H. Mamduhi",
      "John Lygeros",
      "Alisa Rupenyan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14449"
  },
  {
    "id": "arXiv:2205.14452",
    "title": "Stochastic Gradient Methods with Compressed Communication for  Decentralized Saddle Point Problems",
    "abstract": "We propose two stochastic gradient algorithms to solve a class of\nsaddle-point problems in a decentralized setting (without a central server).\nThe proposed algorithms are the first to achieve sub-linear/linear computation\nand communication complexities using respectively stochastic\ngradient/stochastic variance reduced gradient oracles with compressed\ninformation exchange to solve non-smooth strongly-convex strongly-concave\nsaddle-point problems in decentralized setting. Our first algorithm is a\nRestart-based Decentralized Proximal Stochastic Gradient method with\nCompression (C-RDPSG) for general stochastic settings. We provide rigorous\ntheoretical guarantees of C-RDPSG with gradient computation complexity and\ncommunication complexity of order $\\mathcal{O}( (1+\\delta)^4\n\\frac{1}{L^2}{\\kappa_f^2}\\kappa_g^2 \\frac{1}{\\epsilon} )$, to achieve an\n$\\epsilon$-accurate saddle-point solution, where $\\delta$ denotes the\ncompression factor, $\\kappa_f$ and $\\kappa_g$ denote respectively the condition\nnumbers of objective function and communication graph, and $L$ denotes the\nsmoothness parameter of the smooth part of the objective function. Next, we\npresent a Decentralized Proximal Stochastic Variance Reduced Gradient algorithm\nwith Compression (C-DPSVRG) for finite sum setting which exhibits gradient\ncomputation complexity and communication complexity of order\n$\\mathcal{O}((1+\\delta)\\kappa_f^2 \\kappa_g \\log(\\frac{1}{\\epsilon}))$.\nExtensive numerical experiments show competitive performance of the proposed\nalgorithms and provide support to the theoretical results obtained.",
    "descriptor": "",
    "authors": [
      "Chhavi Sharma",
      "Vishnu Narayanan",
      "P. Balamurugan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14452"
  },
  {
    "id": "arXiv:2205.14457",
    "title": "An adaptive admittance controller for collaborative drilling with a  robot based on subtask classification via deep learning",
    "abstract": "In this paper, we propose a supervised learning approach based on an\nArtificial Neural Network (ANN) model for real-time classification of subtasks\nin a physical human-robot interaction (pHRI) task involving contact with a\nstiff environment. In this regard, we consider three subtasks for a given pHRI\ntask: Idle, Driving, and Contact. Based on this classification, the parameters\nof an admittance controller that regulates the interaction between human and\nrobot are adjusted adaptively in real time to make the robot more transparent\nto the operator (i.e. less resistant) during the Driving phase and more stable\nduring the Contact phase. The Idle phase is primarily used to detect the\ninitiation of task. Experimental results have shown that the ANN model can\nlearn to detect the subtasks under different admittance controller conditions\nwith an accuracy of 98% for 12 participants. Finally, we show that the\nadmittance adaptation based on the proposed subtask classifier leads to 20%\nlower human effort (i.e. higher transparency) in the Driving phase and 25%\nlower oscillation amplitude (i.e. higher stability) during drilling in the\nContact phase compared to an admittance controller with fixed parameters.",
    "descriptor": "",
    "authors": [
      "Pouya P. Niaz",
      "Berk Guler",
      "Alireza Madani",
      "Yusuf Aydin",
      "Cagatay Basdogan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14457"
  },
  {
    "id": "arXiv:2205.14458",
    "title": "Variational Transformer: A Framework Beyond the Trade-off between  Accuracy and Diversity for Image Captioning",
    "abstract": "Accuracy and Diversity are two essential metrizable manifestations in\ngenerating natural and semantically correct captions. Many efforts have been\nmade to enhance one of them with another decayed due to the trade-off gap.\nHowever, compromise does not make the progress. Decayed diversity makes the\ncaptioner a repeater, and decayed accuracy makes it a fake advisor. In this\nwork, we exploit a novel Variational Transformer framework to improve accuracy\nand diversity simultaneously. To ensure accuracy, we introduce the \"Invisible\nInformation Prior\" along with the \"Auto-selectable GMM\" to instruct the encoder\nto learn the precise language information and object relation in different\nscenes. To ensure diversity, we propose the \"Range-Median Reward\" baseline to\nretain more diverse candidates with higher rewards during the RL-based training\nprocess. Experiments show that our method achieves the simultaneous promotion\nof accuracy (CIDEr) and diversity (self-CIDEr), up to 1.1 and 4.8 percent,\ncompared with the baseline. Also, our method outperforms others under the newly\nproposed measurement of the trade-off gap, with at least 3.55 percent\npromotion.",
    "descriptor": "",
    "authors": [
      "Longzhen Yang",
      "Shaohua Shang",
      "Yihang Liu",
      "Yitao Peng",
      "Lianghua He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14458"
  },
  {
    "id": "arXiv:2205.14459",
    "title": "CyCLIP: Cyclic Contrastive Language-Image Pretraining",
    "abstract": "Recent advances in contrastive representation learning over paired image-text\ndata have led to models such as CLIP that achieve state-of-the-art performance\nfor zero-shot classification and distributional robustness. Such models\ntypically require joint reasoning in the image and text representation spaces\nfor downstream inference tasks. Contrary to prior beliefs, we demonstrate that\nthe image and text representations learned via a standard contrastive objective\nare not interchangeable and can lead to inconsistent downstream predictions. To\nmitigate this issue, we formalize consistency and propose CyCLIP, a framework\nfor contrastive representation learning that explicitly optimizes for the\nlearned representations to be geometrically consistent in the image and text\nspace. In particular, we show that consistent representations can be learned by\nexplicitly symmetrizing (a) the similarity between the two mismatched\nimage-text pairs (cross-modal consistency); and (b) the similarity between the\nimage-image pair and the text-text pair (in-modal consistency). Empirically, we\nshow that the improved consistency in CyCLIP translates to significant gains\nover CLIP, with gains ranging from 10%-24% for zero-shot classification\naccuracy on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet1K) and 10%-27%\nfor robustness to various natural distribution shifts. The code is available at\nhttps://github.com/goel-shashank/CyCLIP.",
    "descriptor": "\nComments: 19 pages, 11 tables, 6 figures\n",
    "authors": [
      "Shashank Goel",
      "Hritik Bansal",
      "Sumit Bhatia",
      "Ryan A. Rossi",
      "Vishwa Vinay",
      "Aditya Grover"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14459"
  },
  {
    "id": "arXiv:2205.14460",
    "title": "Visual Perception of Building and Household Vulnerability from Streets",
    "abstract": "In developing countries, building codes often are outdated or not enforced.\nAs a result, a large portion of the housing stock is substandard and vulnerable\nto natural hazards and climate related events. Assessing housing quality is key\nto inform public policies and private investments. Standard assessment methods\nare typically carried out only on a sample / pilot basis due to its high costs\nor, when complete, tend to be obsolete due to the lack of compliance with\nrecommended updating standards or not accessible to most users with the level\nof detail needed to take key policy or business decisions. Thus, we propose an\nevaluation framework that is cost-efficient for first capture and future\nupdates, and is reliable at the block level. The framework complements existing\nwork of using street view imagery combined with deep learning to automatically\nextract building information to assist the identification of housing\ncharacteristics. We then check its potential for scalability and higher level\nreliability. For that purpose, we create an index, which synthesises the\nhighest possible level of granularity of data at the housing unit and at the\nhousehold level at the block level, and assess whether the predictions made by\nour model could be used to approximate vulnerability conditions with a lower\nbudget and in selected areas. Our results indicated that the predictions from\nthe images are clearly correlated with the index.",
    "descriptor": "",
    "authors": [
      "Chaofeng Wang",
      "Sarah Elizabeth Antos",
      "Jessica Grayson Gosling Goldsmith",
      "Luis Miguel Triveno"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14460"
  },
  {
    "id": "arXiv:2205.14462",
    "title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
    "abstract": "As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.",
    "descriptor": "\nComments: Accepted in the 13th Edition of Language Resources and Evaluation Conference (LREC 2022)\n",
    "authors": [
      "Mohammad Faiyaz Khan",
      "S.M. Sadiq-Ur-Rahman Shifath",
      "Md Saiful Islam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14462"
  },
  {
    "id": "arXiv:2205.14465",
    "title": "Espresso: Revisiting Gradient Compression from the System Perspective",
    "abstract": "Gradient compression (GC) is a promising approach to addressing the\ncommunication bottleneck in distributed deep learning (DDL). However, it is\nchallenging to find the optimal compression strategy for applying GC to DDL\nbecause of the intricate interactions among tensors. To fully unleash the\nbenefits of GC, two questions must be addressed: 1) How to express all\ncompression strategies and the corresponding interactions among tensors of any\nDDL training job? 2) How to quickly select a near-optimal compression strategy?\nIn this paper, we propose Espresso to answer these questions. It first designs\na decision tree abstraction to express all the compression strategies and\ndevelops empirical models to timeline tensor computation, communication, and\ncompression to enable Espresso to derive the intricate interactions among\ntensors. It then designs a compression decision algorithm that analyzes tensor\ninteractions to eliminate and prioritize strategies and optimally offloads\ncompression to CPUs. Experimental evaluations show that Espresso can improve\nthe training throughput over the start-of-the-art compression-enabled system by\nup to 77% for representative DDL training jobs. Moreover, the computational\ntime needed to select the compression strategy is measured in milliseconds, and\nthe selected strategy is only a few percent from optimal.",
    "descriptor": "",
    "authors": [
      "Zhuang Wang",
      "Haibin Lin",
      "Yibo Zhu",
      "T. S. Eugene Ng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14465"
  },
  {
    "id": "arXiv:2205.14467",
    "title": "Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of  Black-Box Predictors",
    "abstract": "Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an\nunlabeled target domain supervised by a black-box predictor trained on a source\ndomain. It does not require access to both the source-domain data and the\npredictor parameters, thus addressing the data privacy and portability issues\nof standard domain adaptation. Existing DABP approaches mostly rely on model\ndistillation from the black-box predictor, \\emph{i.e.}, training the model with\nits noisy target-domain predictions, which however inevitably introduces the\nconfirmation bias accumulated from the prediction noises. To mitigate such\nbias, we propose a new method, named BETA, to incorporate knowledge\ndistillation and noisy label learning into one coherent framework. This is\nenabled by a new divide-to-adapt strategy. BETA divides the target domain into\nan easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain. Then\nit deploys mutually-teaching twin networks to filter the predictor errors for\neach other and improve them progressively, from the easy to hard subdomains. As\nsuch, BETA effectively purifies the noisy labels and reduces error\naccumulation. We theoretically show that the target error of BETA is minimized\nby decreasing the noise ratio of the subdomains. Extensive experiments\ndemonstrate BETA outperforms existing methods on all DABP benchmarks, and is\neven comparable with the standard domain adaptation methods that use the\nsource-domain data.",
    "descriptor": "\nComments: A black-box model adaptation approach that proposes divide-to-adapt to suppress the confirmation bias\n",
    "authors": [
      "Jianfei Yang",
      "Xiangyu Peng",
      "Kai Wang",
      "Zheng Zhu",
      "Jiashi Feng",
      "Lihua Xie",
      "Yang You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14467"
  },
  {
    "id": "arXiv:2205.14472",
    "title": "Perceptually Optimized Color Selection for Visualization",
    "abstract": "We propose an approach, called the Equilibrium Distribution Model (EDM), for\nautomatically selecting colors with optimum perceptual contrast for scientific\nvisualization. Given any number of features that need to be emphasized in a\nvisualization task, our approach derives evenly distributed points in the\nCIELAB color space to assign colors to the features so that the minimum\nEuclidean Distance among the colors are optimized. Our approach can assign\ncolors with high perceptual contrast even for very high numbers of features,\nwhere other color selection methods typically fail. We compare our approach\nwith the widely used Harmonic color selection scheme and demonstrate that while\nthe harmonic scheme can achieve reasonable color contrast for visualizing up to\n20 different features, our Equilibrium scheme provides significantly better\ncontrast and achieves perceptible contrast for visualizing even up to 100\nunique features.",
    "descriptor": "\nComments: 2 pages, 4 figures, 1 table, Poster presented at IEEE Visualization Conference, Oct 21 - 26, 2018, Berlin, Germany\n",
    "authors": [
      "Subhrajyoti Maji",
      "John Dingliana"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14472"
  },
  {
    "id": "arXiv:2205.14473",
    "title": "Efficient-Adam: Communication-Efficient Distributed Adam with Complexity  Analysis",
    "abstract": "Distributed adaptive stochastic gradient methods have been widely used for\nlarge-scale nonconvex optimization, such as training deep learning models.\nHowever, their communication complexity on finding $\\varepsilon$-stationary\npoints has rarely been analyzed in the nonconvex setting. In this work, we\npresent a novel communication-efficient distributed Adam in the\nparameter-server model for stochastic nonconvex optimization, dubbed {\\em\nEfficient-Adam}. Specifically, we incorporate a two-way quantization scheme\ninto Efficient-Adam to reduce the communication cost between the workers and\nserver. Simultaneously, we adopt a two-way error feedback strategy to reduce\nthe biases caused by the two-way quantization on both the server and workers,\nrespectively. In addition, we establish the iteration complexity for the\nproposed Efficient-Adam with a class of quantization operators, and further\ncharacterize its communication complexity between the server and workers when\nan $\\varepsilon$-stationary point is achieved. Finally, we apply Efficient-Adam\nto solve a toy stochastic convex optimization problem and train deep learning\nmodels on real-world vision and language tasks. Extensive experiments together\nwith a theoretical guarantee justify the merits of Efficient Adam.",
    "descriptor": "",
    "authors": [
      "Congliang Chen",
      "Li Shen",
      "Wei Liu",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14473"
  },
  {
    "id": "arXiv:2205.14474",
    "title": "DeepRM: Deep Recurrent Matching for 6D Pose Refinement",
    "abstract": "Precise 6D pose estimation of rigid objects from RGB images is a critical but\nchallenging task in robotics and augmented reality. To address this problem, we\npropose DeepRM, a novel recurrent network architecture for 6D pose refinement.\nDeepRM leverages initial coarse pose estimates to render synthetic images of\ntarget objects. The rendered images are then matched with the observed images\nto predict a rigid transform for updating the previous pose estimate. This\nprocess is repeated to incrementally refine the estimate at each iteration.\nLSTM units are used to propagate information through each refinement step,\nsignificantly improving overall performance. In contrast to many 2-stage\nPerspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a\nscalable backbone that can be tuned via a single parameter for accuracy and\nefficiency. During training, a multi-scale optical flow head is added to\npredict the optical flow between the observed and synthetic images. Optical\nflow prediction stabilizes the training process, and enforces the learning of\nfeatures that are relevant to the task of pose estimation. Our results\ndemonstrate that DeepRM achieves state-of-the-art performance on two widely\naccepted challenging datasets.",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Alexander Avery",
      "Andreas Savakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14474"
  },
  {
    "id": "arXiv:2205.14475",
    "title": "Performance Analysis of Self-Interference Cancellation in Full-Duplex  Massive MIMO Systems: Subtraction versus Spatial Suppression",
    "abstract": "Massive multiple-input multiple-output (MIMO) and full-duplex (FD) are\npromising candidates for achieving the spectral efficiency to meet the needs of\n5G communications. One essential key to realizing practical FD massive MIMO\nsystems is how to effectively mitigate the self-interference (SI).\nConventionally, however, the performance comparison of different SI methods by\nreflecting the actual channel characteristics was insufficient in the\nliterature. Accordingly, this paper presents a performance analysis of SI\ncancellation (SIC) methods in FD massive MIMO systems. Analytical and numerical\nresults confirm that, in an imperfect channel-estimation case, the ergodic\nrates performance of the spatial suppression in the uplink outperforms those of\nthe SI subtraction, due to the correlation between the precoder and the\nestimation error of the SI channel. In addition, we discuss which method\nperforms better under different given system constraints such as uplink and\ndownlink sum rates, the total transmit power, and the power scaling law.",
    "descriptor": "\nComments: 15 pages, 8 figures, (under revision) submitted to IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Soo-Min Kim",
      "Yeon-Geun Lim",
      "Linglong Dai",
      "Chan-Byoung Chae"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14475"
  },
  {
    "id": "arXiv:2205.14477",
    "title": "MDMLP: Image Classification from Scratch on Small Datasets with MLP",
    "abstract": "The attention mechanism has become a go-to technique for natural language\nprocessing and computer vision tasks. Recently, the MLP-Mixer and other\nMLP-based architectures, based simply on multi-layer perceptrons (MLPs), are\nalso powerful compared to CNNs and attention techniques and raises a new\nresearch direction. However, the high capability of the MLP-based networks\nseverely relies on large volume of training data, and lacks of explanation\nability compared to the Vision Transformer (ViT) or ConvNets. When trained on\nsmall datasets, they usually achieved inferior results than ConvNets. To\nresolve it, we present (i) multi-dimensional MLP (MDMLP), a conceptually simple\nand lightweight MLP-based architecture yet achieves SOTA when training from\nscratch on small-size datasets; (ii) multi-dimension MLP Attention Tool\n(MDAttnTool), a novel and efficient attention mechanism based on MLPs. Even\nwithout strong data augmentation, MDMLP achieves 90.90% accuracy on CIFAR10\nwith only 0.3M parameters, while the well-known MLP-Mixer achieves 85.45% with\n17.1M parameters. In addition, the lightweight MDAttnTool highlights objects in\nimages, indicating its explanation power. Our code is available at\nhttps://github.com/Amoza-Theodore/MDMLP.",
    "descriptor": "",
    "authors": [
      "Tian Lv",
      "Chongyang Bai",
      "Chaojie Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14477"
  },
  {
    "id": "arXiv:2205.14478",
    "title": "Overcoming Congestion in Distributed Coloring",
    "abstract": "We present a new technique to efficiently sample and communicate a large\nnumber of elements from a distributed sampling space. When used in the context\nof a recent LOCAL algorithm for $(\\operatorname{degree}+1)$-list-coloring\n(D1LC), this allows us to solve D1LC in $O(\\log^5 \\log n)$ CONGEST rounds, and\nin only $O(\\log^* n)$ rounds when the graph has minimum degree $\\Omega(\\log^7\nn)$, w.h.p.\nThe technique also has immediate applications in testing some graph\nproperties locally, and for estimating the sparsity/density of local subgraphs\nin $O(1)$ CONGEST rounds, w.h.p.",
    "descriptor": "\nComments: This paper incorporates results from the technical report arXiv:2105.04700 on adapting LOCAL algorithms to CONGEST. This excludes the other results in arXiv:2105.04700, which were refactored in arXiv:2112.00604\n",
    "authors": [
      "Magn\u00fas M. Halld\u00f3rsson",
      "Alexandre Nolin",
      "Tigran Tonoyan"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14478"
  },
  {
    "id": "arXiv:2205.14479",
    "title": "TinyIREE: An ML Execution Environment for Embedded Systems from  Compilation to Deployment",
    "abstract": "Machine learning model deployment for training and execution has been an\nimportant topic for industry and academic research in the last decade. Much of\nthe attention has been focused on developing specific toolchains to support\nacceleration hardware. In this paper, we present IREE, a unified compiler and\nruntime stack with the explicit goal to scale down machine learning programs to\nthe smallest footprints for mobile and edge devices, while maintaining the\nability to scale up to larger deployment targets. IREE adopts a compiler-based\napproach and optimizes for heterogeneous hardware accelerators through the use\nof the MLIR compiler infrastructure which provides the means to quickly design\nand implement multi-level compiler intermediate representations (IR). More\nspecifically, this paper is focused on TinyIREE, which is a set of deployment\noptions in IREE that accommodate the limited memory and computation resources\nin embedded systems and bare-metal platforms, while also demonstrating IREE's\nintuitive workflow that generates workloads for different ISA extensions and\nABIs through LLVM.",
    "descriptor": "\nComments: 9 pages, 3 figures, to be published in IEEE Micro\n",
    "authors": [
      "Hsin-I Cindy Liu",
      "Marius Brehler",
      "Mahesh Ravishankar",
      "Nicolas Vasilache",
      "Ben Vanik",
      "Stella Laurenzo"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.14479"
  },
  {
    "id": "arXiv:2205.14484",
    "title": "Happenstance: Utilizing Semantic Search to Track Russian State Media  Narratives about the Russo-Ukrainian War On Reddit",
    "abstract": "In the buildup to and in the weeks following the Russian Federation's\ninvasion of Ukraine, Russian disinformation outlets output torrents of\nmisleading and outright false information. In this work, we study the\ncoordinated information campaign to understand the most prominent\ndisinformation narratives touted by the Russian government to English-speaking\naudiences. To do this, we first perform sentence-level topic analysis using the\nlarge-language model MPNet on articles published by nine different Russian\ndisinformation websites and the new Russian \"fact-checking\" website\nwaronfakes.com. We show that smaller websites like katehon.com were highly\neffective at producing topics that were later echoed by other disinformation\nsites. After analyzing the set of Russian information narratives, we analyze\ntheir correspondence with narratives and topics of discussion on the r/Russia\nand 10 other political subreddits. Using MPNet and a semantic search algorithm,\nwe map these subreddits' comments to the set of topics extracted from our set\nof disinformation websites, finding that 39.6% of r/Russia comments\ncorresponded to narratives from Russian disinformation websites, compared to\n8.86% on r/politics.",
    "descriptor": "",
    "authors": [
      "Hans W. A. Hanley",
      "Deepak Kumar",
      "Zakir Durumeric"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14484"
  },
  {
    "id": "arXiv:2205.14492",
    "title": "A New High-Performance Approach to Approximate Pattern-Matching for  Plagiarism Detection in Blockchain-Based Non-Fungible Tokens (NFTs)",
    "abstract": "We are presenting a fast and innovative approach to performing approximate\npattern-matching for plagiarism detection, using an NDFA-based approach that\nsignificantly enhances performance compared to other existing similarity\nmeasures. We outline the advantages of our approach in the context of\nblockchain-based non-fungible tokens (NFTs). We present, formalize, discuss and\ntest our proposed approach in several real-world scenarios and with different\nsimilarity measures commonly used in plagiarism detection, and observe\nsignificant throughput enhancements throughout the entire spectrum of tests,\nwith little to no compromises on the accuracy of the detection process overall.\nWe conclude that our approach is suitable and adequate to perform approximate\npattern-matching for plagiarism detection, and outline research directions for\nfuture improvements.",
    "descriptor": "",
    "authors": [
      "Ciprian Pungila",
      "Darius Galis",
      "Viorel Negru"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14492"
  },
  {
    "id": "arXiv:2205.14495",
    "title": "Task-Agnostic Continual Reinforcement Learning: In Praise of a Simple  Baseline",
    "abstract": "We study task-agnostic continual reinforcement learning (TACRL) in which\nstandard RL challenges are compounded with partial observability stemming from\ntask agnosticism, as well as additional difficulties of continual learning\n(CL), i.e., learning on a non-stationary sequence of tasks. Here we compare\nTACRL methods with their soft upper bounds prescribed by previous literature:\nmulti-task learning (MTL) methods which do not have to deal with non-stationary\ndata distributions, as well as task-aware methods, which are allowed to operate\nunder full observability. We consider a previously unexplored and\nstraightforward baseline for TACRL, replay-based recurrent RL (3RL), in which\nwe augment an RL algorithm with recurrent mechanisms to address partial\nobservability and experience replay mechanisms to address catastrophic\nforgetting in CL.\nStudying empirical performance in a sequence of RL tasks, we find surprising\noccurrences of 3RL matching and overcoming the MTL and task-aware soft upper\nbounds. We lay out hypotheses that could explain this inflection point of\ncontinual and task-agnostic learning research. Our hypotheses are empirically\ntested in continuous control tasks via a large-scale study of the popular\nmulti-task and continual learning benchmark Meta-World. By analyzing different\ntraining statistics including gradient conflict, we find evidence that 3RL's\noutperformance stems from its ability to quickly infer how new tasks relate\nwith the previous ones, enabling forward transfer.",
    "descriptor": "",
    "authors": [
      "Massimo Caccia",
      "Jonas Mueller",
      "Taesup Kim",
      "Laurent Charlin",
      "Rasool Fakoor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14495"
  },
  {
    "id": "arXiv:2205.14496",
    "title": "SuperVoice: Text-Independent Speaker Verification Using Ultrasound  Energy in Human Speech",
    "abstract": "Voice-activated systems are integrated into a variety of desktop, mobile, and\nInternet-of-Things (IoT) devices. However, voice spoofing attacks, such as\nimpersonation and replay attacks, in which malicious attackers synthesize the\nvoice of a victim or simply replay it, have brought growing security concerns.\nExisting speaker verification techniques distinguish individual speakers via\nthe spectrographic features extracted from an audible frequency range of voice\ncommands. However, they often have high error rates and/or long delays. In this\npaper, we explore a new direction of human voice research by scrutinizing the\nunique characteristics of human speech at the ultrasound frequency band. Our\nresearch indicates that the high-frequency ultrasound components (e.g. speech\nfricatives) from 20 to 48 kHz can significantly enhance the security and\naccuracy of speaker verification. We propose a speaker verification system,\nSUPERVOICE that uses a two-stream DNN architecture with a feature fusion\nmechanism to generate distinctive speaker models. To test the system, we create\na speech dataset with 12 hours of audio (8,950 voice samples) from 127\nparticipants. In addition, we create a second spoofed voice dataset to evaluate\nits security. In order to balance between controlled recordings and real-world\napplications, the audio recordings are collected from two quiet rooms by 8\ndifferent recording devices, including 7 smartphones and an ultrasound\nmicrophone. Our evaluation shows that SUPERVOICE achieves 0.58% equal error\nrate in the speaker verification task, it only takes 120 ms for testing an\nincoming utterance, outperforming all existing speaker verification systems.\nMoreover, within 91 ms processing time, SUPERVOICE achieves 0% equal error rate\nin detecting replay attacks launched by 5 different loudspeakers.",
    "descriptor": "",
    "authors": [
      "Hanqing Guo",
      "Qiben Yan",
      "Nikolay Ivanov",
      "Ying Zhu",
      "Li Xiao",
      "Eric J. Hunter"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14496"
  },
  {
    "id": "arXiv:2205.14497",
    "title": "BadDet: Backdoor Attacks on Object Detection",
    "abstract": "Deep learning models have been deployed in numerous real-world applications\nsuch as autonomous driving and surveillance. However, these models are\nvulnerable in adversarial environments. Backdoor attack is emerging as a severe\nsecurity threat which injects a backdoor trigger into a small portion of\ntraining data such that the trained model behaves normally on benign inputs but\ngives incorrect predictions when the specific trigger appears. While most\nresearch in backdoor attacks focuses on image classification, backdoor attacks\non object detection have not been explored but are of equal importance. Object\ndetection has been adopted as an important module in various security-sensitive\napplications such as autonomous driving. Therefore, backdoor attacks on object\ndetection could pose severe threats to human lives and properties. We propose\nfour kinds of backdoor attacks for object detection task: 1) Object Generation\nAttack: a trigger can falsely generate an object of the target class; 2)\nRegional Misclassification Attack: a trigger can change the prediction of a\nsurrounding object to the target class; 3) Global Misclassification Attack: a\nsingle trigger can change the predictions of all objects in an image to the\ntarget class; and 4) Object Disappearance Attack: a trigger can make the\ndetector fail to detect the object of the target class. We develop appropriate\nmetrics to evaluate the four backdoor attacks on object detection. We perform\nexperiments using two typical object detection models -- Faster-RCNN and YOLOv3\non different datasets. More crucially, we demonstrate that even fine-tuning on\nanother benign dataset cannot remove the backdoor hidden in the object\ndetection model. To defend against these backdoor attacks, we propose Detector\nCleanse, an entropy-based run-time detection framework to identify poisoned\ntesting samples for any deployed object detector.",
    "descriptor": "",
    "authors": [
      "Shih-Han Chan",
      "Yinpeng Dong",
      "Jun Zhu",
      "Xiaolu Zhang",
      "Jun Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14497"
  },
  {
    "id": "arXiv:2205.14498",
    "title": "Towards a Security Stress-Test for Cloud Configurations",
    "abstract": "Securing cloud configurations is an elusive task, which is left up to system\nadministrators who have to base their decisions on ``trial and error''\nexperimentations or by observing good practices (e.g., CIS Benchmarks). We\npropose a knowledge, AND/OR, graphs approach to model cloud deployment security\nobjects and vulnerabilities. In this way, we can capture relationships between\nconfigurations, permissions (e.g., CAP\\_SYS\\_ADMIN), and security profiles\n(e.g., AppArmor and SecComp), as first-class citizens. Such an approach allows\nus to suggest alternative and safer configurations, support administrators in\nthe study of what-if scenarios, and scale the analysis to large scale\ndeployments. We present an initial validation and illustrate the approach with\nthree real vulnerabilities from known sources.",
    "descriptor": "\nComments: Conference: The IEEE International Conference on Cloud Computing (CLOUD) 2022\n",
    "authors": [
      "Francesco Minna",
      "Fabio Massacci",
      "Katja Tuma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14498"
  },
  {
    "id": "arXiv:2205.14500",
    "title": "Optimal Decision Diagrams for Classification",
    "abstract": "Decision diagrams for classification have some notable advantages over\ndecision trees, as their internal connections can be determined at training\ntime and their width is not bound to grow exponentially with their depth.\nAccordingly, decision diagrams are usually less prone to data fragmentation in\ninternal nodes. However, the inherent complexity of training these classifiers\nacted as a long-standing barrier to their widespread adoption. In this context,\nwe study the training of optimal decision diagrams (ODDs) from a mathematical\nprogramming perspective. We introduce a novel mixed-integer linear programming\nmodel for training and demonstrate its applicability for many datasets of\npractical importance. Further, we show how this model can be easily extended\nfor fairness, parsimony, and stability notions. We present numerical analyses\nshowing that our model allows training ODDs in short computational times, and\nthat ODDs achieve better accuracy than optimal decision trees, while allowing\nfor improved stability without significant accuracy losses.",
    "descriptor": "",
    "authors": [
      "Alexandre M. Florio",
      "Pedro Martins",
      "Maximilian Schiffer",
      "Thiago Serra",
      "Thibaut Vidal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14500"
  },
  {
    "id": "arXiv:2205.14503",
    "title": "Towards Distributed 2-Approximation Steiner Minimal Trees in  Billion-edge Graphs",
    "abstract": "Given an edge-weighted graph and a set of known seed vertices, a network\nscientist often desires to understand the graph relationships to explain\nconnections between the seed vertices. When the seed set is 3 or larger Steiner\nminimal tree - min-weight acyclic connected subgraph (of the input graph) that\ncontains all the seed vertices - is an attractive generalization of shortest\nweighted paths. In general, computing a Steiner minimal tree is NP-hard, but\nseveral polynomial-time algorithms have been designed and proven to yield\nSteiner trees whose total weight is bounded within 2 times the Steiner minimal\ntree. In this paper, we present a parallel 2-approximation Steiner minimal tree\nalgorithm and its MPI-based distributed implementation. In place of distance\ncomputation between all pairs of seed vertices, an expensive phase in many\nalgorithms, our solution exploits Voronoi cell computation. Also, this approach\nhas higher parallel efficiency than others that involve minimum spanning tree\ncomputation on the entire graph. Furthermore, our distributed design exploits\nasynchronous processing and a message prioritization scheme to accelerate\nconvergence of distance computation, and harnesses both vertex and edge centric\nprocessing to offer fast time-to-solution. We demonstrate scalability and\nperformance of our solution using real-world graphs with up to 128 billion\nedges and 512 compute nodes (8K processes). We compare our solution with the\nstate-of-the-art exact Steiner minimal tree solver, SCIP-Jack, and two serial\nalgorithms. Our solution comfortably outperforms these related works on graphs\nwith 10s million edges and offers decent strong scaling - up to 90% efficient.\nWe empirically show that, on average, the total distance of the Steiner tree\nidentified by our solution is 1.0527 times greater than the Steiner minimal\ntree - well within the theoretical bound of less than equal to 2.",
    "descriptor": "\nComments: The 36th IEEE International Parallel and Distributed Processing Symposium (IEEE IPDPS 2022)\n",
    "authors": [
      "Tahsin Reza",
      "Geoffrey Sanders",
      "Roger Pearce"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14503"
  },
  {
    "id": "arXiv:2205.14507",
    "title": "HPC Extensions to the OpenKIM Processing Pipeline",
    "abstract": "The Open Knowledgebase of Interatomic Models (OpenKIM) is an NSF Science\nGateway that archives fully functional computer implementations of interatomic\nmodels (potentials and force fields) and simulation codes that use them to\ncompute material properties. Interatomic models are coupled with compatible\nsimulation codes and executed in a fully automated manner by the OpenKIM\nprocessing pipeline, a cloud-based computation platform. The pipeline as\npreviously introduced in the literature was insufficient to support the\nlarge-scale computations that have become necessary within the materials\nscience community. Accordingly, we present extensions made to the pipeline that\nallow it to utilize High-Performance Computing (HPC) resources in an efficient\nand performant fashion.",
    "descriptor": "",
    "authors": [
      "Daniel S. Karls",
      "Steven M. Clark",
      "Brendon A. Waters",
      "Ryan S. Elliott",
      "Ellad B. Tadmor"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14507"
  },
  {
    "id": "arXiv:2205.14508",
    "title": "Core-set Selection Using Metrics-based Explanations (CSUME) for  multiclass ECG",
    "abstract": "The adoption of deep learning-based healthcare decision support systems such\nas the detection of irregular cardiac rhythm is hindered by challenges such as\nlack of access to quality data and the high costs associated with the\ncollection and annotation of data. The collection and processing of large\nvolumes of healthcare data is a continuous process. The performance of\ndata-hungry Deep Learning models (DL) is highly dependent on the quantity and\nquality of the data. While the need for data quantity has been established\nthrough research adequately, we show how a selection of good quality data\nimproves deep learning model performance. In this work, we take\nElectrocardiogram (ECG) data as a case study and propose a model performance\nimprovement methodology for algorithm developers, that selects the most\ninformative data samples from incoming streams of multi-class ECG data. Our\nCore-Set selection methodology uses metrics-based explanations to select the\nmost informative ECG data samples. This also provides an understanding (for\nalgorithm developers) as to why a sample was selected as more informative over\nothers for the improvement of deep learning model performance. Our experimental\nresults show a 9.67% and 8.69% precision and recall improvement with a\nsignificant training data volume reduction of 50%. Additionally, our proposed\nmethodology asserts the quality and annotation of ECG samples from incoming\ndata streams. It allows automatic detection of individual data samples that do\nnot contribute to model learning thus minimizing possible negative effects on\nmodel performance. We further discuss the potential generalizability of our\napproach by experimenting with a different dataset and deep learning\narchitecture.",
    "descriptor": "\nComments: Accepted IEEE ICHI Copyright owned by IEEE\n",
    "authors": [
      "Sagnik Dakshit",
      "Barbara Mukami Maweu",
      "Sristi Dakshit",
      "Balakrishnan Prabhakaran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14508"
  },
  {
    "id": "arXiv:2205.14519",
    "title": "History-Restricted Online Learning",
    "abstract": "We introduce the concept of history-restricted no-regret online learning\nalgorithms. An online learning algorithm $\\mathcal{A}$ is\n$M$-history-restricted if its output at time $t$ can be written as a function\nof the $M$ previous rewards. This class of online learning algorithms is quite\nnatural to consider from many perspectives: they may be better models of human\nagents and they do not store long-term information (thereby ensuring ``the\nright to be forgotten''). We first demonstrate that a natural approach to\nconstructing history-restricted algorithms from mean-based no-regret learning\nalgorithms (e.g. running Hedge over the last $M$ rounds) fails, and that such\nalgorithms incur linear regret. We then construct a history-restricted\nalgorithm that achieves a per-round regret of $\\Theta(1/\\sqrt{M})$, which we\ncomplement with a tight lower bound. Finally, we empirically explore\ndistributions where history-restricted online learners have favorable\nperformance compared to other no-regret algorithms.",
    "descriptor": "\nComments: 38 pages, 28 figures, in submission\n",
    "authors": [
      "Jon Schneider",
      "Kiran Vodrahalli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14519"
  },
  {
    "id": "arXiv:2205.14521",
    "title": "Learning Non-Autoregressive Models from Search for Unsupervised Sentence  Summarization",
    "abstract": "Text summarization aims to generate a short summary for an input text. In\nthis work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS)\napproach, which does not require parallel data for training. Our NAUS first\nperforms edit-based search towards a heuristically defined score, and generates\na summary as pseudo-groundtruth. Then, we train an encoder-only\nnon-autoregressive Transformer based on the search result. We also propose a\ndynamic programming approach for length-control decoding, which is important\nfor the summarization task. Experiments on two datasets show that NAUS achieves\nstate-of-the-art performance for unsupervised summarization, yet largely\nimproving inference efficiency. Further, our algorithm is able to perform\nexplicit length-transfer summary generation.",
    "descriptor": "",
    "authors": [
      "Puyuan Liu",
      "Chenyang Huang",
      "Lili Mou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14521"
  },
  {
    "id": "arXiv:2205.14522",
    "title": "A Character-Level Length-Control Algorithm for Non-Autoregressive  Sentence Summarization",
    "abstract": "Sentence summarization aims at compressing a long sentence into a short one\nthat keeps the main gist, and has extensive real-world applications such as\nheadline generation. In previous work, researchers have developed various\napproaches to improve the ROUGE score, which is the main evaluation metric for\nsummarization, whereas controlling the summary length has not drawn much\nattention. In our work, we address a new problem of explicit character-level\nlength control for summarization, and propose a dynamic programming algorithm\nbased on the Connectionist Temporal Classification (CTC) model. Results show\nthat our approach not only achieves higher ROUGE scores but also yields more\ncomplete sentences.",
    "descriptor": "",
    "authors": [
      "Puyuan Liu",
      "Xiang Zhang",
      "Lili Mou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14522"
  },
  {
    "id": "arXiv:2205.14523",
    "title": "Risk of Stochastic Systems for Temporal Logic Specifications",
    "abstract": "The wide availability of data coupled with the computational advances in\nartificial intelligence and machine learning promise to enable many future\ntechnologies such as autonomous driving. While there has been a variety of\nsuccessful demonstrations of these technologies, critical system failures have\nrepeatedly been reported. Even if rare, such system failures pose a serious\nbarrier to adoption without a rigorous risk assessment. This paper presents a\nframework for the systematic and rigorous risk verification of systems. We\nconsider a wide range of system specifications formulated in signal temporal\nlogic (STL) and model the system as a stochastic process, permitting\ndiscrete-time and continuous-time stochastic processes. We then define the STL\nrobustness risk as the risk of lacking robustness against failure. This\ndefinition is motivated as system failures are often caused by missing\nrobustness to modeling errors, system disturbances, and distribution shifts in\nthe underlying data generating process. Within the definition, we permit\ngeneral classes of risk measures and focus on tail risk measures such as the\nvalue-at-risk and the conditional value-at-risk. While the STL robustness risk\nis in general hard to compute, we propose the approximate STL robustness risk\nas a more tractable notion that upper bounds the STL robustness risk. We show\nhow the approximate STL robustness risk can accurately be estimated from system\ntrajectory data. For discrete-time stochastic processes, we show under which\nconditions the approximate STL robustness risk can even be computed exactly. We\nillustrate our verification algorithm in the autonomous driving simulator CARLA\nand show how a least risky controller can be selected among four neural network\nlane keeping controllers for five meaningful system specifications.",
    "descriptor": "",
    "authors": [
      "Lars Lindemann",
      "Lejun Jiang",
      "Nikolai Matni",
      "George J. Pappas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14523"
  },
  {
    "id": "arXiv:2205.14526",
    "title": "Group-wise Reinforcement Feature Generation for Optimal and Explainable  Representation Space Reconstruction",
    "abstract": "Representation (feature) space is an environment where data points are\nvectorized, distances are computed, patterns are characterized, and geometric\nstructures are embedded. Extracting a good representation space is critical to\naddress the curse of dimensionality, improve model generalization, overcome\ndata sparsity, and increase the availability of classic models. Existing\nliterature, such as feature engineering and representation learning, is limited\nin achieving full automation (e.g., over heavy reliance on intensive labor and\nempirical experiences), explainable explicitness (e.g., traceable\nreconstruction process and explainable new features), and flexible optimal\n(e.g., optimal feature space reconstruction is not embedded into downstream\ntasks). Can we simultaneously address the automation, explicitness, and optimal\nchallenges in representation space reconstruction for a machine learning task?\nTo answer this question, we propose a group-wise reinforcement generation\nperspective. We reformulate representation space reconstruction into an\ninteractive process of nested feature generation and selection, where feature\ngeneration is to generate new meaningful and explicit features, and feature\nselection is to eliminate redundant features to control feature sizes. We\ndevelop a cascading reinforcement learning method that leverages three\ncascading Markov Decision Processes to learn optimal generation policies to\nautomate the selection of features and operations and the feature crossing. We\ndesign a group-wise generation strategy to cross a feature group, an operation,\nand another feature group to generate new features and find the strategy that\ncan enhance exploration efficiency and augment reward signals of cascading\nagents. Finally, we present extensive experiments to demonstrate the\neffectiveness, efficiency, traceability, and explicitness of our system.",
    "descriptor": "\nComments: KDD 2022\n",
    "authors": [
      "Dongjie Wang",
      "Yanjie Fu",
      "Kunpeng Liu",
      "Xiaolin Li",
      "Yan Solihin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14526"
  },
  {
    "id": "arXiv:2205.14528",
    "title": "Measuring the Monetary Value of Online Volunteer Work",
    "abstract": "Online volunteers are a crucial labor force that keeps many for-profit\nsystems afloat (e.g. social media platforms and online review sites). Despite\ntheir substantial role in upholding highly valuable technological systems,\nonline volunteers have no way of knowing the value of their work. This paper\nuses content moderation as a case study and measures its monetary value to make\napparent volunteer labor's value. Using a novel dataset of private logs\ngenerated by moderators, we use linear mixed-effect regression and estimate\nthat Reddit moderators worked a minimum of 466 hours per day in 2020. These\nhours amount to 3.4 million USD a year based on the median hourly wage for\ncomparable content moderation services in the U.S. We discuss how this\ninformation may inform pathways to alleviate the one-sided relationship between\ntechnology companies and online volunteers.",
    "descriptor": "",
    "authors": [
      "Hanlin Li",
      "Brent Hecht",
      "Stevie Chancellor"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.14528"
  },
  {
    "id": "arXiv:2205.14529",
    "title": "All That's Happening behind the Scenes: Putting the Spotlight on  Volunteer Moderator Labor in Reddit",
    "abstract": "Online volunteers are an uncompensated yet valuable labor force for many\nsocial platforms. For example, volunteer content moderators perform a vast\namount of labor to maintain online communities. However, as social platforms\nlike Reddit favor revenue generation and user engagement, moderators are\nunder-supported to manage the expansion of online communities. To preserve\nthese online communities, developers and researchers of social platforms must\naccount for and support as much of this labor as possible. In this paper, we\nquantitatively characterize the publicly visible and invisible actions taken by\nmoderators on Reddit, using a unique dataset of private moderator logs for 126\nsubreddits and over 900 moderators. Our analysis of this dataset reveals the\nheterogeneity of moderation work across both communities and moderators.\nMoreover, we find that analyzing only visible work - the dominant way that\nmoderation work has been studied thus far - drastically underestimates the\namount of human moderation labor on a subreddit. We discuss the implications of\nour results on content moderation research and social platforms.",
    "descriptor": "",
    "authors": [
      "Hanlin Li",
      "Brent Hecht",
      "Stevie Chancellor"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.14529"
  },
  {
    "id": "arXiv:2205.14531",
    "title": "Fair Division Algorithms for Electricity Distribution",
    "abstract": "In many developing countries, the total electricity demand is larger than the\nlimited generation capacity of power stations. Many countries adopt the common\npractice of routine load shedding - disconnecting entire regions from the power\nsupply - to maintain a balance between demand and supply. Load shedding results\nin inflicting hardship and discomfort on households, which is even worse and\nhence unfair to those whose need for electricity is higher than that of others\nduring load shedding hours. Recently, Oluwasuji et al. [2020] presented this\nproblem and suggested several heuristic solutions. In this work, we study the\nelectricity distribution problem as a problem of fair division, model it using\nthe related literature on cake-cutting problems, and discuss some insights on\nwhich parts of the time intervals are allocated to each household. We consider\nfour cases: identical demand, uniform utilities; identical demand, additive\nutilities; different demand, uniform utilities; different demand, additive\nutilities. We provide the solution for the first two cases and discuss the\nnovel concept of q-times bin packing in relation to the remaining cases. We\nalso show how the fourth case is related to the consensus k-division problem.\nOne can study objectives and constraints using utilitarian and egalitarian\nsocial welfare metrics, as well as trying to keep the number of cuts as small\nas possible. A secondary objective can be to minimize the maximum\nutility-difference between agents.",
    "descriptor": "\nComments: 16 pages, 1 figure\n",
    "authors": [
      "Dinesh Kumar Baghel",
      "Vadim E. Levit",
      "Erel Segal-Halevi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.14531"
  },
  {
    "id": "arXiv:2205.14540",
    "title": "SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners",
    "abstract": "Self-supervised Masked Autoencoders (MAE) are emerging as a new pre-training\nparadigm in computer vision. MAE learns semantics implicitly via reconstructing\nlocal patches, requiring thousands of pre-training epochs to achieve favorable\nperformance. This paper incorporates explicit supervision, i.e., golden labels,\ninto the MAE framework. The proposed Supervised MAE (SupMAE) only exploits a\nvisible subset of image patches for classification, unlike the standard\nsupervised pre-training where all image patches are used. SupMAE is efficient\nand can achieve comparable performance with MAE using only 30% compute when\nevaluated on ImageNet with the ViT-B/16 model. Detailed ablation studies are\nconducted to verify the proposed components.",
    "descriptor": "\nComments: Technical report. Codes are available\n",
    "authors": [
      "Feng Liang",
      "Yangguang Li",
      "Diana Marculescu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14540"
  },
  {
    "id": "arXiv:2205.14543",
    "title": "Spatial Locality and Granularity Change in Caching",
    "abstract": "Caches exploit temporal and spatial locality to allow a small memory to\nprovide fast access to data stored in large, slow memory. The temporal aspect\nof locality is extremely well studied and understood, but the spatial aspect\nmuch less so. We seek to gain an increased understanding of spatial locality by\ndefining and studying the Granularity-Change Caching Problem. This problem\nmodifies the traditional caching setup by grouping data items into blocks, such\nthat a cache can choose any subset of a block to load for the same cost as\nloading any individual item in the block.\nWe show that modeling such spatial locality significantly changes the caching\nproblem. This begins with a proof that Granularity-Change Caching is\nNP-Complete in the offline setting, even when all items have unit size and all\nblocks have unit load cost. In the online setting, we show a lower bound for\ncompetitive ratios of deterministic policies that is significantly worse than\ntraditional caching. Moreover, we present a deterministic replacement policy\ncalled Item-Block Layered Partitioning and show that it obtains a competitive\nratio close to that lower bound. Moreover, our bounds reveal a new issue\narising in the Granularity-Change Caching Problem where the choice of offline\ncache size affects the competitiveness of different online algorithms relative\nto one another. To deal with this issue, we extend a prior (temporal) locality\nmodel to account for spatial locality, and provide a general lower bound in\naddition to an upper bound for Item-Block Layered Partitioning.",
    "descriptor": "\nComments: 13 pages (including references), 6 figures, and 2 tables\n",
    "authors": [
      "Nathan Beckmann",
      "Phillip B Gibbons",
      "Charles McGuffey"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14543"
  },
  {
    "id": "arXiv:2205.14545",
    "title": "Functional Linear Regression of CDFs",
    "abstract": "The estimation of cumulative distribution functions (CDF) is an important\nlearning task with a great variety of downstream applications, e.g., risk\nassessments in predictions and decision making. We study functional regression\nof contextual CDFs where each data point is sampled from a linear combination\nof context dependent CDF bases. We propose estimation methods that estimate\nCDFs accurately everywhere. In particular, given $n$ samples with $d$ bases, we\nshow estimation error upper bounds of $\\widetilde O(\\sqrt{d/n})$ for fixed\ndesign, random design, and adversarial context cases. We also derive matching\ninformation theoretic lower bounds, establishing minimax optimality for CDF\nfunctional regression. To complete our study, we consider agnostic settings\nwhere there is a mismatch in the data generation process. We characterize the\nerror of the proposed estimator in terms of the mismatched error, and show that\nthe estimator is well-behaved under model mismatch.",
    "descriptor": "\nComments: 34 pages, 1 figure\n",
    "authors": [
      "Qian Zhang",
      "Anuran Makur",
      "Kamyar Azizzadenesheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.14545"
  },
  {
    "id": "arXiv:2205.14546",
    "title": "The Missing Invariance Principle Found -- the Reciprocal Twin of  Invariant Risk Minimization",
    "abstract": "Machine learning models often generalize poorly to out-of-distribution (OOD)\ndata as a result of relying on features that are spuriously correlated with the\nlabel during training. Recently, the technique of Invariant Risk Minimization\n(IRM) was proposed to learn predictors that only use invariant features by\nconserving the feature-conditioned class expectation $\\mathbb{E}_e[y|f(x)]$\nacross environments. However, more recent studies have demonstrated that IRM\ncan fail in various task settings. Here, we identify a fundamental flaw of IRM\nformulation that causes the failure. We then introduce a complementary notion\nof invariance, MRI, that is based on conserving the class-conditioned feature\nexpectation $\\mathbb{E}_e[f(x)|y]$ across environments, that corrects for the\nflaw in IRM. Further, we introduce a simplified, practical version of the MRI\nformulation called as MRI-v1. We note that this constraint is convex which\nconfers it with an advantage over the practical version of IRM, IRM-v1, which\nimposes non-convex constraints. We prove that in a general linear problem\nsetting, MRI-v1 can guarantee invariant predictors given sufficient\nenvironments. We also empirically demonstrate that MRI strongly out-performs\nIRM and consistently achieves near-optimal OOD generalization in image-based\nnonlinear problems.",
    "descriptor": "",
    "authors": [
      "Dongsung Huh",
      "Avinash Baidya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14546"
  },
  {
    "id": "arXiv:2205.14548",
    "title": "Image Super-resolution with An Enhanced Group Convolutional Neural  Network",
    "abstract": "CNNs with strong learning abilities are widely chosen to resolve\nsuper-resolution problem. However, CNNs depend on deeper network architectures\nto improve performance of image super-resolution, which may increase\ncomputational cost in general. In this paper, we present an enhanced\nsuper-resolution group CNN (ESRGCNN) with a shallow architecture by fully\nfusing deep and wide channel features to extract more accurate low-frequency\ninformation in terms of correlations of different channels in single image\nsuper-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is\nuseful to inherit more long-distance contextual information for resolving\nlong-term dependency. An adaptive up-sampling operation is gathered into a CNN\nto obtain an image super-resolution model with low-resolution images of\ndifferent sizes. Extensive experiments report that our ESRGCNN surpasses the\nstate-of-the-arts in terms of SISR performance, complexity, execution speed,\nimage quality evaluation and visual effect in SISR. Code is found at\nhttps://github.com/hellloxiaotian/ESRGCNN.",
    "descriptor": "",
    "authors": [
      "Chunwei Tian",
      "Yixuan Yuan",
      "Shichao Zhang",
      "Chia-Wen Lin",
      "Wangmeng Zuo",
      "David Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.14548"
  },
  {
    "id": "arXiv:2205.14549",
    "title": "Asymmetric Local Information Privacy and the Watchdog Mechanism",
    "abstract": "This paper proposes a novel watchdog privatization scheme by generalizing\nlocal information privacy (LIP) to enhance data utility. To protect the\nsensitive features $S$ correlated with some useful data $X$, LIP restricts the\nlift, the ratio of the posterior belief to the prior on $S$ after and before\naccessing $X$. For each $x$, both maximum and minimum lift over sensitive\nfeatures are measures of the privacy risk of publishing this symbol and should\nbe restricted for the privacy-preserving purpose. Previous works enforce the\nsame bound for both max-lift and min-lift. However, empirical observations show\nthat the min-lift is usually much smaller than the max-lift. In this work, we\ngeneralize the LIP definition to consider the unequal values of max and min\nlift, i.e., considering different bounds for max-lift and min-lift. This new\ndefinition is applied to the watchdog privacy mechanism. We demonstrate that\nthe utility is enhanced under a given privacy constraint on local differential\nprivacy. At the same time, the resulting max-lift is lower and, therefore,\ntightly restricts other privacy leakages, e.g., mutual information, maximal\nleakage, and $\\alpha$-leakage.",
    "descriptor": "",
    "authors": [
      "Mohammad Amin Zarrabian",
      "Ni Ding",
      "Parastoo Sadeghi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14549"
  },
  {
    "id": "arXiv:2205.14550",
    "title": "Machine Learning for Microcontroller-Class Hardware -- A Review",
    "abstract": "The advancements in machine learning opened a new opportunity to bring\nintelligence to the low-end Internet-of-Things nodes such as microcontrollers.\nConventional machine learning deployment has high memory and compute footprint\nhindering their direct deployment on ultra resource-constrained microcontroller\nnodes. This paper highlights the unique challenges of enabling onboard machine\nlearning for microcontroller class devices. Recently, researchers have used a\nspecialized model development cycle for resource-limited applications to ensure\nthe compute and latency budget is within the limits while still maintaining the\ndesired accuracy. We introduce a closed-loop widely applicable workflow of\nmachine learning model development for microcontroller class devices and show\nthat several classes of applications adopt a specific instance of it. We\npresent both qualitative and numerical insights into different stages of model\ndevelopment by showcasing several applications. Finally, we identify the open\nresearch challenges and unsolved questions demanding careful considerations\nmoving forward.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Swapnil Sayan Saha",
      "Sandeep Singh Sandha",
      "Mani Srivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14550"
  },
  {
    "id": "arXiv:2205.14553",
    "title": "A Model of One-Shot Generalization",
    "abstract": "We provide a theoretical framework to study a phenomenon that we call\none-shot generalization. This phenomenon refers to the ability of an algorithm\nto perform transfer learning within a single task, meaning that it correctly\nclassifies a test point that has a single exemplar in the training set. We\npropose a simple data model and use it to study this phenomenon in two ways.\nFirst, we prove a non-asymptotic base-line -- kernel methods based on\nnearest-neighbor classification cannot perform one-shot generalization,\nindependently of the choice of the kernel and the size of the training set.\nSecond, we empirically show that the most direct neural network architecture\nfor our data model performs one-shot generalization almost perfectly. This\nstark differential leads us to believe that the one-shot generalization\nmechanism is partially responsible for the empirical success of neural\nnetworks.",
    "descriptor": "",
    "authors": [
      "Thomas Laurent",
      "James H. von Brecht",
      "Xavier Bresson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14553"
  },
  {
    "id": "arXiv:2205.14555",
    "title": "Two New Piggybacking Designs with Lower Repair Bandwidth",
    "abstract": "Piggybacking codes are a special class of MDS array codes that can achieve\nsmall repair bandwidth with small sub-packetization by first creating some\ninstances of an $(n,k)$ MDS code, such as a Reed-Solomon (RS) code, and then\ndesigning the piggyback function. In this paper, we propose a new piggybacking\ncoding design which designs the piggyback function over some instances of both\n$(n,k)$ MDS code and $(n,k')$ MDS code, when $k\\geq k'$. We show that our new\npiggybacking design can significantly reduce the repair bandwidth for\nsingle-node failures. When $k=k'$, we design piggybacking code that is MDS code\nand we show that the designed code has lower repair bandwidth for single-node\nfailures than all existing piggybacking codes when the number of parity node\n$r=n-k\\geq8$ and the sub-packetization $\\alpha<r$.\nMoreover, we propose another piggybacking codes by designing $n$ piggyback\nfunctions of some instances of $(n,k)$ MDS code and adding the $n$ piggyback\nfunctions into the $n$ newly created empty entries with no data symbols. We\nshow that our code can significantly reduce repair bandwidth for single-node\nfailures at a cost of slightly more storage overhead. In addition, we show that\nour code can recover any $r+1$ node failures for some parameters. We also show\nthat our code has lower repair bandwidth than locally repairable codes (LRCs)\nunder the same fault-tolerance and redundancy for some parameters.",
    "descriptor": "",
    "authors": [
      "Zhengyi Jiang",
      "Hanxu Hou",
      "Yunghsiang S. Han",
      "Patrick P. C. Lee",
      "Bo Bai",
      "Zhongyi Huang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14555"
  },
  {
    "id": "arXiv:2205.14557",
    "title": "Representation Gap in Deep Reinforcement Learning",
    "abstract": "Deep reinforcement learning gives the promise that an agent learns good\npolicy from high-dimensional information. Whereas representation learning\nremoves irrelevant and redundant information and retains pertinent information.\nWe consider the representation capacity of action value function and\ntheoretically reveal its inherent property, \\textit{representation gap} with\nits target action value function. This representation gap is favorable.\nHowever, through illustrative experiments, we show that the representation of\naction value function grows similarly compared with its target value function,\ni.e. the undesirable inactivity of the representation gap\n(\\textit{representation overlap}). Representation overlap results in a loss of\nrepresentation capacity, which further leads to sub-optimal learning\nperformance. To activate the representation gap, we propose a simple but\neffective framework \\underline{P}olicy \\underline{O}ptimization from\n\\underline{P}reventing \\underline{R}epresentation \\underline{O}verlaps (POPRO),\nwhich regularizes the policy evaluation phase through differing the\nrepresentation of action value function from its target. We also provide the\nconvergence rate guarantee of POPRO. We evaluate POPRO on gym continuous\ncontrol suites. The empirical results show that POPRO using pixel inputs\noutperforms or parallels the sample-efficiency of methods that use state-based\nfeatures.",
    "descriptor": "\nComments: 24 pages, 6 figures\n",
    "authors": [
      "Qiang He",
      "Huangyuan Su",
      "Jieyu Zhang",
      "Xinwen Hou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14557"
  },
  {
    "id": "arXiv:2205.14558",
    "title": "Exploiting Partial FDD Reciprocity for Beam Based Pilot Precoding and  CSI Feedback in Deep Learning",
    "abstract": "Massive MIMO systems can achieve high spectrum and energy efficiency in\ndownlink (DL) based on accurate estimate of channel state information (CSI).\nExisting works have developed learning-based DL CSI estimation that lowers\nuplink feedback overhead. One often overlooked problem is the limited number of\nDL pilots available for CSI estimation. One proposed solution leverages\ntemporal CSI coherence by utilizing past CSI estimates and only sending\nCSI-reference symbols (CSI-RS) for partial arrays to preserve CSI recovery\nperformance. Exploiting CSI correlations, FDD channel reciprocity is helpful to\nbase stations with direct access to uplink CSI. In this work, we propose a new\nlearning-based feedback architecture and a reconfigurable CSI-RS placement\nscheme to reduce DL CSI training overhead and to improve encoding efficiency of\nCSI feedback. Our results demonstrate superior performance in both indoor and\noutdoor scenarios by the proposed framework for CSI recovery at substantial\nreduction of computation power and storage requirements at UEs.",
    "descriptor": "",
    "authors": [
      "Yu-Chien Lin",
      "Ta-Sung Lee",
      "Zhi Ding"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14558"
  },
  {
    "id": "arXiv:2205.14560",
    "title": "A well-balanced moving mesh discontinuous Galerkin method for the Ripa  model on triangular meshes",
    "abstract": "A well-balanced moving mesh discontinuous Galerkin (DG) method is proposed\nfor the numerical solution of the Ripa model -- a generalization of the shallow\nwater equations that accounts for effects of water temperature variations.\nThermodynamic processes are important particularly in the upper layers of the\nocean where the variations of sea surface temperature play a fundamental role\nin climate change. The well-balance property which requires numerical schemes\nto preserve the lake-at-rest steady state is crucial to the simulation of\nperturbation waves over that steady state such as waves on a lake or tsunami\nwaves in the deep ocean. To ensure the well-balance, positivity-preserving, and\nhigh-order properties, a DG-interpolation scheme (with or without scaling\npositivity-preserving limiter) and special treatments pertaining to the Ripa\nmodel are employed in the transfer of both the flow variables and bottom\ntopography from the old mesh to the new one and in the TVB limiting process.\nMesh adaptivity is realized using an MMPDE moving mesh approach and a metric\ntensor based on an equilibrium variable and water depth. A motivation is to\nadapt the mesh according to both the perturbations of the lake-at-rest steady\nstate and the water depth distribution (bottom structure). Numerical examples\nin one and two dimensions are presented to demonstrate the well-balance,\nhigh-order accuracy, and positivity-preserving properties of the method and its\nability to capture small perturbations of the lake-at-rest steady state.",
    "descriptor": "\nComments: 37 pages, 24 figures. arXiv admin note: substantial text overlap with arXiv:2006.15187\n",
    "authors": [
      "Weizhang Huang",
      "Ruo Li",
      "Jianxian Qiu",
      "Min Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14560"
  },
  {
    "id": "arXiv:2205.14566",
    "title": "ProxyMix: Proxy-based Mixup Training with Label Refinery for Source-Free  Domain Adaptation",
    "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Owing to privacy concerns\nand heavy data transmission, source-free UDA, exploiting the pre-trained source\nmodels instead of the raw source data for target learning, has been gaining\npopularity in recent years. Some works attempt to recover unseen source domains\nwith generative models, however introducing additional network parameters.\nOther works propose to fine-tune the source model by pseudo labels, while noisy\npseudo labels may misguide the decision boundary, leading to unsatisfied\nresults. To tackle these issues, we propose an effective method named\nProxy-based Mixup training with label refinery (ProxyMix). First of all, to\navoid additional parameters and explore the information in the source model,\nProxyMix defines the weights of the classifier as the class prototypes and then\nconstructs a class-balanced proxy source domain by the nearest neighbors of the\nprototypes to bridge the unseen source domain and the target domain. To improve\nthe reliability of pseudo labels, we further propose the frequency-weighted\naggregation strategy to generate soft pseudo labels for unlabeled target data.\nThe proposed strategy exploits the internal structure of target features, pulls\ntarget features to their semantic neighbors, and increases the weights of\nlow-frequency classes samples during gradient updating. With the proxy domain\nand the reliable pseudo labels, we employ two kinds of mixup regularization,\ni.e., inter- and intra-domain mixup, in our framework, to align the proxy and\nthe target domain, enforcing the consistency of predictions, thereby further\nmitigating the negative impacts of noisy labels. Experiments on three 2D image\nand one 3D point cloud object recognition benchmarks demonstrate that ProxyMix\nyields state-of-the-art performance for source-free UDA tasks.",
    "descriptor": "",
    "authors": [
      "Yuhe Ding",
      "Lijun Sheng",
      "Jian Liang",
      "Aihua Zheng",
      "Ran He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14566"
  },
  {
    "id": "arXiv:2205.14567",
    "title": "Input-to-State Safety with Input Delay in Longitudinal Vehicle Control",
    "abstract": "Safe longitudinal control is discussed for a connected automated truck\ntraveling behind a preceding connected vehicle. A controller is proposed based\non control barrier function theory and predictor feedback for provably safe,\ncollision-free behavior by taking into account the significant response time of\nthe truck as input delay and the uncertainty of its dynamical model as input\ndisturbance. The benefits of the proposed controller compared to control\ndesigns that neglect the delay or treat the delay as disturbance are shown by\nnumerical simulations.",
    "descriptor": "\nComments: Submitted to the 17th IFAC Workshop on Time Delay Systems. 6 pages, 3 figures\n",
    "authors": [
      "Tamas G. Molnar",
      "Anil Alan",
      "Adam K. Kiss",
      "Aaron D. Ames",
      "Gabor Orosz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14567"
  },
  {
    "id": "arXiv:2205.14570",
    "title": "AutoDisc: Automatic Distillation Schedule for Large Language Model  Compression",
    "abstract": "Driven by the teacher-student paradigm, knowledge distillation is one of the\nde facto ways for language model compression. Recent studies have uncovered\nthat conventional distillation is less effective when facing a large capacity\ngap between the teacher and the student, and introduced teacher assistant-based\ndistillation to bridge the gap. As a connection, the scale and the performance\nof the teacher assistant is crucial for transferring the knowledge from the\nteacher to the student. However, existing teacher assistant-based methods\nmanually select the scale of the teacher assistant, which fails to identify the\nteacher assistant with the optimal scale-performance tradeoff. To this end, we\npropose an Automatic Distillation Schedule (AutoDisc) for large language model\ncompression. In particular, AutoDisc first specifies a set of teacher assistant\ncandidates at different scales with gridding and pruning, and then optimizes\nall candidates in an once-for-all optimization with two approximations. The\nbest teacher assistant scale is automatically selected according to the\nscale-performance tradeoff. AutoDisc is evaluated with an extensive set of\nexperiments on a language understanding benchmark GLUE. Experimental results\ndemonstrate the improved performance and applicability of our AutoDisc. We\nfurther apply AutoDisc on a language model with over one billion parameters and\nshow the scalability of AutoDisc.",
    "descriptor": "\nComments: Work in progress. Code will be available soon\n",
    "authors": [
      "Chen Zhang",
      "Yang Yang",
      "Qifan Wang",
      "Jiahao Liu",
      "Jingang Wang",
      "Wei Wu",
      "Dawei Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14570"
  },
  {
    "id": "arXiv:2205.14571",
    "title": "Provable Benefits of Representational Transfer in Reinforcement Learning",
    "abstract": "We study the problem of representational transfer in RL, where an agent first\npretrains in a number of source tasks to discover a shared representation,\nwhich is subsequently used to learn a good policy in a target task. We propose\na new notion of task relatedness between source and target tasks, and develop a\nnovel approach for representational transfer under this assumption. Concretely,\nwe show that given generative access to source tasks, we can discover a\nrepresentation, using which subsequent linear RL techniques quickly converge to\na near-optimal policy, with only online access to the target task.\nThe sample complexity is close to knowing the ground truth features in the\ntarget task, and comparable to prior representation learning results in the\nsource tasks. We complement our positive results with lower bounds without\ngenerative access, and validate our findings with empirical evaluation on rich\nobservation MDPs that require deep exploration.",
    "descriptor": "",
    "authors": [
      "Alekh Agarwal",
      "Yuda Song",
      "Wen Sun",
      "Kaiwen Wang",
      "Mengdi Wang",
      "Xuezhou Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14571"
  },
  {
    "id": "arXiv:2205.14572",
    "title": "No-regret Learning in Repeated First-Price Auctions with Budget  Constraints",
    "abstract": "Recently the online advertising market has exhibited a gradual shift from\nsecond-price auctions to first-price auctions. Although there has been a line\nof works concerning online bidding strategies in first-price auctions, it still\nremains open how to handle budget constraints in the problem. In the present\npaper, we initiate the study for a buyer with budgets to learn online bidding\nstrategies in repeated first-price auctions. We propose an RL-based bidding\nalgorithm against the optimal non-anticipating strategy under stationary\ncompetition. Our algorithm obtains $\\widetilde O(\\sqrt T)$-regret if the bids\nare all revealed at the end of each round. With the restriction that the buyer\nonly sees the winning bid after each round, our modified algorithm obtains\n$\\widetilde O(T^{\\frac{7}{12}})$-regret by techniques developed from survival\nanalysis. Our analysis extends to the more general scenario where the buyer has\nany bounded instantaneous utility function with regrets of the same order.",
    "descriptor": "\nComments: 23 pages, 1 figure\n",
    "authors": [
      "Rui Ai",
      "Chang Wang",
      "Chenchen Li",
      "Jinshan Zhang",
      "Wenhan Huang",
      "Xiaotie Deng"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14572"
  },
  {
    "id": "arXiv:2205.14573",
    "title": "ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation",
    "abstract": "We view the reconstruction of CAD models in the boundary representation\n(B-Rep) as the detection of geometric primitives of different orders, i.e.\nvertices, edges and surface patches, and the correspondence of primitives,\nwhich are holistically modeled as a chain complex, and show that by modeling\nsuch comprehensive structures more complete and regularized reconstructions can\nbe achieved. We solve the complex generation problem in two steps. First, we\npropose a novel neural framework that consists of a sparse CNN encoder for\ninput point cloud processing and a tri-path transformer decoder for generating\ngeometric primitives and their mutual relationships with estimated\nprobabilities. Second, given the probabilistic structure predicted by the\nneural network, we recover a definite B-Rep chain complex by solving a global\noptimization maximizing the likelihood under structural validness constraints\nand applying geometric refinements. Extensive tests on large scale CAD datasets\ndemonstrate that the modeling of B-Rep chain complex structure enables more\naccurate detection for learning and more constrained reconstruction for\noptimization, leading to structurally more faithful and complete CAD B-Rep\nmodels than previous results.",
    "descriptor": "\nComments: This article is published by ACM Trans. Graph. (SIGGRAPH 2022). This is the author's preprint version\n",
    "authors": [
      "Haoxiang Guo",
      "Shilin Liu",
      "Hao Pan",
      "Yang Liu",
      "Xin Tong",
      "Baining Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.14573"
  },
  {
    "id": "arXiv:2205.14574",
    "title": "Feature-Aligned Video Raindrop Removal with Temporal Constraints",
    "abstract": "Existing adherent raindrop removal methods focus on the detection of the\nraindrop locations, and then use inpainting techniques or generative networks\nto recover the background behind raindrops. Yet, as adherent raindrops are\ndiverse in sizes and appearances, the detection is challenging for both single\nimage and video. Moreover, unlike rain streaks, adherent raindrops tend to\ncover the same area in several frames. Addressing these problems, our method\nemploys a two-stage video-based raindrop removal method. The first stage is the\nsingle image module, which generates initial clean results. The second stage is\nthe multiple frame module, which further refines the initial results using\ntemporal constraints, namely, by utilizing multiple input frames in our process\nand applying temporal consistency between adjacent output frames. Our single\nimage module employs a raindrop removal network to generate initial raindrop\nremoval results, and create a mask representing the differences between the\ninput and initial output. Once the masks and initial results for consecutive\nframes are obtained, our multiple-frame module aligns the frames in both the\nimage and feature levels and then obtains the clean background. Our method\ninitially employs optical flow to align the frames, and then utilizes\ndeformable convolution layers further to achieve feature-level frame alignment.\nTo remove small raindrops and recover correct backgrounds, a target frame is\npredicted from adjacent frames. A series of unsupervised losses are proposed so\nthat our second stage, which is the video raindrop removal module, can\nself-learn from video data without ground truths. Experimental results on real\nvideos demonstrate the state-of-art performance of our method both\nquantitatively and qualitatively.",
    "descriptor": "",
    "authors": [
      "Wending Yan",
      "Lu Xu",
      "Wenhan Yang",
      "Robby T. Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14574"
  },
  {
    "id": "arXiv:2205.14575",
    "title": "3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction",
    "abstract": "Recently, the transformer model has been successfully employed for the\nmulti-view 3D reconstruction problem. However, challenges remain on designing\nan attention mechanism to explore the multiview features and exploit their\nrelations for reinforcing the encoding-decoding modules. This paper proposes a\nnew model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a\nnovel coarse-to-fine(C2F) attention mechanism for encoding multi-view features\nand rectifying defective 3D objects. C2F attention mechanism enables the model\nto learn multi-view information flow and synthesize 3D surface correction in a\ncoarse to fine-grained manner. The proposed model is evaluated by ShapeNet and\nMulti-view Real-life datasets. Experimental results show that 3D-C2FT achieves\nnotable results and outperforms several competing models on these datasets.",
    "descriptor": "",
    "authors": [
      "Leslie Ching Ow Tiong",
      "Dick Sigmund",
      "Andrew Beng Jin Teoh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14575"
  },
  {
    "id": "arXiv:2205.14576",
    "title": "Problem-Space Evasion Attacks in the Android OS: a Survey",
    "abstract": "Android is the most popular OS worldwide. Therefore, it is a target for\nvarious kinds of malware. As a countermeasure, the security community works day\nand night to develop appropriate Android malware detection systems, with\nML-based or DL-based systems considered as some of the most common types.\nAgainst these detection systems, intelligent adversaries develop a wide set of\nevasion attacks, in which an attacker slightly modifies a malware sample to\nevade its target detection system. In this survey, we address problem-space\nevasion attacks in the Android OS, where attackers manipulate actual APKs,\nrather than their extracted feature vector. We aim to explore this kind of\nattacks, frequently overlooked by the research community due to a lack of\nknowledge of the Android domain, or due to focusing on general mathematical\nevasion attacks - i.e., feature-space evasion attacks. We discuss the different\naspects of problem-space evasion attacks, using a new taxonomy, which focuses\non key ingredients of each problem-space attack, such as the attacker model,\nthe attacker's mode of operation, and the functional assessment of post-attack\napplications.",
    "descriptor": "",
    "authors": [
      "Harel Berger",
      "Dr. Chen Hajaj",
      "Dr. Amit Dvir"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14576"
  },
  {
    "id": "arXiv:2205.14579",
    "title": "Leg Shaping and Event-Driven Control of a Small-Scale, Low-DoF, Two-Mode  Robot",
    "abstract": "Among small-scale mobile robots, multi-modal locomotion can help compensate\nfor limited actuator capabilities. However, supporting multiple locomotion\nmodes or gaits in small terrestrial robots typically requires complex designs\nwith low locomotion efficiency. In this work, legged and rolling gaits are\nachieved by a 10~cm robot having just two degrees of freedom (DoF). This is\nacheived by leg shaping that facilitates whole body rolling and event-driven\ncontrol that maintains motion using simple inertial sensor measurements. Speeds\nof approximately 0.4 and 2.2 body lengths per second are achieved in legged and\nrolling modes, respectively, with low cost of transport. The proposed design\napproach and control techniques may aid in design of further miniaturized\nrobots reliant on transducers with small range-of-motion.",
    "descriptor": "",
    "authors": [
      "Dingkun Guo",
      "Larissa Wermers",
      "Kenn R. Oldham"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14579"
  },
  {
    "id": "arXiv:2205.14582",
    "title": "Mean square stability conditions for platoons with lossy inter-vehicle  communication channels",
    "abstract": "This paper studies the mean-square stability of heterogeneous LTI vehicular\nplatoons with inter-vehicle communication channels affected by random data\nloss. We consider a discrete-time platoon system with predecessor following\ntopology and constant time-headway spacing policy. Lossy channels are modeled\nby Bernoulli processes allowed to be correlated in space. We make use of a\nclass of compensation strategies to reduce the effect of data loss. Necessary\nand sufficient conditions are derived to guarantee the convergence of the mean\nand variance of the tracking errors, which depend not only on the controller\ndesign but also on the compensation strategy and the probabilities of\nsuccessful transmission. Through numerical simulations, we illustrate the\ntheoretical results, describing different platoon behaviors. We also provide\ninsights on the mean-square stability as a necessary condition for string\nstability in this stochastic setting.",
    "descriptor": "",
    "authors": [
      "Marco A. Gordon",
      "Francisco J. Vargas",
      "Andr\u00e9s A. Peters"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14582"
  },
  {
    "id": "arXiv:2205.14583",
    "title": "Learning Locality and Isotropy in Dialogue Modeling",
    "abstract": "Existing dialogue modeling methods have achieved promising performance on\nvarious dialogue tasks with the aid of Transformer and the large-scale\npre-trained language models. However, some recent studies revealed that the\ncontext representations produced by these methods suffer the problem of\nanisotropy. In this paper, we find that the generated representations are also\nnot conversational, losing the conversation structure information during the\ncontext modeling stage. To this end, we identify two properties in dialogue\nmodeling, i.e., locality and isotropy, and present a simple method for dialogue\nrepresentation calibration, namely SimDRC, to build isotropic and\nconversational feature spaces. Experimental results show that our approach\nsignificantly outperforms the current state-of-the-art models on three dialogue\ntasks across the automatic and human evaluation metrics. More in-depth analyses\nfurther confirm the effectiveness of our proposed approach.",
    "descriptor": "\nComments: 18 pages, 4 figures\n",
    "authors": [
      "Han Wu",
      "Haochen Tan",
      "Mingjie Zhan",
      "Gangming Zhao",
      "Shaoqing Lu",
      "Ding Liang",
      "Linqi Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14583"
  },
  {
    "id": "arXiv:2205.14584",
    "title": "Making Real Memristive Processing-in-Memory Faster and Reliable",
    "abstract": "Memristive technologies are attractive candidates to replace conventional\nmemory technologies, and can also be used to perform logic and arithmetic\noperations using a technique called 'stateful logic.' Combining data storage\nand computation in the memory array enables a novel non-von Neumann\narchitecture, where both the operations are performed within a memristive\nMemory Processing Unit (mMPU). The mMPU relies on adding computing capabilities\nto the memristive memory cells without changing the basic memory array\nstructure. The use of an mMPU alleviates the primary restriction on performance\nand energy in a von Neumann machine, which is the data transfer between CPU and\nmemory. Here, the various aspects of mMPU are discussed, including its\narchitecture and implications on the computing system and software, as well as\nexamining the microarchitectural aspects. We show how mMPU can be improved to\naccelerate different applications and how the poor reliability of memristors\ncan be improved as part of the mMPU operation.",
    "descriptor": "",
    "authors": [
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.14584"
  },
  {
    "id": "arXiv:2205.14585",
    "title": "Towards an unsupervised large-scale 2D and 3D building mapping with  LiDAR",
    "abstract": "A 2D and 3D building map provides invaluable information for understanding\nhuman activities and their impacts on Earth and its environment. Despite\nenormous efforts to improve the quality of building maps, current large-scale\nbuilding maps have lots of errors and are limited to providing only 2D building\ninformation. This study presents a state-of-the-art 2D and 3D building\nextraction algorithm with airborne LiDAR data that is suitable for large-scale\nbuilding mapping. Our algorithm operates in a fully unsupervised manner and\ndoes not require either any training label or training procedure. Our algorithm\nrequires only simple operations of morphological filtering and planarity-based\nfiltering but can produce an accurate 2D and 3D building map. A quantitative\nand qualitative evaluation in a large-scale dataset (-550 sqkm) of Denver and\nNew York City showed that our algorithm outperforms the deep learning-based\nMicrosoft's building mapping algorithm even without any parameter tuning. More\nextensive evaluations in different conditions of landscapes confirmed that our\nalgorithm is scalable and can be improved further with appropriate parameter\nselection. Our algorithm is more advantageous than other image-based building\nextraction algorithms in that it is more computationally efficient, more\naccurate, and more explainable. Our proposed algorithm that can produce an\naccurate large-scale 2D and 3D building map provides a great potential towards\na global-scale 2D and 3D building mapping with airborne LiDAR data.",
    "descriptor": "",
    "authors": [
      "Hunsoo Song",
      "Jinha Jung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14585"
  },
  {
    "id": "arXiv:2205.14586",
    "title": "Formal Methods for Characterization and Analysis of Quality  Specifications in Component-based Systems",
    "abstract": "Component-based design paradigm is of paramount importance due to prolific\ngrowth in the complexity of modern-day systems. Since the components are\ndeveloped primarily by multi-party vendors and often assembled to realize the\noverall system, it is an onus of the designer to certify both the functional\nand non-functional requirements of such systems. Several of the earlier works\nconcentrated on formally analyzing the behavioral correctness, safety,\nsecurity, reliability and robustness of such compositional systems. However,\nthe assurance for quality measures of such systems is also considered as an\nimportant parameter for their acceptance. Formalization of quality measures is\nstill at an immature state and often dictated by the user satisfaction. This\npaper presents a novel compositional framework for reliable quality analysis of\ncomponent-based systems from the formal quality specifications of its\nconstituent components. The proposed framework enables elegant and generic\ncomputation methods for quality attributes of various component-based system\nstructures. In addition to this, we provide a formal query-driven quality\nassessment and design exploration framework which enables the designer to\nexplore various component structures and operating setups and finally converge\ninto better acceptable systems. A detailed case-study is presented over a\ncomponent-based system structure to show the efficacy and practicality of our\nproposed framework.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Aritra Hazra"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14586"
  },
  {
    "id": "arXiv:2205.14589",
    "title": "Masked Distillation with Receptive Tokens",
    "abstract": "Distilling from the feature maps can be fairly effective for dense prediction\ntasks since both the feature discriminability and localization priors can be\nwell transferred. However, not every pixel contributes equally to the\nperformance, and a good student should learn from what really matters to the\nteacher. In this paper, we introduce a learnable embedding dubbed receptive\ntoken to localize those pixels of interests (PoIs) in the feature map, with a\ndistillation mask generated via pixel-wise attention. Then the distillation\nwill be performed on the mask via pixel-wise reconstruction. In this way, a\ndistillation mask actually indicates a pattern of pixel dependencies within\nfeature maps of teacher. We thus adopt multiple receptive tokens to investigate\nmore sophisticated and informative pixel dependencies to further enhance the\ndistillation. To obtain a group of masks, the receptive tokens are learned via\nthe regular task loss but with teacher fixed, and we also leverage a Dice loss\nto enrich the diversity of learned masks. Our method dubbed MasKD is simple and\npractical, and needs no priors of tasks in application. Experiments show that\nour MasKD can achieve state-of-the-art performance consistently on object\ndetection and semantic segmentation benchmarks. Code is available at:\nhttps://github.com/hunto/MasKD .",
    "descriptor": "",
    "authors": [
      "Tao Huang",
      "Yuan Zhang",
      "Shan You",
      "Fei Wang",
      "Chen Qian",
      "Jian Cao",
      "Chang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14589"
  },
  {
    "id": "arXiv:2205.14590",
    "title": "Independent and Decentralized Learning in Markov Potential Games",
    "abstract": "We propose a multi-agent reinforcement learning dynamics, and analyze its\nconvergence properties in infinite-horizon discounted Markov potential games.\nWe focus on the independent and decentralized setting, where players can only\nobserve the realized state and their own reward in every stage. Players do not\nhave knowledge of the game model, and cannot coordinate with each other. In\neach stage of our learning dynamics, players update their estimate of a\nperturbed Q-function that evaluates their total contingent payoff based on the\nrealized one-stage reward in an asynchronous manner. Then, players\nindependently update their policies by incorporating a smoothed optimal\none-stage deviation strategy based on the estimated Q-function. A key feature\nof the learning dynamics is that the Q-function estimates are updated at a\nfaster timescale than the policies. We prove that the policies induced by our\nlearning dynamics converge to a stationary Nash equilibrium in Markov potential\ngames with probability 1. Our results build on the theory of two timescale\nasynchronous stochastic approximation, and new analysis on the monotonicity of\npotential function along the trajectory of policy updates in Markov potential\ngames.",
    "descriptor": "\nComments: 42 pages, 3 figures\n",
    "authors": [
      "Chinmay Maheshwari",
      "Manxi Wu",
      "Druv Pai",
      "Shankar Sastry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14590"
  },
  {
    "id": "arXiv:2205.14591",
    "title": "Joint Abductive and Inductive Neural Logical Reasoning",
    "abstract": "Neural logical reasoning (NLR) is a fundamental task in knowledge discovery\nand artificial intelligence. NLR aims at answering multi-hop queries with\nlogical operations on structured knowledge bases based on distributed\nrepresentations of queries and answers. While previous neural logical reasoners\ncan give specific entity-level answers, i.e., perform inductive reasoning from\nthe perspective of logic theory, they are not able to provide descriptive\nconcept-level answers, i.e., perform abductive reasoning, where each concept is\na summary of a set of entities. In particular, the abductive reasoning task\nattempts to infer the explanations of each query with descriptive concepts,\nwhich make answers comprehensible to users and is of great usefulness in the\nfield of applied ontology. In this work, we formulate the problem of the joint\nabductive and inductive neural logical reasoning (AI-NLR), solving which needs\nto address challenges in incorporating, representing, and operating on\nconcepts. We propose an original solution named ABIN for AI-NLR. Firstly, we\nincorporate description logic-based ontological axioms to provide the source of\nconcepts. Then, we represent concepts and queries as fuzzy sets, i.e., sets\nwhose elements have degrees of membership, to bridge concepts and queries with\nentities. Moreover, we design operators involving concepts on top of the fuzzy\nset representation of concepts and queries for optimization and inference.\nExtensive experimental results on two real-world datasets demonstrate the\neffectiveness of ABIN for AI-NLR.",
    "descriptor": "",
    "authors": [
      "Zhenwei Tang",
      "Shichao Pei",
      "Xi Peng",
      "Fuzhen Zhuang",
      "Xiangliang Zhang",
      "Robert Hoehndorf"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14591"
  },
  {
    "id": "arXiv:2205.14592",
    "title": "An adaptive granularity clustering method based on hyper-ball",
    "abstract": "The purpose of cluster analysis is to classify elements according to their\nsimilarity. Its applications range from astronomy to bioinformatics and pattern\nrecognition. Our method is based on the idea that the data with similar\ndistribution form a hyper-ball and the adjacent hyper-balls form a cluster.\nBased on the cognitive law of \"large scale first\", this method can identify\nclusters without considering shape in a simple and non-parametric way.\nExperimental results on several datasets demonstrate the effectiveness of the\nalgorithm.",
    "descriptor": "\nComments: 5 pages, 1 figures\n",
    "authors": [
      "Shu-yin Xia",
      "Jiang Xie",
      "Guo-yin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14592"
  },
  {
    "id": "arXiv:2205.14593",
    "title": "Dynamic Graph Learning Based on Hierarchical Memory for  Origin-Destination Demand Prediction",
    "abstract": "Recent years have witnessed a rapid growth of applying deep spatiotemporal\nmethods in traffic forecasting. However, the prediction of origin-destination\n(OD) demands is still a challenging problem since the number of OD pairs is\nusually quadratic to the number of stations. In this case, most of the existing\nspatiotemporal methods fail to handle spatial relations on such a large scale.\nTo address this problem, this paper provides a dynamic graph representation\nlearning framework for OD demands prediction. In particular, a hierarchical\nmemory updater is first proposed to maintain a time-aware representation for\neach node, and the representations are updated according to the most recently\nobserved OD trips in continuous-time and multiple discrete-time ways. Second, a\nspatiotemporal propagation mechanism is provided to aggregate representations\nof neighbor nodes along a random spatiotemporal route which treats origin and\ndestination as two different semantic entities. Last, an objective function is\ndesigned to derive the future OD demands according to the most recent node\nrepresentations, and also to tackle the data sparsity problem in OD prediction.\nExtensive experiments have been conducted on two real-world datasets, and the\nexperimental results demonstrate the superiority of the proposed method. The\ncode and data are available at https://github.com/Rising0321/HMOD.",
    "descriptor": "",
    "authors": [
      "Ruixing Zhang",
      "Liangzhe Han",
      "Boyi Liu",
      "Jiayuan Zeng",
      "Leilei Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14593"
  },
  {
    "id": "arXiv:2205.14594",
    "title": "BiasEnsemble: Revisiting the Importance of Amplifying Bias for Debiasing",
    "abstract": "In image classification, \"debiasing\" aims to train a classifier to be less\nsusceptible to dataset bias, the strong correlation between peripheral\nattributes of data samples and a target class. For example, even if the frog\nclass in the dataset mainly consists of frog images with a swamp background\n(i.e., bias-aligned samples), a debiased classifier should be able to correctly\nclassify a frog at a beach (i.e., bias-conflicting samples). Recent debiasing\napproaches commonly use two components for debiasing, a biased model $f_B$ and\na debiased model $f_D$. $f_B$ is trained to focus on bias-aligned samples while\n$f_D$ is mainly trained with bias-conflicting samples by concentrating on\nsamples which $f_B$ fails to learn, leading $f_D$ to be less susceptible to the\ndataset bias. While the state-of-the-art debiasing techniques have aimed to\nbetter train $f_D$, we focus on training $f_B$, an overlooked component until\nnow. Our empirical analysis reveals that removing the bias-conflicting samples\nfrom the training set for $f_B$ is important for improving the debiasing\nperformance of $f_D$. This is due to the fact that the bias-conflicting samples\nwork as noisy samples for amplifying the bias for $f_B$. To this end, we\npropose a novel biased sample selection method BiasEnsemble which removes the\nbias-conflicting samples via leveraging additional biased models to construct a\nbias-amplified dataset for training $f_B$. Our simple yet effective approach\ncan be directly applied to existing reweighting-based debiasing approaches,\nobtaining consistent performance boost and achieving the state-of-the-art\nperformance on both synthetic and real-world datasets.",
    "descriptor": "",
    "authors": [
      "Jungsoo Lee",
      "Jeonghoon Park",
      "Daeyoung Kim",
      "Juyoung Lee",
      "Edward Choi",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14594"
  },
  {
    "id": "arXiv:2205.14601",
    "title": "YASM (Yet Another Surveillance Mechanism)",
    "abstract": "Client-Side Scanning (CSS) see in the Child Sexual Abuse Material Detection\n(CSAMD) represent ubiquitous mass scanning. Apple proposed to scan their\nsystems for such imagery. CSAMD was since pushed back, but the European Union\ndecided to propose forced CSS to combat and prevent child sexual abuse and\nweaken encryption. CSS is mass surveillance of personal property, pictures and\ntext, without considerations of privacy and cybersecurity and the law. We first\nargue why CSS should be limited or not used and discuss issues with the way\npictures cryptographically are handled and how the CSAMD preserves privacy. In\nthe second part, we analyse the possible human rights violations which CSS in\ngeneral can cause within the regime of the European Convention on Human Rights.\nThe focus is the harm which the system may cause to individuals, and we also\ncomment on the proposed Child Abuse Regulation. We find that CSS is problematic\nbecause they can rarely fulfil their purposes, as seen with antivirus software.\nThe costs for attempting to solve issues such as CSAM outweigh the benefits and\nis not likely to change. The CSAMD as proposed is not likely to preserve the\nprivacy or security in the way of which it is described source materials. We\nalso find that CSS in general would likely violate the Right to a Fair Trial,\nRight to Privacy and Freedom of Expression. Pictures could have been obtained\nin a way that could make any trial against a legitimate perpetrator\ninadmissible or violate their right for a fair trial, the lack of any\nsafeguards to protect privacy on national legal level, which would violate the\nRight for Privacy, and it is unclear if the kind of scanning could pass the\nlegal test which Freedom of Expression requires. Finally, we find significant\nissues with the proposed Regulation, as it relies on techno-solutionist\narguments and disregards knowledge on cybersecurity.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Kaspar Rosager Ludvigsen",
      "Shishir Nagaraja",
      "Angela Daly"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14601"
  },
  {
    "id": "arXiv:2205.14606",
    "title": "A General Multiple Data Augmentation Based Framework for Training Deep  Neural Networks",
    "abstract": "Deep neural networks (DNNs) often rely on massive labelled data for training,\nwhich is inaccessible in many applications. Data augmentation (DA) tackles data\nscarcity by creating new labelled data from available ones. Different DA\nmethods have different mechanisms and therefore using their generated labelled\ndata for DNN training may help improving DNN's generalisation to different\ndegrees. Combining multiple DA methods, namely multi-DA, for DNN training,\nprovides a way to boost generalisation. Among existing multi-DA based DNN\ntraining methods, those relying on knowledge distillation (KD) have received\ngreat attention. They leverage knowledge transfer to utilise the labelled data\nsets created by multiple DA methods instead of directly combining them for\ntraining DNNs. However, existing KD-based methods can only utilise certain\ntypes of DA methods, incapable of utilising the advantages of arbitrary DA\nmethods. We propose a general multi-DA based DNN training framework capable to\nuse arbitrary DA methods. To train a DNN, our framework replicates a certain\nportion in the latter part of the DNN into multiple copies, leading to multiple\nDNNs with shared blocks in their former parts and independent blocks in their\nlatter parts. Each of these DNNs is associated with a unique DA and a newly\ndevised loss that allows comprehensively learning from the data generated by\nall DA methods and the outputs from all DNNs in an online and adaptive way. The\noverall loss, i.e., the sum of each DNN's loss, is used for training the DNN.\nEventually, one of the DNNs with the best validation performance is chosen for\ninference. We implement the proposed framework by using three distinct DA\nmethods and apply it for training representative DNNs. Experiments on the\npopular benchmarks of image classification demonstrate the superiority of our\nmethod to several existing single-DA and multi-DA based training methods.",
    "descriptor": "\nComments: accepted by the 2022 IEEE International Joint Conference on Neural Networks (IJCNN 2022)\n",
    "authors": [
      "Binyan Hu",
      "Yu Sun",
      "A. K. Qin"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14606"
  },
  {
    "id": "arXiv:2205.14611",
    "title": "Forensic Artefact Discovery and Attribution from Android Cryptocurrency  Wallet Applications",
    "abstract": "Cryptocurrency has been (ab)used to purchase illicit goods and services such\nas drugs, weapons and child pornography (also referred to as child sexual abuse\nmaterials), and thus mobile devices (where cryptocurrency wallet applications\nare installed) are a potential source of evidence in a criminal investigation.\nNot surprisingly, there has been increased focus on the security of\ncryptocurrency wallets, although forensic extraction and attribution of\nforensic artefacts from such wallets is understudied. In this paper, we examine\nBitcoin and Dogecoin. The latter is increasingly popular partly due to\nendorsements from celebrities and being positioned as an introductory path to\ncryptocurrency for newcomers. Specifically, we demonstrate how one can acquire\nforensic artefacts from Android Bitcoin and Dogecoin cryptocurrency wallets,\nsuch as wallet IDs, transaction IDs, timestamp information, email addresses,\ncookies, and OAuth tokens.",
    "descriptor": "",
    "authors": [
      "Eugene Chang",
      "Paul Darcy",
      "Kim-Kwang Raymond Choo",
      "Nhien-An Le-Khac"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14611"
  },
  {
    "id": "arXiv:2205.14612",
    "title": "Do Residual Neural Networks discretize Neural Ordinary Differential  Equations?",
    "abstract": "Neural Ordinary Differential Equations (Neural ODEs) are the continuous\nanalog of Residual Neural Networks (ResNets). We investigate whether the\ndiscrete dynamics defined by a ResNet are close to the continuous one of a\nNeural ODE. We first quantify the distance between the ResNet's hidden state\ntrajectory and the solution of its corresponding Neural ODE. Our bound is tight\nand, on the negative side, does not go to 0 with depth N if the residual\nfunctions are not smooth with depth. On the positive side, we show that this\nsmoothness is preserved by gradient descent for a ResNet with linear residual\nfunctions and small enough initial loss. It ensures an implicit regularization\ntowards a limit Neural ODE at rate 1 over N, uniformly with depth and\noptimization time. As a byproduct of our analysis, we consider the use of a\nmemory-free discrete adjoint method to train a ResNet by recovering the\nactivations on the fly through a backward pass of the network, and show that\nthis method theoretically succeeds at large depth if the residual functions are\nLipschitz with the input. We then show that Heun's method, a second order ODE\nintegration scheme, allows for better gradient estimation with the adjoint\nmethod when the residual functions are smooth with depth. We experimentally\nvalidate that our adjoint method succeeds at large depth, and that Heun method\nneeds fewer layers to succeed. We finally use the adjoint method successfully\nfor fine-tuning very deep ResNets without memory consumption in the residual\nlayers.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Michael E. Sander",
      "Pierre Ablin",
      "Gabriel Peyr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14612"
  },
  {
    "id": "arXiv:2205.14619",
    "title": "Graph Structure Based Data Augmentation Method",
    "abstract": "In this paper, we propose a novel graph-based data augmentation method that\ncan generally be applied to medical waveform data with graph structures. In the\nprocess of recording medical waveform data, such as electrocardiogram (ECG) or\nelectroencephalogram (EEG), angular perturbations between the measurement leads\nexist due to discrepancies in lead positions. The data samples with large\nangular perturbations often cause inaccuracy in algorithmic prediction tasks.\nWe design a graph-based data augmentation technique that exploits the inherent\ngraph structures within the medical waveform data to improve both performance\nand robustness. In addition, we show that the performance gain from graph\naugmentation results from robustness by testing against adversarial attacks.\nSince the bases of performance gain are orthogonal, the graph augmentation can\nbe used in conjunction with existing data augmentation techniques to further\nimprove the final performance. We believe that our graph augmentation method\nopens up new possibilities to explore in data augmentation.",
    "descriptor": "",
    "authors": [
      "Kyung Geun Kim",
      "Byeong Tak Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14619"
  },
  {
    "id": "arXiv:2205.14620",
    "title": "IFRNet: Intermediate Feature Refine Network for Efficient Frame  Interpolation",
    "abstract": "Prevailing video frame interpolation algorithms, that generate the\nintermediate frames from consecutive inputs, typically rely on complex model\narchitectures with heavy parameters or large delay, hindering them from diverse\nreal-time applications. In this work, we devise an efficient encoder-decoder\nbased network, termed IFRNet, for fast intermediate frame synthesizing. It\nfirst extracts pyramid features from given inputs, and then refines the\nbilateral intermediate flow fields together with a powerful intermediate\nfeature until generating the desired output. The gradually refined intermediate\nfeature can not only facilitate intermediate flow estimation, but also\ncompensate for contextual details, making IFRNet do not need additional\nsynthesis or refinement module. To fully release its potential, we further\npropose a novel task-oriented optical flow distillation loss to focus on\nlearning the useful teacher knowledge towards frame synthesizing. Meanwhile, a\nnew geometry consistency regularization term is imposed on the gradually\nrefined intermediate features to keep better structure layout. Experiments on\nvarious benchmarks demonstrate the excellent performance and fast inference\nspeed of proposed approaches. Code is available at\nhttps://github.com/ltkong218/IFRNet.",
    "descriptor": "\nComments: Accepted by CVPR 2022\n",
    "authors": [
      "Lingtong Kong",
      "Boyuan Jiang",
      "Donghao Luo",
      "Wenqing Chu",
      "Xiaoming Huang",
      "Ying Tai",
      "Chengjie Wang",
      "Jie Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14620"
  },
  {
    "id": "arXiv:2205.14623",
    "title": "SKFlow: Learning Optical Flow with Super Kernels",
    "abstract": "Optical flow estimation is a classical yet challenging task in computer\nvision. One of the essential factors in accurately predicting optical flow is\nto alleviate occlusions between frames. However, it is still a thorny problem\nfor current top-performing optical flow estimation methods due to insufficient\nlocal evidence to model occluded areas. In this paper, we propose Super Kernel\nFlow Network (SKFlow), a CNN architecture to ameliorate the impacts of\nocclusions on optical flow estimation. SKFlow benefits from the super kernels\nwhich bring enlarged receptive fields to complement the absent matching\ninformation and recover the occluded motions. We present efficient super kernel\ndesigns by utilizing conical connections and hybrid depth-wise convolutions.\nExtensive experiments demonstrate the effectiveness of SKFlow on multiple\nbenchmarks, especially in the occluded areas. Without pre-trained backbones on\nImageNet and with modest increase in computation, SKFlow achieves compelling\nperformance and ranks $\\textbf{1st}$ among current published methods on Sintel\nbenchmark. On the challenging Sintel final pass test set, SKFlow attains the\naverage end-point error of $2.23$, which surpasses the best published result\n$2.47$ by $9.72\\%$.",
    "descriptor": "",
    "authors": [
      "Shangkun Sun",
      "Yuanqi Chen",
      "Yu Zhu",
      "Guodong Guo",
      "Ge Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14623"
  },
  {
    "id": "arXiv:2205.14625",
    "title": "Cervical Glandular Cell Detection from Whole Slide Image with  Out-Of-Distribution Data",
    "abstract": "Cervical glandular cell (GC) detection is a key step in computer-aided\ndiagnosis for cervical adenocarcinomas screening. It is challenging to\naccurately recognize GCs in cervical smears in which squamous cells are the\nmajor. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads\ndecreasing reliability of machine learning system for GC detection. Although,\nthe State-Of-The-Art (SOTA) deep learning model can outperform pathologists in\npreselected regions of interest, the mass False Positive (FP) prediction with\nhigh probability is still unsolved when facing such gigapixel whole slide\nimage. This paper proposed a novel PolarNet based on the morphological prior\nknowledge of GC trying to solve the FP problem via a self-attention mechanism\nin eight-neighbor. It estimates the polar orientation of nucleus of GC. As a\nplugin module, PolarNet can guide the deep feature and predicted confidence of\ngeneral object detection models. In experiments, we discovered that general\nmodels based on four different frameworks can reject FP in small image set and\nincrease the mean of average precision (mAP) by $\\text{0.007}\\sim\\text{0.015}$\nin average, where the highest exceeds the recent cervical cell detection model\n0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\\% on\naccuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of\ncomputational time. Code is available in\n\\href{https://github.com/Chrisa142857/PolarNet-GCdet}{https://github.com/Chrisa142857/PolarNet-GCdet}.",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Ziquan Wei",
      "Shenghua Cheng",
      "Xiuli Liu",
      "Shaoqun Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14625"
  },
  {
    "id": "arXiv:2205.14629",
    "title": "Superclass Adversarial Attack",
    "abstract": "Adversarial attacks have only focused on changing the predictions of the\nclassifier, but their danger greatly depends on how the class is mistaken. For\nexample, when an automatic driving system mistakes a Persian cat for a Siamese\ncat, it is hardly a problem. However, if it mistakes a cat for a 120km/h\nminimum speed sign, serious problems can arise. As a stepping stone to more\nthreatening adversarial attacks, we consider the superclass adversarial attack,\nwhich causes misclassification of not only fine classes, but also superclasses.\nWe conducted the first comprehensive analysis of superclass adversarial attacks\n(an existing and 19 new methods) in terms of accuracy, speed, and stability,\nand identified several strategies to achieve better performance. Although this\nstudy is aimed at superclass misclassification, the findings can be applied to\nother problem settings involving multiple classes, such as top-k and\nmulti-label classification attacks.",
    "descriptor": "",
    "authors": [
      "Soichiro Kumano",
      "Hiroshi Kera",
      "Toshihiko Yamasaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14629"
  },
  {
    "id": "arXiv:2205.14630",
    "title": "Physical Activation Functions (PAFs): An Approach for More Efficient  Induction of Physics into Physics-Informed Neural Networks (PINNs)",
    "abstract": "In recent years, the gap between Deep Learning (DL) methods and analytical or\nnumerical approaches in scientific computing is tried to be filled by the\nevolution of Physics-Informed Neural Networks (PINNs). However, still, there\nare many complications in the training of PINNs and optimal interleaving of\nphysical models. Here, we introduced the concept of Physical Activation\nFunctions (PAFs). This concept offers that instead of using general activation\nfunctions (AFs) such as ReLU, tanh, and sigmoid for all the neurons, one can\nuse generic AFs that their mathematical expression is inherited from the\nphysical laws of the investigating phenomena. The formula of PAFs may be\ninspired by the terms in the analytical solution of the problem. We showed that\nthe PAFs can be inspired by any mathematical formula related to the\ninvestigating phenomena such as the initial or boundary conditions of the PDE\nsystem. We validated the advantages of PAFs for several PDEs including the\nharmonic oscillations, Burgers, Advection-Convection equation, and the\nheterogeneous diffusion equations. The main advantage of PAFs was in the more\nefficient constraining and interleaving of PINNs with the investigating\nphysical phenomena and their underlying mathematical models. This added\nconstraint significantly improved the predictions of PINNs for the testing data\nthat was out-of-training distribution. Furthermore, the application of PAFs\nreduced the size of the PINNs up to 75% in different cases. Also, the value of\nloss terms was reduced by 1 to 2 orders of magnitude in some cases which is\nnoteworthy for upgrading the training of the PINNs. The iterations required for\nfinding the optimum values were also significantly reduced. It is concluded\nthat using the PAFs helps in generating PINNs with less complexity and much\nmore validity for longer ranges of prediction.",
    "descriptor": "\nComments: 26 pages, 9 figures\n",
    "authors": [
      "Jassem Abbasi",
      "P\u00e5l \u00d8steb\u00f8 Andersen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14630"
  },
  {
    "id": "arXiv:2205.14631",
    "title": "Anchor Prediction: A Topic Modeling Approach",
    "abstract": "Networks of documents connected by hyperlinks, such as Wikipedia, are\nubiquitous. Hyperlinks are inserted by the authors to enrich the text and\nfacilitate the navigation through the network. However, authors tend to insert\nonly a fraction of the relevant hyperlinks, mainly because this is a time\nconsuming task. In this paper we address an annotation, which we refer to as\nanchor prediction. Even though it is conceptually close to link prediction or\nentity linking, it is a different task that require developing a specific\nmethod to solve it. Given a source document and a target document, this task\nconsists in automatically identifying anchors in the source document, i.e words\nor terms that should carry a hyperlink pointing towards the target document. We\npropose a contextualized relational topic model, CRTM, that models directed\nlinks between documents as a function of the local context of the anchor in the\nsource document and the whole content of the target document. The model can be\nused to predict anchors in a source document, given the target document,\nwithout relying on a dictionary of previously seen mention or title, nor any\nexternal knowledge graph. Authors can benefit from CRTM, by letting it\nautomatically suggest hyperlinks, given a new document and the set of target\ndocument to connect to. It can also benefit to readers, by dynamically\ninserting hyperlinks between the documents they're reading. Experiments\nconducted on several Wikipedia corpora (in English, Italian and German)\nhighlight the practical usefulness of anchor prediction and demonstrate the\nrelevancy of our approach.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Jean Dupuy",
      "Adrien Guille",
      "Julien Jacques"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14631"
  },
  {
    "id": "arXiv:2205.14637",
    "title": "Perceiving the Invisible: Proposal-Free Amodal Panoptic Segmentation",
    "abstract": "Amodal panoptic segmentation aims to connect the perception of the world to\nits cognitive understanding. It entails simultaneously predicting the semantic\nlabels of visible scene regions and the entire shape of traffic participant\ninstances, including regions that may be occluded. In this work, we formulate a\nproposal-free framework that tackles this task as a multi-label and multi-class\nproblem by first assigning the amodal masks to different layers according to\ntheir relative occlusion order and then employing amodal instance regression on\neach layer independently while learning background semantics. We propose the\n\\net architecture that incorporates a shared backbone and an asymmetrical\ndual-decoder consisting of several modules to facilitate within-scale and\ncross-scale feature aggregations, bilateral feature propagation between\ndecoders, and integration of global instance-level and local pixel-level\nocclusion reasoning. Further, we propose the amodal mask refiner that resolves\nthe ambiguity in complex occlusion scenarios by explicitly leveraging the\nembedding of unoccluded instance masks. Extensive evaluation on the BDD100K-APS\nand KITTI-360-APS datasets demonstrate that our approach set the new\nstate-of-the-art on both benchmarks.",
    "descriptor": "",
    "authors": [
      "Rohit Mohan",
      "Abhinav Valada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14637"
  },
  {
    "id": "arXiv:2205.14641",
    "title": "LV-Linker: Supporting Linked Exploration of Phone Usage Log Data and  Screen Video Data",
    "abstract": "Prior HCI studies often analyzed smartphone app usage data for usability and\nuser experience research purposes. App usage videos are often collected by a\nscreen recording app in order to better analyze the app usage behaviors (e.g.,\napp usage time, screen transition, and notification handling). However, it is\ndifficult to analyze app usage videos along with multiple user interaction\nstream data. When the length of a video is long, data analysis tends to take a\nlong time due to the volume of user interaction data. This is even more\ndifficult for novice researchers due to a lack of data analysis experience. In\nthis paper, we propose LV-Linker (Log and Video Linker), a visualization tool\nthat helps researchers quickly explore the app usage log and video data by\nlinking multiple time series log data with the video data. We conducted a\npreliminary user study with eight participants to evaluate the benefits of\nlinking, by measuring task completion time, helpfulness, and subjective task\nworkload. Our results showed that offering a linking feature significantly\nlowers the task completion time and task workload.",
    "descriptor": "\nComments: 9 pages, 4 figures, will be published in ACM CHI 2024 after revision\n",
    "authors": [
      "Hansoo Lee",
      "Sangwook Lee",
      "Youngji Koh",
      "Uichin Lee"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.14641"
  },
  {
    "id": "arXiv:2205.14643",
    "title": "Micro-Expression Recognition Based on Attribute Information Embedding  and Cross-modal Contrastive Learning",
    "abstract": "Facial micro-expressions recognition has attracted much attention recently.\nMicro-expressions have the characteristics of short duration and low intensity,\nand it is difficult to train a high-performance classifier with the limited\nnumber of existing micro-expressions. Therefore, recognizing micro-expressions\nis a challenge task. In this paper, we propose a micro-expression recognition\nmethod based on attribute information embedding and cross-modal contrastive\nlearning. We use 3D CNN to extract RGB features and FLOW features of\nmicro-expression sequences and fuse them, and use BERT network to extract text\ninformation in Facial Action Coding System. Through cross-modal contrastive\nloss, we embed attribute information in the visual network, thereby improving\nthe representation ability of micro-expression recognition in the case of\nlimited samples. We conduct extensive experiments in CASME II and MMEW\ndatabases, and the accuracy is 77.82% and 71.04%, respectively. The comparative\nexperiments show that this method has better recognition effect than other\nmethods for micro-expression recognition.",
    "descriptor": "\nComments: This paper has been accepted by IJCNN2022\n",
    "authors": [
      "Yanxin Song",
      "Jianzong Wang",
      "Tianbo Wu",
      "Zhangcheng Huang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14643"
  },
  {
    "id": "arXiv:2205.14647",
    "title": "Methodologies, Workloads, and Tools for Processing-in-Memory: Enabling  the Adoption of Data-Centric Architectures",
    "abstract": "The increasing prevalence and growing size of data in modern applications\nhave led to high costs for computation in traditional processor-centric\ncomputing systems. Moving large volumes of data between memory devices (e.g.,\nDRAM) and computing elements (e.g., CPUs, GPUs) across bandwidth-limited memory\nchannels can consume more than 60% of the total energy in modern systems. To\nmitigate these costs, the processing-in-memory (PIM) paradigm moves computation\ncloser to where the data resides, reducing (and in some cases eliminating) the\nneed to move data between memory and the processor. There are two main\napproaches to PIM: (1) processing-near-memory (PnM), where PIM logic is added\nto the same die as memory or to the logic layer of 3D-stacked memory; and (2)\nprocessing-using-memory (PuM), which uses the operational principles of memory\ncells to perform computation. Many works from academia and industry have shown\nthe benefits of PnM and PuM for a wide range of workloads from different\ndomains. However, fully adopting PIM in commercial systems is still very\nchallenging due to the lack of tools and system support for PIM architectures\nacross the computer architecture stack, which includes: (i) workload\ncharacterization methodologies and benchmark suites targeting PIM\narchitectures; (ii) frameworks that can facilitate the implementation of\ncomplex operations and algorithms using the underlying PIM primitives; (iii)\ncompiler support and compiler optimizations targeting PIM architectures; (iv)\noperating system support for PIM-aware virtual memory, memory management, data\nallocation, and data mapping; and (v) efficient data coherence and consistency\nmechanisms. Our goal in this work is to provide tools and system support for\nPnM and PuM architectures, aiming to ease the adoption of PIM in current and\nfuture systems.",
    "descriptor": "",
    "authors": [
      "Geraldo F. Oliveira",
      "Juan G\u00f3mez-Luna",
      "Saugata Ghose",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.14647"
  },
  {
    "id": "arXiv:2205.14649",
    "title": "Speaker Identification using Speech Recognition",
    "abstract": "The audio data is increasing day by day throughout the globe with the\nincrease of telephonic conversations, video conferences and voice messages.\nThis research provides a mechanism for identifying a speaker in an audio file,\nbased on the human voice biometric features like pitch, amplitude, frequency\netc. We proposed an unsupervised learning model where the model can learn\nspeech representation with limited dataset. Librispeech dataset was used in\nthis research and we were able to achieve word error rate of 1.8.",
    "descriptor": "\nComments: 3 pages\n",
    "authors": [
      "Syeda Rabia Arshad",
      "Syed Mujtaba Haider",
      "Abdul Basit Mughal"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14649"
  },
  {
    "id": "arXiv:2205.14651",
    "title": "Contributions to Representation Learning with Graph Autoencoders and  Applications to Music Recommendation",
    "abstract": "Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as\ntwo powerful groups of unsupervised node embedding methods, with various\napplications to graph-based machine learning problems such as link prediction\nand community detection. Nonetheless, at the beginning of this Ph.D. project,\nGAE and VGAE models were also suffering from key limitations, preventing them\nfrom being adopted in the industry. In this thesis, we present several\ncontributions to improve these models, with the general aim of facilitating\ntheir use to address industrial-level problems involving graph representations.\nFirstly, we propose two strategies to overcome the scalability issues of\nprevious GAE and VGAE models, permitting to effectively train these models on\nlarge graphs with millions of nodes and edges. These strategies leverage graph\ndegeneracy and stochastic subgraph decoding techniques, respectively. Besides,\nwe introduce Gravity-Inspired GAE and VGAE, providing the first extensions of\nthese models for directed graphs, that are ubiquitous in industrial\napplications. We also consider extensions of GAE and VGAE models for dynamic\ngraphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily\ncomplex, and we propose to simplify them by leveraging linear encoders. Lastly,\nwe introduce Modularity-Aware GAE and VGAE to improve community detection on\ngraphs, while jointly preserving good performances on link prediction. In the\nlast part of this thesis, we evaluate our methods on several graphs extracted\nfrom the music streaming service Deezer. We put the emphasis on graph-based\nmusic recommendation problems. In particular, we show that our methods can\nimprove the detection of communities of similar musical items to recommend to\nusers, that they can effectively rank similar artists in a cold start setting,\nand that they permit modeling the music genre perception across cultures.",
    "descriptor": "\nComments: Ph.D. thesis defended at \\'Ecole Polytechnique (IPP) in March 2022. As mentioned in this thesis, several chapters present results also published in scientific articles written with co-authors\n",
    "authors": [
      "Guillaume Salha-Galvan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14651"
  },
  {
    "id": "arXiv:2205.14655",
    "title": "Network Decoding",
    "abstract": "We consider the problem of error control in a coded, multicast network,\nfocusing on the scenario where the errors can occur only on a proper subset of\nthe network edges. We model this problem via an adversarial noise, presenting a\nformal framework and a series of techniques to obtain upper and lower bounds on\nthe network's (1-shot) capacity, improving on the best currently known results.\nIn particular, we show that traditional cut-set bounds are not tight in general\nin the presence of a restricted adversary, and that the non-tightness of these\nis caused precisely by the restrictions imposed on the noise (and not, as one\nmay expect, by the alphabet size). We also show that, in sharp contrast with\nthe typical situation within network coding, capacity cannot be achieved in\ngeneral by combining linear network coding with end-to-end channel coding, not\neven when the underlying network has a single source and a single terminal. We\nfinally illustrate how network decoding techniques are necessary to achieve\ncapacity in the scenarios we examine, exhibiting capacity-achieving schemes and\nlower bounds for various classes of networks.",
    "descriptor": "",
    "authors": [
      "Allison Beemer",
      "Altan Berdan Kilic",
      "Alberto Ravagnani"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.14655"
  },
  {
    "id": "arXiv:2205.14657",
    "title": "COFS: Controllable Furniture layout Synthesis",
    "abstract": "Scalable generation of furniture layouts is essential for many applications\nin virtual reality, augmented reality, game development and synthetic data\ngeneration. Many existing methods tackle this problem as a sequence generation\nproblem which imposes a specific ordering on the elements of the layout making\nsuch methods impractical for interactive editing or scene completion.\nAdditionally, most methods focus on generating layouts unconditionally and\noffer minimal control over the generated layouts. We propose COFS, an\narchitecture based on standard transformer architecture blocks from language\nmodeling. The proposed model is invariant to object order by design, removing\nthe unnatural requirement of specifying an object generation order.\nFurthermore, the model allows for user interaction at multiple levels enabling\nfine grained control over the generation process. Our model consistently\noutperforms other methods which we verify by performing quantitative\nevaluations. Our method is also faster to train and sample from, compared to\nexisting methods.",
    "descriptor": "\nComments: Initial Version\n",
    "authors": [
      "Wamiq Reyaz Para",
      "Paul Guerrero",
      "Niloy Mitra",
      "Peter Wonka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14657"
  },
  {
    "id": "arXiv:2205.14659",
    "title": "Glance to Count: Learning to Rank with Anchors for Weakly-supervised  Crowd Counting",
    "abstract": "Crowd image is arguably one of the most laborious data to annotate. In this\npaper, we devote to reduce the massive demand of densely labeled crowd data,\nand propose a novel weakly-supervised setting, in which we leverage the binary\nranking of two images with high-contrast crowd counts as training guidance. To\nenable training under this new setting, we convert the crowd count regression\nproblem to a ranking potential prediction problem. In particular, we tailor a\nSiamese Ranking Network that predicts the potential scores of two images\nindicating the ordering of the counts. Hence, the ultimate goal is to assign\nappropriate potentials for all the crowd images to ensure their orderings obey\nthe ranking labels. On the other hand, potentials reveal the relative crowd\nsizes but cannot yield an exact crowd count. We resolve this problem by\nintroducing \"anchors\" during the inference stage. Concretely, anchors are a few\nimages with count labels used for referencing the corresponding counts from\npotential scores by a simple linear mapping function. We conduct extensive\nexperiments to study various combinations of supervision, and we show that the\nproposed method outperforms existing weakly-supervised methods without\nadditional labeling effort by a large margin.",
    "descriptor": "",
    "authors": [
      "Zheng Xiong",
      "Liangyu Chai",
      "Wenxi Liu",
      "Yongtuo Liu",
      "Sucheng Ren",
      "Shengfeng He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14659"
  },
  {
    "id": "arXiv:2205.14660",
    "title": "SFE-AI at SemEval-2022 Task 11: Low-Resource Named Entity Recognition  using Large Pre-trained Language Models",
    "abstract": "Large scale pre-training models have been widely used in named entity\nrecognition (NER) tasks. However, model ensemble through parameter averaging or\nvoting can not give full play to the differentiation advantages of different\nmodels, especially in the open domain. This paper describes our NER system in\nthe SemEval 2022 task11: MultiCoNER. We proposed an effective system to\nadaptively ensemble pre-trained language models by a Transformer layer. By\nassigning different weights to each model for different inputs, we adopted the\nTransformer layer to integrate the advantages of diverse models effectively.\nExperimental results show that our method achieves superior performances in\nFarsi and Dutch.",
    "descriptor": "",
    "authors": [
      "Changyu Hou",
      "Jun Wang",
      "Yixuan Qiao",
      "Peng Jiang",
      "Peng Gao",
      "Guotong Xie",
      "Qizhi Lin",
      "Xiaopeng Wang",
      "Xiandi Jiang",
      "Benqi Wang",
      "Qifeng Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14660"
  },
  {
    "id": "arXiv:2205.14661",
    "title": "IRS Aided MEC Systems with Binary Offloading: A Unified Framework for  Dynamic IRS Beamforming",
    "abstract": "In this paper, we develop a unified dynamic intelligent reflecting surface\n(IRS) beamforming framework to boost the sum computation rate of an IRS-aided\nmobile edge computing (MEC) system, where each device follows a binary\noffloading policy. Specifically, the task of each device has to be either\nexecuted locally or offloaded to MEC servers as a whole with the aid of given\nnumber of IRS beamforming vectors available. By flexibly controlling the number\nof IRS reconfiguration times, the system can achieve a balance between the\nperformance and associated signalling overhead. We aim to maximize the sum\ncomputation rate by jointly optimizing the computational mode selection for\neach device, offloading time allocation, and IRS beamforming vectors across\ntime. Since the resulting optimization problem is non-convex and NP-hard, there\nare generally no standard methods to solve it optimally. To tackle this\nproblem, we first propose a penalty-based successive convex approximation\nalgorithm, where all the associated variables in the inner-layer iterations are\noptimized simultaneously and the obtained solution is guaranteed to be locally\noptimal. Then, we further derive the offloading activation condition for each\ndevice by deeply exploiting the intrinsic structure of the original\noptimization problem. According to the offloading activation condition, a\nlow-complexity algorithm based on the successive refinement method is proposed\nto obtain high-quality solutions, which is more appealing for practical systems\nwith a large number of devices and IRS elements. Moreover, the optimal\ncondition for the proposed low-complexity algorithm is revealed. Numerical\nresults demonstrate the effectiveness of our proposed algorithms and also\nunveil the fundamental performance-cost tradeoff of the proposed dynamic IRS\nbeamforming framework.",
    "descriptor": "",
    "authors": [
      "Guangji Chen",
      "Qingqing Wu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14661"
  },
  {
    "id": "arXiv:2205.14664",
    "title": "Heterogeneous Data-Centric Architectures for Modern Data-Intensive  Applications: Case Studies in Machine Learning and Databases",
    "abstract": "Today's computing systems require moving data back-and-forth between\ncomputing resources (e.g., CPUs, GPUs, accelerators) and off-chip main memory\nso that computation can take place on the data. Unfortunately, this data\nmovement is a major bottleneck for system performance and energy consumption.\nOne promising execution paradigm that alleviates the data movement bottleneck\nin modern and emerging applications is processing-in-memory (PIM), where the\ncost of data movement to/from main memory is reduced by placing computation\ncapabilities close to memory.\nNaively employing PIM to accelerate data-intensive workloads can lead to\nsub-optimal performance due to the many design constraints PIM substrates\nimpose. Therefore, many recent works co-design specialized PIM accelerators and\nalgorithms to improve performance and reduce the energy consumption of (i)\napplications from various application domains; and (ii) various computing\nenvironments, including cloud systems, mobile systems, and edge devices.\nWe showcase the benefits of co-designing algorithms and hardware in a way\nthat efficiently takes advantage of the PIM paradigm for two modern\ndata-intensive applications: (1) machine learning inference models for edge\ndevices and (2) hybrid transactional/analytical processing databases for cloud\nsystems. We follow a two-step approach in our system design. In the first step,\nwe extensively analyze the computation and memory access patterns of each\napplication to gain insights into its hardware/software requirements and major\nsources of performance and energy bottlenecks in processor-centric systems. In\nthe second step, we leverage the insights from the first step to co-design\nalgorithms and hardware accelerators to enable high-performance and\nenergy-efficient data-centric architectures for each application.",
    "descriptor": "",
    "authors": [
      "Geraldo F. Oliveira",
      "Amirali Boroumand",
      "Saugata Ghose",
      "Juan G\u00f3mez-Luna",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14664"
  },
  {
    "id": "arXiv:2205.14665",
    "title": "Multi-Domain Virtual Network Embedding Algorithm based on Horizontal  Federated Learning",
    "abstract": "Network Virtualization (NV) is an emerging network dynamic planning technique\nto overcome network rigidity. As its necessary challenge, Virtual Network\nEmbedding (VNE) enhances the scalability and flexibility of the network by\ndecoupling the resources and services of the underlying physical network. For\nthe future multi-domain physical network modeling with the characteristics of\ndynamics, heterogeneity, privacy, and real-time, the existing related works\nperform satisfactorily. Federated learning (FL) jointly optimizes the network\nby sharing parameters among multiple parties and is widely employed in disputes\nover data privacy and data silos. Aiming at the NV challenge of multi-domain\nphysical networks, this work is the first to propose using FL to model VNE, and\npresents a VNE architecture based on Horizontal Federated Learning (HFL)\n(HFL-VNE). Specifically, combined with the distributed training paradigm of FL,\nwe deploy local servers in each physical domain, which can effectively focus on\nlocal features and reduce resource fragmentation. A global server is deployed\nto aggregate and share training parameters, which enhances local data privacy\nand significantly improves learning efficiency. Furthermore, we deploy the Deep\nReinforcement Learning (DRL) model in each server to dynamically adjust and\noptimize the resource allocation of the multi-domain physical network. In\nDRL-assisted FL, HFL-VNE jointly optimizes decision-making through specific\nlocal and federated reward mechanisms and loss functions. Finally, the\nsuperiority of HFL-VNE is proved by combining simulation experiments and\ncomparing it with related works.",
    "descriptor": "",
    "authors": [
      "Peiying Zhang",
      "Ning Chen",
      "Shibao Li",
      "Kim-Kwang Raymond Choo",
      "Chunxiao Jiang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14665"
  },
  {
    "id": "arXiv:2205.14669",
    "title": "Joint Constrained Bayesian Optimization of Planning, Guidance, Control,  and State Estimation of an Autonomous Underwater Vehicle",
    "abstract": "The performance of a guidance, navigation and control (GNC) system of an\nautonomous underwater vehicle (AUV) heavily depends on the correct tuning of\nits parameters. Our objective is to automatically tune these parameters with\nrespect to arbitrary high-level control objectives within different operational\nscenarios. In contrast to literature, an overall tuning is performed for the\nentire GNC system, which is new in the context of autonomous underwater\nvehicles. The main challenges in solving the optimization problem are\ncomputationally expensive objective function evaluations, crashing simulations\ndue to infeasible parametrization and the numerous tunable parameters (in our\ncase 13). These challenges are met by using constrained Bayesian optimization\nwith crash constraints. The method is demonstrated in simulation on a GNC\nsystem of an underactuated miniature AUV designed within the TRIPLE-nanoAUV\ninitiative for exploration of sub-glacial lakes. We quantify the substantial\nreduction in energy consumption achieved by tuning the overall system.\nFurthermore, different parametrizations are automatically generated for\ndifferent power consumption functions, robustness, and accuracy requirements.\nE.g. energy consumption can be reduced by ~28%, if the maximum allowed\ndeviation from the planned path is increased by ~65%. This shows the versatile\npractical applicability of the optimization-based tuning approach.",
    "descriptor": "\nComments: Accepted for publication at ECC 2022\n",
    "authors": [
      "David Stenger",
      "Maximilian Nitsch",
      "Dirk Abel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14669"
  },
  {
    "id": "arXiv:2205.14673",
    "title": "Continuous finite element subgrid basis functions for Discontinuous  Galerkin schemes on unstructured polygonal Voronoi meshes",
    "abstract": "We propose a new high order accurate nodal discontinuous Galerkin (DG) method\nfor the solution of nonlinear hyperbolic systems of partial differential\nequations (PDE) on unstructured polygonal Voronoi meshes. Rather than using\nclassical polynomials of degree N inside each element, in our new approach the\ndiscrete solution is represented by piecewise continuous polynomials of degree\nN within each Voronoi element, using a continuous finite element basis defined\non a subgrid inside each polygon. We call the resulting subgrid basis an\nagglomerated finite element (AFE) basis for the DG method on general polygons,\nsince it is obtained by the agglomeration of the finite element basis functions\nassociated with the subgrid triangles. The basis functions on each sub-triangle\nare defined, as usual, on a universal reference element, hence allowing to\ncompute universal mass, flux and stiffness matrices for the subgrid triangles\nonce and for all in a pre-processing stage for the reference element only.\nConsequently, the construction of an efficient quadrature-free algorithm is\npossible, despite the unstructured nature of the computational grid. High order\nof accuracy in time is achieved thanks to the ADER approach, making use of an\nelement-local space-time Galerkin finite element predictor.\nThe novel schemes are carefully validated against a set of typical benchmark\nproblems for the compressible Euler and Navier-Stokes equations. The numerical\nresults have been checked with reference solutions available in literature and\nalso systematically compared, in terms of computational efficiency and\naccuracy, with those obtained by the corresponding modal DG version of the\nscheme.",
    "descriptor": "",
    "authors": [
      "Walter Boscheri",
      "Michael Dumbser",
      "Elena Gaburro"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14673"
  },
  {
    "id": "arXiv:2205.14676",
    "title": "Diminishing Empirical Risk Minimization for Unsupervised Anomaly  Detection",
    "abstract": "Unsupervised anomaly detection (AD) is a challenging task in realistic\napplications. Recently, there is an increasing trend to detect anomalies with\ndeep neural networks (DNN). However, most popular deep AD detectors cannot\nprotect the network from learning contaminated information brought by anomalous\ndata, resulting in unsatisfactory detection performance and overfitting issues.\nIn this work, we identify one reason that hinders most existing DNN-based\nanomaly detection methods from performing is the wide adoption of the Empirical\nRisk Minimization (ERM). ERM assumes that the performance of an algorithm on an\nunknown distribution can be approximated by averaging losses on the known\ntraining set. This averaging scheme thus ignores the distinctions between\nnormal and anomalous instances. To break through the limitations of ERM, we\npropose a novel Diminishing Empirical Risk Minimization (DERM) framework.\nSpecifically, DERM adaptively adjusts the impact of individual losses through a\nwell-devised aggregation strategy. Theoretically, our proposed DERM can\ndirectly modify the gradient contribution of each individual loss in the\noptimization process to suppress the influence of outliers, leading to a robust\nanomaly detector. Empirically, DERM outperformed the state-of-the-art on the\nunsupervised AD benchmark consisting of 18 datasets.",
    "descriptor": "\nComments: 8 pages, 4 figures, to be published in IJCNN at IEEE WCCI 2022\n",
    "authors": [
      "Shaoshen Wang",
      "Yanbin Liu",
      "Ling Chen",
      "Chengqi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14676"
  },
  {
    "id": "arXiv:2205.14677",
    "title": "Structured references from PDF articles: assessing the tools for  bibliographic reference extraction and parsing",
    "abstract": "Many solutions have been provided to extract bibliographic references from\nPDF papers. Machine learning, rule-based and regular expressions approaches\nwere among the most used methods adopted in tools for addressing this task.\nThis work aims to identify and evaluate all and only the tools which, given a\nfull-text paper in PDF format, can recognise, extract and parse bibliographic\nreferences. We identified seven tools: Anystyle, Cermine, ExCite, Grobid,\nPdfssa4met, Scholarcy and Science Parse. We compared and evaluated them against\na corpus of 56 PDF articles published in 27 different subject areas. Indeed,\nAnystyle obtained the best overall score, followed by Cermine. However, in some\nof the subtasks investigated alongside the overall results (e.g. how much the\narticle layout affected the tools' performances), other tools had better\nresults for specific tasks.",
    "descriptor": "",
    "authors": [
      "Alessia Cioffi",
      "Silvio Peroni"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2205.14677"
  },
  {
    "id": "arXiv:2205.14683",
    "title": "The impact of memory on learning sequence-to-sequence tasks",
    "abstract": "The recent success of neural networks in machine translation and other fields\nhas drawn renewed attention to learning sequence-to-sequence (seq2seq) tasks.\nWhile there exists a rich literature that studies classification and regression\nusing solvable models of neural networks, learning seq2seq tasks is\nsignificantly less studied from this perspective. Here, we propose a simple\nmodel for a seq2seq task that gives us explicit control over the degree of\nmemory, or non-Markovianity, in the sequences -- the stochastic\nswitching-Ornstein-Uhlenbeck (SSOU) model. We introduce a measure of\nnon-Markovianity to quantify the amount of memory in the sequences. For a\nminimal auto-regressive (AR) learning model trained on this task, we identify\ntwo learning regimes corresponding to distinct phases in the stationary state\nof the SSOU process. These phases emerge from the interplay between two\ndifferent time scales that govern the sequence statistics. Moreover, we observe\nthat while increasing the memory of the AR model always improves performance,\nincreasing the non-Markovianity of the input sequences can improve or degrade\nperformance. Finally, our experiments with recurrent and convolutional neural\nnetworks show that our observations carry over to more complicated neural\nnetwork architectures.",
    "descriptor": "\nComments: Code to reproduce our experiments available at this https URL\n",
    "authors": [
      "Alireza Seif",
      "Sarah A.M. Loos",
      "Gennaro Tucci",
      "\u00c9dgar Rold\u00e1n",
      "Sebastian Goldt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14683"
  },
  {
    "id": "arXiv:2205.14686",
    "title": "Saliency Map Based Data Augmentation",
    "abstract": "Data augmentation is a commonly applied technique with two seemingly related\nadvantages. With this method one can increase the size of the training set\ngenerating new samples and also increase the invariance of the network against\nthe applied transformations. Unfortunately all images contain both relevant and\nirrelevant features for classification therefore this invariance has to be\nclass specific. In this paper we will present a new method which uses saliency\nmaps to restrict the invariance of neural networks to certain regions,\nproviding higher test accuracy in classification tasks.",
    "descriptor": "",
    "authors": [
      "Jalal Al-afandi",
      "B\u00e1lint Magyar",
      "Andr\u00e1s Horv\u00e1th"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14686"
  },
  {
    "id": "arXiv:2205.14687",
    "title": "2-Dimensional Euclidean Preferences",
    "abstract": "A preference profile with m alternatives and n voters is 2-dimensional\nEuclidean if both the alternatives and the voters can be placed into a\n2-dimensional space such that for each pair of alternatives, every voter\nprefers the one which has a shorter Euclidean distance to the voter. We study\nhow 2-dimensional Euclidean preference profiles depend on the values m and n.\nWe find that any profile with at most two voters or at most three alternatives\nis 2-dimensional Euclidean while for three voters, we can show this property\nfor up to seven alternatives. The results are tight in terms of Bogomolnaia and\nLaslier [2, Proposition 15(1)].",
    "descriptor": "",
    "authors": [
      "Laurent Bulteau",
      "Jiehua Chen"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.14687"
  },
  {
    "id": "arXiv:2205.14690",
    "title": "CoNT: Contrastive Neural Text Generation",
    "abstract": "Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.",
    "descriptor": "",
    "authors": [
      "Chenxin An",
      "Jiangtao Feng",
      "Kai Lv",
      "Lingpeng Kong",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14690"
  },
  {
    "id": "arXiv:2205.14691",
    "title": "On the Robustness of Safe Reinforcement Learning under Observational  Perturbations",
    "abstract": "Safe reinforcement learning (RL) trains a policy to maximize the task reward\nwhile satisfying safety constraints. While prior works focus on the performance\noptimality, we find that the optimal solutions of many safe RL problems are not\nrobust and safe against carefully designed observational perturbations. We\nformally analyze the unique properties of designing effective state adversarial\nattackers in the safe RL setting. We show that baseline adversarial attack\ntechniques for standard RL tasks are not always effective for safe RL and\nproposed two new approaches - one maximizes the cost and the other maximizes\nthe reward. One interesting and counter-intuitive finding is that the maximum\nreward attack is strong, as it can both induce unsafe behaviors and make the\nattack stealthy by maintaining the reward. We further propose a more effective\nadversarial training framework for safe RL and evaluate it via comprehensive\nexperiments. This work sheds light on the inherited connection between\nobservational robustness and safety in RL and provides a pioneer work for\nfuture safe RL studies.",
    "descriptor": "\nComments: 27 pages, 3 figures, 3 tables\n",
    "authors": [
      "Zuxin Liu",
      "Zijian Guo",
      "Zhepeng Cen",
      "Huan Zhang",
      "Jie Tan",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14691"
  },
  {
    "id": "arXiv:2205.14692",
    "title": "Generalization bounds and algorithms for estimating conditional average  treatment effect of dosage",
    "abstract": "We investigate the task of estimating the conditional average causal effect\nof treatment-dosage pairs from a combination of observational data and\nassumptions on the causal relationships in the underlying system. This has been\na longstanding challenge for fields of study such as epidemiology or economics\nthat require a treatment-dosage pair to make decisions but may not be able to\nrun randomized trials to precisely quantify their effect and heterogeneity\nacross individuals. In this paper, we extend (Shalit et al, 2017) to give new\nbounds on the counterfactual generalization error in the context of a\ncontinuous dosage parameter which relies on a different approach to defining\ncounterfactuals and assignment bias adjustment. This result then guides the\ndefinition of new learning objectives that can be used to train representation\nlearning algorithms for which we show empirically new state-of-the-art\nperformance results across several benchmark datasets for this problem,\nincluding in comparison to doubly-robust estimation methods.",
    "descriptor": "",
    "authors": [
      "Alexis Bellot",
      "Anish Dhir",
      "Giulia Prando"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14692"
  },
  {
    "id": "arXiv:2205.14693",
    "title": "VD-PCR: Improving Visual Dialog with Pronoun Coreference Resolution",
    "abstract": "The visual dialog task requires an AI agent to interact with humans in\nmulti-round dialogs based on a visual environment. As a common linguistic\nphenomenon, pronouns are often used in dialogs to improve the communication\nefficiency. As a result, resolving pronouns (i.e., grounding pronouns to the\nnoun phrases they refer to) is an essential step towards understanding dialogs.\nIn this paper, we propose VD-PCR, a novel framework to improve Visual Dialog\nunderstanding with Pronoun Coreference Resolution in both implicit and explicit\nways. First, to implicitly help models understand pronouns, we design novel\nmethods to perform the joint training of the pronoun coreference resolution and\nvisual dialog tasks. Second, after observing that the coreference relationship\nof pronouns and their referents indicates the relevance between dialog rounds,\nwe propose to explicitly prune the irrelevant history rounds in visual dialog\nmodels' input. With pruned input, the models can focus on relevant dialog\nhistory and ignore the distraction in the irrelevant one. With the proposed\nimplicit and explicit methods, VD-PCR achieves state-of-the-art experimental\nresults on the VisDial dataset.",
    "descriptor": "\nComments: The manuscript version of the paper. The published version is available at this https URL . The data, code and models are available at: this https URL KnowComp/VD-PCR\n",
    "authors": [
      "Xintong Yu",
      "Hongming Zhang",
      "Ruixin Hong",
      "Yangqiu Song",
      "Changshui Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14693"
  },
  {
    "id": "arXiv:2205.14694",
    "title": "Learning Security Strategies through Game Play and Optimal Stopping",
    "abstract": "We study automated intrusion prevention using reinforcement learning.\nFollowing a novel approach, we formulate the interaction between an attacker\nand a defender as an optimal stopping game and let attack and defense\nstrategies evolve through reinforcement learning and self-play. The\ngame-theoretic perspective allows us to find defender strategies that are\neffective against dynamic attackers. The optimal stopping formulation gives us\ninsight into the structure of optimal strategies, which we show to have\nthreshold properties. To obtain the optimal defender strategies, we introduce\nT-FP, a fictitious self-play algorithm that learns Nash equilibria through\nstochastic approximation. We show that T-FP outperforms a state-of-the-art\nalgorithm for our use case. Our overall method for learning and evaluating\nstrategies includes two systems: a simulation system where defender strategies\nare incrementally learned and an emulation system where statistics are produced\nthat drive simulation runs and where learned strategies are evaluated. We\nconclude that this approach can produce effective defender strategies for a\npractical IT infrastructure.",
    "descriptor": "",
    "authors": [
      "Kim Hammar",
      "Rolf Stadler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14694"
  },
  {
    "id": "arXiv:2205.14697",
    "title": "Evaluating Automated Driving Planner Robustness against Adversarial  Influence",
    "abstract": "Evaluating the robustness of automated driving planners is a critical and\nchallenging task. Although methodologies to evaluate vehicles are well\nestablished, they do not yet account for a reality in which vehicles with\nautonomous components share the road with adversarial agents. Our approach,\nbased on probabilistic trust models, aims to help researchers assess the\nrobustness of protections for machine learning-enabled planners against\nadversarial influence. In contrast with established practices that evaluate\nsafety using the same evaluation dataset for all vehicles, we argue that\nadversarial evaluation fundamentally requires a process that seeks to defeat a\nspecific protection. Hence, we propose that evaluations be based on estimating\nthe difficulty for an adversary to determine conditions that effectively induce\nunsafe behavior. This type of inference requires precise statements about\nthreats, protections, and aspects of planning decisions to be guarded. We\ndemonstrate our approach by evaluating protections for planners relying on\ncamera-based object detectors.",
    "descriptor": "\nComments: To appear at the 2022 Workshop on Deception Against Planning Systems and Planning in Adversarial Conditions (DAPSPAC)\n",
    "authors": [
      "Andres Molina-Markham",
      "Silvia G. Ionescu",
      "Erin Lanus",
      "Derek Ng",
      "Sam Sommerer",
      "Joseph J. Rushanan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14697"
  },
  {
    "id": "arXiv:2205.14701",
    "title": "Modeling Beats and Downbeats with a Time-Frequency Transformer",
    "abstract": "Transformer is a successful deep neural network (DNN) architecture that has\nshown its versatility not only in natural language processing but also in music\ninformation retrieval (MIR). In this paper, we present a novel\nTransformer-based approach to tackle beat and downbeat tracking. This approach\nemploys SpecTNT (Spectral-Temporal Transformer in Transformer), a variant of\nTransformer that models both spectral and temporal dimensions of a\ntime-frequency input of music audio. A SpecTNT model uses a stack of blocks,\nwhere each consists of two levels of Transformer encoders. The lower-level (or\nspectral) encoder handles the spectral features and enables the model to pay\nattention to harmonic components of each frame. Since downbeats indicate bar\nboundaries and are often accompanied by harmonic changes, this step may help\ndownbeat modeling. The upper-level (or temporal) encoder aggregates useful\nlocal spectral information to pay attention to beat/downbeat positions. We also\npropose an architecture that combines SpecTNT with a state-of-the-art model,\nTemporal Convolutional Networks (TCN), to further improve the performance.\nExtensive experiments demonstrate that our approach can significantly\noutperform TCN in downbeat tracking while maintaining comparable result in beat\ntracking.",
    "descriptor": "\nComments: This paper is accepted for publication at ICASSP 2022\n",
    "authors": [
      "Yun-Ning Hung",
      "Ju-Chiang Wang",
      "Xuchen Song",
      "Wei-Tsung Lu",
      "Minz Won"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14701"
  },
  {
    "id": "arXiv:2205.14704",
    "title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt  Learning",
    "abstract": "Prompt learning approaches have made waves in natural language processing by\ninducing better few-shot performance while they still follow a parametric-based\nlearning paradigm; the oblivion and rote memorization problems in learning may\nencounter unstable generalization issues. Specifically, vanilla prompt learning\nmay struggle to utilize atypical instances by rote during fully-supervised\ntraining or overfit shallow patterns with low-shot data. To alleviate such\nlimitations, we develop RetroPrompt with the motivation of decoupling knowledge\nfrom memorization to help the model strike a balance between generalization and\nmemorization. In contrast with vanilla prompt learning, RetroPrompt constructs\nan open-book knowledge-store from training instances and implements a retrieval\nmechanism during the process of input, training and inference, thus equipping\nthe model with the ability to retrieve related contexts from the training\ncorpus as cues for enhancement. Extensive experiments demonstrate that\nRetroPrompt can obtain better performance in both few-shot and zero-shot\nsettings. Besides, we further illustrate that our proposed RetroPrompt can\nyield better generalization abilities with new datasets. Detailed analysis of\nmemorization indeed reveals RetroPrompt can reduce the reliance of language\nmodels on memorization; thus, improving generalization for downstream tasks.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Xiang Chen",
      "Lei Li",
      "Ningyu Zhang",
      "Xiaozhuan Liang",
      "Shumin Deng",
      "Chuanqi Tan",
      "Fei Huang",
      "Luo Si",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14704"
  },
  {
    "id": "arXiv:2205.14705",
    "title": "Evaluating the Socioeconomic Status of a Large Social Event Attendees",
    "abstract": "In this study, Call Detail Records (CDRs) from downtown Budapest were\nanalysed, focusing on a large-scale event in August 2014. The attendees of the\nmain event of the Hungarian State Foundation Day have been analysed based on\ntheir Socioeconomic Status (SES). This paper proposes an approach to estimating\nSES by the price and age of the subscribers' phones, obtained by fusing a\nmobile phone property database with the CDRs. We have found some tendencies\nbetween the attendees based on the location, from where they watched the\nfireworks. However, the results do not show significant differences in this\ngeographical granularity.",
    "descriptor": "",
    "authors": [
      "Kerecsen Szab\u00f3",
      "Gerg\u0151 Pint\u00e9r",
      "Imre Felde"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14705"
  },
  {
    "id": "arXiv:2205.14709",
    "title": "New families of periodic orbits for the planar three-body problem  computed with high precision",
    "abstract": "In this paper we use a Modified Newton's method based on the Continuous\nanalog of Newton's method and high precision arithmetic for a general numerical\nsearch of periodic orbits for the planar three-body problem. We consider\nrelatively short periods and a relatively coarse search-grid. As a result, we\nfound 123 periodic solutions belonging to 105 new topological families that are\nnot included in the database in [Science China Physics, Mechanics and Astronomy\n60.12 (2017)]. The extensive numerical search is achieved by a parallel solving\nof many independent tasks using many cores in a computational cluster.",
    "descriptor": "\nComments: 5 pages, 1 figure. arXiv admin note: substantial text overlap with arXiv:2203.02793\n",
    "authors": [
      "I. Hristov",
      "R. Hristova",
      "I. Puzynin",
      "T. Puzynina",
      "Z. Sharipov",
      "Z. Tukhliev"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Chaotic Dynamics (nlin.CD)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14709"
  },
  {
    "id": "arXiv:2205.14716",
    "title": "Vision-Assisted User Clustering for Robust mmWave-NOMA Systems",
    "abstract": "NOMA is an emerging paradigm for B5G systems to support a large number and\nvariety of connected users, simultaneously. When operated in the mmWave band\nand higher bands, user channels get highly correlated which can be exploited in\nmmWave-NOMA systems to cluster a set of correlated users together and serve\nthem in one beam in the same time slot. Identifying the set of users to cluster\ntogether greatly affects the viability of NOMA systems. Typically, only CSI is\nused to make these clustering decisions. When any problem arises in accessing\nup-to-date and accurate CSI, user clustering will not properly function due to\nits hard-dependency on CSI, and obviously, this will negatively affect the\nrobustness of these NOMA systems. To improve the robustness of the NOMA\nsystems, in this paper, we propose to utilize emerging trends such as\nlocation-aware and camera-equipped base stations (CBSs) which do not require\nany extra radio frequency resource consumption. Specifically, we explore three\ndifferent dimensions of feedback that a CBS can benefit from to solve the user\nclustering problem, namely CSI-based feedback and non-CSI-based feedback,\ncomprised of UE location and the CBS camera feed. We first investigate how the\nvision assistance of a CBS can be used in conjunction with other dimensions of\nfeedback to make clustering decisions in various scenarios. Later, we provide a\nsimple user case study to illustrate how to implement vision-assisted user\nclustering in mmWave-NOMA systems to improve robustness, in which a DL beam\nselection algorithm is trained on the images captured by the CBS to perform\nNOMA clustering. We demonstrate that the user clustering without CSI can\nachieve comparable performance to accurate CSI-based user clustering solutions,\nand user clustering can continue to function without much performance loss even\nin the scenarios where CSI is severely outdated or not available at all.",
    "descriptor": "\nComments: 6 pages, 4 figures, Submitted for possible publication in the Proceedings of IEEE Global Communications Conference (GLOBECOM), 2022\n",
    "authors": [
      "Aditya S. Rajasekaran",
      "Hamza U. Sokun",
      "Omar Maraqa",
      "Halim Yanikomeroglu",
      "Saad Al-Ahmadi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14716"
  },
  {
    "id": "arXiv:2205.14717",
    "title": "Generalized Stochastic Matching",
    "abstract": "In this paper, we generalize the recently studied Stochastic Matching problem\nto more accurately model a significant medical process, kidney exchange, and\nseveral other applications. Up until now the Stochastic Matching problem that\nhas been studied was as follows: given a graph G = (V, E), each edge is\nincluded in the realized sub-graph of G mutually independently with probability\np_e, and the goal is to find a degree-bounded sub-graph Q of G that has an\nexpected maximum matching that approximates the expected maximum matching of\nthe realized sub-graph. This model does not account for possibilities of vertex\ndropouts, which can be found in several applications, e.g. in kidney exchange\nwhen donors or patients opt out of the exchange process as well as in online\nfreelancing and online dating when online profiles are found to be faked. Thus,\nwe will study a more generalized model of Stochastic Matching in which vertices\nand edges are both realized independently with some probabilities p_v, p_e,\nrespectively, which more accurately fits important applications than the\npreviously studied model.\nWe will discuss the first algorithms and analysis for this generalization of\nthe Stochastic Matching model and prove that they achieve good approximation\nratios. In particular, we show that the approximation factor of a natural\nalgorithm for this problem is at least $0.6568$ in unweighted graphs, and $1/2\n+ \\epsilon$ in weighted graphs for some constant $\\epsilon > 0$. We further\nimprove our result for unweighted graphs to $2/3$ using edge degree constrained\nsubgraphs (EDCS).",
    "descriptor": "",
    "authors": [
      "Alireza Farhadi",
      "Jacob Gilbert",
      "MohammadTaghi Hajiaghayi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14717"
  },
  {
    "id": "arXiv:2205.14718",
    "title": "Theory and Applications of Probabilistic Kolmogorov Complexity",
    "abstract": "Diverse applications of Kolmogorov complexity to learning [CIKK16], circuit\ncomplexity [OPS19], cryptography [LP20], average-case complexity [Hir21], and\nproof search [Kra22] have been discovered in recent years. Since the running\ntime of algorithms is a key resource in these fields, it is crucial in the\ncorresponding arguments to consider time-bounded variants of Kolmogorov\ncomplexity. While fruitful interactions between time-bounded Kolmogorov\ncomplexity and different areas of theoretical computer science have been known\nfor quite a while (e.g., [Sip83, Ko91, ABK+06, AF09], to name a few), the\naforementioned results have led to a renewed interest in this topic.\nThe theory of Kolmogorov complexity is well understood, but many useful\nresults and properties of Kolmogorov complexity are not known to hold in\ntime-bounded settings. This creates technical difficulties or leads to\nconditional results when applying methods from time-bounded Kolmogorov\ncomplexity to algorithms and complexity theory. Perhaps even more importantly,\nin many cases it is necessary to consider randomised algorithms. Since random\nstrings have high complexity, the classical theory of time-bounded Kolmogorov\ncomplexity might be inappropriate in such contexts.\nTo mitigate these issues and develop a theory of time-bounded Kolmogorov\ncomplexity that survives in the setting of randomised computations, some recent\npapers [Oli19, LO21, LOS21, GKLO22, LOZ22] have explored probabilistic notions\nof time-bounded Kolmogorov complexity, such as $\\mathsf{rKt}$ complexity\n[Oli19], $\\mathsf{rK}^t$ complexity [LOS21], and $\\mathsf{pK}^t$ complexity\n[GKLO22]. These measures consider different ways of encoding an object via a\nprobabilistic representation. In this survey, we provide an introduction to\nprobabilistic time-bounded Kolmogorov complexity and its applications,\nhighlighting many open problems and research directions.",
    "descriptor": "",
    "authors": [
      "Zhenjian Lu",
      "Igor C. Oliveira"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14718"
  },
  {
    "id": "arXiv:2205.14719",
    "title": "A note on hardness of promise hypergraph colouring",
    "abstract": "We show a slightly simpler proof the following theorem by I. Dinur, O. Regev,\nand C. Smyth: for all $c \\geq 2$, it is NP-hard to find a $c$-colouring of a\n2-coloruable 3-uniform hypergraph. We recast this result in the algebraic\nframework for Promise CSPs, using only a weaker version of the PCP theorem.",
    "descriptor": "",
    "authors": [
      "Marcin Wrochna"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.14719"
  },
  {
    "id": "arXiv:2205.14724",
    "title": "Extrinsic Calibration of Multiple Inertial Sensors from Arbitrary  Trajectories",
    "abstract": "We present a method of extrinsic calibration for a system of multiple\ninertial measurement units (IMUs) that estimates the relative pose of each IMU\non a rigid body using only measurements from the IMUs themselves, without the\nneed to prescribe the trajectory. Our method is based on solving a nonlinear\nleast-squares problem that penalizes inconsistency between measurements from\npairs of IMUs. We validate our method with experiments both in simulation and\nin hardware. In particular, we show that it meets or exceeds the performance --\nin terms of error, success rate, and computation time -- of an existing,\nstate-of-the-art method that does not rely only on IMU measurements and instead\nrequires the use of a camera and a fiducial marker. We also show that the\nperformance of our method is largely insensitive to the choice of trajectory\nalong which IMU measurements are collected.",
    "descriptor": "\nComments: RA-L with ICRA 2022\n",
    "authors": [
      "Jongwon Lee",
      "David Hanley",
      "Timothy Bretl"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14724"
  },
  {
    "id": "arXiv:2205.14725",
    "title": "What are People Talking about in #BackLivesMatter and #StopAsianHate?  Exploring and Categorizing Twitter Topics Emerging in Online Social Movements  through the Latent Dirichlet Allocation Model",
    "abstract": "Minority groups have been using social media to organize social movements\nthat create profound social impacts. Black Lives Matter (BLM) and Stop Asian\nHate (SAH) are two successful social movements that have spread on Twitter that\npromote protests and activities against racism and increase the public's\nawareness of other social challenges that minority groups face. However,\nprevious studies have mostly conducted qualitative analyses of tweets or\ninterviews with users, which may not comprehensively and validly represent all\ntweets. Very few studies have explored the Twitter topics within BLM and SAH\ndialogs in a rigorous, quantified and data-centered approach. Therefore, in\nthis research, we adopted a mixed-methods approach to comprehensively analyze\nBLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation\nmodel to understand the top high-level words and topics and (2) open-coding\nanalysis to identify specific themes across the tweets. We collected more than\none million tweets with the #blacklivesmatter and #stopasianhate hashtags and\ncompared their topics. Our findings revealed that the tweets discussed a\nvariety of influential topics in depth, and social justice, social movements,\nand emotional sentiments were common topics in both movements, though with\nunique subtopics for each movement. Our study contributes to the topic analysis\nof social movements on social media platforms in particular and the literature\non the interplay of AI, ethics, and society in general.",
    "descriptor": "\nComments: Accepted at AAAI and ACM Conference on AI, Ethics, and Society, August 1 to 3, 2022, Oxford, United Kingdom\n",
    "authors": [
      "Xin Tong",
      "Yixuan Li",
      "Jiayi Li",
      "Rongqi Bei",
      "Luyao Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14725"
  },
  {
    "id": "arXiv:2205.14727",
    "title": "CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset  for Conversational AI",
    "abstract": "Human language expression is based on the subjective construal of the\nsituation instead of the objective truth conditions, which means that speakers'\npersonalities and emotions after cognitive processing have an important\ninfluence on conversation. However, most existing datasets for conversational\nAI ignore human personalities and emotions, or only consider part of them. It's\ndifficult for dialogue systems to understand speakers' personalities and\nemotions although large-scale pre-training language models have been widely\nused. In order to consider both personalities and emotions in the process of\nconversation generation, we propose CPED, a large-scale Chinese personalized\nand emotional dialogue dataset, which consists of multi-source knowledge\nrelated to empathy and personal characteristic. These knowledge covers gender,\nBig Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED\ncontains more than 12K dialogues of 392 speakers from 40 TV shows. We release\nthe textual dataset with audio features and video features according to the\ncopyright claims, privacy issues, terms of service of video platforms. We\nprovide detailed description of the CPED construction process and introduce\nthree tasks for conversational AI, including personality recognition, emotion\nrecognition in conversations as well as personalized and emotional conversation\ngeneration. Finally, we provide baseline systems for these tasks and consider\nthe function of speakers' personalities and emotions on conversation. Our\nmotivation is to propose a dataset to be widely adopted by the NLP community as\na new open benchmark for conversational AI research. The full dataset is\navailable at https://github.com/scutcyr/CPED.",
    "descriptor": "",
    "authors": [
      "Yirong Chen",
      "Weiquan Fan",
      "Xiaofen Xing",
      "Jianxin Pang",
      "Minlie Huang",
      "Wenjing Han",
      "Qianfeng Tie",
      "Xiangmin Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.14727"
  },
  {
    "id": "arXiv:2205.14728",
    "title": "L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models,  and Library",
    "abstract": "Despite being the third most popular language in India, the Marathi language\nlacks useful NLP resources. Moreover, popular NLP libraries do not have support\nfor the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a\nlibrary for Marathi natural language processing. We present datasets and\ntransformer models for supervised tasks like sentiment analysis, named entity\nrecognition, and hate speech detection. We have also published a monolingual\nMarathi corpus for unsupervised language modeling tasks. Overall we present\nMahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding\nMahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark\ndatasets and prepare useful resources for Marathi. The resources are available\nat https://github.com/l3cube-pune/MarathiNLP.",
    "descriptor": "",
    "authors": [
      "Raviraj Joshi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14728"
  },
  {
    "id": "arXiv:2205.14735",
    "title": "Dynamic Control of Data-Intensive Services over Edge Computing Networks",
    "abstract": "Next-generation distributed computing networks (e.g., edge and fog computing)\nenable the efficient delivery of delay-sensitive, compute-intensive\napplications by facilitating access to computation resources in close proximity\nto end users. Many of these applications (e.g., augmented/virtual reality) are\nalso data-intensive: in addition to user-specific (live) data streams, they\nrequire access to (static) digital objects (e.g., image database) to complete\nthe required processing tasks. When required objects are not available at the\nservers hosting the associated service functions, they must be fetched from\nother edge locations, incurring additional communication cost and latency. In\nsuch settings, overall service delivery performance shall benefit from jointly\noptimized decisions around (i) routing paths and processing locations for live\ndata streams, together with (ii) cache selection and distribution paths for\nassociated digital objects. In this paper, we address the problem of dynamic\ncontrol of data-intensive services over edge cloud networks. We characterize\nthe network stability region and design the first throughput-optimal control\npolicy that coordinates processing and routing decisions for both live and\nstatic data-streams. Numerical results demonstrate the superior performance\n(e.g., throughput, delay, and resource consumption) obtained via the novel\nmulti-pipeline flow control mechanism of the proposed policy, compared with\nstate-of-the-art algorithms that lack integrated stream processing and data\ndistribution control.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.01944\n",
    "authors": [
      "Yang Cai",
      "Jaime Llorca",
      "Antonia M. Tulino",
      "Andreas F. Molisch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14735"
  },
  {
    "id": "arXiv:2205.14737",
    "title": "Stochastic Zeroth Order Gradient and Hessian Estimators: Variance  Reduction and Refined Bias Bounds",
    "abstract": "We study stochastic zeroth order gradient and Hessian estimators for\nreal-valued functions in $\\mathbb{R}^n$. We show that, via taking finite\ndifference along random orthogonal directions, the variance of the stochastic\nfinite difference estimators can be significantly reduced. In particular, we\ndesign estimators for smooth functions such that, if one uses $ \\Theta \\left( k\n\\right) $ random directions sampled from the Stiefel's manifold $ \\text{St}\n(n,k) $ and finite-difference granularity $\\delta$, the variance of the\ngradient estimator is bounded by $ \\mathcal{O} \\left( \\left( \\frac{n}{k} - 1\n\\right) + \\left( \\frac{n^2}{k} - n \\right) \\delta^2 + \\frac{ n^2 \\delta^4 }{ k\n} \\right) $, and the variance of the Hessian estimator is bounded by\n$\\mathcal{O} \\left( \\left( \\frac{n^2}{k^2} - 1 \\right) + \\left( \\frac{n^4}{k^2}\n- n^2 \\right) \\delta^2 + \\frac{n^4 \\delta^4 }{k^2} \\right) $. When $k = n$, the\nvariances become negligibly small. In addition, we provide improved bias bounds\nfor the estimators. The bias of both gradient and Hessian estimators for smooth\nfunction $f$ is of order $\\mathcal{O} \\left( \\delta^2 \\Gamma \\right)$, where\n$\\delta$ is the finite-difference granularity, and $ \\Gamma $ depends on high\norder derivatives of $f$. Our results are evidenced by empirical observations.",
    "descriptor": "\nComments: code available at: this https URL\n",
    "authors": [
      "Yasong Feng",
      "Tianyu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14737"
  },
  {
    "id": "arXiv:2205.14740",
    "title": "Investigating Participation Mechanisms in EU Code Week",
    "abstract": "Digital competence (DC) is a broad set of skills, attitudes, and knowledge\nfor confident, critical and responsible use of digital technologies in every\naspect of life. DC is fundamental to all people in conducting a productive and\nfulfilling life in an increasingly digital world. However, prejudices,\nmisconceptions, and lack of awareness reduce the diffusion of DC, hindering\ndigital transformation and preventing countries and people from realising their\nfull potential. Teaching Informatics in the curriculum is increasingly\nsupported by the institutions but faces serious challenges, such as teacher\nupskilling and support, and will require several years to observe sizeable\noutcomes. In response, grassroots movements promoting computing literacy in an\ninformal setting have grown, including EU Code Week, whose vision is to develop\ncomputing skills while promoting diversity and raising awareness of the\nimportance of digital skills. Code Week participation is a form of public\nengagement that could be affected by socio-economic and demographic factors, as\nany other form of participation. The aim of the manuscript is twofold: first,\nto offer a detailed and comprehensive statistical description of Code Week's\nparticipation in the EU Member States in terms of penetration, retention,\ndemographic composition, and spatial distribution in order to inform more\neffective awareness-raising campaigns; second, to investigate the impact of\nsocio-economic factors on Code Week involvement. The study identifies a strong\nnegative correlation between participation and income at different geographical\nscales. It also suggests underlying mechanisms driving participation that are\ncoherent with the \"psychosocial\" and the \"resource\" views, i.e. the two most\nwidely accepted explanations of the effect of income on public engagement.",
    "descriptor": "\nComments: 39 pages, 12 figures\n",
    "authors": [
      "Christel Sirocchi",
      "Annika Ostergren Pofantis",
      "Alessandro Bogliolo"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.14740"
  },
  {
    "id": "arXiv:2205.14748",
    "title": "Learning as Conversation: Dialogue Systems Reinforced for Information  Acquisition",
    "abstract": "We propose novel AI-empowered chat bots for learning as conversation where a\nuser does not read a passage but gains information and knowledge through\nconversation with a teacher bot. Our information-acquisition-oriented dialogue\nsystem employs a novel adaptation of reinforced self-play so that the system\ncan be transferred to various domains without in-domain dialogue data, and can\ncarry out conversations both informative and attentive to users. Our extensive\nsubjective and objective evaluations on three large public data corpora\ndemonstrate the effectiveness of our system to deliver knowledge-intensive and\nattentive conversations and help end users substantially gain knowledge without\nreading passages. Our code and datasets are publicly available for follow-up\nresearch.",
    "descriptor": "\nComments: 10 pages, accepted by NAACL 2022\n",
    "authors": [
      "Pengshan Cai",
      "Hui Wan",
      "Fei Liu",
      "Mo Yu",
      "Hong Yu",
      "Sachindra Joshi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14748"
  },
  {
    "id": "arXiv:2205.14749",
    "title": "To test, or not to test: A proactive approach for deciding complete  performance test initiation",
    "abstract": "Software performance testing requires a set of inputs that exercise different\nsections of the code to identify performance issues. However, running tests on\na large set of inputs can be a very time-consuming process. It is even more\nproblematic when test inputs are constantly growing, which is the case with a\nlarge-scale scientific organization such as CERN where the process of\nperforming scientific experiment generates plethora of data that is analyzed by\nphysicists leading to new scientific discoveries. Therefore, in this article,\nwe present a test input minimization approach based on a clustering technique\nto handle the issue of testing on growing data. Furthermore, we use clustering\ninformation to propose an approach that recommends the tester to decide when to\nrun the complete test suite for performance testing. To demonstrate the\nefficacy of our approach, we applied it to two different code updates of a web\nservice which is used at CERN and we found that the recommendation for\nperformance test initiation made by our approach for an update with bottleneck\nis valid.",
    "descriptor": "",
    "authors": [
      "Omar Javed",
      "Prashant Singh",
      "Giles Reger",
      "Salman Toor"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.14749"
  },
  {
    "id": "arXiv:2205.14753",
    "title": "A Framework for Generating Informative Benchmark Instances",
    "abstract": "Benchmarking is an important tool for assessing the relative performance of\nalternative solving approaches. However, the utility of benchmarking is limited\nby the quantity and quality of the available problem instances. Modern\nconstraint programming languages typically allow the specification of a\nclass-level model that is parameterised over instance data. This separation\npresents an opportunity for automated approaches to generate instance data that\ndefine instances that are graded (solvable at a certain difficulty level for a\nsolver) or can discriminate between two solving approaches. In this paper, we\nintroduce a framework that combines these two properties to generate a large\nnumber of benchmark instances, purposely generated for effective and\ninformative benchmarking. We use five problems that were used in the MiniZinc\ncompetition to demonstrate the usage of our framework. In addition to producing\na ranking among solvers, our framework gives a broader understanding of the\nbehaviour of each solver for the whole instance space; for example by finding\nsubsets of instances where the solver performance significantly varies from its\naverage performance.",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Nguyen Dang",
      "\u00d6zg\u00fcr Akg\u00fcn",
      "Joan Espasa",
      "Ian Miguel",
      "Peter Nightingale"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14753"
  },
  {
    "id": "arXiv:2205.14756",
    "title": "EfficientViT: Enhanced Linear Attention for High-Resolution  Low-Computation Visual Recognition",
    "abstract": "Vision Transformer (ViT) has achieved remarkable performance in many vision\ntasks. However, ViT is inferior to convolutional neural networks (CNNs) when\ntargeting high-resolution mobile vision applications. The key computational\nbottleneck of ViT is the softmax attention module which has quadratic\ncomputational complexity with the input resolution. It is essential to reduce\nthe cost of ViT to deploy it on edge devices. Existing methods (e.g., Swin,\nPVT) restrict the softmax attention within local windows or reduce the\nresolution of key/value tensors to reduce the cost, which sacrifices ViT's core\nadvantages on global feature extractions. In this work, we present\nEfficientViT, an efficient ViT architecture for high-resolution low-computation\nvisual recognition. Instead of restricting the softmax attention, we propose to\nreplace softmax attention with linear attention while enhancing its local\nfeature extraction ability with depthwise convolution. EfficientViT maintains\nglobal and local feature extraction capability while enjoying linear\ncomputational complexity. Extensive experiments on COCO object detection and\nCityscapes semantic segmentation demonstrate the effectiveness of our method.\nOn the COCO dataset, EfficientViT achieves 42.6 AP with 4.4G MACs, surpassing\nEfficientDet-D1 by 2.4 AP while having 27.9% fewer MACs. On Cityscapes,\nEfficientViT reaches 78.7 mIoU with 19.1G MACs, outperforming SegFormer by 2.5\nmIoU while requiring less than 1/3 the computational cost. On Qualcomm\nSnapdragon 855 CPU, EfficientViT is 3x faster than EfficientNet while achieving\nhigher ImageNet accuracy.",
    "descriptor": "",
    "authors": [
      "Han Cai",
      "Chuang Gan",
      "Song Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14756"
  },
  {
    "id": "arXiv:2205.14758",
    "title": "Credible, Strategyproof, Optimal, and Bounded Expected-Round Single-Item  Auctions for all Distributions",
    "abstract": "We consider a revenue-maximizing seller with a single item for sale to\nmultiple buyers with i.i.d. valuations. Akbarpour and Li (2020) show that the\nonly optimal, credible, strategyproof auction is the ascending price auction\nwith reserves which has unbounded communication complexity. Recent work of\nFerreira and Weinberg (2020) circumvents their impossibility result assuming\nthe existence of cryptographically secure commitment schemes, and designs a\ntwo-round credible, strategyproof, optimal auction. However, their auction is\nonly credible when buyers' valuations are MHR or $\\alpha$-strongly regular:\nthey show their auction might not be credible even when there is a single buyer\ndrawn from a non-MHR distribution. In this work, under the same cryptographic\nassumptions, we identify a new single-item auction that is credible,\nstrategyproof, revenue optimal, and terminates in constant rounds in\nexpectation for all distributions with finite monopoly price.",
    "descriptor": "\nComments: 22 Pages\n",
    "authors": [
      "Meryem Essaidi",
      "Matheus V. X. Ferreira",
      "S. Matthew Weinberg"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2205.14758"
  },
  {
    "id": "arXiv:2205.14759",
    "title": "Radial Spike and Slab Bayesian Neural Networks for Sparse Data in  Ransomware Attacks",
    "abstract": "Ransomware attacks are increasing at an alarming rate, leading to large\nfinancial losses, unrecoverable encrypted data, data leakage, and privacy\nconcerns. The prompt detection of ransomware attacks is required to minimize\nfurther damage, particularly during the encryption stage. However, the\nfrequency and structure of the observed ransomware attack data makes this task\ndifficult to accomplish in practice. The data corresponding to ransomware\nattacks represents temporal, high-dimensional sparse signals, with limited\nrecords and very imbalanced classes. While traditional deep learning models\nhave been able to achieve state-of-the-art results in a wide variety of\ndomains, Bayesian Neural Networks, which are a class of probabilistic models,\nare better suited to the issues of the ransomware data. These models combine\nideas from Bayesian statistics with the rich expressive power of neural\nnetworks. In this paper, we propose the Radial Spike and Slab Bayesian Neural\nNetwork, which is a new type of Bayesian Neural network that includes a new\nform of the approximate posterior distribution. The model scales well to large\narchitectures and recovers the sparse structure of target functions. We provide\na theoretical justification for using this type of distribution, as well as a\ncomputationally efficient method to perform variational inference. We\ndemonstrate the performance of our model on a real dataset of ransomware\nattacks and show improvement over a large number of baselines, including\nstate-of-the-art models such as Neural ODEs (ordinary differential equations).\nIn addition, we propose to represent low-level events as MITRE ATT\\&CK tactics,\ntechniques, and procedures (TTPs) which allows the model to better generalize\nto unseen ransomware attacks.",
    "descriptor": "",
    "authors": [
      "Jurijs Nazarovs",
      "Jack W. Stokes",
      "Melissa Turcotte",
      "Justin Carroll",
      "Itai Grady"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14759"
  },
  {
    "id": "arXiv:2205.14760",
    "title": "Scalable almost-linear dynamical Ising machines",
    "abstract": "The past decade has seen the emergence of Ising machines targeting hard\ncombinatorial optimization problems by minimizing the Ising Hamiltonian with\nspins represented by continuous dynamical variables. However, capabilities of\nthese machines at larger scales are yet to be fully explored. We investigate an\nIsing machine based on a network of almost-linearly coupled analog spins. We\nshow that such networks leverage the computational resource similar to that of\nthe semidefinite positive relaxation of the Ising model. We estimate the\nexpected performance of the almost-linear machine and benchmark it on a set of\n{0,1}-weighted graphs. We show that the running time of the investigated\nmachine scales polynomially (linearly with the number of edges in the\nconnectivity graph). As an example of the physical realization of the machine,\nwe present a CMOS-compatible implementation comprising an array of vertices\nefficiently storing the continuous spins on charged capacitors and\ncommunicating externally via analog current.",
    "descriptor": "",
    "authors": [
      "Aditya Shukla",
      "Mikhail Erementchouk",
      "Pinaki Mazumder"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.14760"
  },
  {
    "id": "arXiv:2205.14761",
    "title": "Modeling Disagreement in Automatic Data Labelling for Semi-Supervised  Learning in Clinical Natural Language Processing",
    "abstract": "Computational models providing accurate estimates of their uncertainty are\ncrucial for risk management associated with decision making in healthcare\ncontexts. This is especially true since many state-of-the-art systems are\ntrained using the data which has been labelled automatically (self-supervised\nmode) and tend to overfit. In this work, we investigate the quality of\nuncertainty estimates from a range of current state-of-the-art predictive\nmodels applied to the problem of observation detection in radiology reports.\nThis problem remains understudied for Natural Language Processing in the\nhealthcare domain. We demonstrate that Gaussian Processes (GPs) provide\nsuperior performance in quantifying the risks of 3 uncertainty labels based on\nthe negative log predictive probability (NLPP) evaluation metric and mean\nmaximum predicted confidence levels (MMPCL), whilst retaining strong predictive\nperformance.",
    "descriptor": "\nComments: 7 pages, *Equal contribution\n",
    "authors": [
      "Hongshu Liu",
      "Nabeel Seedat",
      "Julia Ive"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14761"
  },
  {
    "id": "arXiv:2205.14762",
    "title": "Rapid Regression Detection in Software Deployments through Sequential  Testing",
    "abstract": "The practice of continuous deployment has enabled companies to reduce\ntime-to-market by increasing the rate at which software can be deployed.\nHowever, deploying more frequently bears the risk that occasionally defective\nchanges are released. For Internet companies, this has the potential to degrade\nthe user experience and increase user abandonment. Therefore, quality control\ngates are an important component of the software delivery process. These are\nused to build confidence in the reliability of a release or change. Towards\nthis end, a common approach is to perform a canary test to evaluate new\nsoftware under production workloads. Detecting defects as early as possible is\nnecessary to reduce exposure and to provide immediate feedback to the\ndeveloper. We present a statistical framework for rapidly detecting regressions\nin software deployments. Our approach is based on sequential tests of\nstochastic order and of equality in distribution. This enables canary tests to\nbe continuously monitored, permitting regressions to be rapidly detected while\nstrictly controlling the false detection probability throughout. The utility of\nthis approach is demonstrated based on two case studies at Netflix.",
    "descriptor": "\nComments: 11 pages. To be published in the Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22)\n",
    "authors": [
      "Michael Lindon",
      "Chris Sanden",
      "Vach\u00e9 Shirikian"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.14762"
  },
  {
    "id": "arXiv:2205.14764",
    "title": "6N-DoF Pose Tracking for Tensegrity Robots",
    "abstract": "Tensegrity robots, which are composed of rigid compressive elements (rods)\nand flexible tensile elements (e.g., cables), have a variety of advantages,\nincluding flexibility, light weight, and resistance to mechanical impact.\nNevertheless, the hybrid soft-rigid nature of these robots also complicates the\nability to localize and track their state. This work aims to address what has\nbeen recognized as a grand challenge in this domain, i.e., the pose tracking of\ntensegrity robots through a markerless, vision-based method, as well as novel,\nonboard sensors that can measure the length of the robot's cables. In\nparticular, an iterative optimization process is proposed to estimate the 6-DoF\nposes of each rigid element of a tensegrity robot from an RGB-D video as well\nas endcap distance measurements from the cable sensors. To ensure the pose\nestimates of rigid elements are physically feasible, i.e., they are not\nresulting in collisions between rods or with the environment, physical\nconstraints are introduced during the optimization. Real-world experiments are\nperformed with a 3-bar tensegrity robot, which performs locomotion gaits. Given\nground truth data from a motion capture system, the proposed method achieves\nless than 1 cm translation error and 3 degrees rotation error, which\nsignificantly outperforms alternatives. At the same time, the approach can\nprovide pose estimates throughout the robot's motion, while motion capture\noften fails due to occlusions.",
    "descriptor": "",
    "authors": [
      "Shiyang Lu",
      "William R. Johnson III",
      "Kun Wang",
      "Xiaonan Huang",
      "Joran Booth",
      "Rebecca Kramer-Bottiglio",
      "Kostas Bekris"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14764"
  },
  {
    "id": "arXiv:2205.14769",
    "title": "UPB at SemEval-2022 Task 5: Enhancing UNITER with Image Sentiment and  Graph Convolutional Networks for Multimedia Automatic Misogyny Identification",
    "abstract": "In recent times, the detection of hate-speech, offensive, or abusive language\nin online media has become an important topic in NLP research due to the\nexponential growth of social media and the propagation of such messages, as\nwell as their impact. Misogyny detection, even though it plays an important\npart in hate-speech detection, has not received the same attention. In this\npaper, we describe our classification systems submitted to the SemEval-2022\nTask 5: MAMI - Multimedia Automatic Misogyny Identification. The shared task\naimed to identify misogynous content in a multi-modal setting by analysing meme\nimages together with their textual captions. To this end, we propose two models\nbased on the pre-trained UNITER model, one enhanced with an image sentiment\nclassifier, whereas the second leverages a Vocabulary Graph Convolutional\nNetwork (VGCN). Additionally, we explore an ensemble using the aforementioned\nmodels. Our best model reaches an F1-score of 71.4% in Sub-task A and 67.3% for\nSub-task B positioning our team in the upper third of the leaderboard. We\nrelease the code and experiments for our models on GitHub",
    "descriptor": "\nComments: Semeval 2022, Task 5 submission 8 pages, 3 figures\n",
    "authors": [
      "Andrei Paraschiv",
      "Mihai Dascalu",
      "Dumitru-Clementin Cercel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14769"
  },
  {
    "id": "arXiv:2205.14772",
    "title": "Unfooling Perturbation-Based Post Hoc Explainers",
    "abstract": "Monumental advancements in artificial intelligence (AI) have lured the\ninterest of doctors, lenders, judges, and other professionals. While these\nhigh-stakes decision-makers are optimistic about the technology, those familiar\nwith AI systems are wary about the lack of transparency of its decision-making\nprocesses. Perturbation-based post hoc explainers offer a model agnostic means\nof interpreting these systems while only requiring query-level access. However,\nrecent work demonstrates that these explainers can be fooled adversarially.\nThis discovery has adverse implications for auditors, regulators, and other\nsentinels. With this in mind, several natural questions arise - how can we\naudit these black box systems? And how can we ascertain that the auditee is\ncomplying with the audit in good faith? In this work, we rigorously formalize\nthis problem and devise a defense against adversarial attacks on\nperturbation-based explainers. We propose algorithms for the detection\n(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our\nnovel conditional anomaly detection approach, KNN-CAD. We demonstrate that our\napproach successfully detects whether a black box system adversarially conceals\nits decision-making process and mitigates the adversarial attack on real-world\ndata for the prevalent explainers, LIME and SHAP.",
    "descriptor": "\nComments: 10 pages (not including references and supplemental)\n",
    "authors": [
      "Zachariah Carmichael",
      "Walter J Scheirer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14772"
  },
  {
    "id": "arXiv:2205.14778",
    "title": "TransforMAP: Transformer for Memory Access Prediction",
    "abstract": "Data Prefetching is a technique that can hide memory latency by fetching data\nbefore it is needed by a program. Prefetching relies on accurate memory access\nprediction, to which task machine learning based methods are increasingly\napplied. Unlike previous approaches that learn from deltas or offsets and\nperform one access prediction, we develop TransforMAP, based on the powerful\nTransformer model, that can learn from the whole address space and perform\nmultiple cache line predictions. We propose to use the binary of memory\naddresses as model input, which avoids information loss and saves a token table\nin hardware. We design a block index bitmap to collect unordered future page\noffsets under the current page address as learning labels. As a result, our\nmodel can learn temporal patterns as well as spatial patterns within a page. In\na practical implementation, this approach has the potential to hide prediction\nlatency because it prefetches multiple cache lines likely to be used in a long\nhorizon. We show that our approach achieves 35.67% MPKI improvement and 20.55%\nIPC improvement in simulation, higher than state-of-the-art Best-Offset\nprefetcher and ISB prefetcher.",
    "descriptor": "",
    "authors": [
      "Pengmiao Zhang",
      "Ajitesh Srivastava",
      "Anant V. Nori",
      "Rajgopal Kannan",
      "Viktor K. Prasanna"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14778"
  },
  {
    "id": "arXiv:2205.14779",
    "title": "Bayes Classification using an approximation to the Joint Probability  Distribution of the Attributes",
    "abstract": "The Naive-Bayes classifier is widely used due to its simplicity, speed and\naccuracy. However this approach fails when, for at least one attribute value in\na test sample, there are no corresponding training samples with that attribute\nvalue. This is known as the zero frequency problem and is typically addressed\nusing Laplace Smoothing. However, Laplace Smoothing does not take into account\nthe statistical characteristics of the neighbourhood of the attribute values of\nthe test sample. Gaussian Naive Bayes addresses this but the resulting Gaussian\nmodel is formed from global information. We instead propose an approach that\nestimates conditional probabilities using information in the neighbourhood of\nthe test sample. In this case we no longer need to make the assumption of\nindependence of attribute values and hence consider the joint probability\ndistribution conditioned on the given class which means our approach (unlike\nthe Gaussian and Laplace approaches) takes into consideration dependencies\namong the attribute values. We illustrate the performance of the proposed\napproach on a wide range of datasets taken from the University of California at\nIrvine (UCI) Machine Learning Repository. We also include results for the\n$k$-NN classifier and demonstrate that the proposed approach is simple, robust\nand outperforms standard approaches.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Patrick Hosein",
      "Kevin Baboolal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14779"
  },
  {
    "id": "arXiv:2205.14781",
    "title": "COVID-19 Literature Mining and Retrieval using Text Mining Approaches",
    "abstract": "The novel coronavirus disease (COVID-19) began in Wuhan, China, in late 2019\nand to date has infected over 148M people worldwide, resulting in 3.12M deaths.\nOn March 10, 2020, the World Health Organisation (WHO) declared it as a global\npandemic. Many academicians and researchers started to publish papers\ndescribing the latest discoveries on covid-19. The large influx of publications\nmade it hard for other researchers to go through a large amount of data and\nfind the appropriate one that helps their research. So, the proposed model\nattempts to extract relavent titles from the large corpus of research\npublications which makes the job easy for the researchers. Allen Institute for\nAI released the CORD-19 dataset, which consists of 2,00,000 journal articles\nrelated to coronavirus-related research publications from PubMed's PMC, WHO\n(World Health Organization), bioRxiv, and medRxiv pre-prints. Along with this\ndocument corpus, they have also provided a topics dataset named topics-rnd3\nconsisting of a list of topics. Each topic has three types of representations\nlike query, question, and narrative. These Datasets are made open for research,\nand also they released a TREC-COVID competition on Kaggle. Using these topics\nlike queries, our goal is to find out the relevant documents in the CORD-19\ndataset. In this research, relevant documents should be recognized for the\nposed topics in topics-rnd3 data set. The proposed model uses Natural Language\nProcessing(NLP) techniques like Bag-of-Words, Average Word-2-Vec, Average BERT\nBase model and Tf-Idf weighted Word2Vec model to fabricate vectors for query,\nquestion, narrative, and combinations of them. Similarly, fabricate vectors for\ntitles in the CORD-19 dataset. After fabricating vectors, cosine similarity is\nused for finding similarities between every two vectors. Cosine similarity\nhelps us to find relevant documents for the given topic.",
    "descriptor": "",
    "authors": [
      "Sanku Satya Uday",
      "Satti Thanuja Pavani",
      "T. Jaya Lakshmi",
      "Rohit Chivukula"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14781"
  },
  {
    "id": "arXiv:2205.14790",
    "title": "Non-Stationary Bandits under Recharging Payoffs: Improved Planning with  Sublinear Regret",
    "abstract": "The stochastic multi-armed bandit setting has been recently studied in the\nnon-stationary regime, where the mean payoff of each action is a non-decreasing\nfunction of the number of rounds passed since it was last played. This model\ncaptures natural behavioral aspects of the users which crucially determine the\nperformance of recommendation platforms, ad placement systems, and more. Even\nassuming prior knowledge of the mean payoff functions, computing an optimal\nplanning in the above model is NP-hard, while the state-of-the-art is a\n$1/4$-approximation algorithm for the case where at most one arm can be played\nper round. We first focus on the setting where the mean payoff functions are\nknown. In this setting, we significantly improve the best-known guarantees for\nthe planning problem by developing a polynomial-time\n$(1-{1}/{e})$-approximation algorithm (asymptotically and in expectation),\nbased on a novel combination of randomized LP rounding and a time-correlated\n(interleaved) scheduling method. Furthermore, our algorithm achieves improved\nguarantees -- compared to prior work -- for the case where more than one arm\ncan be played at each round. Moving to the bandit setting, when the mean payoff\nfunctions are initially unknown, we show how our algorithm can be transformed\ninto a bandit algorithm with sublinear regret.",
    "descriptor": "",
    "authors": [
      "Orestis Papadigenopoulos",
      "Constantine Caramanis",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14790"
  },
  {
    "id": "arXiv:2205.14792",
    "title": "End-to-End Topology-Aware Machine Learning for Power System Reliability  Assessment",
    "abstract": "Conventional power system reliability suffers from the long run time of Monte\nCarlo simulation and the dimension-curse of analytic enumeration methods. This\npaper proposes a preliminary investigation on end-to-end machine learning for\ndirectly predicting the reliability index, e.g., the Loss of Load Probability\n(LOLP). By encoding the system admittance matrix into the input feature, the\nproposed machine learning pipeline can consider the impact of specific topology\nchanges due to regular maintenances of transmission lines. Two models (Support\nVector Machine and Boosting Trees) are trained and compared. Details regarding\nthe training data creation and preprocessing are also discussed. Finally,\nexperiments are conducted on the IEEE RTS-79 system. Results demonstrate the\napplicability of the proposed end-to-end machine learning pipeline in\nreliability assessment.",
    "descriptor": "\nComments: This paper has been accepted by PMAPS 2022 and will be officially presented on 14 June 2022\n",
    "authors": [
      "Yongli Zhu",
      "Chanan Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2205.14792"
  },
  {
    "id": "arXiv:2205.14794",
    "title": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing  Mechanisms in Sequence Learning",
    "abstract": "Recurrent neural networks have a strong inductive bias towards learning\ntemporally compressed representations, as the entire history of a sequence is\nrepresented by a single vector. By contrast, Transformers have little inductive\nbias towards learning temporally compressed representations, as they allow for\nattention over all previously computed elements in a sequence. Having a more\ncompressed representation of a sequence may be beneficial for generalization,\nas a high-level representation may be more easily re-used and re-purposed and\nwill contain fewer irrelevant details. At the same time, excessive compression\nof representations comes at the cost of expressiveness. We propose a solution\nwhich divides computation into two streams. A slow stream that is recurrent in\nnature aims to learn a specialized and compressed representation, by forcing\nchunks of $K$ time steps into a single representation which is divided into\nmultiple vectors. At the same time, a fast stream is parameterized as a\nTransformer to process chunks consisting of $K$ time-steps conditioned on the\ninformation in the slow-stream. In the proposed approach we hope to gain the\nexpressiveness of the Transformer, while encouraging better compression and\nstructuring of representations in the slow stream. We show the benefits of the\nproposed method in terms of improved sample efficiency and generalization\nperformance as compared to various competitive baselines for visual perception\nand sequential decision making tasks.",
    "descriptor": "",
    "authors": [
      "Aniket Didolkar",
      "Kshitij Gupta",
      "Anirudh Goyal",
      "Alex Lamb",
      "Nan Rosemary Ke",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14794"
  },
  {
    "id": "arXiv:2205.14797",
    "title": "Near Optimal Bounds for Replacement Paths and Related Problems in the  CONGEST Model",
    "abstract": "We present several results in the CONGEST model on round complexity for\nReplacement Paths (RPaths), Minimum Weight Cycle (MWC), and All Nodes Shortest\nCycles (ANSC). We study these fundamental problems in both directed and\nundirected graphs, both weighted and unweighted. Many of our results are\noptimal to within a polylog factor: For an $n$-node graph $G$ we establish near\nlinear lower and upper bounds for computing RPaths if $G$ is directed and\nweighted, and for computing MWC and ANSC if $G$ is weighted, directed or\nundirected; near $\\sqrt{n}$ lower and upper bounds for undirected weighted\nRPaths; and $\\Theta(D)$ bound for undirected unweighted RPaths. We also present\nlower and upper bounds for approximation versions of these problems, notably a\n$(2-(1/g))$-approximation algorithm for undirected unweighted MWC that runs in\n$\\tilde{O}(\\sqrt{n}+D)$ rounds, improving on the previous best bound of\n$\\tilde{O}(\\sqrt{ng}+D)$ rounds, where $g$ is the MWC length, and a\n$(1+\\epsilon)$-approximation algorithm for directed weighted RPaths and $(2+\n\\epsilon)$-approximation for weighted undirected MWC, for any constant\n$\\epsilon > 0$, that beat the round complexity lower bound for an exact\nsolution.",
    "descriptor": "",
    "authors": [
      "Vignesh Manoharan",
      "Vijaya Ramachandran"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14797"
  },
  {
    "id": "arXiv:2205.14798",
    "title": "Random Rank: The One and Only Strategyproof and Proportionally Fair  Randomized Facility Location Mechanism",
    "abstract": "Proportionality is an attractive fairness concept that has been applied to a\nrange of problems including the facility location problem, a classic problem in\nsocial choice. In our work, we propose a concept called Strong Proportionality,\nwhich ensures that when there are two groups of agents at different locations,\nboth groups incur the same total cost. We show that although Strong\nProportionality is a well-motivated and basic axiom, there is no deterministic\nstrategyproof mechanism satisfying the property. We then identify a randomized\nmechanism called Random Rank (which uniformly selects a number $k$ between $1$\nto $n$ and locates the facility at the $k$'th highest agent location) which\nsatisfies Strong Proportionality in expectation. Our main theorem characterizes\nRandom Rank as the unique mechanism that achieves universal truthfulness,\nuniversal anonymity, and Strong Proportionality in expectation among all\nrandomized mechanisms. Finally, we show via the AverageOrRandomRank mechanism\nthat even stronger ex-post fairness guarantees can be achieved by weakening\nuniversal truthfulness to strategyproofness in expectation.",
    "descriptor": "",
    "authors": [
      "Haris Aziz",
      "Alexander Lam",
      "Mashbat Suzuki",
      "Toby Walsh"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2205.14798"
  },
  {
    "id": "arXiv:2205.14806",
    "title": "Data Trust and IoT",
    "abstract": "People IoT surroundings have become valuable information sources that can\npositively impact individuals and society. A user IoT data can be used for\ndifferent purposes. For instance, research and improvement of public services.\nHowever, individuals lack the governance power to share their IoT data. Data\ntrust is a concept that brings opportunities to address data sharing in IoT.\nThis research reviews the idea of data trust. Then, we review IoT and its\nunique characteristics that implement data trust a challenge. We further\ndiscuss blockchain technology and how it can be used to enable data trust in\nIoT. Finally, we introduce a blockchain-based solution for data trust in IoT.",
    "descriptor": "",
    "authors": [
      "Mayra Samaniego"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14806"
  },
  {
    "id": "arXiv:2205.14812",
    "title": "TaSIL: Taylor Series Imitation Learning",
    "abstract": "We propose Taylor Series Imitation Learning (TaSIL), a simple augmentation to\nstandard behavior cloning losses in the context of continuous control. TaSIL\npenalizes deviations in the higher-order Taylor series terms between the\nlearned and expert policies. We show that experts satisfying a notion of\n\\emph{incremental input-to-state stability} are easy to learn, in the sense\nthat a small TaSIL-augmented imitation loss over expert trajectories guarantees\na small imitation loss over trajectories generated by the learned policy. We\nprovide sample-complexity bounds for TaSIL that scale as\n$\\tilde{\\mathcal{O}}(1/n)$ in the realizable setting, for $n$ the number of\nexpert demonstrations. Finally, we demonstrate experimentally the relationship\nbetween the robustness of the expert policy and the order of Taylor expansion\nrequired in TaSIL, and compare standard Behavior Cloning, DART, and DAgger with\nTaSIL-loss-augmented variants. In all cases, we show significant improvement\nover baselines across a variety of MuJoCo tasks.",
    "descriptor": "",
    "authors": [
      "Daniel Pfrommer",
      "Thomas T.C.K. Zhang",
      "Stephen Tu",
      "Nikolai Matni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14812"
  },
  {
    "id": "arXiv:2205.14814",
    "title": "Your Contrastive Learning Is Secretly Doing Stochastic Neighbor  Embedding",
    "abstract": "Contrastive learning, especially Self-Supervised Contrastive Learning (SSCL),\nhas achieved great success in extracting powerful features from unlabeled data,\nenabling comparable performance to the supervised counterpart. In this work, we\ncontribute to the theoretical understanding of SSCL and uncover its connection\nto the classic data visualization method, Stochastic Neighbor Embedding (SNE).\nIn the perspective of SNE, whose goal is matching pairwise distance, SSCL can\nbe viewed as a special case with the input space pairwise distance specified by\nconstructed \"positive\" pairs from data augmentation. The established\ncorrespondence facilitates deeper theoretical understandings of learned\nfeatures of SSCL, as well as methodological guidelines for practical\nimprovement. Specifically, through the lens of SNE, not only can we re-derive\nthe alignment and uniformity principle, but also provide novel analysis on\ndomain-agnostic augmentations and implicit bias. To illustrate the practical\nadvantage, we demonstrate that the modifications from SNE to $t$-SNE can also\nbe adopted in the SSCL setting, achieving significant improvement in both\nin-distribution and out-of-distribution generalization.",
    "descriptor": "",
    "authors": [
      "Tianyang Hu",
      "Zhili Liu",
      "Fengwei Zhou",
      "Wenjia Wang",
      "Weiran Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14814"
  },
  {
    "id": "arXiv:2205.14816",
    "title": "Fast Distance Oracles for Any Symmetric Norm",
    "abstract": "In the Distance Oracle problem, the goal is to preprocess $n$ vectors $x_1,\nx_2, \\cdots, x_n$ in a $d$-dimensional metric space $(\\mathbb{X}^d, \\| \\cdot\n\\|_l)$ into a cheap data structure, so that given a query vector $q \\in\n\\mathbb{X}^d$ and a subset $S\\subseteq [n]$ of the input data points, all\ndistances $\\| q - x_i \\|_l$ for $x_i\\in S$ can be quickly approximated (faster\nthan the trivial $\\sim d|S|$ query time). This primitive is a basic subroutine\nin machine learning, data mining and similarity search applications. In the\ncase of $\\ell_p$ norms, the problem is well understood, and optimal data\nstructures are known for most values of $p$.\nOur main contribution is a fast $(1+\\varepsilon)$ distance oracle for any\nsymmetric norm $\\|\\cdot\\|_l$. This class includes $\\ell_p$ norms and Orlicz\nnorms as special cases, as well as other norms used in practice, e.g. top-$k$\nnorms, max-mixture and sum-mixture of $\\ell_p$ norms, small-support norms and\nthe box-norm. We propose a novel data structure with $\\tilde{O}(n (d +\n\\mathrm{mmc}(l)^2 ) )$ preprocessing time and space, and $t_q = \\tilde{O}(d +\n|S| \\cdot \\mathrm{mmc}(l)^2)$ query time, for computing distances to a subset\n$S$ of data points, where $\\mathrm{mmc}(l)$ is a complexity-measure\n(concentration modulus) of the symmetric norm. When $l = \\ell_{p}$ , this\nruntime matches the aforementioned state-of-art oracles.",
    "descriptor": "",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Omri Weinstein",
      "Ruizhe Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14816"
  },
  {
    "id": "arXiv:2205.14817",
    "title": "Mitigating Out-of-Distribution Data Density Overestimation in  Energy-Based Models",
    "abstract": "Deep energy-based models (EBMs), which use deep neural networks (DNNs) as\nenergy functions, are receiving increasing attention due to their ability to\nlearn complex distributions. To train deep EBMs, the maximum likelihood\nestimation (MLE) with short-run Langevin Monte Carlo (LMC) is often used. While\nthe MLE with short-run LMC is computationally efficient compared to an MLE with\nfull Markov Chain Monte Carlo (MCMC), it often assigns high density to\nout-of-distribution (OOD) data. To address this issue, here we systematically\ninvestigate why the MLE with short-run LMC can converge to EBMs with wrong\ndensity estimates, and reveal that the heuristic modifications to LMC\nintroduced by previous works were the main problem. We then propose a Uniform\nSupport Partitioning (USP) scheme that optimizes a set of points to evenly\npartition the support of the EBM and then uses the resulting points to\napproximate the EBM-MLE loss gradient. We empirically demonstrate that USP\navoids the pitfalls of short-run LMC, leading to significantly improved OOD\ndata detection performance on Fashion-MNIST.",
    "descriptor": "",
    "authors": [
      "Beomsu Kim",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14817"
  },
  {
    "id": "arXiv:2205.14819",
    "title": "Universality of group convolutional neural networks based on ridgelet  analysis on groups",
    "abstract": "We investigate the approximation property of group convolutional neural\nnetworks (GCNNs) based on the ridgelet theory. We regard a group convolution as\na matrix element of a group representation, and formulate a versatile GCNN as a\nnonlinear mapping between group representations, which covers typical GCNN\nliteratures such as a cyclic convolution on a multi-channel image,\npermutation-invariant datasets (Deep Sets), and $\\mathrm{E}(n)$-equivariant\nconvolutions. The ridgelet transform is an analysis operator of a depth-2\nnetwork, namely, it maps an arbitrary given target function $f$ to the weight\n$\\gamma$ of a network $S[\\gamma]$ so that the network represents the function\nas $S[\\gamma]=f$. It has been known only for fully-connected networks, and this\nstudy is the first to present the ridgelet transform for (G)CNNs. Since the\nridgelet transform is given as a closed-form integral operator, it provides a\nconstructive proof of the $cc$-universality of GCNNs. Unlike previous\nuniversality arguments on CNNs, we do not need to convert/modify the networks\ninto other universal approximators such as invariant polynomials and\nfully-connected networks.",
    "descriptor": "",
    "authors": [
      "Sho Sonoda",
      "Isao Ishikawa",
      "Masahiro Ikeda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Representation Theory (math.RT)"
    ],
    "url": "https://arxiv.org/abs/2205.14819"
  },
  {
    "id": "arXiv:2205.14822",
    "title": "Exploring students' backtracking behaviors in digital textbooks and its  relationship to learning styles",
    "abstract": "The purpose of this study is to explore students' backtracking patterns in\nusing a digital textbook and reveal the relationship between backtracking\nbehaviors and academic performance as well as learning styles. The study was\ncarried out for two semesters on 102 university students and they are required\nto use a digital textbook system called DITeL to review courseware. Students'\nbacktracking behaviors are characterized by seven backtracking features\nextracted from interaction log data and their learning styles are measured by\nFelder-Silverman learning style model. The results of the study reveal that\nthere is a subgroup of students called backtrackers who backtrack more\nfrequently and performed better than the average students. Furthermore, the\ncausal inference analysis reveals that a higher initial ability can directly\ncause a higher frequency of backtracking, thus affecting the final test score.\nIn addition, most backtrackers are reflective and visual learners, and the\nseven backtracking features are good predictors in automatically identifying\nlearning styles. Based on the results of qualitative data analysis,\nrecommendations were made on how to provide prompt backtracking assistants and\nautomatically detect learning styles in digital textbooks.",
    "descriptor": "",
    "authors": [
      "Bo Jiang",
      "Meijun Gu",
      "Chengjiu Yin"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.14822"
  },
  {
    "id": "arXiv:2205.14825",
    "title": "Bayesian Low-Rank Interpolative Decomposition for Complex Datasets",
    "abstract": "In this paper, we introduce a probabilistic model for learning interpolative\ndecomposition (ID), which is commonly used for feature selection, low-rank\napproximation, and identifying hidden patterns in data, where the matrix\nfactors are latent variables associated with each data dimension. Prior\ndensities with support on the specified subspace are used to address the\nconstraint for the magnitude of the factored component of the observed matrix.\nBayesian inference procedure based on Gibbs sampling is employed. We evaluate\nthe model on a variety of real-world datasets including CCLE EC50, CCLE IC50,\nCTRP EC50,and MovieLens 100K datasets with different sizes, and dimensions, and\nshow that the proposed Bayesian ID GBT and GBTN models lead to smaller\nreconstructive errors compared to existing randomized approaches.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.11025\n",
    "authors": [
      "Jun Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14825"
  },
  {
    "id": "arXiv:2205.14826",
    "title": "Robust Weight Perturbation for Adversarial Training",
    "abstract": "Overfitting widely exists in adversarial robust training of deep networks. An\neffective remedy is adversarial weight perturbation, which injects the\nworst-case weight perturbation during network training by maximizing the\nclassification loss on adversarial examples. Adversarial weight perturbation\nhelps reduce the robust generalization gap; however, it also undermines the\nrobustness improvement. A criterion that regulates the weight perturbation is\ntherefore crucial for adversarial training. In this paper, we propose such a\ncriterion, namely Loss Stationary Condition (LSC) for constrained perturbation.\nWith LSC, we find that it is essential to conduct weight perturbation on\nadversarial data with small classification loss to eliminate robust\noverfitting. Weight perturbation on adversarial data with large classification\nloss is not necessary and may even lead to poor robustness. Based on these\nobservations, we propose a robust perturbation strategy to constrain the extent\nof weight perturbation. The perturbation strategy prevents deep networks from\noverfitting while avoiding the side effect of excessive weight perturbation,\nsignificantly improving the robustness of adversarial training. Extensive\nexperiments demonstrate the superiority of the proposed method over the\nstate-of-the-art adversarial training methods.",
    "descriptor": "\nComments: IJCAI2022\n",
    "authors": [
      "Chaojian Yu",
      "Bo Han",
      "Mingming Gong",
      "Li Shen",
      "Shiming Ge",
      "Bo Du",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14826"
  },
  {
    "id": "arXiv:2205.14831",
    "title": "Temporal Multiresolution Graph Neural Networks For Epidemic Prediction",
    "abstract": "In this paper, we introduce Temporal Multiresolution Graph Neural Networks\n(TMGNN), the first architecture that both learns to construct the multiscale\nand multiresolution graph structures and incorporates the time-series signals\nto capture the temporal changes of the dynamic graphs. We have applied our\nproposed model to the task of predicting future spreading of epidemic and\npandemic based on the historical time-series data collected from the actual\nCOVID-19 pandemic and chickenpox epidemic in several European countries, and\nhave obtained competitive results in comparison to other previous\nstate-of-the-art temporal architectures and graph learning algorithms. We have\nshown that capturing the multiscale and multiresolution structures of graphs is\nimportant to extract either local or global information that play a critical\nrole in understanding the dynamic of a global pandemic such as COVID-19 which\nstarted from a local city and spread to the whole world. Our work brings a\npromising research direction in forecasting and mitigating future epidemics and\npandemics.",
    "descriptor": "",
    "authors": [
      "Truong Son Hy",
      "Viet Bach Nguyen",
      "Long Tran-Thanh",
      "Risi Kondor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14831"
  },
  {
    "id": "arXiv:2205.14833",
    "title": "Walle: An End-to-End, General-Purpose, and Large-Scale Production System  for Device-Cloud Collaborative Machine Learning",
    "abstract": "To break the bottlenecks of mainstream cloud-based machine learning (ML)\nparadigm, we adopt device-cloud collaborative ML and build the first end-to-end\nand general-purpose system, called Walle, as the foundation. Walle consists of\na deployment platform, distributing ML tasks to billion-scale devices in time;\na data pipeline, efficiently preparing task input; and a compute container,\nproviding a cross-platform and high-performance execution environment, while\nfacilitating daily task iteration. Specifically, the compute container is based\non Mobile Neural Network (MNN), a tensor compute engine along with the data\nprocessing and model execution libraries, which are exposed through a refined\nPython thread-level virtual machine (VM) to support diverse ML tasks and\nconcurrent task execution. The core of MNN is the novel mechanisms of operator\ndecomposition and semi-auto search, sharply reducing the workload in manually\noptimizing hundreds of operators for tens of hardware backends and further\nquickly identifying the best backend with runtime optimization for a\ncomputation graph. The data pipeline introduces an on-device stream processing\nframework to enable processing user behavior data at source. The deployment\nplatform releases ML tasks with an efficient push-then-pull method and supports\nmulti-granularity deployment policies. We evaluate Walle in practical\ne-commerce application scenarios to demonstrate its effectiveness, efficiency,\nand scalability. Extensive micro-benchmarks also highlight the superior\nperformance of MNN and the Python thread-level VM. Walle has been in\nlarge-scale production use in Alibaba, while MNN has been open source with a\nbroad impact in the community.",
    "descriptor": "\nComments: Accepted by OSDI 2022\n",
    "authors": [
      "Chengfei Lv",
      "Chaoyue Niu",
      "Renjie Gu",
      "Xiaotang Jiang",
      "Zhaode Wang",
      "Bin Liu",
      "Ziqi Wu",
      "Qiulin Yao",
      "Congyu Huang",
      "Panos Huang",
      "Tao Huang",
      "Hui Shu",
      "Jinde Song",
      "Bin Zou",
      "Peng Lan",
      "Guohuan Xu",
      "Fei Wu",
      "Shaojie Tang",
      "Fan Wu",
      "Guihai Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14833"
  },
  {
    "id": "arXiv:2205.14834",
    "title": "GraMeR: Graph Meta Reinforcement Learning for Multi-Objective Influence  Maximization",
    "abstract": "Influence maximization (IM) is a combinatorial problem of identifying a\nsubset of nodes called the seed nodes in a network (graph), which when\nactivated, provide a maximal spread of influence in the network for a given\ndiffusion model and a budget for seed set size. IM has numerous applications\nsuch as viral marketing, epidemic control, sensor placement and other\nnetwork-related tasks. However, the uses are limited due to the computational\ncomplexity of current algorithms. Recently, learning heuristics for IM have\nbeen explored to ease the computational burden. However, there are serious\nlimitations in current approaches such as: (1) IM formulations only consider\ninfluence via spread and ignore self activation; (2) scalability to large\ngraphs; (3) generalizability across graph families; (4) low computational\nefficiency with a large running time to identify seed sets for every test\nnetwork. In this work, we address each of these limitations through a unique\napproach that involves (1) formulating a generic IM problem as a Markov\ndecision process that handles both intrinsic and influence activations; (2)\nemploying double Q learning to estimate seed nodes; (3) ensuring scalability\nvia sub-graph based representations; and (4) incorporating generalizability via\nmeta-learning across graph families. Extensive experiments are carried out in\nvarious standard networks to validate performance of the proposed Graph Meta\nReinforcement learning (GraMeR) framework. The results indicate that GraMeR is\nmultiple orders faster and generic than conventional approaches.",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Sai Munikoti",
      "Balasubramaniam Natarajan",
      "Mahantesh Halappanavar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14834"
  },
  {
    "id": "arXiv:2205.14837",
    "title": "Enhancing Sequential Recommendation with Graph Contrastive Learning",
    "abstract": "The sequential recommendation systems capture users' dynamic behavior\npatterns to predict their next interaction behaviors. Most existing sequential\nrecommendation methods only exploit the local context information of an\nindividual interaction sequence and learn model parameters solely based on the\nitem prediction loss. Thus, they usually fail to learn appropriate sequence\nrepresentations. This paper proposes a novel recommendation framework, namely\nGraph Contrastive Learning for Sequential Recommendation (GCL4SR).\nSpecifically, GCL4SR employs a Weighted Item Transition Graph (WITG), built\nbased on interaction sequences of all users, to provide global context\ninformation for each interaction and weaken the noise information in the\nsequence data. Moreover, GCL4SR uses subgraphs of WITG to augment the\nrepresentation of each interaction sequence. Two auxiliary learning objectives\nhave also been proposed to maximize the consistency between augmented\nrepresentations induced by the same interaction sequence on WITG, and minimize\nthe difference between the representations augmented by the global context on\nWITG and the local representation of the original sequence. Extensive\nexperiments on real-world datasets demonstrate that GCL4SR consistently\noutperforms state-of-the-art sequential recommendation methods.",
    "descriptor": "\nComments: 8 pages, 3 figures, Accepted by IJCAI 2022\n",
    "authors": [
      "Yixin Zhang",
      "Yong Liu",
      "Yonghui Xu",
      "Hao Xiong",
      "Chenyi Lei",
      "Wei He",
      "Lizhen Cui",
      "Chunyan Miao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14837"
  },
  {
    "id": "arXiv:2205.14838",
    "title": "Fair and Fast Tie-Breaking for Voting",
    "abstract": "We introduce a notion of fairest tie-breaking for voting w.r.t. two\nwidely-accepted fairness criteria: anonymity (all voters being treated equally)\nand neutrality (all alternatives being treated equally). We proposed a\npolynomial-time computable fairest tie-breaking mechanism, called\nmost-favorable-permutation (MFP) breaking, for a wide range of decision spaces,\nincluding single winners, $k$-committees, $k$-lists, and full rankings. We\ncharacterize the semi-random fairness of commonly-studied voting rules with MFP\nbreaking, showing that it is significantly better than existing tie-breaking\nmechanisms, including the commonly-used lexicographic and fixed-agent\nmechanisms.",
    "descriptor": "",
    "authors": [
      "Lirong Xia"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2205.14838"
  },
  {
    "id": "arXiv:2205.14839",
    "title": "Adversarial Bandits Robust to $S$-Switch Regret",
    "abstract": "We study the adversarial bandit problem under $S$ number of switching best\narms for unknown $S$. For handling this problem, we adopt the master-base\nframework using the online mirror descent method (OMD). We first provide a\nmaster-base algorithm with basic OMD, achieving\n$\\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$. For improving the regret bound with respect\nto $T$, we propose to use adaptive learning rates for OMD to control variance\nof loss estimators, and achieve\n$\\tilde{O}(\\min\\{\\mathbb{E}[\\sqrt{SKT\\rho_T(h^\\dagger)}],S\\sqrt{KT}\\})$, where\n$\\rho_T(h^\\dagger)$ is a variance term for loss estimators.",
    "descriptor": "",
    "authors": [
      "Jung-hun Kim",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14839"
  },
  {
    "id": "arXiv:2205.14840",
    "title": "To Federate or Not To Federate: Incentivizing Client Participation in  Federated Learning",
    "abstract": "Federated learning (FL) facilitates collaboration between a group of clients\nwho seek to train a common machine learning model without directly sharing\ntheir local data. Although there is an abundance of research on improving the\nspeed, efficiency, and accuracy of federated training, most works implicitly\nassume that all clients are willing to participate in the FL framework. Due to\ndata heterogeneity, however, the global model may not work well for some\nclients, and they may instead choose to use their own local model. Such\ndisincentivization of clients can be problematic from the server's perspective\nbecause having more participating clients yields a better global model, and\noffers better privacy guarantees to the participating clients. In this paper,\nwe propose an algorithm called IncFL that explicitly maximizes the fraction of\nclients who are incentivized to use the global model by dynamically adjusting\nthe aggregation weights assigned to their updates. Our experiments show that\nIncFL increases the number of incentivized clients by 30-55% compared to\nstandard federated training algorithms, and can also improve the generalization\nperformance of the global model on unseen clients.",
    "descriptor": "",
    "authors": [
      "Yae Jee Cho",
      "Divyansh Jhunjhunwala",
      "Tian Li",
      "Virginia Smith",
      "Gauri Joshi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14840"
  },
  {
    "id": "arXiv:2205.14842",
    "title": "Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning",
    "abstract": "We study data poisoning attacks on online deep reinforcement learning (DRL)\nwhere the attacker is oblivious to the learning algorithm used by the agent and\ndoes not necessarily have full knowledge of the environment. We demonstrate the\nintrinsic vulnerability of state-of-the-art DRL algorithms by designing a\ngeneral reward poisoning framework called adversarial MDP attacks. We\ninstantiate our framework to construct several new attacks which only corrupt\nthe rewards for a small fraction of the total training timesteps and make the\nagent learn a low-performing policy. Our key insight is that the\nstate-of-the-art DRL algorithms strategically explore the environment to find a\nhigh-performing policy. Our attacks leverage this insight to construct a\ncorrupted environment for misleading the agent towards learning low-performing\npolicies with a limited attack budget. We provide a theoretical analysis of the\nefficiency of our attack and perform an extensive evaluation. Our results show\nthat our attacks efficiently poison agents learning with a variety of\nstate-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc. under several\npopular classical control and MuJoCo environments.",
    "descriptor": "",
    "authors": [
      "Yinglun Xu",
      "Qi Zeng",
      "Gagandeep Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14842"
  },
  {
    "id": "arXiv:2205.14846",
    "title": "Precise Learning Curves and Higher-Order Scaling Limits for Dot Product  Kernel Regression",
    "abstract": "As modern machine learning models continue to advance the computational\nfrontier, it has become increasingly important to develop precise estimates for\nexpected performance improvements under different model and data scaling\nregimes. Currently, theoretical understanding of the learning curves that\ncharacterize how the prediction error depends on the number of samples is\nrestricted to either large-sample asymptotics ($m\\to\\infty$) or, for certain\nsimple data distributions, to the high-dimensional asymptotics in which the\nnumber of samples scales linearly with the dimension ($m\\propto d$). There is a\nwide gulf between these two regimes, including all higher-order scaling\nrelations $m\\propto d^r$, which are the subject of the present paper. We focus\non the problem of kernel ridge regression for dot-product kernels and present\nprecise formulas for the test error, bias, and variance, for data drawn\nuniformly from the sphere in the $r$th-order asymptotic scaling regime\n$m\\to\\infty$ with $m/d^r$ held constant. We observe a peak in the learning\ncurve whenever $m \\approx d^r/r!$ for any integer $r$, leading to multiple\nsample-wise descent and nontrivial behavior at multiple scales.",
    "descriptor": "\nComments: 32 pages; 4 + 3 figures;\n",
    "authors": [
      "Lechao Xiao",
      "Jeffrey Pennington"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14846"
  },
  {
    "id": "arXiv:2205.14847",
    "title": "EA$^2$E: Improving Consistency with Event Awareness for Document-Level  Argument Extraction",
    "abstract": "Events are inter-related in documents. Motivated by the\none-sense-per-discourse theory, we hypothesize that a participant tends to play\nconsistent roles across multiple events in the same document. However recent\nwork on document-level event argument extraction models each individual event\nin isolation and therefore causes inconsistency among extracted arguments\nacross events, which will further cause discrepancy for downstream applications\nsuch as event knowledge base population, question answering, and hypothesis\ngeneration. In this work, we formulate event argument consistency as the\nconstraints from event-event relations under the document-level setting. To\nimprove consistency we introduce the Event-Aware Argument Extraction (EA$^2$E)\nmodel with augmented context for training and inference. Experiment results on\nWIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E\ncompared to baseline methods.",
    "descriptor": "\nComments: NAACL 2022 Findings\n",
    "authors": [
      "Qi Zeng",
      "Qiusi Zhan",
      "Heng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14847"
  },
  {
    "id": "arXiv:2205.14850",
    "title": "Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual  Imitation Learning",
    "abstract": "Humans are capable of completing a range of challenging manipulation tasks\nthat require reasoning jointly over modalities such as vision, touch, and\nsound. Moreover, many such tasks are partially-observed; for example, taking a\nnotebook out of a backpack will lead to visual occlusion and require reasoning\nover the history of audio or tactile information. While robust tactile sensing\ncan be costly to capture on robots, microphones near or on a robot's gripper\nare a cheap and easy way to acquire audio feedback of contact events, which can\nbe a surprisingly valuable data source for perception in the absence of vision.\nMotivated by the potential for sound to mitigate visual occlusion, we aim to\nlearn a set of challenging partially-observed manipulation tasks from visual\nand audio inputs. Our proposed system learns these tasks by combining offline\nimitation learning from a modest number of tele-operated demonstrations and\nonline finetuning using human provided interventions. In a set of simulated\ntasks, we find that our system benefits from using audio, and that by using\nonline interventions we are able to improve the success rate of offline\nimitation learning by ~20%. Finally, we find that our system can complete a set\nof challenging, partially-observed tasks on a Franka Emika Panda robot, like\nextracting keys from a bag, with a 70% success rate, 50% higher than a policy\nthat does not use audio.",
    "descriptor": "",
    "authors": [
      "Maximilian Du",
      "Olivia Y. Lee",
      "Suraj Nair",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14850"
  },
  {
    "id": "arXiv:2205.14851",
    "title": "Exposing Fine-grained Adversarial Vulnerability of Face Anti-spoofing  Models",
    "abstract": "Adversarial attacks seriously threaten the high accuracy of face\nanti-spoofing models. Little adversarial noise can perturb their classification\nof live and spoofing. The existing adversarial attacks fail to figure out which\npart of the target face anti-spoofing model is vulnerable, making adversarial\nanalysis tricky. So we propose fine-grained attacks for exposing adversarial\nvulnerability of face anti-spoofing models. Firstly, we propose Semantic\nFeature Augmentation (SFA) module, which makes adversarial noise semantic-aware\nto live and spoofing features. SFA considers the contrastive classes of data\nand texture bias of models in the context of face anti-spoofing, increasing the\nattack success rate by nearly 40% on average. Secondly, we generate\nfine-grained adversarial examples based on SFA and the multitask network with\nauxiliary information. We evaluate three annotations (facial attributes,\nspoofing types and illumination) and two geometric maps (depth and reflection),\non four backbone networks (VGG, Resnet, Densenet and Swin Transformer). We find\nthat facial attributes annotation and state-of-art networks fail to guarantee\nthat models are robust to adversarial attacks. Such adversarial attacks can be\ngeneralized to more auxiliary information and backbone networks, to help our\ncommunity handle the trade-off between accuracy and adversarial robustness.",
    "descriptor": "",
    "authors": [
      "Songlin Yang",
      "Wei Wang",
      "Chenye Xu",
      "Bo Peng",
      "Jing Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14851"
  },
  {
    "id": "arXiv:2205.14852",
    "title": "Benchmarking Unsupervised Anomaly Detection and Localization",
    "abstract": "Unsupervised anomaly detection and localization, as of one the most practical\nand challenging problems in computer vision, has received great attention in\nrecent years. From the time the MVTec AD dataset was proposed to the present,\nnew research methods that are constantly being proposed push its precision to\nsaturation. It is the time to conduct a comprehensive comparison of existing\nmethods to inspire further research. This paper extensively compares 13 papers\nin terms of the performance in unsupervised anomaly detection and localization\ntasks, and adds a comparison of inference efficiency previously ignored by the\ncommunity. Meanwhile, analysis of the MVTec AD dataset are also given,\nespecially the label ambiguity that affects the model fails to achieve full\nmarks. Moreover, considering the proposal of the new MVTec 3D-AD dataset, this\npaper also conducts experiments using the existing state-of-the-art 2D methods\non this new dataset, and reports the corresponding results with analysis.",
    "descriptor": "",
    "authors": [
      "Ye Zheng",
      "Xiang Wang",
      "Yu Qi",
      "Wei Li",
      "Liwei Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14852"
  },
  {
    "id": "arXiv:2205.14853",
    "title": "Informable Multi-Objective and Multi-Directional RRT* System for Robot  Path Planning",
    "abstract": "Multi-objective or multi-destination path planning is crucial for mobile\nrobotics applications such as mobility as a service, robotics inspection, and\nelectric vehicle charging for long trips. This work proposes an anytime\niterative system to concurrently solve the multi-objective path planning\nproblem and determine the visiting order of destinations. The system is\ncomprised of an anytime informable multi-objective and multi-directional RRT*\nalgorithm to form a simple connected graph, and a proposed solver that consists\nof an enhanced cheapest insertion algorithm and a genetic algorithm to solve\nthe relaxed traveling salesman problem in polynomial time. Moreover, a list of\nwaypoints is often provided for robotics inspection and vehicle routing so that\nthe robot can preferentially visit certain equipment or areas of interest. We\nshow that the proposed system can inherently incorporate such knowledge, and\ncan navigate through challenging topology. The proposed anytime system is\nevaluated on large and complex graphs built for real-world driving\napplications. All implementations are coded in multi-threaded C++ and are\navailable at: https://github.com/UMich-BipedLab/IMOMD-RRTStar.",
    "descriptor": "",
    "authors": [
      "Jiunn-Kai Huang",
      "Yingwen Tan",
      "Dongmyeong Lee",
      "Vishnu R. Desaraju",
      "Jessy W. Grizzle"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14853"
  },
  {
    "id": "arXiv:2205.14854",
    "title": "Anti-virus Autobots: Predicting More Infectious Virus Variants for  Pandemic Prevention through Deep Learning",
    "abstract": "More infectious virus variants can arise from rapid mutations in their\nproteins, creating new infection waves. These variants can evade one's immune\nsystem and infect vaccinated individuals, lowering vaccine efficacy. Hence, to\nimprove vaccine design, this project proposes Optimus PPIme - a deep learning\napproach to predict future, more infectious variants from an existing virus\n(exemplified by SARS-CoV-2). The approach comprises an algorithm which acts as\na \"virus\" attacking a host cell. To increase infectivity, the \"virus\" mutates\nto bind better to the host's receptor. 2 algorithms were attempted - greedy\nsearch and beam search. The strength of this variant-host binding was then\nassessed by a transformer network we developed, with a high accuracy of 90%.\nWith both components, beam search eventually proposed more infectious variants.\nTherefore, this approach can potentially enable researchers to develop vaccines\nthat provide protection against future infectious variants before they emerge,\npre-empting outbreaks and saving lives.",
    "descriptor": "",
    "authors": [
      "Glenda Tan Hui En",
      "Koay Tze Erhn",
      "Shen Bingquan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14854"
  },
  {
    "id": "arXiv:2205.14857",
    "title": "Demonstration of LogicLib: An Expressive Multi-Language Interface over  Scalable Datalog System",
    "abstract": "With the ever-increasing volume of data, there is an urgent need to provide\nexpressive and efficient tools to support Big Data analytics. The declarative\nlogical language Datalog has proven very effective at expressing concisely\ngraph, machine learning, and knowledge discovery applications via recursive\nqueries. In this demonstration, we develop Logic Library (LLib), a library of\nrecursive algorithms written in Datalog that can be executed in BigDatalog, a\nDatalog engine on top of Apache Spark developed by us. LLib encapsulates\ncomplex logic-based algorithms into high-level APIs, which simplify the\ndevelopment and provide a unified interface akin to the one of Spark MLlib. As\nLLib is fully compatible with DataFrame, it enables the integrated utilization\nof its built-in applications and new Datalog queries with existing Spark\nfunctions, such as those provided by MLlib and Spark SQL. With a variety of\nexamples, we will (i) show how to write programs with LLib to express a variety\nof applications; (ii) illustrate its user experience in Apache Spark ecosystem;\nand (iii) present a user-friendly interface to interact with the LLib framework\nand monitor the query results.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Mingda Li",
      "Jin Wang",
      "Guorui Xiao",
      "Youfu Li",
      "Carlo Zaniolo"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.14857"
  },
  {
    "id": "arXiv:2205.14859",
    "title": "Cache-Augmented Inbatch Importance Resampling for Training Recommender  Retriever",
    "abstract": "Recommender retrievers aim to rapidly retrieve a fraction of items from the\nentire item corpus when a user query requests, with the representative\ntwo-tower model trained with the log softmax loss. For efficiently training\nrecommender retrievers on modern hardwares, inbatch sampling, where the items\nin the mini-batch are shared as negatives to estimate the softmax function, has\nattained growing interest. However, existing inbatch sampling based strategies\njust correct the sampling bias of inbatch items with item frequency, being\nunable to distinguish the user queries within the mini-batch and still\nincurring significant bias from the softmax. In this paper, we propose a\nCache-Augmented Inbatch Importance Resampling (XIR) for training recommender\nretrievers, which not only offers different negatives to user queries with\ninbatch items, but also adaptively achieves a more accurate estimation of the\nsoftmax distribution. Specifically, XIR resamples items for the given\nmini-batch training pairs based on certain probabilities, where a cache with\nmore frequently sampled items is adopted to augment the candidate item set,\nwith the purpose of reusing the historical informative samples. XIR enables to\nsample query-dependent negatives based on inbatch items and to capture dynamic\nchanges of model training, which leads to a better approximation of the softmax\nand further contributes to better convergence. Finally, we conduct experiments\nto validate the superior performance of the proposed XIR compared with\ncompetitive approaches.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Jin Chen",
      "Defu Lian",
      "Yucheng Li",
      "Baoyun Wang",
      "Kai Zheng",
      "Enhong Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.14859"
  },
  {
    "id": "arXiv:2205.14865",
    "title": "Prompt-aligned Gradient for Prompt Tuning",
    "abstract": "Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we\ncan craft a zero-shot classifier by \"prompt\", e.g., the confidence score of an\nimage being \"[CLASS]\" can be obtained by using the VLM provided similarity\nmeasure between the image and the prompt sentence \"a photo of a [CLASS]\".\nTherefore, prompt shows a great potential for fast adaptation of VLMs to\ndownstream tasks if we fine-tune the prompt-based similarity measure. However,\nwe find a common failure that improper fine-tuning may not only undermine the\nprompt's inherent prediction for the task-related classes, but also for other\nclasses in the VLM vocabulary. Existing methods still address this problem by\nusing traditional anti-overfitting techniques such as early stopping and data\naugmentation, which lack a principled solution specific to prompt. We present\nPrompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from\nforgetting the the general knowledge learned from VLMs. In particular, ProGrad\nonly updates the prompt whose gradient is aligned (or non-conflicting) to the\n\"general direction\", which is represented as the gradient of the KL loss of the\npre-defined prompt prediction. Extensive experiments demonstrate the stronger\nfew-shot generalization ability of ProGrad over state-of-the-art prompt tuning\nmethods. Codes are available at https://github.com/BeierZhu/Prompt-align.",
    "descriptor": "",
    "authors": [
      "Beier Zhu",
      "Yulei Niu",
      "Yucheng Han",
      "Yue Wu",
      "Hanwang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14865"
  },
  {
    "id": "arXiv:2205.14867",
    "title": "Measuring and mitigating voting access disparities: a study of race and  polling locations in Florida and North Carolina",
    "abstract": "Voter suppression and associated racial disparities in access to voting are\nlong-standing civil rights concerns in the United States. Barriers to voting\nhave taken many forms over the decades. A history of violent explicit\ndiscouragement has shifted to more subtle access limitations that can include\nlong lines and wait times, long travel times to reach a polling station, and\nother logistical barriers to voting. Our focus in this work is on quantifying\ndisparities in voting access pertaining to the overall time-to-vote, and how\nthey could be remedied via a better choice of polling location or provisioning\nmore sites where voters can cast ballots. However, appropriately calibrating\naccess disparities is difficult because of the need to account for factors such\nas population density and different community expectations for reasonable\ntravel times.\nIn this paper, we quantify access to polling locations, developing a\nmethodology for the calibrated measurement of racial disparities in polling\nlocation \"load\" and distance to polling locations. We apply this methodology to\na study of real-world data from Florida and North Carolina to identify\ndisparities in voting access from the 2020 election. We also introduce\nalgorithms, with modifications to handle scale, that can reduce these\ndisparities by suggesting new polling locations from a given list of identified\npublic locations (including schools and libraries). Applying these algorithms\non the 2020 election location data also helps to expose and explore tradeoffs\nbetween the cost of allocating more polling locations and the potential impact\non access disparities. The developed voting access measurement methodology and\nalgorithmic remediation technique is a first step in better polling location\nassignment.",
    "descriptor": "",
    "authors": [
      "Mohsen Abbasi",
      "Suresh Venkatasubramanian",
      "Sorelle A. Friedler",
      "Kristian Lum",
      "Calvin Barrett"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14867"
  },
  {
    "id": "arXiv:2205.14870",
    "title": "Compressible-composable NeRF via Rank-residual Decomposition",
    "abstract": "Neural Radiance Field (NeRF) has emerged as a compelling method to represent\n3D objects and scenes for photo-realistic rendering. However, its implicit\nrepresentation causes difficulty in manipulating the models like the explicit\nmesh representation. Several recent advances in NeRF manipulation are usually\nrestricted by a shared renderer network, or suffer from large model size. To\ncircumvent the hurdle, in this paper, we present an explicit neural field\nrepresentation that enables efficient and convenient manipulation of models. To\nachieve this goal, we learn a hybrid tensor rank decomposition of the scene\nwithout neural networks. Motivated by the low-rank approximation property of\nthe SVD algorithm, we propose a rank-residual learning strategy to encourage\nthe preservation of primary information in lower ranks. The model size can then\nbe dynamically adjusted by rank truncation to control the levels of detail,\nachieving near-optimal compression without extra optimization. Furthermore,\ndifferent models can be arbitrarily transformed and composed into one scene by\nconcatenating along the rank dimension. The growth of storage cost can also be\nmitigated by compressing the unimportant objects in the composed scene. We\ndemonstrate that our method is able to achieve comparable rendering quality to\nstate-of-the-art methods, while enabling extra capability of compression and\ncomposition. Code will be made available at\n\\url{https://github.com/ashawkey/CCNeRF}.",
    "descriptor": "",
    "authors": [
      "Jiaxiang Tang",
      "Xiaokang Chen",
      "Jingbo Wang",
      "Gang Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14870"
  },
  {
    "id": "arXiv:2205.14871",
    "title": "Illumination Adaptive Transformer",
    "abstract": "Challenging illumination conditions (low light, underexposure and\noverexposure) in the real world not only cast an unpleasant visual appearance\nbut also taint the computer vision tasks. Existing light adaptive methods often\ndeal with each condition individually. What is more, most of them often operate\non a RAW image or over-simplify the camera image signal processing (ISP)\npipeline. By decomposing the light transformation pipeline into local and\nglobal ISP components, we propose a lightweight fast Illumination Adaptive\nTransformer (IAT) which comprises two transformer-style branches: local\nestimation branch and global ISP branch. While the local branch estimates the\npixel-wise local components relevant to illumination, the global branch defines\nlearnable quires that attend the whole image to decode the parameters. Our IAT\ncould also conduct both object detection and semantic segmentation under\nvarious light conditions. We have extensively evaluated IAT on multiple\nreal-world datasets on 2 low-level tasks and 3 high-level tasks. With only 90k\nparameters and 0.004s processing speed (excluding high-level module), our IAT\nhas consistently achieved superior performance over SOTA. Code is available at\nhttps://github.com/cuiziteng/IlluminationAdaptive-Transformer.",
    "descriptor": "\nComments: 19 pages, 7 figures\n",
    "authors": [
      "Ziteng Cui",
      "Kunchang Li",
      "Lin Gu",
      "Shenghan Su",
      "Peng Gao",
      "Zhengkai Jiang",
      "Yu Qiao",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14871"
  },
  {
    "id": "arXiv:2205.14879",
    "title": "Easter2.0: Improving convolutional models for handwritten text  recognition",
    "abstract": "Convolutional Neural Networks (CNN) have shown promising results for the task\nof Handwritten Text Recognition (HTR) but they still fall behind Recurrent\nNeural Networks (RNNs)/Transformer based models in terms of performance. In\nthis paper, we propose a CNN based architecture that bridges this gap. Our\nwork, Easter2.0, is composed of multiple layers of 1D Convolution, Batch\nNormalization, ReLU, Dropout, Dense Residual connection, Squeeze-and-Excitation\nmodule and make use of Connectionist Temporal Classification (CTC) loss. In\naddition to the Easter2.0 architecture, we propose a simple and effective data\naugmentation technique 'Tiling and Corruption (TACO)' relevant for the task of\nHTR/OCR. Our work achieves state-of-the-art results on IAM handwriting database\nwhen trained using only publicly available training data. In our experiments,\nwe also present the impact of TACO augmentations and Squeeze-and-Excitation\n(SE) on text recognition accuracy. We further show that Easter2.0 is suitable\nfor few-shot learning tasks and outperforms current best methods including\nTransformers when trained on limited amount of annotated data. Code and model\nis available at: https://github.com/kartikgill/Easter2",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Kartik Chaudhary",
      "Raghav Bali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14879"
  },
  {
    "id": "arXiv:2205.14881",
    "title": "Byzantine Fault-Tolerant Min-Max Optimization",
    "abstract": "In this report, we consider a min-max optimization problem under adversarial\nmanipulation, where there are $n$ cost functions $Q_i(x)$'s, up to $f$ of which\nmay be replaced by arbitrary faulty functions by an adversary. The goal is to\nminimize the maximum cost over $x$ among the $n$ functions in spite of the\nfaulty functions. The problem formulation naturally extends to Byzantine\nfault-tolerant distributed min-max optimization. We present a simple algorithm\nfor fault-tolerant min-max optimization, and provide some bounds on the output\nof the algorithm. To the best of our knowledge, we are the first to consider\nthis problem.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Shuo Liu",
      "Nitin Vaidya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14881"
  },
  {
    "id": "arXiv:2205.14882",
    "title": "Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for  Autonomous Driving",
    "abstract": "While separately leveraging monocular 3D object detection and 2D multi-object\ntracking can be straightforwardly applied to sequence images in a\nframe-by-frame fashion, stand-alone tracker cuts off the transmission of the\nuncertainty from the 3D detector to tracking while cannot pass tracking error\ndifferentials back to the 3D detector. In this work, we propose jointly\ntraining 3D detection and 3D tracking from only monocular videos in an\nend-to-end manner. The key component is a novel spatial-temporal information\nflow module that aggregates geometric and appearance features to predict robust\nsimilarity scores across all objects in current and past frames. Specifically,\nwe leverage the attention mechanism of the transformer, in which self-attention\naggregates the spatial information in a specific frame, and cross-attention\nexploits relation and affinities of all objects in the temporal domain of\nsequence frames. The affinities are then supervised to estimate the trajectory\nand guide the flow of information between corresponding 3D objects. In\naddition, we propose a temporal\n-consistency loss that explicitly involves 3D target motion modeling into the\nlearning, making the 3D trajectory smooth in the world coordinate system.\nTime3D achieves 21.4\\% AMOTA, 13.6\\% AMOTP on the nuScenes 3D tracking\nbenchmark, surpassing all published competitors, and running at 38 FPS, while\nTime3D achieves 31.2\\% mAP, 39.4\\% NDS on the nuScenes 3D detection benchmark.",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Peixuan Li",
      "Jieyu Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14882"
  },
  {
    "id": "arXiv:2205.14885",
    "title": "A Connected Component Labeling Algorithm for Implicitly-Defined Domains",
    "abstract": "A connected component labeling algorithm is developed for implicitly-defined\ndomains specified by multivariate polynomials. The algorithm operates by\nrecursively subdividing the constraint domain into hyperrectangular subcells\nuntil the topology thereon is sufficiently simple; in particular, we devise a\ntopology test using properties of Bernstein polynomials. In many cases the\nalgorithm produces a certificate guaranteeing its correctness, i.e., two points\nyield the same label if and only if they are path-connected. To robustly handle\nvarious kinds of edge cases, the algorithm may assign identical labels to\ndistinct components, but only when they are exactly or nearly touching,\nrelative to a user-controlled length scale. A variety of numerical experiments\nassess the effectiveness of the overall approach, including statistical\nanalyses on randomly generated multi-component geometry in 2D and 3D, as well\nas specific examples involving cusps, self-intersections, junctions, and other\nkinds of singularities.",
    "descriptor": "\nComments: 15 pages, 7 figures, 3 algorithms\n",
    "authors": [
      "Robert I. Saye"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14885"
  },
  {
    "id": "arXiv:2205.14886",
    "title": "Neural Shape Mating: Self-Supervised Object Assembly with Adversarial  Shape Priors",
    "abstract": "Learning to autonomously assemble shapes is a crucial skill for many robotic\napplications. While the majority of existing part assembly methods focus on\ncorrectly posing semantic parts to recreate a whole object, we interpret\nassembly more literally: as mating geometric parts together to achieve a snug\nfit. By focusing on shape alignment rather than semantic cues, we can achieve\nacross-category generalization. In this paper, we introduce a novel task,\npairwise 3D geometric shape mating, and propose Neural Shape Mating (NSM) to\ntackle this problem. Given the point clouds of two object parts of an unknown\ncategory, NSM learns to reason about the fit of the two parts and predict a\npair of 3D poses that tightly mate them together. We couple the training of NSM\nwith an implicit shape reconstruction task to make NSM more robust to imperfect\npoint cloud observations. To train NSM, we present a self-supervised data\ncollection pipeline that generates pairwise shape mating data with ground truth\nby randomly cutting an object mesh into two parts, resulting in a dataset that\nconsists of 200K shape mating pairs from numerous object meshes with diverse\ncut types. We train NSM on the collected dataset and compare it with several\npoint cloud registration methods and one part assembly baseline. Extensive\nexperimental results and ablation studies under various settings demonstrate\nthe effectiveness of the proposed algorithm. Additional material is available\nat: https://neural-shape-mating.github.io/",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Yun-Chun Chen",
      "Haoda Li",
      "Dylan Turpin",
      "Alec Jacobson",
      "Animesh Garg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14886"
  },
  {
    "id": "arXiv:2205.14887",
    "title": "Deep Posterior Distribution-based Embedding for Hyperspectral Image  Super-resolution",
    "abstract": "In this paper, we investigate the problem of hyperspectral (HS) image spatial\nsuper-resolution via deep learning. Particularly, we focus on how to embed the\nhigh-dimensional spatial-spectral information of HS images efficiently and\neffectively. Specifically, in contrast to existing methods adopting\nempirically-designed network modules, we formulate HS embedding as an\napproximation of the posterior distribution of a set of carefully-defined HS\nembedding events, including layer-wise spatial-spectral feature extraction and\nnetwork-level feature aggregation. Then, we incorporate the proposed feature\nembedding scheme into a source-consistent super-resolution framework that is\nphysically-interpretable, producing lightweight PDE-Net, in which\nhigh-resolution (HR) HS images are iteratively refined from the residuals\nbetween input low-resolution (LR) HS images and pseudo-LR-HS images degenerated\nfrom reconstructed HR-HS images via probability-inspired HS embedding.\nExtensive experiments over three common benchmark datasets demonstrate that\nPDE-Net achieves superior performance over state-of-the-art methods. Besides,\nthe probabilistic characteristic of this kind of networks can provide the\nepistemic uncertainty of the network outputs, which may bring additional\nbenefits when used for other HS image-based applications. The code will be\npublicly available at https://github.com/jinnh/PDE-Net.",
    "descriptor": "\nComments: 12 pages, 10 figures\n",
    "authors": [
      "Jinhui Hou",
      "Zhiyu Zhu",
      "Junhui Hou",
      "Huanqiang Zeng",
      "Jinjian Wu",
      "Jiantao Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.14887"
  },
  {
    "id": "arXiv:2205.14888",
    "title": "Giant Components in Random Temporal Graphs",
    "abstract": "A temporal graph is a graph whose edges appear only at certain points in\ntime. In these graphs, reachability among the nodes relies on paths that\ntraverse edges in chronological order (temporal paths). Unlike standard paths,\nthese paths are not always composable, thus the reachability relation is\nintransitive and connected components do not form equivalence classes.\nWe investigate the properties of reachability and connected components in\nrandom temporal graphs, using a simple model that consists of permuting\nuniformly at random the edges of an Erd\\\"os-R\\'enyi graph and interpreting the\nposition in this permutation as presence times. This model was introduced in\n[Casteigts et al., FOCS 2021], where thresholds for various reachability\nproperties were identified; for example, sharp threshold for temporal\nconnectivity (all-to-all reachability) is $p=3 \\log n / n$.\nWe generalize several techniques from the above paper in order to\ncharacterize the emergence of a giant connected component, which answers an\nopen question from that paper. The growth of a giant component turns out to be\nquite different from the static case, where a component of size $n^{2/3}$\nemerges at $p_0=1/n$ and subsequently absorbs a constant fraction of all\nvertices, with this fraction gradually approaching 1. In contrast, in temporal\ngraphs, we show that the size of a giant connected component transitions\nabruptly from $o(n)$ nodes to $n - o(n)$ nodes at $p = \\log n / n$.",
    "descriptor": "",
    "authors": [
      "Ruben Becker",
      "Arnaud Casteigts",
      "Pierluigi Crescenzi",
      "Bojana Kodric",
      "Malte Renken",
      "Michael Raskin",
      "Viktor Zamaraev"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.14888"
  },
  {
    "id": "arXiv:2205.14892",
    "title": "Exploring the Open World Using Incremental Extreme Value Machines",
    "abstract": "Dynamic environments require adaptive applications. One particular machine\nlearning problem in dynamic environments is open world recognition. It\ncharacterizes a continuously changing domain where only some classes are seen\nin one batch of the training data and such batches can only be learned\nincrementally. Open world recognition is a demanding task that is, to the best\nof our knowledge, addressed by only a few methods. This work introduces a\nmodification of the widely known Extreme Value Machine (EVM) to enable open\nworld recognition. Our proposed method extends the EVM with a partial model\nfitting function by neglecting unaffected space during an update. This reduces\nthe training time by a factor of 28. In addition, we provide a modified model\nreduction using weighted maximum K-set cover to strictly bound the model\ncomplexity and reduce the computational effort by a factor of 3.5 from 2.1 s to\n0.6 s. In our experiments, we rigorously evaluate openness with two novel\nevaluation protocols. The proposed method achieves superior accuracy of about\n12 % and computational efficiency in the tasks of image classification and face\nrecognition.",
    "descriptor": "\nComments: Accepted at ICPR 2022\n",
    "authors": [
      "Tobias Koch",
      "Felix Liebezeit",
      "Christian Riess",
      "Vincent Christlein",
      "Thomas K\u00f6hler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14892"
  },
  {
    "id": "arXiv:2205.14893",
    "title": "Hybrid Numerical Modeling of Ballistic Clay under Low-Speed Impact using  Artificial Neural Networks",
    "abstract": "Roma Plastilina No. 1 clay has been widely used as a conservative boundary\ncondition in bulletproof vests, namely to play the role of a human body.\nInterestingly, the effect of this boundary condition on the ballistic\nperformance of the vests is indiscernible. Moreover, back face deformation\nshould be characterized by measuring the indentation in the deformed clay,\nwhich is important for determining the lethality of gunshots. Therefore,\nseveral studies have focused on modeling not only bulletproof vests but also\nthe clay backing material. Despite various attempts to develop a suitable\nnumerical model, determining the appropriate physical parameters that can\ncapture the high-strain-rate behavior of clay is still challenging. In this\nstudy, we predicted indentation depth in clay using an artificial neural\nnetwork (ANN) and determined the optimal material parameters required for a\nfinite element method (FEM)-based model using an inverse tracking method. Our\nANN-FEM hybrid model successfully optimized high-strain-rate material\nparameters without the need for any independent mechanical tests. The proposed\nnovel model achieved a high prediction accuracy of over 98% referring impact\ncases.",
    "descriptor": "",
    "authors": [
      "YeonSu Kim",
      "Yoon A Kim",
      "Seo Hwee Park",
      "YunHo Kim"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14893"
  },
  {
    "id": "arXiv:2205.14894",
    "title": "Daisy Bloom Filters",
    "abstract": "Weighted Bloom filters (Bruck, Gao and Jiang, ISIT 2006) are Bloom filters\nthat adapt the number of hash functions according to the query element. That\nis, they use a sequence of hash functions $h_1, h_2, \\dots$ and insert $x$ by\nsetting the bits in $k_x$ positions $h_1(x), h_2(x), \\dots, h_{k_x}(x)$ to 1,\nwhere the parameter $k_x$ depends on $x$. Similarly, a query for $x$ checks\nwhether the bits at positions $h_1(x), h_2(x), \\dots, h_{k_x}(x)$ contain a $0$\n(in which case we know that $x$ was not inserted), or contains only $1$s (in\nwhich case $x$ may have been inserted, but it could also be a false positive).\nIn this paper, we determine a near-optimal choice of the parameters $k_x$ in\na model where $n$ elements are inserted independently from a probability\ndistribution $\\mathcal{P}$ and query elements are chosen from a probability\ndistribution $\\mathcal{Q}$, under a bound on the false positive probability\n$F$. In contrast, the parameter choice of Bruck et al., as well as follow-up\nwork by Wang et al., does not guarantee a nontrivial bound on the false\npositive rate. We refer to our parameterization of the weighted Bloom filter as\na $\\textit{Daisy Bloom filter}$.\nFor many distributions $\\mathcal{P}$ and $\\mathcal{Q}$, the Daisy Bloom\nfilter space usage is significantly smaller than that of Standard Bloom\nfilters. Our upper bound is complemented with an information-theoretical lower\nbound, showing that (with mild restrictions on the distributions $\\mathcal{P}$\nand $\\mathcal{Q}$), the space usage of Daisy Bloom filters is the best possible\nup to a constant factor.\nDaisy Bloom filters can be seen as a fine-grained variant of a recent data\nstructure of Vaidya, Knorr, Mitzenmacher and Kraska. Like their work, we are\nmotivated by settings in which we have prior knowledge of the workload of the\nfilter, possibly in the form of advice from a machine learning algorithm.",
    "descriptor": "\nComments: 16 pages, 1 figure\n",
    "authors": [
      "Ioana O. Bercea",
      "Jakob B\u00e6k Tejs Houen",
      "Rasmus Pagh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14894"
  },
  {
    "id": "arXiv:2205.14895",
    "title": "From Representation to Reasoning: Towards both Evidence and Commonsense  Reasoning for Video Question-Answering",
    "abstract": "Video understanding has achieved great success in representation learning,\nsuch as video caption, video object grounding, and video descriptive\nquestion-answer. However, current methods still struggle on video reasoning,\nincluding evidence reasoning and commonsense reasoning. To facilitate deeper\nvideo understanding towards video reasoning, we present the task of\nCausal-VidQA, which includes four types of questions ranging from scene\ndescription (description) to evidence reasoning (explanation) and commonsense\nreasoning (prediction and counterfactual). For commonsense reasoning, we set up\na two-step solution by answering the question and providing a proper reason.\nThrough extensive experiments on existing VideoQA methods, we find that the\nstate-of-the-art methods are strong in descriptions but weak in reasoning. We\nhope that Causal-VidQA can guide the research of video understanding from\nrepresentation learning to deeper reasoning. The dataset and related resources\nare available at \\url{https://github.com/bcmi/Causal-VidQA.git}.",
    "descriptor": "\nComments: To appear in CVPR 2022\n",
    "authors": [
      "Jiangtong Li",
      "Li Niu",
      "Liqing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.14895"
  },
  {
    "id": "arXiv:2205.14897",
    "title": "Fully Polynomial-Time Distributed Computation in Low-Treewidth Graphs",
    "abstract": "We consider global problems, i.e. problems that take at least diameter time,\neven when the bandwidth is not restricted. We show that all problems considered\nadmit efficient solutions in low-treewidth graphs. By ``efficient'' we mean\nthat the running time has polynomial dependence on the treewidth, a linear\ndependence on the diameter (which is unavoidable), and only a polylogarithmic\ndependence on $n$, the number of nodes in the graph. We present the algorithms\nsolving the following problems in the CONGEST model which all attain\n$\\tilde{O(\\tau^{O(1)}D)}$-round complexity (where $\\tau$ and $D$ denote the\ntreewidth and diameter of the graph, respectively): (1) Exact single-source\nshortest paths (actually, the more general problem of computing a distance\nlabeling scheme) for weighted and directed graphs, (2) exact bipartite\nunweighted maximum matching, and (3) the weighted girth for both directed and\nundirected graphs. We derive all of our results using a single unified\nframework, which consists of two novel technical ingredients, The first is a\nfully polynomial-time distributed tree decomposition algorithm, which outputs a\ndecomposition of width $O(\\tau^2\\log n)$ in $\\tilde{O}(\\tau^{O(1)}D)$ rounds\n(where $n$ is the number of nodes in the graph). The second ingredient, and the\ntechnical highlight of this paper, is the novel concept of a \\emph{stateful\nwalk constraint}, which naturally defines a set of feasible walks in the input\ngraph based on their local properties (e.g., augmenting paths). Given a\nstateful walk constraint, the constrained version of the shortest paths problem\n(or distance labeling) requires the algorithm to output the shortest\n\\emph{constrained} walk (or its distance) for a given source and sink vertices.\nWe show that this problem can be efficiently solved in the CONGEST model by\nreducing it to an \\emph{unconstrained} version of the problem.",
    "descriptor": "",
    "authors": [
      "Taisuke Izumi",
      "Naoki Kitamura",
      "Takamasa Naruse",
      "Gregory Schwartzman"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14897"
  },
  {
    "id": "arXiv:2205.14900",
    "title": "FRAug: Tackling Federated Learning with Non-IID Features via  Representation Augmentation",
    "abstract": "Federated Learning (FL) is a decentralized learning paradigm in which\nmultiple clients collaboratively train deep learning models without\ncentralizing their local data and hence preserve data privacy. Real-world\napplications usually involve a distribution shift across the datasets of the\ndifferent clients, which hurts the generalization ability of the clients to\nunseen samples from their respective data distributions. In this work, we\naddress the recently proposed feature shift problem where the clients have\ndifferent feature distributions while the label distribution is the same. We\npropose Federated Representation Augmentation (FRAug) to tackle this practical\nand challenging problem. Our approach generates synthetic client-specific\nsamples in the embedding space to augment the usually small client datasets.\nFor that, we train a shared generative model to fuse the clients' knowledge,\nlearned from different feature distributions, to synthesize client-agnostic\nembeddings, which are then locally transformed into client-specific embeddings\nby Representation Transformation Networks (RTNets). By transferring knowledge\nacross the clients, the generated embeddings act as a regularizer for the\nclient models and reduce overfitting to the local original datasets, hence\nimproving generalization. Our empirical evaluation on multiple benchmark\ndatasets demonstrates the effectiveness of the proposed method, which\nsubstantially outperforms the current state-of-the-art FL methods for non-IID\nfeatures, including PartialFed and FedBN.",
    "descriptor": "",
    "authors": [
      "Haokun Chen",
      "Ahmed Frikha",
      "Denis Krompass",
      "Volker Tresp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14900"
  },
  {
    "id": "arXiv:2205.14905",
    "title": "Confederated Learning: Federated Learning with Decentralized Edge  Servers",
    "abstract": "Federated learning (FL) is an emerging machine learning paradigm that allows\nto accomplish model training without aggregating data at a central server. Most\nstudies on FL consider a centralized framework, in which a single server is\nendowed with a central authority to coordinate a number of devices to perform\nmodel training in an iterative manner. Due to stringent communication and\nbandwidth constraints, such a centralized framework has limited scalability as\nthe number of devices grows. To address this issue, in this paper, we propose a\nConFederated Learning (CFL) framework. The proposed CFL consists of multiple\nservers, in which each server is connected with an individual set of devices as\nin the conventional FL framework, and decentralized collaboration is leveraged\namong servers to make full use of the data dispersed throughout the network. We\ndevelop an alternating direction method of multipliers (ADMM) algorithm for\nCFL. The proposed algorithm employs a random scheduling policy which randomly\nselects a subset of devices to access their respective servers at each\niteration, thus alleviating the need of uploading a huge amount of information\nfrom devices to servers. Theoretical analysis is presented to justify the\nproposed method. Numerical results show that the proposed method can converge\nto a decent solution significantly faster than gradient-based FL algorithms,\nthus boasting a substantial advantage in terms of communication efficiency.",
    "descriptor": "\nComments: 13 pages, 5 figures\n",
    "authors": [
      "Bin Wang",
      "Jun Fang",
      "Hongbin Li",
      "Xiaojun Yuan",
      "Qing Ling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14905"
  },
  {
    "id": "arXiv:2205.14908",
    "title": "Adaptive color transfer from images to terrain visualizations",
    "abstract": "Terrain mapping is not only dedicated to communicating how high or how steep\na landscape is but can also help to narrate how we feel about a place. However,\ncrafting effective and expressive hypsometric tints is challenging for both\nnonexperts and experts. In this paper, we present a two-step image-to-terrain\ncolor transfer method that can transfer color from arbitrary images to diverse\nterrain models. First, we present a new image color organization method that\norganizes discrete, irregular image colors into a continuous, regular color\ngrid that facilitates a series of color operations, such as local and global\nsearching, categorical color selection and sequential color interpolation.\nSecond, we quantify a series of subjective concerns about elevation color\ncrafting, such as \"the lower, the higher\" principle, color conventions, and\naerial perspectives. We also define color similarity between image and terrain\nvisualization with aesthetic quality. We then mathematically formulate\nimage-to-terrain color transfer as a dual-objective optimization problem and\noffer a heuristic searching method to solve the problem. Finally, we compare\nelevation tints from our method with a standard color scheme on four test\nterrains. The evaluations show that the hypsometric tints from the proposed\nmethod can work as effectively as the standard scheme and that our tints are\nmore visually favorable. We also showcase that our method can transfer emotion\nfrom image to terrain visualization.",
    "descriptor": "",
    "authors": [
      "Mingguang Wu",
      "Yanjie Sun",
      "Shangjing Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14908"
  },
  {
    "id": "arXiv:2205.14912",
    "title": "E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language  Understanding and Generation",
    "abstract": "Sequence-to-sequence (seq2seq) learning has become a popular trend for\npretraining language models, due to its succinct and universal framework.\nHowever, the prior seq2seq pretraining models generally focus on reconstructive\nobjectives on the decoder side and neglect the effect of encoder-side\nsupervisions, which may lead to sub-optimal performance. To this end, we\npropose an encoding-enhanced seq2seq pretraining strategy, namely E2S2, which\nimproves the seq2seq models via integrating more efficient self-supervised\ninformation into the encoders. Specifically, E2S2 contains two self-supervised\nobjectives upon the encoder, which are from two perspectives: 1) denoising the\ncorrupted sentence (denoising objective); 2) learning robust sentence\nrepresentations (contrastive objective). With these two objectives, the encoder\ncan effectively distinguish the noise tokens and capture more syntactic and\nsemantic knowledge, thus strengthening the ability of seq2seq model to\ncomprehend the input sentence and conditionally generate the target. We conduct\nextensive experiments spanning language understanding and generation tasks upon\nthe state-of-the-art seq2seq pretrained language model BART. We show that E2S2\ncan consistently boost the performance, including 1.0% averaged gain on GLUE\nbenchmark and 1.75% F_0.5 score improvement on CoNLL2014 dataset, validating\nthe effectiveness and robustness of our E2S2.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Qihuang Zhong",
      "Liang Ding",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14912"
  },
  {
    "id": "arXiv:2205.14916",
    "title": "A Probabilistic Call-by-Need Lambda-Calculus -- Extended Version",
    "abstract": "To support the understanding of declarative probabilistic programming\nlanguages, we introduce a lambda-calculus with a fair binary probabilistic\nchoice that chooses between its arguments with equal probability. The reduction\nstrategy of the calculus is a call-by-need strategy that performs lazy\nevaluation and implements sharing by recursive let-expressions. Expected\nconvergence of expressions is the limit of the sum of all successful reduction\noutputs weighted by their probability. We use contextual equivalence as program\nsemantics: two expressions are contextually equivalent if and only if the\nexpected convergence of the expressions plugged into any program context is\nalways the same. We develop and illustrate techniques to prove equivalences\nincluding a context lemma, two derived criteria to show equivalences and a\nsyntactic diagram-based method. This finally enables us to show correctness of\na large set of program transformations with respect to the contextual\nequivalence.",
    "descriptor": "\nComments: 18 pages, 11 figures\n",
    "authors": [
      "David Sabel",
      "Manfred Schmidt-Schau\u00df",
      "Luca Maio"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14916"
  },
  {
    "id": "arXiv:2205.14917",
    "title": "Uncertainty Quantification and Resource-Demanding Computer Vision  Applications of Deep Learning",
    "abstract": "Bringing deep neural networks (DNNs) into safety critical applications such\nas automated driving, medical imaging and finance, requires a thorough\ntreatment of the model's uncertainties. Training deep neural networks is\nalready resource demanding and so is also their uncertainty quantification. In\nthis overview article, we survey methods that we developed to teach DNNs to be\nuncertain when they encounter new object classes. Additionally, we present\ntraining methods to learn from only a few labels with help of uncertainty\nquantification. Note that this is typically paid with a massive overhead in\ncomputation of an order of magnitude and more compared to ordinary network\ntraining. Finally, we survey our work on neural architecture search which is\nalso an order of magnitude more resource demanding then ordinary network\ntraining.",
    "descriptor": "",
    "authors": [
      "Julian Burghoff",
      "Robin Chan",
      "Hanno Gottschalk",
      "Annika Muetze",
      "Tobias Riedlinger",
      "Matthias Rottmann",
      "Marius Schubert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14917"
  },
  {
    "id": "arXiv:2205.14919",
    "title": "A Deep Learning Approach for Automatic Detection of Qualitative Features  of Lecturing",
    "abstract": "Artificial Intelligence in higher education opens new possibilities for\nimproving the lecturing process, such as enriching didactic materials, helping\nin assessing students' works or even providing directions to the teachers on\nhow to enhance the lectures. We follow this research path, and in this work, we\nexplore how an academic lecture can be assessed automatically by quantitative\nfeatures. First, we prepare a set of qualitative features based on teaching\npractices and then annotate the dataset of academic lecture videos collected\nfor this purpose. We then show how these features could be detected\nautomatically using machine learning and computer vision techniques. Our\nresults show the potential usefulness of our work.",
    "descriptor": "\nComments: 10 pages, 9 figures\n",
    "authors": [
      "Anna Wroblewska",
      "Jozef Jasek",
      "Bogdan Jastrzebski",
      "Stanislaw Pawlak",
      "Anna Grzywacz",
      "Cheong Siew Ann",
      "Tan Seng Chee",
      "Tomasz Trzcinski",
      "Janusz Holyst"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.14919"
  },
  {
    "id": "arXiv:2205.14922",
    "title": "ACIL: Analytic Class-Incremental Learning with Absolute Memorization and  Privacy Protection",
    "abstract": "Class-incremental learning (CIL) learns a classification model with training\ndata of different classes arising progressively. Existing CIL either suffers\nfrom serious accuracy loss due to catastrophic forgetting, or invades data\nprivacy by revisiting used exemplars. Inspired by linear learning formulations,\nwe propose an analytic class-incremental learning (ACIL) with absolute\nmemorization of past knowledge while avoiding breaching of data privacy (i.e.,\nwithout storing historical data). The absolute memorization is demonstrated in\nthe sense that class-incremental learning using ACIL given present data would\ngive identical results to that from its joint-learning counterpart which\nconsumes both present and historical samples. This equality is theoretically\nvalidated. Data privacy is ensured since no historical data are involved during\nthe learning process. Empirical validations demonstrate ACIL's competitive\naccuracy performance with near-identical results for various incremental task\nsettings (e.g., 5-50 phases). This also allows ACIL to outperform the\nstate-of-the-art methods for large-phase scenarios (e.g., 25 and 50 phases).",
    "descriptor": "",
    "authors": [
      "Huiping Zhuang",
      "Zhenyu Weng",
      "Renchunzi Xie",
      "Kar-Ann Toh",
      "Zhiping Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14922"
  },
  {
    "id": "arXiv:2205.14925",
    "title": "The u-index: a simple metric to objectively measure academic impact of  individual researchers",
    "abstract": "This short paper introduces the u-index, a simple and objective metric to\nevaluate the impact and relevance of academic research output, as a possible\nalternative to widespread metrics such as the h-index or the i10-index. The\nproposed index is designed to address possible issues with standard metrics\nsuch as inflated ratings resulting from self-citations or by means of extended\nco-authorship numbers where every citation has the same impact as for works\nwritten by smaller groups, despite limited individual contributions. The new\nindex makes also possible to differentiate scholars who would otherwise fall\ninto the same h-index group, hence providing further insights into the actual\nimpact of a specific individual researcher.",
    "descriptor": "\nComments: 3 pages, 1 table, no figures\n",
    "authors": [
      "Roberto Dillon"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.14925"
  },
  {
    "id": "arXiv:2205.14926",
    "title": "CalFAT: Calibrated Federated Adversarial Training with Label Skewness",
    "abstract": "Recent studies have shown that, like traditional machine learning, federated\nlearning (FL) is also vulnerable to adversarial attacks. To improve the\nadversarial robustness of FL, few federated adversarial training (FAT) methods\nhave been proposed to apply adversarial training locally before global\naggregation. Although these methods demonstrate promising results on\nindependent identically distributed (IID) data, they suffer from training\ninstability issues on non-IID data with label skewness, resulting in much\ndegraded natural accuracy. This tends to hinder the application of FAT in\nreal-world applications where the label distribution across the clients is\noften skewed. In this paper, we study the problem of FAT under label skewness,\nand firstly reveal one root cause of the training instability and natural\naccuracy degradation issues: skewed labels lead to non-identical class\nprobabilities and heterogeneous local models. We then propose a Calibrated FAT\n(CalFAT) approach to tackle the instability issue by calibrating the logits\nadaptively to balance the classes. We show both theoretically and empirically\nthat the optimization of CalFAT leads to homogeneous local models across the\nclients and much improved convergence rate and final performance.",
    "descriptor": "",
    "authors": [
      "Chen Chen",
      "Yuchen Liu",
      "Xingjun Ma",
      "Lingjuan Lyu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14926"
  },
  {
    "id": "arXiv:2205.14927",
    "title": "Passively Measuring IPFS Churn and Network Size",
    "abstract": "The InterPlanetary File System~(IPFS) is a popular decentralized peer-to-peer\nnetwork for exchanging data. While there are many use cases for IPFS, the\nsuccess of these use cases depends on the network. In this paper, we provide a\npassive measurement study of the IPFS network, investigating peer dynamics and\ncuriosities of the network. With the help of our measurement, we estimate the\nnetwork size and confirm the results of previous active measurement studies.",
    "descriptor": "\nComments: Extended Preprint. Has been accepted for presentation at DINPS 2022\n",
    "authors": [
      "Erik Daniel",
      "Florian Tschorsch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14927"
  },
  {
    "id": "arXiv:2205.14929",
    "title": "Neural Volumetric Object Selection",
    "abstract": "We introduce an approach for selecting objects in neural volumetric 3D\nrepresentations, such as multi-plane images (MPI) and neural radiance fields\n(NeRF). Our approach takes a set of foreground and background 2D user scribbles\nin one view and automatically estimates a 3D segmentation of the desired\nobject, which can be rendered into novel views. To achieve this result, we\npropose a novel voxel feature embedding that incorporates the neural volumetric\n3D representation and multi-view image features from all input views. To\nevaluate our approach, we introduce a new dataset of human-provided\nsegmentation masks for depicted objects in real-world multi-view scene\ncaptures. We show that our approach out-performs strong baselines, including 2D\nsegmentation and 3D segmentation approaches adapted to our task.",
    "descriptor": "\nComments: CVPR 2022 camera ready\n",
    "authors": [
      "Zhongzheng Ren",
      "Aseem Agarwala",
      "Bryan Russell",
      "Alexander G. Schwing",
      "Oliver Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14929"
  },
  {
    "id": "arXiv:2205.14931",
    "title": "A multimedia recommendation model based on collaborative graph",
    "abstract": "As one of the main solutions to the information overload problem, recommender\nsystems are widely used in daily life. In the recent emerging micro-video\nrecommendation scenario, micro-videos contain rich multimedia information,\ninvolving text, image, video and other multimodal data, and these rich\nmultimodal information conceals users' deep interest in the items. Most of the\ncurrent recommendation algorithms based on multimodal data use multimodal\ninformation to expand the information on the item side, but ignore the\ndifferent preferences of users for different modal information, and lack the\nfine-grained mining of the internal connection of multimodal information. To\ninvestigate the problems in the micro-video recommendr system mentioned above,\nwe design a hybrid recommendation model based on multimodal information,\nintroduces multimodal information and user-side auxiliary information in the\nnetwork structure, fully explores the deep interest of users, measures the\nimportance of each dimension of user and item feature representation in the\nscoring prediction task, makes the application of graph neural network in the\nrecommendation system is improved by using an attention mechanism to fuse the\nmulti-layer state output information, allowing the shallow structural features\nprovided by the intermediate layer to better participate in the prediction\ntask. The recommendation accuracy is improved compared with the traditional\nrecommendation algorithm on different data sets, and the feasibility and\neffectiveness of our model is verified.",
    "descriptor": "",
    "authors": [
      "Breda Lim",
      "Shubhi Bansal",
      "Ahmed Buru",
      "Kayla Manthey"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.14931"
  },
  {
    "id": "arXiv:2205.14935",
    "title": "Deep Learning Methods for Fingerprint-Based Indoor Positioning: A Review",
    "abstract": "Outdoor positioning systems based on the Global Navigation Satellite System\nhave several shortcomings that have deemed their use for indoor positioning\nimpractical. Location fingerprinting, which utilizes machine learning, has\nemerged as a viable method and solution for indoor positioning due to its\nsimple concept and accurate performance. In the past, shallow learning\nalgorithms were traditionally used in location fingerprinting. Recently, the\nresearch community started utilizing deep learning methods for fingerprinting\nafter witnessing the great success and superiority these methods have over\ntraditional/shallow machine learning algorithms. This paper provides a\ncomprehensive review of deep learning methods in indoor positioning. First, the\nadvantages and disadvantages of various fingerprint types for indoor\npositioning are discussed. The solutions proposed in the literature are then\nanalyzed, categorized, and compared against various performance evaluation\nmetrics. Since data is key in fingerprinting, a detailed review of publicly\navailable indoor positioning datasets is presented. While incorporating deep\nlearning into fingerprinting has resulted in significant improvements, doing\nso, has also introduced new challenges. These challenges along with the common\nimplementation pitfalls are discussed. Finally, the paper is concluded with\nsome remarks as well as future research trends.",
    "descriptor": "",
    "authors": [
      "Fahad Alhomayani",
      "Mohammad H. Mahoor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14935"
  },
  {
    "id": "arXiv:2205.14937",
    "title": "Gathering despite a linear number of weakly Byzantine agents",
    "abstract": "We study the gathering problem to make multiple agents initially scattered in\narbitrary networks gather at a single node. There exist $k$ agents with unique\nidentifiers (IDs) in the network, and $f$ of them are weakly Byzantine agents,\nwhich behave arbitrarily except for falsifying their IDs. The agents behave in\nsynchronous rounds, and each node does not have any memory like a whiteboard.\nIn the literature, there exists a gathering algorithm that tolerates any number\nof Byzantine agents, while the fastest gathering algorithm requires\n$\\Omega(f^2)$ non-Byzantine agents. This paper proposes an algorithm that\nsolves the gathering problem efficiently with $\\Omega(f)$ non-Byzantine agents\nsince there is a large gap between the number of non-Byzantine agents in\nprevious works. The proposed algorithm achieves the gathering in\n$O(f\\cdot|\\Lambda_{good}|\\cdot X(N))$ rounds in case of $9f+8\\leq k$ and\nsimultaneous startup if $N$ is given to agents, where $|\\Lambda_{good}|$ is the\nlength of the largest ID among non-Byzantine agents, and $X(n)$ is the number\nof rounds required to explore any network composed of $n$ nodes. This algorithm\nis faster than the most fault-tolerant existing algorithm and requires fewer\nnon-Byzantine agents than the fastest algorithm if $n$ is given to agents,\nalthough the guarantees on simultaneous termination and startup delay are not\nthe same. To achieve this property, we propose a new technique to simulate a\nByzantine consensus algorithm for synchronous message-passing systems on agent\nsystems.",
    "descriptor": "\nComments: 29 pages, 2 figures, 2 tables, and 6 pseudo-codes\n",
    "authors": [
      "Jion Hirose",
      "Junya Nakamura",
      "Fukuhito Ooshita",
      "Michiko Inoue"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14937"
  },
  {
    "id": "arXiv:2205.14938",
    "title": "Harnessing spectral representations for subgraph alignment",
    "abstract": "With the rise and advent of graph learning techniques, graph data has become\nubiquitous. However, while several efforts are being devoted to the design of\nnew convolutional architectures, pooling or positional encoding schemes, less\neffort is being spent on problems involving maps between (possibly very large)\ngraphs, such as signal transfer, graph isomorphism and subgraph correspondence.\nWith this paper, we anticipate the need for a convenient framework to deal with\nsuch problems, and focus in particular on the challenging subgraph alignment\nscenario. We claim that, first and foremost, the representation of a map plays\na central role on how these problems should be modeled. Taking the hint from\nrecent work in geometry processing, we propose the adoption of a spectral\nrepresentation for maps that is compact, easy to compute, robust to topological\nchanges, easy to plug into existing pipelines, and is especially effective for\nsubgraph alignment problems. We report for the first time a surprising\nphenomenon where the partiality arising in the subgraph alignment task is\nmanifested as a special structure of the map coefficients, even in the absence\nof exact subgraph isomorphism, and which is consistently observed over\ndifferent families of graphs up to several thousand nodes.",
    "descriptor": "",
    "authors": [
      "Marco Pegoraro",
      "Riccardo Marin",
      "Arianna Rampini",
      "Simone Melzi",
      "Luca Cosmo",
      "Emaneule Rodol\u00e0"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14938"
  },
  {
    "id": "arXiv:2205.14940",
    "title": "Detecting Unknown DGAs without Context Information",
    "abstract": "New malware emerges at a rapid pace and often incorporates Domain Generation\nAlgorithms (DGAs) to avoid blocking the malware's connection to the command and\ncontrol (C2) server. Current state-of-the-art classifiers are able to separate\nbenign from malicious domains (binary classification) and attribute them with\nhigh probability to the DGAs that generated them (multiclass classification).\nWhile binary classifiers can label domains of yet unknown DGAs as malicious,\nmulticlass classifiers can only assign domains to DGAs that are known at the\ntime of training, limiting the ability to uncover new malware families. In this\nwork, we perform a comprehensive study on the detection of new DGAs, which\nincludes an evaluation of 59,690 classifiers. We examine four different\napproaches in 15 different configurations and propose a simple yet effective\napproach based on the combination of a softmax classifier and regular\nexpressions (regexes) to detect multiple unknown DGAs with high probability. At\nthe same time, our approach retains state-of-the-art classification performance\nfor known DGAs. Our evaluation is based on a leave-one-group-out\ncross-validation with a total of 94 DGA families. By using the maximum number\nof known DGAs, our evaluation scenario is particularly difficult and close to\nthe real world. All of the approaches examined are privacy-preserving, since\nthey operate without context and exclusively on a single domain to be\nclassified. We round up our study with a thorough discussion of\nclass-incremental learning strategies that can adapt an existing classifier to\nnewly discovered classes.",
    "descriptor": "\nComments: Accepted at The 17th International Conference on Availability, Reliability and Security (ARES 2022)\n",
    "authors": [
      "Arthur Drichel",
      "Justus von Brandt",
      "Ulrike Meyer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14940"
  },
  {
    "id": "arXiv:2205.14942",
    "title": "Edge YOLO: Real-Time Intelligent Object Detection System Based on  Edge-Cloud Cooperation in Autonomous Vehicles",
    "abstract": "Driven by the ever-increasing requirements of autonomous vehicles, such as\ntraffic monitoring and driving assistant, deep learning-based object detection\n(DL-OD) has been increasingly attractive in intelligent transportation systems.\nHowever, it is difficult for the existing DL-OD schemes to realize the\nresponsible, cost-saving, and energy-efficient autonomous vehicle systems due\nto low their inherent defects of low timeliness and high energy consumption. In\nthis paper, we propose an object detection (OD) system based on edge-cloud\ncooperation and reconstructive convolutional neural networks, which is called\nEdge YOLO. This system can effectively avoid the excessive dependence on\ncomputing power and uneven distribution of cloud computing resources.\nSpecifically, it is a lightweight OD framework realized by combining pruning\nfeature extraction network and compression feature fusion network to enhance\nthe efficiency of multi-scale prediction to the largest extent. In addition, we\ndeveloped an autonomous driving platform equipped with NVIDIA Jetson for\nsystem-level verification. We experimentally demonstrate the reliability and\nefficiency of Edge YOLO on COCO2017 and KITTI data sets, respectively.\nAccording to COCO2017 standard datasets with a speed of 26.6 frames per second\n(FPS), the results show that the number of parameters in the entire network is\nonly 25.67 MB, while the accuracy (mAP) is up to 47.3%.",
    "descriptor": "",
    "authors": [
      "Siyuan Liang",
      "Hao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14942"
  },
  {
    "id": "arXiv:2205.14943",
    "title": "Data-driven Numerical Invariant Synthesis with Automatic Generation of  Attributes",
    "abstract": "We propose a data-driven algorithm for numerical invariant synthesis and\nverification. The algorithm is based on the ICE-DT schema for learning decision\ntrees from samples that include positive and negative states and additionally\nimplications corresponding to transitions in the program. The main issue we\naddress is the discovery of relevant attributes to be used in the learning\nprocess of numerical invariants. We define a method for solving this problem\nthat is guided by the data sample. It is based on the construction of a\nseparator that covers positive states without including negative ones, and that\nis consistent with the implications. The separator is constructed using an\nabstract domain representation of convex sets. The generalization mechanism of\nthe decision tree learning from the constraints of the separator allows the\ninference of general invariants, yet accurate enough for proving the targeted\nproperty. We implemented our algorithm and showed its efficiency.",
    "descriptor": "",
    "authors": [
      "Ahmed Bouajjani",
      "Wael-Amine Boutglay",
      "Peter Habermehl"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14943"
  },
  {
    "id": "arXiv:2205.14949",
    "title": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling",
    "abstract": "Recently, masked image modeling (MIM) has offered a new methodology of\nself-supervised pre-training of vision transformers. A key idea of efficient\nimplementation is to discard the masked image patches (or tokens) throughout\nthe target network (encoder), which requires the encoder to be a plain vision\ntransformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin\nTransformer) have potentially better properties in formulating vision inputs.\nIn this paper, we offer a new design of hierarchical vision transformers named\nHiViT (short for Hierarchical ViT) that enjoys both high efficiency and good\nperformance in MIM. The key is to remove the unnecessary \"local inter-unit\noperations\", deriving structurally simple hierarchical vision transformers in\nwhich mask-units can be serialized like plain vision transformers. For this\npurpose, we start with Swin Transformer and (i) set the masking unit size to be\nthe token size in the main stage of Swin Transformer, (ii) switch off\ninter-unit self-attentions before the main stage, and (iii) eliminate all\noperations after the main stage. Empirical studies demonstrate the advantageous\nperformance of HiViT in terms of fully-supervised, self-supervised, and\ntransfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B\nreports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over\nSwin-B, and the performance gain generalizes to downstream tasks of detection\nand segmentation. Code will be made publicly available.",
    "descriptor": "",
    "authors": [
      "Xiaosong Zhang",
      "Yunjie Tian",
      "Wei Huang",
      "Qixiang Ye",
      "Qi Dai",
      "Lingxi Xie",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14949"
  },
  {
    "id": "arXiv:2205.14950",
    "title": "QB-II for Evaluating the Reliability of Binary-State Networks",
    "abstract": "Current real-life applications of various networks such as utility (gas,\nwater, electric, 4G/5G) networks, the Internet of Things, social networks, and\nsupply chains. Reliability is one of the most popular tools for evaluating\nnetwork performance. The fundamental structure of these networks is a binary\nstate network. Distinctive methods have been proposed to efficiently assess\nbinary-state network reliability. A new algorithm called QB-II (quick\nbinary-addition tree algorithm II) is proposed to improve the efficiency of\nquick BAT, which is based on BAT and outperforms many algorithms. The proposed\nQB-II implements the shortest minimum cuts (MCs) to separate the entire BAT\ninto main-BAT and sub-BATs, and the source-target matrix convolution products\nto connect these subgraphs intelligently to improve the efficiency. Twenty\nbenchmark problems were used to validate the performance of the QB-II.",
    "descriptor": "",
    "authors": [
      "Wei-Chang Yeh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.14950"
  },
  {
    "id": "arXiv:2205.14951",
    "title": "Benchmarking the Robustness of LiDAR-Camera Fusion for 3D Object  Detection",
    "abstract": "There are two critical sensors for 3D perception in autonomous driving, the\ncamera and the LiDAR. The camera provides rich semantic information such as\ncolor, texture, and the LiDAR reflects the 3D shape and locations of\nsurrounding objects. People discover that fusing these two modalities can\nsignificantly boost the performance of 3D perception models as each modality\nhas complementary information to the other. However, we observe that current\ndatasets are captured from expensive vehicles that are explicitly designed for\ndata collection purposes, and cannot truly reflect the realistic data\ndistribution due to various reasons. To this end, we collect a series of\nreal-world cases with noisy data distribution, and systematically formulate a\nrobustness benchmark toolkit, that simulates these cases on any clean\nautonomous driving datasets. We showcase the effectiveness of our toolkit by\nestablishing the robustness benchmark on two widely-adopted autonomous driving\ndatasets, nuScenes and Waymo, then, to the best of our knowledge, holistically\nbenchmark the state-of-the-art fusion methods for the first time. We observe\nthat: i) most fusion methods, when solely developed on these data, tend to fail\ninevitably when there is a disruption to the LiDAR input; ii) the improvement\nof the camera input is significantly inferior to the LiDAR one. We further\npropose an efficient robust training strategy to improve the robustness of the\ncurrent fusion method. The benchmark and code are available at\nhttps://github.com/kcyu2014/lidar-camera-robust-benchmark",
    "descriptor": "\nComments: Technical report. The first three authors contribute equally\n",
    "authors": [
      "Kaicheng Yu",
      "Tang Tao",
      "Hongwei Xie",
      "Zhiwei Lin",
      "Zhongwei Wu",
      "Zhongyu Xia",
      "Tingting Liang",
      "Haiyang Sun",
      "Jiong Deng",
      "Dayang Hao",
      "Yongtao Wang",
      "Xiaodan Liang",
      "Bing Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14951"
  },
  {
    "id": "arXiv:2205.14953",
    "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem",
    "abstract": "Large sequence model (SM) such as GPT series and BERT has displayed\noutstanding performance and generalization capabilities on vision, language,\nand recently reinforcement learning tasks. A natural follow-up question is how\nto abstract multi-agent decision making into an SM problem and benefit from the\nprosperous development of SMs. In this paper, we introduce a novel architecture\nnamed Multi-Agent Transformer (MAT) that effectively casts cooperative\nmulti-agent reinforcement learning (MARL) into SM problems wherein the task is\nto map agents' observation sequence to agents' optimal action sequence. Our\ngoal is to build the bridge between MARL and SMs so that the modeling power of\nmodern sequence models can be unleashed for MARL. Central to our MAT is an\nencoder-decoder architecture which leverages the multi-agent advantage\ndecomposition theorem to transform the joint policy search problem into a\nsequential decision making process; this renders only linear time complexity\nfor multi-agent problems and, most importantly, endows MAT with monotonic\nperformance improvement guarantee. Unlike prior arts such as Decision\nTransformer fit only pre-collected offline data, MAT is trained by online\ntrials and errors from the environment in an on-policy fashion. To validate\nMAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo,\nDexterous Hands Manipulation, and Google Research Football benchmarks. Results\ndemonstrate that MAT achieves superior performance and data efficiency compared\nto strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that\nMAT is an excellent few-short learner on unseen tasks regardless of changes in\nthe number of agents. See our project page at\nhttps://sites.google.com/view/multi-agent-transformer.",
    "descriptor": "",
    "authors": [
      "Muning Wen",
      "Jakub Grudzien Kuba",
      "Runji Lin",
      "Weinan Zhang",
      "Ying Wen",
      "Jun Wang",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14953"
  },
  {
    "id": "arXiv:2205.14955",
    "title": "AttentionCode: Ultra-Reliable Feedback Codes for Short-Packet  Communications",
    "abstract": "Ultra-reliable short-packet communication is a major challenge in future\nwireless networks with critical applications. To achieve ultra-reliable\ncommunications beyond 99.999%, this paper envisions a new interaction-based\ncommunication paradigm that exploits the feedback from the receiver for the\nsixth generation (6G) communication networks and beyond. We present\nAttentionCode, a new class of feedback codes leveraging deep learning (DL)\ntechnologies. The underpinnings of AttentionCode are three architectural\ninnovations: AttentionNet, input restructuring, and adaptation to fading\nchannels, accompanied by several training methods, including large-batch\ntraining, distributed learning, look-ahead optimizer, training-test\nsignal-to-noise ratio (SNR) mismatch, and curriculum learning. The training\nmethods can potentially be generalized to other wireless communication\napplications with machine learning. Numerical experiments verify that\nAttentionCode establishes a new state of the art among all DL-based feedback\ncodes in both additive white Gaussian noise (AWGN) channels and fading\nchannels. In AWGN channels with noiseless feedback, for example, AttentionCode\nachieves a block error rate (BLER) of $10^{-7}$ when the forward channel SNR is\n0dB for a block size of 50 bits, demonstrating the potential of AttentionCode\nto provide ultra-reliable short-packet communications for 6G.",
    "descriptor": "\nComments: Ultra-reliable short-packet communications, feedback, deep learning, the attention mechanism, 6G. 14 pages, 11 figures\n",
    "authors": [
      "Yulin Shao",
      "Emre Ozfatura",
      "Alberto Perotti",
      "Branislav Popovic",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14955"
  },
  {
    "id": "arXiv:2205.14959",
    "title": "Dataset Condensation via Efficient Synthetic-Data Parameterization",
    "abstract": "The great success of machine learning with massive amounts of data comes at a\nprice of huge computation costs and storage for training and tuning. Recent\nstudies on dataset condensation attempt to reduce the dependence on such\nmassive data by synthesizing a compact training dataset. However, the existing\napproaches have fundamental limitations in optimization due to the limited\nrepresentability of synthetic datasets without considering any data regularity\ncharacteristics. To this end, we propose a novel condensation framework that\ngenerates multiple synthetic data with a limited storage budget via efficient\nparameterization considering data regularity. We further analyze the\nshortcomings of the existing gradient matching-based condensation methods and\ndevelop an effective optimization technique for improving the condensation of\ntraining data information. We propose a unified algorithm that drastically\nimproves the quality of condensed data against the current state-of-the-art on\nCIFAR-10, ImageNet, and Speech Commands.",
    "descriptor": "\nComments: ICML 2022; Codes at this https URL\n",
    "authors": [
      "Jang-Hyun Kim",
      "Jinuk Kim",
      "Seong Joon Oh",
      "Sangdoo Yun",
      "Hwanjun Song",
      "Joonhyun Jeong",
      "Jung-Woo Ha",
      "Hyun Oh Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14959"
  },
  {
    "id": "arXiv:2205.14960",
    "title": "FedAUXfdp: Differentially Private One-Shot Federated Distillation",
    "abstract": "Federated learning suffers in the case of non-iid local datasets, i.e., when\nthe distributions of the clients' data are heterogeneous. One promising\napproach to this challenge is the recently proposed method FedAUX, an\naugmentation of federated distillation with robust results on even highly\nheterogeneous client data. FedAUX is a partially $(\\epsilon,\n\\delta)$-differentially private method, insofar as the clients' private data is\nprotected in only part of the training it takes part in. This work contributes\na fully differentially private extension, termed FedAUXfdp. In experiments with\ndeep networks on large-scale image datasets, FedAUXfdp with strong differential\nprivacy guarantees performs significantly better than other equally privatized\nSOTA baselines on non-iid client data in just a single communication round.\nFull privatization results in a negligible reduction in accuracy at all levels\nof data heterogeneity.",
    "descriptor": "",
    "authors": [
      "Haley Hoech",
      "Roman Rischke",
      "Karsten M\u00fcller",
      "Wojciech Samek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14960"
  },
  {
    "id": "arXiv:2205.14962",
    "title": "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks",
    "abstract": "Obtaining the energy of molecular systems typically requires solving the\nassociated Schr\\\"odinger equation. Unfortunately, analytical solutions only\nexist for single-electron systems, and accurate approximate solutions are\nexpensive. In recent work, the potential energy surface network (PESNet) has\nbeen proposed to reduce training time by solving the Schr\\\"odinger equation for\nmany geometries simultaneously. While training significantly faster, inference\nstill required numerical integration limiting the evaluation to a few\ngeometries. Here, we address the inference shortcomings by proposing the\nPotential learning from ab-initio Networks (PlaNet) framework to simultaneously\ntrain a surrogate model that avoids expensive Monte-Carlo integration and,\nthus, reduces inference time from minutes or even hours to milliseconds. In\nthis way, we can accurately model high-resolution multi-dimensional energy\nsurfaces that previously would have been unobtainable via neural wave\nfunctions. Finally, we present PESNet++, an architectural improvement to\nPESNet, that reduces errors by up to 39% and provides new state-of-the-art\nresults for neural wave functions across all systems evaluated.",
    "descriptor": "",
    "authors": [
      "Nicholas Gao",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14962"
  },
  {
    "id": "arXiv:2205.14964",
    "title": "Effectiveness and Scalability of Fuzzing Techniques in CI/CD Pipelines",
    "abstract": "Fuzzing has proven to be a fundamental technique to automated software\ntesting but also a costly one. With the increased adoption of CI/CD practices\nin software development, a natural question to ask is `What are the best ways\nto integrate fuzzing into CI/CD pipelines considering the velocity in code\nchanges and the automated delivery/deployment practices?'. Indeed, a recent\nstudy by B\\\"ohme and Zhu shows that four in every five bugs have been\nintroduced by recent code changes (i.e. regressions). In this paper, we take a\nclose look at the integration of fuzzers to CI/CD pipelines from both automated\nsoftware testing and continuous development angles. Firstly, we study an\noptimization opportunity to triage commits that do not require fuzzing and\nfind, through experimental analysis, that the average fuzzing effort in CI/CD\ncan be reduced by ~63% in three of the nine libraries we analyzed (>40% for six\nlibraries). Secondly, we investigate the impact of fuzzing campaign duration on\nthe CI/CD process: A shorter fuzzing campaign such as 15 minutes (as opposed to\nthe wisdom of 24 hours in the field) facilitates a faster pipeline and can\nstill uncover important bugs, but may also reduce its capability to detect\nsophisticated bugs. Lastly, we discuss a prioritization strategy that\nautomatically assigns resources to fuzzing campaigns based on a set of\npredefined priority strategies. Our findings suggest that continuous fuzzing\n(as part of the automated testing in CI/CD) is indeed beneficial and there are\nmany optimization opportunities to improve the effectiveness and scalability of\nfuzz testing.",
    "descriptor": "\nComments: 12 pages, 5 figures, submitted to ASE'22\n",
    "authors": [
      "Thijs Klooster",
      "Fatih Turkmen",
      "Gerben Broenink",
      "Ruben ten Hove",
      "Marcel B\u00f6hme"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14964"
  },
  {
    "id": "arXiv:2205.14965",
    "title": "PSNet: Fast Data Structuring for Hierarchical Deep Learning on Point  Cloud",
    "abstract": "In order to retain more feature information of local areas on a point cloud,\nlocal grouping and subsampling are the necessary data structuring steps in most\nhierarchical deep learning models. Due to the disorder nature of the points in\na point cloud, the significant time cost may be consumed when grouping and\nsubsampling the points, which consequently results in poor scalability. This\npaper proposes a fast data structuring method called PSNet (Point Structuring\nNet). PSNet transforms the spatial features of the points and matches them to\nthe features of local areas in a point cloud. PSNet achieves grouping and\nsampling at the same time while the existing methods process sampling and\ngrouping in two separate steps (such as using FPS plus kNN). PSNet performs\nfeature transformation pointwise while the existing methods uses the spatial\nrelationship among the points as the reference for grouping. Thanks to these\nfeatures, PSNet has two important advantages: 1) the grouping and sampling\nresults obtained by PSNet is stable and permutation invariant; and 2) PSNet can\nbe easily parallelized. PSNet can replace the data structuring methods in the\nmainstream point cloud deep learning models in a plug-and-play manner. We have\nconducted extensive experiments. The results show that PSNet can improve the\ntraining and reasoning speed significantly while maintaining the model\naccuracy.",
    "descriptor": "",
    "authors": [
      "Luyang Li",
      "Ligang He",
      "Jinjin Gao",
      "Xie Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14965"
  },
  {
    "id": "arXiv:2205.14966",
    "title": "The Source Stabilized Galerkin Formulation for Linear Moving Conductor  Problems with Edge Elements",
    "abstract": "The phenomenon of linear motion of conductor in a magnetic field is commonly\nfound in electric machineries such as, electromagnetic brakes, linear induction\nmotor, electromagnetic flowmeter etc. The design and analysis of the same\nrequires an accurate evaluation of induced currents and the associated reaction\nmagnetic fields. The finite element method is a generally employed numerical\ntechnique for this purpose. However, it needs stabilization techniques to\nprovide an accurate solution. In this work, such a stabilization technique is\ndeveloped for the edge elements. The stability and hence the accuracy is\nbrought in by a suitable representation of the source term. The stability and\naccuracy of the proposed scheme is first shown analytically and then\ndemonstrated with the help of 2D and 3D simulations.",
    "descriptor": "",
    "authors": [
      "Sujata Bhowmick",
      "Sethupathy Subramanian"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14966"
  },
  {
    "id": "arXiv:2205.14967",
    "title": "Universally-Optimal Distributed Exact Min-Cut",
    "abstract": "We present a universally-optimal distributed algorithm for the exact weighted\nmin-cut. The algorithm is guaranteed to complete in $\\widetilde{O}(D +\n\\sqrt{n})$ rounds on every graph, recovering the recent result of Dory, Efron,\nMukhopadhyay, and Nanongkai~[STOC'21], but runs much faster on structured\ngraphs. Specifically, the algorithm completes in $\\widetilde{O}(D)$ rounds on\n(weighted) planar graphs or, more generally, any (weighted) excluded-minor\nfamily.\nWe obtain this result by designing an aggregation-based algorithm: each node\nreceives only an aggregate of the messages sent to it. While somewhat\nrestrictive, recent work shows any such black-box algorithm can be simulated on\nany minor of the communication network. Furthermore, we observe this also\nallows for the addition of (a small number of) arbitrarily-connected virtual\nnodes to the network. We leverage these capabilities to design a min-cut\nalgorithm that is significantly simpler compared to prior distributed work. We\nhope this paper showcases how working within this paradigm yields\nsimple-to-design and ultra-efficient distributed algorithms for global\nproblems.\nOur main technical contribution is a distributed algorithm that, given any\ntree $T$, computes the minimum cut that $2$-respects $T$ (i.e., cuts at most\n$2$ edges of $T$) in universally near-optimal time. Moreover, our algorithm\ngives a \\emph{deterministic} $\\widetilde{O}(D)$-round 2-respecting cut solution\nfor excluded-minor families and a \\emph{deterministic} $\\widetilde{O}(D +\n\\sqrt{n})$-round solution for general graphs, the latter resolving a question\nof Dory, et al.~[STOC'21]",
    "descriptor": "\nComments: 34 pages, accepted to PODC 2022\n",
    "authors": [
      "Mohsen Ghaffari",
      "Goran Zuzic"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14967"
  },
  {
    "id": "arXiv:2205.14968",
    "title": "Two improvements of the foliation based quad meshing method",
    "abstract": "Quadrilateral meshes with high level structure and feature preserving\nproperty benefit industrial applications the most. Generation of such quad mesh\nremains a challenge. Quad meshes generated using surface foliation have the\nhighest level structure, however they lack of the feature preserving ability.\nIn this paper, we analyze the boundary curvature with Gauss-Bonnet theorem to\ndetermine whether a boundary rectangle corner preserving foliation based method\nexists. When it exists, we adopt a modified double cover technique together\nwith surface foliation method to generate a corner feature preserving quad\nmesh. The experiments demonstrate the efficacy of our algorithm.",
    "descriptor": "",
    "authors": [
      "Xiaopeng Zheng",
      "Hao Wang",
      "Peng Zheng",
      "Zhihui Qi",
      "Zhijun Cheng",
      "Ru Lin",
      "Jiayou Zhao"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.14968"
  },
  {
    "id": "arXiv:2205.14969",
    "title": "Guided Diffusion Model for Adversarial Purification",
    "abstract": "With wider application of deep neural networks (DNNs) in various algorithms\nand frameworks, security threats have become one of the concerns. Adversarial\nattacks disturb DNN-based image classifiers, in which attackers can\nintentionally add imperceptible adversarial perturbations on input images to\nfool the classifiers. In this paper, we propose a novel purification approach,\nreferred to as guided diffusion model for purification (GDMP), to help protect\nclassifiers from adversarial attacks. The core of our approach is to embed\npurification into the diffusion denoising process of a Denoised Diffusion\nProbabilistic Model (DDPM), so that its diffusion process could submerge the\nadversarial perturbations with gradually added Gaussian noises, and both of\nthese noises can be simultaneously removed following a guided denoising\nprocess. On our comprehensive experiments across various datasets, the proposed\nGDMP is shown to reduce the perturbations raised by adversarial attacks to a\nshallow range, thereby significantly improving the correctness of\nclassification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under\nPGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on\nthe challenging ImageNet dataset.",
    "descriptor": "",
    "authors": [
      "Jinyi Wang",
      "Zhaoyang Lyu",
      "Dahua Lin",
      "Bo Dai",
      "Hongfei Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14969"
  },
  {
    "id": "arXiv:2205.14970",
    "title": "Towards Personalized Bundle Creative Generation with Contrastive  Non-Autoregressive Decoding",
    "abstract": "Current bundle generation studies focus on generating a combination of items\nto improve user experience. In real-world applications, there is also a great\nneed to produce bundle creatives that consist of mixture types of objects\n(e.g., items, slogans and templates) for achieving better promotion effect. We\nstudy a new problem named bundle creative generation: for given users, the goal\nis to generate personalized bundle creatives that the users will be interested\nin. To take both quality and efficiency into account, we propose a contrastive\nnon-autoregressive model that captures user preferences with ingenious decoding\nobjective. Experiments on large-scale real-world datasets verify that our\nproposed model shows significant advantages in terms of creative quality and\ngeneration speed.",
    "descriptor": "\nComments: SIGIR 2022 (short)\n",
    "authors": [
      "Penghui Wei",
      "Shaoguo Liu",
      "Xuanhua Yang",
      "Liang Wang",
      "Bo Zheng"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.14970"
  },
  {
    "id": "arXiv:2205.14971",
    "title": "Knowledge Distillation for 6D Pose Estimation by Keypoint Distribution  Alignment",
    "abstract": "Knowledge distillation facilitates the training of a compact student network\nby using a deep teacher one. While this has achieved great success in many\ntasks, it remains completely unstudied for image-based 6D object pose\nestimation. In this work, we introduce the first knowledge distillation method\nfor 6D pose estimation. Specifically, we follow a standard approach to 6D pose\nestimation, consisting of predicting the 2D image locations of object\nkeypoints. In this context, we observe the compact student network to struggle\npredicting precise 2D keypoint locations. Therefore, to address this, instead\nof training the student with keypoint-to-keypoint supervision, we introduce a\nstrategy based the optimal transport theory that distills the teacher's\nkeypoint \\emph{distribution} into the student network, facilitating its\ntraining. Our experiments on several benchmarks show that our distillation\nmethod yields state-of-the-art results with different compact student models.",
    "descriptor": "",
    "authors": [
      "Shuxuan Guo",
      "Yinlin Hu",
      "Jose M. Alvarez",
      "Mathieu Salzmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14971"
  },
  {
    "id": "arXiv:2205.14976",
    "title": "Rethinking Saliency Map: An Context-aware Perturbation Method to Explain  EEG-based Deep Learning Model",
    "abstract": "Deep learning is widely used to decode the electroencephalogram (EEG) signal.\nHowever, there are few attempts to specifically investigate how to explain the\nEEG-based deep learning models. We conduct a review to summarize the existing\nworks explaining the EEG-based deep learning model. Unfortunately, we find that\nthere is no appropriate method to explain them. Based on the characteristic of\nEEG data, we suggest a context-aware perturbation method to generate a saliency\nmap from the perspective of the raw EEG signal. Moreover, we also justify that\nthe context information can be used to suppress the artifacts in the EEG-based\ndeep learning model. In practice, some users might want a simple version of the\nexplanation, which only indicates a few features as salient points. To this\nend, we propose an optional area limitation strategy to restrict the\nhighlighted region. To validate our idea and make a comparison with the other\nmethods, we select three representative EEG-based models to implement\nexperiments on the emotional EEG dataset DEAP. The results of the experiments\nsupport the advantages of our method.",
    "descriptor": "",
    "authors": [
      "Hanqi Wang",
      "Xiaoguang Zhu",
      "Tao Chen",
      "Chengfang Li",
      "Liang Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14976"
  },
  {
    "id": "arXiv:2205.14978",
    "title": "Approximating k-Edge-Connected Spanning Subgraphs via a Near-Linear Time  LP Solver",
    "abstract": "In the $k$-edge-connected spanning subgraph ($k$ECSS) problem, our goal is to\ncompute a minimum-cost sub-network that is resilient against up to $k$ link\nfailures: Given an $n$-node $m$-edge graph with a cost function on the edges,\nour goal is to compute a minimum-cost $k$-edge-connected spanning subgraph.\nThis NP-hard problem generalizes the minimum spanning tree problem and is the\n\"uniform case\" of a much broader class of survival network design problems\n(SNDP). A factor of two has remained the best approximation ratio for\npolynomial-time algorithms for the whole class of SNDP, even for a special case\nof $2$ECSS. The fastest $2$-approximation algorithm is however rather slow,\ntaking $O(mn k)$ time [Khuller, Vishkin, STOC'92]. A faster time complexity of\n$O(n^2)$ can be obtained, but with a higher approximation guarantee of $(2k-1)$\n[Gabow, Goemans, Williamson, IPCO'93].\nOur main contribution is an algorithm that $(1+\\epsilon)$-approximates the\noptimal fractional solution in $\\tilde O(m/\\epsilon^2)$ time (independent of\n$k$), which can be turned into a $(2+\\epsilon)$ approximation algorithm that\nruns in time $\\tilde O\\left(\\frac{m}{\\epsilon^2} +\n\\frac{k^2n^{1.5}}{\\epsilon^2}\\right)$ for (integral) $k$ECSS; this improves the\nrunning time of the aforementioned results while keeping the approximation\nratio arbitrarily close to a factor of two.",
    "descriptor": "",
    "authors": [
      "Parinya Chalermsook",
      "Chien-Chung Huang",
      "Danupon Nanongkai",
      "Thatchaphol Saranurak",
      "Pattara Sukprasert",
      "Sorrachai Yingchareonthawornchai"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14978"
  },
  {
    "id": "arXiv:2205.14981",
    "title": "ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual  Open-retrieval Question Answering System",
    "abstract": "This paper introduces our proposed system for the MIA Shared Task on\nCross-lingual Open-retrieval Question Answering (COQA). In this challenging\nscenario, given an input question the system has to gather evidence documents\nfrom a multilingual pool and generate from them an answer in the language of\nthe question. We devised several approaches combining different model variants\nfor three main components: Data Augmentation, Passage Retrieval, and Answer\nGeneration. For passage retrieval, we evaluated the monolingual BM25 ranker\nagainst the ensemble of re-rankers based on multilingual pretrained language\nmodels (PLMs) and also variants of the shared task baseline, re-training it\nfrom scratch using a recently introduced contrastive loss that maintains a\nstrong gradient signal throughout training by means of mixed negative samples.\nFor answer generation, we focused on language- and domain-specialization by\nmeans of continued language model (LM) pretraining of existing multilingual\nencoders. Additionally, for both passage retrieval and answer generation, we\naugmented the training data provided by the task organizers with automatically\ngenerated question-answer pairs created from Wikipedia passages to mitigate the\nissue of data scarcity, particularly for the low-resource languages for which\nno training data were provided. Our results show that language- and\ndomain-specialization as well as data augmentation help, especially for\nlow-resource languages.",
    "descriptor": "",
    "authors": [
      "Chia-Chien Hung",
      "Tommaso Green",
      "Robert Litschko",
      "Tornike Tsereteli",
      "Sotaro Takeshita",
      "Marco Bombieri",
      "Goran Glava\u0161",
      "Simone Paolo Ponzetto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14981"
  },
  {
    "id": "arXiv:2205.14986",
    "title": "GMML is All you Need",
    "abstract": "Vision transformers have generated significant interest in the computer\nvision community because of their flexibility in exploiting contextual\ninformation, whether it is sharply confined local, or long range global.\nHowever, they are known to be data hungry. This has motivated the research in\nself-supervised transformer pretraining, which does not need to decode the\nsemantic information conveyed by labels to link it to the image properties, but\nrather focuses directly on extracting a concise representation of the image\ndata that reflects the notion of similarity, and is invariant to nuisance\nfactors. The key vehicle for the self-learning process used by the majority of\nself-learning methods is the generation of multiple views of the training data\nand the creation of pretext tasks which use these views to define the notion of\nimage similarity, and data integrity. However, this approach lacks the natural\npropensity to extract contextual information. We propose group masked model\nlearning (GMML), a self-supervised learning (SSL) mechanism for pretraining\nvision transformers with the ability to extract the contextual information\npresent in all the concepts in an image. GMML achieves this by manipulating\nrandomly groups of connected tokens, ensuingly covering a meaningful part of a\nsemantic concept, and then recovering the hidden semantic information from the\nvisible part of the concept. GMML implicitly introduces a novel data\naugmentation process. Unlike most of the existing SSL approaches, GMML does not\nrequire momentum encoder, nor rely on careful implementation details such as\nlarge batches and gradient stopping, which are all artefacts of most of the\ncurrent self-supervised learning techniques. The source code is publicly\navailable for the community to train on bigger corpora:\nhttps://github.com/Sara-Ahmed/GMML.",
    "descriptor": "",
    "authors": [
      "Sara Atito",
      "Muhammad Awais",
      "Josef Kittler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14986"
  },
  {
    "id": "arXiv:2205.14988",
    "title": "Quantum Multi-Armed Bandits and Stochastic Linear Bandits Enjoy  Logarithmic Regrets",
    "abstract": "Multi-arm bandit (MAB) and stochastic linear bandit (SLB) are important\nmodels in reinforcement learning, and it is well-known that classical\nalgorithms for bandits with time horizon $T$ suffer $\\Omega(\\sqrt{T})$ regret.\nIn this paper, we study MAB and SLB with quantum reward oracles and propose\nquantum algorithms for both models with $O(\\mbox{poly}(\\log T))$ regrets,\nexponentially improving the dependence in terms of $T$. To the best of our\nknowledge, this is the first provable quantum speedup for regrets of bandit\nproblems and in general exploitation in reinforcement learning. Compared to\nprevious literature on quantum exploration algorithms for MAB and reinforcement\nlearning, our quantum input model is simpler and only assumes quantum oracles\nfor each individual arm.",
    "descriptor": "\nComments: 14+6 pages\n",
    "authors": [
      "Zongqi Wan",
      "Zhijie Zhang",
      "Tongyang Li",
      "Jialin Zhang",
      "Xiaoming Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14988"
  },
  {
    "id": "arXiv:2205.14989",
    "title": "Combining E-Graphs with Abstract Interpretation",
    "abstract": "E-graphs are a data structure that compactly represents equivalent\nexpressions. They are constructed via the repeated application of rewrite\nrules. Often in practical applications, conditional rewrite rules are crucial,\nbut their application requires the detection -- at the time the e-graph is\nbeing built -- that a condition is valid in the domain of application.\nDetecting condition validity amounts to proving a property of the program.\nAbstract interpretation is a general method to learn such properties,\ntraditionally used in static analysis tools. We demonstrate that abstract\ninterpretation and e-graph analysis naturally reinforce each other through a\ntight integration because (i) the e-graph clustering of equivalent expressions\ninduces natural precision refinement of abstractions and (ii) precise\nabstractions allow the application of deeper rewrite rules (and hence\npotentially even greater precision). We develop the theory behind this\nintuition and present an exemplar interval arithmetic implementation, which we\napply to the FPBench suite of benchmarks.",
    "descriptor": "",
    "authors": [
      "Samuel Coward",
      "George A. Constantinides",
      "Theo Drane"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14989"
  },
  {
    "id": "arXiv:2205.14999",
    "title": "CompleteDT: Point Cloud Completion with Dense Augment Inference  Transformers",
    "abstract": "Point cloud completion task aims to predict the missing part of incomplete\npoint clouds and generate complete point clouds with details. In this paper, we\npropose a novel point cloud completion network, CompleteDT, which is based on\nthe transformer. CompleteDT can learn features within neighborhoods and explore\nthe relationship among these neighborhoods. By sampling the incomplete point\ncloud to obtain point clouds with different resolutions, we extract features\nfrom these point clouds in a self-guided manner, while converting these\nfeatures into a series of $patches$ based on the geometrical structure. To\nfacilitate transformers to leverage sufficient information about point clouds,\nwe provide a plug-and-play module named Relation-Augment Attention Module\n(RAA), consisting of Point Cross-Attention Module (PCA) and Point Dense\nMulti-Scale Attention Module (PDMA). These two modules can enhance the ability\nto learn features within Patches and consider the correlation among these\nPatches. Thus, RAA enables to learn structures of incomplete point clouds and\ncontribute to infer the local details of complete point clouds generated. In\naddition, we predict the complete shape from $patches$ with an efficient\ngeneration module, namely, Multi-resolution Point Fusion Module (MPF). MPF\ngradually generates complete point clouds from $patches$, and updates $patches$\nbased on these generated point clouds. Experimental results show that our\nmethod largely outperforms the state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Jun Li",
      "Shangwei Guo",
      "Zhengchao Lai",
      "Xiantong Meng",
      "Shaokun Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14999"
  },
  {
    "id": "arXiv:2205.15006",
    "title": "On the Placement and Sustainability of Drone FSO Backhaul Relays",
    "abstract": "We consider free-space optical (FSO) communication links for the backhaul\nconnectivity of small cells (SCs) where a UAV with an FSO apparatus can serve\nas a backhaul relay node. We demonstrate how such drone relay stations (DRSs)\ncan be deployed in a high-rise urban area in order to provide FSO line-of-sight\n(LOS) links that are unobstructed by buildings. Also, in our solution we\nconsider the case where solar panels are mounted on DRSs such that placing the\nDRS in a sunny location is prioritized, and we show the gain in terms of number\nof required trips to recharge the UAV.",
    "descriptor": "",
    "authors": [
      "Salim Janji",
      "Adam Samorzewski",
      "Ma\u0142gorzata Wasilewska",
      "Adrian Kliks"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.15006"
  },
  {
    "id": "arXiv:2205.15008",
    "title": "Efficient Transformed Gaussian Processes for Non-Stationary Dependent  Multi-class Classification",
    "abstract": "This work introduces the Efficient Transformed Gaussian Process (ETGP), a new\nway of creating C stochastic processes characterized by: 1) the C processes are\nnon-stationary, 2) the C processes are dependent by construction without\nneeding a mixing matrix, 3) training and making predictions is very efficient\nsince the number of Gaussian Processes (GP) operations (e.g. inverting the\ninducing point's covariance matrix) do not depend on the number of processes.\nThis makes the ETGP particularly suited for multi-class problems with a very\nlarge number of classes, which are the problems studied in this work. ETGPs\nexploit the recently proposed Transformed Gaussian Process (TGP), a stochastic\nprocess specified by transforming a Gaussian Process using an invertible\ntransformation. However, unlike TGPs, ETGPs are constructed by transforming a\nsingle sample from a GP using C invertible transformations. We derive an\nefficient sparse variational inference algorithm for the proposed model and\ndemonstrate its utility in 5 classification tasks which include\nlow/medium/large datasets and a different number of classes, ranging from just\na few to hundreds. Our results show that ETGPs, in general, outperform\nstate-of-the-art methods for multi-class classification based on GPs, and have\na lower computational cost (around one order of magnitude smaller).",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Juan Maro\u00f1as",
      "Daniel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15008"
  },
  {
    "id": "arXiv:2205.15009",
    "title": "Carleman Lifting for Nonlinear System Identification with Guaranteed  Error Bounds",
    "abstract": "This paper concerns identification of uncontrolled or closed loop nonlinear\nsystems using a set of trajectories that are generated by the system in a\ndomain of attraction. The objective is to ensure that the trajectories of the\nidentified systems are close to the trajectories of the real system, as\nquantified by an error bound that is \\textit{prescribed a priori}. A majority\nof existing methods for nonlinear system identification rely on techniques such\nas neural networks, autoregressive moving averages, and spectral decomposition\nthat do not provide systematic approaches to meet pre-defined error bounds. The\ndeveloped method is based on Carleman linearization-based lifting of the\nnonlinear system to an infinite dimensional linear system. The linear system is\nthen truncated to a suitable order, computed based on the prescribed error\nbound, and parameters of the truncated linear system are estimated from data.\nThe effectiveness of the technique is demonstrated by identifying an\napproximation of the Van der Pol oscillator from data within a prescribed error\nbound.",
    "descriptor": "",
    "authors": [
      "Moad Abudia",
      "Joel A. Rosenfeld",
      "Rushikesh Kamalapurkar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.15009"
  },
  {
    "id": "arXiv:2205.15011",
    "title": "Moore's Law is dead, long live Moore's Law!",
    "abstract": "Moore's Law has been used by semiconductor industry as predicative indicators\nof the industry and it has become a self-fulfilling prophecy. Now more people\ntend to agree that the original Moore's Law started to falter. This paper\nproposes a possible quantitative modification to Moore's Law. It can cover\nother derivative laws of Moore's Law as well. It intends to more accurately\npredict the roadmap of chip's performance and energy consumption.",
    "descriptor": "",
    "authors": [
      "Nick Zhang"
    ],
    "subjectives": [
      "General Literature (cs.GL)"
    ],
    "url": "https://arxiv.org/abs/2205.15011"
  },
  {
    "id": "arXiv:2205.15014",
    "title": "Task-Prior Conditional Variational Auto-Encoder for Few-Shot Image  Classification",
    "abstract": "Transductive methods always outperform inductive methods in few-shot image\nclassification scenarios. However, the existing few-shot methods contain a\nlatent condition: the number of samples in each class is the same, which may be\nunrealistic. To cope with those cases where the query shots of each class are\nnonuniform (i.e. nonuniform few-shot learning), we propose a Task-Prior\nConditional Variational Auto-Encoder model named TP-VAE, conditioned on support\nshots and constrained by a task-level prior regularization. Our method obtains\nhigh performance in the more challenging nonuniform few-shot scenarios.\nMoreover, our method outperforms the state-of-the-art in a wide range of\nstandard few-shot image classification scenarios. Among them, the accuracy of\n1-shot increased by about 3\\%.",
    "descriptor": "\nComments: few-shot leanring, meta learning, transducitve learning, imgae classification\n",
    "authors": [
      "Zaiyun Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.15014"
  },
  {
    "id": "arXiv:2205.15016",
    "title": "A Fundamental Probabilistic Fuzzy Logic Framework Suitable for Causal  Reasoning",
    "abstract": "In this paper, we introduce a fundamental framework to create a bridge\nbetween Probability Theory and Fuzzy Logic. Indeed, our theory formulates a\nrandom experiment of selecting crisp elements with the criterion of having a\ncertain fuzzy attribute. To do so, we associate some specific crisp random\nvariables to the random experiment. Then, several formulas are presented, which\nmake it easier to compute different conditional probabilities and expected\nvalues of these random variables. Also, we provide measure theoretical basis\nfor our probabilistic fuzzy logic framework. Note that in our theory, the\nprobability density functions of continuous distributions which come from the\naforementioned random variables include the Dirac delta function as a term.\nFurther, we introduce an application of our theory in Causal Inference.",
    "descriptor": "\nComments: 60 pages, 7 figures, 1 table, 2 links to Github\n",
    "authors": [
      "Amir Saki",
      "Usef Faghihi"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.15016"
  },
  {
    "id": "arXiv:2205.15018",
    "title": "A reconfigurable integrated electronic tongue and its use in accelerated  analysis of juices and wines",
    "abstract": "Potentiometric electronic tongues (ETs) leveraging trends in miniaturization\nand internet of things (IoT) bear promise for facile mobile chemical analysis\nof complex multicomponent liquids, such as beverages. In this work,\nhand-crafted feature extraction from the transient potentiometric response of\nan array of low-selective miniaturized polymeric sensors is combined with a\ndata pipeline for deployment of trained machine learning models on a cloud\nback-end or edge device. The sensor array demonstrated sensitivity to different\norganic acids and exhibited interesting performance for the fingerprinting of\nfruit juices and wines, including differentiation of samples through supervised\nlearning based on sensory descriptors and prediction of consumer acceptability\nof aged juice samples. Product authentication, quality control and support of\nsensory evaluation are some of the applications that are expected to benefit\nfrom integrated electronic tongues that facilitate the characterization of\ncomplex properties of multi-component liquids.",
    "descriptor": "",
    "authors": [
      "Gianmarco Gabrieli",
      "Michal Muszynski",
      "Patrick W. Ruch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15018"
  },
  {
    "id": "arXiv:2205.15021",
    "title": "Agnostic Physics-Driven Deep Learning",
    "abstract": "This work establishes that a physical system can perform statistical learning\nwithout gradient computations, via an Agnostic Equilibrium Propagation\n(Aeqprop) procedure that combines energy minimization, homeostatic control, and\nnudging towards the correct response. In Aeqprop, the specifics of the system\ndo not have to be known: the procedure is based only on external manipulations,\nand produces a stochastic gradient descent without explicit gradient\ncomputations. Thanks to nudging, the system performs a true, order-one gradient\nstep for each training sample, in contrast with order-zero methods like\nreinforcement or evolutionary strategies, which rely on trial and error. This\nprocedure considerably widens the range of potential hardware for statistical\nlearning to any system with enough controllable parameters, even if the details\nof the system are poorly known. Aeqprop also establishes that in natural\n(bio)physical systems, genuine gradient-based statistical learning may result\nfrom generic, relatively simple mechanisms, without backpropagation and its\nrequirement for analytic knowledge of partial derivatives.",
    "descriptor": "",
    "authors": [
      "Benjamin Scellier",
      "Siddhartha Mishra",
      "Yoshua Bengio",
      "Yann Ollivier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15021"
  },
  {
    "id": "arXiv:2205.15023",
    "title": "Scalable Multi-Agent Model-Based Reinforcement Learning",
    "abstract": "Recent Multi-Agent Reinforcement Learning (MARL) literature has been largely\nfocused on Centralized Training with Decentralized Execution (CTDE) paradigm.\nCTDE has been a dominant approach for both cooperative and mixed environments\ndue to its capability to efficiently train decentralized policies. While in\nmixed environments full autonomy of the agents can be a desirable outcome,\ncooperative environments allow agents to share information to facilitate\ncoordination. Approaches that leverage this technique are usually referred as\ncommunication methods, as full autonomy of agents is compromised for better\nperformance. Although communication approaches have shown impressive results,\nthey do not fully leverage this additional information during training phase.\nIn this paper, we propose a new method called MAMBA which utilizes Model-Based\nReinforcement Learning (MBRL) to further leverage centralized training in\ncooperative environments. We argue that communication between agents is enough\nto sustain a world model for each agent during execution phase while imaginary\nrollouts can be used for training, removing the necessity to interact with the\nenvironment. These properties yield sample efficient algorithm that can scale\ngracefully with the number of agents. We empirically confirm that MAMBA\nachieves good performance while reducing the number of interactions with the\nenvironment up to an orders of magnitude compared to Model-Free\nstate-of-the-art approaches in challenging domains of SMAC and Flatland.",
    "descriptor": "\nComments: AAMAS'2022, cite this https URL\n",
    "authors": [
      "Vladimir Egorov",
      "Aleksei Shpilman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15023"
  },
  {
    "id": "arXiv:2205.15025",
    "title": "An Efficient Modern Baseline for FloodNet VQA",
    "abstract": "Designing efficient and reliable VQA systems remains a challenging problem,\nmore so in the case of disaster management and response systems. In this work,\nwe revisit fundamental combination methods like concatenation, addition and\nelement-wise multiplication with modern image and text feature abstraction\nmodels. We design a simple and efficient system which outperforms pre-existing\nmethods on the FloodNet dataset and achieves state-of-the-art performance. This\nsimplified system requires significantly less training and inference time than\nmodern VQA architectures. We also study the performance of various backbones\nand report their consolidated results. Code is available at\nhttps://github.com/sahilkhose/floodnet_vqa.",
    "descriptor": "\nComments: Under review, 4 pages, 2 figures, 1 table\n",
    "authors": [
      "Aditya Kane",
      "Sahil Khose"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.15025"
  },
  {
    "id": "arXiv:2205.15027",
    "title": "Symbol Emergence as Inter-personal Categorization with Head-to-head  Latent Word",
    "abstract": "In this study, we propose a head-to-head type (H2H-type) inter-personal\nmultimodal Dirichlet mixture (Inter-MDM) by modifying the original Inter-MDM,\nwhich is a probabilistic generative model that represents the symbol emergence\nbetween two agents as multiagent multimodal categorization. A\nMetropolis--Hastings method-based naming game based on the Inter-MDM enables\ntwo agents to collaboratively perform multimodal categorization and share signs\nwith a solid mathematical foundation of convergence. However, the conventional\nInter-MDM presumes a tail-to-tail connection across a latent word variable,\ncausing inflexibility of the further extension of Inter-MDM for modeling a more\ncomplex symbol emergence. Therefore, we propose herein a head-to-head type\n(H2H-type) Inter-MDM that treats a latent word variable as a child node of an\ninternal variable of each agent in the same way as many prior studies of\nmultimodal categorization. On the basis of the H2H-type Inter-MDM, we propose a\nnaming game in the same way as the conventional Inter-MDM. The experimental\nresults show that the H2H-type Inter-MDM yields almost the same performance as\nthe conventional Inter-MDM from the viewpoint of multimodal categorization and\nsign sharing.",
    "descriptor": "\nComments: 7 pages, 4 figures, 5 tables\n",
    "authors": [
      "Kazuma Furukawa",
      "Akira Taniguchi",
      "Yoshinobu Hagiwara",
      "Tadahiro Taniguchi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.15027"
  },
  {
    "id": "arXiv:2205.15031",
    "title": "Neural Copula: A unified framework for estimating generic  high-dimensional Copula functions",
    "abstract": "The Copula is widely used to describe the relationship between the marginal\ndistribution and joint distribution of random variables. The estimation of\nhigh-dimensional Copula is difficult, and most existing solutions rely either\non simplified assumptions or on complicating recursive decompositions.\nTherefore, people still hope to obtain a generic Copula estimation method with\nboth universality and simplicity. To reach this goal, a novel neural\nnetwork-based method (named Neural Copula) is proposed in this paper. In this\nmethod, a hierarchical unsupervised neural network is constructed to estimate\nthe marginal distribution function and the Copula function by solving\ndifferential equations. In the training program, various constraints are\nimposed on both the neural network and its derivatives. The Copula estimated by\nthe proposed method is smooth and has an analytic expression. The effectiveness\nof the proposed method is evaluated on both real-world datasets and complex\nnumerical simulations. Experimental results show that Neural Copula's fitting\nquality for complex distributions is much better than classical methods. The\nrelevant code for the experiments is available on GitHub. (We encourage the\nreader to run the program for a better understanding of the proposed method).",
    "descriptor": "",
    "authors": [
      "Zhi Zeng",
      "Ting Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15031"
  },
  {
    "id": "arXiv:2205.15034",
    "title": "SMUDLP: Self-Teaching Multi-Frame Unsupervised Endoscopic Depth  Estimation with Learnable Patchmatch",
    "abstract": "Unsupervised monocular trained depth estimation models make use of adjacent\nframes as a supervisory signal during the training phase. However, temporally\ncorrelated frames are also available at inference time for many clinical\napplications, e.g., surgical navigation. The vast majority of monocular systems\ndo not exploit this valuable signal that could be deployed to enhance the depth\nestimates. Those that do, achieve only limited gains due to the unique\nchallenges in endoscopic scenes, such as low and homogeneous textures and\ninter-frame brightness fluctuations. In this work, we present SMUDLP, a novel\nand unsupervised paradigm for multi-frame monocular endoscopic depth\nestimation. The SMUDLP integrates a learnable patchmatch module to adaptively\nincrease the discriminative ability in low-texture and homogeneous-texture\nregions, and enforces cross-teaching and self-teaching consistencies to provide\nefficacious regularizations towards brightness fluctuations. Our detailed\nexperiments on both SCARED and Hamlyn datasets indicate that the SMUDLP exceeds\nstate-of-the-art competitors by a large margin, including those that use single\nor multiple frames at inference time. The source code and trained models will\nbe publicly available upon the acceptance.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Shuwei Shao",
      "Zhongcai Pei",
      "Weihai Chen",
      "Xingming Wu",
      "Zhong Liu",
      "Zhengguo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15034"
  },
  {
    "id": "arXiv:2205.15037",
    "title": "Snoopy: A Webpage Fingerprinting Framework with Finite Query Model for  Mass-Surveillance",
    "abstract": "Internet users are vulnerable to privacy attacks despite the use of\nencryption. Webpage fingerprinting, an attack that analyzes encrypted traffic,\ncan identify the webpages visited by a user in a given website. Recent research\nworks have been successful in demonstrating webpage fingerprinting attacks on\nindividual users, but have been unsuccessful in extending their attack for\nmass-surveillance. The key challenges in performing mass-scale webpage\nfingerprinting arises from (i) the sheer number of combinations of user\nbehavior and preferences to account for, and; (ii) the bound on the number of\nwebsite queries imposed by the defense mechanisms (e.g., DDoS defense) deployed\nat the website. These constraints preclude the use of conventional\ndata-intensive ML-based techniques. In this work, we propose Snoopy, a\nfirst-of-its-kind framework, that performs webpage fingerprinting for a large\nnumber of users visiting a website. Snoopy caters to the generalization\nrequirements of mass-surveillance while complying with a bound on the number of\nwebsite accesses (finite query model) for traffic sample collection. For this,\nSnoopy uses a feature (i.e., sequence of encrypted resource sizes) that is\neither unaffected or predictably affected by different browsing contexts (OS,\nbrowser, caching, cookie settings). Snoopy uses static analysis techniques to\npredict the variations caused by factors such as header sizes, MTU, and User\nAgent String that arise from the diversity in browsing contexts. We show that\nSnoopy achieves approximately 90% accuracy when evaluated on most websites,\nacross various browsing contexts. A simple ensemble of Snoopy and an ML-based\ntechnique achieves approximately 97% accuracy while adhering to the finite\nquery model, in cases when Snoopy alone does not perform well.",
    "descriptor": "\nComments: The codes used for the analyses presented in the paper will be made available online only after the manuscript is accepted for publication at any conference/journal\n",
    "authors": [
      "Gargi Mitra",
      "Prasanna Karthik Vairam",
      "Sandip Saha",
      "Nitin Chandrachoodan",
      "V. Kamakoti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.15037"
  },
  {
    "id": "arXiv:2205.15043",
    "title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch",
    "abstract": "Training deep reinforcement learning (DRL) models usually requires high\ncomputation costs. Therefore, compressing DRL models possesses immense\npotential for training acceleration and model deployment. However, existing\nmethods that generate small models mainly adopt the knowledge distillation\nbased approach by iteratively training a dense network, such that the training\nprocess still demands massive computing resources. Indeed, sparse training from\nscratch in DRL has not been well explored and is particularly challenging due\nto non-stationarity in bootstrap training. In this work, we propose a novel\nsparse DRL training framework, \"the \\textbf{R}igged \\textbf{R}einforcement\n\\textbf{L}earning \\textbf{L}ottery\" (RLx2), which is capable of training a DRL\nagent \\emph{using an ultra-sparse network throughout} for off-policy\nreinforcement learning. The systematic RLx2 framework contains three key\ncomponents: gradient-based topology evolution, multi-step Temporal Difference\n(TD) targets, and dynamic-capacity replay buffer. RLx2 enables efficient\ntopology exploration and robust Q-value estimation simultaneously. We\ndemonstrate state-of-the-art sparse training performance in several continuous\ncontrol tasks using RLx2, showing $7.5\\times$-$20\\times$ model compression with\nless than $3\\%$ performance degradation, and up to $20\\times$ and $50\\times$\nFLOPs reduction for training and inference, respectively.",
    "descriptor": "",
    "authors": [
      "Yiqin Tan",
      "Pihe Hu",
      "Ling Pan",
      "Longbo Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15043"
  },
  {
    "id": "arXiv:2205.15045",
    "title": "Intelligent optoelectronic processor for orbital angular momentum  spectrum measurement",
    "abstract": "Structured light carrying orbital angular momentum (OAM) has propelled the\nadvancement of massive fundamental discoveries and applications such as\ncommunication and quantum analogy, thereby posing increasing demands on its\neffective detection schemes. Conventional schemes based on e.g. phase retrieval\nare troubled with low speed, complicated system, poor robustness, etc. Here, we\ndevise an intelligent processor composed of photonic and electronic neurons for\nmeasuring the OAM spectrum in an accurate, fast and direct manner.\nSpecifically, cascaded optical layers extract invisible topological charge (TC)\ninformation from incoming light and a shallow electronic layer predicts the\nexact OAM spectrum. Compared with existing alternatives, the integration of\noptical-computing in the system promises us a compact single-shot measurement\nscheme with high detection speed and energy efficiency (optical operations /\nelectronic operations ~10^3), neither necessitating reference wave nor\nrepetitive steps. Results show our processor is endowed with salient\ngeneralization ability and robustness against diverse structured light and\nadverse effects. Besides, it exhibits exceptional performance (mean squared\nerror ~10^(-5)) on unseen experimental data though trained on a fully simulated\ndataset, which sharply eases the workload on collecting massive labelled\nexperimental training set. We further raise a model interpretation paradigm to\nreveal the underlying physical mechanisms in hybrid deep learning system, in\ndistinct to conventional black-box neural networks. Our work not only\ncontributes to the explorations on OAM physics and applications, and also\nbroadly inspires the advanced link between intelligent computing and physical\neffects.",
    "descriptor": "",
    "authors": [
      "Hao Wang",
      "Ziyu Zhan",
      "Futai Hu",
      "Yuan Meng",
      "Zeqi Liu",
      "Xing Fu",
      "Qiang Liu"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2205.15045"
  },
  {
    "id": "arXiv:2205.15049",
    "title": "Metrizing Fairness",
    "abstract": "We study supervised learning problems for predicting properties of\nindividuals who belong to one of two demographic groups, and we seek predictors\nthat are fair according to statistical parity. This means that the\ndistributions of the predictions within the two groups should be close with\nrespect to the Kolmogorov distance, and fairness is achieved by penalizing the\ndissimilarity of these two distributions in the objective function of the\nlearning problem. In this paper, we showcase conceptual and computational\nbenefits of measuring unfairness with integral probability metrics (IPMs) other\nthan the Kolmogorov distance. Conceptually, we show that the generator of any\nIPM can be interpreted as a family of utility functions and that unfairness\nwith respect to this IPM arises if individuals in the two demographic groups\nhave diverging expected utilities. We also prove that the\nunfairness-regularized prediction loss admits unbiased gradient estimators if\nunfairness is measured by the squared $\\mathcal L^2$-distance or by a squared\nmaximum mean discrepancy. In this case, the fair learning problem is\nsusceptible to efficient stochastic gradient descent (SGD) algorithms.\nNumerical experiments on real data show that these SGD algorithms outperform\nstate-of-the-art methods for fair learning in that they achieve superior\naccuracy-unfairness trade-offs -- sometimes orders of magnitude faster.\nFinally, we identify conditions under which statistical parity can improve\nprediction accuracy.",
    "descriptor": "",
    "authors": [
      "Yves Rychener",
      "Bahar Taskesen",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15049"
  },
  {
    "id": "arXiv:2205.15051",
    "title": "X-SCITLDR: Cross-Lingual Extreme Summarization of Scholarly Documents",
    "abstract": "The number of scientific publications nowadays is rapidly increasing, causing\ninformation overload for researchers and making it hard for scholars to keep up\nto date with current trends and lines of work. Consequently, recent work on\napplying text mining technologies for scholarly publications has investigated\nthe application of automatic text summarization technologies, including extreme\nsummarization, for this domain. However, previous work has concentrated only on\nmonolingual settings, primarily in English. In this paper, we fill this\nresearch gap and present an abstractive cross-lingual summarization dataset for\nfour different languages in the scholarly domain, which enables us to train and\nevaluate models that process English papers and generate summaries in German,\nItalian, Chinese and Japanese. We present our new X-SCITLDR dataset for\nmultilingual summarization and thoroughly benchmark different models based on a\nstate-of-the-art multilingual pre-trained model, including a two-stage\n`summarize and translate' approach and a direct cross-lingual model. We\nadditionally explore the benefits of intermediate-stage training using English\nmonolingual summarization and machine translation as intermediate tasks and\nanalyze performance in zero- and few-shot scenarios.",
    "descriptor": "\nComments: JCDL2022\n",
    "authors": [
      "Sotaro Takeshita",
      "Tommaso Green",
      "Niklas Friedrich",
      "Kai Eckert",
      "Simone Paolo Ponzetto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.15051"
  },
  {
    "id": "arXiv:2205.15052",
    "title": "Reconfigurable Intelligent Surface Aided Mobile Edge Computing over  Intermittent mmWave Links",
    "abstract": "The advent of Reconfigurable Intelligent Surfaces (RISs) in wireless\ncommunication networks unlocks the way to support high frequency radio access\n(e.g. in millimeter wave) while overcoming their sensitivity to the presence of\ndeep fading and blockages. In support of this vision, this work exhibits the\nforward-looking perception of using RIS to enhance the connectivity of the\ncommunication links in edge computing scenarios, to support computation\noffloading services. We consider a multi-user MIMO system, and we formulate a\nlong-term optimization problem aiming to ensure a bounded end-to-end delay with\nthe minimum users average transmit power, by jointly selecting uplink user\nprecoding, RIS reflectivity parameters, and computation resources at a mobile\nedge host. Thanks to the marriage of Lyapunov stochastic optimization,\nprojected gradient techniques and convex optimization, the problem is\nefficiently solved in a per-slot basis, requiring only the observation of\ninstantaneous realizations of time-varying radio channels and task arrivals,\nand that of communication and computing buffers. Numerical simulations show the\neffectiveness of our method and the benefits of the RIS, in striking the best\ntrade-off between power consumption and delay for different blocking\nconditions, also when different levels of channel knowledge are assumed.",
    "descriptor": "",
    "authors": [
      "Fatima Ezzahra Airod",
      "Mattia Merluzzi",
      "Paolo Di Lorenzo",
      "Emilio Calvanese Strinati"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.15052"
  },
  {
    "id": "arXiv:2205.15053",
    "title": "Deblurring Photographs of Characters Using Deep Neural Networks",
    "abstract": "In this paper, we present our approach for the Helsinki Deblur Challenge\n(HDC2021). The task of this challenge is to deblur images of characters without\nknowing the point spread function (PSF). The organizers provided a dataset of\npairs of sharp and blurred images. Our method consists of three steps: First,\nwe estimate a warping transformation of the images to align the sharp images\nwith the blurred ones. Next, we estimate the PSF using a quasi-Newton method.\nThe estimated PSF allows to generate additional pairs of sharp and blurred\nimages. Finally, we train a deep convolutional neural network to reconstruct\nthe sharp images from the blurred images. Our method is able to successfully\nreconstruct images from the first 10 stages of the HDC 2021 data. Our code is\navailable at \\url{https://github.com/hhu-machine-learning/hdc2021-psfnn}.",
    "descriptor": "\nComments: 15 pages, 13 figures\n",
    "authors": [
      "Thomas Germer",
      "Tobias Uelwer",
      "Stefan Harmeling"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15053"
  },
  {
    "id": "arXiv:2205.15059",
    "title": "Hilbert Curve Projection Distance for Distribution Comparison",
    "abstract": "Distribution comparison plays a central role in many machine learning tasks\nlike data classification and generative modeling. In this study, we propose a\nnovel metric, called Hilbert curve projection (HCP) distance, to measure the\ndistance between two probability distributions with high robustness and low\ncomplexity. In particular, we first project two high-dimensional probability\ndensities using Hilbert curve to obtain a coupling between them, and then\ncalculate the transport distance between these two densities in the original\nspace, according to the coupling. We show that HCP distance is a proper metric\nand is well-defined for absolutely continuous probability measures.\nFurthermore, we demonstrate that the empirical HCP distance converges to its\npopulation counterpart at a rate of no more than $O(n^{-1/2d})$ under\nregularity conditions. To suppress the curse-of-dimensionality, we also develop\ntwo variants of the HCP distance using (learnable) subspace projections.\nExperiments on both synthetic and real-world data show that our HCP distance\nworks as an effective surrogate of the Wasserstein distance with low complexity\nand overcomes the drawbacks of the sliced Wasserstein distance.",
    "descriptor": "\nComments: 28 pages, 19 figures\n",
    "authors": [
      "Tao Li",
      "Cheng Meng",
      "Jun Yu",
      "Hongteng Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15059"
  },
  {
    "id": "arXiv:2205.15060",
    "title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue  System",
    "abstract": "In this paper, we present Duplex Conversation, a multi-turn, multimodal\nspoken dialogue system that enables telephone-based agents to interact with\ncustomers like a human. We use the concept of full-duplex in telecommunication\nto demonstrate what a human-like interactive experience should be and how to\nachieve smooth turn-taking through three subtasks: user state detection,\nbackchannel selection, and barge-in detection. Besides, we propose\nsemi-supervised learning with multimodal data augmentation to leverage\nunlabeled data to increase model generalization. Experimental results on three\nsub-tasks show that the proposed method achieves consistent improvements\ncompared with baselines. We deploy the Duplex Conversation to Alibaba\nintelligent customer service and share lessons learned in production. Online\nA/B experiments show that the proposed system can significantly reduce response\nlatency by 50%.",
    "descriptor": "\nComments: Accepted by KDD 2022, ADS track\n",
    "authors": [
      "Ting-En Lin",
      "Yuchuan Wu",
      "Fei Huang",
      "Luo Si",
      "Jian Sun",
      "Yongbin Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.15060"
  },
  {
    "id": "arXiv:2205.15062",
    "title": "A Transistor Operations Model for Deep Learning Energy Consumption  Scaling",
    "abstract": "Deep Learning (DL) has transformed the automation of a wide range of\nindustries and finds increasing ubiquity in society. The increasing complexity\nof DL models and its widespread adoption has led to the energy consumption\ndoubling every 3-4 months. Currently, the relationship between DL model\nconfiguration and energy consumption is not well established. Current FLOPs and\nMACs based methods only consider the linear operations. In this paper, we\ndevelop a bottom-level Transistor Operations (TOs) method to expose the role of\nactivation functions and neural network structure in energy consumption scaling\nwith DL model configuration. TOs allows us uncovers the role played by\nnon-linear operations (e.g. division/root operations performed by activation\nfunctions and batch normalisation). As such, our proposed TOs model provides\ndevelopers with a hardware-agnostic index for how energy consumption scales\nwith model settings. To validate our work, we analyse the TOs energy scaling of\na feed-forward DNN model set and achieve a 98.2% - 99.97% precision in\nestimating its energy consumption. We believe this work can be extended to any\nDL model.",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Chen Li",
      "Antonios Tsourdos",
      "Weisi Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.15062"
  },
  {
    "id": "arXiv:2205.15063",
    "title": "A Personalized Recommender System for Pervasive Social Networks",
    "abstract": "The current availability of interconnected portable devices, and the advent\nof the Web 2.0, raise the problem of supporting anywhere and anytime access to\na huge amount of content, generated and shared by mobile users. In this work we\npropose a novel framework for pervasive social networks, called Pervasive\nPLIERS (pPLIERS), able to discover and select, in a highly personalized way,\ncontents of interest for single mobile users. pPLIERS exploits the recently\nproposed PLIERS tag based recommender system as context a reasoning tool able\nto adapt recommendations to heterogeneous interest profiles of different users.\npPLIERS effectively operates also when limited knowledge about the network is\nmaintained. It is implemented in a completely decentralized environment, in\nwhich new contents are continuously generated and diffused through the network,\nand it relies only on the exchange of single nodes knowledge during proximity\ncontacts and through device to device communications. We evaluated pPLIERS by\nsimulating its behaviour in three different scenarios: a big event (Expo 2015),\na conference venue (ACM KDD 2015), and a working day in the city of Helsinki.\nFor each scenario, we used real or synthetic mobility traces and we extracted\nreal datasets from Twitter interactions to characterise the generation and\nsharing of user contents.",
    "descriptor": "",
    "authors": [
      "Valerio Arnaboldi",
      "Mattia G. Campana",
      "Franca Delmastro",
      "Elena Pagani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.15063"
  },
  {
    "id": "arXiv:2205.15064",
    "title": "SEREN: Knowing When to Explore and When to Exploit",
    "abstract": "Efficient reinforcement learning (RL) involves a trade-off between\n\"exploitative\" actions that maximise expected reward and \"explorative'\" ones\nthat sample unvisited states. To encourage exploration, recent approaches\nproposed adding stochasticity to actions, separating exploration and\nexploitation phases, or equating reduction in uncertainty with reward. However,\nthese techniques do not necessarily offer entirely systematic approaches making\nthis trade-off. Here we introduce SElective Reinforcement Exploration Network\n(SEREN) that poses the exploration-exploitation trade-off as a game between an\nRL agent -- \\exploiter, which purely exploits known rewards, and another RL\nagent -- \\switcher, which chooses at which states to activate a pure\nexploration policy that is trained to minimise system uncertainty and override\nExploiter. Using a form of policies known as impulse control, \\switcher is able\nto determine the best set of states to switch to the exploration policy while\nExploiter is free to execute its actions everywhere else. We prove that SEREN\nconverges quickly and induces a natural schedule towards pure exploitation.\nThrough extensive empirical studies in both discrete (MiniGrid) and continuous\n(MuJoCo) control benchmarks, we show that SEREN can be readily combined with\nexisting RL algorithms to yield significant improvement in performance relative\nto state-of-the-art algorithms.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.02618, arXiv:2103.09159\n",
    "authors": [
      "Changmin Yu",
      "David Mguni",
      "Dong Li",
      "Aivar Sootla",
      "Jun Wang",
      "Neil Burgess"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15064"
  },
  {
    "id": "arXiv:2205.15065",
    "title": "Performance and Coexistence Evaluation of IEEE 802.11be Multi-link  Operation",
    "abstract": "Wi-Fi 7 is already in the making, and Multi-Link Operation (MLO) is one of\nthe main features proposed in its correspondent IEEE 802.11be amendment. MLO\nwill allow devices to coordinate multiple radio interfaces to access separate\nchannels through a single association, aiming for improved throughput, network\ndelay, and overall spectrum reuse efficiency. In this work, we study three\nreference scenarios to evaluate the performance of the two main MLO\nimplementations -- Multi-Link Multi-Radio (MLMR) and Multi-Link Single-Radio\n(MLSR) -- , the interplay between multiple nodes employing them, and their\ncoexistence with legacy Single-Link devices. Importantly, our results reveal\nthat the potential of MLMR is mainly unleashed in isolated deployments or under\nunloaded network conditions. Instead, in medium- to high-load scenarios, MLSR\nmay prove more effective in reducing the latency while guaranteeing fairness\nwith contending Single-Link nodes.",
    "descriptor": "",
    "authors": [
      "Marc Carrascosa-Zamacois",
      "Lorenzo Galati-Giordano",
      "Anders Jonsson",
      "Giovanni Geraci",
      "Boris Bellalta"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.15065"
  },
  {
    "id": "arXiv:2205.15066",
    "title": "On the External Validity of Average-Case Analyses of Graph Algorithms",
    "abstract": "The number one criticism of average-case analysis is that we do not actually\nknow the probability distribution of real-world inputs. Thus, analyzing an\nalgorithm on some random model has no implications for practical performance.\nAt its core, this criticism doubts the existence of external validity, i.e., it\nassumes that algorithmic behavior on the somewhat simple and clean models does\nnot translate beyond the models to practical performance real-world input. With\nthis paper, we provide a first step towards studying the question of external\nvalidity systematically. To this end, we evaluate the performance of six graph\nalgorithms on a collection of 2751 sparse real-world networks depending on two\nproperties; the heterogeneity (variance in the degree distribution) and\nlocality (tendency of edges to connect vertices that are already close). We\ncompare this with the performance on generated networks with varying locality\nand heterogeneity. We find that the performance in the idealized setting of\nnetwork models translates surprisingly well to real-world networks. Moreover,\nheterogeneity and locality appear to be the core properties impacting the\nperformance of many graph algorithms.",
    "descriptor": "\nComments: 42 pages, 19 figures, preprint (full version)\n",
    "authors": [
      "Thomas Bl\u00e4sius",
      "Philipp Fischbeck"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.15066"
  },
  {
    "id": "arXiv:2205.15068",
    "title": "Embedding Graphs on Grassmann Manifold",
    "abstract": "Learning efficient graph representation is the key to favorably addressing\ndownstream tasks on graphs, such as node or graph property prediction. Given\nthe non-Euclidean structural property of graphs, preserving the original graph\ndata's similarity relationship in the embedded space needs specific tools and a\nsimilarity metric. This paper develops a new graph representation learning\nscheme, namely EGG, which embeds approximated second-order graph\ncharacteristics into a Grassmann manifold. The proposed strategy leverages\ngraph convolutions to learn hidden representations of the corresponding\nsubspace of the graph, which is then mapped to a Grassmann point of a low\ndimensional manifold through truncated singular value decomposition (SVD). The\nestablished graph embedding approximates denoised correlationship of node\nattributes, as implemented in the form of a symmetric matrix space for\nEuclidean calculation. The effectiveness of EGG is demonstrated using both\nclustering and classification tasks at the node level and graph level. It\noutperforms baseline models on various benchmarks.",
    "descriptor": "",
    "authors": [
      "Bingxin Zhou",
      "Xuebin Zheng",
      "Yu Guang Wang",
      "Ming Li",
      "Junbin Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15068"
  },
  {
    "id": "arXiv:2205.15075",
    "title": "Align then Fusion: Generalized Large-scale Multi-view Clustering with  Anchor Matching Correspondences",
    "abstract": "Multi-view anchor graph clustering selects representative anchors to avoid\nfull pair-wise similarities and therefore reduce the complexity of graph\nmethods. Although widely applied in large-scale applications, existing\napproaches do not pay sufficient attention to establishing correct\ncorrespondences between the anchor sets across views. To be specific, anchor\ngraphs obtained from different views are not aligned column-wisely. Such an\nAnchor-Unaligned Problem (AUP) would cause inaccurate graph fusion and degrade\nthe clustering performance. Under multi-view scenarios, generating correct\ncorrespondences could be extremely difficult since anchors are not consistent\nin feature dimensions. To solve this challenging issue, we propose the first\nstudy of a generalized and flexible anchor graph fusion framework termed Fast\nMulti-View Anchor-Correspondence Clustering (FMVACC). Specifically, we show how\nto find anchor correspondence with both feature and structure information,\nafter which anchor graph fusion is performed column-wisely. Moreover, we\ntheoretically show the connection between FMVACC and existing multi-view late\nfusion and partial view-aligned clustering, which further demonstrates our\ngenerality. Extensive experiments on seven benchmark datasets demonstrate the\neffectiveness and efficiency of our proposed method. Moreover, the proposed\nalignment module also shows significant performance improvement applying to\nexisting multi-view anchor graph competitors indicating the importance of\nanchor alignment.",
    "descriptor": "",
    "authors": [
      "Siwei Wang",
      "Xinwang Liu",
      "Suyuan Liu",
      "Jiaqi Jin",
      "Wenxuan Tu",
      "Xinzhong Zhu",
      "En Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15075"
  },
  {
    "id": "arXiv:2205.15076",
    "title": "Improved Algorithms for Bandit with Graph Feedback via Regret  Decomposition",
    "abstract": "The problem of bandit with graph feedback generalizes both the multi-armed\nbandit (MAB) problem and the learning with expert advice problem by encoding in\na directed graph how the loss vector can be observed in each round of the game.\nThe mini-max regret is closely related to the structure of the feedback graph\nand their connection is far from being fully understood. We propose a new\nalgorithmic framework for the problem based on a partition of the feedback\ngraph. Our analysis reveals the interplay between various parts of the graph by\ndecomposing the regret to the sum of the regret caused by small parts and the\nregret caused by their interaction. As a result, our algorithm can be viewed as\nan interpolation and generalization of the optimal algorithms for MAB and\nlearning with expert advice. Our framework unifies previous algorithms for both\nstrongly observable graphs and weakly observable graphs, resulting in improved\nand optimal regret bounds on a wide range of graph families including graphs of\nbounded degree and strongly observable graphs with a few corrupted arms.",
    "descriptor": "",
    "authors": [
      "Yuchen He",
      "Chihao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15076"
  },
  {
    "id": "arXiv:2205.15077",
    "title": "On the Value of Retransmissions for Age of Information in Random Access  Networks without Feedback",
    "abstract": "We focus on a slotted ALOHA system without feedback, in which nodes transmit\ntime-stamped updates to a common gateway. Departing from the classical\ngenerate-at-will model, we assume that each transmitter may not always have\nfresh information to deliver, and tackle the fundamental question of whether\nsending stale packets can be beneficial from an age-of-information (AoI)\nstandpoint. Leaning on a signal-flow-graph analysis of Markov processes the\nstudy reveals that, when packets can be lost due to channel impairments,\nretransmissions can indeed lower AoI for low generation rates of new\ninformation, although at a cost in terms of throughput.",
    "descriptor": "",
    "authors": [
      "Andrea Munari"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.15077"
  },
  {
    "id": "arXiv:2205.15083",
    "title": "CGMN: A Contrastive Graph Matching Network for Self-Supervised Graph  Similarity Learning",
    "abstract": "Graph similarity learning refers to calculating the similarity score between\ntwo graphs, which is required in many realistic applications, such as visual\ntracking, graph classification, and collaborative filtering. As most of the\nexisting graph neural networks yield effective graph representations of a\nsingle graph, little effort has been made for jointly learning two graph\nrepresentations and calculating their similarity score. In addition, existing\nunsupervised graph similarity learning methods are mainly clustering-based,\nwhich ignores the valuable information embodied in graph pairs. To this end, we\npropose a contrastive graph matching network (CGMN) for self-supervised graph\nsimilarity learning in order to calculate the similarity between any two input\ngraph objects. Specifically, we generate two augmented views for each graph in\na pair respectively. Then, we employ two strategies, namely cross-view\ninteraction and cross-graph interaction, for effective node representation\nlearning. The former is resorted to strengthen the consistency of node\nrepresentations in two views. The latter is utilized to identify node\ndifferences between different graphs. Finally, we transform node\nrepresentations into graph-level representations via pooling operations for\ngraph similarity computation. We have evaluated CGMN on eight real-world\ndatasets, and the experiment results show that the proposed new approach is\nsuperior to the state-of-the-art methods in graph similarity learning\ndownstream tasks.",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Di Jin",
      "Luzhi Wang",
      "Yizhen Zheng",
      "Xiang Li",
      "Fei Jiang",
      "Wei Lin",
      "Shirui Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15083"
  },
  {
    "id": "arXiv:2205.15086",
    "title": "Retrieving and Ranking Relevant JavaScript Technologies from Web  Repositories",
    "abstract": "The selection of software technologies is an important but complex task. We\nconsider developers of JavaScript (JS) applications, for whom the assessment of\nJS libraries has become difficult and time-consuming due to the growing number\nof technology options available. A common strategy is to browse software\nrepositories via search engines (e.g., NPM, or Google), although it brings some\nproblems. First, given a technology need, the engines might return a long list\nof results, which often causes information overload issues. Second, the results\nshould be ranked according to criteria of interest for the developer. However,\ndeciding how to weight these criteria to make a decision is not\nstraightforward. In this work, we propose a two-phase approach for assisting\ndevelopers to retrieve and rank JS technologies in a semi-automated fashion.\nThe first-phase (ST-Retrieval) uses a meta-search technique for collecting JS\ntechnologies that meet the developer's needs. The second-phase (called\nST-Rank), relies on a machine learning technique to infer, based on criteria\nused by other projects in the Web, a ranking of the output of ST-Retrieval. We\nevaluated our approach with NPM and obtained satisfactory results in terms of\nthe accuracy of the technologies retrieved and the order in which they were\nranked.",
    "descriptor": "",
    "authors": [
      "Hernan C. Vazquez",
      "J. Andres Diaz Pace",
      "Claudia Marcos",
      "Santiago Vidal"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15086"
  },
  {
    "id": "arXiv:2205.15094",
    "title": "CHALLENGER: Training with Attribution Maps",
    "abstract": "We show that utilizing attribution maps for training neural networks can\nimprove regularization of models and thus increase performance. Regularization\nis key in deep learning, especially when training complex models on relatively\nsmall datasets. In order to understand inner workings of neural networks,\nattribution methods such as Layer-wise Relevance Propagation (LRP) have been\nextensively studied, particularly for interpreting the relevance of input\nfeatures. We introduce Challenger, a module that leverages the explainable\npower of attribution maps in order to manipulate particularly relevant input\npatterns. Therefore, exposing and subsequently resolving regions of ambiguity\ntowards separating classes on the ground-truth data manifold, an issue that\narises particularly when training models on rather small datasets. Our\nChallenger module increases model performance through building more diverse\nfilters within the network and can be applied to any input data domain. We\ndemonstrate that our approach results in substantially better classification as\nwell as calibration performance on datasets with only a few samples up to\ndatasets with thousands of samples. In particular, we show that our generic\ndomain-independent approach yields state-of-the-art results in vision, natural\nlanguage processing and on time series tasks.",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Christian Tomani",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15094"
  },
  {
    "id": "arXiv:2205.15097",
    "title": "On the impact of the physical layer model on the performance of  D2D-offloading in vehicular environments",
    "abstract": "Offloading data traffic from Infrastructure-to-Device (I2D) to\nDevice-to-Device (D2D) communications is a powerful tool for reducing\ncongestion, energy consumption, and spectrum usage of mobile cellular networks.\nPrior network-level studies on D2D data offloading focus on high level\nperformance metrics as the offloading efficiency, and take into account the\nradio propagation aspects by using simplistic wireless channel models. We\nconsider a D2D data offloading protocol tailored to highly dynamic scenarios as\nvehicular environments, and evaluate its performance focusing on physical layer\naspects, like energy consumption and spectral efficiency. We do this by taking\ninto account more realistic models of the wireless channel, with respect to the\nsimplistic ones generally used in the previous studies. Our objective is\ntwofold: first, to quantify the performance gain of the considered D2D\noffloading protocol with respect to a classic cellular network, based on I2D\ncommunications, in terms of energy consumption and spectral efficiency. Second,\nto show that using simplistic channel models may prevent to accurately evaluate\nthe performance gain. Additionally, the use of more elaborated models allows to\nobtain insightful information on relevant system-level parameters settings,\nwhich would not be possible to obtain by using simple models. The considered\nchannel models have been proposed and validated, in the recent years, through\nlarge-scale measurements campaigns.\nOur results show that the considered protocol is able to achieve a reduction\nin the energy consumption of up to 35%, and an increase in the system spectral\nefficiency of 50%, with respect to the benchmark cellular system. The use of\ndifferent channel models in evaluating these metrics may result, in the worst\ncase, in a sixfold underestimation of the achieved improvement.",
    "descriptor": "",
    "authors": [
      "Loreto Pescosolido",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.15097"
  },
  {
    "id": "arXiv:2205.15100",
    "title": "Meta Representation Learning with Contextual Linear Bandits",
    "abstract": "Meta-learning seeks to build algorithms that rapidly learn how to solve new\nlearning problems based on previous experience. In this paper we investigate\nmeta-learning in the setting of stochastic linear bandit tasks. We assume that\nthe tasks share a low dimensional representation, which has been partially\nacquired from previous learning tasks. We aim to leverage this information in\norder to learn a new downstream bandit task, which shares the same\nrepresentation. Our principal contribution is to show that if the learned\nrepresentation estimates well the unknown one, then the downstream task can be\nefficiently learned by a greedy policy that we propose in this work. We derive\nan upper bound on the regret of this policy, which is, up to logarithmic\nfactors, of order $r\\sqrt{N}(1\\vee \\sqrt{d/T})$, where $N$ is the horizon of\nthe downstream task, $T$ is the number of training tasks, $d$ the ambient\ndimension and $r \\ll d$ the dimension of the representation. We highlight that\nour strategy does not need to know $r$. We note that if $T> d$ our bound\nachieves the same rate of optimal minimax bandit algorithms using the true\nunderlying representation. Our analysis is inspired and builds in part upon\nprevious work on meta-learning in the i.i.d. full information setting\n\\citep{tripuraneni2021provable,boursier2022trace}. As a separate contribution\nwe show how to relax certain assumptions in those works, thereby improving\ntheir representation learning and risk analysis.",
    "descriptor": "",
    "authors": [
      "Leonardo Cella",
      "Karim Lounici",
      "Massimiliano Pontil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15100"
  },
  {
    "id": "arXiv:2205.15104",
    "title": "FLICU: A Federated Learning Workflow for Intensive Care Unit Mortality  Prediction",
    "abstract": "Although Machine Learning (ML) can be seen as a promising tool to improve\nclinical decision-making for supporting the improvement of medication plans,\nclinical procedures, diagnoses, or medication prescriptions, it remains limited\nby access to healthcare data. Healthcare data is sensitive, requiring strict\nprivacy practices, and typically stored in data silos, making traditional\nmachine learning challenging. Federated learning can counteract those\nlimitations by training machine learning models over data silos while keeping\nthe sensitive data localized. This study proposes a federated learning workflow\nfor ICU mortality prediction. Hereby, the applicability of federated learning\nas an alternative to centralized machine learning and local machine learning is\ninvestigated by introducing federated learning to the binary classification\nproblem of predicting ICU mortality. We extract multivariate time series data\nfrom the MIMIC-III database (lab values and vital signs), and benchmark the\npredictive performance of four deep sequential classifiers (FRNN, LSTM, GRU,\nand 1DCNN) varying the patient history window lengths (8h, 16h, 24h, 48h) and\nthe number of FL clients (2, 4, 8). The experiments demonstrate that both\ncentralized machine learning and federated learning are comparable in terms of\nAUPRC and F1-score. Furthermore, the federated approach shows superior\nperformance over local machine learning. Thus, the federated approach can be\nseen as a valid and privacy-preserving alternative to centralized machine\nlearning for classifying ICU mortality when sharing sensitive patient data\nbetween hospitals is not possible.",
    "descriptor": "",
    "authors": [
      "Lena Mondrejevski",
      "Ioanna Miliou",
      "Annaclaudia Montanino",
      "David Pitts",
      "Jaakko Hollm\u00e9n",
      "Panagiotis Papapetrou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15104"
  },
  {
    "id": "arXiv:2205.15107",
    "title": "Accurate and Efficient Modeling of 802.15.4 Unslotted CSMA/CA through  Event Chains Computation",
    "abstract": "Many analytical models have been proposed for evaluating the performance of\nevent-driven 802.15.4 Wireless Sensor Networks (WSNs), in Non-Beacon Enabled\n(NBE) mode. However, existing models do not provide accurate analysis of\nlarge-scale WSNs, due to tractability issues and/or simplifying assumptions. In\nthis paper, we propose a new approach called Event Chains Computation (ECC) to\nmodel the unslotted CSMA/CA algorithm used for channel access in NBE mode. ECC\nrelies on the idea that outcomes of the CSMA/CA algorithm can be represented as\nchains of events that subsequently occur in the network. Although ECC can\ngenerate all the possible outcomes, it only considers chains with a probability\nto occur greater than a pre-defined threshold to reduce complexity.\nFurthermore, ECC parallelizes the computation by managing different chains\nthrough different threads. Our results show that, by an appropriate threshold\nselection, the time to derive performance metrics can be drastically reduced,\nwith negligible impact on accuracy. We also show that the computation time\ndecreases almost linearly with the number of employed threads. We validate our\nmodel through simulations and testbed experiments, and use it to investigate\nthe impact of different parameters on the WSN performance, in terms of delivery\nratio, latency, and energy consumption.",
    "descriptor": "",
    "authors": [
      "Domenico De Guglielmo",
      "Francesco Restuccia",
      "Giuseppe Anastasi",
      "Marco Conti",
      "Sajal K. Das"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.15107"
  },
  {
    "id": "arXiv:2205.15110",
    "title": "Proper Posture: Designing Posture Feedback Across Musical Instruments",
    "abstract": "There is a recommended body posture and hand position for playing every\nmusical instrument, allowing efficient and quick movements without blockage.\nDue to humans' limited cognitive capabilities, they struggle to concentrate on\nseveral things simultaneously and thus sometimes lose the correct position\nwhile playing their instrument. Incorrect positions when playing an instrument\ncan lead to injuries and movement disorders in the long run. Previous work in\nHCI mainly focused on developing systems to assist in learning an instrument.\nHowever, the design space for posture correction when playing a musical\ninstrument has not yet been explored. In this position paper, we present our\nvision of providing subtle vibrotactile or thermal feedback to guide the focus\nof attention back to the correct posture when playing a musical instrument. We\ndiscuss our concept with a focus on motion recognition and feedback modalities.\nFinally, we outline the next steps for future research.",
    "descriptor": "\nComments: CHI 22, Workshop \"Intelligent Music Interfaces: When Interactive Assistance and Augmentation Meet Musical Instruments\"\n",
    "authors": [
      "Bettina Eska",
      "Jasmin Niess",
      "Florian M\u00fcller"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.15110"
  },
  {
    "id": "arXiv:2205.15111",
    "title": "A k nearest neighbours classifiers ensemble based on extended  neighbourhood rule and features subsets",
    "abstract": "kNN based ensemble methods minimise the effect of outliers by identifying a\nset of data points in the given feature space that are nearest to an unseen\nobservation in order to predict its response by using majority voting. The\nordinary ensembles based on kNN find out the k nearest observations in a region\n(bounded by a sphere) based on a predefined value of k. This scenario, however,\nmight not work in situations when the test observation follows the pattern of\nthe closest data points with the same class that lie on a certain path not\ncontained in the given sphere. This paper proposes a k nearest neighbour\nensemble where the neighbours are determined in k steps. Starting from the\nfirst nearest observation of the test point, the algorithm identifies a single\nobservation that is closest to the observation at the previous step. At each\nbase learner in the ensemble, this search is extended to k steps on a random\nbootstrap sample with a random subset of features selected from the feature\nspace. The final predicted class of the test point is determined by using a\nmajority vote in the predicted classes given by all base models. This new\nensemble method is applied on 17 benchmark datasets and compared with other\nclassical methods, including kNN based models, in terms of classification\naccuracy, kappa and Brier score as performance metrics. Boxplots are also\nutilised to illustrate the difference in the results given by the proposed and\nother state-of-the-art methods. The proposed method outperformed the rest of\nthe classical methods in the majority of cases. The paper gives a detailed\nsimulation study for further assessment.",
    "descriptor": "\nComments: This paper is submitted to pattern recognotion and has 26 pages, 9 figures and 5 tables\n",
    "authors": [
      "Amjad Ali",
      "Muhammad Hamraz",
      "Naz Gul",
      "Dost Muhammad Khan",
      "Zardad Khan",
      "Saeed Aldahmani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15111"
  },
  {
    "id": "arXiv:2205.15112",
    "title": "Robotic grasp detection based on Transformer",
    "abstract": "Grasp detection in a cluttered environment is still a great challenge for\nrobots. Currently, the Transformer mechanism has been successfully applied to\nvisual tasks, and its excellent ability of global context information\nextraction provides a feasible way to improve the performance of robotic grasp\ndetection in cluttered scenes. However, the insufficient inductive bias ability\nof the original Transformer model requires large-scale datasets training, which\nis difficult to obtain for grasp detection. In this paper, we propose a grasp\ndetection model based on encoder-decoder structure. The encoder uses a\nTransformer network to extract global context information. The decoder uses a\nfully convolutional neural network to improve the inductive bias capability of\nthe model and combine features extracted by the encoder to predict the final\ngrasp configuration. Experiments on the VMRD dataset demonstrate that our model\nperforms much better in overlapping object scenes. Meanwhile, on the Cornell\nGrasp dataset, our approach achieves an accuracy of 98.1%, which is comparable\nwith state-of-the-art algorithms.",
    "descriptor": "",
    "authors": [
      "Mingshuai Dong",
      "Xiuli Yu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.15112"
  },
  {
    "id": "arXiv:2205.15113",
    "title": "Online Agnostic Multiclass Boosting",
    "abstract": "Boosting is a fundamental approach in machine learning that enjoys both\nstrong theoretical and practical guarantees. At a high-level, boosting\nalgorithms cleverly aggregate weak learners to generate predictions with\narbitrarily high accuracy. In this way, boosting algorithms convert weak\nlearners into strong ones. Recently, Brukhim et al. extended boosting to the\nonline agnostic binary classification setting. A key ingredient in their\napproach is a clean and simple reduction to online convex optimization, one\nthat efficiently converts an arbitrary online convex optimizer to an agnostic\nonline booster. In this work, we extend this reduction to multiclass problems\nand give the first boosting algorithm for online agnostic mutliclass\nclassification. Our reduction also enables the construction of algorithms for\nstatistical agnostic, online realizable, and statistical realizable multiclass\nboosting.",
    "descriptor": "",
    "authors": [
      "Vinod Raman",
      "Ambuj Tewari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15113"
  },
  {
    "id": "arXiv:2205.15115",
    "title": "A Novel Control-Oriented Cell Transition Model Including Service  Stations on Highways",
    "abstract": "In this paper, we propose a novel model that describes how the traffic\nevolution on a highway stretch is affected by the presence of a service\nstation. The presented model enhances the classical CTM dynamics by adding the\ndynamics associated with the service stations, where the vehicles may stop\nbefore merging back into the mainstream. We name it CTMs. We discuss its\nflexibility in describing different complex scenarios where multiple stations\nare characterized by different drivers' average stopping times corresponding to\ndifferent services. The model has been developed to help design control\nstrategies aimed at decreasing traffic congestion. Thus, we discuss how\nclassical control schemes can interact with the proposed \\gls{CTMs}. Finally,\nwe validate the proposed model through numerical simulations and assess the\neffects of service stations on traffic evolution, which appear to be\nbeneficial, especially for relatively short congested periods.",
    "descriptor": "\nComments: This work is submitted to the 61st IEEE CDC 2022\n",
    "authors": [
      "Carlo Cenedese",
      "Michele Cucuzzella",
      "Antonella Ferrara",
      "John Lygeros"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Econometrics (econ.EM)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.15115"
  },
  {
    "id": "arXiv:2205.15117",
    "title": "OOD Link Prediction Generalization Capabilities of Message-Passing GNNs  in Larger Test Graphs",
    "abstract": "This work provides the first theoretical study on the ability of graph\nMessage Passing Neural Networks (gMPNNs) -- such as Graph Neural Networks\n(GNNs) -- to perform inductive out-of-distribution (OOD) link prediction tasks,\nwhere deployment (test) graph sizes are larger than training graphs. We first\nprove non-asymptotic bounds showing that link predictors based on\npermutation-equivariant (structural) node embeddings obtained by gMPNNs can\nconverge to a random guess as test graphs get larger. We then propose a\ntheoretically-sound gMPNN that outputs structural pairwise (2-node) embeddings\nand prove non-asymptotic bounds showing that, as test graphs grow, these\nembeddings converge to embeddings of a continuous function that retains its\nability to predict links OOD. Empirical results on random graphs show agreement\nwith our theoretical results.",
    "descriptor": "\nComments: Under submission\n",
    "authors": [
      "Yangze Zhou",
      "Gitta Kutyniok",
      "Bruno Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15117"
  },
  {
    "id": "arXiv:2205.15118",
    "title": "Pressure Data-Driven Variational Multiscale Reduced Order Models",
    "abstract": "In this paper, we develop data-driven closure/correction terms to increase\nthe pressure and velocity accuracy of reduced order models (ROMs) for fluid\nflows. Specifically, we propose the first pressure-based data-driven\nvariational multiscale ROM, in which we use the available data to construct\nclosure/correction terms for both the momentum equation and the continuity\nequation. Our numerical investigation of the two-dimensional flow past a\ncircular cylinder at Re=50000 in the marginally-resolved regime shows that the\nnovel pressure data-driven variational multiscale ROM yields significantly more\naccurate velocity and pressure approximations than the standard ROM and, more\nimportantly, than the original data-driven variational multiscale ROM (i.e.,\nwithout pressure components). In particular, our numerical results show that\nadding the closure/correction term in the momentum equation significantly\nimproves both the velocity and the pressure approximations, whereas adding the\nclosure/correction term in the continuity equation improves only the pressure\napproximation.",
    "descriptor": "",
    "authors": [
      "Anna Ivagnes",
      "Giovanni Stabile",
      "Andrea Mola",
      "Traian Iliescu",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.15118"
  },
  {
    "id": "arXiv:2205.15121",
    "title": "Towards Supporting Intelligence in 5G/6G Core Networks: NWDAF  Implementation and Initial Analysis",
    "abstract": "Wireless networks, in the fifth-generation and beyond, must support diverse\nnetwork applications which will support the numerous and demanding connections\nof today's and tomorrow's devices. Requirements such as high data rates, low\nlatencies, and reliability are crucial considerations and artificial\nintelligence is incorporated to achieve these requirements for a large number\nof connected devices. Specifically, intelligent methods and frameworks for\nadvanced analysis are employed by the 5G Core Network Data Analytics Function\n(NWDAF) to detect patterns and ascribe detailed action information to\naccommodate end users and improve network performance. To this end, the work\npresented in this paper incorporates a functional NWDAF into a 5G network\ndeveloped using open source software. Furthermore, an analysis of the network\ndata collected by the NWDAF and the valuable insights which can be drawn from\nit have been presented with detailed Network Function interactions. An example\napplication of such insights used for intelligent network management is\noutlined. Finally, the expected limitations of 5G networks are discussed as\nmotivation for the development of 6G networks.",
    "descriptor": "\nComments: 6 pages, 6 figures, accepted in IWCMC 2022\n",
    "authors": [
      "Ali Chouman",
      "Dimitrios Michael Manias",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15121"
  },
  {
    "id": "arXiv:2205.15124",
    "title": "Generalizing Hierarchical Bayesian Bandits",
    "abstract": "A contextual bandit is a popular and practical framework for online learning\nto act under uncertainty. In many problems, the number of actions is huge and\ntheir mean rewards are correlated. In this work, we introduce a general\nframework for capturing such correlations through a two-level graphical model\nwhere actions are related through multiple shared latent parameters. We propose\na Thompson sampling algorithm G-HierTS that uses this structure to explore\nefficiently and bound its Bayes regret. The regret has two terms, one for\nlearning action parameters and the other for learning the shared latent\nparameters. The terms reflect the structure of our model as well as the quality\nof priors. Our theoretical findings are validated empirically using both\nsynthetic and real-world problems. We also experiment with G-HierTS that\nmaintains a factored posterior over latent parameters. While this approximation\ndoes not come with guarantees, it improves computational efficiency with a\nminimal impact on empirical regret.",
    "descriptor": "\nComments: 27 pages, 5 figures\n",
    "authors": [
      "Imad Aouali",
      "Branislav Kveton",
      "Sumeet Katariya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15124"
  },
  {
    "id": "arXiv:2205.15126",
    "title": "Elastic Monte Carlo Tree Search with State Abstraction for Strategy Game  Playing",
    "abstract": "Strategy video games challenge AI agents with their combinatorial search\nspace caused by complex game elements. State abstraction is a popular technique\nthat reduces the state space complexity. However, current state abstraction\nmethods for games depend on domain knowledge, making their application to new\ngames expensive. State abstraction methods that require no domain knowledge are\nstudied extensively in the planning domain. However, no evidence shows they\nscale well with the complexity of strategy games. In this paper, we propose\nElastic MCTS, an algorithm that uses state abstraction to play strategy games.\nIn Elastic MCTS, the nodes of the tree are clustered dynamically, first grouped\ntogether progressively by state abstraction, and then separated when an\niteration threshold is reached. The elastic changes benefit from efficient\nsearching brought by state abstraction but avoid the negative influence of\nusing state abstraction for the whole search. To evaluate our method, we make\nuse of the general strategy games platform Stratega to generate scenarios of\nvarying complexity. Results show that Elastic MCTS outperforms MCTS baselines\nwith a large margin, while reducing the tree size by a factor of $10$. Code can\nbe found at: https://github.com/egg-west/Stratega",
    "descriptor": "\nComments: 8 pages, 3 figures; Published on IEEE Conference on Games 2022\n",
    "authors": [
      "Linjie Xu",
      "Jorge Hurtado-Grueso",
      "Dominic Jeurissen",
      "Diego Perez Liebana",
      "Alexander Dockhorn"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15126"
  },
  {
    "id": "arXiv:2205.15127",
    "title": "Universal Deep GNNs: Rethinking Residual Connection in GNNs from a Path  Decomposition Perspective for Preventing the Over-smoothing",
    "abstract": "The performance of GNNs degrades as they become deeper due to the\nover-smoothing. Among all the attempts to prevent over-smoothing, residual\nconnection is one of the promising methods due to its simplicity. However,\nrecent studies have shown that GNNs with residual connections only slightly\nslow down the degeneration. The reason why residual connections fail in GNNs is\nstill unknown. In this paper, we investigate the forward and backward behavior\nof GNNs with residual connections from a novel path decomposition perspective.\nWe find that the recursive aggregation of the median length paths from the\nbinomial distribution of residual connection paths dominates output\nrepresentation, resulting in over-smoothing as GNNs go deeper. Entangled\npropagation and weight matrices cause gradient smoothing and prevent GNNs with\nresidual connections from optimizing to the identity mapping. Based on these\nfindings, we present a Universal Deep GNNs (UDGNN) framework with cold-start\nadaptive residual connections (DRIVE) and feedforward modules. Extensive\nexperiments demonstrate the effectiveness of our method, which achieves\nstate-of-the-art results over non-smooth heterophily datasets by simply\nstacking standard GNNs.",
    "descriptor": "",
    "authors": [
      "Jie Chen",
      "Weiqi Liu",
      "Zhizhong Huang",
      "Junbin Gao",
      "Junping Zhang",
      "Jian Pu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15127"
  },
  {
    "id": "arXiv:2205.15128",
    "title": "Domain Constraints in Feature Space: Strengthening Robustness of Android  Malware Detection against Realizable Adversarial Examples",
    "abstract": "Strengthening the robustness of machine learning-based malware detectors\nagainst realistic evasion attacks remains one of the major obstacles for\nAndroid malware detection. To that end, existing work has focused on\ninterpreting domain constraints of Android malware in the problem space, where\nproblem-space realizable adversarial examples are generated. In this paper, we\nprovide another promising way to achieve the same goal but based on\ninterpreting the domain constraints in the feature space, where feature-space\nrealizable adversarial examples are generated. Specifically, we present a novel\napproach to extracting feature-space domain constraints by learning meaningful\nfeature dependencies from data, and applying them based on a novel robust\nfeature space. Experimental results successfully demonstrate the effectiveness\nof our novel robust feature space in providing adversarial robustness for\nDREBIN, a state-of-the-art Android malware detector. For example, it can\ndecrease the evasion rate of a realistic gradient-based attack by $96.4\\%$ in a\nlimited-knowledge (transfer) setting and by $13.8\\%$ in a more challenging,\nperfect-knowledge setting. In addition, we show that directly using our learned\ndomain constraints in the adversarial retraining framework leads to about\n$84\\%$ improvement in a limited-knowledge setting, with up to $377\\times$\nfaster implementation than using problem-space adversarial examples.",
    "descriptor": "",
    "authors": [
      "Hamid Bostani",
      "Zhuoran Liu",
      "Zhengyu Zhao",
      "Veelasha Moonsamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.15128"
  },
  {
    "id": "arXiv:2205.15129",
    "title": "On the SCD semismooth* Newton method for generalized equations with  application to a class of static contact problems with Coulomb friction",
    "abstract": "In the paper, a variant of the \\ssstar Newton method is developed for the\nnumerical solution of generalized equations, in which the multi-valued part is\na so-called SCD (subspace containing derivative) mapping. Under a rather mild\nregularity requirement, the method exhibits (locally) superlinear convergence\nbehavior. From the main conceptual algorithm, two implementable variants are\nderived whose efficiency is tested via a generalized equation modeling a\ndiscretized static contact problem with Coulomb friction.",
    "descriptor": "",
    "authors": [
      "H. Gfrerer",
      "M. Mandlmayr",
      "J.V. Outrata",
      "J. Valdman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.15129"
  },
  {
    "id": "arXiv:2205.15130",
    "title": "Why Adversarial Training of ReLU Networks Is Difficult?",
    "abstract": "This paper mathematically derives an analytic solution of the adversarial\nperturbation on a ReLU network, and theoretically explains the difficulty of\nadversarial training. Specifically, we formulate the dynamics of the\nadversarial perturbation generated by the multi-step attack, which shows that\nthe adversarial perturbation tends to strengthen eigenvectors corresponding to\na few top-ranked eigenvalues of the Hessian matrix of the loss w.r.t. the\ninput. We also prove that adversarial training tends to strengthen the\ninfluence of unconfident input samples with large gradient norms in an\nexponential manner. Besides, we find that adversarial training strengthens the\ninfluence of the Hessian matrix of the loss w.r.t. network parameters, which\nmakes the adversarial training more likely to oscillate along directions of a\nfew samples, and boosts the difficulty of adversarial training. Crucially, our\nproofs provide a unified explanation for previous findings in understanding\nadversarial training.",
    "descriptor": "",
    "authors": [
      "Xu Cheng",
      "Hao Zhang",
      "Yue Xin",
      "Wen Shen",
      "Jie Ren",
      "Quanshi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15130"
  },
  {
    "id": "arXiv:2205.15131",
    "title": "Goal-Oriented A-Posteriori Estimation of Model Error as an Aid to  Parameter Estimation",
    "abstract": "In this work, a Bayesian model calibration framework is presented that\nutilizes goal-oriented a-posterior error estimates in quantities of interest\n(QoIs) for classes of high-fidelity models characterized by PDEs. It is shown\nthat for a large class of computational models it is possible to develop a\ncomputationally inexpensive procedure for calibrating parameters of\nhigh-fidelity models of physical events when the parameters of a low-fidelity\n(surrogate) models are known with acceptable accuracy. The main ingredient in\nthe proposed model calibration scheme are goal-oriented a-posteriori estimates\nof error in QoIs computed using a so-called lower fidelity model compared to\nthose of an uncalibrated higher fidelity model. The estimates of error in QoIs\nare used to define likelihood functions in Bayesian inversion analysis. A\nstandard Bayesian approach is employed to compute the posterior distribution of\nmodel parameters of high-fidelity models. As applications, parameters in a\nquasi-linear second-order elliptic boundary-value problem (BVP) are calibrated\nusing a second-order linear elliptic BVP. In a second application, parameters\nof a tumor growth model involving nonlinear time-dependent PDEs are calibrated\nusing a lower fidelity linear tumor growth model with known parameter values.",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Prashant K. Jha",
      "J. Tinsley Oden"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.15131"
  },
  {
    "id": "arXiv:2205.15133",
    "title": "Compressing and Comparing the Generative Spaces of Procedural Content  Generators",
    "abstract": "The past decade has seen a rapid increase in the level of research interest\nin procedural content generation (PCG) for digital games, and there are now\nnumerous research avenues focused on new approaches for driving and applying\nPCG systems. An area in which progress has been comparatively slow is the\ndevelopment of generalisable approaches for comparing alternative PCG systems,\nespecially in terms of their generative spaces. It is to this area that this\npaper aims to make a contribution, by exploring the utility of data compression\nalgorithms in compressing the generative spaces of PCG systems. We hope that\nthis approach could be the basis for developing useful qualitative tools for\ncomparing PCG systems to help designers better understand and optimize their\ngenerators. In this work we assess the efficacy of a selection of algorithms\nacross sets of levels for 2D tile-based games by investigating how much their\nrespective generative space compressions correlate with level behavioral\ncharacteristics. We conclude that the approach looks to be a promising one\ndespite some inconsistency in efficacy in alternative domains, and that of the\nalgorithms tested Multiple Correspondence Analysis appears to perform the most\neffectively.",
    "descriptor": "\nComments: 8 pages, 3 Figures, 2 Tables, Accepted to Conference on Games 2022\n",
    "authors": [
      "Oliver Withington",
      "Laurissa Tokarchuk"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.15133"
  },
  {
    "id": "arXiv:2205.15135",
    "title": "Group Probability-Weighted Tree Sums for Interpretable Modeling of  Heterogeneous Data",
    "abstract": "Machine learning in high-stakes domains, such as healthcare, faces two\ncritical challenges: (1) generalizing to diverse data distributions given\nlimited training data while (2) maintaining interpretability. To address these\nchallenges, we propose an instance-weighted tree-sum method that effectively\npools data across diverse groups to output a concise, rule-based model. Given\ndistinct groups of instances in a dataset (e.g., medical patients grouped by\nage or treatment site), our method first estimates group membership\nprobabilities for each instance. Then, it uses these estimates as instance\nweights in FIGS (Tan et al. 2022), to grow a set of decision trees whose values\nsum to the final prediction. We call this new method Group Probability-Weighted\nTree Sums (G-FIGS). G-FIGS achieves state-of-the-art prediction performance on\nimportant clinical datasets; e.g., holding the level of sensitivity fixed at\n92%, G-FIGS increases specificity for identifying cervical spine injury by up\nto 10% over CART and up to 3% over FIGS alone, with larger gains at higher\nsensitivity levels. By keeping the total number of rules below 16 in FIGS, the\nfinal models remain interpretable, and we find that their rules match medical\ndomain expertise. All code, data, and models are released on Github.",
    "descriptor": "",
    "authors": [
      "Keyan Nasseri",
      "Chandan Singh",
      "James Duncan",
      "Aaron Kornblith",
      "Bin Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15135"
  },
  {
    "id": "arXiv:2205.15137",
    "title": "A Fast Gear-Shifting Actuator for Robotic Tasks with Contacts",
    "abstract": "Vehicle power-trains use a variable transmission (multiple gear-ratios) to\nminimize motor size and maximize efficiency while meeting a wide-range of\noperating points. Robots could similarly benefit from variable transmission to\nsave weight and improve energy efficiency; leading to potentially\ngroundbreaking improvements for mobile and wearable robotic systems. However,\nvariable transmissions in a robotic context leads to new challenges regarding\nthe gear-shifting methodology: 1) order-of-magnitude variations of reduction\nratios are desired, and 2) contact situations during manipulation/locomotion\ntasks lead to impulsive behavior at the moment when gear-shifting is required.\nThis paper present an actuator with a gear-shifting methodology that can\nseamlessly change between two very different reduction ratios during dynamic\ncontact situations. Experimental results demonstrate the ability to execute a\ngear-shift from a 1:23 reduction to a 1:474 reduction in less than 30ms during\ncontact with a rigid object.",
    "descriptor": "",
    "authors": [
      "Alexandre Girard",
      "H. Harry Asada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.15137"
  },
  {
    "id": "arXiv:2205.15138",
    "title": "Deciding Hyperproperties Combined with Functional Specifications",
    "abstract": "We study satisfiability for HyperLTL with a $\\forall^*\\exists^*$ quantifier\nprefix, known to be highly undecidable in general. HyperLTL can express system\nproperties that relate multiple traces (so-called hyperproperties), which are\noften combined with trace properties that specify functional behavior on single\ntraces. Following this conceptual split, we first define several safety and\nliveness fragments of $\\forall^*\\exists^*$ HyperLTL, and characterize the\ncomplexity of their (often much easier) satisfiability problem. We then add LTL\ntrace properties as functional specifications. Though (highly) undecidable in\nmany cases, this way of combining \"simple\" HyperLTL and arbitrary LTL also\nleads to interesting new decidable fragments. This systematic study of\n$\\forall^*\\exists^*$ fragments is complemented by a new (incomplete) algorithm\nfor $\\forall\\exists^*$-HyperLTL satisfiability.",
    "descriptor": "\nComments: LICS 2022\n",
    "authors": [
      "Raven Beutner",
      "David Carral",
      "Bernd Finkbeiner",
      "Jana Hofmann",
      "Markus Kr\u00f6tzsch"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.15138"
  },
  {
    "id": "arXiv:2205.15139",
    "title": "Detecting fake news by enhanced text representation with  multi-EDU-structure awareness",
    "abstract": "Since fake news poses a serious threat to society and individuals, numerous\nstudies have been brought by considering text, propagation and user profiles.\nDue to the data collection problem, these methods based on propagation and user\nprofiles are less applicable in the early stages. A good alternative method is\nto detect news based on text as soon as they are released, and a lot of\ntext-based methods were proposed, which usually utilized words, sentences or\nparagraphs as basic units. But, word is a too fine-grained unit to express\ncoherent information well, sentence or paragraph is too coarse to show specific\ninformation. Which granularity is better and how to utilize it to enhance text\nrepresentation for fake news detection are two key problems. In this paper, we\nintroduce Elementary Discourse Unit (EDU) whose granularity is between word and\nsentence, and propose a multi-EDU-structure awareness model to improve text\nrepresentation for fake news detection, namely EDU4FD. For the\nmulti-EDU-structure awareness, we build the sequence-based EDU representations\nand the graph-based EDU representations. The former is gotten by modeling the\ncoherence between consecutive EDUs with TextCNN that reflect the semantic\ncoherence. For the latter, we first extract rhetorical relations to build the\nEDU dependency graph, which can show the global narrative logic and help\ndeliver the main idea truthfully. Then a Relation Graph Attention Network\n(RGAT) is set to get the graph-based EDU representation. Finally, the two EDU\nrepresentations are incorporated as the enhanced text representation for fake\nnews detection, using a gated recursive unit combined with a global attention\nmechanism. Experiments on four cross-source fake news datasets show that our\nmodel outperforms the state-of-the-art text-based methods.",
    "descriptor": "",
    "authors": [
      "Yuhang Wang",
      "Li Wang",
      "Yanjie Yang",
      "Yilin Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15139"
  },
  {
    "id": "arXiv:2205.15140",
    "title": "FiltPIM: In-Memory Filter for DNA Sequencing",
    "abstract": "Aligning the entire genome of an organism is a compute-intensive task.\nPre-alignment filters substantially reduce computation complexity by filtering\npotential alignment locations. The base-count filter successfully removes over\n68% of the potential locations through a histogram-based heuristic. This paper\npresents FiltPIM, an efficient design of the basecount filter that is based on\nmemristive processing-in-memory. The in-memory design reduces CPU-to-memory\ndata transfer and utilizes both intra-crossbar and inter-crossbar memristive\nstateful-logic parallelism. The reduction in data transfer and the efficient\nstateful-logic computation together improve filtering time by 100x compared to\na CPU implementation of the filter.",
    "descriptor": "\nComments: Published in 2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)\n",
    "authors": [
      "Marcel Khalifa",
      "Rotem Ben-Hur",
      "Ronny Ronen",
      "Orian Leitersdorf",
      "Leonid Yavits",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.15140"
  },
  {
    "id": "arXiv:2205.15141",
    "title": "Theme Aspect Argumentation Model for Handling Fallacies",
    "abstract": "In this paper, we present a novel approach to identify fallacies through\nformal constraints, as a viable alternative to more traditional fallacy\nclassifications by informal criteria. To achieve this objective, we introduce a\nnovel argumentation model, the theme aspect argumentation model, which can do\nboth: modelling of a given argumentation as it is expressed (rhetoric\nmodelling); and deeper semantic analysis of the rhetoric argumentation model.\nBy the help of formal constraints on the theme aspect argumentation model, it\nis for example possible to see if 'attack's which are claimed to be attacks at\nthe rhetoric level are really attacks. We present core formal constraints that\na reasonable argumentation should observe, and then more formal constraints\nthat improve fallacy identification capability. We show and prove consequences\nof these formal constraints. We then define the concept of normal forms and\nthat of logico-rhetorical conclusion, which we use to demonstrate detection of\nspecific fallacies, informal and logical.",
    "descriptor": "",
    "authors": [
      "Ryuta Arisaka"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15141"
  },
  {
    "id": "arXiv:2205.15142",
    "title": "On Avoiding Local Minima Using Gradient Descent With Large Learning  Rates",
    "abstract": "It has been widely observed in training of neural networks that when applying\ngradient descent (GD), a large step size is essential for obtaining superior\nmodels. However, the effect of large step sizes on the success of GD is not\nwell understood theoretically. We argue that a complete understanding of the\nmechanics leading to GD's success may indeed require considering effects of\nusing a large step size. To support this claim, we prove on a certain class of\nfunctions that GD with large step size follows a different trajectory than GD\nwith a small step size, leading to convergence to the global minimum. We also\ndemonstrate the difference in trajectories for small and large learning rates\nwhen GD is applied on a neural network, observing effects of an escape from a\nlocal minimum with a large step size, which shows this behavior is indeed\nrelevant in practice. Finally, through a novel set of experiments, we show even\nthough stochastic noise is beneficial, it is not enough to explain success of\nSGD and a large learning rate is essential for obtaining the best performance\neven in stochastic settings.",
    "descriptor": "",
    "authors": [
      "Amirkeivan Mohtashami",
      "Martin Jaggi",
      "Sebastian Stich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.15142"
  },
  {
    "id": "arXiv:2205.15145",
    "title": "Machine Learning Methods for Health-Index Prediction in Coating Chambers",
    "abstract": "Coating chambers create thin layers that improve the mechanical and optical\nsurface properties in jewelry production using physical vapor deposition. In\nsuch a process, evaporated material condensates on the walls of such chambers\nand, over time, causes mechanical defects and unstable processes. As a result,\nmanufacturers perform extensive maintenance procedures to reduce production\nloss. Current rule-based maintenance strategies neglect the impact of specific\nrecipes and the actual condition of the vacuum chamber. Our overall goal is to\npredict the future condition of the coating chamber to allow cost and quality\noptimized maintenance of the equipment. This paper describes the derivation of\na novel health indicator that serves as a step toward condition-based\nmaintenance for coating chambers. We indirectly use gas emissions of the\nchamber's contamination to evaluate the machine's condition. Our approach\nrelies on process data and does not require additional hardware installation.\nFurther, we evaluated multiple machine learning algorithms for a\ncondition-based forecast of the health indicator that also reflects production\nplanning. Our results show that models based on decision trees are the most\neffective and outperform all three benchmarks, improving at least $0.22$ in the\nmean average error. Our work paves the way for cost and quality optimized\nmaintenance of coating applications.",
    "descriptor": "",
    "authors": [
      "Clemens Heistracher",
      "Anahid Jalali",
      "J\u00fcrgen Schneeweiss",
      "Klaudia Kovacs",
      "Catherine Laflamme",
      "Bernhard Haslhofer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15145"
  },
  {
    "id": "arXiv:2205.15146",
    "title": "Batch Normalization Is Blind to the First and Second Derivatives of the  Loss",
    "abstract": "In this paper, we prove the effects of the BN operation on the\nback-propagation of the first and second derivatives of the loss. When we do\nthe Taylor series expansion of the loss function, we prove that the BN\noperation will block the influence of the first-order term and most influence\nof the second-order term of the loss. We also find that such a problem is\ncaused by the standardization phase of the BN operation. Experimental results\nhave verified our theoretical conclusions, and we have found that the BN\noperation significantly affects feature representations in specific tasks,\nwhere losses of different samples share similar analytic formulas.",
    "descriptor": "",
    "authors": [
      "Zhanpeng Zhou",
      "Wen Shen",
      "Huixin Chen",
      "Ling Tang",
      "Quanshi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15146"
  },
  {
    "id": "arXiv:2205.15147",
    "title": "Environmental Monitoring for Smart Cities",
    "abstract": "This work presents an innovative, multidisciplinary and cost-effective\necosystem of ICT solutions able to collect, process and distribute\ngeo-referenced information about the influence of pollution and micro-climatic\nconditions on the quality of life in Smart Cities. The system has been\ndeveloped and experimentally evaluated in the framework of the research project\nSHE, co-funded by the Tuscany Region (Italy). Specifically, an innovative\nmonitoring network has been developed, constituted by fixed and mobile sensor\nnodes, which provided comparable measurements in stationary and mobile\nconditions. In addition, sensor data have been enriched with those generated by\ncitizens through the use of a dedicated mobile application, exploiting\nparticipatory sensing and MSN paradigms.",
    "descriptor": "",
    "authors": [
      "Bacco Manlio",
      "Delmastro Franca",
      "Ferro Erina",
      "Gotta Alberto"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.15147"
  },
  {
    "id": "arXiv:2205.15150",
    "title": "Principle Components Analysis based frameworks for efficient missing  data imputation algorithms",
    "abstract": "Missing data is a commonly occurring problem in practice, and imputation,\ni.e., filling the missing entries of the data, is a popular way to deal with\nthis problem. This motivates multiple works on imputation to deal with missing\ndata of various types and dimensions. However, for high-dimensional datasets,\nthese imputation methods can be computationally expensive. Therefore, in this\nwork, we propose Principle Component Analysis Imputation (PCAI), a simple\nframework based on Principle Component Analysis (PCA) to speed up the\nimputation process of many available imputation techniques. Next, based on\nPCAI, we propose PCA Imputation - Classification (PIC), an imputation-dimension\nreduction-classification framework to deal with missing data classification\nproblems where it is desirable to reduce the dimensions before training a\nclassification model. Our experiments show that the proposed frameworks can be\nutilized with various imputation algorithms and improve the imputation speed\nsignificantly. Interestingly, the frameworks aid imputation methods that rely\non many parameters by reducing the dimension of the data and hence, reducing\nthe number of parameters needed to be estimated. Moreover, they not only can\nachieve compatible mean square error/higher classification accuracy compared to\nthe traditional imputation style on the original missing dataset but many times\ndeliver even better results. In addition, the frameworks also help to tackle\nthe memory issue that many imputation approaches have by reducing the number of\nfeatures.",
    "descriptor": "",
    "authors": [
      "Thu Nguyen",
      "Hoang Thien Ly",
      "Michael Alexander Riegler",
      "P\u00e5l Halvorsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15150"
  },
  {
    "id": "arXiv:2205.15156",
    "title": "Towards Efficient 3D Object Detection with Knowledge Distillation",
    "abstract": "Despite substantial progress in 3D object detection, advanced 3D detectors\noften suffer from heavy computation overheads. To this end, we explore the\npotential of knowledge distillation (KD) for developing efficient 3D object\ndetectors, focusing on popular pillar- and voxel-based detectors.Without\nwell-developed teacher-student pairs, we first study how to obtain student\nmodels with good trade offs between accuracy and efficiency from the\nperspectives of model compression and input resolution reduction. Then, we\nbuild a benchmark to assess existing KD methods developed in the 2D domain for\n3D object detection upon six well-constructed teacher-student pairs. Further,\nwe propose an improved KD pipeline incorporating an enhanced logit KD method\nthat performs KD on only a few pivotal positions determined by teacher\nclassification response, and a teacher-guided student model initialization to\nfacilitate transferring teacher model's feature extraction ability to students\nthrough weight inheritance. Finally, we conduct extensive experiments on the\nWaymo dataset. Our best performing model achieves $65.75\\%$ LEVEL 2 mAPH,\nsurpassing its teacher model and requiring only $44\\%$ of teacher flops. Our\nmost efficient model runs 51 FPS on an NVIDIA A100, which is $2.2\\times$ faster\nthan PointPillar with even higher accuracy. Code will be available.",
    "descriptor": "",
    "authors": [
      "Jihan Yang",
      "Shaoshuai Shi",
      "Runyu Ding",
      "Zhe Wang",
      "Xiaojuan Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15156"
  },
  {
    "id": "arXiv:2205.15159",
    "title": "Updating belief functions over Belnap--Dunn logic",
    "abstract": "Belief and plausibility are weaker measures of uncertainty than that of\nprobability. They are motivated by the situations when full probabilistic\ninformation is not available. However, information can also be contradictory.\nTherefore, the framework of classical logic is not necessarily the most\nadequate. Belnap-Dunn logic was introduced to reason about incomplete and\ncontradictory information. Klein et al and Bilkova et al generalize the notion\nof probability measures and belief functions to Belnap-Dunn logic,\nrespectively. In this article, we study how to update belief functions with new\npieces of information. We present a first approach via a frame semantics of\nBelnap-Dunn logic.",
    "descriptor": "",
    "authors": [
      "Sabine Frittella",
      "Ondrej Majer",
      "Sajad Nazari"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.15159"
  },
  {
    "id": "arXiv:2205.15160",
    "title": "On algorithmic applications of sim-width and mim-width of $(H_1,  H_2)$-free graphs",
    "abstract": "Mim-width and sim-width are among the most powerful graph width parameters,\nwith sim-width more powerful than mim-width, which is in turn more powerful\nthan clique-width. While several $\\mathsf{NP}$-hard graph problems become\ntractable for graph classes whose mim-width is bounded and quickly computable,\nno algorithmic applications of boundedness of sim-width are known. In [Kang et\nal., A width parameter useful for chordal and co-comparability graphs,\nTheoretical Computer Science, 704:1-17, 2017], it is asked whether\n\\textsc{Independent Set} and \\textsc{$3$-Colouring} are $\\mathsf{NP}$-complete\non graphs of sim-width at most $1$. We observe that, for each $k \\in\n\\mathbb{N}$, \\textsc{List $k$-Colouring} is polynomial-time solvable for graph\nclasses whose sim-width is bounded and quickly computable. Moreover, we show\nthat if the same holds for \\textsc{Independent Set}, then \\textsc{Independent\n$\\mathcal{H}$-Packing} is polynomial-time solvable for graph classes whose\nsim-width is bounded and quickly computable. This problem is a common\ngeneralisation of \\textsc{Independent Set}, \\textsc{Induced Matching},\n\\textsc{Dissociation Set} and \\textsc{$k$-Separator}. We also make progress\ntoward classifying the mim-width of $(H_1,H_2)$-free graphs in the case $H_1$\nis complete or edgeless. Our results solve some open problems in [Brettell et\nal., Bounding the mim-width of hereditary graph classes, Journal of Graph\nTheory, 99(1):117-151, 2022].",
    "descriptor": "",
    "authors": [
      "Andrea Munaro",
      "Shizhou Yang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.15160"
  },
  {
    "id": "arXiv:2205.15162",
    "title": "A practical optimal control approach for two-speed actuators",
    "abstract": "This paper addresses the closed-loop control of an actuator with both a\ncontinuous input variable (motor torque) and a discrete input variable (mode\nselection). In many applications, robots have to bear large loads while moving\nslowly and also have to move quickly through the air with almost no load,\nleading to conflicting requirements for their actuators. An actuator with\nmultiple gear ratios, like in a powertrain, can address this issue by allowing\nan effective use of power over a wide range of output speed. However, having\ndiscrete modes of operation adds complexity to the high-level control and\nplanning. Here a controller for two-speed actuators that automatically select\nboth the best gear ratio and the motor torque is developed. The approach is to:\nfirst derive a low-dimensional model, then use dynamic programming to find the\nbest actions for all possible situations, and last use regression analysis to\nextract simplified global feedback laws. This approach produces simple\npractical nearly-optimal feedback laws. A controller that globally minimizes a\nquadratic cost function is derived for a two-speed actuator prototype, global\nstability is proven and performance is demonstrated experimentally.",
    "descriptor": "",
    "authors": [
      "Alexandre Girard",
      "H. Harry Asada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.15162"
  },
  {
    "id": "arXiv:2205.15163",
    "title": "You Have Earned a Trophy: Characterize In-Game Achievements and Their  Completions",
    "abstract": "Achievement systems have been actively adopted in gaming platforms to\nmaintain players' interests. Among them, trophies in PlayStation games are one\nof the most successful achievement systems. While the importance of trophy\ndesign has been casually discussed in many game developers' forums, there has\nbeen no systematic study of the historical dataset of trophies yet. In this\nwork, we construct a complete dataset of PlayStation games and their trophies\nand investigate them from both the developers' and players' perspectives.",
    "descriptor": "\nComments: Preprint of the paper accepted at the 14th International ACM Conference on Web Science in 2022 (WebSci'22)\n",
    "authors": [
      "Haewoon Kwak"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.15163"
  },
  {
    "id": "arXiv:2205.15168",
    "title": "Subrank and Optimal Reduction of Scalar Multiplications to Generic  Tensors",
    "abstract": "Since the seminal works of Strassen and Valiant it has been a central theme\nin algebraic complexity theory to understand the relative complexity of\nalgebraic problems, that is, to understand which algebraic problems (be it\nbilinear maps like matrix multiplication in Strassen's work, or the determinant\nand permanent polynomials in Valiant's) can be reduced to each other (under the\nappropriate notion of reduction).\nIn this paper we determine precisely how many independent scalar\nmultiplications can be reduced to a given bilinear map (this number is called\nthe subrank, and extends the concept of matrix diagonalization to tensors), for\nessentially all (i.e. generic) bilinear maps. Namely, we prove for a generic\nbilinear map $T : V \\times V \\to V$ where $\\dim(V) = n$ that $\\theta(\\sqrt{n})$\nindependent scalar multiplications can be reduced to $T$. Our result\nsignificantly improves on the previous upper bound from the work of Strassen\n(1991) and B\\\"urgisser (1990) which was $n^{2/3 + o(1)}$. Our full result is\nmuch more general and applies not only to bilinear maps and 3-tensors but also\nto $k$-tensors, for which we find that the generic subrank is\n$\\theta(n^{1/(k-1)})$. Moreover, as an application we prove that the subrank is\nnot additive under the direct sum.\nThe subrank plays a central role in several areas of complexity theory\n(matrix multiplication algorithms, barrier results) and combinatorics (e.g.,\nthe cap set problem and sunflower problem). As a consequence of our result we\nobtain several large separations between the subrank and tensor methods that\nhave received much interest recently, notably the slice rank (Tao, 2016),\nanalytic rank (Gowers--Wolf, 2011; Lovett, 2018;\nBhrushundi--Harsha--Hatami--Kopparty--Kumar, 2020), geometric rank\n(Kopparty--Moshkovitz--Zuiddam, 2020), and G-stable rank (Derksen, 2020).",
    "descriptor": "",
    "authors": [
      "Harm Derksen",
      "Visu Makam",
      "Jeroen Zuiddam"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2205.15168"
  },
  {
    "id": "arXiv:2205.15171",
    "title": "Parameter Efficient Diff Pruning for Bias Mitigation",
    "abstract": "In recent years language models have achieved state of the art performance on\na wide variety of natural language processing tasks. As these models are\ncontinuously growing in size it becomes increasingly important to explore\nmethods to make them more storage efficient. At the same time their increase\ncognitive abilities increase the danger that societal bias existing in datasets\nare implicitly encoded in the model weights. We propose an architecture which\ndeals with these two challenges at the same time using two techniques:\nDiffPruning and Adverserial Training. The result is a modular architecture\nwhich extends the original DiffPurning setup with and additional sparse\nsubnetwork applied as a mask to diminish the effects of a predefined protected\nattribute at inference time.",
    "descriptor": "",
    "authors": [
      "Lukas Hauzenberger",
      "Navid Rekabsaz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.15171"
  },
  {
    "id": "arXiv:2205.15172",
    "title": "Billions of Parameters Are Worth More Than In-domain Training Data: A  case study in the Legal Case Entailment Task",
    "abstract": "Recent work has shown that language models scaled to billions of parameters,\nsuch as GPT-3, perform remarkably well in zero-shot and few-shot scenarios. In\nthis work, we experiment with zero-shot models in the legal case entailment\ntask of the COLIEE 2022 competition. Our experiments show that scaling the\nnumber of parameters in a language model improves the F1 score of our previous\nzero-shot result by more than 6 points, suggesting that stronger zero-shot\ncapability may be a characteristic of larger models, at least for this task.\nOur 3B-parameter zero-shot model outperforms all models, including ensembles,\nin the COLIEE 2021 test set and also achieves the best performance of a single\nmodel in the COLIEE 2022 competition, second only to the ensemble composed of\nthe 3B model itself and a smaller version of the same model. Despite the\nchallenges posed by large language models, mainly due to latency constraints in\nreal-time applications, we provide a demonstration of our zero-shot monoT5-3b\nmodel being used in production as a search engine, including for legal\ndocuments. The code for our submission and the demo of our system are available\nat https://github.com/neuralmind-ai/coliee and\nhttps://neuralsearchx.neuralmind.ai, respectively.",
    "descriptor": "",
    "authors": [
      "Guilherme Moraes Rosa",
      "Luiz Bonifacio",
      "Vitor Jeronymo",
      "Hugo Abonizio",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.15172"
  },
  {
    "id": "arXiv:2205.15173",
    "title": "Self-Supervised Pre-training of Vision Transformers for Dense Prediction  Tasks",
    "abstract": "We present a new self-supervised pre-training of Vision Transformers for\ndense prediction tasks. It is based on a contrastive loss across views that\ncompares pixel-level representations to global image representations. This\nstrategy produces better local features suitable for dense prediction tasks as\nopposed to contrastive pre-training based on global image representation only.\nFurthermore, our approach does not suffer from a reduced batch size since the\nnumber of negative examples needed in the contrastive loss is in the order of\nthe number of local features. We demonstrate the effectiveness of our\npre-training strategy on two dense prediction tasks: semantic segmentation and\nmonocular depth estimation.",
    "descriptor": "",
    "authors": [
      "Jaonary Rabarisoa",
      "Velentin Belissen",
      "Florian Chabot",
      "Quoc-Cuong Pham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15173"
  },
  {
    "id": "arXiv:2205.15175",
    "title": "ShuffleMixer: An Efficient ConvNet for Image Super-Resolution",
    "abstract": "Lightweight and efficiency are critical drivers for the practical application\nof image super-resolution (SR) algorithms. We propose a simple and effective\napproach, ShuffleMixer, for lightweight image super-resolution that explores\nlarge convolution and channel split-shuffle operation. In contrast to previous\nSR models that simply stack multiple small kernel convolutions or complex\noperators to learn representations, we explore a large kernel ConvNet for\nmobile-friendly SR design. Specifically, we develop a large depth-wise\nconvolution and two projection layers based on channel splitting and shuffling\nas the basic component to mix features efficiently. Since the contexts of\nnatural images are strongly locally correlated, using large depth-wise\nconvolutions only is insufficient to reconstruct fine details. To overcome this\nproblem while maintaining the efficiency of the proposed module, we introduce\nFused-MBConvs into the proposed network to model the local connectivity of\ndifferent features. Experimental results demonstrate that the proposed\nShuffleMixer is about 6x smaller than the state-of-the-art methods in terms of\nmodel parameters and FLOPs while achieving competitive performance. In NTIRE\n2022, our primary method won the model complexity track of the Efficient\nSuper-Resolution Challenge [23]. The code is available at\nhttps://github.com/sunny2109/MobileSR-NTIRE2022.",
    "descriptor": "\nComments: Winner of the model complexity track in NTIRE2022 Efficient Super-Resolution Challenge, CVPR 2022. The code is available at this https URL\n",
    "authors": [
      "Long Sun",
      "Jinshan Pan",
      "Jinhui Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15175"
  },
  {
    "id": "arXiv:2205.15178",
    "title": "Development of a hybrid model-based data-driven collision avoidance  algorithm for vehicles in low adhesion conditions",
    "abstract": "Winter conditions, characterized by the presence of ice and snow on the\nground, are more likely to lead to road accidents. This paper presents an\nexperimental proof of concept of a collision avoidance algorithm for vehicles\nevolving in low adhesion conditions, implemented on a 1/5th scale car platform.\nIn the proposed approach, a model-based estimator first processes the\nhigh-dimensional sensors data of the IMU, LIDAR and encoders to estimate\nphysically relevant vehicle and ground conditions parameters such as the\ninertial velocity of the vehicle $v$, the friction coefficient $\\mu$, the\ncohesion $c$ and the internal shear angle $\\phi$. Then, a data-driven predictor\nis trained to predict the optimal maneuver to perform in the situation\ncharacterized by the estimated parameters. Experiments show that it is possible\nto 1) produce a real-time estimate of the relevant ground parameters, and 2)\ndetermine an optimal collision avoidance maneuver based on the estimated\nparameters.",
    "descriptor": "",
    "authors": [
      "Olivier Lecompte",
      "William Therrien",
      "Alexandre Girard"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.15178"
  },
  {
    "id": "arXiv:2205.15180",
    "title": "T-Wise Presence Condition Coverage and Sampling for Configurable Systems",
    "abstract": "Sampling techniques, such as t-wise interaction sampling are used to enable\nefficient testing for configurable systems. This is achieved by generating a\nsmall yet representative sample of configurations for a system, which\ncircumvents testing the entire solution space. However, by design, most recent\napproaches for t-wise interaction sampling only consider combinations of\nconfiguration options from a configurable system's variability model and do not\ntake into account their mapping onto the solution space, thus potentially\nleaving critical implementation artifacts untested. Tartler et al. address this\nproblem by considering presence conditions of implementation artifacts rather\nthan pure configuration options, but do not consider the possible interactions\nbetween these artifacts. In this paper, we introduce t-wise presence condition\ncoverage, which extends the approach of Tartler et al. by using presence\nconditions extracted from the code as basis to cover t-wise interactions. This\nensures that all t-wise interactions of implementation artifacts are included\nin the sample and that the chance of detecting combinations of faulty\nconfiguration options is increased. We evaluate our approach in terms of\ntesting efficiency and testing effectiveness by comparing the approach to\nexisting t-wise interaction sampling techniques. We show that t-wise presence\ncondition sampling is able to produce mostly smaller samples compared to t-wise\ninteraction sampling, while guaranteeing a t-wise presence condition coverage\nof 100%.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Sebastian Krieter",
      "Thomas Th\u00fcm",
      "Sandro Schulze",
      "Sebastian Ruland",
      "Malte Lochau",
      "Gunter Saake",
      "Thomas Leich"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.15180"
  },
  {
    "id": "arXiv:2205.15181",
    "title": "A Review and Evaluation of Elastic Distance Functions for Time Series  Clustering",
    "abstract": "Time series clustering is the act of grouping time series data without\nrecourse to a label. Algorithms that cluster time series can be classified into\ntwo groups: those that employ a time series specific distance measure; and\nthose that derive features from time series. Both approaches usually rely on\ntraditional clustering algorithms such as $k$-means. Our focus is on distance\nbased time series that employ elastic distance measures, i.e. distances that\nperform some kind of realignment whilst measuring distance. We describe nine\ncommonly used elastic distance measures and compare their performance with\nk-means and k-medoids clustering. Our findings are surprising. The most popular\ntechnique, dynamic time warping (DTW), performs worse than Euclidean distance\nwith k-means, and even when tuned, is no better. Using k-medoids rather than\nk-means improved the clusterings for all nine distance measures. DTW is not\nsignificantly better than Euclidean distance with k-medoids. Generally,\ndistance measures that employ editing in conjunction with warping perform\nbetter, and one distance measure, the move-split-merge (MSM) method, is the\nbest performing measure of this study. We also compare to clustering with DTW\nusing barycentre averaging (DBA). We find that DBA does improve DTW k-means,\nbut that the standard DBA is still worse than using MSM. Our conclusion is to\nrecommend MSM with k-medoids as the benchmark algorithm for clustering time\nseries with elastic distance measures. We provide implementations, results and\nguidance on reproducing results on the associated GitHub repository.",
    "descriptor": "",
    "authors": [
      "Chris Holder",
      "Matthew Middlehurst",
      "Anthony Bagnall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15181"
  },
  {
    "id": "arXiv:2205.15187",
    "title": "Do Deep Neural Networks Always Perform Better When Eating More Data?",
    "abstract": "Data has now become a shortcoming of deep learning. Researchers in their own\nfields share the thinking that \"deep neural networks might not always perform\nbetter when they eat more data,\" which still lacks experimental validation and\na convincing guiding theory. Here to fill this lack, we design experiments from\nIdentically Independent Distribution(IID) and Out of Distribution(OOD), which\ngive powerful answers. For the purpose of guidance, based on the discussion of\nresults, two theories are proposed: under IID condition, the amount of\ninformation determines the effectivity of each sample, the contribution of\nsamples and difference between classes determine the amount of sample\ninformation and the amount of class information; under OOD condition, the\ncross-domain degree of samples determine the contributions, and the\nbias-fitting caused by irrelevant elements is a significant factor of\ncross-domain. The above theories provide guidance from the perspective of data,\nwhich can promote a wide range of practical applications of artificial\nintelligence.",
    "descriptor": "",
    "authors": [
      "Jiachen Yang",
      "Zhuo Zhang",
      "Yicheng Gong",
      "Shukun Ma",
      "Xiaolan Guo",
      "Yue Yang",
      "Shuai Xiao",
      "Jiabao Wen",
      "Yang Li",
      "Xinbo Gao",
      "Wen Lu",
      "Qinggang Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15187"
  },
  {
    "id": "arXiv:2205.15195",
    "title": "Personalized Acoustic Echo Cancellation for Full-duplex Communications",
    "abstract": "Deep neural networks (DNNs) have shown promising results for acoustic echo\ncancellation (AEC). But the DNN-based AEC models let through all near-end\nspeakers including the interfering speech. In light of recent studies on\npersonalized speech enhancement, we investigate the feasibility of personalized\nacoustic echo cancellation (PAEC) in this paper for full-duplex communications,\nwhere background noise and interfering speakers may coexist with acoustic\nechoes. Specifically, we first propose a novel backbone neural network termed\nas gated temporal convolutional neural network (GTCNN) that outperforms\nstate-of-the-art AEC models in performance. Speaker embeddings like d-vectors\nare further adopted as auxiliary information to guide the GTCNN to focus on the\ntarget speaker. A special case in PAEC is that speech snippets of both parties\non the call are enrolled. Experimental results show that auxiliary information\nfrom either the near-end speaker or the far-end speaker can improve the\nDNN-based AEC performance. Nevertheless, there is still much room for\nimprovement in the utilization of the finite-dimensional speaker embeddings.",
    "descriptor": "\nComments: submitted to INTERSPEECH 22\n",
    "authors": [
      "Shimin Zhang",
      "Ziteng Wang",
      "Yukai Ju",
      "Yihui Fu",
      "Yueyue Na",
      "Qiang Fu",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.15195"
  },
  {
    "id": "arXiv:2205.15196",
    "title": "PAC Generalisation via Invariant Representations",
    "abstract": "One method for obtaining generalizable solutions to machine learning tasks\nwhen presented with diverse training environments is to find invariant\nrepresentations of the data. These are representations of the covariates such\nthat the best model on top of the representation is invariant across training\nenvironments. In the context of linear Structural Equation Models (SEMs),\ninvariant representations might allow us to learn models with\nout-of-distribution guarantees, i.e., models that are robust to interventions\nin the SEM. To address the invariant representation problem in a finite sample\nsetting, we consider the notion of $\\epsilon$-approximate invariance. We study\nthe following question: If a representation is approximately invariant with\nrespect to a given number of training interventions, will it continue to be\napproximately invariant on a larger collection of unseen SEMs? This larger\ncollection of SEMs is generated through a parameterized family of\ninterventions. Inspired by PAC learning, we obtain finite-sample\nout-of-distribution generalization guarantees for approximate invariance that\nholds probabilistically over a family of linear SEMs without faithfulness\nassumptions. Our results show bounds that do not scale in ambient dimension\nwhen intervention sites are restricted to lie in a constant size subset of\nin-degree bounded nodes. We also show how to extend our results to a linear\nindirect observation model that incorporates latent variables.",
    "descriptor": "",
    "authors": [
      "Advait Parulekar",
      "Karthikeyan Shanmugam",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15196"
  },
  {
    "id": "arXiv:2205.15198",
    "title": "STN: Scalable Tensorizing Networks via Structure-Aware Training and  Adaptive Compression",
    "abstract": "Deep neural networks (DNNs) have delivered a remarkable performance in many\ntasks of computer vision. However, over-parameterized representations of\npopular architectures dramatically increase their computational complexity and\nstorage costs, and hinder their availability in edge devices with constrained\nresources. Regardless of many tensor decomposition (TD) methods that have been\nwell-studied for compressing DNNs to learn compact representations, they suffer\nfrom non-negligible performance degradation in practice. In this paper, we\npropose Scalable Tensorizing Networks (STN), which dynamically and adaptively\nadjust the model size and decomposition structure without retraining. First, we\naccount for compression during training by adding a low-rank regularizer to\nguarantee networks' desired low-rank characteristics in full tensor format.\nThen, considering network layers exhibit various low-rank structures, STN is\nobtained by a data-driven adaptive TD approach, for which the topological\nstructure of decomposition per layer is learned from the pre-trained model, and\nthe ranks are selected appropriately under specified storage constraints. As a\nresult, STN is compatible with arbitrary network architectures and achieves\nhigher compression performance and flexibility over other tensorizing versions.\nComprehensive experiments on several popular architectures and benchmarks\nsubstantiate the superiority of our model towards improving parameter\nefficiency.",
    "descriptor": "",
    "authors": [
      "Chang Nie",
      "Huan Wang",
      "Lu Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15198"
  },
  {
    "id": "arXiv:2205.15201",
    "title": "Controller design and experimental evaluation of a motorised assistance  for a patient transfer floor lift",
    "abstract": "Patient transfer is a critical task in patient care and medical sectors.\nHowever, it is challenging because it exposes caregivers to injury risks.\nAvailable transfer devices, like floor lifts, lead to improvements but are far\nfrom perfect. They do not eliminate the caregivers' risk for musculoskeletal\ndisorders, and they can be burdensome to use due to their poor manoeuvrability.\nThis paper presents a new motorised floor lift with a single central motorised\nwheel connected to an instrumented handle. It proposes admittance controllers\n(the reference velocity of the motorised wheel, is computed as a function of\nthe force applied on the handle), 1) to achieve the best manoeuvrability, 2) to\nreduce the effort required in a task, decreasing the risk of injuries among\ncaregivers and 3) while guaranteeing the security and comfort of patients. Two\ncontroller designs, one with a linear admittance law and a non-linear\nadmittance law with variable damping, were developed and implemented on a\nprototype. Tests were performed on seven participants to evaluate the\nperformance of the assistance system and the controllers. The experimental\nresults show that 1) the motorised assistance with the variable damping\ncontroller improves manoeuvrability by 28%, 2) reduces the amount of effort\nrequired to push the lift by 66% and 3) provides the same level of patient\ncomfort compared to a standard unassisted floor lift.",
    "descriptor": "",
    "authors": [
      "Donatien Callon",
      "Mathieu Nadeau",
      "Alexandre Girard"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.15201"
  },
  {
    "id": "arXiv:2205.15202",
    "title": "A Small Leak Will Sink Many Ships: Vulnerabilities Related to Mini  Programs Permissions",
    "abstract": "As a new format of mobile application, mini programs, which function within a\nlarger app and are built with HTML, CSS, and JavaScript web technology, have\nbecome the way to do almost everything in China. This paper presents our\nresearch on the permissions of mini programs. We conducted a systematic study\non 9 popular mobile app ecosystems, which host over 7 million mini programs,\nand tested over 2,580 APIs to understand these emerging systems better. We\nextracted a common abstracted model for mini programs permission control and\nrevealed six categories of potential security vulnerabilities in the permission\nenvironments. It is alarming that the current popular mobile app ecosystems\n(host apps) under study have at least one security vulnerability. We present\nthe corresponding attack methods to dissect these potential weaknesses further\nto exploit the discovered vulnerabilities. To prove that the revealed\nvulnerabilities may cause severe consequences in real-world use, we show three\nkinds of attacks related to the mini programs' permissions. We have responsibly\ndisclosed the newly discovered vulnerabilities, officially confirmed and\nrevised. Finally, we put forward systematic suggestions to strengthen the\nstandardization of mini programs.",
    "descriptor": "",
    "authors": [
      "Jianyi Zhang",
      "Leixin Yang",
      "Yuyang Han",
      "Zhi Sun",
      "Zixiao Xiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.15202"
  },
  {
    "id": "arXiv:2205.15203",
    "title": "Exponentials as Substitutions and the Cost of Cut Elimination in Linear  Logic",
    "abstract": "This paper introduces the exponential substitution calculus (ESC), a new\npresentation of cut elimination for IMELL, based on proof terms and building on\nthe idea that exponentials can be seen as explicit substitutions. The idea in\nitself is not new, but here it is pushed to a new level, inspired by Accattoli\nand Kesner's linear substitution calculus (LSC).\nOne of the key properties of the LSC is that it naturally models the sub-term\nproperty of abstract machines, that is the key ingredient for the study of\nreasonable time cost models for the $\\lambda$-calculus. The new ESC is then\nused to design a cut elimination strategy with the sub-term property, providing\nthe first polynomial cost model for cut elimination with unconstrained\nexponentials.\nFor the ESC, we also prove untyped confluence and typed strong normalization,\nshowing that it is an alternative to proof nets for an advanced study of cut\nelimination.",
    "descriptor": "\nComments: Version with proofs of the LICS 2022 paper with the same title\n",
    "authors": [
      "Beniamino Accattoli"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.15203"
  },
  {
    "id": "arXiv:2205.15204",
    "title": "Programming with rules and everything else, seamlessly",
    "abstract": "Logic rules are powerful for expressing complex reasoning and analysis\nproblems. At the same time, they are inconvenient or impossible to use for many\nother aspects of applications. Integrating rules in a language with sets and\nfunctions, and furthermore with updates to objects, has been a subject of\nsignificant study. What's lacking is a language that integrates all constructs\nseamlessly.\nThis paper presents a language, Alda, that supports all of rules, sets,\nfunctions, updates, and objects as seamlessly integrated built-ins, including\nconcurrent and distributed processes. The key idea is to support predicates as\nset-valued variables that can be used and updated in any scope, and support\nqueries and inference with both explicit and automatic calls to an inference\nfunction. We develop a complete formal semantics for Alda. We design a\ncompilation framework that ensures the declarative semantics of rules, while\nalso being able to exploit available optimizations. We describe a prototype\nimplementation that builds on a powerful extension of Python and employs an\nefficient logic rule engine. We develop a range of benchmarks and present\nresults of experiments to demonstrate Alda's power for programming and\ngenerally good performance.",
    "descriptor": "",
    "authors": [
      "Yanhong A. Liu",
      "Scott D. Stoller",
      "Yi Tong",
      "Bo Lin",
      "K. Tuncay Tekle"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.15204"
  },
  {
    "id": "arXiv:2205.15209",
    "title": "Flowification: Everything is a Normalizing Flow",
    "abstract": "We develop a method that can be used to turn any multi-layer perceptron or\nconvolutional network into a normalizing flow. In some cases this requires the\naddition of uncorrelated noise to the model but in the simplest case no\nadditional parameters. The technique we develop can be applied to a broad range\nof architectures, allowing them to be used for a wide range of tasks. Our\nmodels also allow existing density estimation techniques to be combined with\nhigh performance feature extractors. In contrast to standard density estimation\ntechniques that require specific architectures and specialized knowledge, our\napproach can leverage design knowledge from other domains and is a step closer\nto the realization of general purpose architectures. We investigate the\nefficacy of linear and convolutional layers for the task of density estimation\non standard datasets. Our results suggest standard layers lack something\nfundamental in comparison to other normalizing flows.",
    "descriptor": "",
    "authors": [
      "B\u00e1lint M\u00e1t\u00e9",
      "Samuel Klein",
      "Tobias Golling",
      "Fran\u00e7ois Fleuret"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15209"
  },
  {
    "id": "arXiv:2205.15210",
    "title": "The Devil is in the Pose: Ambiguity-free 3D Rotation-invariant Learning  via Pose-aware Convolution",
    "abstract": "Rotation-invariant (RI) 3D deep learning methods suffer performance\ndegradation as they typically design RI representations as input that lose\ncritical global information comparing to 3D coordinates. Most state-of-the-arts\naddress it by incurring additional blocks or complex global representations in\na heavy and ineffective manner. In this paper, we reveal that the global\ninformation loss stems from an unexplored pose information loss problem, which\ncan be solved more efficiently and effectively as we only need to restore more\nlightweight local pose in each layer, and the global information can be\nhierarchically aggregated in the deep networks without extra efforts. To\naddress this problem, we develop a Pose-aware Rotation Invariant Convolution\n(i.e., PaRI-Conv), which dynamically adapts its kernels based on the relative\nposes. To implement it, we propose an Augmented Point Pair Feature (APPF) to\nfully encode the RI relative pose information, and a factorized dynamic kernel\nfor pose-aware kernel generation, which can further reduce the computational\ncost and memory burden by decomposing the kernel into a shared basis matrix and\na pose-aware diagonal matrix. Extensive experiments on shape classification and\npart segmentation tasks show that our PaRI-Conv surpasses the state-of-the-art\nRI methods while being more compact and efficient.",
    "descriptor": "\nComments: Accepted by CVPR 2022\n",
    "authors": [
      "Ronghan Chen",
      "Yang Cong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15210"
  },
  {
    "id": "arXiv:2205.15211",
    "title": "Type Systems in Resource-Aware Programming: Opportunities and Challenges",
    "abstract": "Type systems provide software developers immediate feedback about a subset of\ncorrectness properties of their programs. IDE integrations often take advantage\nof type systems to present errors, suggest completions and even improve\nnavigation. On the other hand, understanding the time and energy consumption of\nthe execution of a program requires manual testing.\nIn this paper, we identify existing work on using type systems for energy\nawareness, and define the requirements for a practical approach, which the\nexisting approaches do not address fully.\nFurthermore, we also discuss how existing type systems can help generalize\nrefactors for energy-efficiency.",
    "descriptor": "",
    "authors": [
      "Alcides Fonseca",
      "Guilherme Espada"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.15211"
  },
  {
    "id": "arXiv:2205.15213",
    "title": "Gradient Backpropagation Through Combinatorial Algorithms: Identity with  Projection Works",
    "abstract": "Embedding discrete solvers as differentiable layers has given modern deep\nlearning architectures combinatorial expressivity and discrete reasoning\ncapabilities. The derivative of these solvers is zero or undefined, therefore a\nmeaningful replacement is crucial for effective gradient-based learning. Prior\nworks rely on smoothing the solver with input perturbations, relaxing the\nsolver to continuous problems, or interpolating the loss landscape with\ntechniques that typically require additional solver calls, introduce extra\nhyper-parameters or compromise performance. We propose a principled approach to\nexploit the geometry of the discrete solution space to treat the solver as a\nnegative identity on the backward pass and further provide a theoretical\njustification. Our experiments demonstrate that such a straightforward\nhyper-parameter-free approach is on-par with or outperforms previous more\ncomplex methods on numerous experiments such as Traveling Salesman Problem,\nShortest Path, Deep Graph Matching, and backpropagating through discrete\nsamplers. Furthermore, we substitute the previously proposed problem-specific\nand label-dependent margin by a generic regularization procedure that prevents\ncost collapse and increases robustness.",
    "descriptor": "",
    "authors": [
      "Subham Sekhar Sahoo",
      "Marin Vlastelica",
      "Anselm Paulus",
      "V\u00edt Musil",
      "Volodymyr Kuleshov",
      "Georg Martius"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15213"
  },
  {
    "id": "arXiv:2205.15217",
    "title": "GraphWalks: Efficient Shape Agnostic Geodesic Shortest Path Estimation",
    "abstract": "Geodesic paths and distances are among the most popular intrinsic properties\nof 3D surfaces. Traditionally, geodesic paths on discrete polygon surfaces were\ncomputed using shortest path algorithms, such as Dijkstra. However, such\nalgorithms have two major limitations. They are non-differentiable which limits\ntheir direct usage in learnable pipelines and they are considerably time\ndemanding. To address such limitations and alleviate the computational burden,\nwe propose a learnable network to approximate geodesic paths. The proposed\nmethod is comprised by three major components: a graph neural network that\nencodes node positions in a high dimensional space, a path embedding that\ndescribes previously visited nodes and a point classifier that selects the next\npoint in the path. The proposed method provides efficient approximations of the\nshortest paths and geodesic distances estimations. Given that all of the\ncomponents of our method are fully differentiable, it can be directly plugged\ninto any learnable pipeline as well as customized under any differentiable\nconstraint. We extensively evaluate the proposed method with several\nqualitative and quantitative experiments.",
    "descriptor": "\nComments: CVPRw 2022\n",
    "authors": [
      "Rolandos Alexandros Potamias",
      "Alexandros Neofytou",
      "Kyriaki-Margarita Bintsi",
      "Stefanos Zafeiriou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15217"
  },
  {
    "id": "arXiv:2205.15218",
    "title": "A Graph and Attentive Multi-Path Convolutional Network for Traffic  Prediction",
    "abstract": "Traffic prediction is an important and yet highly challenging problem due to\nthe complexity and constantly changing nature of traffic systems. To address\nthe challenges, we propose a graph and attentive multi-path convolutional\nnetwork (GAMCN) model to predict traffic conditions such as traffic speed\nacross a given road network into the future. Our model focuses on the spatial\nand temporal factors that impact traffic conditions. To model the spatial\nfactors, we propose a variant of the graph convolutional network (GCN) named\nLPGCN to embed road network graph vertices into a latent space, where vertices\nwith correlated traffic conditions are close to each other. To model the\ntemporal factors, we use a multi-path convolutional neural network (CNN) to\nlearn the joint impact of different combinations of past traffic conditions on\nthe future traffic conditions. Such a joint impact is further modulated by an\nattention} generated from an embedding of the prediction time, which encodes\nthe periodic patterns of traffic conditions. We evaluate our model on\nreal-world road networks and traffic data. The experimental results show that\nour model outperforms state-of-art traffic prediction models by up to 18.9% in\nterms of prediction errors and 23.4% in terms of prediction efficiency.",
    "descriptor": "\nComments: Accepted to appear in IEEE Transactions on Knowledge and Data Engineering\n",
    "authors": [
      "Jianzhong Qi",
      "Zhuowei Zhao",
      "Egemen Tanin",
      "Tingru Cui",
      "Neema Nassir",
      "Majid Sarvi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15218"
  },
  {
    "id": "arXiv:2205.15219",
    "title": "Automatic Short Math Answer Grading via In-context Meta-learning",
    "abstract": "Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.",
    "descriptor": "",
    "authors": [
      "Mengxue Zhang",
      "Sami Baral",
      "Neil Heffernan",
      "Andrew Lan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15219"
  },
  {
    "id": "arXiv:2205.15223",
    "title": "Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained  Models",
    "abstract": "Pre-trained masked language models successfully perform few-shot learning by\nformulating downstream tasks as text infilling. However, as a strong\nalternative in full-shot settings, discriminative pre-trained models like\nELECTRA do not fit into the paradigm. In this work, we adapt prompt-based\nfew-shot learning to ELECTRA and show that it outperforms masked language\nmodels in a wide range of tasks. ELECTRA is pre-trained to distinguish if a\ntoken is generated or original. We naturally extend that to prompt-based\nfew-shot learning by training to score the originality of the target options\nwithout introducing new parameters. Our method can be easily adapted to tasks\ninvolving multi-token predictions without extra computation overhead. Analysis\nshows that ELECTRA learns distributions that align better with downstream\ntasks.",
    "descriptor": "",
    "authors": [
      "Mengzhou Xia",
      "Mikel Artetxe",
      "Jingfei Du",
      "Danqi Chen",
      "Ves Stoyanov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15223"
  },
  {
    "id": "arXiv:2205.15225",
    "title": "Few-shot Class-incremental Learning for 3D Point Cloud Objects",
    "abstract": "Few-shot class-incremental learning (FSCIL) aims to incrementally fine-tune a\nmodel trained on base classes for a novel set of classes using a few examples\nwithout forgetting the previous training. Recent efforts of FSCIL address this\nproblem primarily on 2D image data. However, due to the advancement of camera\ntechnology, 3D point cloud data has become more available than ever, which\nwarrants considering FSCIL on 3D data. In this paper, we address FSCIL in the\n3D domain. In addition to well-known problems of catastrophic forgetting of\npast knowledge and overfitting of few-shot data, 3D FSCIL can bring newer\nchallenges. For example, base classes may contain many synthetic instances in a\nrealistic scenario. In contrast, only a few real-scanned samples (from RGBD\nsensors) of novel classes are available in incremental steps. Due to the data\nvariation from synthetic to real, FSCIL endures additional challenges,\ndegrading performance in later incremental steps. We attempt to solve this\nproblem by using Microshapes (orthogonal basis vectors) describing any 3D\nobjects using a pre-defined set of rules. It supports incremental training with\nfew-shot examples minimizing synthetic to real data variation. We propose new\ntest protocols for 3D FSCIL using popular synthetic datasets, ModelNet and\nShapeNet, and 3D real-scanned datasets, ScanObjectNN, and Common Objects in 3D\n(CO3D). By comparing state-of-the-art methods, we establish the effectiveness\nof our approach in the 3D domain.",
    "descriptor": "",
    "authors": [
      "Townim Chowdhury",
      "Ali Cheraghian",
      "Sameera Ramasinghe",
      "Sahar Ahmadi",
      "Morteza Saberi",
      "Shafin Rahman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15225"
  },
  {
    "id": "arXiv:2205.15231",
    "title": "A Survey in Mathematical Language Processing",
    "abstract": "Informal mathematical text underpins real-world quantitative reasoning and\ncommunication. Developing sophisticated methods of retrieval and abstraction\nfrom this dual modality is crucial in the pursuit of the vision of automating\ndiscovery in quantitative science and mathematics. We track the development of\ninformal mathematical language processing approaches across five strategic\nsub-areas in recent years, highlighting the prevailing successful\nmethodological elements along with existing limitations.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Jordan Meadows",
      "Andre Freitas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.15231"
  },
  {
    "id": "arXiv:2205.15234",
    "title": "Few-Shot Adaptation of Pre-Trained Networks for Domain Shift",
    "abstract": "Deep networks are prone to performance degradation when there is a domain\nshift between the source (training) data and target (test) data. Recent\ntest-time adaptation methods update batch normalization layers of pre-trained\nsource models deployed in new target environments with streaming data to\nmitigate such performance degradation. Although such methods can adapt\non-the-fly without first collecting a large target domain dataset, their\nperformance is dependent on streaming conditions such as mini-batch size and\nclass-distribution, which can be unpredictable in practice. In this work, we\npropose a framework for few-shot domain adaptation to address the practical\nchallenges of data-efficient adaptation. Specifically, we propose a constrained\noptimization of feature normalization statistics in pre-trained source models\nsupervised by a small support set from the target domain. Our method is easy to\nimplement and improves source model performance with as few as one sample per\nclass for classification tasks. Extensive experiments on 5 cross-domain\nclassification and 4 semantic segmentation datasets show that our method\nachieves more accurate and reliable performance than test-time adaptation,\nwhile not being constrained by streaming conditions.",
    "descriptor": "\nComments: Accepted to IJCAI 2022\n",
    "authors": [
      "Wenyu Zhang",
      "Li Shen",
      "Wanyue Zhang",
      "Chuan-Sheng Foo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15234"
  },
  {
    "id": "arXiv:2205.15235",
    "title": "Non-convex online learning via algorithmic equivalence",
    "abstract": "We study an algorithmic equivalence technique between nonconvex gradient\ndescent and convex mirror descent. We start by looking at a harder problem of\nregret minimization in online non-convex optimization. We show that under\ncertain geometric and smoothness conditions, online gradient descent applied to\nnon-convex functions is an approximation of online mirror descent applied to\nconvex functions under reparameterization. In continuous time, the gradient\nflow with this reparameterization was shown to be exactly equivalent to\ncontinuous-time mirror descent by Amid and Warmuth 2020, but theory for the\nanalogous discrete time algorithms is left as an open problem. We prove an\n$O(T^{\\frac{2}{3}})$ regret bound for non-convex online gradient descent in\nthis setting, answering this open problem. Our analysis is based on a new and\nsimple algorithmic equivalence method.",
    "descriptor": "",
    "authors": [
      "Udaya Ghai",
      "Zhou Lu",
      "Elad Hazan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.15235"
  },
  {
    "id": "arXiv:2205.15236",
    "title": "RankSim: Ranking Similarity Regularization for Deep Imbalanced  Regression",
    "abstract": "Data imbalance, in which a plurality of the data samples come from a small\nproportion of labels, poses a challenge in training deep neural networks.\nUnlike classification, in regression the labels are continuous, potentially\nboundless, and form a natural ordering. These distinct features of regression\ncall for new techniques that leverage the additional information encoded in\nlabel-space relationships. This paper presents the RankSim (ranking similarity)\nregularizer for deep imbalanced regression, which encodes an inductive bias\nthat samples that are closer in label space should also be closer in feature\nspace. In contrast to recent distribution smoothing based approaches, RankSim\ncaptures both nearby and distant relationships: for a given data sample,\nRankSim encourages the sorted list of its neighbors in label space to match the\nsorted list of its neighbors in feature space. RankSim is complementary to\nconventional imbalanced learning techniques, including re-weighting, two-stage\ntraining, and distribution smoothing, and lifts the state-of-the-art\nperformance on three imbalanced regression benchmarks: IMDB-WIKI-DIR,\nAgeDB-DIR, and STS-B-DIR.",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Yu Gong",
      "Greg Mori",
      "Frederick Tung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15236"
  },
  {
    "id": "arXiv:2205.15237",
    "title": "VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models",
    "abstract": "Recent advances in vision-language pre-training (VLP) have demonstrated\nimpressive performance in a range of vision-language (VL) tasks. However, there\nexist several challenges for measuring the community's progress in building\ngeneral multi-modal intelligence. First, most of the downstream VL datasets are\nannotated using raw images that are already seen during pre-training, which may\nresult in an overestimation of current VLP models' generalization ability.\nSecond, recent VLP work mainly focuses on absolute performance but overlooks\nthe efficiency-performance trade-off, which is also an important indicator for\nmeasuring progress.\nTo this end, we introduce the Vision-Language Understanding Evaluation (VLUE)\nbenchmark, a multi-task multi-dimension benchmark for evaluating the\ngeneralization capabilities and the efficiency-performance trade-off (``Pareto\nSOTA'') of VLP models. We demonstrate that there is a sizable generalization\ngap for all VLP models when testing on out-of-distribution test sets annotated\non images from a more diverse distribution that spreads across cultures.\nMoreover, we find that measuring the efficiency-performance trade-off of VLP\nmodels leads to complementary insights for several design choices of VLP. We\nrelease the VLUE benchmark to promote research on building vision-language\nmodels that generalize well to more diverse images and concepts unseen during\npre-training, and are practical in terms of efficiency-performance trade-off.",
    "descriptor": "\nComments: ICML 2022, Benchmark website at this https URL\n",
    "authors": [
      "Wangchunshu Zhou",
      "Yan Zeng",
      "Shizhe Diao",
      "Xinsong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15237"
  },
  {
    "id": "arXiv:2205.15241",
    "title": "Multi-Game Decision Transformers",
    "abstract": "A longstanding goal of the field of AI is a strategy for compiling diverse\nexperience into a highly capable, generalist agent. In the subfields of vision\nand language, this was largely achieved by scaling up transformer-based models\nand training them on large, diverse datasets. Motivated by this progress, we\ninvestigate whether the same strategy can be used to produce generalist\nreinforcement learning agents. Specifically, we show that a single\ntransformer-based model - with a single set of weights - trained purely offline\ncan play a suite of up to 46 Atari games simultaneously at close-to-human\nperformance. When trained and evaluated appropriately, we find that the same\ntrends observed in language and vision hold, including scaling of performance\nwith model size and rapid adaptation to new games via fine-tuning. We compare\nseveral approaches in this multi-game setting, such as online and offline RL\nmethods and behavioral cloning, and find that our Multi-Game Decision\nTransformer models offer the best scalability and performance. We release the\npre-trained models and code to encourage further research in this direction.\nAdditional information, videos and code can be seen at:\nsites.google.com/view/multi-game-transformers",
    "descriptor": "",
    "authors": [
      "Kuang-Huei Lee",
      "Ofir Nachum",
      "Mengjiao Yang",
      "Lisa Lee",
      "Daniel Freeman",
      "Winnie Xu",
      "Sergio Guadarrama",
      "Ian Fischer",
      "Eric Jang",
      "Henryk Michalewski",
      "Igor Mordatch"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15241"
  },
  {
    "id": "arXiv:2205.15242",
    "title": "Re-parameterizing Your Optimizers rather than Architectures",
    "abstract": "The well-designed structures in neural networks reflect the prior knowledge\nincorporated into the models. However, though different models have various\npriors, we are used to training them with model-agnostic optimizers (e.g.,\nSGD). In this paper, we propose a novel paradigm of incorporating\nmodel-specific prior knowledge into optimizers and using them to train generic\n(simple) models. As an implementation, we propose a novel methodology to add\nprior knowledge by modifying the gradients according to a set of model-specific\nhyper-parameters, which is referred to as Gradient Re-parameterization, and the\noptimizers are named RepOptimizers. For the extreme simplicity of model\nstructure, we focus on a VGG-style plain model and showcase that such a simple\nmodel trained with a RepOptimizer, which is referred to as RepOpt-VGG, performs\non par with the recent well-designed models. From a practical perspective,\nRepOpt-VGG is a favorable base model because of its simple structure, high\ninference speed and training efficiency. Compared to Structural\nRe-parameterization, which adds priors into models via constructing extra\ntraining-time structures, RepOptimizers require no extra forward/backward\ncomputations and solve the problem of quantization. The code and models are\npublicly available at https://github.com/DingXiaoH/RepOptimizers.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Xiaohan Ding",
      "Honghao Chen",
      "Xiangyu Zhang",
      "Kaiqi Huang",
      "Jungong Han",
      "Guiguang Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15242"
  },
  {
    "id": "arXiv:2205.15245",
    "title": "Residual Q-Networks for Value Function Factorizing in Multi-Agent  Reinforcement Learning",
    "abstract": "Multi-Agent Reinforcement Learning (MARL) is useful in many problems that\nrequire the cooperation and coordination of multiple agents. Learning optimal\npolicies using reinforcement learning in a multi-agent setting can be very\ndifficult as the number of agents increases. Recent solutions such as Value\nDecomposition Networks (VDN), QMIX, QTRAN and QPLEX adhere to the centralized\ntraining and decentralized execution scheme and perform factorization of the\njoint action-value functions. However, these methods still suffer from\nincreased environmental complexity, and at times fail to converge in a stable\nmanner. We propose a novel concept of Residual Q-Networks (RQNs) for MARL,\nwhich learns to transform the individual Q-value trajectories in a way that\npreserves the Individual-Global-Max criteria (IGM), but is more robust in\nfactorizing action-value functions. The RQN acts as an auxiliary network that\naccelerates convergence and will become obsolete as the agents reach the\ntraining objectives. The performance of the proposed method is compared against\nseveral state-of-the-art techniques such as QPLEX, QMIX, QTRAN and VDN, in a\nrange of multi-agent cooperative tasks. The results illustrate that the\nproposed method, in general, converges faster, with increased stability and\nshows robust performance in a wider family of environments. The improvements in\nresults are more prominent in environments with severe punishments for\nnon-cooperative behaviours and especially in the absence of complete state\ninformation during training time.",
    "descriptor": "\nComments: Accepted for publication on IEEE Transactions on Neural Networks and Learning Systems\n",
    "authors": [
      "Rafael Pina",
      "Varuna De Silva",
      "Joosep Hook",
      "Ahmet Kondoz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.15245"
  },
  {
    "id": "arXiv:2205.15254",
    "title": "Pooling Revisited: Your Receptive Field is Suboptimal",
    "abstract": "The size and shape of the receptive field determine how the network\naggregates local information and affect the overall performance of a model\nconsiderably. Many components in a neural network, such as kernel sizes and\nstrides for convolution and pooling operations, influence the configuration of\na receptive field. However, they still rely on hyperparameters, and the\nreceptive fields of existing models result in suboptimal shapes and sizes.\nHence, we propose a simple yet effective Dynamically Optimized Pooling\noperation, referred to as DynOPool, which optimizes the scale factors of\nfeature maps end-to-end by learning the desirable size and shape of its\nreceptive field in each layer. Any kind of resizing modules in a deep neural\nnetwork can be replaced by the operations with DynOPool at a minimal cost.\nAlso, DynOPool controls the complexity of a model by introducing an additional\nloss term that constrains computational cost. Our experiments show that the\nmodels equipped with the proposed learnable resizing module outperform the\nbaseline networks on multiple datasets in image classification and semantic\nsegmentation.",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Dong-Hwan Jang",
      "Sanghyeok Chu",
      "Joonhyuk Kim",
      "Bohyung Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15254"
  },
  {
    "id": "arXiv:2205.15258",
    "title": "Transparency, Governance and Regulation of Algorithmic Tools Deployed in  the Criminal Justice System: a UK Case Study",
    "abstract": "We present a survey of tools used in the criminal justice system in the UK in\nthree categories: data infrastructure, data analysis, and risk prediction. Many\ntools are currently in deployment, offering potential benefits, including\nimproved efficiency and consistency. However, there are also important\nconcerns. Transparent information about these tools, their purpose, how they\nare used, and by whom is difficult to obtain. Even when information is\navailable, it is often insufficient to enable a satisfactory evaluation. More\nwork is needed to establish governance mechanisms to ensure that tools are\ndeployed in a transparent, safe and ethical way. We call for more engagement\nwith stakeholders and greater documentation of the intended goal of a tool, how\nit will achieve this goal compared to other options, and how it will be\nmonitored in deployment. We highlight additional points to consider when\nevaluating the trustworthiness of deployed tools and make concrete proposals\nfor policy.",
    "descriptor": "\nComments: ACM Conference on Fairness, Accountability, and Transparency 2022\n",
    "authors": [
      "Miri Zilka",
      "Holli Sargeant",
      "Adrian Weller"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.15258"
  },
  {
    "id": "arXiv:2205.15263",
    "title": "bsnsing: A decision tree induction method based on recursive optimal  boolean rule composition",
    "abstract": "This paper proposes a new mixed-integer programming (MIP) formulation to\noptimize split rule selection in the decision tree induction process, and\ndevelops an efficient search algorithm that is able to solve practical\ninstances of the MIP model faster than commercial solvers. The formulation is\nnovel for it directly maximizes the Gini reduction, an effective split\nselection criterion which has never been modeled in a mathematical program for\nits nonconvexity. The proposed approach differs from other optimal\nclassification tree models in that it does not attempt to optimize the whole\ntree, therefore the flexibility of the recursive partitioning scheme is\nretained and the optimization model is more amenable. The approach is\nimplemented in an open-source R package named bsnsing. Benchmarking experiments\non 75 open data sets suggest that bsnsing trees are the most capable of\ndiscriminating new cases compared to trees trained by other decision tree codes\nincluding the rpart, C50, party and tree packages in R. Compared to other\noptimal decision tree packages, including DL8.5, OSDT, GOSDT and indirectly\nmore, bsnsing stands out in its training speed, ease of use and broader\napplicability without losing in prediction accuracy.",
    "descriptor": "",
    "authors": [
      "Yanchao Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Mathematical Software (cs.MS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15263"
  },
  {
    "id": "arXiv:2205.15265",
    "title": "Going Beyond One-Hot Encoding in Classification: Can Human Uncertainty  Improve Model Performance?",
    "abstract": "Technological and computational advances continuously drive forward the broad\nfield of deep learning. In recent years, the derivation of quantities\ndescribing theuncertainty in the prediction - which naturally accompanies the\nmodeling process - has sparked general interest in the deep learning community.\nOften neglected in the machine learning setting is the human uncertainty that\ninfluences numerous labeling processes. As the core of this work, label\nuncertainty is explicitly embedded into the training process via distributional\nlabels. We demonstrate the effectiveness of our approach on image\nclassification with a remote sensing data set that contains multiple label\nvotes by domain experts for each image: The incorporation of label uncertainty\nhelps the model to generalize better to unseen data and increases model\nperformance. Similar to existing calibration methods, the distributional labels\nlead to better-calibrated probabilities, which in turn yield more certain and\ntrustworthy predictions.",
    "descriptor": "",
    "authors": [
      "Christoph Koller",
      "G\u00f6ran Kauermann",
      "Xiao Xiang Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15265"
  },
  {
    "id": "arXiv:2205.15266",
    "title": "(Spectral) Chebyshev collocation methods for solving differential  equations",
    "abstract": "Recently, the efficient numerical solution of Hamiltonian problems has been\ntackled by defining the class of energy-conserving Runge-Kutta methods named\nHamiltonian Boundary Value Methods (HBVMs). Their derivation relies on the\nexpansion of the vector field along the Legendre orthonormal basis.\nInterestingly, this approach can be extended to cope with other orthonormal\nbases and, in particular, we here consider the case of the Chebyshev polynomial\nbasis. The corresponding Runge-Kutta methods were previously obtained by\nCostabile and Napoli [33]. In this paper, the use of a different framework\nallows us to carry out a novel analysis of the methods also when they are used\nas spectral formulae in time, along with some generalizations of the methods.",
    "descriptor": "\nComments: 25 pages, 2 figures, 2 tables\n",
    "authors": [
      "Pierluigi Amodio",
      "Luigi Brugnano",
      "Felice Iavernaro"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.15266"
  },
  {
    "id": "arXiv:2205.15269",
    "title": "Kernel Neural Optimal Transport",
    "abstract": "We study the Neural Optimal Transport (NOT) algorithm which uses the general\noptimal transport formulation and learns stochastic transport plans. We show\nthat NOT with the weak quadratic cost might learn fake plans which are not\noptimal. To resolve this issue, we introduce kernel weak quadratic costs. We\nshow that they provide improved theoretical guarantees and practical\nperformance. We test NOT with kernel costs on the unpaired image-to-image\ntranslation task.",
    "descriptor": "",
    "authors": [
      "Alexander Korotin",
      "Daniil Selikhanovych",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15269"
  },
  {
    "id": "arXiv:2205.15270",
    "title": "SAT-Based Extraction of Behavioural Models for Java Libraries with  Collections",
    "abstract": "Behavioural models are a valuable tool for software verification, testing,\nmonitoring, publishing etc. However, they are rarely provided by the software\ndevelopers and have to be extracted either from the source or from the compiled\ncode. In the context of Java programs, a number of approaches exist for\nbuilding behavioural models. Most of these approaches rely on the analysis of\nthe compiled bytecode. Instead, we are looking to extract behavioural models in\nthe form of Finite State Machines (FSMs) from the Java source code to ensure\nthat the obtained FSMs can be easily understood by the software developers and,\nif necessary, updated or integrated into the original source code, e.g. in the\nform of annotations. Modern software systems are huge, rely on external\nlibraries and interact with their environment. Hence, extracting useful\nbehavioural models requires abstraction. In this paper, we present an initial\napproach to this problem by focusing on the extraction of FSMs modelling\nlibrary APIs. We focus on the analysis of Java code involving the use of\ncollections. To this end, we encode the operational semantics of collection\noperations using patterns of Boolean predicates. These patterns are\ninstantiated based on the analysis of the source code of API implementation\nmethods to form an encoding of the possible FSM transitions. A SAT solver is\nthen used to determine the enabledness conditions (guards) of these\ntransitions.",
    "descriptor": "",
    "authors": [
      "Larisa Safina",
      "Simon Bliudze"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.15270"
  },
  {
    "id": "arXiv:2205.15271",
    "title": "MetaSSD: Meta-Learned Self-Supervised Detection",
    "abstract": "Deep learning-based symbol detector gains increasing attention due to the\nsimple algorithm design than the traditional model-based algorithms such as\nViterbi and BCJR. The supervised learning framework is often employed to\npredict the input symbols, where training symbols are used to train the model.\nThere are two major limitations in the supervised approaches: a) a model needs\nto be retrained from scratch when new train symbols come to adapt to a new\nchannel status, and b) the length of the training symbols needs to be longer\nthan a certain threshold to make the model generalize well on unseen symbols.\nTo overcome these challenges, we propose a meta-learning-based self-supervised\nsymbol detector named MetaSSD. Our contribution is two-fold: a) meta-learning\nhelps the model adapt to a new channel environment based on experience with\nvarious meta-training environments, and b) self-supervised learning helps the\nmodel to use relatively less supervision than the previously suggested\nlearning-based detectors. In experiments, MetaSSD outperforms OFDM-MMSE with\nnoisy channel information and shows comparable results with BCJR. Further\nablation studies show the necessity of each component in our framework.",
    "descriptor": "\nComments: Accepted by ISIT 2022\n",
    "authors": [
      "Moon Jeong Park",
      "Jungseul Ok",
      "Yo-Seb Jeon",
      "Dongwoo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15271"
  },
  {
    "id": "arXiv:2205.15276",
    "title": "Multi-Object Grasping -- Types and Taxonomy",
    "abstract": "This paper proposes 12 multi-object grasps (MOGs) types from a human and\nrobot grasping data set. The grasp types are then analyzed and organized into a\nMOG taxonomy. This paper first presents three MOG data collection setups: a\nhuman finger tracking setup for multi-object grasping demonstrations, a real\nsystem with Barretthand, UR5e arm, and a MOG algorithm, a simulation system\nwith the same settings as the real system. Then the paper describes a novel\nstochastic grasping routine designed based on a biased random walk to explore\nthe robotic hand's configuration space for feasible MOGs. Based on observations\nin both the human demonstrations and robotic MOG solutions, this paper proposes\n12 MOG types in two groups: shape-based types and function-based types. The new\nMOG types are compared using six characteristics and then compiled into a\ntaxonomy. This paper then introduces the observed MOG type combinations and\nshows examples of 16 different combinations.",
    "descriptor": "",
    "authors": [
      "Yu Sun",
      "Eliza Amatova",
      "Tianze Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.15276"
  },
  {
    "id": "arXiv:2205.15278",
    "title": "EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware  Motion Model",
    "abstract": "Although significant progress has been made to audio-driven talking face\ngeneration, existing methods either neglect facial emotion or cannot be applied\nto arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model\n(EAMM) to generate one-shot emotional talking faces by involving an emotion\nsource video. Specifically, we first propose an Audio2Facial-Dynamics module,\nwhich renders talking faces from audio-driven unsupervised zero- and\nfirst-order key-points motion. Then through exploring the motion model's\nproperties, we further propose an Implicit Emotion Displacement Learner to\nrepresent emotion-related facial dynamics as linearly additive displacements to\nthe previously acquired motion representations. Comprehensive experiments\ndemonstrate that by incorporating the results from both modules, our method can\ngenerate satisfactory talking face results on arbitrary subjects with realistic\nemotion patterns.",
    "descriptor": "",
    "authors": [
      "Xinya Ji",
      "Hang Zhou",
      "Kaisiyuan Wang",
      "Qianyi Wu",
      "Wayne Wu",
      "Feng Xu",
      "Xun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15278"
  },
  {
    "id": "arXiv:2205.15279",
    "title": "The openESEA Modelling Language for Ethical, Social and Environmental  Accounting: Technical Report",
    "abstract": "Over the years ethical, social and environmental accounting (ESEA) has become\na common practice among responsible organisations. ESEA entails assessing and\nreporting organisations\" performance on environmental, social and governance\ntopics. In this report, we present a textual grammar for specifying ESEA\nmethods. With the grammar ESEA models can be created. Such models can be\ninterpreted by our open-source, model-driven tool, called openESEA. The report\npresents the metamodel of the grammar, the grammar itself, and explanations of\neach grammar primitive.",
    "descriptor": "",
    "authors": [
      "Vijanti Ramautar",
      "Sergio Espa\u00f1a"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/2205.15279"
  },
  {
    "id": "arXiv:2205.15281",
    "title": "Learning Open Domain Multi-hop Search Using Reinforcement Learning",
    "abstract": "We propose a method to teach an automated agent to learn how to search for\nmulti-hop paths of relations between entities in an open domain. The method\nlearns a policy for directing existing information retrieval and machine\nreading resources to focus on relevant regions of a corpus. The approach\nformulates the learning problem as a Markov decision process with a state\nrepresentation that encodes the dynamics of the search process and a reward\nstructure that minimizes the number of documents that must be processed while\nstill finding multi-hop paths. We implement the method in an actor-critic\nreinforcement learning algorithm and evaluate it on a dataset of search\nproblems derived from a subset of English Wikipedia. The algorithm finds a\nfamily of policies that succeeds in extracting the desired information while\nprocessing fewer documents compared to several baseline heuristic algorithms.",
    "descriptor": "\nComments: Accepted for publication at the Structured and Unstructured Knowledge Integration (SUKI) workshop, held at NAACL-HLT 2022\n",
    "authors": [
      "Enrique Noriega-Atala",
      "Mihai Surdeanu",
      "Clayton T. Morrison"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15281"
  },
  {
    "id": "arXiv:2205.15285",
    "title": "Fast Dynamic Radiance Fields with Time-Aware Neural Voxels",
    "abstract": "Neural radiance fields (NeRF) have shown great success in modeling 3D scenes\nand synthesizing novel-view images. However, most previous NeRF methods take\nmuch time to optimize one single scene. Explicit data structures, e.g. voxel\nfeatures, show great potential to accelerate the training process. However,\nvoxel features face two big challenges to be applied to dynamic scenes, i.e.\nmodeling temporal information and capturing different scales of point motions.\nWe propose a radiance field framework by representing scenes with time-aware\nvoxel features, named as TiNeuVox. A tiny coordinate deformation network is\nintroduced to model coarse motion trajectories and temporal information is\nfurther enhanced in the radiance network. A multi-distance interpolation method\nis proposed and applied on voxel features to model both small and large\nmotions. Our framework significantly accelerates the optimization of dynamic\nradiance fields while maintaining high rendering quality. Empirical evaluation\nis performed on both synthetic and real scenes. Our TiNeuVox completes training\nwith only 8 minutes and 8-MB storage cost while showing similar or even better\nrendering performance than previous dynamic NeRF methods.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Jiemin Fang",
      "Taoran Yi",
      "Xinggang Wang",
      "Lingxi Xie",
      "Xiaopeng Zhang",
      "Wenyu Liu",
      "Matthias Nie\u00dfner",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.15285"
  },
  {
    "id": "arXiv:2205.15286",
    "title": "Accelerating spiking neural network training",
    "abstract": "Spiking neural networks (SNN) are a type of artificial network inspired by\nthe use of action potentials in the brain. There is a growing interest in\nemulating these networks on neuromorphic computers due to their improved energy\nconsumption and speed, which are the main scaling issues of their counterpart\nthe artificial neural network (ANN). Significant progress has been made in\ndirectly training SNNs to perform on par with ANNs in terms of accuracy. These\nmethods are however slow due to their sequential nature, leading to long\ntraining times. We propose a new technique for directly training\nsingle-spike-per-neuron SNNs which eliminates all sequential computation and\nrelies exclusively on vectorised operations. We demonstrate over a $\\times 10$\nspeedup in training with robust classification performance on real datasets of\nlow to medium spatio-temporal complexity (Fashion-MNIST and Neuromophic-MNIST).\nOur proposed solution manages to solve certain tasks with over a $95.68 \\%$\nreduction in spike counts relative to a conventionally trained SNN, which could\nsignificantly reduce energy requirements when deployed on neuromorphic\ncomputers.",
    "descriptor": "\nComments: 18 pages, 5 figures, under review at NeurIPS 2022\n",
    "authors": [
      "Luke Taylor",
      "Andrew King",
      "Nicol Harper"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.15286"
  },
  {
    "id": "arXiv:2205.15288",
    "title": "Self-Supervised Visual Representation Learning with Semantic Grouping",
    "abstract": "In this paper, we tackle the problem of learning visual representations from\nunlabeled scene-centric data. Existing works have demonstrated the potential of\nutilizing the underlying complex structure within scene-centric data; still,\nthey commonly rely on hand-crafted objectness priors or specialized pretext\ntasks to build a learning framework, which may harm generalizability. Instead,\nwe propose contrastive learning from data-driven semantic slots, namely\nSlotCon, for joint semantic grouping and representation learning. The semantic\ngrouping is performed by assigning pixels to a set of learnable prototypes,\nwhich can adapt to each sample by attentive pooling over the feature and form\nnew slots. Based on the learned data-dependent slots, a contrastive objective\nis employed for representation learning, which enhances the discriminability of\nfeatures, and conversely facilitates grouping semantically coherent pixels\ntogether. Compared with previous efforts, by simultaneously optimizing the two\ncoupled objectives of semantic grouping and contrastive learning, our approach\nbypasses the disadvantages of hand-crafted priors and is able to learn\nobject/group-level representations from scene-centric images. Experiments show\nour approach effectively decomposes complex scenes into semantic groups for\nfeature learning and significantly benefits downstream tasks, including object\ndetection, instance segmentation, and semantic segmentation. The code will be\nmade publicly available.",
    "descriptor": "",
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Anlin Zheng",
      "Xiangyu Zhang",
      "Xiaojuan Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15288"
  },
  {
    "id": "arXiv:2205.15290",
    "title": "Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label  Classification using Vision Transformer",
    "abstract": "Lung cancer is the cancer leading cause of cancer-related death worldwide.\nLung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most\ncommon histologic subtypes of NSCLC. Histology is an essential tool for lung\ncancer diagnosis. Pathologists make classifications according to the dominant\nsubtypes. Although morphology remains the standard for diagnosis, significant\ntool needs to be developed to elucidate the diagnosis. In our study, we utilize\nthe pre-trained Vision Transformer (ViT) model to classify multiple label lung\ncancer on histologic slices (from dataset LC25000), in both Zero-Shot and\nFew-Shot manners. Then we compare the performance of Zero-Shot and Few-Shot ViT\non accuracy, precision, recall, sensitivity and specificity. Our study show\nthat the pre-trained ViT model has a good performance in Zero-Shot setting, a\ncompetitive accuracy ($99.87\\%$) in Few-Shot setting ({epoch = 1}) and an\noptimal result ($100.00\\%$) in Few-Shot seeting ({epoch = 5}).",
    "descriptor": "",
    "authors": [
      "Fu-Ming Guo",
      "Yingfang Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15290"
  },
  {
    "id": "arXiv:2205.15292",
    "title": "On the solvability of weakly linear systems of fuzzy relation equations",
    "abstract": "Systems of fuzzy relation equations and inequalities in which an unknown\nfuzzy relation is on the one side of the equation or inequality are linear\nsystems. They are the most studied ones, and a vast literature on linear\nsystems focuses on finding solutions and solvability criteria for such systems.\nThe situation is quite different with the so-called weakly linear systems, in\nwhich an unknown fuzzy relation is on both sides of the equation or inequality.\nPrecisely, the scholars have only given the characterization of the set of\nexact solutions to such systems. This paper describes the set of fuzzy\nrelations that solve weakly linear systems to a certain degree and provides\nways to compute them. We pay special attention to developing the algorithms for\ncomputing fuzzy preorders and fuzzy equivalences that are solutions to some\nextent to weakly linear systems. We establish additional properties for the set\nof such approximate solutions over some particular types of complete residuated\nlattices. We demonstrate the advantage of this approach via many examples that\narise from the problem of aggregation of fuzzy networks.",
    "descriptor": "",
    "authors": [
      "Stefan Stanimirovic",
      "Ivana Micic"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.15292"
  },
  {
    "id": "arXiv:2205.15294",
    "title": "Efficient $\u03a6$-Regret Minimization in Extensive-Form Games via Online  Mirror Descent",
    "abstract": "A conceptually appealing approach for learning Extensive-Form Games (EFGs) is\nto convert them to Normal-Form Games (NFGs). This approach enables us to\ndirectly translate state-of-the-art techniques and analyses in NFGs to learning\nEFGs, but typically suffers from computational intractability due to the\nexponential blow-up of the game size introduced by the conversion. In this\npaper, we address this problem in natural and important setups for the\n\\emph{$\\Phi$-Hedge} algorithm -- A generic algorithm capable of learning a\nlarge class of equilibria for NFGs. We show that $\\Phi$-Hedge can be directly\nused to learn Nash Equilibria (zero-sum settings), Normal-Form Coarse\nCorrelated Equilibria (NFCCE), and Extensive-Form Correlated Equilibria (EFCE)\nin EFGs. We prove that, in those settings, the \\emph{$\\Phi$-Hedge} algorithms\nare equivalent to standard Online Mirror Descent (OMD) algorithms for EFGs with\nsuitable dilated regularizers, and run in polynomial time. This new connection\nfurther allows us to design and analyze a new class of OMD algorithms based on\nmodifying its log-partition function. In particular, we design an improved\nalgorithm with balancing techniques that achieves a sharp\n$\\widetilde{\\mathcal{O}}(\\sqrt{XAT})$ EFCE-regret under bandit-feedback in an\nEFG with $X$ information sets, $A$ actions, and $T$ episodes. To our best\nknowledge, this is the first such rate and matches the information-theoretic\nlower bound.",
    "descriptor": "",
    "authors": [
      "Yu Bai",
      "Chi Jin",
      "Song Mei",
      "Ziang Song",
      "Tiancheng Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15294"
  },
  {
    "id": "arXiv:2205.15299",
    "title": "Adapting Rapid Motor Adaptation for Bipedal Robots",
    "abstract": "Recent advances in legged locomotion have enabled quadrupeds to walk on\nchallenging terrains. However, bipedal robots are inherently more unstable and\nhence it's harder to design walking controllers for them. In this work, we\nleverage recent advances in rapid adaptation for locomotion control, and extend\nthem to work on bipedal robots. Similar to existing works, we start with a base\npolicy which produces actions while taking as input an estimated extrinsics\nvector from an adaptation module. This extrinsics vector contains information\nabout the environment and enables the walking controller to rapidly adapt\nonline. However, the extrinsics estimator could be imperfect, which might lead\nto poor performance of the base policy which expects a perfect estimator. In\nthis paper, we propose A-RMA (Adapting RMA), which additionally adapts the base\npolicy for the imperfect extrinsics estimator by finetuning it using model-free\nRL. We demonstrate that A-RMA outperforms a number of RL-based baseline\ncontrollers and model-based controllers in simulation, and show zero-shot\ndeployment of a single A-RMA policy to enable a bipedal robot, Cassie, to walk\nin a variety of different scenarios in the real world beyond what it has seen\nduring training. Videos and results at https://ashish-kmr.github.io/a-rma/",
    "descriptor": "\nComments: First two authors contributed equally. Website at this https URL\n",
    "authors": [
      "Ashish Kumar",
      "Zhongyu Li",
      "Jun Zeng",
      "Deepak Pathak",
      "Koushil Sreenath",
      "Jitendra Malik"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.15299"
  },
  {
    "id": "arXiv:2205.15300",
    "title": "A Combination of Deep Neural Networks and K-Nearest Neighbors for Credit  Card Fraud Detection",
    "abstract": "Detection of a Fraud transaction on credit cards became one of the major\nproblems for financial institutions, organizations and companies. As the global\nfinancial system is highly connected to non-cash transactions and online\noperations fraud makers invent more effective ways to access customers'\nfinances. The main problem in credit card fraud detection is that the number of\nfraud transactions is significantly lower than genuine ones. The aim of the\npaper is to implement new techniques, which contains of under-sampling\nalgorithms, K-nearest Neighbor Algorithm (KNN) and Deep Neural Network (KNN) on\nnew obtained dataset. The performance evaluation showed that DNN model gives\nprecise high accuracy (98.12%), which shows the good ability of presented\nmethod to detect fraudulent transactions.",
    "descriptor": "",
    "authors": [
      "Dinara Rzayeva",
      "Saber Malekzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15300"
  },
  {
    "id": "arXiv:2205.15301",
    "title": "Can Transformer be Too Compositional? Analysing Idiom Processing in  Neural Machine Translation",
    "abstract": "Unlike literal expressions, idioms' meanings do not directly follow from\ntheir parts, posing a challenge for neural machine translation (NMT). NMT\nmodels are often unable to translate idioms accurately and over-generate\ncompositional, literal translations. In this work, we investigate whether the\nnon-compositionality of idioms is reflected in the mechanics of the dominant\nNMT model, Transformer, by analysing the hidden states and attention patterns\nfor models with English as source language and one of seven European languages\nas target language. When Transformer emits a non-literal translation - i.e.\nidentifies the expression as idiomatic - the encoder processes idioms more\nstrongly as single lexical units compared to literal expressions. This\nmanifests in idioms' parts being grouped through attention and in reduced\ninteraction between idioms and their context. In the decoder's cross-attention,\nfigurative inputs result in reduced attention on source-side tokens. These\nresults suggest that Transformer's tendency to process idioms as compositional\nexpressions contributes to literal translations of idioms.",
    "descriptor": "\nComments: Published at ACL 2022\n",
    "authors": [
      "Verna Dankers",
      "Christopher G. Lucas",
      "Ivan Titov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.15301"
  },
  {
    "id": "arXiv:2110.15209",
    "title": "Re-calibrating Photometric Redshift Probability Distributions Using  Feature-space Regression",
    "abstract": "Many astrophysical analyses depend on estimates of redshifts (a proxy for\ndistance) determined from photometric (i.e., imaging) data alone. Inaccurate\nestimates of photometric redshift uncertainties can result in large systematic\nerrors. However, probability distribution outputs from many photometric\nredshift methods do not follow the frequentist definition of a Probability\nDensity Function (PDF) for redshift -- i.e., the fraction of times the true\nredshift falls between two limits $z_{1}$ and $z_{2}$ should be equal to the\nintegral of the PDF between these limits. Previous works have used the global\ndistribution of Probability Integral Transform (PIT) values to re-calibrate\nPDFs, but offsetting inaccuracies in different regions of feature space can\nconspire to limit the efficacy of the method. We leverage a recently developed\nregression technique that characterizes the local PIT distribution at any\nlocation in feature space to perform a local re-calibration of photometric\nredshift PDFs. Though we focus on an example from astrophysics, our method can\nproduce PDFs which are calibrated at all locations in feature space for any use\ncase.",
    "descriptor": "\nComments: Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021)\n",
    "authors": [
      "Biprateep Dey",
      "Jeffrey A. Newman",
      "Brett H. Andrews",
      "Rafael Izbicki",
      "Ann B. Lee",
      "David Zhao",
      "Markus Michael Rau",
      "Alex I. Malz"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.15209"
  },
  {
    "id": "arXiv:2205.10169",
    "title": "Beam Training and Tracking in MmWave Communication: A Survey",
    "abstract": "Communicating on millimeter wave (mmWave) bands is ushering in a new epoch of\nmobile communication which provides the availability of 10 Gbps high data rate\ntransmission. However, mmWave links are easily prone to short transmission\nrange communication because of the serious free space path loss and the\nblockage by obstacles. To overcome these challenges, highly directional beams\nare exploited to achieve robust links by hybrid beamforming. Accurately\naligning the transmitter and receiver beams, i.e. beam training, is vitally\nimportant to high data rate transmission. However, it may cause huge overhead\nwhich has negative effects on initial access, handover, and tracking. Besides,\nthe mobility patterns of users are complicated and dynamic, which may cause\ntracking error and large tracking latency. An efficient beam tracking method\nhas a positive effect on sustaining robust links. This article provides an\noverview of the beam training and tracking technologies on mmWave bands and\nreveals the insights for future research in the 6th Generation (6G) mobile\nnetwork. Especially, some open research problems are proposed to realize fast,\naccurate, and robust beam training and tracking. We hope that this survey\nprovides guidelines for the researchers in the area of mmWave communications.",
    "descriptor": "",
    "authors": [
      "Yi Wang",
      "Zhiqing Wei",
      "Zhiyong Feng"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.10169"
  },
  {
    "id": "arXiv:2205.14147",
    "title": "FlowNet-PET: Unsupervised Learning to Perform Respiratory Motion  Correction in PET Imaging",
    "abstract": "To correct for breathing motion in PET imaging, an interpretable and\nunsupervised deep learning technique, FlowNet-PET, was constructed. The network\nwas trained to predict the optical flow between two PET frames from different\nbreathing amplitude ranges. As a result, the trained model groups different\nretrospectively-gated PET images together into a motion-corrected single bin,\nproviding a final image with similar counting statistics as a non-gated image,\nbut without the blurring effects that were initially observed. As a\nproof-of-concept, FlowNet-PET was applied to anthropomorphic digital phantom\ndata, which provided the possibility to design robust metrics to quantify the\ncorrections. When comparing the predicted optical flows to the ground truths,\nthe median absolute error was found to be smaller than the pixel and slice\nwidths, even for the phantom with a diaphragm movement of 21 mm. The\nimprovements were illustrated by comparing against images without motion and\ncomputing the intersection over union (IoU) of the tumors as well as the\nenclosed activity and coefficient of variation (CoV) within the no-motion tumor\nvolume before and after the corrections were applied. The average relative\nimprovements provided by the network were 54%, 90%, and 76% for the IoU, total\nactivity, and CoV, respectively. The results were then compared against the\nconventional retrospective phase binning approach. FlowNet-PET achieved similar\nresults as retrospective binning, but only required one sixth of the scan\nduration. The code and data used for training and analysis has been made\npublicly available (https://github.com/teaghan/FlowNet_PET).",
    "descriptor": "",
    "authors": [
      "Teaghan O'Briain",
      "Carlos Uribe",
      "Kwang Moo Yi",
      "Jonas Teuwen",
      "Ioannis Sechopoulos",
      "Magdalena Bazalova-Carter"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14147"
  },
  {
    "id": "arXiv:2205.14174",
    "title": "Private and Byzantine-Proof Cooperative Decision-Making",
    "abstract": "The cooperative bandit problem is a multi-agent decision problem involving a\ngroup of agents that interact simultaneously with a multi-armed bandit, while\ncommunicating over a network with delays. The central idea in this problem is\nto design algorithms that can efficiently leverage communication to obtain\nimprovements over acting in isolation. In this paper, we investigate the\nstochastic bandit problem under two settings - (a) when the agents wish to make\ntheir communication private with respect to the action sequence, and (b) when\nthe agents can be byzantine, i.e., they provide (stochastically) incorrect\ninformation. For both these problem settings, we provide upper-confidence bound\nalgorithms that obtain optimal regret while being (a) differentially-private\nand (b) tolerant to byzantine agents. Our decentralized algorithms require no\ninformation about the network of connectivity between agents, making them\nscalable to large dynamic systems. We test our algorithms on a competitive\nbenchmark of random graphs and demonstrate their superior performance with\nrespect to existing robust algorithms. We hope that our work serves as an\nimportant step towards creating distributed decision-making systems that\nmaintain privacy.",
    "descriptor": "\nComments: Full version of AAMAS 2020 paper uploaded to arXiv\n",
    "authors": [
      "Abhimanyu Dubey",
      "Alex Pentland"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.14174"
  },
  {
    "id": "arXiv:2205.14189",
    "title": "Optimizing Objective Functions from Trained ReLU Neural Networks via  Sampling",
    "abstract": "This paper introduces scalable, sampling-based algorithms that optimize\ntrained neural networks with ReLU activations. We first propose an iterative\nalgorithm that takes advantage of the piecewise linear structure of ReLU neural\nnetworks and reduces the initial mixed-integer optimization problem (MIP) into\nmultiple easy-to-solve linear optimization problems (LPs) through sampling.\nSubsequently, we extend this approach by searching around the neighborhood of\nthe LP solution computed at each iteration. This scheme allows us to devise a\nsecond, enhanced algorithm that reduces the initial MIP problem into smaller,\neasier-to-solve MIPs. We analytically show the convergence of the methods and\nwe provide a sample complexity guarantee. We also validate the performance of\nour algorithms by comparing them against state-of-the-art MIP-based methods.\nFinally, we show computationally how the sampling algorithms can be used\neffectively to warm-start MIP-based methods.",
    "descriptor": "",
    "authors": [
      "Georgia Perakis",
      "Asterios Tsiourvas"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14189"
  },
  {
    "id": "arXiv:2205.14202",
    "title": "Robust Phi-Divergence MDPs",
    "abstract": "In recent years, robust Markov decision processes (MDPs) have emerged as a\nprominent modeling framework for dynamic decision problems affected by\nuncertainty. In contrast to classical MDPs, which only account for\nstochasticity by modeling the dynamics through a stochastic process with a\nknown transition kernel, robust MDPs additionally account for ambiguity by\noptimizing in view of the most adverse transition kernel from a prescribed\nambiguity set. In this paper, we develop a novel solution framework for robust\nMDPs with s-rectangular ambiguity sets that decomposes the problem into a\nsequence of robust Bellman updates and simplex projections. Exploiting the rich\nstructure present in the simplex projections corresponding to phi-divergence\nambiguity sets, we show that the associated s-rectangular robust MDPs can be\nsolved substantially faster than with state-of-the-art commercial solvers as\nwell as a recent first-order solution scheme, thus rendering them attractive\nalternatives to classical MDPs in practical applications.",
    "descriptor": "",
    "authors": [
      "Chin Pang Ho",
      "Marek Petrik",
      "Wolfram Wiesemann"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14202"
  },
  {
    "id": "arXiv:2205.14214",
    "title": "Bias in the representative volume element method: periodize the ensemble  instead of its realizations",
    "abstract": "We study the Representative Volume Element (RVE) method, which is a method to\napproximately infer the effective behavior $a_{\\text{hom}}$ of a stationary\nrandom medium. The latter is described by a coefficient field $a(x)$ generated\nfrom a given ensemble $\\langle\\cdot\\rangle$ and the corresponding linear\nelliptic operator $-\\nabla\\cdot a\\nabla$. In line with the theory of\nhomogenization, the method proceeds by computing $d = 3$ correctors (d denoting\nthe space dimension).To be numerically tractable, this computation has to be\ndone on a finite domain: the so-called \"representative\" volume element, i. e. a\nlarge box with, say, periodic boundary conditions. The main message of this\narticle is: Periodize the ensemble instead of its realizations. By this we mean\nthat it is better to sample from a suitably periodized ensemble than to\nperiodically extend the restriction of a realization $a(x)$ from the\nwhole-space ensemble $\\langle\\cdot\\rangle$. We make this point by investigating\nthe bias (or systematic error), i. e. the difference between $a_{\\text{hom}}$\nand the expected value of the RVE method, in terms of its scaling w. r. t. the\nlateral size $L$ of the box. In case of periodizing $a(x)$, we heuristically\nargue that this error is generically $O(L^{-1})$. In case of a suitable\nperiodization of $\\langle\\cdot\\rangle$, we rigorously show that it is\n$O(L^{-d})$. In fact, we give a characterization of the leading-order error\nterm for both strategies, and argue that even in the isotropic case it is\ngenerically non-degenerate. We carry out the rigorous analysis in the\nconvenient setting of ensembles $\\langle\\cdot\\rangle$ of Gaussian type with\nintegrable covariance, which allow for a straightforward periodization and\nwhich make the Price theorem and the Malliavin calculus available for optimal\nstochastic estimates of correctors.",
    "descriptor": "",
    "authors": [
      "Nicolas Clozeau",
      "Marc Josien",
      "Felix Otto",
      "Qiang Xu"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14214"
  },
  {
    "id": "arXiv:2205.14232",
    "title": "Competitive Gradient Optimization",
    "abstract": "We study the problem of convergence to a stationary point in zero-sum games.\nWe propose competitive gradient optimization (CGO ), a gradient-based method\nthat incorporates the interactions between the two players in zero-sum games\nfor optimization updates. We provide continuous-time analysis of CGO and its\nconvergence properties while showing that in the continuous limit, CGO\npredecessors degenerate to their gradient descent ascent (GDA) variants. We\nprovide a rate of convergence to stationary points and further propose a\ngeneralized class of $\\alpha$-coherent function for which we provide\nconvergence analysis. We show that for strictly $\\alpha$-coherent functions,\nour algorithm convergences to a saddle point. Moreover, we propose optimistic\nCGO (OCGO), an optimistic variant, for which we show convergence rate to saddle\npoints in $\\alpha$-coherent class of functions.",
    "descriptor": "",
    "authors": [
      "Abhijeet Vyas",
      "Kamyar Azizzadenesheli"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14232"
  },
  {
    "id": "arXiv:2205.14234",
    "title": "Two-Leg Deep Space Relay Architectures: Performance, Challenges, and  Perspectives",
    "abstract": "In this paper, architectures for interplanetary communications that feature\nthe use of a data relay are investigated. In the considered \"two-leg\"\narchitecture, a spacecraft orbiting the Earth, or in orbit at a Lagrange point,\nreceives data from a deep space probe (leg-1) and relays them towards ground\n(leg-2). Different wireless technologies for the interplanetary link, namely,\nradio frequencies above the Ka band and optical frequencies, are considered.\nMoreover, the cases of transparent and regenerative relaying as well as\ndifferent different orbital configurations are addressed, offering a thorough\nanalysis of such systems from different viewpoints. Results show that, under\ncertain constraints in terms of pointing accuracy and onboard antenna size, the\nadoption of a two-leg architecture can achieve the data rates supported by\ndirect space-to-Earth link configurations with remarkably smaller ground\nstation antennas.",
    "descriptor": "\nComments: 17 pages, 7 figures, 13 tables. To appear in IEEE Transactions on Aerospace and Electronic Systems, 2022\n",
    "authors": [
      "Dario Modenini",
      "Alfredo Locarini",
      "Lorenzo Valentini",
      "Alberto Faedi",
      "Paolo Tortora",
      "Davide Rovelli",
      "Nicol\u00f2 Mazzali",
      "Marco Chiani",
      "Enrico Paolini"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14234"
  },
  {
    "id": "arXiv:2205.14240",
    "title": "Deterministic Langevin Monte Carlo with Normalizing Flows for Bayesian  Inference",
    "abstract": "We propose a general purpose Bayesian inference algorithm for expensive\nlikelihoods, replacing the stochastic term in the Langevin equation with a\ndeterministic density gradient term. The particle density is evaluated from the\ncurrent particle positions using a Normalizing Flow (NF), which is\ndifferentiable and has good generalization properties in high dimensions. We\ntake advantage of NF preconditioning and NF based Metropolis-Hastings updates\nfor a faster and unbiased convergence. We show on various examples that the\nmethod is competitive against state of the art sampling methods.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Uros Seljak",
      "Richard D.P. Grumitt",
      "Biwei Dai"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.14240"
  },
  {
    "id": "arXiv:2205.14249",
    "title": "Experience report of physics-informed neural networks in fluid  simulations: pitfalls and frustration",
    "abstract": "The deep learning boom motivates researchers and practitioners of\ncomputational fluid dynamics eager to integrate the two areas.The PINN\n(physics-informed neural network) method is one such attempt. While most\nreports in the literature show positive outcomes of applying the PINN method,\nour experiments with it stifled such optimism. This work presents our\nnot-so-successful story of using PINN to solve two fundamental flow problems:\n2D Taylor-Green vortex at $Re = 100$ and 2D cylinder flow at $Re = 200$. The\nPINN method solved the 2D Taylor-Green vortex problem with acceptable results,\nand we used this flow as an accuracy and performance benchmark. About 32 hours\nof training were required for the PINN method's accuracy to match the accuracy\nof a $16 \\times 16$ finite-difference simulation, which took less than 20\nseconds. The 2D cylinder flow, on the other hand, did not even result in a\nphysical solution. The PINN method behaved like a steady-flow solver and did\nnot capture the vortex shedding phenomenon. By sharing our experience, we would\nlike to emphasize that the PINN method is still a work-in-progress. More work\nis needed to make PINN feasible for real-world problems.",
    "descriptor": "\nComments: 8 pages, 9 figures\n",
    "authors": [
      "Pi-Yueh Chuang",
      "Lorena A. Barba"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14249"
  },
  {
    "id": "arXiv:2205.14264",
    "title": "The Analysis of Optimization Algorithms, A Dissipativity Approach",
    "abstract": "Optimization problems in engineering and applied mathematics are typically\nsolved in an iterative fashion, by systematically adjusting the variables of\ninterest until an adequate solution is found. The iterative algorithms that\ngovern these systematic adjustments can be viewed as a control system. In\ncontrol systems, the output in measured and the input is adjusted using\nfeedback to drive the error to zero. Similarly, in iterative algorithms, the\noptimization objective is evaluated and the candidate solution is adjusted to\ndrive it toward the optimal point. Choosing an algorithm that works well for a\nvariety of optimization problems is akin to robust controller design. Just as\ndissipativity theory can be used to analyze the stability properties of control\nsystems, it can also be used to analyze the convergence properties of iterative\nalgorithms. By defining an appropriate notion of \"energy\" that dissipates with\nevery iteration of the algorithm, the convergence properties of the algorithm\ncan be characterized. This article formalizes the connection between iterative\nalgorithms and control systems and shows through examples how dissipativity\ntheory can be used to analyze the performance of many classes of optimization\nalgorithms. This control-theoretic viewpoint enables the selection and tuning\nof optimization algorithms to be performed in an automated and systematic way.",
    "descriptor": "",
    "authors": [
      "Laurent Lessard"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14264"
  },
  {
    "id": "arXiv:2205.14278",
    "title": "Uniform Convergence and Generalization for Nonconvex Stochastic Minimax  Problems",
    "abstract": "This paper studies the uniform convergence and generalization bounds for\nnonconvex-(strongly)-concave (NC-SC/NC-C) stochastic minimax optimization. We\nfirst establish the uniform convergence between the empirical minimax problem\nand the population minimax problem and show the\n$\\tilde{\\mathcal{O}}(d\\kappa^2\\epsilon^{-2})$ and\n$\\tilde{\\mathcal{O}}(d\\epsilon^{-4})$ sample complexities respectively for the\nNC-SC and NC-C settings, where $d$ is the dimension number and $\\kappa$ is the\ncondition number. To the best of our knowledge, this is the first uniform\nconvergence measured by the first-order stationarity in stochastic minimax\noptimization. Based on the uniform convergence, we shed light on the sample and\ngradient complexities required for finding an approximate stationary point for\nstochastic minimax optimization in the NC-SC and NC-C settings.",
    "descriptor": "",
    "authors": [
      "Siqi Zhang",
      "Yifan Hu",
      "Liang Zhang",
      "Niao He"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14278"
  },
  {
    "id": "arXiv:2205.14283",
    "title": "Rethinking Bayesian Learning for Data Analysis: The Art of Prior and  Inference in Sparsity-Aware Modeling",
    "abstract": "Sparse modeling for signal processing and machine learning has been at the\nfocus of scientific research for over two decades. Among others, supervised\nsparsity-aware learning comprises two major paths paved by: a) discriminative\nmethods and b) generative methods. The latter, more widely known as Bayesian\nmethods, enable uncertainty evaluation w.r.t. the performed predictions.\nFurthermore, they can better exploit related prior information and naturally\nintroduce robustness into the model, due to their unique capacity to\nmarginalize out uncertainties related to the parameter estimates. Moreover,\nhyper-parameters associated with the adopted priors can be learnt via the\ntraining data. To implement sparsity-aware learning, the crucial point lies in\nthe choice of the function regularizer for discriminative methods and the\nchoice of the prior distribution for Bayesian learning. Over the last decade or\nso, due to the intense research on deep learning, emphasis has been put on\ndiscriminative techniques. However, a come back of Bayesian methods is taking\nplace that sheds new light on the design of deep neural networks, which also\nestablish firm links with Bayesian models and inspire new paths for\nunsupervised learning, such as Bayesian tensor decomposition.\nThe goal of this article is two-fold. First, to review, in a unified way,\nsome recent advances in incorporating sparsity-promoting priors into three\nhighly popular data modeling tools, namely deep neural networks, Gaussian\nprocesses, and tensor decomposition. Second, to review their associated\ninference techniques from different aspects, including: evidence maximization\nvia optimization and variational inference methods. Challenges such as small\ndata dilemma, automatic model structure search, and natural prediction\nuncertainty evaluation are also discussed. Typical signal processing and\nmachine learning tasks are demonstrated.",
    "descriptor": "\nComments: 64 pages, 16 figures, 6 tables, 98 references, submitted to IEEE Signal Processing Magazine\n",
    "authors": [
      "Lei Cheng",
      "Feng Yin",
      "Sergios Theodoridis",
      "Sotirios Chatzis",
      "Tsung-Hui Chang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14283"
  },
  {
    "id": "arXiv:2205.14284",
    "title": "Provably Auditing Ordinary Least Squares in Low Dimensions",
    "abstract": "Measuring the stability of conclusions derived from Ordinary Least Squares\nlinear regression is critically important, but most metrics either only measure\nlocal stability (i.e. against infinitesimal changes in the data), or are only\ninterpretable under statistical assumptions. Recent work proposes a simple,\nglobal, finite-sample stability metric: the minimum number of samples that need\nto be removed so that rerunning the analysis overturns the conclusion,\nspecifically meaning that the sign of a particular coefficient of the estimated\nregressor changes. However, besides the trivial exponential-time algorithm, the\nonly approach for computing this metric is a greedy heuristic that lacks\nprovable guarantees under reasonable, verifiable assumptions; the heuristic\nprovides a loose upper bound on the stability and also cannot certify lower\nbounds on it.\nWe show that in the low-dimensional regime where the number of covariates is\na constant but the number of samples is large, there are efficient algorithms\nfor provably estimating (a fractional version of) this metric. Applying our\nalgorithms to the Boston Housing dataset, we exhibit regression analyses where\nwe can estimate the stability up to a factor of $3$ better than the greedy\nheuristic, and analyses where we can certify stability to dropping even a\nmajority of the samples.",
    "descriptor": "\nComments: 32 pages, 4 figures\n",
    "authors": [
      "Ankur Moitra",
      "Dhruv Rohatgi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2205.14284"
  },
  {
    "id": "arXiv:2205.14301",
    "title": "Uncertainty quantification of two-phase flow in porous media via  coupled-TgNN surrogate model",
    "abstract": "Uncertainty quantification (UQ) of subsurface two-phase flow usually requires\nnumerous executions of forward simulations under varying conditions. In this\nwork, a novel coupled theory-guided neural network (TgNN) based surrogate model\nis built to facilitate computation efficiency under the premise of satisfactory\naccuracy. The core notion of this proposed method is to bridge two separate\nblocks on top of an overall network. They underlie the TgNN model in a coupled\nform, which reflects the coupling nature of pressure and water saturation in\nthe two-phase flow equation. The TgNN model not only relies on labeled data,\nbut also incorporates underlying scientific theory and experiential rules\n(e.g., governing equations, stochastic parameter fields, boundary and initial\nconditions, well conditions, and expert knowledge) as additional components\ninto the loss function. The performance of the TgNN-based surrogate model for\ntwo-phase flow problems is tested by different numbers of labeled data and\ncollocation points, as well as the existence of data noise. The proposed\nTgNN-based surrogate model offers an effective way to solve the coupled\nnonlinear two-phase flow problem and demonstrates good accuracy and strong\nrobustness when compared with the purely data-driven surrogate model. By\ncombining the accurate TgNN-based surrogate model with the Monte Carlo method,\nUQ tasks can be performed at a minimum cost to evaluate statistical quantities.\nSince the heterogeneity of the random fields strongly impacts the results of\nthe surrogate model, corresponding variance and correlation length are added to\nthe input of the neural network to maintain its predictive capacity. The\nresults show that the TgNN-based surrogate model achieves satisfactory\naccuracy, stability, and efficiency in UQ problems of subsurface two-phase\nflow.",
    "descriptor": "",
    "authors": [
      "Jian Li",
      "Dongxiao Zhang",
      "Tianhao He",
      "Qiang Zheng"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14301"
  },
  {
    "id": "arXiv:2205.14317",
    "title": "A Confidence Machine for Sparse High-Order Interaction Model",
    "abstract": "In predictive modeling for high-stake decision-making, predictors must be not\nonly accurate but also reliable. Conformal prediction (CP) is a promising\napproach for obtaining the confidence of prediction results with fewer\ntheoretical assumptions. To obtain the confidence set by so-called full-CP, we\nneed to refit the predictor for all possible values of prediction results,\nwhich is only possible for simple predictors. For complex predictors such as\nrandom forests (RFs) or neural networks (NNs), split-CP is often employed where\nthe data is split into two parts: one part for fitting and another to compute\nthe confidence set. Unfortunately, because of the reduced sample size, split-CP\nis inferior to full-CP both in fitting as well as confidence set computation.\nIn this paper, we develop a full-CP of sparse high-order interaction model\n(SHIM), which is sufficiently flexible as it can take into account high-order\ninteractions among variables. We resolve the computational challenge for\nfull-CP of SHIM by introducing a novel approach called homotopy mining. Through\nnumerical experiments, we demonstrate that SHIM is as accurate as complex\npredictors such as RF and NN and enjoys the superior statistical power of\nfull-CP.",
    "descriptor": "",
    "authors": [
      "Diptesh Das",
      "Eugene Ndiaye",
      "Ichiro Takeuchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14317"
  },
  {
    "id": "arXiv:2205.14335",
    "title": "On the Sample Complexity of Stabilizing Linear Systems via Policy  Gradient Methods",
    "abstract": "Stabilizing unknown dynamical systems with the direct use of data samples has\ndrawn increasing attention in both control and machine learning communities. In\nthis paper, we study the sample complexity of stabilizing linear time-invariant\nsystems via Policy Gradient (PG) methods. Our analysis is built upon a\ndiscounted Linear Quadratic Regulator (LQR) framework which alternatively\nupdates the policy and the discount factor of the LQR. In sharp contrast to the\nexisting literature, we propose an explicit rule to adaptively adjust the\ndiscount factor by characterizing the stability margin using Lyapunov theory,\nwhich has independent interests of its own. We show that the number of\niterations per discount factor is uniformly upper bounded, which enables us to\nprove the sample complexity of stabilizing linear systems via PG methods.\nParticularly, it only adds a coefficient logarithmic in the spectral radius of\nthe state matrix to the sample complexity of solving LQR problems. We perform\nnumerical experiments to verify our theoretical findings and empirically\nevaluate the effectiveness of our results on nonlinear systems.",
    "descriptor": "",
    "authors": [
      "Feiran Zhao",
      "Xingyun Fu",
      "Keyou You"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14335"
  },
  {
    "id": "arXiv:2205.14366",
    "title": "Topology Optimization of Graded Truss Lattices Based on On-the-Fly  Homogenization",
    "abstract": "We introduce a computational framework for the topology optimization of\ncellular structures with spatially varying architecture, which is applied to\nfunctionally graded truss lattices under quasistatic loading. We make use of a\nfirst-order homogenization approach, which replaces the discrete truss by an\neffective continuum description to be treated by finite elements in a\nmacroscale boundary value problem. By defining the local truss architecture\nthrough a set of Bravais vectors, we formulate the optimization problem with\nregards to the spatially varying basis vectors and demonstrate its feasibility\nand performance through a series of benchmark problems in 2D (though the method\nis sufficiently general to also apply in 3D, as discussed). Both the\ndisplacement field and the topology are continuously varying unknown fields on\nthe macroscale, and a regularization is included for well-posedness. We argue\nthat prior solutions obtained from aligning trusses along the directions of\nprincipal stresses are included as a special case. The outlined approach\nresults in heterogeneous truss architectures with a smoothly varying unit cell,\nenabling easy fabrication with a tunable length scale (the latter avoiding the\nill-posedness stemming from classical nonconvex methods without an intrinsic\nlength scale).",
    "descriptor": "",
    "authors": [
      "Bastian Telgen",
      "Ole Sigmund",
      "Dennis M. Kochmann"
    ],
    "subjectives": [
      "Other Condensed Matter (cond-mat.other)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.14366"
  },
  {
    "id": "arXiv:2205.14372",
    "title": "On stationary inflection points in step responses",
    "abstract": "The step and impulse responses of a proper, rational transfer function are\nwell-behaved analytic functions. We prove that such a response cannot have an\ninflection point such that the tangent at that point is parallel to the\ntime-axis. Hence when a step or impulse response of a finite-dimensional LTI\nsystem crosses any given level, that crossing must be transversal, and can\nnever be tangential.",
    "descriptor": "",
    "authors": [
      "Maben Rabi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14372"
  },
  {
    "id": "arXiv:2205.14450",
    "title": "Zero-Hopf Bifurcation of Limit Cycles in Certain Differential Systems",
    "abstract": "This paper studies the number of limit cycles that may bifurcate from an\nequilibrium of an autonomous system of differential equations. The system in\nquestion is assumed to be of dimension $n$, have a zero-Hopf equilibrium at the\norigin, and consist only of homogeneous terms of order $m$. Using the\nfirst-order averaging method, we prove that at most $(m-1)\\cdot m^{n-2}$ limit\ncycles can bifurcate from the origin of the system for generic $n\\geq3$ and\n$m\\geq2$. Using the averaging method of order $k$, we show that the system has\nno more than $(km)^{n-1}$ limit cycles that can bifurcate from the origin. The\nexact numbers of limit cycles or tight bounds on the numbers are determined by\ncomputing the mixed volumes of some polynomial systems obtained from the\naveraged functions. Based on symbolic and algebraic computation, a general and\nalgorithmic approach is proposed to derive sufficient conditions for a given\ndifferential system to have a prescribed number of limit cycles. The\neffectiveness of the proposed approach is illustrated by a family of\nthird-order differential equations and by a four-dimensional hyperchaotic\ndifferential system.",
    "descriptor": "",
    "authors": [
      "Bo Huang",
      "Dongming Wang"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2205.14450"
  },
  {
    "id": "arXiv:2205.14461",
    "title": "Collaborative likelihood-ratio estimation over graphs",
    "abstract": "Assuming we have i.i.d observations from two unknown probability density\nfunctions (pdfs), $p$ and $p'$, the likelihood-ratio estimation (LRE) is an\nelegant approach to compare the two pdfs just by relying on the available data,\nand without knowing the pdfs explicitly. In this paper we introduce a\ngraph-based extension of this problem: Suppose each node $v$ of a fixed graph\nhas access to observations coming from two unknown node-specific pdfs, $p_v$\nand $p'_v$; the goal is then to compare the respective $p_v$ and $p'_v$ of each\nnode by also integrating information provided by the graph structure. This\nsetting is interesting when the graph conveys some sort of `similarity' between\nthe node-wise estimation tasks, which suggests that the nodes can collaborate\nto solve more efficiently their individual tasks, while on the other hand\ntrying to limit the data sharing among them. Our main contribution is a\ndistributed non-parametric framework for graph-based LRE, called GRULSIF, that\nincorporates in a novel way elements from f-divengence functionals, Kernel\nmethods, and Multitask Learning. Among the several applications of LRE, we\nchoose the two-sample hypothesis testing to develop a proof of concept for our\ngraph-based learning framework. Our experiments compare favorably the\nperformance of our approach against state-of-the-art non-parametric statistical\ntests that apply at each node independently, and thus disregard the graph\nstructure.",
    "descriptor": "",
    "authors": [
      "Alejandro de la Concha",
      "Argyris Kalogeratos",
      "Nicolas Vayatis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14461"
  },
  {
    "id": "arXiv:2205.14485",
    "title": "Noise-Aware Statistical Inference with Differentially Private Synthetic  Data",
    "abstract": "While generation of synthetic data under differential privacy (DP) has\nreceived a lot of attention in the data privacy community, analysis of\nsynthetic data has received much less. Existing work has shown that simply\nanalysing DP synthetic data as if it were real does not produce valid\ninferences of population-level quantities. For example, confidence intervals\nbecome too narrow, which we demonstrate with a simple experiment. We tackle\nthis problem by combining synthetic data analysis techniques from the field of\nmultiple imputation, and synthetic data generation using noise-aware Bayesian\nmodeling into a pipeline NA+MI that allows computing accurate uncertainty\nestimates for population-level quantities from DP synthetic data. To implement\nNA+MI for discrete data generation from marginal queries, we develop a novel\nnoise-aware synthetic data generation algorithm NAPSU-MQ using the principle of\nmaximum entropy. Our experiments demonstrate that the pipeline is able to\nproduce accurate confidence intervals from DP synthetic data. The intervals\nbecome wider with tighter privacy to accurately capture the additional\nuncertainty stemming from DP noise.",
    "descriptor": "\nComments: 20 pages, 8 figures\n",
    "authors": [
      "Ossi R\u00e4is\u00e4",
      "Joonas J\u00e4lk\u00f6",
      "Samuel Kaski",
      "Antti Honkela"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14485"
  },
  {
    "id": "arXiv:2205.14506",
    "title": "Introducing Non-Linearity into Quantum Generative Models",
    "abstract": "The evolution of an isolated quantum system is linear, and hence quantum\nalgorithms are reversible, including those that utilize quantum circuits as\ngenerative machine learning models. However, some of the most successful\nclassical generative models, such as those based on neural networks, involve\nhighly non-linear and thus non-reversible dynamics. In this paper, we explore\nthe effect of these dynamics in quantum generative modeling by introducing a\nmodel that adds non-linear activations via a neural network structure onto the\nstandard Born Machine framework - the Quantum Neuron Born Machine (QNBM). To\nachieve this, we utilize a previously introduced Quantum Neuron subroutine,\nwhich is a repeat-until-success circuit with mid-circuit measurements and\nclassical control. After introducing the QNBM, we investigate how its\nperformance depends on network size, by training a 3-layer QNBM with 4 output\nneurons and various input and hidden layer sizes. We then compare our\nnon-linear QNBM to the linear Quantum Circuit Born Machine (QCBM). We allocate\nsimilar time and memory resources to each model, such that the only major\ndifference is the qubit overhead required by the QNBM. With gradient-based\ntraining, we show that while both models can easily learn a trivial uniform\nprobability distribution, on a more challenging class of distributions, the\nQNBM achieves an almost 3x smaller error rate than a QCBM with a similar number\nof tunable parameters. We therefore show that non-linearity is a useful\nresource in quantum generative models, and we put forth the QNBM as a new model\nwith good generative performance and potential for quantum advantage.",
    "descriptor": "",
    "authors": [
      "Kaitlin Gili",
      "Mykolas Sveistrys",
      "Chris Ballance"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14506"
  },
  {
    "id": "arXiv:2205.14515",
    "title": "Additive Higher-Order Factorization Machines",
    "abstract": "In the age of big data and interpretable machine learning, approaches need to\nwork at scale and at the same time allow for a clear mathematical understanding\nof the method's inner workings. While there exist inherently interpretable\nsemi-parametric regression techniques for large-scale applications to account\nfor non-linearity in the data, their model complexity is still often\nrestricted. One of the main limitations are missing interactions in these\nmodels, which are not included for the sake of better interpretability, but\nalso due to untenable computational costs. To address this shortcoming, we\nderive a scalable high-order tensor product spline model using a factorization\napproach. Our method allows to include all (higher-order) interactions of\nnon-linear feature effects while having computational costs proportional to a\nmodel without interactions. We prove both theoretically and empirically that\nour methods scales notably better than existing approaches, derive meaningful\npenalization schemes and also discuss further theoretical aspects. We finally\ninvestigate predictive and estimation performance both with synthetic and real\ndata.",
    "descriptor": "",
    "authors": [
      "David R\u00fcgamer"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14515"
  },
  {
    "id": "arXiv:2205.14520",
    "title": "Transfer Learning as a Method to Reproduce High-Fidelity NLTE Opacities  in Simulations",
    "abstract": "Simulations of high-energy density physics often need non-local thermodynamic\nequilibrium (NLTE) opacity data. This data, however, is expensive to produce at\nrelatively low-fidelity. It is even more so at high-fidelity such that the\nopacity calculations can contribute ninety-five percent of the total\ncomputation time. This proportion can even reach large proportions. Neural\nnetworks can be used to replace the standard calculations of low-fidelity data,\nand the neural networks can be trained to reproduce artificial, high-fidelity\nopacity spectra. In this work, it is demonstrated that a novel neural network\narchitecture trained to reproduce high-fidelity krypton spectra through\ntransfer learning can be used in simulations. Further, it is demonstrated that\nthis can be done while achieving a relative percent error of the peak radiative\ntemperature of the hohlraum of approximately 1\\% to 4\\% while achieving a 19.4x\nspeed up.",
    "descriptor": "",
    "authors": [
      "Michael D. Vander Wal",
      "Ryan G. McClarren",
      "Kelli D. Humbird"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)",
      "Plasma Physics (physics.plasm-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14520"
  },
  {
    "id": "arXiv:2205.14539",
    "title": "Improving VAE-based Representation Learning",
    "abstract": "Latent variable models like the Variational Auto-Encoder (VAE) are commonly\nused to learn representations of images. However, for downstream tasks like\nsemantic classification, the representations learned by VAE are less\ncompetitive than other non-latent variable models. This has led to some\nspeculations that latent variable models may be fundamentally unsuitable for\nrepresentation learning. In this work, we study what properties are required\nfor good representations and how different VAE structure choices could affect\nthe learned properties. We show that by using a decoder that prefers to learn\nlocal features, the remaining global features can be well captured by the\nlatent, which significantly improves performance of a downstream classification\ntask. We further apply the proposed model to semi-supervised learning tasks and\ndemonstrate improvements in data efficiency.",
    "descriptor": "",
    "authors": [
      "Mingtian Zhang",
      "Tim Z. Xiao",
      "Brooks Paige",
      "David Barber"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14539"
  },
  {
    "id": "arXiv:2205.14552",
    "title": "Graph Agnostic Estimators with Staggered Rollout Designs under Network  Interference",
    "abstract": "Randomized experiments are widely used to estimate causal effects across a\nvariety of domains. However, classical causal inference approaches rely on\ncritical independence assumptions that are violated by network interference,\nwhen the treatment of one individual influences the outcomes of others. All\nexisting approaches require at least approximate knowledge of the network,\nwhich may be unavailable and costly to collect. We consider the task of\nestimating the total treatment effect (TTE), or the average difference between\nthe outcomes when the whole population is treated versus when the whole\npopulation is untreated. By leveraging a staggered rollout design, in which\ntreatment is incrementally given to random subsets of individuals, we derive\nunbiased estimators for TTE that do not rely on any prior structural knowledge\nof the network, as long as the network interference effects are constrained to\nlow-degree interactions among neighbors of an individual. We derive bounds on\nthe variance of the estimators, and we show in experiments that our estimator\nperforms well against baselines on simulated data. Central to our theoretical\ncontribution is a connection between staggered rollout observations and\npolynomial extrapolation.",
    "descriptor": "\nComments: 24 pages, 16 figures, submitted to Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Mayleen Cortez",
      "Matthew Eichhorn",
      "Christina Lee Yu"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.14552"
  },
  {
    "id": "arXiv:2205.14568",
    "title": "Calibrated Predictive Distributions via Diagnostics for Conditional  Coverage",
    "abstract": "Uncertainty quantification is crucial for assessing the predictive ability of\nAI algorithms. A large body of work (including normalizing flows and Bayesian\nneural networks) has been devoted to describing the entire predictive\ndistribution (PD) of a target variable Y given input features $\\mathbf{X}$.\nHowever, off-the-shelf PDs are usually far from being conditionally calibrated;\ni.e., the probability of occurrence of an event given input $\\mathbf{X}$ can be\nsignificantly different from the predicted probability. Most current research\non predictive inference (such as conformal prediction) concerns constructing\nprediction sets, that do not only provide correct uncertainties on average over\nthe entire population (that is, averaging over $\\mathbf{X}$), but that are also\napproximately conditionally calibrated with accurate uncertainties for\nindividual instances. It is often believed that the problem of obtaining and\nassessing entire conditionally calibrated PDs is too challenging to approach.\nIn this work, we show that recalibration as well as validation are indeed\nattainable goals in practice. Our proposed method relies on the idea of\nregressing probability integral transform (PIT) scores against $\\mathbf{X}$.\nThis regression gives full diagnostics of conditional coverage across the\nentire feature space and can be used to recalibrate misspecified PDs. We\nbenchmark our corrected prediction bands against oracle bands and\nstate-of-the-art predictive inference algorithms for synthetic data, including\nsettings with distributional shift and dependent high-dimensional sequence\ndata. Finally, we demonstrate an application to the physical sciences in which\nwe assess and produce calibrated PDs for measurements of galaxy distances using\nimaging data (i.e., photometric redshifts).",
    "descriptor": "\nComments: 10 pages, 6 figures. Under review\n",
    "authors": [
      "Biprateep Dey",
      "David Zhao",
      "Jeffrey A. Newman",
      "Brett H. Andrews",
      "Rafael Izbicki",
      "Ann B. Lee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.14568"
  },
  {
    "id": "arXiv:2205.14595",
    "title": "Safeguarding NOMA Networks via Reconfigurable Dual-Functional Surface  under Imperfect CSI",
    "abstract": "This paper investigates the use of the reconfigurable dual-functional surface\nto guarantee the full-space secure transmission in non-orthogonal multiple\naccess (NOMA) networks. In the presence of eavesdroppers, the downlink\ncommunication from the base station to the legitimate users is safeguarded by\nthe simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS), where three practical operating protocols, namely energy\nsplitting (ES), mode selection (MS), and time splitting (TS), are studied. The\njoint optimization of power allocation, active and passive beamforming is\ninvestigated to maximize the secrecy energy efficiency (SEE), taking into\naccount the imperfect channel state information (CSI) of all channels. For ES,\nby approximating the semi-infinite constraints with the S-procedure and general\nsign-definiteness, the problem is solved by an alternating optimization\nframework. Besides, the proposed algorithm is extended to the MS protocol by\nsolving a mixed-integer non-convex problem. While for TS, a two-layer iterative\nmethod is proposed. Simulation results show that: 1) The proposed STAR-RIS\nassisted NOMA networks are able to provide up to 33.6\\% higher SEE than\nconventional RIS counterparts; 2) TS and ES protocols are generally preferable\nfor low and high power domain, respectively; 3) The accuracy of CSI estimation\nand the bit resolution power consumption are crucial to reap the SEE benefits\noffered by STAR-RIS.",
    "descriptor": "\nComments: This paper has been accepted by the IEEE Journal of Selected Topics in Signal Processing\n",
    "authors": [
      "Wen Wang",
      "Wanli Ni",
      "Hui Tian",
      "Zhaohui Yang",
      "Chongwen Huang",
      "Kai-Kit Wong"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14595"
  },
  {
    "id": "arXiv:2205.14608",
    "title": "Aircraft and Differential Flatness",
    "abstract": "We investigate apparent and intrinsic singularities of a flat model of\naircrafts, illustrated with numerical simulations using Python and Maple. We\nconsider failure situations and maneuvers for which apparent singularities of\nthe previously known flat outputs may appear, making necessary to use some of\nthe new flat outputs we consider.\nBasically, the aircraft flat outputs are $x$, $y$, $z$, the coordinates of\nthe gravity center, completed with any function of the sideslip angle $\\beta$,\nthe angle of attack $\\alpha$, the bank angle $\\mu$ and the thrust $F$. The\nchoice of $\\beta$ was previously used, but does not allow gravity-free flight,\nfor which $\\mu$ is the best choice, as well as for decrabe maneuver. The choice\nof $F$ is adapted for dead-stick landing conditions with $\\beta\\neq0$, such as\nforward slip maneuver.\nThis approach also allows to replace usual control with new controls in case\nof failures, e.g. differential thrust can be used in case of rudder failure.\nOur results are illustrated by numerical simulations, using realistic non\nlinear aerodynamics models. In a first stage, we investigate the ability of the\nflatness based control to reject perturbations. Since flatness in that case\nrequires some model simplification, in a second stage, we focus on model errors\nand show that a suitable feed-back allows to keep trajectories with the\ncomplete real model close to the trajectories planned with the simplified one.",
    "descriptor": "\nComments: 26 pages, 21 figures\n",
    "authors": [
      "Yirmeyahu J. Kaminski",
      "Fran\u00e7ois Ollivier"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Symbolic Computation (cs.SC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14608"
  },
  {
    "id": "arXiv:2205.14613",
    "title": "A Conditional Randomization Test for Sparse Logistic Regression in  High-Dimension",
    "abstract": "Identifying the relevant variables for a classification model with correct\nconfidence levels is a central but difficult task in high-dimension. Despite\nthe core role of sparse logistic regression in statistics and machine learning,\nit still lacks a good solution for accurate inference in the regime where the\nnumber of features $p$ is as large as or larger than the number of samples $n$.\nHere, we tackle this problem by improving the Conditional Randomization Test\n(CRT). The original CRT algorithm shows promise as a way to output p-values\nwhile making few assumptions on the distribution of the test statistics. As it\ncomes with a prohibitive computational cost even in mildly high-dimensional\nproblems, faster solutions based on distillation have been proposed. Yet, they\nrely on unrealistic hypotheses and result in low-power solutions. To improve\nthis, we propose \\emph{CRT-logit}, an algorithm that combines a\nvariable-distillation step and a decorrelation step that takes into account the\ngeometry of $\\ell_1$-penalized logistic regression problem. We provide a\ntheoretical analysis of this procedure, and demonstrate its effectiveness on\nsimulations, along with experiments on large-scale brain-imaging and genomics\ndatasets.",
    "descriptor": "",
    "authors": [
      "Binh T. Nguyen",
      "Bertrand Thirion",
      "Sylvain Arlot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.14613"
  },
  {
    "id": "arXiv:2205.14622",
    "title": "Unified Approach to Secret Sharing and Symmetric Private Information  Retrieval with Colluding Servers in Quantum Systems",
    "abstract": "This paper unifiedly addresses two kinds of key quantum secure tasks, i.e.,\nquantum versions of secret sharing (SS) and symmetric private information\nretrieval (SPIR) by using multi-target monotone span program (MMSP), which\ncharacterizes the classical linear protocols of SS and SPIR. In particular, two\nkinds of quantum extensions of SS are known; One is the classical-quantum (CQ)\nsetting, in which the secret to be sent is classical information and the shares\nare quantum systems. The other is the quantum-quantum (QQ) setting, in which\nthe secret to be sent is a quantum state and the shares are quantum systems. We\nnewly introduce the third setting, i.e., the entanglement-assisted (EA)\nsetting, which is defined by modifying the CQ setting with allowing prior\nentanglement between the dealer and the end-user who recovers the secret by\ncollecting the shares. Showing that the linear version of SS with the EA\nsetting is directly linked to MMSP, we characterize linear quantum versions of\nSS with the CQ ad QQ settings via MMSP. Further, we also introduce the EA\nsetting of SPIR, which is shown to link to MMSP. In addition, we discuss the\nrelation with the quantum version of maximum distance separable (MDS) codes.",
    "descriptor": "",
    "authors": [
      "Masahito Hayashi",
      "Seunghoan Song"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14622"
  },
  {
    "id": "arXiv:2205.14627",
    "title": "Continuous Generative Neural Networks",
    "abstract": "In this work, we present and study Continuous Generative Neural Networks\n(CGNNs), namely, generative models in the continuous setting. The architecture\nis inspired by DCGAN, with one fully connected layer, several convolutional\nlayers and nonlinear activation functions. In the continuous $L^2$ setting, the\ndimensions of the spaces of each layer are replaced by the scales of a\nmultiresolution analysis of a compactly supported wavelet. We present\nconditions on the convolutional filters and on the nonlinearity that guarantee\nthat a CGNN is injective. This theory finds applications to inverse problems,\nand allows for deriving Lipschitz stability estimates for (possibly nonlinear)\ninfinite-dimensional inverse problems with unknowns belonging to the manifold\ngenerated by a CGNN. Several numerical simulations, including image deblurring,\nillustrate and validate this approach.",
    "descriptor": "\nComments: 34 pages, 7 figures\n",
    "authors": [
      "Giovanni S. Alberti",
      "Matteo Santacesaria",
      "Silvia Sciutto"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14627"
  },
  {
    "id": "arXiv:2205.14634",
    "title": "Assessing the accuracy of the Australian Senate count: Key steps for a  rigorous and transparent audit",
    "abstract": "This paper explains the main principles and some of the technical details for\nauditing the scanning and digitisation of the Australian Senate ballot papers.\nWe give a short summary of the motivation for auditing paper ballots, explain\nthe necessary supporting steps for a rigorous and transparent audit, and\nsuggest some statistical methods that would be appropriate for the Australian\nSenate.",
    "descriptor": "",
    "authors": [
      "Michelle Blom",
      "Philip B. Stark",
      "Peter J. Stuckey",
      "Vanessa Teague",
      "Damjan Vukcevic"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.14634"
  },
  {
    "id": "arXiv:2205.14662",
    "title": "No-Regret Learning in Network Stochastic Zero-Sum Games",
    "abstract": "No-regret learning has been widely used to compute a Nash equilibrium in\ntwo-person zero-sum games. However, there is still a lack of regret analysis\nfor network stochastic zero-sum games, where players competing in two\nsubnetworks only have access to some local information, and the cost functions\ninclude uncertainty. Such a game model can be found in security games, when a\ngroup of inspectors work together to detect a group of evaders. In this paper,\nwe propose a distributed stochastic mirror descent (D-SMD) method, and\nestablish the regret bounds $O(\\sqrt{T})$ and $O(\\log T)$ in the expected sense\nfor convex-concave and strongly convex-strongly concave costs, respectively.\nOur bounds match those of the best known first-order online optimization\nalgorithms. We then prove the convergence of the time-averaged iterates of\nD-SMD to the set of Nash equilibria. Finally, we show that the actual iterates\nof D-SMD almost surely converge to the Nash equilibrium in the strictly\nconvex-strictly concave setting.",
    "descriptor": "",
    "authors": [
      "Shijie Huang",
      "Jinlong Lei",
      "Yiguang Hong"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.14662"
  },
  {
    "id": "arXiv:2205.14700",
    "title": "To catch a chorus, verse, intro, or anything else: Analyzing a song with  structural functions",
    "abstract": "Conventional music structure analysis algorithms aim to divide a song into\nsegments and to group them with abstract labels (e.g., 'A', 'B', and 'C').\nHowever, explicitly identifying the function of each segment (e.g., 'verse' or\n'chorus') is rarely attempted, but has many applications. We introduce a\nmulti-task deep learning framework to model these structural semantic labels\ndirectly from audio by estimating \"verseness,\" \"chorusness,\" and so forth, as a\nfunction of time. We propose a 7-class taxonomy (i.e., intro, verse, chorus,\nbridge, outro, instrumental, and silence) and provide rules to consolidate\nannotations from four disparate datasets. We also propose to use a\nspectral-temporal Transformer-based model, called SpecTNT, which can be trained\nwith an additional connectionist temporal localization (CTL) loss. In\ncross-dataset evaluations using four public datasets, we demonstrate the\neffectiveness of the SpecTNT model and CTL loss, and obtain strong results\noverall: the proposed system outperforms state-of-the-art chorus-detection and\nboundary-detection methods at detecting choruses and boundaries, respectively.",
    "descriptor": "\nComments: This manuscript is accepted by ICASSP 2022\n",
    "authors": [
      "Ju-Chiang Wang",
      "Yun-Ning Hung",
      "Jordan B. L. Smith"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.14700"
  },
  {
    "id": "arXiv:2205.14714",
    "title": "Heterogeneous Treatment Effects Estimation: When Machine Learning meets  multiple treatment regime",
    "abstract": "In many scientific and engineering domains, inferring the effect of treatment\nand exploring its heterogeneity is crucial for optimization and decision\nmaking. In addition to Machine Learning based models (e.g. Random Forests or\nNeural Networks), many meta-algorithms have been developed to estimate the\nConditional Average Treatment Effect (CATE) function in the binary setting,\nwith the main advantage of not restraining the estimation to a specific\nsupervised learning method. However, this task becomes more challenging when\nthe treatment is not binary. In this paper, we investigate the Rubin Causal\nModel under the multi-treatment regime and we focus on estimating heterogeneous\ntreatment effects. We generalize \\textit{Meta-learning} algorithms to estimate\nthe CATE for each possible treatment value. Using synthetic and semi-synthetic\nsimulation datasets, we assess the quality of each meta-learner in\nobservational data, and we highlight in particular the performances of the\nX-learner.",
    "descriptor": "\nComments: 37 pages, 9 figures\n",
    "authors": [
      "Naoufal Acharki",
      "Josselin Garnier",
      "Antoine Bertoncello",
      "Ramiro Lugo"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.14714"
  },
  {
    "id": "arXiv:2205.14729",
    "title": "CMOS-Compatible Ising Machines built using Bistable Latches Coupled  through Ferroelectric Transistor Arrays",
    "abstract": "Realizing compact and scalable Ising machines that are compatible with\nCMOS-process technology is crucial to the effectiveness and practicality of\nusing such hardware platforms for accelerating computationally intractable\nproblems. Besides the need for realizing compact Ising spins, the\nimplementation of the coupling network, which describes the spin interaction,\nis also a potential bottleneck in the scalability of such platforms. Therefore,\nin this work, we propose an Ising machine platform that exploits the novel\nbehavior of compact bi-stable CMOS-latches (cross-coupled inverters) as\nclassical Ising spins interacting through highly scalable and CMOS-process\ncompatible ferroelectric-HfO2-based Ferroelectric FETs (FeFETs) which act as\ncoupling elements. We experimentally demonstrate the prototype building blocks\nof this system, and evaluate the behavior of the scaled system using\nsimulations. We project that the proposed architecture can compute Ising\nsolutions with an efficiency of ~1.04 x 10^8 solutions/W/second. Our work not\nonly provides a pathway to realizing CMOS-compatible designs but also to\novercoming their scaling challenges.",
    "descriptor": "\nComments: 29 pages, 10 figures\n",
    "authors": [
      "Antik Mallick",
      "Zijian Zhao",
      "Mohammad Khairul Bashar",
      "Shamiul Alam",
      "Md Mazharul Islam",
      "Yi Xiao",
      "Yixin Xu",
      "Ahmedullah Aziz",
      "Vijaykrishnan Narayanan",
      "Kai Ni",
      "Nikhil Shukla"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.14729"
  },
  {
    "id": "arXiv:2205.14741",
    "title": "CP2K on the road to exascale",
    "abstract": "The CP2K program package, which can be considered as the swiss army knife of\natomistic simulations, is presented with a special emphasis on ab-initio\nmolecular dynamics using the second-generation Car-Parrinello method. After\noutlining current and near-term development efforts with regards to massively\nparallel low-scaling post-Hartree-Fock and eigenvalue solvers, novel approaches\non how we plan to take full advantage of future low-precision hardware\narchitectures are introduced. Our focus here is on combining our submatrix\nmethod with the approximate computing paradigm to address the immanent exascale\nera.",
    "descriptor": "\nComments: 7 pages, 2 figures\n",
    "authors": [
      "Thomas D. K\u00fchne",
      "Christian Plessl",
      "Robert Schade",
      "Ole Sch\u00fctt"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.14741"
  },
  {
    "id": "arXiv:2205.14751",
    "title": "A Generative Adversarial Network-based Selective Ensemble  Characteristic-to-Expression Synthesis (SE-CTES) Approach and Its  Applications in Healthcare",
    "abstract": "Investigating the causal relationships between characteristics and\nexpressions plays a critical role in healthcare analytics. Effective synthesis\nfor expressions using given characteristics can make great contributions to\nhealth risk management and medical decision-making. For example, predicting the\nresulting physiological symptoms on patients from given treatment\ncharacteristics is helpful for the disease prevention and personalized\ntreatment strategy design. Therefore, the objective of this study is to\neffectively synthesize the expressions based on given characteristics. However,\nthe mapping from characteristics to expressions is usually from a relatively\nlow dimension space to a high dimension space, but most of the existing methods\nsuch as regression models could not effectively handle such mapping. Besides,\nthe relationship between characteristics and expressions may contain not only\ndeterministic patterns, but also stochastic patterns. To address these\nchallenges, this paper proposed a novel selective ensemble\ncharacteristic-to-expression synthesis (SE-CTES) approach inspired by\ngenerative adversarial network (GAN). The novelty of the proposed method can be\nsummarized into three aspects: (1) GAN-based architecture for deep neural\nnetworks are incorporated to learn the relatively low dimensional mapping to\nhigh dimensional mapping containing both deterministic and stochastic patterns;\n(2) the weights of the two mismatching errors in the GAN-based architecture are\nproposed to be different to reduce the learning bias in the training process;\nand (3) a selective ensemble learning framework is proposed to reduce the\nprediction bias and improve the synthesis stability. To validate the\neffectiveness of the proposed approach, extensive numerical simulation studies\nand a real-world healthcare case study were applied and the results\ndemonstrated that the proposed method is very promising.",
    "descriptor": "",
    "authors": [
      "Yuxuan Li",
      "Ying Lin",
      "Chenang Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14751"
  },
  {
    "id": "arXiv:2205.14775",
    "title": "An Optimization-based Algorithm for Non-stationary Kernel Bandits  without Prior Knowledge",
    "abstract": "We propose an algorithm for non-stationary kernel bandits that does not\nrequire prior knowledge of the degree of non-stationarity. The algorithm\nfollows randomized strategies obtained by solving optimization problems that\nbalance exploration and exploitation. It adapts to non-stationarity by\nrestarting when a change in the reward function is detected. Our algorithm\nenjoys a tighter dynamic regret bound than previous work on the non-stationary\nkernel bandit setting. Moreover, when applied to the non-stationary linear\nbandit setting by using a linear kernel, our algorithm is nearly minimax\noptimal, solving an open problem in the non-stationary linear bandit\nliterature. We extend our algorithm to use a neural network for dynamically\nadapting the feature mapping to observed data. We prove a dynamic regret bound\nof the extension using the neural tangent kernel theory. We demonstrate\nempirically that our algorithm and the extension can adapt to varying degrees\nof non-stationarity.",
    "descriptor": "",
    "authors": [
      "Kihyuk Hong",
      "Yuhang Li",
      "Ambuj Tewari"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14775"
  },
  {
    "id": "arXiv:2205.14784",
    "title": "Transient Behavior of Gossip Opinion Dynamics with Community Structure",
    "abstract": "We study transient behavior of gossip opinion dynamics, in which agents\nrandomly interact pairwise over a graph with community structure. We first\nstudy behavior of the model over a weighted graph with two communities. Edges\nwithin a community have identical weights different from edge weights between\ncommunities. A sharp phase transition is discovered: When edge weights within\ncommunities are larger than those between communities and those between regular\nand stubborn agents, most agents in the same community hold opinions close to\nthe initial average opinion of that community with large probability, at an\nearly stage of the process. However, if the difference between intra- and\ninter-community weights is small enough, most of the agents instead hold\nopinions close to everyone's initial average opinion at the early stage. In\ncontrast, when the influence of stubborn agents is large enough, agent opinions\nsettle quickly into their steady states. We then conduct numerical experiments\nto validate the theoretical results, and demonstrate extensions to multiple\ncommunities and stochastic block models, by providing two numerical examples.\nDifferent from the traditional asymptotic analysis in most opinion dynamics\nliterature, the paper characterizes the influences of stubborn agents and\ncommunity structure on the initial phase of the opinion evolution.",
    "descriptor": "",
    "authors": [
      "Yu Xing",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14784"
  },
  {
    "id": "arXiv:2205.14807",
    "title": "BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for  Binaural Audio Synthesis",
    "abstract": "Binaural audio plays a significant role in constructing immersive augmented\nand virtual realities. As it is expensive to record binaural audio from the\nreal world, synthesizing them from mono audio has attracted increasing\nattention. This synthesis process involves not only the basic physical warping\nof the mono audio, but also room reverberations and head/ear related\nfiltrations, which, however, are difficult to accurately simulate in\ntraditional digital signal processing. In this paper, we formulate the\nsynthesis process from a different perspective by decomposing the binaural\naudio into a common part that shared by the left and right channels as well as\na specific part that differs in each channel. Accordingly, we propose\nBinauralGrad, a novel two-stage framework equipped with diffusion models to\nsynthesize them respectively. Specifically, in the first stage, the common\ninformation of the binaural audio is generated with a single-channel diffusion\nmodel conditioned on the mono audio, based on which the binaural audio is\ngenerated by a two-channel diffusion model in the second stage. Combining this\nnovel perspective of two-stage synthesis with advanced generative models (i.e.,\nthe diffusion models),the proposed BinauralGrad is able to generate accurate\nand high-fidelity binaural audio samples. Experiment results show that on a\nbenchmark dataset, BinauralGrad outperforms the existing baselines by a large\nmargin in terms of both object and subject evaluation metrics (Wave L2: 0.128\nvs. 0.157, MOS: 3.80 vs. 3.61). The generated audio samples are available\nonline.",
    "descriptor": "\nComments: Demo page: this https URL\n",
    "authors": [
      "Yichong Leng",
      "Zehua Chen",
      "Junliang Guo",
      "Haohe Liu",
      "Jiawei Chen",
      "Xu Tan",
      "Danilo Mandic",
      "Lei He",
      "Xiang-Yang Li",
      "Tao Qin",
      "Sheng Zhao",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.14807"
  },
  {
    "id": "arXiv:2205.14811",
    "title": "Last-iterate convergence analysis of stochastic momentum methods for  neural networks",
    "abstract": "The stochastic momentum method is a commonly used acceleration technique for\nsolving large-scale stochastic optimization problems in artificial neural\nnetworks. Current convergence results of stochastic momentum methods under\nnon-convex stochastic settings mostly discuss convergence in terms of the\nrandom output and minimum output. To this end, we address the convergence of\nthe last iterate output (called last-iterate convergence) of the stochastic\nmomentum methods for non-convex stochastic optimization problems, in a way\nconformal with traditional optimization theory. We prove the last-iterate\nconvergence of the stochastic momentum methods under a unified framework,\ncovering both stochastic heavy ball momentum and stochastic Nesterov\naccelerated gradient momentum. The momentum factors can be fixed to be\nconstant, rather than time-varying coefficients in existing analyses. Finally,\nthe last-iterate convergence of the stochastic momentum methods is verified on\nthe benchmark MNIST and CIFAR-10 datasets.",
    "descriptor": "\nComments: 21pages, 4figures\n",
    "authors": [
      "Dongpo Xu",
      "Jinlan Liu",
      "Yinghua Lu",
      "Jun Kong",
      "Danilo Mandic"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14811"
  },
  {
    "id": "arXiv:2205.14818",
    "title": "Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student  Settings and its Superiority to Kernel Methods",
    "abstract": "While deep learning has outperformed other methods for various tasks,\ntheoretical frameworks that explain its reason have not been fully established.\nTo address this issue, we investigate the excess risk of two-layer ReLU neural\nnetworks in a teacher-student regression model, in which a student network\nlearns an unknown teacher network through its outputs. Especially, we consider\nthe student network that has the same width as the teacher network and is\ntrained in two phases: first by noisy gradient descent and then by the vanilla\ngradient descent. Our result shows that the student network provably reaches a\nnear-global optimal solution and outperforms any kernel methods estimator (more\ngenerally, linear estimators), including neural tangent kernel approach, random\nfeature model, and other kernel methods, in a sense of the minimax optimal\nrate. The key concept inducing this superiority is the non-convexity of the\nneural network models. Even though the loss landscape is highly non-convex, the\nstudent network adaptively learns the teacher neurons.",
    "descriptor": "\nComments: 29 pages, 1 figure\n",
    "authors": [
      "Shunta Akiyama",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14818"
  },
  {
    "id": "arXiv:2205.14828",
    "title": "Lepton Flavour Violation Identification in Tau Decay ($\u03c4^{-}  \\rightarrow \u03bc^{-}\u03bc^{-}\u03bc^{+}$) Using Artificial Intelligence",
    "abstract": "The discovery of neutrino oscillation, proving that neutrinos do have masses,\nreveals the misfits of particles in the current Standard Model (SM) theory. In\ntheory, neutrinos having masses could result in lepton flavour not being a\nsymmetry called Lepton Flavour Violation (LFV). While SM theory extensions\nallowed LFV processes, their branching fractions are too small, making them\nunobservable even with the strongest equipment up-to-date. With that,\nscientists in recent years have generated LFV-like processes from the combined\nLHCb and Monte-Carlo-Simulated data in an attempt to identify LFV using\nArtificial Intelligence (AI), specifically Machine Learning (ML) and Deep\nLearning (DL). This paper reports the contribution of the author on Flavours of\nPhysics: Finding $\\tau \\rightarrow \\mu\\mu\\mu$ competition on Kaggle. The\nperformance of several algorithms in AI has been presented, such as XGBoost,\nLightGBM, custom 1-D Dense Block Neural Networks (DBNNs), and custom 1-D\nConvolutional Neural Networks (CNNs) in identifying LFV signals, specifically\n$\\tau^{-} \\rightarrow \\mu^{-}\\mu^{-}\\mu^{+}$ decay from the combined LHCb and\nMonte-Carlo-Simulated data that imitates the signatures of the said decay.\nKolmogorov-Smirnov (KS) and Cramer-von Mises (CvM) tests were also conducted to\nverify the validity of predictions for each of the trained algorithms. The\nresult shows decent performances among algorithms, except for the LightGBM, for\nfailing the CvM test, and a 20-layered CNN for having recorded a considerably\nlow AUC. Meanwhile, XGBoost and a 10-layered DBNN recorded the highest AUC of\n0.88. The main contribution of this paper is the extensive experiment involving\ncustom DBNN and CNN algorithms in different layers, all of which have been\nrarely used in the past years in identifying LFV-like signatures, unlike GBMs\nand tree-based algorithms, which have been more popular in the said task.",
    "descriptor": "",
    "authors": [
      "Reymond Mesuga"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14828"
  },
  {
    "id": "arXiv:2205.14829",
    "title": "Adaptive Learning for Discovery",
    "abstract": "In this paper, we study a sequential decision-making problem, called Adaptive\nSampling for Discovery (ASD). Starting with a large unlabeled dataset,\nalgorithms for ASD adaptively label the points with the goal to maximize the\nsum of responses.\nThis problem has wide applications to real-world discovery problems, for\nexample drug discovery with the help of machine learning models. ASD algorithms\nface the well-known exploration-exploitation dilemma. The algorithm needs to\nchoose points that yield information to improve model estimates but it also\nneeds to exploit the model. We rigorously formulate the problem and propose a\ngeneral information-directed sampling (IDS) algorithm. We provide theoretical\nguarantees for the performance of IDS in linear, graph and low-rank models. The\nbenefits of IDS are shown in both simulation experiments and real-data\nexperiments for discovering chemical reaction conditions.",
    "descriptor": "",
    "authors": [
      "Ziping Xu",
      "Eunjae Shim",
      "Ambuj Tewari",
      "Paul Zimmerman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14829"
  },
  {
    "id": "arXiv:2205.14832",
    "title": "LatticeOPT: A heuristic topology optimization framework for thin-walled,  2D extruded lattices",
    "abstract": "This paper introduces a heuristic topology optimization framework for\nthin-walled, 2D extruded lattice structures subject to complex high-speed\nloading. The proposed framework optimizes the wall thickness distribution in\nthe lattice cross section through different thickness update schemes, inspired\nby the idea of equalization of absorbed energy density across all lattice\nwalls. The proposed framework is ubiquitous and can be used in explicit dynamic\nsimulations, which is the primary numerical method used in crashworthiness\nstudies. No information on the material tangent stiffness matrix is required,\nand complex material behaviors and complex loading conditions can be handled.\nThree numerical examples are presented to demonstrate framework capabilities:\n(1) Optimization of a long, slender column under axial compression to maximize\nspecific energy absorption, (2) Optimization of a lattice-filled sandwich panel\nunder off-center blast loading to minimize material damage, (3) Generation of a\nperiodic lattice core design under blast loading. The results show that the\nframework can effectively increase specific energy absorption or minimize\nmaterial damage with as few as 25 finite element simulations and optimization\niterations.",
    "descriptor": "",
    "authors": [
      "Junyan He",
      "Shashank Kushwaha",
      "Diab Abueidda",
      "Iwona Jasiuk"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.14832"
  },
  {
    "id": "arXiv:2205.14845",
    "title": "QFaaS: A Serverless Function-as-a-Service Framework for Quantum  Computing",
    "abstract": "Recent breakthroughs in quantum hardware are creating opportunities for its\nuse in many applications. However, quantum software engineering is still in its\ninfancy with many challenges, especially dealing with the diversity of quantum\nprogramming languages and hardware platforms. To alleviate these challenges, we\npropose QFaaS, a novel Quantum Function-as-a-Service framework, which leverages\nthe advantages of the serverless model and the state-of-the-art software\nengineering approaches to advance practical quantum computing. Our framework\nprovides essential components of a quantum serverless platform to simplify the\nsoftware development and adapt to the quantum cloud computing paradigm, such as\ncombining hybrid quantum-classical computation, containerizing functions, and\nintegrating DevOps features. We design QFaaS as a unified quantum computing\nframework by supporting well-known quantum languages and software development\nkits (Qiskit, Q#, Cirq, and Braket), executing the quantum tasks on multiple\nsimulators and quantum cloud providers (IBM Quantum and Amazon Braket). This\npaper proposes architectural design, principal components, the life cycle of\nhybrid quantum-classical function, operation workflow, and implementation of\nQFaaS. We present two practical use cases and perform the evaluations on\nquantum computers and simulators to demonstrate our framework's ability to ease\nthe burden on traditional engineers to expedite the ongoing quantum software\ntransition.",
    "descriptor": "\nComments: 35 pages, 15 figures\n",
    "authors": [
      "Hoa T. Nguyen",
      "Muhammad Usman",
      "Rajkumar Buyya"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.14845"
  },
  {
    "id": "arXiv:2205.14855",
    "title": "Leave-one-out Singular Subspace Perturbation Analysis for Spectral  Clustering",
    "abstract": "The singular subspaces perturbation theory is of fundamental importance in\nprobability and statistics. It has various applications across different\nfields. We consider two arbitrary matrices where one is a leave-one-column-out\nsubmatrix of the other one and establish a novel perturbation upper bound for\nthe distance between two corresponding singular subspaces. It is well-suited\nfor mixture models and results in a sharper and finer statistical analysis than\nclassical perturbation bounds such as Wedin's Theorem. Powered by this\nleave-one-out perturbation theory, we provide a deterministic entrywise\nanalysis for the performance of the spectral clustering under mixture models.\nOur analysis leads to an explicit exponential error rate for the clustering of\nsub-Gaussian mixture models. For the mixture of isotropic Gaussians, the rate\nis optimal under a weaker signal-to-noise condition than that of L\\\"offler et\nal. (2021).",
    "descriptor": "",
    "authors": [
      "Anderson Y. Zhang",
      "Harrison H. Zhou"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Spectral Theory (math.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.14855"
  },
  {
    "id": "arXiv:2205.14913",
    "title": "Skyrmion-based Leaky Integrate and Fire Neurons for Neuromorphic  Applications",
    "abstract": "Spintronics is an important emerging technology for data storage and\ncomputation. In this field, magnetic skyrmion-based devices are attractive due\nto their small size and energy consumption. However, controlling the creation,\ndeletion and motion of skyrmions is challenging. Here we propose a novel\nenergy-efficient skyrmion-based device structure, and demonstrate its use as\nleaky integrate (LIF) and fire neuron for neuromorphic computing. Here we show\nthat skyrmions can be confined by patterning the geometry of the free layer in\na magnetic tunnel junction (MTJ), and demonstrate that the size of the skyrmion\ncan be adjusted by applying pulsed voltage stresses. A spiking neural network\n(SNN) made of such skyrmion-based LIF neurons shows the capability of\nclassifying images from the Modified National Institute of Standards and\nTechnology (MNIST) dataset.",
    "descriptor": "\nComments: Under review in Applied Physics Letters\n",
    "authors": [
      "Aijaz H. Lone",
      "Selma Amara",
      "Fernando Aguirre",
      "Mario Lanza",
      "H. Fariborzi"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.14913"
  },
  {
    "id": "arXiv:2205.14915",
    "title": "Skyrmion-Magnetic Tunnel Junction Synapse with Mixed Synaptic Plasticity  for Neuromorphic Computing",
    "abstract": "Magnetic skyrmion-based data storage and unconventional computing devices\nhave gained increasing attention due to their topological protection, small\nsize, and low driving current. However, skyrmion creation, deletion, and motion\nare still being studied. In this study, we propose a skyrmion-based\nneuromorphic magnetic tunnel junction (MTJ) device with both long- and\nshort-term plasticity (LTP and STP) (mixed synaptic plasticity). We showed that\nplasticity could be controlled by magnetic field, spin-orbit torque (SOT), and\nthe voltage-controlled magnetic anisotropy (VCMA) switching mechanism. LTP\ndepends on the skyrmion density and is manipulated by the SOT and magnetic\nfield while STP is controlled by the VCMA. The LTP property of the device was\nutilized for static image recognition. By incorporating the STP feature, the\ndevice gained additional temporal filtering ability and could adapt to a\ndynamic environment. The skyrmions were conserved and confined to a nanotrack\nto minimize the skyrmion nucleation energy. The synapse device was trained and\ntested for emulating a deep neural network. We observed that when the skyrmion\ndensity was increased, the inference accuracy improved: 90% accuracy was\nachieved by the system at the highest density. We further demonstrated the\ndynamic environment learning and inference capabilities of the proposed device.",
    "descriptor": "\nComments: Submitted to Physical Review Applied\n",
    "authors": [
      "Aijaz H. Lone",
      "Arnab Ganguly",
      "Selma Amara",
      "Gobind Das",
      "H. Fariborzi"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.14915"
  },
  {
    "id": "arXiv:2205.14923",
    "title": "Unbalanced CO-Optimal Transport",
    "abstract": "Optimal transport (OT) compares probability distributions by computing a\nmeaningful alignment between their samples. CO-optimal transport (COOT) takes\nthis comparison further by inferring an alignment between features as well.\nWhile this approach leads to better alignments and generalizes both OT and\nGromov-Wasserstein distances, we provide a theoretical result showing that it\nis sensitive to outliers that are omnipresent in real-world data. This prompts\nus to propose unbalanced COOT for which we provably show its robustness to\nnoise in the compared datasets. To the best of our knowledge, this is the first\nsuch result for OT methods in incomparable spaces. With this result in hand, we\nprovide empirical evidence of this robustness for the challenging tasks of\nheterogeneous domain adaptation with and without varying proportions of classes\nand simultaneous alignment of samples and features across single-cell\nmeasurements.",
    "descriptor": "",
    "authors": [
      "Quang Huy Tran",
      "Hicham Janati",
      "Nicolas Courty",
      "R\u00e9mi Flamary",
      "Ievgen Redko",
      "Pinar Demetci",
      "Ritambhara Singh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14923"
  },
  {
    "id": "arXiv:2205.14977",
    "title": "Fast Nonlinear Vector Quantile Regression",
    "abstract": "Quantile regression (QR) is a powerful tool for estimating one or more\nconditional quantiles of a target variable $\\mathrm{Y}$ given explanatory\nfeatures $\\boldsymbol{\\mathrm{X}}$. A limitation of QR is that it is only\ndefined for scalar target variables, due to the formulation of its objective\nfunction, and since the notion of quantiles has no standard definition for\nmultivariate distributions. Recently, vector quantile regression (VQR) was\nproposed as an extension of QR for high-dimensional target variables, thanks to\na meaningful generalization of the notion of quantiles to multivariate\ndistributions. Despite its elegance, VQR is arguably not applicable in practice\ndue to several limitations: (i) it assumes a linear model for the quantiles of\nthe target $\\mathrm{Y}$ given the features $\\boldsymbol{\\mathrm{X}}$; (ii) its\nexact formulation is intractable even for modestly-sized problems in terms of\ntarget dimensions, number of regressed quantile levels, or number of features,\nand its relaxed dual formulation may violate the monotonicity of the estimated\nquantiles; (iii) no fast or scalable solvers for VQR currently exist. In this\nwork we fully address these limitations, namely: (i) We extend VQR to the\nnon-linear case, showing substantial improvement over linear VQR; (ii) We\npropose vector monotone rearrangement, a method which ensures the estimates\nobtained by VQR relaxations are monotone functions; (iii) We provide fast,\nGPU-accelerated solvers for linear and nonlinear VQR which maintain a fixed\nmemory footprint with number of samples and quantile levels, and demonstrate\nthat they scale to millions of samples and thousands of quantile levels; (iv)\nWe release an optimized python package of our solvers as to widespread the use\nof VQR in real-world applications.",
    "descriptor": "\nComments: 27 pages, 11 figures, submitted to NeurIPS 2022\n",
    "authors": [
      "Aviv A. Rosenberg",
      "Sanketh Vedula",
      "Yaniv Romano",
      "Alex M. Bronstein"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14977"
  },
  {
    "id": "arXiv:2205.14987",
    "title": "A Continuous Time Framework for Discrete Denoising Models",
    "abstract": "We provide the first complete continuous time framework for denoising\ndiffusion models of discrete data. This is achieved by formulating the forward\nnoising process and corresponding reverse time generative process as Continuous\nTime Markov Chains (CTMCs). The model can be efficiently trained using a\ncontinuous time version of the ELBO. We simulate the high dimensional CTMC\nusing techniques developed in chemical physics and exploit our continuous time\nframework to derive high performance samplers that we show can outperform\ndiscrete time methods for discrete data. The continuous time treatment also\nenables us to derive a novel theoretical result bounding the error between the\ngenerated sample distribution and the true data distribution.",
    "descriptor": "\nComments: 41 pages, 12 figures\n",
    "authors": [
      "Andrew Campbell",
      "Joe Benton",
      "Valentin De Bortoli",
      "Tom Rainforth",
      "George Deligiannidis",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14987"
  },
  {
    "id": "arXiv:2205.15003",
    "title": "Running the Dual-PQC GAN on noisy simulators and real quantum hardware",
    "abstract": "In an earlier work, we introduced dual-Parameterized Quantum Circuit (PQC)\nGenerative Adversarial Networks (GAN), an advanced prototype of a quantum GAN.\nWe applied the model on a realistic High-Energy Physics (HEP) use case: the\nexact theoretical simulation of a calorimeter response with a reduced problem\nsize. This paper explores the dual- PQC GAN for a more practical usage by\ntesting its performance in the presence of different types of quantum noise,\nwhich are the major obstacles to overcome for successful deployment using\nnear-term quantum devices. The results propose the possibility of running the\nmodel on current real hardware, but improvements are still required in some\nareas.",
    "descriptor": "\nComments: 6 pages, 5 figures, Proceedings of the 20th International Workshop on Advanced Computing and Analysis Techniques in Physics Research (ACAT 2021)\n",
    "authors": [
      "Su Yeon Chang",
      "Edwin Agnew",
      "El\u00edas F. Combarro",
      "Michele Grossi",
      "Steven Herbert",
      "Sofia Vallecorsa"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15003"
  },
  {
    "id": "arXiv:2205.15015",
    "title": "A principled framework for the design and analysis of token algorithms",
    "abstract": "We consider a decentralized optimization problem, in which $n$ nodes\ncollaborate to optimize a global objective function using local communications\nonly. While many decentralized algorithms focus on \\emph{gossip} communications\n(pairwise averaging), we consider a different scheme, in which a ``token'' that\ncontains the current estimate of the model performs a random walk over the\nnetwork, and updates its model using the local model of the node it is at.\nIndeed, token algorithms generally benefit from improved communication\nefficiency and privacy guarantees. We frame the token algorithm as a randomized\ngossip algorithm on a conceptual graph, which allows us to prove a series of\nconvergence results for variance-reduced and accelerated token algorithms for\nthe complete graph. We also extend these results to the case of multiple tokens\nby extending the conceptual graph, and to general graphs by tweaking the\ncommunication procedure. The reduction from token to well-studied gossip\nalgorithms leads to tight rates for many token algorithms, and we illustrate\ntheir performance empirically.",
    "descriptor": "",
    "authors": [
      "Hadrien Hendrikx"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.15015"
  },
  {
    "id": "arXiv:2205.15019",
    "title": "Protein Structure and Sequence Generation with Equivariant Denoising  Diffusion Probabilistic Models",
    "abstract": "Proteins are macromolecules that mediate a significant fraction of the\ncellular processes that underlie life. An important task in bioengineering is\ndesigning proteins with specific 3D structures and chemical properties which\nenable targeted functions. To this end, we introduce a generative model of both\nprotein structure and sequence that can operate at significantly larger scales\nthan previous molecular generative modeling approaches. The model is learned\nentirely from experimental data and conditions its generation on a compact\nspecification of protein topology to produce a full-atom backbone configuration\nas well as sequence and side-chain predictions. We demonstrate the quality of\nthe model via qualitative and quantitative analysis of its samples. Videos of\nsampling trajectories are available at https://nanand2.github.io/proteins .",
    "descriptor": "",
    "authors": [
      "Namrata Anand",
      "Tudor Achim"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15019"
  },
  {
    "id": "arXiv:2205.15020",
    "title": "Gene selection from microarray expression data: A Multi-objective PSO  with adaptive K-nearest neighborhood",
    "abstract": "Cancer detection is one of the key research topics in the medical field.\nAccurate detection of different cancer types is valuable in providing better\ntreatment facilities and risk minimization for patients. This paper deals with\nthe classification problem of human cancer diseases by using gene expression\ndata. It is presented a new methodology to analyze microarray datasets and\nefficiently classify cancer diseases. The new method first employs Signal to\nNoise Ratio (SNR) to find a list of a small subset of non-redundant genes.\nThen, after normalization, it is used Multi-Objective Particle Swarm\nOptimization (MOPSO) for feature selection and employed Adaptive K-Nearest\nNeighborhood (KNN) for cancer disease classification. This method improves the\nclassification accuracy of cancer classification by reducing the number of\nfeatures. The proposed methodology is evaluated by classifying cancer diseases\nin five cancer datasets. The results are compared with the most recent\napproaches, which increases the classification accuracy in each dataset.",
    "descriptor": "",
    "authors": [
      "Yasamin Kowsari",
      "Sanaz Nakhodchi",
      "Davoud Gholamiangonabadi"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.15020"
  },
  {
    "id": "arXiv:2205.15032",
    "title": "Structure of non-negative posets of Dynkin type $\\mathbb{A}_n$",
    "abstract": "We study, in terms of directed graphs, partially ordered sets (posets)\n$I=(\\{1,\\ldots, n\\}, \\preceq_I)$ that are non-negative in the sense that their\nsymmetric Gram matrix $G_I:=\\frac{1}{2}(C_I +\nC_I^{tr})\\in\\mathbb{M}_{|I|}(\\mathbb{Q})$ is positive semi-definite, where\n$C_I\\in\\mathbb{M}_n(\\mathbb{Z})$ is the incidence matrix of $I$ encoding the\nrelation $\\preceq_I$. We give a complete, up to isomorphism, structural\ndescription of connected posets $I$ of Dynkin type\n$\\mathrm{Dyn}_I=\\mathbb{A}_n$ in terms of their Hasse digraphs $\\mathcal{H}(I)$\nthat uniquely determine $I$. One of the main results of the paper is the proof\nthat the matrix $G_I$ is of rank $n$ or $n-1$, i.e., every non-negative poset\n$I$ with $\\mathrm{Dyn}_I=\\mathbb{A}_n$ is either positive or principal.\nMoreover, we depict explicit shapes of Hasse digraphs $\\mathcal{H}(I)$ of all\nnon-negative posets $I$ with $\\mathrm{Dyn}_I=\\mathbb{A}_n$. We show that\n$\\mathcal{H}(I)$ is isomorphic to an oriented path or cycle with at least two\nsinks. By giving explicit formulae for the number of all possible orientations\nof the path and cycle graphs, up to the isomorphism of unlabeled digraphs, we\ndevise formulae for the number of non-negative posets of Dynkin type\n$\\mathbb{A}_n$.",
    "descriptor": "",
    "authors": [
      "Marcin G\u0105siorek"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.15032"
  },
  {
    "id": "arXiv:2205.15050",
    "title": "Multi-fidelity robust controller design with gradient sampling",
    "abstract": "Robust controllers that stabilize dynamical systems even under disturbances\nand noise are often formulated as solutions of nonsmooth, nonconvex\noptimization problems. While methods such as gradient sampling can handle the\nnonconvexity and nonsmoothness, the costs of evaluating the objective function\nmay be substantial, making robust control challenging for dynamical systems\nwith high-dimensional state spaces. In this work, we introduce multi-fidelity\nvariants of gradient sampling that leverage low-cost, low-fidelity models with\nlow-dimensional state spaces for speeding up the optimization process while\nnonetheless providing convergence guarantees for a high-fidelity model of the\nsystem of interest, which is primarily accessed only in the last phase of the\noptimization process. Our first multi-fidelity method initiates gradient\nsampling on higher fidelity models with starting points obtained from cheaper,\nlower fidelity models. Our second multi-fidelity method relies on ensembles of\ngradients that are computed from low- and high-fidelity models. Numerical\nexperiments with controlling the cooling of a steel rail profile and laminar\nflow in a cylinder wake demonstrate that our new multi-fidelity gradient\nsampling methods achieve up to two orders of magnitude speedup compared to the\nsingle-fidelity gradient sampling method that relies on the high-fidelity model\nalone.",
    "descriptor": "\nComments: 27 pages, 4 figures\n",
    "authors": [
      "Steffen W. R. Werner",
      "Michael L. Overton",
      "Benjamin Peherstorfer"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.15050"
  },
  {
    "id": "arXiv:2205.15056",
    "title": "Stock Trading Optimization through Model-based Reinforcement Learning  with Resistance Support Relative Strength",
    "abstract": "Reinforcement learning (RL) is gaining attention by more and more researchers\nin quantitative finance as the agent-environment interaction framework is\naligned with decision making process in many business problems. Most of the\ncurrent financial applications using RL algorithms are based on model-free\nmethod, which still faces stability and adaptivity challenges. As lots of\ncutting-edge model-based reinforcement learning (MBRL) algorithms mature in\napplications such as video games or robotics, we design a new approach that\nleverages resistance and support (RS) level as regularization terms for action\nin MBRL, to improve the algorithm's efficiency and stability. From the\nexperiment results, we can see RS level, as a market timing technique, enhances\nthe performance of pure MBRL models in terms of various measurements and\nobtains better profit gain with less riskiness. Besides, our proposed method\neven resists big drop (less maximum drawdown) during COVID-19 pandemic period\nwhen the financial market got unpredictable crisis. Explanations on why control\nof resistance and support level can boost MBRL is also investigated through\nnumerical experiments, such as loss of actor-critic network and prediction\nerror of the transition dynamical model. It shows that RS indicators indeed\nhelp the MBRL algorithms to converge faster at early stage and obtain smaller\ncritic loss as training episodes increase.",
    "descriptor": "",
    "authors": [
      "Huifang Huang",
      "Ting Gao",
      "Yi Gui",
      "Jin Guo",
      "Peng Zhang"
    ],
    "subjectives": [
      "Mathematical Finance (q-fin.MF)",
      "Machine Learning (cs.LG)",
      "Portfolio Management (q-fin.PM)"
    ],
    "url": "https://arxiv.org/abs/2205.15056"
  },
  {
    "id": "arXiv:2205.15079",
    "title": "On the reduction of nonlinear electromechanical systems",
    "abstract": "The present work revisits the reduction of the nonlinear dynamics of an\nelectromechanical system through a quasi-steady state hypothesis, discussing\nthe fundamental aspects of this type of approach and clarifying some confusing\npoints found in the literature. Expressions for the characteristic time scales\nof dynamics are deduced from a physical analysis that establishes an analogy\nbetween electromechanical dynamics and the kinetics of a chemical reaction. It\nprovides a physical justification, supplemented by non-dimensionalization and\nscaling of the equations, to reduce the dynamics of interest by assuming a\nquasi-steady state for the electrical subsystem, eliminating the inductive term\nfrom the electrical equation. Numerical experiments help to illustrate the\ntypical behavior of the electromechanical system, a boundary layer phenomenon\nnear the initial dynamic state, and the validity limits of the\nelectromechanical quasi-steady-state assumption discussed here.",
    "descriptor": "",
    "authors": [
      "Americo Cunha Jr",
      "Marcelo Pereira",
      "Rafael Avan\u00e7o",
      "Angelo Marcelo Tusset",
      "Jos\u00e9 Manoel Balthazar"
    ],
    "subjectives": [
      "Classical Physics (physics.class-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Differential Geometry (math.DG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.15079"
  },
  {
    "id": "arXiv:2205.15096",
    "title": "Linear versus centred chromatic numbers",
    "abstract": "We prove that the linear chromatic number of any $k\\times k$ pseudogrid is\n$\\Omega(k)$. By an argument of Kun et al (Algorithmica, 2021), this result\ngives a tighter upper bound on the treedepth of a graph as a function of its\nlinear chromatic number and gives further evidence in support of their\nconjecture that the treedepth of any graph is upper bounded by a linear\nfunction of its linear chromatic number.",
    "descriptor": "\nComments: 17 pages; 5 figures\n",
    "authors": [
      "Prosenjit Bose",
      "Vida Dujmovi\u0107",
      "Hussein Houdrouge",
      "Mehrnoosh Javarsineh",
      "Pat Morin"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.15096"
  },
  {
    "id": "arXiv:2205.15136",
    "title": "Optimal Gradient Sliding and its Application to Distributed Optimization  Under Similarity",
    "abstract": "We study structured convex optimization problems, with additive objective\n$r:=p + q$, where $r$ is ($\\mu$-strongly) convex, $q$ is $L_q$-smooth and\nconvex, and $p$ is $L_p$-smooth, possibly nonconvex. For such a class of\nproblems, we proposed an inexact accelerated gradient sliding method that can\nskip the gradient computation for one of these components while still achieving\noptimal complexity of gradient calls of $p$ and $q$, that is,\n$\\mathcal{O}(\\sqrt{L_p/\\mu})$ and $\\mathcal{O}(\\sqrt{L_q/\\mu})$,\nrespectively. This result is much sharper than the classic black-box complexity\n$\\mathcal{O}(\\sqrt{(L_p+L_q)/\\mu})$, especially when the difference between\n$L_q$ and $L_q$ is large. We then apply the proposed method to solve\ndistributed optimization problems over master-worker architectures, under\nagents' function similarity, due to statistical data similarity or otherwise.\nThe distributed algorithm achieves for the first time lower complexity bounds\non {\\it both} communication and local gradient calls, with the former having\nbeing a long-standing open problem. Finally the method is extended to\ndistributed saddle-problems (under function similarity) by means of solving a\nclass of variational inequalities, achieving lower communication and\ncomputation complexity bounds.",
    "descriptor": "\nComments: 24 pages, 2 new algorithms, 12 theorems, 2 figures\n",
    "authors": [
      "Dmitry Kovalev",
      "Aleksandr Beznosikov",
      "Ekaterina Borodich",
      "Alexander Gasnikov",
      "Gesualdo Scutari"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15136"
  },
  {
    "id": "arXiv:2205.15170",
    "title": "GAN-based Medical Image Small Region Forgery Detection via a Two-Stage  Cascade Framework",
    "abstract": "Using generative adversarial network (GAN)\\cite{RN90} for data enhancement of\nmedical images is significantly helpful for many computer-aided diagnosis (CAD)\ntasks. A new attack called CT-GAN has emerged. It can inject or remove lung\ncancer lesions to CT scans. Because the tampering region may even account for\nless than 1\\% of the original image, even state-of-the-art methods are\nchallenging to detect the traces of such tampering.\nThis paper proposes a cascade framework to detect GAN-based medical image\nsmall region forgery like CT-GAN. In the local detection stage, we train the\ndetector network with small sub-images so that interference information in\nauthentic regions will not affect the detector. We use depthwise separable\nconvolution and residual to prevent the detector from over-fitting and enhance\nthe ability to find forged regions through the attention mechanism. The\ndetection results of all sub-images in the same image will be combined into a\nheatmap. In the global classification stage, using gray level co-occurrence\nmatrix (GLCM) can better extract features of the heatmap. Because the shape and\nsize of the tampered area are uncertain, we train PCA and SVM methods for\nclassification. Our method can classify whether a CT image has been tampered\nand locate the tampered position. Sufficient experiments show that our method\ncan achieve excellent performance.",
    "descriptor": "",
    "authors": [
      "Jianyi Zhang",
      "Xuanxi Huang",
      "Yaqi Liu",
      "Yuyang Han",
      "Zixiao Xiang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15170"
  },
  {
    "id": "arXiv:2205.15174",
    "title": "Modeling and simulation of nematic LCE rods",
    "abstract": "We introduce a nonlinear, one-dimensional bending-twisting model for an\ninextensible bi-rod that is composed of a nematic liquid crystal elastomer. The\nmodel combines an elastic energy that is quadratic in curvature and torsion\nwith a Frank-Oseen energy for the liquid crystal elastomer. Moreover, the model\nfeatures a nematic-elastic coupling that relates the crystalline orientation\nwith a spontaneous bending-twisting term. We show that the model can be derived\nas a $\\Gamma$-limit from three-dimensional nonlinear elasticity. Moreover, we\nintroduce a numerical scheme to compute critical points of the bending-twisting\nmodel via a discrete gradient flow. We present various numerical simulations\nthat illustrate the rich mechanical behavior predicted by the model. The\nnumerical experiments reveal good stability and approximation properties of the\nmethod.",
    "descriptor": "",
    "authors": [
      "S\u00f6ren Bartels",
      "Max Griehl",
      "Jakob Keck",
      "Stefan Neukamm"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.15174"
  },
  {
    "id": "arXiv:2205.15186",
    "title": "Double-cover-based analysis of the Bethe permanent of non-negative  matrices",
    "abstract": "The permanent of a non-negative matrix appears naturally in many information\nprocessing scenarios. Because of the intractability of the permanent beyond\nsmall matrices, various approximation techniques have been developed in the\npast. In this paper, we study the Bethe approximation of the permanent and add\nto the body of literature showing that this approximation is very well behaved\nin many respects. Our main technical tool are topological double covers of the\nnormal factor graph whose partition function equals the permanent of interest,\nalong with a transformation of these double covers.",
    "descriptor": "\nComments: 17 pages, 4 figures\n",
    "authors": [
      "Kit Shing Ng",
      "Pascal O. Vontobel"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.15186"
  },
  {
    "id": "arXiv:2205.15189",
    "title": "Independence number of intersection graphs of axis-parallel segments",
    "abstract": "We prove that for any triangle-free intersection graph of $n$ axis-parallel\nsegments in the plane, the independence number $\\alpha$ of this graph is at\nleast $\\alpha \\ge n/4 + \\Omega(\\sqrt{n})$. We complement this with a\nconstruction of a graph in this class satisfying $\\alpha \\le n/4 + c \\sqrt{n}$\nfor an absolute constant $c$, which demonstrates the optimality of our result.",
    "descriptor": "\nComments: 10 pages and 3 figures\n",
    "authors": [
      "Marco Caoduro",
      "Jana Cslovjecsek",
      "Micha\u0142 Pilipczuk",
      "Karol W\u0119grzycki"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.15189"
  },
  {
    "id": "arXiv:2205.15190",
    "title": "Vehicle Route Planning using Dynamically Weighted Dijkstra's Algorithm  with Traffic Prediction",
    "abstract": "Traditional vehicle routing algorithms do not consider the changing nature of\ntraffic. While implementations of Dijkstra's algorithm with varying weights\nexist, the weights are often changed after the outcome of algorithm is\nexecuted, which may not always result in the optimal route being chosen. Hence,\nthis paper proposes a novel vehicle routing algorithm that improves upon\nDijkstra's algorithm using a traffic prediction model based on the traffic flow\nin a road network. Here, Dijkstra's algorithm is adapted to be dynamic and time\ndependent using traffic flow theory principles during the planning stage\nitself. The model provides predicted traffic parameters and travel time across\neach edge of the road network at every time instant, leading to better routing\nresults. The dynamic algorithm proposed here predicts changes in traffic\nconditions at each time step of planning to give the optimal forward-looking\npath. The proposed algorithm is verified by comparing it with conventional\nDijkstra's algorithm on a graph with randomly simulated traffic, and is shown\nto predict the optimal route better with continuously changing traffic.",
    "descriptor": "",
    "authors": [
      "Piyush Udhan",
      "Akhilesh Ganeshkar",
      "Poobigan Murugesan",
      "Abhishek Raj Permani",
      "Sameep Sanjeeva",
      "Parth Deshpande"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.15190"
  },
  {
    "id": "arXiv:2205.15215",
    "title": "Support Recovery in Sparse PCA with Incomplete Data",
    "abstract": "We study a practical algorithm for sparse principal component analysis (PCA)\nof incomplete and noisy data. Our algorithm is based on the semidefinite\nprogram (SDP) relaxation of the non-convex $l_1$-regularized PCA problem. We\nprovide theoretical and experimental evidence that SDP enables us to exactly\nrecover the true support of the sparse leading eigenvector of the unknown true\nmatrix, despite only observing an incomplete (missing uniformly at random) and\nnoisy version of it. We derive sufficient conditions for exact recovery, which\ninvolve matrix incoherence, the spectral gap between the largest and\nsecond-largest eigenvalues, the observation probability and the noise variance.\nWe validate our theoretical results with incomplete synthetic data, and show\nencouraging and meaningful results on a gene expression dataset.",
    "descriptor": "",
    "authors": [
      "Hanbyul Lee",
      "Qifan Song",
      "Jean Honorio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15215"
  },
  {
    "id": "arXiv:2205.15239",
    "title": "Conformal Credal Self-Supervised Learning",
    "abstract": "In semi-supervised learning, the paradigm of self-training refers to the idea\nof learning from pseudo-labels suggested by the learner itself. Across various\ndomains, corresponding methods have proven effective and achieve\nstate-of-the-art performance. However, pseudo-labels typically stem from ad-hoc\nheuristics, relying on the quality of the predictions though without\nguaranteeing their validity. One such method, so-called credal self-supervised\nlearning, maintains pseudo-supervision in the form of sets of (instead of\nsingle) probability distributions over labels, thereby allowing for a flexible\nyet uncertainty-aware labeling. Again, however, there is no justification\nbeyond empirical effectiveness. To address this deficiency, we make use of\nconformal prediction, an approach that comes with guarantees on the validity of\nset-valued predictions. As a result, the construction of credal sets of labels\nis supported by a rigorous theoretical foundation, leading to better calibrated\nand less error-prone supervision for unlabeled data. Along with this, we\npresent effective algorithms for learning from credal self-supervision. An\nempirical study demonstrates excellent calibration properties of the\npseudo-supervision, as well as the competitiveness of our method on several\nbenchmark datasets.",
    "descriptor": "\nComments: 26 pages, 5 figures, 10 tables\n",
    "authors": [
      "Julian Lienen",
      "Caglar Demir",
      "Eyke H\u00fcllermeier"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15239"
  },
  {
    "id": "arXiv:2205.15268",
    "title": "Federated X-Armed Bandit",
    "abstract": "This work establishes the first framework of federated $\\mathcal{X}$-armed\nbandit, where different clients face heterogeneous local objective functions\ndefined on the same domain and are required to collaboratively figure out the\nglobal optimum. We propose the first federated algorithm for such problems,\nnamed \\texttt{Fed-PNE}. By utilizing the topological structure of the global\nobjective inside the hierarchical partitioning and the weak smoothness\nproperty, our algorithm achieves sublinear cumulative regret with respect to\nboth the number of clients and the evaluation budget. Meanwhile, it only\nrequires logarithmic communications between the central server and clients,\nprotecting the client privacy. Experimental results on synthetic functions and\nreal datasets validate the advantages of \\texttt{Fed-PNE} over single-client\nalgorithms and federated multi-armed bandit algorithms.",
    "descriptor": "",
    "authors": [
      "Wenjie Li",
      "Qifan Song",
      "Jean Honorio",
      "Guang Lin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15268"
  },
  {
    "id": "arXiv:2205.15275",
    "title": "Categorification of Extended Persistence Diagrams",
    "abstract": "The extended persistence diagram introduced by Cohen-Steiner, Edelsbrunner,\nand Harer is an invariant of real-valued continuous functions, which are\n$\\mathbb{F}$-tame in the sense that all open interlevel sets have degree-wise\nfinite-dimensional cohomology with coefficients in a fixed field $\\mathbb{F}$.\nWe show that relative interlevel set cohomology (RISC), which is based on the\nMayer--Vietoris pyramid by Carlsson, de Silva, and Morozov, categorifies this\ninvariant. More specifically, we define an abelian Frobenius category\n$\\mathrm{pres}(\\mathcal{J})$ of presheaves, which are presentable in some\nsense, such that for an $\\mathbb{F}$-tame function $f \\colon X \\rightarrow\n\\mathbb{R}$ its RISC $h(f)$ is an object of $\\mathrm{pres}(\\mathcal{J})$ and\nmoreover, the extended persistence diagram of $f$ uniquely determines - and is\ndetermined by - the corresponding element $[h(f)] \\in K_0\n(\\mathrm{pres}(\\mathcal{J}))$ in the Grothendieck group $K_0\n(\\mathrm{pres}(\\mathcal{J}))$ of the abelian category\n$\\mathrm{pres}(\\mathcal{J})$. As an intermediate step we show that\n$\\mathrm{pres}(\\mathcal{J})$ is the Abelianization of the (localized) category\nof complexes of $\\mathbb{F}$-linear sheaves on $\\mathbb{R}$, which are tame in\nthe sense that sheaf cohomology of any open interval is finite-dimensional in\neach degree. This yields a close link between derived level set persistence by\nCurry, Kashiwara, and Schapira and the categorification of extended persistence\ndiagrams.",
    "descriptor": "\nComments: 66 pages + 12 pages appendix, 14 figures\n",
    "authors": [
      "Ulrich Bauer",
      "Benedikt Fluhr"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)",
      "K-Theory and Homology (math.KT)"
    ],
    "url": "https://arxiv.org/abs/2205.15275"
  },
  {
    "id": "arXiv:2205.15280",
    "title": "Testing for Geometric Invariance and Equivariance",
    "abstract": "Invariant and equivariant models incorporate the symmetry of an object to be\nestimated (here non-parametric regression functions $f : \\mathcal{X}\n\\rightarrow \\mathbb{R}$). These models perform better (with respect to $L^2$\nloss) and are increasingly being used in practice, but encounter problems when\nthe symmetry is falsely assumed. In this paper we present a framework for\ntesting for $G$-equivariance for any semi-group $G$. This will give confidence\nto the use of such models when the symmetry is not known a priori. These tests\nare independent of the model and are computationally quick, so can be easily\nused before model fitting to test their validity.",
    "descriptor": "\nComments: 15 Pages, 6 Figures\n",
    "authors": [
      "Louis G. Christie",
      "John A. D. Aston"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15280"
  },
  {
    "id": "arXiv:1703.01347",
    "title": "Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles",
    "abstract": "Comments: 30 pages",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Jung-hun Kim",
      "Se-Young Yun",
      "Minchan Jeong",
      "Jun Hyun Nam",
      "Jinwoo Shin",
      "Richard Combes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1703.01347"
  },
  {
    "id": "arXiv:1708.02975",
    "title": "Anomaly Detection on Graph Time Series",
    "abstract": "Comments: Very preminary work with some fatal mistakes. Some other work covering this will appear soon",
    "descriptor": "\nComments: Very preminary work with some fatal mistakes. Some other work covering this will appear soon\n",
    "authors": [
      "Daniel Hsu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1708.02975"
  },
  {
    "id": "arXiv:1709.09585",
    "title": "DeepTransport: Learning Spatial-Temporal Dependency for Traffic  Condition Forecasting",
    "abstract": "Comments: Accepted at IJCNN 2018",
    "descriptor": "\nComments: Accepted at IJCNN 2018\n",
    "authors": [
      "Xingyi Cheng",
      "Ruiqing Zhang",
      "Jie Zhou",
      "Wei Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1709.09585"
  },
  {
    "id": "arXiv:1809.00781",
    "title": "Matrix Infinitely Divisible Series: Tail Inequalities and Their  Applications",
    "abstract": "Comments: Final Version",
    "descriptor": "\nComments: Final Version\n",
    "authors": [
      "Chao Zhang",
      "Xianjie Gao",
      "Min-Hsiu Hsieh",
      "Hanyuan Hang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/1809.00781"
  },
  {
    "id": "arXiv:1809.06056",
    "title": "A Grover-search Based Quantum Learning Scheme for Classification",
    "abstract": "Comments: final version",
    "descriptor": "\nComments: final version\n",
    "authors": [
      "Yuxuan Du",
      "Min-Hsiu Hsieh",
      "Tongliang Liu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1809.06056"
  },
  {
    "id": "arXiv:1811.04218",
    "title": "Properties of Noncommutative Renyi and Augustin Information",
    "abstract": "Comments: Final Version",
    "descriptor": "\nComments: Final Version\n",
    "authors": [
      "Hao-Chung Cheng",
      "Li Gao",
      "Min-Hsiu Hsieh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1811.04218"
  },
  {
    "id": "arXiv:1906.11455",
    "title": "PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation",
    "abstract": "PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation",
    "descriptor": "",
    "authors": [
      "Ruixuan Luo",
      "Jingjing Xu",
      "Yi Zhang",
      "Zhiyuan Zhang",
      "Xuancheng Ren",
      "Xu Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/1906.11455"
  },
  {
    "id": "arXiv:1907.01949",
    "title": "Supervised Uncertainty Quantification for Segmentation with Multiple  Annotations",
    "abstract": "Comments: MICCAI 2019. Fixed a few typos",
    "descriptor": "\nComments: MICCAI 2019. Fixed a few typos\n",
    "authors": [
      "Shi Hu",
      "Daniel Worrall",
      "Stefan Knegt",
      "Bas Veeling",
      "Henkjan Huisman",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1907.01949"
  },
  {
    "id": "arXiv:1907.13413",
    "title": "On The Smoothness of Cross-Validation-Based Estimators Of Classifier  Performance",
    "abstract": "Comments: The paper is currently under review in Pattern Recognition Letters (PRL)",
    "descriptor": "\nComments: The paper is currently under review in Pattern Recognition Letters (PRL)\n",
    "authors": [
      "Waleed A. Yousef"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1907.13413"
  },
  {
    "id": "arXiv:1908.08600",
    "title": "Online Causal Inference for Advertising in Real-Time Bidding Auctions",
    "abstract": "Online Causal Inference for Advertising in Real-Time Bidding Auctions",
    "descriptor": "",
    "authors": [
      "Caio Waisman",
      "Harikesh S. Nair",
      "Carlos Carrion"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1908.08600"
  },
  {
    "id": "arXiv:1912.03841",
    "title": "A Logic that Captures $\u03b2$P on Ordered Structures",
    "abstract": "Comments: 15 pages. This article was reported with a title \"Logarithmic-Bounded Second-Order Quantifiers and Limited Nondeterminism\" in National Conference on Modern Logic 2019, on November 9 in Beijing",
    "descriptor": "\nComments: 15 pages. This article was reported with a title \"Logarithmic-Bounded Second-Order Quantifiers and Limited Nondeterminism\" in National Conference on Modern Logic 2019, on November 9 in Beijing\n",
    "authors": [
      "Kexu Wang",
      "Xishun Zhao"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/1912.03841"
  },
  {
    "id": "arXiv:1912.12636",
    "title": "Training of Quantized Deep Neural Networks using a Magnetic Tunnel  Junction-Based Synapse",
    "abstract": "Comments: Published in Semiconductor Science and Technology, Vol 36",
    "descriptor": "\nComments: Published in Semiconductor Science and Technology, Vol 36\n",
    "authors": [
      "Tzofnat Greenberg Toledo",
      "Ben Perach",
      "Itay Hubara",
      "Daniel Soudry",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/1912.12636"
  },
  {
    "id": "arXiv:2003.00317",
    "title": "A Bayesian approach to tissue-fraction estimation for oncological PET  segmentation",
    "abstract": "A Bayesian approach to tissue-fraction estimation for oncological PET  segmentation",
    "descriptor": "",
    "authors": [
      "Ziping Liu",
      "Joyce C. Mhlanga",
      "Richard Laforest",
      "Paul-Robert Derenoncourt",
      "Barry A. Siegel",
      "Abhinav K. Jha"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2003.00317"
  },
  {
    "id": "arXiv:2003.01227",
    "title": "Fast Predictive Uncertainty for Classification with Bayesian Deep  Networks",
    "abstract": "Comments: Updated version. Accepted for publication at UAI2022",
    "descriptor": "\nComments: Updated version. Accepted for publication at UAI2022\n",
    "authors": [
      "Marius Hobbhahn",
      "Agustinus Kristiadi",
      "Philipp Hennig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.01227"
  },
  {
    "id": "arXiv:2006.04583",
    "title": "Vertex removal in biclique graphs",
    "abstract": "Vertex removal in biclique graphs",
    "descriptor": "",
    "authors": [
      "Leandro Montero"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2006.04583"
  },
  {
    "id": "arXiv:2006.07046",
    "title": "Disentangled Representation Learning and Generation with Manifold  Optimization",
    "abstract": "Disentangled Representation Learning and Generation with Manifold  Optimization",
    "descriptor": "",
    "authors": [
      "Arun Pandey",
      "Michael Fanuel",
      "Joachim Schreurs",
      "Johan A. K. Suykens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.07046"
  },
  {
    "id": "arXiv:2006.14222",
    "title": "Set Based Stochastic Subsampling",
    "abstract": "Comments: 20 pages",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Bruno Andreis",
      "Seanie Lee",
      "A. Tuan Nguyen",
      "Juho Lee",
      "Eunho Yang",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.14222"
  },
  {
    "id": "arXiv:2007.03941",
    "title": "Liaison, safeguard, and well-being: analyzing the role of social robots  during the COVID-19 pandemic",
    "abstract": "Comments: 23 pages, 3 figures",
    "descriptor": "\nComments: 23 pages, 3 figures\n",
    "authors": [
      "Laura Aymerich-Franch",
      "Iliana Ferrer"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2007.03941"
  },
  {
    "id": "arXiv:2007.07296",
    "title": "FedBoosting: Federated Learning with Gradient Protected Boosting for  Text Recognition",
    "abstract": "Comments: The source code can be found at this https URL",
    "descriptor": "\nComments: The source code can be found at this https URL\n",
    "authors": [
      "Hanchi Ren",
      "Jingjing Deng",
      "Xianghua Xie",
      "Xiaoke Ma",
      "Yichuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.07296"
  },
  {
    "id": "arXiv:2007.11921",
    "title": "Quantum Differentially Private Sparse Regression Learning",
    "abstract": "Comments: Final Version. Accepted in IEEE Transactions on Information Theory on 19 March, 2022",
    "descriptor": "\nComments: Final Version. Accepted in IEEE Transactions on Information Theory on 19 March, 2022\n",
    "authors": [
      "Yuxuan Du",
      "Min-Hsiu Hsieh",
      "Tongliang Liu",
      "Shan You",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.11921"
  },
  {
    "id": "arXiv:2007.13930",
    "title": "Extreme event probability estimation using PDE-constrained optimization  and large deviation theory, with application to tsunamis",
    "abstract": "Extreme event probability estimation using PDE-constrained optimization  and large deviation theory, with application to tsunamis",
    "descriptor": "",
    "authors": [
      "Shanyin Tong",
      "Eric Vanden-Eijnden",
      "Georg Stadler"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2007.13930"
  },
  {
    "id": "arXiv:2008.05440",
    "title": "DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape  Generation",
    "abstract": "Comments: Accept to ACM Transaction on Graphics 2022, 26 pages",
    "descriptor": "\nComments: Accept to ACM Transaction on Graphics 2022, 26 pages\n",
    "authors": [
      "Jie Yang",
      "Kaichun Mo",
      "Yu-Kun Lai",
      "Leonidas J. Guibas",
      "Lin Gao"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2008.05440"
  },
  {
    "id": "arXiv:2009.13619",
    "title": "Ferrite: A Judgmental Embedding of Session Types in Rust",
    "abstract": "Ferrite: A Judgmental Embedding of Session Types in Rust",
    "descriptor": "",
    "authors": [
      "Ruofei Chen",
      "Stephanie Balzer"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2009.13619"
  },
  {
    "id": "arXiv:2010.09088",
    "title": "Energy-based error bound of physics-informed neural network solutions in  elasticity",
    "abstract": "Energy-based error bound of physics-informed neural network solutions in  elasticity",
    "descriptor": "",
    "authors": [
      "Mengwu Guo",
      "Ehsan Haghighat"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.09088"
  },
  {
    "id": "arXiv:2010.10217",
    "title": "Quantum circuit architecture search for variational quantum algorithms",
    "abstract": "Comments: Final version. See also a concurrent paper [arXiv:2010.08561]",
    "descriptor": "\nComments: Final version. See also a concurrent paper [arXiv:2010.08561]\n",
    "authors": [
      "Yuxuan Du",
      "Tao Huang",
      "Shan You",
      "Min-Hsiu Hsieh",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.10217"
  },
  {
    "id": "arXiv:2011.01816",
    "title": "A Deep Learning based Detection Method for Combined  Integrity-Availability Cyber Attacks in Power System",
    "abstract": "A Deep Learning based Detection Method for Combined  Integrity-Availability Cyber Attacks in Power System",
    "descriptor": "",
    "authors": [
      "Wangkun Xu",
      "Fei Teng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.01816"
  },
  {
    "id": "arXiv:2011.10396",
    "title": "Double Self-weighted Multi-view Clustering via Adaptive View Fusion",
    "abstract": "Comments: Corresponding author: Yuchong Hu",
    "descriptor": "\nComments: Corresponding author: Yuchong Hu\n",
    "authors": [
      "Xiang Fang",
      "Yuchong Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.10396"
  },
  {
    "id": "arXiv:2012.00675",
    "title": "Topological Learning for Brain Networks",
    "abstract": "Comments: Accepted for publication in the Annals of Applied Statistics; 32 pages, 14 figures, 4 tables",
    "descriptor": "\nComments: Accepted for publication in the Annals of Applied Statistics; 32 pages, 14 figures, 4 tables\n",
    "authors": [
      "Tananun Songdechakraiwut",
      "Moo K. Chung"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2012.00675"
  },
  {
    "id": "arXiv:2012.02026",
    "title": "Towards an AI assistant for power grid operators",
    "abstract": "Towards an AI assistant for power grid operators",
    "descriptor": "",
    "authors": [
      "Antoine Marot",
      "Alexandre Rozier",
      "Matthieu Dussartre",
      "Laure Crochepierre",
      "Benjamin Donnot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.02026"
  },
  {
    "id": "arXiv:2012.04980",
    "title": "A Discrete Model of Collective Marching on Rings",
    "abstract": "Comments: Corrected various typographical errors and some minor mathematical error in the statement of the theorem about erratic behavior. In an upcoming extended version we refine the bounds of Theorem 10 to O(log(k)n^2,mn)",
    "descriptor": "\nComments: Corrected various typographical errors and some minor mathematical error in the statement of the theorem about erratic behavior. In an upcoming extended version we refine the bounds of Theorem 10 to O(log(k)n^2,mn)\n",
    "authors": [
      "Michael Amir",
      "Noa Agmon",
      "Alfred M. Bruckstein"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Discrete Mathematics (cs.DM)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2012.04980"
  },
  {
    "id": "arXiv:2012.05874",
    "title": "Hindsight and Sequential Rationality of Correlated Play: Corrections",
    "abstract": "Comments: Corrected technical report with corrections highlighted and annotated. 28 pages and 15 figures",
    "descriptor": "\nComments: Corrected technical report with corrections highlighted and annotated. 28 pages and 15 figures\n",
    "authors": [
      "Dustin Morrill",
      "Ryan D'Orazio",
      "Reca Sarfati",
      "Marc Lanctot",
      "James R. Wright",
      "Amy Greenwald",
      "Michael Bowling"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.05874"
  },
  {
    "id": "arXiv:2012.05895",
    "title": "Probing Few-Shot Generalization with Attributes",
    "abstract": "Comments: Technical report, 26 pages",
    "descriptor": "\nComments: Technical report, 26 pages\n",
    "authors": [
      "Mengye Ren",
      "Eleni Triantafillou",
      "Kuan-Chieh Wang",
      "James Lucas",
      "Jake Snell",
      "Xaq Pitkow",
      "Andreas S. Tolias",
      "Richard Zemel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.05895"
  },
  {
    "id": "arXiv:2012.13154",
    "title": "Adversarial Momentum-Contrastive Pre-Training",
    "abstract": "Comments: 8 pages;",
    "descriptor": "\nComments: 8 pages;\n",
    "authors": [
      "Cong Xu",
      "Dan Li",
      "Min Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.13154"
  },
  {
    "id": "arXiv:2101.03280",
    "title": "Modeling and Detecting Communities in Node Attributed Networks",
    "abstract": "Modeling and Detecting Communities in Node Attributed Networks",
    "descriptor": "",
    "authors": [
      "Ren Ren",
      "Jinliang Shao",
      "Adrian N. Bishop",
      "Wei Xing Zheng"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2101.03280"
  },
  {
    "id": "arXiv:2101.06224",
    "title": "Multi-point dimensionality reduction to improve projection layout  reliability",
    "abstract": "Multi-point dimensionality reduction to improve projection layout  reliability",
    "descriptor": "",
    "authors": [
      "Farshad Barahimi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.06224"
  },
  {
    "id": "arXiv:2101.06381",
    "title": "DivSwapper: Towards Diversified Patch-based Arbitrary Style Transfer",
    "abstract": "Comments: Accepted by IJCAI 2022 (AI, Arts & Creativity Track)",
    "descriptor": "\nComments: Accepted by IJCAI 2022 (AI, Arts & Creativity Track)\n",
    "authors": [
      "Zhizhong Wang",
      "Lei Zhao",
      "Haibo Chen",
      "Zhiwen Zuo",
      "Ailin Li",
      "Wei Xing",
      "Dongming Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.06381"
  },
  {
    "id": "arXiv:2101.07077",
    "title": "Yet Another Representation of Binary Decision Trees: A Mathematical  Demonstration",
    "abstract": "Yet Another Representation of Binary Decision Trees: A Mathematical  Demonstration",
    "descriptor": "",
    "authors": [
      "Jinxiong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.07077"
  },
  {
    "id": "arXiv:2102.03888",
    "title": "OPT-GAN: A Broad-Spectrum Global Optimizer for Black-box Problems by  Learning Distribution",
    "abstract": "OPT-GAN: A Broad-Spectrum Global Optimizer for Black-box Problems by  Learning Distribution",
    "descriptor": "",
    "authors": [
      "Minfang Lu",
      "Shuai Ning",
      "Shuangrong Liu",
      "Fengyang Sun",
      "Bo Zhang",
      "Bo Yang",
      "Lin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2102.03888"
  },
  {
    "id": "arXiv:2102.06982",
    "title": "DeepRA: Predicting Joint Damage From Radiographs Using CNN with  Attention",
    "abstract": "DeepRA: Predicting Joint Damage From Radiographs Using CNN with  Attention",
    "descriptor": "",
    "authors": [
      "Neelambuj Chaturvedi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06982"
  },
  {
    "id": "arXiv:2102.09243",
    "title": "Improved Deep Reinforcement Learning with Expert Demonstrations for  Urban Autonomous Driving",
    "abstract": "Improved Deep Reinforcement Learning with Expert Demonstrations for  Urban Autonomous Driving",
    "descriptor": "",
    "authors": [
      "Haochen Liu",
      "Zhiyu Huang",
      "Jingda Wu",
      "Chen Lv"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2102.09243"
  },
  {
    "id": "arXiv:2102.11008",
    "title": "InsNet: An Efficient, Flexible, and Performant Insertion-based Text  Generation Model",
    "abstract": "InsNet: An Efficient, Flexible, and Performant Insertion-based Text  Generation Model",
    "descriptor": "",
    "authors": [
      "Sidi Lu",
      "Tao Meng",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.11008"
  },
  {
    "id": "arXiv:2102.12037",
    "title": "Conditional Image Generation by Conditioning Variational Auto-Encoders",
    "abstract": "Comments: 37 pages, 20 figures",
    "descriptor": "\nComments: 37 pages, 20 figures\n",
    "authors": [
      "William Harvey",
      "Saeid Naderiparizi",
      "Frank Wood"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.12037"
  },
  {
    "id": "arXiv:2103.01452",
    "title": "Social Profit Optimization with Demand Response Management in  Electricity Market: A Multi-timescale Leader-following Approach",
    "abstract": "Comments: 33 pages, 15 figures",
    "descriptor": "\nComments: 33 pages, 15 figures\n",
    "authors": [
      "Jianzheng Wang",
      "Yipeng Pang",
      "Guoqiang Hu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2103.01452"
  },
  {
    "id": "arXiv:2103.11841",
    "title": "Asymptotic Coefficients and Errors for Chebyshev Polynomial  Approximations with Weak Endpoint Singularities: Effects of Different Bases",
    "abstract": "Asymptotic Coefficients and Errors for Chebyshev Polynomial  Approximations with Weak Endpoint Singularities: Effects of Different Bases",
    "descriptor": "",
    "authors": [
      "Xiaolong Zhang",
      "John P. Boyd"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.11841"
  },
  {
    "id": "arXiv:2104.06324",
    "title": "Modeling the dynamics of language change: logistic regression,  Piotrowski's law, and a handful of examples in Polish",
    "abstract": "Modeling the dynamics of language change: logistic regression,  Piotrowski's law, and a handful of examples in Polish",
    "descriptor": "",
    "authors": [
      "Rafa\u0142 L. G\u00f3rski",
      "Maciej Eder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.06324"
  },
  {
    "id": "arXiv:2104.07361",
    "title": "Scale Invariant Monte Carlo under Linear Function Approximation with  Curvature based step-size",
    "abstract": "Comments: 42 pages, 9 figures (9 pages main body with 5 figures)",
    "descriptor": "\nComments: 42 pages, 9 figures (9 pages main body with 5 figures)\n",
    "authors": [
      "Rahul Madhavan",
      "Hemanta Makwana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.07361"
  },
  {
    "id": "arXiv:2104.08661",
    "title": "Explaining Answers with Entailment Trees",
    "abstract": "Comments: published in EMNLP 2021",
    "descriptor": "\nComments: published in EMNLP 2021\n",
    "authors": [
      "Bhavana Dalvi",
      "Peter Jansen",
      "Oyvind Tafjord",
      "Zhengnan Xie",
      "Hannah Smith",
      "Leighanna Pipatanangkura",
      "Peter Clark"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.08661"
  },
  {
    "id": "arXiv:2104.13137",
    "title": "A non-singular boundary element method for interactions between  acoustical field sources and structures",
    "abstract": "A non-singular boundary element method for interactions between  acoustical field sources and structures",
    "descriptor": "",
    "authors": [
      "Qiang Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Physics (physics.class-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2104.13137"
  },
  {
    "id": "arXiv:2105.09673",
    "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a  three-Layer ReLU Network",
    "abstract": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a  three-Layer ReLU Network",
    "descriptor": "",
    "authors": [
      "Amit Daniely",
      "Elad Granot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.09673"
  },
  {
    "id": "arXiv:2105.10130",
    "title": "Convergence of a spatial semi-discretization for a backward semilinear  stochastic parabolic equation",
    "abstract": "Convergence of a spatial semi-discretization for a backward semilinear  stochastic parabolic equation",
    "descriptor": "",
    "authors": [
      "Binjie Li",
      "Xiaoping Xie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.10130"
  },
  {
    "id": "arXiv:2105.15013",
    "title": "SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning",
    "abstract": "SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning",
    "descriptor": "",
    "authors": [
      "Jianhong Wang",
      "Yuan Zhang",
      "Yunjie Gu",
      "Tae-Kyun Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2105.15013"
  },
  {
    "id": "arXiv:2106.01628",
    "title": "A Coalgebraic Approach to Dualities for Neighborhood Frames",
    "abstract": "A Coalgebraic Approach to Dualities for Neighborhood Frames",
    "descriptor": "",
    "authors": [
      "Guram Bezhanishvili",
      "Nick Bezhanishvili",
      "Jim de Groot"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.01628"
  },
  {
    "id": "arXiv:2106.05421",
    "title": "Data-Driven Invariant Learning for Probabilistic Programs",
    "abstract": "Comments: 34 pages",
    "descriptor": "\nComments: 34 pages\n",
    "authors": [
      "Jialu Bao",
      "Nitesh Trivedi",
      "Drashti Pathak",
      "Justin Hsu",
      "Subhajit Roy"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.05421"
  },
  {
    "id": "arXiv:2106.06196",
    "title": "CausalAdv: Adversarial Robustness through the Lens of Causality",
    "abstract": "Comments: ICLR2022, 20 pages, 3 figures",
    "descriptor": "\nComments: ICLR2022, 20 pages, 3 figures\n",
    "authors": [
      "Yonggang Zhang",
      "Mingming Gong",
      "Tongliang Liu",
      "Gang Niu",
      "Xinmei Tian",
      "Bo Han",
      "Bernhard Sch\u00f6lkopf",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06196"
  },
  {
    "id": "arXiv:2106.06745",
    "title": "Minimization and Canonization of GFG Transition-Based Automata",
    "abstract": "Comments: 33 pages, 12 figures, some typos corrected, slightly modified some proofs and figures, added a few lines between theorems to further clarify some notions and assumptions. arXiv admin note: substantial text overlap with arXiv:2009.10885",
    "descriptor": "\nComments: 33 pages, 12 figures, some typos corrected, slightly modified some proofs and figures, added a few lines between theorems to further clarify some notions and assumptions. arXiv admin note: substantial text overlap with arXiv:2009.10885\n",
    "authors": [
      "Bader Abu Radi",
      "Orna Kupferman"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2106.06745"
  },
  {
    "id": "arXiv:2106.09766",
    "title": "On Use of the Moore-Penrose Pseudoinverse for Evaluating the RGA of  Non-Square Systems",
    "abstract": "Comments: Final published version with minor revisions",
    "descriptor": "\nComments: Final published version with minor revisions\n",
    "authors": [
      "Rafal Qasim Al Yousuf",
      "Jeffrey Uhlmann"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.09766"
  },
  {
    "id": "arXiv:2106.10185",
    "title": "NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model  Weights",
    "abstract": "Comments: 21 pages, 18 figures",
    "descriptor": "\nComments: 21 pages, 18 figures\n",
    "authors": [
      "Kirill Bykov",
      "Anna Hedstr\u00f6m",
      "Shinichi Nakajima",
      "Marina M.-C. H\u00f6hne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.10185"
  },
  {
    "id": "arXiv:2106.10595",
    "title": "Heterogeneous Multi-task Learning with Expert Diversity",
    "abstract": "Comments: 10 pages, 7 figures, BIOKDD, IEEE/ACM",
    "descriptor": "\nComments: 10 pages, 7 figures, BIOKDD, IEEE/ACM\n",
    "authors": [
      "Raquel Aoki",
      "Frederick Tung",
      "Gabriel L. Oliveira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.10595"
  },
  {
    "id": "arXiv:2106.11420",
    "title": "Policy Smoothing for Provably Robust Reinforcement Learning",
    "abstract": "Comments: Published as a conference paper at ICLR 2022",
    "descriptor": "\nComments: Published as a conference paper at ICLR 2022\n",
    "authors": [
      "Aounon Kumar",
      "Alexander Levine",
      "Soheil Feizi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.11420"
  },
  {
    "id": "arXiv:2106.13367",
    "title": "SeaNet -- Towards A Knowledge Graph Based Autonomic Management of  Software Defined Networks",
    "abstract": "SeaNet -- Towards A Knowledge Graph Based Autonomic Management of  Software Defined Networks",
    "descriptor": "",
    "authors": [
      "Qianru Zhou",
      "Alasdair J.G. Gray",
      "Stephen McLaughlin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.13367"
  },
  {
    "id": "arXiv:2107.00689",
    "title": "Aerial Map-Based Navigation Using Semantic Segmentation and Pattern  Matching",
    "abstract": "Comments: 6 pages, 4 figures",
    "descriptor": "\nComments: 6 pages, 4 figures\n",
    "authors": [
      "Youngjoo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2107.00689"
  },
  {
    "id": "arXiv:2107.01104",
    "title": "An Efficient Particle Tracking Algorithm for Large-Scale Parallel  Pseudo-Spectral Simulations of Turbulence",
    "abstract": "An Efficient Particle Tracking Algorithm for Large-Scale Parallel  Pseudo-Spectral Simulations of Turbulence",
    "descriptor": "",
    "authors": [
      "Cristian C. Lalescu",
      "B\u00e9renger Bramas",
      "Markus Rampp",
      "Michael Wilczek"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2107.01104"
  },
  {
    "id": "arXiv:2107.02847",
    "title": "Transfer Learning in Information Criteria-based Feature Selection",
    "abstract": "Comments: Accepted to the Journal of Machine Learning Research",
    "descriptor": "\nComments: Accepted to the Journal of Machine Learning Research\n",
    "authors": [
      "Shaohan Chen",
      "Nikolaos V. Sahinidis",
      "Chuanhou Gao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.02847"
  },
  {
    "id": "arXiv:2107.08036",
    "title": "The strong converse exponent of discriminating infinite-dimensional  quantum states",
    "abstract": "Comments: 39 pages. v4: Some proofs simplified, some statements improved, some typos corrected. Submitted version",
    "descriptor": "\nComments: 39 pages. v4: Some proofs simplified, some statements improved, some typos corrected. Submitted version\n",
    "authors": [
      "Mil\u00e1n Mosonyi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2107.08036"
  },
  {
    "id": "arXiv:2107.08924",
    "title": "Epistemic Neural Networks",
    "abstract": "Epistemic Neural Networks",
    "descriptor": "",
    "authors": [
      "Ian Osband",
      "Zheng Wen",
      "Seyed Mohammad Asghari",
      "Vikranth Dwaracherla",
      "Morteza Ibrahimi",
      "Xiuyuan Lu",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.08924"
  },
  {
    "id": "arXiv:2107.10938",
    "title": "BGP-Multipath Routing in the Internet",
    "abstract": "Comments: 38 pages, 8 figures, 8 tables",
    "descriptor": "\nComments: 38 pages, 8 figures, 8 tables\n",
    "authors": [
      "Jie Li",
      "Vasileios Giotsas",
      "Yangyang Wang",
      "Shi Zhou"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2107.10938"
  },
  {
    "id": "arXiv:2107.11914",
    "title": "Stabilizer codes for Open Quantum Systems",
    "abstract": "Stabilizer codes for Open Quantum Systems",
    "descriptor": "",
    "authors": [
      "Francisco Revson F. Pereira",
      "Stefano Mancini",
      "Giuliano G. La Guardia"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2107.11914"
  },
  {
    "id": "arXiv:2107.13668",
    "title": "Discovering User-Interpretable Capabilities of Black-Box Planning Agents",
    "abstract": "Comments: KR 2022",
    "descriptor": "\nComments: KR 2022\n",
    "authors": [
      "Pulkit Verma",
      "Shashank Rao Marpally",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.13668"
  },
  {
    "id": "arXiv:2108.00632",
    "title": "Skeena: Efficient and Consistent Cross-Engine Transactions",
    "abstract": "Comments: To appear at SIGMOD 2022",
    "descriptor": "\nComments: To appear at SIGMOD 2022\n",
    "authors": [
      "Jianqiu Zhang",
      "Kaisong Huang",
      "Tianzheng Wang",
      "King Lv"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2108.00632"
  },
  {
    "id": "arXiv:2108.02000",
    "title": "Do What You Know: Coupling Knowledge with Action in Discrete-Event  Systems",
    "abstract": "Do What You Know: Coupling Knowledge with Action in Discrete-Event  Systems",
    "descriptor": "",
    "authors": [
      "Richard Ean",
      "Karen Rudie"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2108.02000"
  },
  {
    "id": "arXiv:2108.05539",
    "title": "Put the Bear on the Chair! Intelligent Robot Interaction with Previously  Unseen Chairs via Robot Imagination",
    "abstract": "Comments: IEEE ICRA 2022. Video demos are available at this https URL",
    "descriptor": "\nComments: IEEE ICRA 2022. Video demos are available at this https URL\n",
    "authors": [
      "Hongtao Wu",
      "Xin Meng",
      "Sipu Ruan",
      "Gregory Chirikjian"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2108.05539"
  },
  {
    "id": "arXiv:2108.07425",
    "title": "NeuralSound: Learning-based Modal Sound Synthesis With Acoustic Transfer",
    "abstract": "NeuralSound: Learning-based Modal Sound Synthesis With Acoustic Transfer",
    "descriptor": "",
    "authors": [
      "Xutong Jin",
      "Sheng Li",
      "Guoping Wang",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Graphics (cs.GR)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2108.07425"
  },
  {
    "id": "arXiv:2108.07622",
    "title": "Two-Timescale Design for Reconfigurable Intelligent Surface-Aided  Massive MIMO Systems with Imperfect CSI",
    "abstract": "Comments: Revision in IEEE TIT. Keywords: Reconfigurable Intelligent Surface, Intelligent Reflecting Surface, Massive MIMO, Channel estimation, etc",
    "descriptor": "\nComments: Revision in IEEE TIT. Keywords: Reconfigurable Intelligent Surface, Intelligent Reflecting Surface, Massive MIMO, Channel estimation, etc\n",
    "authors": [
      "Kangda Zhi",
      "Cunhua Pan",
      "Hong Ren",
      "Kezhi Wang",
      "Maged Elkashlan",
      "Marco Di Renzo",
      "Robert Schober",
      "H. Vincent Poor",
      "Jiangzhou Wang",
      "Lajos Hanzo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2108.07622"
  },
  {
    "id": "arXiv:2108.09298",
    "title": "Structure and Interleavings of Relative Interlevel Set Cohomology",
    "abstract": "Comments: 40 pages + 10 pages appendix, 22 figures; this paper shares an appendix with an earlier version of arXiv:2007.01834; corrected the caption of figure 2.3, added missing assumption to corollary 3.15 (formerly corollary 3.13), added figure 3.2, added a corollary to theorem 3.5, added details to remark 4.1, added missing assumption to lemma A.15, strengthened lemma 2.3, several minor edits",
    "descriptor": "\nComments: 40 pages + 10 pages appendix, 22 figures; this paper shares an appendix with an earlier version of arXiv:2007.01834; corrected the caption of figure 2.3, added missing assumption to corollary 3.15 (formerly corollary 3.13), added figure 3.2, added a corollary to theorem 3.5, added details to remark 4.1, added missing assumption to lemma A.15, strengthened lemma 2.3, several minor edits\n",
    "authors": [
      "Ulrich Bauer",
      "Magnus Bakke Botnan",
      "Benedikt Fluhr"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2108.09298"
  },
  {
    "id": "arXiv:2108.09733",
    "title": "A universally consistent learning rule with a universally monotone error",
    "abstract": "Comments: latex, 27 pp., 4 figures, to appear in JMLR. The version, reformatted in the JMLR style, incorporates a number of remarks by two anonymous referees that have helped to fix some confusing passages and make the paper more readable",
    "descriptor": "\nComments: latex, 27 pp., 4 figures, to appear in JMLR. The version, reformatted in the JMLR style, incorporates a number of remarks by two anonymous referees that have helped to fix some confusing passages and make the paper more readable\n",
    "authors": [
      "Vladimir Pestov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.09733"
  },
  {
    "id": "arXiv:2108.12821",
    "title": "Analyzing and Mitigating Interference in Neural Architecture Search",
    "abstract": "Comments: ICML 2022, Spotlight",
    "descriptor": "\nComments: ICML 2022, Spotlight\n",
    "authors": [
      "Jin Xu",
      "Xu Tan",
      "Kaitao Song",
      "Renqian Luo",
      "Yichong Leng",
      "Tao Qin",
      "Tie-Yan Liu",
      "Jian Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.12821"
  },
  {
    "id": "arXiv:2109.01817",
    "title": "Low SNR Capacity of Keyhole MIMO Channel in Nakagami-m Fading With Full  CSI",
    "abstract": "Low SNR Capacity of Keyhole MIMO Channel in Nakagami-m Fading With Full  CSI",
    "descriptor": "",
    "authors": [
      "Kamal Singh",
      "Chandradeep Singh",
      "Kuang-Hao Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2109.01817"
  },
  {
    "id": "arXiv:2109.03349",
    "title": "Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite  Relaxations and Scalable Global Optimization",
    "abstract": "Comments: IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "descriptor": "\nComments: IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
    "authors": [
      "Heng Yang",
      "Luca Carlone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2109.03349"
  },
  {
    "id": "arXiv:2109.03544",
    "title": "What really changes when developers intend to improve their source code:  a commit-level study of static metric value and static analysis warning  changes",
    "abstract": "What really changes when developers intend to improve their source code:  a commit-level study of static metric value and static analysis warning  changes",
    "descriptor": "",
    "authors": [
      "Alexander Trautsch",
      "Johannes Erbel",
      "Steffen Herbold",
      "Jens Grabowski"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2109.03544"
  },
  {
    "id": "arXiv:2109.03999",
    "title": "Towards Efficient Synchronous Federated Training: A Survey on System  Optimization Strategies",
    "abstract": "Comments: This article has been accepted for publication in IEEE Transactions on Big Data. This is the author's version which has not been fully edited and content may change prior to final publication",
    "descriptor": "\nComments: This article has been accepted for publication in IEEE Transactions on Big Data. This is the author's version which has not been fully edited and content may change prior to final publication\n",
    "authors": [
      "Zhifeng Jiang",
      "Wei Wang",
      "Bo Li",
      "Qiang Yang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2109.03999"
  },
  {
    "id": "arXiv:2109.05422",
    "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
    "abstract": "Comments: Accepted by AAAI2022",
    "descriptor": "\nComments: Accepted by AAAI2022\n",
    "authors": [
      "Chuanxin Tang",
      "Yucheng Zhao",
      "Guangting Wang",
      "Chong Luo",
      "Wenxuan Xie",
      "Wenjun Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.05422"
  },
  {
    "id": "arXiv:2109.05675",
    "title": "Online Unsupervised Learning of Visual Representations and Categories",
    "abstract": "Comments: Technical report, 32 pages",
    "descriptor": "\nComments: Technical report, 32 pages\n",
    "authors": [
      "Mengye Ren",
      "Tyler R. Scott",
      "Michael L. Iuzzolino",
      "Michael C. Mozer",
      "Richard Zemel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.05675"
  },
  {
    "id": "arXiv:2109.05773",
    "title": "Fast Variational AutoEncoder with Inverted Multi-Index for Collaborative  Filtering",
    "abstract": "Comments: 11 pages, WWW 2022",
    "descriptor": "\nComments: 11 pages, WWW 2022\n",
    "authors": [
      "Jin Chen",
      "Defu Lian",
      "Binbin Jin",
      "Xu Huang",
      "Kai Zheng",
      "Enhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2109.05773"
  },
  {
    "id": "arXiv:2109.06148",
    "title": "DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection",
    "abstract": "Comments: Main paper: 8 pages, References: 2 pages, Appendix: 7 pages; Main paper: 6 figures, Appendix: 6 figures",
    "descriptor": "\nComments: Main paper: 8 pages, References: 2 pages, Appendix: 7 pages; Main paper: 6 figures, Appendix: 6 figures\n",
    "authors": [
      "Steven Lang",
      "Fabrizio Ventola",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.06148"
  },
  {
    "id": "arXiv:2109.11265",
    "title": "End-to-End Dense Video Grounding via Parallel Regression",
    "abstract": "Comments: Technical report",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Fengyuan Shi",
      "Limin Wang",
      "Weilin Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.11265"
  },
  {
    "id": "arXiv:2109.12355",
    "title": "Suboptimal nonlinear model predictive control with input move-blocking",
    "abstract": "Comments: Added \"expansion\" operator to Equation (17) and Remark 8;",
    "descriptor": "\nComments: Added \"expansion\" operator to Equation (17) and Remark 8;\n",
    "authors": [
      "Artemi Makarow",
      "Christoph R\u00f6smann",
      "Torsten Bertram"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2109.12355"
  },
  {
    "id": "arXiv:2109.12575",
    "title": "Paradigm Shift in Natural Language Processing",
    "abstract": "Comments: Accepted to Machine Intelligence Research. Published version: this https URL Website: this https URL",
    "descriptor": "\nComments: Accepted to Machine Intelligence Research. Published version: this https URL Website: this https URL\n",
    "authors": [
      "Tianxiang Sun",
      "Xiangyang Liu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.12575"
  },
  {
    "id": "arXiv:2109.14174",
    "title": "Cross-Camera Human Motion Transfer by Time Series Analysis",
    "abstract": "Comments: 10 pages, 9 figures",
    "descriptor": "\nComments: 10 pages, 9 figures\n",
    "authors": [
      "Yaping Zhao",
      "Guanghan Li",
      "Zhongrui Wang",
      "Edmund Y. Lam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.14174"
  },
  {
    "id": "arXiv:2110.00269",
    "title": "A Survey of Knowledge Enhanced Pre-trained Models",
    "abstract": "Comments: 16 pages, 11 figures",
    "descriptor": "\nComments: 16 pages, 11 figures\n",
    "authors": [
      "Jian Yang",
      "Gang Xiao",
      "Yulong Shen",
      "Wei Jiang",
      "Xinyu Hu",
      "Ying Zhang",
      "Jinghui Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.00269"
  },
  {
    "id": "arXiv:2110.01346",
    "title": "Clustering with Respect to the Information Distance",
    "abstract": "Comments: 12 pages; 3 figures",
    "descriptor": "\nComments: 12 pages; 3 figures\n",
    "authors": [
      "Andrei Romashchenko"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.01346"
  },
  {
    "id": "arXiv:2110.02501",
    "title": "On the Surrogate Gap between Contrastive and Supervised Losses",
    "abstract": "Comments: Accepted by ICML2022",
    "descriptor": "\nComments: Accepted by ICML2022\n",
    "authors": [
      "Han Bao",
      "Yoshihiro Nagano",
      "Kento Nozawa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02501"
  },
  {
    "id": "arXiv:2110.03141",
    "title": "Efficient Sharpness-aware Minimization for Improved Training of Neural  Networks",
    "abstract": "Efficient Sharpness-aware Minimization for Improved Training of Neural  Networks",
    "descriptor": "",
    "authors": [
      "Jiawei Du",
      "Hanshu Yan",
      "Jiashi Feng",
      "Joey Tianyi Zhou",
      "Liangli Zhen",
      "Rick Siow Mong Goh",
      "Vincent Y. F. Tan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03141"
  },
  {
    "id": "arXiv:2110.06910",
    "title": "On the Double Descent of Random Features Models Trained with SGD",
    "abstract": "Comments: 41 pages, 4 figures. This version provides more discussion on fair assumptions and tightness of our results",
    "descriptor": "\nComments: 41 pages, 4 figures. This version provides more discussion on fair assumptions and tightness of our results\n",
    "authors": [
      "Fanghui Liu",
      "Johan A.K. Suykens",
      "Volkan Cevher"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06910"
  },
  {
    "id": "arXiv:2110.07728",
    "title": "Pre-training Molecular Graph Representation with 3D Geometry",
    "abstract": "Pre-training Molecular Graph Representation with 3D Geometry",
    "descriptor": "",
    "authors": [
      "Shengchao Liu",
      "Hanchen Wang",
      "Weiyang Liu",
      "Joan Lasenby",
      "Hongyu Guo",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2110.07728"
  },
  {
    "id": "arXiv:2110.07786",
    "title": "Learning the Koopman Eigendecomposition: A Diffeomorphic Approach",
    "abstract": "Comments: Accepted for presentation at the 2022 American Control Conference (ACC)",
    "descriptor": "\nComments: Accepted for presentation at the 2022 American Control Conference (ACC)\n",
    "authors": [
      "Petar Bevanda",
      "Johannes Kirmayr",
      "Stefan Sosnowski",
      "Sandra Hirche"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.07786"
  },
  {
    "id": "arXiv:2110.11498",
    "title": "Security-Constrained Unit Commitment Con-sidering Locational Frequency  Stability in Low-Inertia Power Grids",
    "abstract": "Security-Constrained Unit Commitment Con-sidering Locational Frequency  Stability in Low-Inertia Power Grids",
    "descriptor": "",
    "authors": [
      "Mingjian Tuo",
      "Xingpeng Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.11498"
  },
  {
    "id": "arXiv:2110.12379",
    "title": "Variational quantum algorithm for Gaussian discrete solitons and their  boson sampling",
    "abstract": "Comments: Largely revised and detailed version with 21 figures and 20 pages",
    "descriptor": "\nComments: Largely revised and detailed version with 21 figures and 20 pages\n",
    "authors": [
      "Claudio Conti"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2110.12379"
  },
  {
    "id": "arXiv:2110.13799",
    "title": "Hinge Policy Optimization: Reinterpreting PPO-Clip and Attaining Global  Optimality",
    "abstract": "Comments: 28 pages, 1 figure",
    "descriptor": "\nComments: 28 pages, 1 figure\n",
    "authors": [
      "Nai-Chieh Huang",
      "Ping-Chun Hsieh",
      "Kuo-Hao Ho",
      "Hsuan-Yu Yao",
      "Kai-Chun Hu",
      "Liang-Chun Ouyang",
      "I-Chen Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13799"
  },
  {
    "id": "arXiv:2110.14047",
    "title": "Bounding the Distance to Unsafe Sets with Convex Optimization",
    "abstract": "Comments: 28 pages, 16 figures",
    "descriptor": "\nComments: 28 pages, 16 figures\n",
    "authors": [
      "Jared Miller",
      "Mario Sznaier"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14047"
  },
  {
    "id": "arXiv:2111.00043",
    "title": "Multivariate soft rank via entropic optimal transport: sample efficiency  and generative modeling",
    "abstract": "Comments: 43 pages, 10 figures. Replacement note: Title change, author changes, new theoretical results, revised and expanded experimental evaluations",
    "descriptor": "\nComments: 43 pages, 10 figures. Replacement note: Title change, author changes, new theoretical results, revised and expanded experimental evaluations\n",
    "authors": [
      "Shoaib Bin Masud",
      "Matthew Werenski",
      "James M. Murphy",
      "Shuchin Aeron"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.00043"
  },
  {
    "id": "arXiv:2111.00272",
    "title": "A Framework for Transforming Specifications in Reinforcement Learning",
    "abstract": "A Framework for Transforming Specifications in Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Rajeev Alur",
      "Suguman Bansal",
      "Osbert Bastani",
      "Kishor Jothimurugan"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2111.00272"
  },
  {
    "id": "arXiv:2111.02008",
    "title": "Deterministic Min-cut in Poly-logarithmic Max-flows",
    "abstract": "Comments: Updated version of FOCS 2020 paper",
    "descriptor": "\nComments: Updated version of FOCS 2020 paper\n",
    "authors": [
      "Jason Li",
      "Debmalya Panigrahi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.02008"
  },
  {
    "id": "arXiv:2111.03253",
    "title": "Dynamic Data Augmentation with Gating Networks for Time Series  Recognition",
    "abstract": "Comments: Accepted to ICPR2022",
    "descriptor": "\nComments: Accepted to ICPR2022\n",
    "authors": [
      "Daisuke Oba",
      "Shinnosuke Matsuo",
      "Brian Kenji Iwana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.03253"
  },
  {
    "id": "arXiv:2111.04027",
    "title": "Riesz transform associated with the fractional Fourier transform and  applications in image edge detection",
    "abstract": "Comments: 29 pages",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Zunwei Fu",
      "Loukas Grafakos",
      "Yan Lin",
      "Yue Wu",
      "Shuhui Yang"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2111.04027"
  },
  {
    "id": "arXiv:2111.05301",
    "title": "Adaptable Register File Organization for Vector Processors",
    "abstract": "Comments: 28th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2022)",
    "descriptor": "\nComments: 28th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2022)\n",
    "authors": [
      "Crist\u00f3bal Ram\u00edrez Lazo",
      "Enrico Reggiani",
      "Carlos Rojas Morales",
      "Roger Figueras Bagu\u00e9",
      "Luis Alfonso Villa Vargas",
      "Marco Antonio Ram\u00edrez Salinas",
      "Mateo Valero Cort\u00e9s",
      "Osman Sabri Unsal",
      "Adri\u00e1n Cristal"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2111.05301"
  },
  {
    "id": "arXiv:2111.07093",
    "title": "AttacKG: Constructing Technique Knowledge Graph from Cyber Threat  Intelligence Reports",
    "abstract": "AttacKG: Constructing Technique Knowledge Graph from Cyber Threat  Intelligence Reports",
    "descriptor": "",
    "authors": [
      "Zhenyuan Li",
      "Jun Zeng",
      "Yan Chen",
      "Zhenkai Liang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2111.07093"
  },
  {
    "id": "arXiv:2111.07990",
    "title": "Fast First-Order Methods for Monotone Strongly DR-Submodular  Maximization",
    "abstract": "Comments: Major revisions (compared to the previous arXiv version) such as proposing a new algorithm (the SDRFW algorithm) and new experiments",
    "descriptor": "\nComments: Major revisions (compared to the previous arXiv version) such as proposing a new algorithm (the SDRFW algorithm) and new experiments\n",
    "authors": [
      "Omid Sadeghi",
      "Maryam Fazel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.07990"
  },
  {
    "id": "arXiv:2111.08861",
    "title": "A label efficient two-sample test",
    "abstract": "Comments: Accepted to the 38th conference on Uncertainty in Artificial Intelligence (UAI2022)",
    "descriptor": "\nComments: Accepted to the 38th conference on Uncertainty in Artificial Intelligence (UAI2022)\n",
    "authors": [
      "Weizhi Li",
      "Gautam Dasarathy",
      "Karthikeyan Natesan Ramamurthy",
      "Visar Berisha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.08861"
  },
  {
    "id": "arXiv:2111.10126",
    "title": "On the Download Rate of Homomorphic Secret Sharing",
    "abstract": "On the Download Rate of Homomorphic Secret Sharing",
    "descriptor": "",
    "authors": [
      "Ingerid Fosli",
      "Yuval Ishai",
      "Victor I. Kolobov",
      "Mary Wootters"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2111.10126"
  },
  {
    "id": "arXiv:2111.12221",
    "title": "Source-free unsupervised domain adaptation for cross-modality abdominal  multi-organ segmentation",
    "abstract": "Source-free unsupervised domain adaptation for cross-modality abdominal  multi-organ segmentation",
    "descriptor": "",
    "authors": [
      "Jin Hong",
      "Yu-Dong Zhang",
      "Weitian Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.12221"
  },
  {
    "id": "arXiv:2111.13879",
    "title": "ML-based Handover Prediction and AP Selection in Cognitive Wi-Fi  Networks",
    "abstract": "ML-based Handover Prediction and AP Selection in Cognitive Wi-Fi  Networks",
    "descriptor": "",
    "authors": [
      "Muhammad Asif Khan",
      "Ridha Hamila",
      "Adel Gastli",
      "Serkan Kiranyaz",
      "Nasser Ahmed Al-Emadi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.13879"
  },
  {
    "id": "arXiv:2111.14467",
    "title": "What Drives Readership? An Online Study on User Interface Types and  Popularity Bias Mitigation in News Article Recommendations",
    "abstract": "Comments: ECIR 2022 Conference",
    "descriptor": "\nComments: ECIR 2022 Conference\n",
    "authors": [
      "Emanuel Lacic",
      "Leon Fadljevic",
      "Franz Weissenboeck",
      "Stefanie Lindstaedt",
      "Dominik Kowald"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2111.14467"
  },
  {
    "id": "arXiv:2111.15246",
    "title": "Hallucinated Neural Radiance Fields in the Wild",
    "abstract": "Comments: Accepted by CVPR 2022. Project website: this https URL",
    "descriptor": "\nComments: Accepted by CVPR 2022. Project website: this https URL\n",
    "authors": [
      "Xingyu Chen",
      "Qi Zhang",
      "Xiaoyu Li",
      "Yue Chen",
      "Ying Feng",
      "Xuan Wang",
      "Jue Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.15246"
  },
  {
    "id": "arXiv:2112.00510",
    "title": "Trimap-guided Feature Mining and Fusion Network for Natural Image  Matting",
    "abstract": "Comments: submitted to Computer Vision and Image Understanding",
    "descriptor": "\nComments: submitted to Computer Vision and Image Understanding\n",
    "authors": [
      "Weihao Jiang",
      "Dongdong Yu",
      "Zhaozhi Xie",
      "Yaoyi Li",
      "Zehuan Yuan",
      "Hongtao Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00510"
  },
  {
    "id": "arXiv:2112.00980",
    "title": "Trap of Feature Diversity in the Learning of MLPs",
    "abstract": "Trap of Feature Diversity in the Learning of MLPs",
    "descriptor": "",
    "authors": [
      "Dongrui Liu",
      "Shaobo Wang",
      "Jie Ren",
      "Kangrui Wang",
      "Sheng Yin",
      "Huiqi Deng",
      "Quanshi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.00980"
  },
  {
    "id": "arXiv:2112.02869",
    "title": "Deep Retinex Fusion for Adaptive Infrared and Visible Image  Super-resolution Fusion",
    "abstract": "Comments: 10 pages, 9 figures",
    "descriptor": "\nComments: 10 pages, 9 figures\n",
    "authors": [
      "Yuanjie Gu",
      "Zhibo Xiao",
      "Hailun Wang",
      "Cheng Liu",
      "Shouyu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2112.02869"
  },
  {
    "id": "arXiv:2112.04085",
    "title": "Diffeomorphically Learning Stable Koopman Operators",
    "abstract": "Comments: Revised version submitted to IEEE Control Systems Letters (L-CSS) with substantially revised exposition, evaluation and proof of Lemma 2 (previously Lemma 8)",
    "descriptor": "\nComments: Revised version submitted to IEEE Control Systems Letters (L-CSS) with substantially revised exposition, evaluation and proof of Lemma 2 (previously Lemma 8)\n",
    "authors": [
      "Petar Bevanda",
      "Max Beier",
      "Sebastian Kerz",
      "Armin Lederer",
      "Stefan Sosnowski",
      "Sandra Hirche"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.04085"
  },
  {
    "id": "arXiv:2112.04355",
    "title": "COSMIC: fast closed-form identification from large-scale data for LTV  systems",
    "abstract": "COSMIC: fast closed-form identification from large-scale data for LTV  systems",
    "descriptor": "",
    "authors": [
      "Maria Carvalho",
      "Claudia Soares",
      "Pedro Louren\u00e7o",
      "Rodrigo Ventura"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.04355"
  },
  {
    "id": "arXiv:2112.05921",
    "title": "Simultaneous Localization and Mapping: Through the Lens of Nonlinear  Optimization",
    "abstract": "Comments: 22 pages",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Amay Saxena",
      "Chih-Yuan Chiu",
      "Joseph Menke",
      "Ritika Shrivastava",
      "Shankar Sastry"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.05921"
  },
  {
    "id": "arXiv:2112.06397",
    "title": "N-Cloth: Predicting 3D Cloth Deformation with Mesh-Based Networks",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Yudi Li",
      "Min Tang",
      "Yun Yang",
      "Zi Huang",
      "Ruofeng Tong",
      "Shuangcai Yang",
      "Yao Li",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.06397"
  },
  {
    "id": "arXiv:2112.07823",
    "title": "Bayesian Graph Contrastive Learning",
    "abstract": "Bayesian Graph Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Arman Hasanzadeh",
      "Mohammadreza Armandpour",
      "Ehsan Hajiramezanali",
      "Mingyuan Zhou",
      "Nick Duffield",
      "Krishna Narayanan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.07823"
  },
  {
    "id": "arXiv:2112.07918",
    "title": "M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural  Architecture Search",
    "abstract": "M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural  Architecture Search",
    "descriptor": "",
    "authors": [
      "Junjun Wu",
      "Huiyu Kuang",
      "Qinghua Lu",
      "Zeqin Lin",
      "Qingwu Shi",
      "Xilin Liu",
      "Xiaoman Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.07918"
  },
  {
    "id": "arXiv:2112.08693",
    "title": "Helmholtz equation and non-singular boundary elements applied to  multi-disciplinary physical problems",
    "abstract": "Helmholtz equation and non-singular boundary elements applied to  multi-disciplinary physical problems",
    "descriptor": "",
    "authors": [
      "Evert Klaseboer",
      "Qiang Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Physics (physics.class-ph)",
      "Computational Physics (physics.comp-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2112.08693"
  },
  {
    "id": "arXiv:2112.08696",
    "title": "Few-Shot Semantic Parsing with Language Models Trained On Code",
    "abstract": "Comments: NAACL 2022",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Richard Shin",
      "Benjamin Van Durme"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08696"
  },
  {
    "id": "arXiv:2112.08866",
    "title": "Detecting Model Misspecification in Amortized Bayesian Inference with  Neural Networks",
    "abstract": "Detecting Model Misspecification in Amortized Bayesian Inference with  Neural Networks",
    "descriptor": "",
    "authors": [
      "Marvin Schmitt",
      "Paul-Christian B\u00fcrkner",
      "Ullrich K\u00f6the",
      "Stefan T. Radev"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.08866"
  },
  {
    "id": "arXiv:2112.09118",
    "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "abstract": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Gautier Izacard",
      "Mathilde Caron",
      "Lucas Hosseini",
      "Sebastian Riedel",
      "Piotr Bojanowski",
      "Armand Joulin",
      "Edouard Grave"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.09118"
  },
  {
    "id": "arXiv:2112.09329",
    "title": "Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion  Cylinders",
    "abstract": "Comments: CVPR 2022",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Mikaela Angelina Uy",
      "Yen-yu Chang",
      "Minhyuk Sung",
      "Purvi Goel",
      "Joseph Lambourne",
      "Tolga Birdal",
      "Leonidas Guibas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.09329"
  },
  {
    "id": "arXiv:2112.11136",
    "title": "Adversarial Gradient Driven Exploration for Deep Click-Through Rate  Prediction",
    "abstract": "Comments: This paper is accepted by the KDD2022",
    "descriptor": "\nComments: This paper is accepted by the KDD2022\n",
    "authors": [
      "Kailun Wu",
      "Zhangming Chan",
      "Weijie Bian",
      "Lejian Ren",
      "Shiming Xiang",
      "Shuguang Han",
      "Hongbo Deng",
      "Bo Zheng"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.11136"
  },
  {
    "id": "arXiv:2112.14457",
    "title": "Stochastic dynamic matching: A mixed graph-theory and linear-algebra  approach",
    "abstract": "Stochastic dynamic matching: A mixed graph-theory and linear-algebra  approach",
    "descriptor": "",
    "authors": [
      "C\u00e9line Comte",
      "Fabien Mathieu",
      "Ana Bu\u0161i\u0107"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2112.14457"
  },
  {
    "id": "arXiv:2112.14726",
    "title": "Uniqueness Theorems for Tomographic Phase Retrieval with Few Diffraction  Patterns",
    "abstract": "Uniqueness Theorems for Tomographic Phase Retrieval with Few Diffraction  Patterns",
    "descriptor": "",
    "authors": [
      "Albert Fannjiang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2112.14726"
  },
  {
    "id": "arXiv:2201.01134",
    "title": "Network Collaborator: Knowledge Transfer Between Network Reconstruction  and Community Detection from Dynamics",
    "abstract": "Comments: Submit to IEEE Computational Intelligence Magazine",
    "descriptor": "\nComments: Submit to IEEE Computational Intelligence Magazine\n",
    "authors": [
      "Kai Wu",
      "Chao Wang",
      "Junyuan Chen",
      "Jing Liu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.01134"
  },
  {
    "id": "arXiv:2201.01810",
    "title": "Privacy-Friendly Peer-to-Peer Energy Trading: A Game Theoretical  Approach",
    "abstract": "Comments: To be published in IEEE Power & Energy Society General Meeting (GM), 2022",
    "descriptor": "\nComments: To be published in IEEE Power & Energy Society General Meeting (GM), 2022\n",
    "authors": [
      "Kamil Erdayandi",
      "Amrit Paudel",
      "Lucas Cordeiro",
      "Mustafa A. Mustafa"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.01810"
  },
  {
    "id": "arXiv:2201.01893",
    "title": "Flow-Guided Sparse Transformer for Video Deblurring",
    "abstract": "Comments: ICML 2022; The First Transformer-based method for Video Deblurring",
    "descriptor": "\nComments: ICML 2022; The First Transformer-based method for Video Deblurring\n",
    "authors": [
      "Jing Lin",
      "Yuanhao Cai",
      "Xiaowan Hu",
      "Haoqian Wang",
      "Youliang Yan",
      "Xueyi Zou",
      "Henghui Ding",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.01893"
  },
  {
    "id": "arXiv:2201.02374",
    "title": "As-Continuous-As-Possible Extrusion Fabrication of Surface Models",
    "abstract": "Comments: 16 pages, 23 figures",
    "descriptor": "\nComments: 16 pages, 23 figures\n",
    "authors": [
      "Fanchao Zhong",
      "Yonglai Xu",
      "Haisen Zhao",
      "Lin Lu"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2201.02374"
  },
  {
    "id": "arXiv:2201.05506",
    "title": "Geometry of Dependency Equilibria",
    "abstract": "Comments: 20 pages, to appear in Rendiconti dell'Istituto di Matematica dell'Universit\\`a di Trieste",
    "descriptor": "\nComments: 20 pages, to appear in Rendiconti dell'Istituto di Matematica dell'Universit\\`a di Trieste\n",
    "authors": [
      "Irem Portakal",
      "Bernd Sturmfels"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.05506"
  },
  {
    "id": "arXiv:2201.06274",
    "title": "Detecting danger in gridworlds using Gromov's Link Condition",
    "abstract": "Comments: 17 pages, 12 figures, 4 appendices; some parts rewritten and rearranged to improve exposition, no changes to mathematical content",
    "descriptor": "\nComments: 17 pages, 12 figures, 4 appendices; some parts rewritten and rearranged to improve exposition, no changes to mathematical content\n",
    "authors": [
      "Thomas F Burns",
      "Robert Tang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Combinatorics (math.CO)",
      "Geometric Topology (math.GT)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2201.06274"
  },
  {
    "id": "arXiv:2201.08133",
    "title": "CoAvoid: Secure, Privacy-Preserved Tracing of Contacts for Infectious  Diseases",
    "abstract": "CoAvoid: Secure, Privacy-Preserved Tracing of Contacts for Infectious  Diseases",
    "descriptor": "",
    "authors": [
      "Teng Li",
      "Siwei Yin",
      "Runze Yu",
      "Yebo Feng",
      "Lei Jiao",
      "Yulong Shen",
      "Jianfeng Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.08133"
  },
  {
    "id": "arXiv:2201.08619",
    "title": "Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object  Detectors in the Physical World",
    "abstract": "Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object  Detectors in the Physical World",
    "descriptor": "",
    "authors": [
      "Hua Ma",
      "Yinshan Li",
      "Yansong Gao",
      "Alsharif Abuadbba",
      "Zhi Zhang",
      "Anmin Fu",
      "Hyoungshick Kim",
      "Said F. Al-Sarawi",
      "Nepal Surya",
      "Derek Abbott"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.08619"
  },
  {
    "id": "arXiv:2201.09882",
    "title": "GlobalWalk: Learning Global-aware Node Embeddings via Biased Sampling",
    "abstract": "GlobalWalk: Learning Global-aware Node Embeddings via Biased Sampling",
    "descriptor": "",
    "authors": [
      "Zhengrong Xue",
      "Ziao Guo",
      "Yiwei Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.09882"
  },
  {
    "id": "arXiv:2201.10185",
    "title": "Zero-Shot Sketch Based Image Retrieval using Graph Transformer",
    "abstract": "Comments: Accepted at ICPR 2022",
    "descriptor": "\nComments: Accepted at ICPR 2022\n",
    "authors": [
      "Sumrit Gupta",
      "Ushasi Chaudhuri",
      "Biplab Banerjee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.10185"
  },
  {
    "id": "arXiv:2201.10547",
    "title": "Dynamic Thresholding for Online Distributed Data Selection",
    "abstract": "Dynamic Thresholding for Online Distributed Data Selection",
    "descriptor": "",
    "authors": [
      "Mariel A. Werner",
      "Anastasios Angelopoulos",
      "Stephen Bates",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.10547"
  },
  {
    "id": "arXiv:2201.10700",
    "title": "Deep Image Deblurring: A Survey",
    "abstract": "Comments: To appear in International Journal of Computer Vision (IJCV)",
    "descriptor": "\nComments: To appear in International Journal of Computer Vision (IJCV)\n",
    "authors": [
      "Kaihao Zhang",
      "Wenqi Ren",
      "Wenhan Luo",
      "Wei-Sheng Lai",
      "Bjorn Stenger",
      "Ming-Hsuan Yang",
      "Hongdong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10700"
  },
  {
    "id": "arXiv:2201.10918",
    "title": "Behavior Tree-Based Task Planning for Multiple Mobile Robots using a  Data Distribution Service",
    "abstract": "Comments: 8 pages, 12 figures",
    "descriptor": "\nComments: 8 pages, 12 figures\n",
    "authors": [
      "Seungwoo Jeong",
      "Taekwon Ga",
      "Inhwan Jeong",
      "Jongeun Choi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.10918"
  },
  {
    "id": "arXiv:2201.10953",
    "title": "Dual-Tasks Siamese Transformer Framework for Building Damage Assessment",
    "abstract": "Comments: IGARSS 2022",
    "descriptor": "\nComments: IGARSS 2022\n",
    "authors": [
      "Hongruixuan Chen",
      "Edoardo Nemni",
      "Sofia Vallecorsa",
      "Xi Li",
      "Chen Wu",
      "Lars Bromley"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.10953"
  },
  {
    "id": "arXiv:2201.10963",
    "title": "Learning to Compose Diversified Prompts for Image Emotion Classification",
    "abstract": "Comments: 10 pages, 5 figures",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Sinuo Deng",
      "Lifang Wu",
      "Ge Shi",
      "Lehao Xing",
      "Meng Jian",
      "Ye Xiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2201.10963"
  },
  {
    "id": "arXiv:2201.11147",
    "title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding",
    "abstract": "Comments: Accepted by ICLR 2022",
    "descriptor": "\nComments: Accepted by ICLR 2022\n",
    "authors": [
      "Ningyu Zhang",
      "Zhen Bi",
      "Xiaozhuan Liang",
      "Siyuan Cheng",
      "Haosen Hong",
      "Shumin Deng",
      "Jiazhang Lian",
      "Qiang Zhang",
      "Huajun Chen"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11147"
  },
  {
    "id": "arXiv:2201.11489",
    "title": "The Implicit Bias of Benign Overfitting",
    "abstract": "Comments: Some minor edits",
    "descriptor": "\nComments: Some minor edits\n",
    "authors": [
      "Ohad Shamir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.11489"
  },
  {
    "id": "arXiv:2201.11613",
    "title": "Domain-Invariant Representation Learning from EEG with Private Encoders",
    "abstract": "Comments: 5 pages, 1 figure",
    "descriptor": "\nComments: 5 pages, 1 figure\n",
    "authors": [
      "David Bethge",
      "Philipp Hallgarten",
      "Tobias Grosse-Puppendahl",
      "Mohamed Kari",
      "Ralf Mikut",
      "Albrecht Schmidt",
      "Ozan \u00d6zdenizci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2201.11613"
  },
  {
    "id": "arXiv:2201.12433",
    "title": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of  Graph Convolutional Networks",
    "abstract": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of  Graph Convolutional Networks",
    "descriptor": "",
    "authors": [
      "Yuhang Yao",
      "Carlee Joe-Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12433"
  },
  {
    "id": "arXiv:2201.12440",
    "title": "Certifying Model Accuracy under Distribution Shifts",
    "abstract": "Certifying Model Accuracy under Distribution Shifts",
    "descriptor": "",
    "authors": [
      "Aounon Kumar",
      "Alexander Levine",
      "Tom Goldstein",
      "Soheil Feizi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12440"
  },
  {
    "id": "arXiv:2201.12558",
    "title": "The KFIoU Loss for Rotated Object Detection",
    "abstract": "Comments: 19 pages, 5 figures, 11 tables, tensorflow code: this https URL, pytorch code: this https URL",
    "descriptor": "\nComments: 19 pages, 5 figures, 11 tables, tensorflow code: this https URL, pytorch code: this https URL\n",
    "authors": [
      "Xue Yang",
      "Yue Zhou",
      "Gefan Zhang",
      "Jirui Yang",
      "Wentao Wang",
      "Junchi Yan",
      "Xiaopeng Zhang",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12558"
  },
  {
    "id": "arXiv:2201.12886",
    "title": "N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",
    "abstract": "N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",
    "descriptor": "",
    "authors": [
      "Cristian Challu",
      "Kin G. Olivares",
      "Boris N. Oreshkin",
      "Federico Garza",
      "Max Mergenthaler-Canseco",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12886"
  },
  {
    "id": "arXiv:2202.01113",
    "title": "Tailoring Gradient Methods for Differentially-Private Distributed  Optimization",
    "abstract": "Tailoring Gradient Methods for Differentially-Private Distributed  Optimization",
    "descriptor": "",
    "authors": [
      "Yongqiang Wang",
      "Angelia Nedic"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.01113"
  },
  {
    "id": "arXiv:2202.02763",
    "title": "Riemannian Score-Based Generative Modeling",
    "abstract": "Comments: 31 pages",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Valentin De Bortoli",
      "Emile Mathieu",
      "Michael Hutchinson",
      "James Thornton",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.02763"
  },
  {
    "id": "arXiv:2202.03026",
    "title": "Context Autoencoder for Self-Supervised Representation Learning",
    "abstract": "Context Autoencoder for Self-Supervised Representation Learning",
    "descriptor": "",
    "authors": [
      "Xiaokang Chen",
      "Mingyu Ding",
      "Xiaodi Wang",
      "Ying Xin",
      "Shentong Mo",
      "Yunhao Wang",
      "Shumin Han",
      "Ping Luo",
      "Gang Zeng",
      "Jingdong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.03026"
  },
  {
    "id": "arXiv:2202.04770",
    "title": "Unsupervised Time-Series Representation Learning with Iterative Bilinear  Temporal-Spectral Fusion",
    "abstract": "Comments: Accepted by ICML 2022",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Ling Yang",
      "Shenda Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.04770"
  },
  {
    "id": "arXiv:2202.04936",
    "title": "Robust Graph Representation Learning for Local Corruption Recovery",
    "abstract": "Robust Graph Representation Learning for Local Corruption Recovery",
    "descriptor": "",
    "authors": [
      "Bingxin Zhou",
      "Yuanhong Jiang",
      "Yu Guang Wang",
      "Jingwei Liang",
      "Junbin Gao",
      "Shirui Pan",
      "Xiaoqun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04936"
  },
  {
    "id": "arXiv:2202.05226",
    "title": "Deadwooding: Robust Global Pruning for Deep Neural Networks",
    "abstract": "Comments: 21 pages, 7 figures",
    "descriptor": "\nComments: 21 pages, 7 figures\n",
    "authors": [
      "Sawinder Kaur",
      "Ferdinando Fioretto",
      "Asif Salekin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05226"
  },
  {
    "id": "arXiv:2202.06239",
    "title": "Supported Policy Optimization for Offline Reinforcement Learning",
    "abstract": "Comments: Code is available at this https URL",
    "descriptor": "\nComments: Code is available at this https URL\n",
    "authors": [
      "Jialong Wu",
      "Haixu Wu",
      "Zihan Qiu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.06239"
  },
  {
    "id": "arXiv:2202.06417",
    "title": "A Contrastive Framework for Neural Text Generation",
    "abstract": "Comments: 22 pages, 8 figures, and 10 tables (v2 adds some experimental results)",
    "descriptor": "\nComments: 22 pages, 8 figures, and 10 tables (v2 adds some experimental results)\n",
    "authors": [
      "Yixuan Su",
      "Tian Lan",
      "Yan Wang",
      "Dani Yogatama",
      "Lingpeng Kong",
      "Nigel Collier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.06417"
  },
  {
    "id": "arXiv:2202.07133",
    "title": "Sim-to-Real Domain Adaptation for Lane Detection and Classification in  Autonomous Driving",
    "abstract": "Comments: Accepted by IV 2022",
    "descriptor": "\nComments: Accepted by IV 2022\n",
    "authors": [
      "Chuqing Hu",
      "Sinclair Hudson",
      "Martin Ethier",
      "Mohammad Al-Sharman",
      "Derek Rayside",
      "William Melek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.07133"
  },
  {
    "id": "arXiv:2202.07186",
    "title": "Interpolants and Explicit Definitions in Extensions of the Description  Logic EL",
    "abstract": "Interpolants and Explicit Definitions in Extensions of the Description  Logic EL",
    "descriptor": "",
    "authors": [
      "Marie Fortin",
      "Boris Konev",
      "Frank Wolter"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2202.07186"
  },
  {
    "id": "arXiv:2202.07356",
    "title": "Realistic Counterfactual Explanations with Learned Relations",
    "abstract": "Realistic Counterfactual Explanations with Learned Relations",
    "descriptor": "",
    "authors": [
      "Xintao Xiang",
      "Artem Lenskiy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.07356"
  },
  {
    "id": "arXiv:2202.07643",
    "title": "Lie Point Symmetry Data Augmentation for Neural PDE Solvers",
    "abstract": "Comments: Published at ICML 2022, Github: this https URL",
    "descriptor": "\nComments: Published at ICML 2022, Github: this https URL\n",
    "authors": [
      "Johannes Brandstetter",
      "Max Welling",
      "Daniel E. Worrall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.07643"
  },
  {
    "id": "arXiv:2202.07880",
    "title": "$\\rm{C {\\small IS}}^2$: A Simplified Commonsense Inference Evaluation  for Story Prose",
    "abstract": "Comments: Published at the Workshop on Commonsense Representation and Reasoning (CSRR) @ ACL 2022",
    "descriptor": "\nComments: Published at the Workshop on Commonsense Representation and Reasoning (CSRR) @ ACL 2022\n",
    "authors": [
      "Bryan Li",
      "Lara J. Martin",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.07880"
  },
  {
    "id": "arXiv:2202.08480",
    "title": "Structural and Semantic Contrastive Learning for Unsupervised Node  Representation Learning",
    "abstract": "Structural and Semantic Contrastive Learning for Unsupervised Node  Representation Learning",
    "descriptor": "",
    "authors": [
      "Kaize Ding",
      "Yancheng Wang",
      "Yingzhen Yang",
      "Huan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08480"
  },
  {
    "id": "arXiv:2202.08992",
    "title": "Enhanced Multi-Objective A* Using Balanced Binary Search Trees",
    "abstract": "Comments: Accepted to SoCS 2022, 11 pages, 4 figures",
    "descriptor": "\nComments: Accepted to SoCS 2022, 11 pages, 4 figures\n",
    "authors": [
      "Zhongqiang Ren",
      "Richard Zhan",
      "Sivakumar Rathinam",
      "Maxim Likhachev",
      "Howie Choset"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.08992"
  },
  {
    "id": "arXiv:2202.09892",
    "title": "Towards a Framework for Comparing the Complexity of Robotic Tasks",
    "abstract": "Towards a Framework for Comparing the Complexity of Robotic Tasks",
    "descriptor": "",
    "authors": [
      "Michelle Ho",
      "Alec Farid",
      "Anirudha Majumdar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2202.09892"
  },
  {
    "id": "arXiv:2202.10185",
    "title": "OSegNet: Operational Segmentation Network for COVID-19 Detection using  Chest X-ray Images",
    "abstract": "OSegNet: Operational Segmentation Network for COVID-19 Detection using  Chest X-ray Images",
    "descriptor": "",
    "authors": [
      "Aysen Degerli",
      "Serkan Kiranyaz",
      "Muhammad E. H. Chowdhury",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10185"
  },
  {
    "id": "arXiv:2202.10765",
    "title": "Transporters with Visual Foresight for Solving Unseen Rearrangement  Tasks",
    "abstract": "Comments: Video demos and more details can be found on \\url{this https URL}",
    "descriptor": "\nComments: Video demos and more details can be found on \\url{this https URL}\n",
    "authors": [
      "Hongtao Wu",
      "Jikai Ye",
      "Xin Meng",
      "Chris Paxton",
      "Gregory Chirikjian"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10765"
  },
  {
    "id": "arXiv:2202.11091",
    "title": "Efficient and Differentiable Conformal Prediction with General Function  Classes",
    "abstract": "Comments: Appearing at ICLR 2022",
    "descriptor": "\nComments: Appearing at ICLR 2022\n",
    "authors": [
      "Yu Bai",
      "Song Mei",
      "Huan Wang",
      "Yingbo Zhou",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.11091"
  },
  {
    "id": "arXiv:2202.12384",
    "title": "TwistSLAM: Constrained SLAM in Dynamic Environment",
    "abstract": "Comments: This work has been accepted at IEEE Robotics and Automation Letters",
    "descriptor": "\nComments: This work has been accepted at IEEE Robotics and Automation Letters\n",
    "authors": [
      "Mathieu Gonzalez",
      "Eric Marchand",
      "Amine Kacete",
      "J\u00e9r\u00f4me Royan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.12384"
  },
  {
    "id": "arXiv:2202.12570",
    "title": "Towards Learning Causal Representations from Multi-Instance Bags",
    "abstract": "Towards Learning Causal Representations from Multi-Instance Bags",
    "descriptor": "",
    "authors": [
      "Weijia Zhang",
      "Xuanhui Zhang",
      "Hanwen Deng",
      "Min-Ling Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12570"
  },
  {
    "id": "arXiv:2202.12887",
    "title": "Biological error correction codes generate fault-tolerant neural  networks",
    "abstract": "Biological error correction codes generate fault-tolerant neural  networks",
    "descriptor": "",
    "authors": [
      "Alexander Zlokapa",
      "Andrew K. Tan",
      "John M. Martyn",
      "Ila R. Fiete",
      "Max Tegmark",
      "Isaac L. Chuang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.12887"
  },
  {
    "id": "arXiv:2202.13001",
    "title": "Non-stationary Bandits and Meta-Learning with a Small Set of Optimal  Arms",
    "abstract": "Non-stationary Bandits and Meta-Learning with a Small Set of Optimal  Arms",
    "descriptor": "",
    "authors": [
      "MohammadJavad Azizi",
      "Thang Duong",
      "Yasin Abbasi-Yadkori",
      "Andr\u00e1s Gy\u00f6rgy",
      "Claire Vernade",
      "Mohammad Ghavamzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.13001"
  },
  {
    "id": "arXiv:2202.13763",
    "title": "A System Level Approach to Regret Optimal Control",
    "abstract": "Comments: Accepted at L-CSS",
    "descriptor": "\nComments: Accepted at L-CSS\n",
    "authors": [
      "Alexandre Didier",
      "Jerome Sieber",
      "Melanie N. Zeilinger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.13763"
  },
  {
    "id": "arXiv:2203.01297",
    "title": "Sparse matrix multiplication in the low-Bandwidth model",
    "abstract": "Sparse matrix multiplication in the low-Bandwidth model",
    "descriptor": "",
    "authors": [
      "Chetan Gupta",
      "Juho Hirvonen",
      "Janne Korhonen",
      "Jan Studen\u00fd",
      "Jukka Suomela"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2203.01297"
  },
  {
    "id": "arXiv:2203.01874",
    "title": "Thermodynamics-informed graph neural networks",
    "abstract": "Comments: 14 pages, 11 figures",
    "descriptor": "\nComments: 14 pages, 11 figures\n",
    "authors": [
      "Quercus Hern\u00e1ndez",
      "Alberto Bad\u00edas",
      "Francisco Chinesta",
      "El\u00edas Cueto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2203.01874"
  },
  {
    "id": "arXiv:2203.02383",
    "title": "Distributed Methods with Absolute Compression and Error Compensation",
    "abstract": "Comments: MOTOR2022; Changes in v2: some minor typos were fixed, several places were clarified; 17 pages, 1 figure",
    "descriptor": "\nComments: MOTOR2022; Changes in v2: some minor typos were fixed, several places were clarified; 17 pages, 1 figure\n",
    "authors": [
      "Marina Danilova",
      "Eduard Gorbunov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.02383"
  },
  {
    "id": "arXiv:2203.02486",
    "title": "The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set  Methods",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Thomas G. Dietterich",
      "Alexander Guyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.02486"
  },
  {
    "id": "arXiv:2203.02762",
    "title": "DrawingInStyles: Portrait Image Generation and Editing with Spatially  Conditioned StyleGAN",
    "abstract": "Comments: 14 pages, 14 figures",
    "descriptor": "\nComments: 14 pages, 14 figures\n",
    "authors": [
      "Wanchao Su",
      "Hui Ye",
      "Shu-Yu Chen",
      "Lin Gao",
      "Hongbo Fu"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.02762"
  },
  {
    "id": "arXiv:2203.03022",
    "title": "HEAR: Holistic Evaluation of Audio Representations",
    "abstract": "Comments: to appear in Proceedings of Machine Learning Research (PMLR): NeurIPS 2021 Competition Track",
    "descriptor": "\nComments: to appear in Proceedings of Machine Learning Research (PMLR): NeurIPS 2021 Competition Track\n",
    "authors": [
      "Joseph Turian",
      "Jordie Shier",
      "Humair Raj Khan",
      "Bhiksha Raj",
      "Bj\u00f6rn W. Schuller",
      "Christian J. Steinmetz",
      "Colin Malloy",
      "George Tzanetakis",
      "Gissel Velarde",
      "Kirk McNally",
      "Max Henry",
      "Nicolas Pinto",
      "Camille Noufi",
      "Christian Clough",
      "Dorien Herremans",
      "Eduardo Fonseca",
      "Jesse Engel",
      "Justin Salamon",
      "Philippe Esling",
      "Pranay Manocha",
      "Shinji Watanabe",
      "Zeyu Jin",
      "Yonatan Bisk"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.03022"
  },
  {
    "id": "arXiv:2203.03535",
    "title": "Influencing Long-Term Behavior in Multiagent Reinforcement Learning",
    "abstract": "Comments: Spotlight presentation at the Gamification and Multiagent Solutions Workshop (ICLR 2022); Under review as a conference paper",
    "descriptor": "\nComments: Spotlight presentation at the Gamification and Multiagent Solutions Workshop (ICLR 2022); Under review as a conference paper\n",
    "authors": [
      "Dong-Ki Kim",
      "Matthew Riemer",
      "Miao Liu",
      "Jakob N. Foerster",
      "Michael Everett",
      "Chuangchuang Sun",
      "Gerald Tesauro",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2203.03535"
  },
  {
    "id": "arXiv:2203.03815",
    "title": "Online Target Localization using Adaptive Belief Propagation in the HMM  Framework",
    "abstract": "Online Target Localization using Adaptive Belief Propagation in the HMM  Framework",
    "descriptor": "",
    "authors": [
      "Min-Won Seo",
      "Solmaz S. Kia"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.03815"
  },
  {
    "id": "arXiv:2203.04192",
    "title": "Reward-Biased Maximum Likelihood Estimation for Neural Contextual  Bandits",
    "abstract": "Reward-Biased Maximum Likelihood Estimation for Neural Contextual  Bandits",
    "descriptor": "",
    "authors": [
      "Yu-Heng Hung",
      "Ping-Chun Hsieh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.04192"
  },
  {
    "id": "arXiv:2203.05788",
    "title": "CBlockSim: A Modular High-Performance Blockchain Simulator",
    "abstract": "CBlockSim: A Modular High-Performance Blockchain Simulator",
    "descriptor": "",
    "authors": [
      "Xuyang Ma",
      "Han Wu",
      "Du Xu",
      "Katinka Wolter"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2203.05788"
  },
  {
    "id": "arXiv:2203.06925",
    "title": "WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named  Entity Recognition",
    "abstract": "WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named  Entity Recognition",
    "descriptor": "",
    "authors": [
      "Renjie Zhou",
      "Qiang Hu",
      "Jian Wan",
      "Jilin Zhang",
      "Qiang Liu",
      "Tianxiang Hu",
      "Jianjun Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.06925"
  },
  {
    "id": "arXiv:2203.09023",
    "title": "Measuring Consumer Perceived Warm-Glow for Technology Adoption Modeling",
    "abstract": "Measuring Consumer Perceived Warm-Glow for Technology Adoption Modeling",
    "descriptor": "",
    "authors": [
      "Antonios Saravanos",
      "Dongnanzi Zheng",
      "Stavros Zervoudakis"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2203.09023"
  },
  {
    "id": "arXiv:2203.09081",
    "title": "Do We Really Need a Learnable Classifier at the End of Deep Neural  Network?",
    "abstract": "Do We Really Need a Learnable Classifier at the End of Deep Neural  Network?",
    "descriptor": "",
    "authors": [
      "Yibo Yang",
      "Shixiang Chen",
      "Xiangtai Li",
      "Liang Xie",
      "Zhouchen Lin",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.09081"
  },
  {
    "id": "arXiv:2203.09481",
    "title": "Diffusion Probabilistic Modeling for Video Generation",
    "abstract": "Diffusion Probabilistic Modeling for Video Generation",
    "descriptor": "",
    "authors": [
      "Ruihan Yang",
      "Prakhar Srivastava",
      "Stephan Mandt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.09481"
  },
  {
    "id": "arXiv:2203.10078",
    "title": "Bayesian Inversion for Nonlinear Imaging Models using Deep Generative  Priors",
    "abstract": "Bayesian Inversion for Nonlinear Imaging Models using Deep Generative  Priors",
    "descriptor": "",
    "authors": [
      "Pakshal Bohra",
      "Thanh-an Pham",
      "Jonathan Dong",
      "Michael Unser"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.10078"
  },
  {
    "id": "arXiv:2203.10973",
    "title": "A Local Convergence Theory for the Stochastic Gradient Descent Method in  Non-Convex Optimization With Non-isolated Local Minima",
    "abstract": "A Local Convergence Theory for the Stochastic Gradient Descent Method in  Non-Convex Optimization With Non-isolated Local Minima",
    "descriptor": "",
    "authors": [
      "Taehee Ko",
      "Xiantao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.10973"
  },
  {
    "id": "arXiv:2203.11116",
    "title": "Collective Decision Making using Attractive and Repulsive Forces in  Markovian Opinion Dynamics",
    "abstract": "Comments: Revised version of our original submission. Major changes include a new example application throughout the paper, which consists of a Yield/Go state traffic intersection problem, a reformulation of the repulsive force function, an updated derivation of the marginalized model and a results section that considers the Yield/Go intersection problem",
    "descriptor": "\nComments: Revised version of our original submission. Major changes include a new example application throughout the paper, which consists of a Yield/Go state traffic intersection problem, a reformulation of the repulsive force function, an updated derivation of the marginalized model and a results section that considers the Yield/Go intersection problem\n",
    "authors": [
      "Carl-Johan Heiker",
      "Paolo Falcone"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2203.11116"
  },
  {
    "id": "arXiv:2203.11768",
    "title": "Sustainable Development Goal Target Interactions in the Philippines: A  Two-Method Approach",
    "abstract": "Sustainable Development Goal Target Interactions in the Philippines: A  Two-Method Approach",
    "descriptor": "",
    "authors": [
      "Vena Pearl Bongolan",
      "Spencer C. Soria",
      "Roselle Leah K. Rivera"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2203.11768"
  },
  {
    "id": "arXiv:2203.12023",
    "title": "Generative Modeling Helps Weak Supervision (and Vice Versa)",
    "abstract": "Generative Modeling Helps Weak Supervision (and Vice Versa)",
    "descriptor": "",
    "authors": [
      "Benedikt Boecking",
      "Nicholas Roberts",
      "Willie Neiswanger",
      "Stefano Ermon",
      "Frederic Sala",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.12023"
  },
  {
    "id": "arXiv:2203.12136",
    "title": "Wasserstein Distributionally Robust Optimization with Wasserstein  Barycenters",
    "abstract": "Wasserstein Distributionally Robust Optimization with Wasserstein  Barycenters",
    "descriptor": "",
    "authors": [
      "Tim Tsz-Kit Lau",
      "Han Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2203.12136"
  },
  {
    "id": "arXiv:2203.12997",
    "title": "Hierarchical Nearest Neighbor Graph Embedding for Efficient  Dimensionality Reduction",
    "abstract": "Comments: CVPR 2022",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "M. Saquib Sarfraz",
      "Marios Koulakis",
      "Constantin Seibold",
      "Rainer Stiefelhagen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2203.12997"
  },
  {
    "id": "arXiv:2203.13203",
    "title": "Constrained Parameter Inference as a Principle for Learning",
    "abstract": "Constrained Parameter Inference as a Principle for Learning",
    "descriptor": "",
    "authors": [
      "Nasir Ahmad",
      "Ellen Schrader",
      "Marcel van Gerven"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2203.13203"
  },
  {
    "id": "arXiv:2203.13310",
    "title": "MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection",
    "abstract": "MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection",
    "descriptor": "",
    "authors": [
      "Renrui Zhang",
      "Han Qiu",
      "Tai Wang",
      "Ziyu Guo",
      "Xuanzhuo Xu",
      "Yu Qiao",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2203.13310"
  },
  {
    "id": "arXiv:2203.13655",
    "title": "Gransformer: Transformer-based Graph Generation",
    "abstract": "Gransformer: Transformer-based Graph Generation",
    "descriptor": "",
    "authors": [
      "Ahmad Khajenezhad",
      "Seyed Ali Osia",
      "Mahmood Karimian",
      "Hamid Beigy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.13655"
  },
  {
    "id": "arXiv:2203.15177",
    "title": "Min-Max Similarity: A Contrastive Learning Based Semi-Supervised  Learning Network for Surgical Tools Segmentation",
    "abstract": "Min-Max Similarity: A Contrastive Learning Based Semi-Supervised  Learning Network for Surgical Tools Segmentation",
    "descriptor": "",
    "authors": [
      "Ange Lou",
      "Kareem Tawfik",
      "Xing Yao",
      "Ziteng Liu",
      "Jack Noble"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2203.15177"
  },
  {
    "id": "arXiv:2203.16437",
    "title": "Weakly supervised causal representation learning",
    "abstract": "Comments: Published at the ICLR 2022 workshop on Objects, Structure and Causality",
    "descriptor": "\nComments: Published at the ICLR 2022 workshop on Objects, Structure and Causality\n",
    "authors": [
      "Johann Brehmer",
      "Pim de Haan",
      "Phillip Lippe",
      "Taco Cohen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.16437"
  },
  {
    "id": "arXiv:2204.00932",
    "title": "Automatic Transformation of Natural to Unified Modeling Language: A  Systematic Review",
    "abstract": "Comments: Accepted to 20th IEEE/ACIS International Conference on Software Engineering, Management and Applications (SERA 2022). May 25-27, 2022, Las Vegas, USA",
    "descriptor": "\nComments: Accepted to 20th IEEE/ACIS International Conference on Software Engineering, Management and Applications (SERA 2022). May 25-27, 2022, Las Vegas, USA\n",
    "authors": [
      "Sharif Ahmed",
      "Arif Ahmed",
      "Nasir U. Eisty"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2204.00932"
  },
  {
    "id": "arXiv:2204.02075",
    "title": "Complex-Valued Autoencoders for Object Discovery",
    "abstract": "Complex-Valued Autoencoders for Object Discovery",
    "descriptor": "",
    "authors": [
      "Sindy L\u00f6we",
      "Phillip Lippe",
      "Maja Rudolph",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.02075"
  },
  {
    "id": "arXiv:2204.02350",
    "title": "Information-Theoretic Policy Learning from Partial Observations with  Fully Informed Decision Makers",
    "abstract": "Information-Theoretic Policy Learning from Partial Observations with  Fully Informed Decision Makers",
    "descriptor": "",
    "authors": [
      "Tom Lefebvre"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.02350"
  },
  {
    "id": "arXiv:2204.02877",
    "title": "PAnDR: Fast Adaptation to New Environments from Offline Experiences via  Decoupling Policy and Environment Representations",
    "abstract": "Comments: Accepted on IJCAI 2022 and a previous version of this work is presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022)",
    "descriptor": "\nComments: Accepted on IJCAI 2022 and a previous version of this work is presented at the Generalizable Policy Learning in the Physical World Workshop (ICLR 2022)\n",
    "authors": [
      "Tong Sang",
      "Hongyao Tang",
      "Yi Ma",
      "Jianye Hao",
      "Yan Zheng",
      "Zhaopeng Meng",
      "Boyan Li",
      "Zhen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.02877"
  },
  {
    "id": "arXiv:2204.02980",
    "title": "Analysis of Different Losses for Deep Learning Image Colorization",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2204.02850",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2204.02850\n",
    "authors": [
      "Coloma Ballester",
      "Aur\u00e9lie Bugeau",
      "Hernan Carrillo",
      "Micha\u00ebl Cl\u00e9ment",
      "R\u00e9mi Giraud",
      "Lara Raad",
      "Patricia Vitoria"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.02980"
  },
  {
    "id": "arXiv:2204.03609",
    "title": "Pin the Memory: Learning to Generalize Semantic Segmentation",
    "abstract": "Comments: Accepted to CVPR 2022",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Jin Kim",
      "Jiyoung Lee",
      "Jungin Park",
      "Dongbo Min",
      "Kwanghoon Sohn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.03609"
  },
  {
    "id": "arXiv:2204.03768",
    "title": "Global ECG Classification by Self-Operational Neural Networks with  Feature Injection",
    "abstract": "Global ECG Classification by Self-Operational Neural Networks with  Feature Injection",
    "descriptor": "",
    "authors": [
      "Muhammad Uzair Zahid",
      "Serkan Kiranyaz",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.03768"
  },
  {
    "id": "arXiv:2204.03822",
    "title": "DiversiTree: A New Method to Efficiently Compute Diverse Sets of  Near-Optimal Solutions to Mixed-Integer Optimization Problems",
    "abstract": "Comments: 22 pages, 12 figures, submitted to INFORMS Journal on Computing",
    "descriptor": "\nComments: 22 pages, 12 figures, submitted to INFORMS Journal on Computing\n",
    "authors": [
      "Izuwa Ahanor",
      "Hugh Medal",
      "Andrew C. Trapp"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2204.03822"
  },
  {
    "id": "arXiv:2204.04245",
    "title": "Online Emotions During the Storming of the U.S. Capitol: Evidence from  the Social Media Network Parler",
    "abstract": "Online Emotions During the Storming of the U.S. Capitol: Evidence from  the Social Media Network Parler",
    "descriptor": "",
    "authors": [
      "Johannes Jakubik",
      "Michael V\u00f6ssing",
      "Dominik B\u00e4r",
      "Nicolas Pr\u00f6llochs",
      "Stefan Feuerriegel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2204.04245"
  },
  {
    "id": "arXiv:2204.06331",
    "title": "Transparent Shape from Single Polarization Images",
    "abstract": "Transparent Shape from Single Polarization Images",
    "descriptor": "",
    "authors": [
      "Mingqi Shao",
      "Chongkun Xia",
      "Zhendong Yang",
      "Junnan Huang",
      "Xueqian Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.06331"
  },
  {
    "id": "arXiv:2204.06676",
    "title": "DRAGON (Differentiable Graph Execution) : A suite of Hardware Simulation  and Optimization tools for Modern AI/Non-AI Workloads",
    "abstract": "DRAGON (Differentiable Graph Execution) : A suite of Hardware Simulation  and Optimization tools for Modern AI/Non-AI Workloads",
    "descriptor": "",
    "authors": [
      "Khushal Sethi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2204.06676"
  },
  {
    "id": "arXiv:2204.07128",
    "title": "Label Semantic Aware Pre-training for Few-shot Text Classification",
    "abstract": "Comments: Accepted at ACL 2022",
    "descriptor": "\nComments: Accepted at ACL 2022\n",
    "authors": [
      "Aaron Mueller",
      "Jason Krone",
      "Salvatore Romeo",
      "Saab Mansour",
      "Elman Mansimov",
      "Yi Zhang",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.07128"
  },
  {
    "id": "arXiv:2204.08176",
    "title": "HRCF: Enhancing Collaborative Filtering via Hyperbolic Geometric  Regularization",
    "abstract": "Comments: Proceedings of the ACM Web Conference 2022 (WWW '22); fixed some typos",
    "descriptor": "\nComments: Proceedings of the ACM Web Conference 2022 (WWW '22); fixed some typos\n",
    "authors": [
      "Menglin Yang",
      "Min Zhou",
      "Jiahong Liu",
      "Defu Lian",
      "Irwin King"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.08176"
  },
  {
    "id": "arXiv:2204.08353",
    "title": "Stability Analysis and Stabilization of Continuous-Time Linear Systems  with Distributed Delays",
    "abstract": "Comments: The contents will be constantly updated to ensure the book can be utilized as a comprehensive reference for the research on distributed delay systems",
    "descriptor": "\nComments: The contents will be constantly updated to ensure the book can be utilized as a comprehensive reference for the research on distributed delay systems\n",
    "authors": [
      "Qian Feng"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.08353"
  },
  {
    "id": "arXiv:2204.08915",
    "title": "Bots, Disinformation, and the First Trump Impeachment",
    "abstract": "Bots, Disinformation, and the First Trump Impeachment",
    "descriptor": "",
    "authors": [
      "Michael Rossetti",
      "Tauhid Zaman"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2204.08915"
  },
  {
    "id": "arXiv:2204.09410",
    "title": "Generating 3D Molecules for Target Protein Binding",
    "abstract": "Comments: Accepted by ICML 2022 as a Long Presentation",
    "descriptor": "\nComments: Accepted by ICML 2022 as a Long Presentation\n",
    "authors": [
      "Meng Liu",
      "Youzhi Luo",
      "Kanji Uchino",
      "Koji Maruhashi",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.09410"
  },
  {
    "id": "arXiv:2204.09903",
    "title": "Beyond the Prototype: Divide-and-conquer Proxies for Few-shot  Segmentation",
    "abstract": "Comments: accepted to IJCAI 2022 Long Oral",
    "descriptor": "\nComments: accepted to IJCAI 2022 Long Oral\n",
    "authors": [
      "Chunbo Lang",
      "Binfei Tu",
      "Gong Cheng",
      "Junwei Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.09903"
  },
  {
    "id": "arXiv:2204.10050",
    "title": "SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence  Embedding",
    "abstract": "Comments: Data available at this https URL and competition website at this https URL",
    "descriptor": "\nComments: Data available at this https URL and competition website at this https URL\n",
    "authors": [
      "Harish Tayyar Madabushi",
      "Edward Gow-Smith",
      "Marcos Garcia",
      "Carolina Scarton",
      "Marco Idiart",
      "Aline Villavicencio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.10050"
  },
  {
    "id": "arXiv:2204.10250",
    "title": "Lipschitz (non-)equivalence of the Gromov--Hausdorff distances,  including on ultrametric spaces",
    "abstract": "Comments: corrected the case of equal infinite cardinalities of X and \\Delta",
    "descriptor": "\nComments: corrected the case of equal infinite cardinalities of X and \\Delta\n",
    "authors": [
      "Vladyslav Oles",
      "Kevin R. Vixie"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2204.10250"
  },
  {
    "id": "arXiv:2204.11218",
    "title": "Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask  Training",
    "abstract": "Comments: Accepted by NAACL 2022",
    "descriptor": "\nComments: Accepted by NAACL 2022\n",
    "authors": [
      "Yuanxin Liu",
      "Fandong Meng",
      "Zheng Lin",
      "Peng Fu",
      "Yanan Cao",
      "Weiping Wang",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.11218"
  },
  {
    "id": "arXiv:2204.12993",
    "title": "Counterfactual harm",
    "abstract": "Comments: Included path-specific harm and expanded appendices with examples",
    "descriptor": "\nComments: Included path-specific harm and expanded appendices with examples\n",
    "authors": [
      "Jonathan G. Richens",
      "Rory Beard",
      "Daniel H. Thompson"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.12993"
  },
  {
    "id": "arXiv:2204.13791",
    "title": "Depth Estimation with Simplified Transformer",
    "abstract": "Comments: Accepted for the CVPR 2022 Transformers For Vision (T4V) workshop",
    "descriptor": "\nComments: Accepted for the CVPR 2022 Transformers For Vision (T4V) workshop\n",
    "authors": [
      "John Yang",
      "Le An",
      "Anurag Dixit",
      "Jinkyu Koo",
      "Su Inn Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13791"
  },
  {
    "id": "arXiv:2204.13902",
    "title": "Fast Sampling of Diffusion Models with Exponential Integrator",
    "abstract": "Comments: 22 pages,16 figures, Code is available at this https URL",
    "descriptor": "\nComments: 22 pages,16 figures, Code is available at this https URL\n",
    "authors": [
      "Qinsheng Zhang",
      "Yongxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13902"
  },
  {
    "id": "arXiv:2204.14095",
    "title": "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model  Pretraining",
    "abstract": "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model  Pretraining",
    "descriptor": "",
    "authors": [
      "Yuting Gao",
      "Jinfeng Liu",
      "Zihan Xu",
      "Jun Zhang",
      "Ke Li",
      "Rongrong Ji",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.14095"
  },
  {
    "id": "arXiv:2205.00074",
    "title": "Sizing Battery Energy Storage and PV System in an Extreme Fast Charging  Station Considering Uncertainties and Battery Degradation",
    "abstract": "Sizing Battery Energy Storage and PV System in an Extreme Fast Charging  Station Considering Uncertainties and Battery Degradation",
    "descriptor": "",
    "authors": [
      "Waqas ur Rehman",
      "Rui Bo",
      "Hossein Mehdipourpicha",
      "Jonathan Kimball"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.00074"
  },
  {
    "id": "arXiv:2205.00844",
    "title": "AFD Types Sparse Representations vs. the Karhunen-Loeve Expansion for  Decomposing Stochastic Processes",
    "abstract": "AFD Types Sparse Representations vs. the Karhunen-Loeve Expansion for  Decomposing Stochastic Processes",
    "descriptor": "",
    "authors": [
      "Tao Qian",
      "Wei Qu"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Functional Analysis (math.FA)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.00844"
  },
  {
    "id": "arXiv:2205.01355",
    "title": "Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion  Networks",
    "abstract": "Comments: SIGGRAPH 22 Conference Paper",
    "descriptor": "\nComments: SIGGRAPH 22 Conference Paper\n",
    "authors": [
      "Xiaoyu Pan",
      "Jiaming Mai",
      "Xinwei Jiang",
      "Dongxue Tang",
      "Jingxiang Li",
      "Tianjia Shao",
      "Kun Zhou",
      "Xiaogang Jin",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.01355"
  },
  {
    "id": "arXiv:2205.01896",
    "title": "An Online Generalized Multiscale finite element method for heat and mass  transfer problem with artificial ground freezing",
    "abstract": "An Online Generalized Multiscale finite element method for heat and mass  transfer problem with artificial ground freezing",
    "descriptor": "",
    "authors": [
      "Denis Spiridonov",
      "Sergei Stepanov",
      "Vasil`ev Vasiliy"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.01896"
  },
  {
    "id": "arXiv:2205.02357",
    "title": "Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge  Graph Completion",
    "abstract": "Comments: Accepted by SIGIR 2022. Fix typos",
    "descriptor": "\nComments: Accepted by SIGIR 2022. Fix typos\n",
    "authors": [
      "Xiang Chen",
      "Ningyu Zhang",
      "Lei Li",
      "Shumin Deng",
      "Chuanqi Tan",
      "Changliang Xu",
      "Fei Huang",
      "Luo Si",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.02357"
  },
  {
    "id": "arXiv:2205.02553",
    "title": "Automated Imbalanced Classification via Layered Learning",
    "abstract": "Automated Imbalanced Classification via Layered Learning",
    "descriptor": "",
    "authors": [
      "Vitor Cerqueira",
      "Luis Torgo",
      "Paula Branco",
      "Colin Bellinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.02553"
  },
  {
    "id": "arXiv:2205.02708",
    "title": "Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular  Property Prediction",
    "abstract": "Comments: 19 pages, 6 figures, 5 tables, 1 algorithm",
    "descriptor": "\nComments: 19 pages, 6 figures, 5 tables, 1 algorithm\n",
    "authors": [
      "Wenlin Chen",
      "Austin Tripp",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02708"
  },
  {
    "id": "arXiv:2205.03168",
    "title": "Defending against Reconstruction Attacks through Differentially Private  Federated Learning for Classification of Heterogeneous Chest X-Ray Data",
    "abstract": "Defending against Reconstruction Attacks through Differentially Private  Federated Learning for Classification of Heterogeneous Chest X-Ray Data",
    "descriptor": "",
    "authors": [
      "Joceline Ziegler",
      "Bjarne Pfitzner",
      "Heinrich Schulz",
      "Axel Saalbach",
      "Bert Arnrich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.03168"
  },
  {
    "id": "arXiv:2205.03769",
    "title": "On the Performance Optimization of Two-way Hybrid VLC/RF based IoT  System over Cellular Spectrum",
    "abstract": "Comments: 10 pages, 7 figures",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Sutanu Ghosh",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.03769"
  },
  {
    "id": "arXiv:2205.04274",
    "title": "Detecting and Understanding Harmful Memes: A Survey",
    "abstract": "Comments: Accepted at IJCAI-ECAI 2022 (Survey Track) - Editorial Feedback Revised, 9 pages (7 main + 2 reference pages)",
    "descriptor": "\nComments: Accepted at IJCAI-ECAI 2022 (Survey Track) - Editorial Feedback Revised, 9 pages (7 main + 2 reference pages)\n",
    "authors": [
      "Shivam Sharma",
      "Firoj Alam",
      "Md. Shad Akhtar",
      "Dimitar Dimitrov",
      "Giovanni Da San Martino",
      "Hamed Firooz",
      "Alon Halevy",
      "Fabrizio Silvestri",
      "Preslav Nakov",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.04274"
  },
  {
    "id": "arXiv:2205.04747",
    "title": "Controlling Extra-Textual Attributes about Dialogue Participants -- A  Case Study of English-to-Polish Neural Machine Translation",
    "abstract": "Comments: 9 pages, 9 figures, EAMT2022 camera-ready",
    "descriptor": "\nComments: 9 pages, 9 figures, EAMT2022 camera-ready\n",
    "authors": [
      "Sebastian T. Vincent",
      "Lo\u00efc Barrault",
      "Carolina Scarton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.04747"
  },
  {
    "id": "arXiv:2205.05055",
    "title": "Data Distributional Properties Drive Emergent In-Context Learning in  Transformers",
    "abstract": "Data Distributional Properties Drive Emergent In-Context Learning in  Transformers",
    "descriptor": "",
    "authors": [
      "Stephanie C.Y. Chan",
      "Adam Santoro",
      "Andrew K. Lampinen",
      "Jane X. Wang",
      "Aaditya Singh",
      "Pierre H. Richemond",
      "Jay McClelland",
      "Felix Hill"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05055"
  },
  {
    "id": "arXiv:2205.06985",
    "title": "Generating Tips from Song Reviews: A New Dataset and Framework",
    "abstract": "Generating Tips from Song Reviews: A New Dataset and Framework",
    "descriptor": "",
    "authors": [
      "Jingya Zang",
      "Cuiyun Gao",
      "Yupan Chen",
      "Ruifeng Xu",
      "Lanjun Zhou",
      "Xuan Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.06985"
  },
  {
    "id": "arXiv:2205.07026",
    "title": "Performance Analysis of Irregular Repetition Slotted Aloha with  Multi-Cell Interference",
    "abstract": "Comments: Accepted at IEEE SPAWC 2022",
    "descriptor": "\nComments: Accepted at IEEE SPAWC 2022\n",
    "authors": [
      "Chirag Ramesh Srivatsa",
      "Chandra R. Murthy"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.07026"
  },
  {
    "id": "arXiv:2205.07384",
    "title": "Incorporating Prior Knowledge into Neural Networks through an Implicit  Composite Kernel",
    "abstract": "Comments: 17 pages, 14 figures, 1 table, submitted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: 17 pages, 14 figures, 1 table, submitted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Ziyang Jiang",
      "Tongshu Zheng",
      "David Carlson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.07384"
  },
  {
    "id": "arXiv:2205.07614",
    "title": "Rethinking Reinforcement Learning based Logic Synthesis",
    "abstract": "Comments: nine pages; one figure;",
    "descriptor": "\nComments: nine pages; one figure;\n",
    "authors": [
      "Chao Wang",
      "Chen Chen",
      "Dong Li",
      "Bin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.07614"
  },
  {
    "id": "arXiv:2205.08454",
    "title": "Dynamic Optimization Fabrics for Motion Generation",
    "abstract": "Comments: Paper submitted to IEEE T-RO on 05/12/2022",
    "descriptor": "\nComments: Paper submitted to IEEE T-RO on 05/12/2022\n",
    "authors": [
      "Max Spahn",
      "Martijn Wisse",
      "Javier Alonso-Mora"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.08454"
  },
  {
    "id": "arXiv:2205.08479",
    "title": "Opportunistic Routing in Quantum Networks",
    "abstract": "Comments: This paper is accepted in INFOCOM (IEEE Conference on Computer Communications) 2022",
    "descriptor": "\nComments: This paper is accepted in INFOCOM (IEEE Conference on Computer Communications) 2022\n",
    "authors": [
      "Ali Farahbakhsh",
      "Chen Feng"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.08479"
  },
  {
    "id": "arXiv:2205.08520",
    "title": "Measuring Plagiarism in Introductory Programming Course Assignments",
    "abstract": "Comments: Accepted at IEEE conference, the 2022 8th International Conference on Information Technology Trends (ITT), at Higher Colleges of Technology - Dubai Men's Campus on 25-26 May 2022, Dubai, United Arab Emirates",
    "descriptor": "\nComments: Accepted at IEEE conference, the 2022 8th International Conference on Information Technology Trends (ITT), at Higher Colleges of Technology - Dubai Men's Campus on 25-26 May 2022, Dubai, United Arab Emirates\n",
    "authors": [
      "Muhammad Humayoun",
      "Muhammad Adnan Hashmi",
      "Ali Hanzala Khan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.08520"
  },
  {
    "id": "arXiv:2205.08671",
    "title": "K-textures, a self-supervised hard clustering deep learning algorithm  for satellite image segmentation",
    "abstract": "Comments: 19 pages, 10 figures, submitted to Frontiers in Environmental Science, section Environmental Informatics and Remote Sensing, Research Topic: Advances in Machine Learning and Deep Learning for Monitoring Terrestrial Ecosystems",
    "descriptor": "\nComments: 19 pages, 10 figures, submitted to Frontiers in Environmental Science, section Environmental Informatics and Remote Sensing, Research Topic: Advances in Machine Learning and Deep Learning for Monitoring Terrestrial Ecosystems\n",
    "authors": [
      "Fabien H. Wagner",
      "Ricardo Dalagnol",
      "Alber H. S\u00e1nchez",
      "Mayumi C.M. Hirye",
      "Samuel Favrichon",
      "Jake H. Lee",
      "Steffen Mauceri",
      "Yan Yang",
      "Sassan Saatchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.08671"
  },
  {
    "id": "arXiv:2205.08689",
    "title": "Spatial-Temporal Interactive Dynamic Graph Convolution Network for  Traffic Forecasting",
    "abstract": "Spatial-Temporal Interactive Dynamic Graph Convolution Network for  Traffic Forecasting",
    "descriptor": "",
    "authors": [
      "Aoyu Liu",
      "Yaying Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.08689"
  },
  {
    "id": "arXiv:2205.08978",
    "title": "Fast Neural Network based Solving of Partial Differential Equations",
    "abstract": "Fast Neural Network based Solving of Partial Differential Equations",
    "descriptor": "",
    "authors": [
      "Jaroslaw Rzepecki",
      "Daniel Bates",
      "Chris Doran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.08978"
  },
  {
    "id": "arXiv:2205.09418",
    "title": "Leveraging Dynamic Objects for Relative Localization Correction in a  Connected Autonomous Vehicle Network",
    "abstract": "Comments: ISPRS congress 2022",
    "descriptor": "\nComments: ISPRS congress 2022\n",
    "authors": [
      "Yunshuang Yuan",
      "Monika Sester"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.09418"
  },
  {
    "id": "arXiv:2205.09702",
    "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency  Analysis",
    "abstract": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency  Analysis",
    "descriptor": "",
    "authors": [
      "Maciej Besta",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.09702"
  },
  {
    "id": "arXiv:2205.09853",
    "title": "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and  Interpolation",
    "abstract": "Comments: 9 pages, 4 figures, 7 tables",
    "descriptor": "\nComments: 9 pages, 4 figures, 7 tables\n",
    "authors": [
      "Vikram Voleti",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09853"
  },
  {
    "id": "arXiv:2205.09884",
    "title": "Time Series Anomaly Detection via Reinforcement Learning-Based Model  Selection",
    "abstract": "Comments: 6 pages, 3 figures, submitted to IEEE Canadian Conference on Electrical and Computer Engineering (CCECE) 2022",
    "descriptor": "\nComments: 6 pages, 3 figures, submitted to IEEE Canadian Conference on Electrical and Computer Engineering (CCECE) 2022\n",
    "authors": [
      "Jiuqi Elise Zhang",
      "Di Wu",
      "Benoit Boulet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09884"
  },
  {
    "id": "arXiv:2205.10484",
    "title": "Nuclear Norm Maximization Based Curiosity-Driven Learning",
    "abstract": "Nuclear Norm Maximization Based Curiosity-Driven Learning",
    "descriptor": "",
    "authors": [
      "Chao Chen",
      "Zijian Gao",
      "Kele Xu",
      "Sen Yang",
      "Yiying Li",
      "Bo Ding",
      "Dawei Feng",
      "Huaimin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10484"
  },
  {
    "id": "arXiv:2205.10663",
    "title": "Transformer based Generative Adversarial Network for Liver Segmentation",
    "abstract": "Transformer based Generative Adversarial Network for Liver Segmentation",
    "descriptor": "",
    "authors": [
      "Ugur Demir",
      "Zheyuan Zhang",
      "Bin Wang",
      "Matthew Antalek",
      "Elif Keles",
      "Debesh Jha",
      "Amir Borhani",
      "Daniela Ladner",
      "Ulas Bagci"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10663"
  },
  {
    "id": "arXiv:2205.10747",
    "title": "Language Models with Image Descriptors are Strong Few-Shot  Video-Language Learners",
    "abstract": "Language Models with Image Descriptors are Strong Few-Shot  Video-Language Learners",
    "descriptor": "",
    "authors": [
      "Zhenhailong Wang",
      "Manling Li",
      "Ruochen Xu",
      "Luowei Zhou",
      "Jie Lei",
      "Xudong Lin",
      "Shuohang Wang",
      "Ziyi Yang",
      "Chenguang Zhu",
      "Derek Hoiem",
      "Shih-Fu Chang",
      "Mohit Bansal",
      "Heng Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10747"
  },
  {
    "id": "arXiv:2205.10929",
    "title": "rgpdOS: GDPR Enforcement By The Operating System",
    "abstract": "rgpdOS: GDPR Enforcement By The Operating System",
    "descriptor": "",
    "authors": [
      "Alain Tchana",
      "Raphael Colin",
      "Adrien Le Berre",
      "Vincent Berger",
      "Benoit Combemale",
      "Natacha Crooks",
      "Ludovic Pailler"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10929"
  },
  {
    "id": "arXiv:2205.10995",
    "title": "From Width-Based Model Checking to Width-Based Automated Theorem Proving",
    "abstract": "From Width-Based Model Checking to Width-Based Automated Theorem Proving",
    "descriptor": "",
    "authors": [
      "Mateus de Oliveira Oliveira",
      "Farhad Vadiee"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.10995"
  },
  {
    "id": "arXiv:2205.11398",
    "title": "Fine-Grained Counting with Crowd-Sourced Supervision",
    "abstract": "Comments: In Computer Vision for Animal Behavior Tracking and Modeling Workshop at CVPR 2022. 4 pages, 3 figures",
    "descriptor": "\nComments: In Computer Vision for Animal Behavior Tracking and Modeling Workshop at CVPR 2022. 4 pages, 3 figures\n",
    "authors": [
      "Justin Kay",
      "Catherine M. Foley",
      "Tom Hart"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11398"
  },
  {
    "id": "arXiv:2205.11418",
    "title": "On non-monimial APcN permutations over finite fields of even  characteristic",
    "abstract": "Comments: We made a revised version, where Sec 4 was revised as follows: h(x) in previous version is replaced with h_alpha(x). The proof that h_alpha(x) is (A)PcN is exactly same to the case of h(x) in previous version",
    "descriptor": "\nComments: We made a revised version, where Sec 4 was revised as follows: h(x) in previous version is replaced with h_alpha(x). The proof that h_alpha(x) is (A)PcN is exactly same to the case of h(x) in previous version\n",
    "authors": [
      "Jaeseong Jeong",
      "Namhun Koo",
      "Soonhak Kwon"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11418"
  },
  {
    "id": "arXiv:2205.11461",
    "title": "Undecidability of Network Coding, Conditional Information Inequalities,  and Conditional Independence Implication",
    "abstract": "Comments: 20 pages, 8 figures",
    "descriptor": "\nComments: 20 pages, 8 figures\n",
    "authors": [
      "Cheuk Ting Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.11461"
  },
  {
    "id": "arXiv:2205.11664",
    "title": "Towards Model Generalization for Monocular 3D Object Detection",
    "abstract": "Comments: Some mistakes are raised up and we need to re-write the paper and re-order the paper structure",
    "descriptor": "\nComments: Some mistakes are raised up and we need to re-write the paper and re-order the paper structure\n",
    "authors": [
      "Zhenyu Li",
      "Zehui Chen",
      "Ang Li",
      "Liangji Fang",
      "Qinhong Jiang",
      "Xianming Liu",
      "Junjun Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11664"
  },
  {
    "id": "arXiv:2205.11736",
    "title": "Towards a Defense against Backdoor Attacks in Continual Federated  Learning",
    "abstract": "Towards a Defense against Backdoor Attacks in Continual Federated  Learning",
    "descriptor": "",
    "authors": [
      "Shuaiqi Wang",
      "Jonathan Hayase",
      "Giulia Fanti",
      "Sewoong Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11736"
  },
  {
    "id": "arXiv:2205.12268",
    "title": "Wavelet Feature Maps Compression for Image-to-Image CNNs",
    "abstract": "Wavelet Feature Maps Compression for Image-to-Image CNNs",
    "descriptor": "",
    "authors": [
      "Shahaf E. Finder",
      "Yair Zohav",
      "Maor Ashkenazi",
      "Eran Treister"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12268"
  },
  {
    "id": "arXiv:2205.12442",
    "title": "Lyapunov function approach for approximation algorithm design and  analysis: with applications in submodular maximization",
    "abstract": "Lyapunov function approach for approximation algorithm design and  analysis: with applications in submodular maximization",
    "descriptor": "",
    "authors": [
      "Donglei Du"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12442"
  },
  {
    "id": "arXiv:2205.12465",
    "title": "FBNETGEN: Task-aware GNN-based fMRI Analysis via Functional Brain  Network Generation",
    "abstract": "Comments: This paper has been accepted for presentation in MIDL 2022",
    "descriptor": "\nComments: This paper has been accepted for presentation in MIDL 2022\n",
    "authors": [
      "Xuan Kan",
      "Hejie Cui",
      "Joshua Lukemire",
      "Ying Guo",
      "Carl Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.12465"
  },
  {
    "id": "arXiv:2205.12524",
    "title": "Accelerating Diffusion Models via Early Stop of the Diffusion Process",
    "abstract": "Comments: Code is released at this https URL",
    "descriptor": "\nComments: Code is released at this https URL\n",
    "authors": [
      "Zhaoyang Lyu",
      "Xudong XU",
      "Ceyuan Yang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12524"
  },
  {
    "id": "arXiv:2205.12693",
    "title": "Contrastive Learning with Boosted Memorization",
    "abstract": "Contrastive Learning with Boosted Memorization",
    "descriptor": "",
    "authors": [
      "Zhihan Zhou",
      "Jiangchao Yao",
      "Yanfeng Wang",
      "Bo Han",
      "Ya Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12693"
  },
  {
    "id": "arXiv:2205.12986",
    "title": "Transcormer: Transformer for Sentence Scoring with Sliding Language  Modeling",
    "abstract": "Transcormer: Transformer for Sentence Scoring with Sliding Language  Modeling",
    "descriptor": "",
    "authors": [
      "Kaitao Song",
      "Yichong Leng",
      "Xu Tan",
      "Yicheng Zou",
      "Tao Qin",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12986"
  },
  {
    "id": "arXiv:2205.13060",
    "title": "Designing an Efficient End-to-end Machine Learning Pipeline for  Real-time Empty-shelf Detection",
    "abstract": "Comments: 7 figures, 3 tables, 10 pages",
    "descriptor": "\nComments: 7 figures, 3 tables, 10 pages\n",
    "authors": [
      "Dipendra Jha",
      "Ata Mahjoubfar",
      "Anupama Joshi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13060"
  },
  {
    "id": "arXiv:2205.13087",
    "title": "New Explicit Good Linear Sum-Rank-Metric Codes",
    "abstract": "Comments: 23 pages, corrected version, only closing to the GV-like bound",
    "descriptor": "\nComments: 23 pages, corrected version, only closing to the GV-like bound\n",
    "authors": [
      "Hao Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13087"
  },
  {
    "id": "arXiv:2205.13137",
    "title": "MixMIM: Mixed and Masked Image Modeling for Efficient Visual  Representation Learning",
    "abstract": "Comments: preprint. Code: this https URL",
    "descriptor": "\nComments: preprint. Code: this https URL\n",
    "authors": [
      "Jihao Liu",
      "Xin Huang",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13137"
  },
  {
    "id": "arXiv:2205.13202",
    "title": "More Recent Advances in (Hyper)Graph Partitioning",
    "abstract": "More Recent Advances in (Hyper)Graph Partitioning",
    "descriptor": "",
    "authors": [
      "\u00dcmit V. \u00c7ataly\u00fcrek",
      "Karen D. Devine",
      "Marcelo Fonseca Faraj",
      "Lars Gottesb\u00fcren",
      "Tobias Heuer",
      "Henning Meyerhenke",
      "Peter Sanders",
      "Sebastian Schlag",
      "Christian Schulz",
      "Daniel Seemaier",
      "Dorothea Wagner"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13202"
  },
  {
    "id": "arXiv:2205.13341",
    "title": "QUIC-FL: Quick Unbiased Compression for Federated Learning",
    "abstract": "QUIC-FL: Quick Unbiased Compression for Federated Learning",
    "descriptor": "",
    "authors": [
      "Ran Ben Basat",
      "Shay Vargaftik",
      "Amit Portnoy",
      "Gil Einziger",
      "Yaniv Ben-Itzhak",
      "Michael Mitzenmacher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.13341"
  },
  {
    "id": "arXiv:2205.13579",
    "title": "CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal  Assignment and Pseudo-Label Refinement",
    "abstract": "CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal  Assignment and Pseudo-Label Refinement",
    "descriptor": "",
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13579"
  },
  {
    "id": "arXiv:2205.13619",
    "title": "Fairness in Recommendation: A Survey",
    "abstract": "Comments: 36 pages, 2 figures, 1 table",
    "descriptor": "\nComments: 36 pages, 2 figures, 1 table\n",
    "authors": [
      "Yunqi Li",
      "Hanxiong Chen",
      "Shuyuan Xu",
      "Yingqiang Ge",
      "Juntao Tan",
      "Shuchang Liu",
      "Yongfeng Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13619"
  },
  {
    "id": "arXiv:2205.13697",
    "title": "FedFormer: Contextual Federation with Attention in Reinforcement  Learning",
    "abstract": "Comments: Our source code can be found at this https URL Submitted to NeurIPS2022",
    "descriptor": "\nComments: Our source code can be found at this https URL Submitted to NeurIPS2022\n",
    "authors": [
      "Liam Hebert",
      "Lukasz Golab",
      "Pascal Poupart",
      "Robin Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13697"
  },
  {
    "id": "arXiv:2205.13833",
    "title": "A Decentralised Control Strategy for Secondary Voltage Regulation",
    "abstract": "A Decentralised Control Strategy for Secondary Voltage Regulation",
    "descriptor": "",
    "authors": [
      "Jitendra Kumar Goyal",
      "Vinu Thomas",
      "Bogdan Marinescu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13833"
  },
  {
    "id": "arXiv:2205.13880",
    "title": "TraClets: Harnessing the power of computer vision for trajectory  classification",
    "abstract": "TraClets: Harnessing the power of computer vision for trajectory  classification",
    "descriptor": "",
    "authors": [
      "Ioannis Kontopoulos",
      "Antonios Makris",
      "Konstantinos Tserpes",
      "Vania Bogorny"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13880"
  },
  {
    "id": "arXiv:2205.13888",
    "title": "Workload-Aware Game-Theoretic Framework for Wireless Federated Learning",
    "abstract": "Workload-Aware Game-Theoretic Framework for Wireless Federated Learning",
    "descriptor": "",
    "authors": [
      "Jiawei Liu",
      "Guopeng Zhang",
      "Kezhi Wang",
      "Kun Yang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13888"
  },
  {
    "id": "arXiv:2205.13902",
    "title": "Sample-Efficient Optimisation with Probabilistic Transformer Surrogates",
    "abstract": "Sample-Efficient Optimisation with Probabilistic Transformer Surrogates",
    "descriptor": "",
    "authors": [
      "Alexandre Maraval",
      "Matthieu Zimmer",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "Jun Wang",
      "Haitham Bou Ammar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13902"
  },
  {
    "id": "arXiv:2205.13921",
    "title": "Federated Semi-Supervised Learning with Prototypical Networks",
    "abstract": "Federated Semi-Supervised Learning with Prototypical Networks",
    "descriptor": "",
    "authors": [
      "Woojung Kim",
      "Keondo Park",
      "Kihyuk Sohn",
      "Raphael Shu",
      "Hyung-Sin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13921"
  },
  {
    "id": "arXiv:2205.14083",
    "title": "Sharpness-Aware Training for Free",
    "abstract": "Sharpness-Aware Training for Free",
    "descriptor": "",
    "authors": [
      "Jiawei Du",
      "Daquan Zhou",
      "Jiashi Feng",
      "Vincent Y. F. Tan",
      "Joey Tianyi Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14083"
  },
  {
    "id": "arXiv:2205.14087",
    "title": "OpenCalib: A Multi-sensor Calibration Toolbox for Autonomous Driving",
    "abstract": "Comments: 16 pages, 31 figures",
    "descriptor": "\nComments: 16 pages, 31 figures\n",
    "authors": [
      "Guohang Yan",
      "Liu Zhuochun",
      "Chengjie Wang",
      "Chunlei Shi",
      "Pengjin Wei",
      "Xinyu Cai",
      "Tao Ma",
      "Zhizheng Liu",
      "Zebin Zhong",
      "Yuqian Liu",
      "Ming Zhao",
      "Zheng Ma",
      "Yikang Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14087"
  }
]