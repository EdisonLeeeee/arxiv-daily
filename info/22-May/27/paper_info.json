[
  {
    "id": "arXiv:2205.12959",
    "title": "Uniform Generalization Bound on Time and Inverse Temperature for  Gradient Descent Algorithm and its Application to Analysis of Simulated  Annealing",
    "abstract": "In this paper, we propose a novel uniform generalization bound on the time\nand inverse temperature for stochastic gradient Langevin dynamics (SGLD) in a\nnon-convex setting. While previous works derive their generalization bounds by\nuniform stability, we use Rademacher complexity to make our generalization\nbound independent of the time and inverse temperature. Using Rademacher\ncomplexity, we can reduce the problem to derive a generalization bound on the\nwhole space to that on a bounded region and therefore can remove the effect of\nthe time and inverse temperature from our generalization bound. As an\napplication of our generalization bound, an evaluation on the effectiveness of\nthe simulated annealing in a non-convex setting is also described. For the\nsample size $n$ and time $s$, we derive evaluations with orders $\\sqrt{n^{-1}\n\\log (n+1)}$ and $|(\\log)^4(s)|^{-1}$, respectively. Here, $(\\log)^4$ denotes\nthe $4$ times composition of the logarithmic function.",
    "descriptor": "",
    "authors": [
      "Keisuke Suzuki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12959"
  },
  {
    "id": "arXiv:2205.12960",
    "title": "Towards Symbolic Time Series Representation Improved by Kernel Density  Estimators",
    "abstract": "This paper deals with symbolic time series representation. It builds up on\nthe popular mapping technique Symbolic Aggregate approXimation algorithm (SAX),\nwhich is extensively utilized in sequence classification, pattern mining,\nanomaly detection, time series indexing and other data mining tasks. However,\nthe disadvantage of this method is, that it works reliably only for time series\nwith Gaussian-like distribution. In our previous work we have proposed an\nimprovement of SAX, called dwSAX, which can deal with Gaussian as well as\nnon-Gaussian data distribution. Recently we have made further progress in our\nsolution - edwSAX. Our goal was to optimally cover the information space by\nmeans of sufficient alphabet utilization; and to satisfy lower bounding\ncriterion as tight as possible. We describe here our approach, including\nevaluation on commonly employed tasks such as time series reconstruction error\nand Euclidean distance lower bounding with promising improvements over SAX.",
    "descriptor": "",
    "authors": [
      "Matej Kloska",
      "Viera Rozinajova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12960"
  },
  {
    "id": "arXiv:2205.12961",
    "title": "Towards Green AI with tensor networks -- Sustainability and innovation  enabled by efficient algorithms",
    "abstract": "The current standard to compare the performance of AI algorithms is mainly\nbased on one criterion: the model's accuracy. In this context, algorithms with\na higher accuracy (or similar measures) are considered as better. To achieve\nnew state-of-the-art results, algorithmic development is accompanied by an\nexponentially increasing amount of compute. While this has enabled AI research\nto achieve remarkable results, AI progress comes at a cost: it is\nunsustainable. In this paper, we present a promising tool for sustainable and\nthus Green AI: tensor networks (TNs). Being an established tool from\nmultilinear algebra, TNs have the capability to improve efficiency without\ncompromising accuracy. Since they can reduce compute significantly, we would\nlike to highlight their potential for Green AI. We elaborate in both a kernel\nmachine and deep learning setting how efficiency gains can be achieved with\nTNs. Furthermore, we argue that better algorithms should be evaluated in terms\nof both accuracy and efficiency. To that end, we discuss different efficiency\ncriteria and analyze efficiency in an exemplifying experimental setting for\nkernel ridge regression. With this paper, we want to raise awareness about\nGreen AI and showcase its positive impact on sustainability and AI research.\nOur key contribution is to demonstrate that TNs enable efficient algorithms and\ntherefore contribute towards Green AI. In this sense, TNs pave the way for\nbetter algorithms in AI.",
    "descriptor": "",
    "authors": [
      "Eva Memmel",
      "Clara Menzen",
      "Jetze Schuurmans",
      "Frederiek Wesel",
      "Kim Batselier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12961"
  },
  {
    "id": "arXiv:2205.12986",
    "title": "Transcormer: Transformer for Sentence Scoring with Sliding Language  Modeling",
    "abstract": "Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.",
    "descriptor": "",
    "authors": [
      "Kaitao Song",
      "Yichong Leng",
      "Xu Tan",
      "Yicheng Zou",
      "Tao Qin",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12986"
  },
  {
    "id": "arXiv:2205.12992",
    "title": "Open Arms: Open-Source Arms, Hands & Control",
    "abstract": "Open Arms is a novel open-source platform of realistic human-like robotic\nhands and arms hardware with 28 Degree-of-Freedom (DoF), designed to extend the\ncapabilities and accessibility of humanoid robotic grasping and manipulation.\nThe Open Arms framework includes an open SDK and development environment,\nsimulation tools, and application development tools to build and operate Open\nArms. This paper describes these hands controls, sensing, mechanisms, aesthetic\ndesign, and manufacturing and their real-world applications with a teleoperated\nnursing robot. From 2015 to 2022, we have designed and established the\nmanufacturing of Open Arms as a low-cost, high functionality robotic arms\nhardware and software framework to serve both humanoid robot applications and\nthe urgent demand for low-cost prosthetics. Using the techniques of consumer\nproduct manufacturing, we set out to define modular, low-cost techniques for\napproximating the dexterity and sensitivity of human hands. To demonstrate the\ndexterity and control of our hands, we present a novel Generative Grasping\nResidual CNN (GGR-CNN) model that can generate robust antipodal grasps from\ninput images of various objects at real-time speeds (22ms). We achieved\nstate-of-the-art accuracy of 92.4% using our model architecture on a standard\nCornell Grasping Dataset, which contains a diverse set of household objects.",
    "descriptor": "\nComments: Submitted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "David Hanson",
      "Alishba Imran",
      "Gerardo Morales",
      "Vytas Krisciunas",
      "Aditya Sagi",
      "Aman Malali",
      "Rushali Mohbe",
      "Raviteja Upadrashta"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12992"
  },
  {
    "id": "arXiv:2205.13001",
    "title": "Towards Diverse and Natural Scene-aware 3D Human Motion Synthesis",
    "abstract": "The ability to synthesize long-term human motion sequences in real-world\nscenes can facilitate numerous applications. Previous approaches for\nscene-aware motion synthesis are constrained by pre-defined target objects or\npositions and thus limit the diversity of human-scene interactions for\nsynthesized motions. In this paper, we focus on the problem of synthesizing\ndiverse scene-aware human motions under the guidance of target action\nsequences. To achieve this, we first decompose the diversity of scene-aware\nhuman motions into three aspects, namely interaction diversity (e.g. sitting on\ndifferent objects with different poses in the given scenes), path diversity\n(e.g. moving to the target locations following different paths), and the motion\ndiversity (e.g. having various body movements during moving). Based on this\nfactorized scheme, a hierarchical framework is proposed, with each sub-module\nresponsible for modeling one aspect. We assess the effectiveness of our\nframework on two challenging datasets for scene-aware human motion synthesis.\nThe experiment results show that the proposed framework remarkably outperforms\nprevious methods in terms of diversity and naturalness.",
    "descriptor": "",
    "authors": [
      "Jingbo Wang",
      "Yu Rong",
      "Jingyuan Liu",
      "Sijie Yan",
      "Dahua Lin",
      "Bo Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13001"
  },
  {
    "id": "arXiv:2205.13005",
    "title": "QGNN: Value Function Factorisation with Graph Neural Networks",
    "abstract": "In multi-agent reinforcement learning, the use of a global objective is a\npowerful tool for incentivising cooperation. Unfortunately, it is not\nsample-efficient to train individual agents with a global reward, because it\ndoes not necessarily correlate with an agent's individual actions. This problem\ncan be solved by factorising the global value function into local value\nfunctions. Early work in this domain performed factorisation by conditioning\nlocal value functions purely on local information. Recently, it has been shown\nthat providing both local information and an encoding of the global state can\npromote cooperative behaviour. In this paper we propose QGNN, the first value\nfactorisation method to use a graph neural network (GNN) based model. The\nmulti-layer message passing architecture of QGNN provides more representational\ncomplexity than models in prior work, allowing it to produce a more effective\nfactorisation. QGNN also introduces a permutation invariant mixer which is able\nto match the performance of other methods, even with significantly fewer\nparameters. We evaluate our method against several baselines, including\nQMIX-Att, GraphMIX, QMIX, VDN, and hybrid architectures. Our experiments\ninclude Starcraft, the standard benchmark for credit assignment; Estimate Game,\na custom environment that explicitly models inter-agent dependencies; and\nCoalition Structure Generation, a foundational problem with real-world\napplications. The results show that QGNN outperforms state-of-the-art value\nfactorisation baselines consistently.",
    "descriptor": "",
    "authors": [
      "Ryan Kortvelesy",
      "Amanda Prorok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13005"
  },
  {
    "id": "arXiv:2205.13008",
    "title": "Runtime Composition Of Systems of Interacting Cyber-Physical Components",
    "abstract": "We introduce a transition system based specification of cyber-physical\nsystems whose semantics is compositional with respect to a family of algebraic\nproducts. We give sufficient conditions for execution of a product to be\ncorrectly implemented by a lazy expansion of the product construction. The\ntransition system algebra is implemented in the Maude rewriting logic system,\nand we report a simple case study illustrating compositional specification.",
    "descriptor": "",
    "authors": [
      "Benjamin Lion",
      "Farhad Arbab",
      "Carolyn Talcott"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.13008"
  },
  {
    "id": "arXiv:2205.13011",
    "title": "Flying Hydraulically Amplified Electrostatic Gripper System for Aerial  Object Manipulation",
    "abstract": "Rapid and versatile object manipulation in air is an open challenge. An\nenergy-efficient and adaptive soft gripper combined with an agile aerial\nvehicle could revolutionize aerial robotic manipulation in areas such as\nwarehousing. This paper presents a bio-inspired gripper powered by\nhydraulically amplified electrostatic actuators mounted to a quadcopter that\ncan interact safely and naturally with its environment. Our gripping concept is\nmotivated by an eagle's talon. Our custom multi-actuator type is inspired by a\nprevious scorpion tail design (consisting of a base electrode and pouches\nstacked adjacently) and spider-inspired joints (classic pouch motors with a\nflexible hinge layer). A fusion of these two concepts realizes a higher force\noutput than single-actuator types under considerable deflections of up to\n25{\\deg}. By adding a sandwich hinge layer structure to the classic pouch motor\nconcept we improve the overall robustness of the gripper. We show, for the\nfirst time, that soft manipulation in air is possible using electrostatic\nactuation. This study demonstrates the high potential of untethered\nhydraulically amplified actuators for the future of robotic manipulation. Our\nlightweight and bio-inspired system opens up the use of hydraulic electrostatic\nactuators in aerial mobile systems.",
    "descriptor": "\nComments: 16 pages, 12 figures\n",
    "authors": [
      "Dario Tscholl",
      "Stephan-Daniel Gravert",
      "Aurel X. Appius",
      "Robert K. Katzschmann"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13011"
  },
  {
    "id": "arXiv:2205.13012",
    "title": "TSEM: Temporally Weighted Spatiotemporal Explainable Neural Network for  Multivariate Time Series",
    "abstract": "Deep learning has become a one-size-fits-all solution for technical and\nbusiness domains thanks to its flexibility and adaptability. It is implemented\nusing opaque models, which unfortunately undermines the outcome\ntrustworthiness. In order to have a better understanding of the behavior of a\nsystem, particularly one driven by time series, a look inside a deep learning\nmodel so-called posthoc eXplainable Artificial Intelligence (XAI) approaches,\nis important. There are two major types of XAI for time series data, namely\nmodel-agnostic and model-specific. Model-specific approach is considered in\nthis work. While other approaches employ either Class Activation Mapping (CAM)\nor Attention Mechanism, we merge the two strategies into a single system,\nsimply called the Temporally Weighted Spatiotemporal Explainable Neural Network\nfor Multivariate Time Series (TSEM). TSEM combines the capabilities of RNN and\nCNN models in such a way that RNN hidden units are employed as attention\nweights for the CNN feature maps temporal axis. The result shows that TSEM\noutperforms XCM. It is similar to STAM in terms of accuracy, while also\nsatisfying a number of interpretability criteria, including causality,\nfidelity, and spatiotemporality.",
    "descriptor": "",
    "authors": [
      "Anh-Duy Pham",
      "Anastassia Kuestenmacher",
      "Paul G. Ploeger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13012"
  },
  {
    "id": "arXiv:2205.13013",
    "title": "Learning Deterministic Finite Automata Decompositions from Examples and  Demonstrations",
    "abstract": "The identification of a deterministic finite automaton (DFA) from labeled\nexamples is a well-studied problem in the literature; however, prior work\nfocuses on the identification of monolithic DFAs. Although monolithic DFAs\nprovide accurate descriptions of systems' behavior, they lack simplicity and\ninterpretability; moreover, they fail to capture sub-tasks realized by the\nsystem and introduce inductive biases away from the inherent decomposition of\nthe overall task. In this paper, we present an algorithm for learning\nconjunctions of DFAs from labeled examples. Our approach extends an existing\nSAT-based method to systematically enumerate Pareto-optimal candidate\nsolutions. We highlight the utility of our approach by integrating it with a\nstate-of-the-art algorithm for learning DFAs from demonstrations. Our\nexperiments show that the algorithm learns sub-tasks realized by the labeled\nexamples, and it is scalable in the domains of interest.",
    "descriptor": "\nComments: Preprint, under review\n",
    "authors": [
      "Niklas Lauffer",
      "Beyazit Yalcinkaya",
      "Marcell Vazquez-Chanlatte",
      "Ameesh Shah",
      "Sanjit A. Seshia"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.13013"
  },
  {
    "id": "arXiv:2205.13016",
    "title": "BiT: Robustly Binarized Multi-distilled Transformer",
    "abstract": "Modern pre-trained transformers have rapidly advanced the state-of-the-art in\nmachine learning, but have also grown in parameters and computational\ncomplexity, making them increasingly difficult to deploy in\nresource-constrained environments. Binarization of the weights and activations\nof the network can significantly alleviate these issues, however is technically\nchallenging from an optimization perspective. In this work, we identify a\nseries of improvements which enables binary transformers at a much higher\naccuracy than what was possible previously. These include a two-set\nbinarization scheme, a novel elastic binary activation function with learned\nparameters, and a method to quantize a network to its limit by successively\ndistilling higher precision models into lower precision students. These\napproaches allow for the first time, fully binarized transformer models that\nare at a practical level of accuracy, approaching a full-precision BERT\nbaseline on the GLUE language understanding benchmark within as little as 5.9%.",
    "descriptor": "",
    "authors": [
      "Zechun Liu",
      "Barlas Oguz",
      "Aasish Pappu",
      "Lin Xiao",
      "Scott Yih",
      "Meng Li",
      "Raghuraman Krishnamoorthi",
      "Yashar Mehdad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13016"
  },
  {
    "id": "arXiv:2205.13018",
    "title": "On the Reliability of Computing-in-Memory Accelerators for Deep Neural  Networks",
    "abstract": "Computing-in-memory with emerging non-volatile memory (nvCiM) is shown to be\na promising candidate for accelerating deep neural networks (DNNs) with high\nenergy efficiency. However, most non-volatile memory (NVM) devices suffer from\nreliability issues, resulting in a difference between actual data involved in\nthe nvCiM computation and the weight value trained in the data center. Thus,\nmodels actually deployed on nvCiM platforms achieve lower accuracy than their\ncounterparts trained on the conventional hardware (e.g., GPUs). In this\nchapter, we first offer a brief introduction to the opportunities and\nchallenges of nvCiM DNN accelerators and then show the properties of different\ntypes of NVM devices. We then introduce the general architecture of nvCiM DNN\naccelerators. After that, we discuss the source of unreliability and how to\nefficiently model their impact. Finally, we introduce representative works that\nmitigate the impact of device variations.",
    "descriptor": "\nComments: System Dependability And Analytics, 978-3-031-02062-9, Chapter 9\n",
    "authors": [
      "Zheyu Yan",
      "Xiaobo Sharon Hu",
      "Yiyu Shi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.13018"
  },
  {
    "id": "arXiv:2205.13020",
    "title": "People counting system for retail analytics using edge AI",
    "abstract": "Developments in IoT applications are playing an important role in our\nday-to-day life, starting from business predictions to self driving cars. One\nof the area, most influenced by the field of AI and IoT is retail analytics. In\nRetail Analytics, Conversion Rates - a metric which is most often used by\nretail stores to measure how many people have visited the store and how many\npurchases has happened. This retail conversion rate assess the marketing\noperations, increasing stock, store outlet and running promotions ..etc. Our\nproject intends to build a cost-effective people counting system with AI at\nEdge, where it calculates Conversion rates using total number of people counted\nby the system and number of transactions for the day, which helps in providing\nanalytical insights for retail store optimization with a very minimum hardware\nrequirements.",
    "descriptor": "\nComments: 5 pages, 3 figures. We proposed a novel framework design (highlighted in abstract) instead of enhancing a DL model or openVINO. To demonstrate the importance of our framework, we have chosen a retail computer vision problem, people counting system and attempted to construct an end-to-end solution with our suggested framework\n",
    "authors": [
      "Karthik Reddy Kanjula",
      "Vishnu Vardhan Reddy",
      "Jnanesh K P",
      "Jeffy S Abraham",
      "Tanuja K"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13020"
  },
  {
    "id": "arXiv:2205.13022",
    "title": "Towards Using Data-Centric Approach for Better Code Representation  Learning",
    "abstract": "Despite the recent trend of creating source code models and applying them to\nsoftware engineering tasks, the quality of such models is insufficient for\nreal-world application. In this work, we focus on improving existing code\nlearning models from the data-centric perspective instead of designing new\nsource code models. We shed some light on this direction by using a so-called\ndata-influence method to identify noisy samples of pre-trained code learning\nmodels. The data-influence method is to assess the similarity of a target\nsample to the correct samples to determine whether or not such the target\nsample is noisy. The results of our evaluation show that data-influence methods\ncan identify noisy samples for the code classification and defection prediction\ntasks. We envision that the data-centric approach will be a key driver for\ndeveloping source code models that are useful in practice.",
    "descriptor": "",
    "authors": [
      "Anh Dau",
      "Thang Nguyen-Duc",
      "Hoang Thanh-Tung",
      "Nghi Bui"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.13022"
  },
  {
    "id": "arXiv:2205.13026",
    "title": "Preference Dynamics Under Personalized Recommendations",
    "abstract": "Many projects (both practical and academic) have designed algorithms to match\nusers to content they will enjoy under the assumption that user's preferences\nand opinions do not change with the content they see. Evidence suggests that\nindividuals' preferences are directly shaped by what content they see --\nradicalization, rabbit holes, polarization, and boredom are all example\nphenomena of preferences affected by content. Polarization in particular can\noccur even in ecosystems with \"mass media,\" where no personalization takes\nplace, as recently explored in a natural model of preference dynamics\nby~\\citet{hkazla2019geometric} and~\\citet{gaitonde2021polarization}. If all\nusers' preferences are drawn towards content they already like, or are repelled\nfrom content they already dislike, uniform consumption of media leads to a\npopulation of heterogeneous preferences converging towards only two poles.\nIn this work, we explore whether some phenomenon akin to polarization occurs\nwhen users receive \\emph{personalized} content recommendations. We use a\nsimilar model of preference dynamics, where an individual's preferences move\ntowards content the consume and enjoy, and away from content they consume and\ndislike. We show that standard user reward maximization is an almost trivial\ngoal in such an environment (a large class of simple algorithms will achieve\nonly constant regret). A more interesting objective, then, is to understand\nunder what conditions a recommendation algorithm can ensure stationarity of\nuser's preferences. We show how to design a content recommendations which can\nachieve approximate stationarity, under mild conditions on the set of available\ncontent, when a user's preferences are known, and how one can learn enough\nabout a user's preferences to implement such a strategy even when user\npreferences are initially unknown.",
    "descriptor": "\nComments: EC 2022\n",
    "authors": [
      "Sarah Dean",
      "Jamie Morgenstern"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13026"
  },
  {
    "id": "arXiv:2205.13028",
    "title": "Formalizing Preferences Over Runtime Distributions",
    "abstract": "When trying to solve a computational problem we are often faced with a choice\namong algorithms that are all guaranteed to return the right answer but that\ndiffer in their runtime distributions (e.g., SAT solvers, sorting algorithms).\nThis paper aims to lay theoretical foundations for such choices by formalizing\npreferences over runtime distributions. It might seem that we should simply\nprefer the algorithm that minimizes expected runtime. However, such preferences\nwould be driven by exactly how slow our algorithm is on bad inputs, whereas in\npractice we are typically willing to cut off occasional, sufficiently long runs\nbefore they finish. We propose a principled alternative, taking a\nutility-theoretic approach to characterize the scoring functions that describe\npreferences over algorithms. These functions depend on the way our value for\nsolving our problem decreases with time and on the distribution from which\ncaptimes are drawn. We describe examples of realistic utility functions and\nshow how to leverage a maximum-entropy approach for modeling underspecified\ncaptime distributions. Finally, we show how to efficiently estimate an\nalgorithm's expected utility from runtime samples.",
    "descriptor": "",
    "authors": [
      "Devon R. Graham",
      "Kevin Leyton-Brown",
      "Tim Roughgarden"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.13028"
  },
  {
    "id": "arXiv:2205.13030",
    "title": "A parameterized approximation algorithm for the Multiple Allocation  $k$-Hub Center",
    "abstract": "In the Multiple Allocation $k$-Hub Center (MA$k$HC), we are given a connected\nedge-weighted graph $G$, sets of clients $\\mathcal{C}$ and hub locations\n$\\mathcal{H}$, where ${V(G) = \\mathcal{C} \\cup \\mathcal{H}}$, a set of demands\n$\\mathcal{D} \\subseteq \\mathcal{C}^2$ and a positive integer $k$. A solution is\na set of hubs $H \\subseteq \\mathcal{H}$ of size $k$ such that every demand\n$(a,b)$ is satisfied by a path starting in $a$, going through some vertex of\n$H$, and ending in $b$. The objective is to minimize the largest length of a\npath. We show that finding a $(3-\\epsilon)$-approximation is NP-hard already\nfor planar graphs. For arbitrary graphs, the approximation lower bound holds\neven if we parameterize by $k$ and the value $r$ of an optimal solution. An\nexact FPT algorithm is also unlikely when the parameter combines $k$ and\nvarious graph widths, including pathwidth. To confront these hardness barriers,\nwe give a $(2+\\epsilon)$-approximation algorithm parameterized by treewidth,\nand, as a byproduct, for unweighted planar graphs, we give a\n$(2+\\epsilon)$-approximation algorithm parameterized by $k$ and $r$. Compared\nto classical location problems, computing the length of a path depends on\nnon-local decisions. This turns standard dynamic programming algorithms\nimpractical, thus our algorithm approximates this length using only local\ninformation. We hope these ideas find application in other problems with\nsimilar cost structure.",
    "descriptor": "",
    "authors": [
      "Marcelo P. L. Benedito",
      "Lucas P. Melo",
      "Lehilton L. C. Pedrosa"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.13030"
  },
  {
    "id": "arXiv:2205.13032",
    "title": "Diagonally implicit Runge-Kutta schemes: Discrete energy-balance laws  and compactness properties",
    "abstract": "We study diagonally implicit Runge-Kutta (DIRK) schemes when applied to\nabstract evolution problems that fit into the Gelfand-triple framework. We\nintroduce novel stability notions that are well-suited to this setting and\nprovide simple, necessary and sufficient, conditions to verify that a DIRK\nscheme is stable in our sense and in Bochner-type norms. We use several popular\nDIRK schemes in order to illustrate cases that satisfy the required structural\nstability properties and cases that do not. In addition, under some mild\nstructural conditions on the problem we can guarantee compactness of families\nof discrete solutions with respect to time discretization.",
    "descriptor": "",
    "authors": [
      "Abner J. Salgado",
      "Ignacio Tomas"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13032"
  },
  {
    "id": "arXiv:2205.13033",
    "title": "Concurrent Neural Tree and Data Preprocessing AutoML for Image  Classification",
    "abstract": "Deep Neural Networks (DNN's) are a widely-used solution for a variety of\nmachine learning problems. However, it is often necessary to invest a\nsignificant amount of a data scientist's time to pre-process input data, test\ndifferent neural network architectures, and tune hyper-parameters for optimal\nperformance. Automated machine learning (autoML) methods automatically search\nthe architecture and hyper-parameter space for optimal neural networks.\nHowever, current state-of-the-art (SOTA) methods do not include traditional\nmethods for manipulating input data as part of the algorithmic search space. We\nadapt the Evolutionary Multi-objective Algorithm Design Engine (EMADE), a\nmulti-objective evolutionary search framework for traditional machine learning\nmethods, to perform neural architecture search. We also integrate EMADE's\nsignal processing and image processing primitives. These primitives allow EMADE\nto manipulate input data before ingestion into the simultaneously evolved DNN.\nWe show that including these methods as part of the search space shows\npotential to provide benefits to performance on the CIFAR-10 image\nclassification benchmark dataset.",
    "descriptor": "\nComments: 4 pages Proceedings of the Genetic and Evolutionary Computation Conference Companion July 2021 Pages 1990 to 1993\n",
    "authors": [
      "Anish Thite",
      "Mohan Dodda",
      "Pulak Agarwal",
      "Jason Zutty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.13033"
  },
  {
    "id": "arXiv:2205.13034",
    "title": "EvoVGM: A Deep Variational Generative Model for Evolutionary Parameter  Estimation",
    "abstract": "Most evolutionary-oriented deep generative models do not explicitly consider\nthe underlying evolutionary dynamics of biological sequences as it is performed\nwithin the Bayesian phylogenetic inference framework. In this study, we propose\na method for a deep variational Bayesian generative model that jointly\napproximates the true posterior of local biological evolutionary parameters and\ngenerates sequence alignments. Moreover, it is instantiated and tuned for\ncontinuous-time Markov chain substitution models such as JC69 and GTR. We train\nthe model via a low-variance variational objective function and a gradient\nascent algorithm. Here, we show the consistency and effectiveness of the method\non synthetic sequence alignments simulated with several evolutionary scenarios\nand on a real virus sequence alignment.",
    "descriptor": "\nComments: 10 pages, 9 figures, submitted\n",
    "authors": [
      "Amine M. Remita",
      "Abdoulaye Banir\u00e9 Diallo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2205.13034"
  },
  {
    "id": "arXiv:2205.13037",
    "title": "Neuromorphic Artificial Intelligence Systems",
    "abstract": "Modern AI systems, based on von Neumann architecture and classical neural\nnetworks, have a number of fundamental limitations in comparison with the\nbrain. This article discusses such limitations and the ways they can be\nmitigated. Next, it presents an overview of currently available neuromorphic AI\nprojects in which these limitations are overcame by bringing some brain\nfeatures into the functioning and organization of computing systems (TrueNorth,\nLoihi, Tianjic, SpiNNaker, BrainScaleS, NeuronFlow, DYNAP, Akida). Also, the\narticle presents the principle of classifying neuromorphic AI systems by the\nbrain features they use (neural networks, parallelism and asynchrony, impulse\nnature of information transfer, local learning, sparsity, analog and in-memory\ncomputing). In addition to new architectural approaches used in neuromorphic\ndevices based on existing silicon microelectronics technologies, the article\nalso discusses the prospects of using new memristor element base. Examples of\nrecent advances in the use of memristors in euromorphic applications are also\ngiven.",
    "descriptor": "",
    "authors": [
      "Dmitry Ivanov",
      "Aleksandr Chezhegov",
      "Andrey Grunin",
      "Mikhail Kiselev",
      "Denis Larionov"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.13037"
  },
  {
    "id": "arXiv:2205.13038",
    "title": "Improving Subgraph Representation Learning via Multi-View Augmentation",
    "abstract": "Subgraph representation learning based on Graph Neural Network (GNN) has\nbroad applications in chemistry and biology, such as molecule property\nprediction and gene collaborative function prediction. On the other hand, graph\naugmentation techniques have shown promising results in improving graph-based\nand node-based classification tasks but are rarely explored in the GNN-based\nsubgraph representation learning literature. In this work, we developed a novel\nmultiview augmentation mechanism to improve subgraph representation learning\nand thus the accuracy of downstream prediction tasks. The augmentation\ntechnique creates multiple variants of subgraphs and embeds these variants into\nthe original graph to achieve both high training efficiency, scalability, and\nimproved accuracy. Experiments on several real-world subgraph benchmarks\ndemonstrate the superiority of our proposed multi-view augmentation techniques.",
    "descriptor": "",
    "authors": [
      "Yili Shen",
      "Jiaxu Yan",
      "Cheng-Wei Ju",
      "Jun Yi",
      "Zhou Lin",
      "Hui Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13038"
  },
  {
    "id": "arXiv:2205.13039",
    "title": "On Infinite Separations Between Simple and Optimal Mechanisms",
    "abstract": "We consider a revenue-maximizing seller with $k$ heterogeneous items for sale\nto a single additive buyer, whose values are drawn from a known, possibly\ncorrelated prior $\\mathcal{D}$. It is known that there exist priors\n$\\mathcal{D}$ such that simple mechanisms -- those with bounded menu complexity\n-- extract an arbitrarily small fraction of the optimal revenue. This paper\nconsiders the opposite direction: given a correlated distribution $\\mathcal{D}$\nwitnessing an infinite separation between simple and optimal mechanisms, what\ncan be said about $\\mathcal{D}$?\nPrevious work provides a framework for constructing such $\\mathcal{D}$: it\ntakes as input a sequence of $k$-dimensional vectors satisfying some geometric\nproperty, and produces a $\\mathcal{D}$ witnessing an infinite gap. Our first\nmain result establishes that this framework is without loss: every\n$\\mathcal{D}$ witnessing an infinite separation could have resulted from this\nframework. Even earlier work provided a more streamlined framework. Our second\nmain result establishes that this restrictive framework is not tight. That is,\nwe provide an instance $\\mathcal{D}$ witnessing an infinite gap, but which\nprovably could not have resulted from the restrictive framework.\nAs a corollary, we discover a new kind of mechanism which can witness these\ninfinite separations on instances where the previous ''aligned'' mechanisms do\nnot.",
    "descriptor": "",
    "authors": [
      "C. Alexandros Psomas",
      "Ariel Schvartzman",
      "S. Matthew Weinberg"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13039"
  },
  {
    "id": "arXiv:2205.13042",
    "title": "How explainable are adversarially-robust CNNs?",
    "abstract": "Three important criteria of existing convolutional neural networks (CNNs) are\n(1) test-set accuracy; (2) out-of-distribution accuracy; and (3)\nexplainability. While these criteria have been studied independently, their\nrelationship is unknown. For example, do CNNs that have a stronger\nout-of-distribution performance have also stronger explainability? Furthermore,\nmost prior feature-importance studies only evaluate methods on 2-3 common\nvanilla ImageNet-trained CNNs, leaving it unknown how these methods generalize\nto CNNs of other architectures and training algorithms. Here, we perform the\nfirst, large-scale evaluation of the relations of the three criteria using 9\nfeature-importance methods and 12 ImageNet-trained CNNs that are of 3 training\nalgorithms and 5 CNN architectures. We find several important insights and\nrecommendations for ML practitioners. First, adversarially robust CNNs have a\nhigher explainability score on gradient-based attribution methods (but not\nCAM-based or perturbation-based methods). Second, AdvProp models, despite being\nhighly accurate more than both vanilla and robust models alone, are not\nsuperior in explainability. Third, among 9 feature attribution methods tested,\nGradCAM and RISE are consistently the best methods. Fourth, Insertion and\nDeletion are biased towards vanilla and robust models respectively, due to\ntheir strong correlation with the confidence score distributions of a CNN.\nFifth, we did not find a single CNN to be the best in all three criteria, which\ninterestingly suggests that CNNs are harder to interpret as they become more\naccurate.",
    "descriptor": "",
    "authors": [
      "Mehdi Nourelahi",
      "Lars Kotthoff",
      "Peijie Chen",
      "Anh Nguyen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13042"
  },
  {
    "id": "arXiv:2205.13044",
    "title": "Near-Optimal Goal-Oriented Reinforcement Learning in Non-Stationary  Environments",
    "abstract": "We initiate the study of dynamic regret minimization for goal-oriented\nreinforcement learning modeled by a non-stationary stochastic shortest path\nproblem with changing cost and transition functions. We start by establishing a\nlower bound $\\Omega((B_{\\star} SAT_{\\star}(\\Delta_c +\nB_{\\star}^2\\Delta_P))^{1/3}K^{2/3})$, where $B_{\\star}$ is the maximum expected\ncost of the optimal policy of any episode starting from any state, $T_{\\star}$\nis the maximum hitting time of the optimal policy of any episode starting from\nthe initial state, $SA$ is the number of state-action pairs, $\\Delta_c$ and\n$\\Delta_P$ are the amount of changes of the cost and transition functions\nrespectively, and $K$ is the number of episodes. The different roles of\n$\\Delta_c$ and $\\Delta_P$ in this lower bound inspire us to design algorithms\nthat estimate costs and transitions separately. Specifically, assuming the\nknowledge of $\\Delta_c$ and $\\Delta_P$, we develop a simple but sub-optimal\nalgorithm and another more involved minimax optimal algorithm (up to\nlogarithmic terms). These algorithms combine the ideas of finite-horizon\napproximation [Chen et al., 2022a], special Bernstein-style bonuses of the MVP\nalgorithm [Zhang et al., 2020], adaptive confidence widening [Wei and Luo,\n2021], as well as some new techniques such as properly penalizing long-horizon\npolicies. Finally, when $\\Delta_c$ and $\\Delta_P$ are unknown, we develop a\nvariant of the MASTER algorithm [Wei and Luo, 2021] and integrate the\naforementioned ideas into it to achieve $\\widetilde{O}(\\min\\{B_{\\star}\nS\\sqrt{ALK},\n(B_{\\star}^2S^2AT_{\\star}(\\Delta_c+B_{\\star}\\Delta_P))^{1/3}K^{2/3}\\})$ regret,\nwhere $L$ is the unknown number of changes of the environment.",
    "descriptor": "",
    "authors": [
      "Liyu Chen",
      "Haipeng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13044"
  },
  {
    "id": "arXiv:2205.13045",
    "title": "QADAM: Quantization-Aware DNN Accelerator Modeling for Pareto-Optimality",
    "abstract": "As the machine learning and systems communities strive to achieve higher\nenergy-efficiency through custom deep neural network (DNN) accelerators, varied\nbit precision or quantization levels, there is a need for design space\nexploration frameworks that incorporate quantization-aware processing elements\n(PE) into the accelerator design space while having accurate and fast power,\nperformance, and area models. In this work, we present QADAM, a highly\nparameterized quantization-aware power, performance, and area modeling\nframework for DNN accelerators. Our framework can facilitate future research on\ndesign space exploration and Pareto-efficiency of DNN accelerators for various\ndesign choices such as bit precision, PE type, scratchpad sizes of PEs, global\nbuffer size, number of total PEs, and DNN configurations. Our results show that\ndifferent bit precisions and PE types lead to significant differences in terms\nof performance per area and energy. Specifically, our framework identifies a\nwide range of design points where performance per area and energy varies more\nthan 5x and 35x, respectively. We also show that the proposed lightweight\nprocessing elements (LightPEs) consistently achieve Pareto-optimal results in\nterms of accuracy and hardware-efficiency. With the proposed framework, we show\nthat LightPEs achieve on par accuracy results and up to 5.7x more performance\nper area and energy improvement when compared to the best INT16 based design.",
    "descriptor": "\nComments: Accepted paper at the Machine Learning for Computer Architecture and Systems (MLArchSys) Workshop in conjunction with ISCA 2021. This is an extended version of arXiv:2205.08648\n",
    "authors": [
      "Ahmet Inci",
      "Siri Garudanagiri Virupaksha",
      "Aman Jain",
      "Venkata Vivek Thallam",
      "Ruizhou Ding",
      "Diana Marculescu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13045"
  },
  {
    "id": "arXiv:2205.13054",
    "title": "Scalable and Low-Latency Federated Learning with Cooperative Mobile Edge  Networking",
    "abstract": "Federated learning (FL) enables collaborative model training without\ncentralizing data. However, the traditional FL framework is cloud-based and\nsuffers from high communication latency. On the other hand, the edge-based FL\nframework that relies on an edge server co-located with access point for model\naggregation has low communication latency but suffers from degraded model\naccuracy due to the limited coverage of edge server. In light of high-accuracy\nbut high-latency cloud-based FL and low-latency but low-accuracy edge-based FL,\nthis paper proposes a new FL framework based on cooperative mobile edge\nnetworking called cooperative federated edge learning (CFEL) to enable both\nhigh-accuracy and low-latency distributed intelligence at mobile edge networks.\nConsidering the unique two-tier network architecture of CFEL, a novel federated\noptimization method dubbed cooperative edge-based federated averaging\n(CE-FedAvg) is further developed, wherein each edge server both coordinates\ncollaborative model training among the devices within its own coverage and\ncooperates with other edge servers to learn a shared global model through\ndecentralized consensus. Experimental results based on benchmark datasets show\nthat CFEL can largely speed up the convergence speed and reduce the training\ntime to achieve a target model accuracy compared with prior FL frameworks.",
    "descriptor": "",
    "authors": [
      "Zhenxiao Zhang",
      "Zhidong Gao",
      "Yuanxiong Guo",
      "Yanmin Gong"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13054"
  },
  {
    "id": "arXiv:2205.13057",
    "title": "Increasing Fault Tolerance and Throughput with Adaptive Control Plane in  Smart Factories",
    "abstract": "Future smart factories are expected to deploy an emerging dynamic Virtual\nReality (VR) applications with high bandwidth wireless connections in the THz\ncommunication bands, where a factory worker can follow activities through\n360{\\deg}video streams with high quality resolution. THz communications, while\npromising as a high bandwidth wireless communication technology, are however\nknown for low fault tolerance, and are sensible to external factors. Since THz\nchannel states are in general hard to estimate, what is needed is a system that\ncan adaptively react to transceiver configurations in terms of coding and\nmodulation. To this end, we propose an adaptive control plane that can help us\nconfigure the THz communication system. The control plane implements a workflow\nalgorithm designed to adaptively choose between various coding and modulation\nschemes depending on THz channel states. The results show that an adaptive\ncontrol plane can improve throughput and signal resolution quality, with\ntheoretically zeroed bit error probability and a maximum achievable throughput\nin the scenarios analayzed.",
    "descriptor": "\nComments: This paper is uploaded here for research community, thus it is for non-commercial purposes\n",
    "authors": [
      "Cao Vien Phung",
      "Admela Jukan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.13057"
  },
  {
    "id": "arXiv:2205.13060",
    "title": "Designing an Efficient End-to-end Machine Learning Pipeline for  Real-time Empty-shelf Detection",
    "abstract": "On-Shelf Availability (OSA) of products in retail stores is a critical\nbusiness criterion in the fast moving consumer goods and retails sector. When a\nproduct is out-of-stock (OOS) and a customer cannot find it on its designed\nshelf, this causes a negative impact on the customer's behaviors and future\ndemands. Several methods are being adopted by retailers today to detect empty\nshelves and ensure high OSA of products; however, such methods are generally\nineffective and infeasible since they are either manual, expensive or less\naccurate. Recently machine learning based solutions have been proposed, but\nthey suffer from high computation cost and low accuracy problem due to lack of\nlarge annotated datasets of on-shelf products. Here, we present an elegant\napproach for designing an end-to-end machine learning (ML) pipeline for\nreal-time empty shelf detection. Considering the strong dependency between the\nquality of ML models and the quality of data, we focus on the importance of\nproper data collection, cleaning and correct data annotation before delving\ninto modeling. Since an empty-shelf detection solution should be\ncomputationally-efficient for real-time predictions, we explore different\nrun-time optimizations to improve the model performance. Our dataset contains\n1000 images, collected and annotated by following well-defined guidelines. Our\nlow-latency model achieves a mean average F1-score of 68.5%, and can process up\nto 67 images/s on Intel Xeon Gold and up to 860 images/s on an A100 GPU. Our\nannotated dataset is publicly available along with our optimized models.",
    "descriptor": "\nComments: 7 figures, 3 tables, 10 pages. arXiv admin note: text overlap with arXiv:2102.07750 by other authors\n",
    "authors": [
      "Dipendra Jha",
      "Ata Mahjoubfar",
      "Anupama Joshi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13060"
  },
  {
    "id": "arXiv:2205.13061",
    "title": "RENs: Relevance Encoding Networks",
    "abstract": "The manifold assumption for high-dimensional data assumes that the data is\ngenerated by varying a set of parameters obtained from a low-dimensional latent\nspace. Deep generative models (DGMs) are widely used to learn data\nrepresentations in an unsupervised way. DGMs parameterize the underlying\nlow-dimensional manifold in the data space using bottleneck architectures such\nas variational autoencoders (VAEs). The bottleneck dimension for VAEs is\ntreated as a hyperparameter that depends on the dataset and is fixed at design\ntime after extensive tuning. As the intrinsic dimensionality of most real-world\ndatasets is unknown, often, there is a mismatch between the intrinsic\ndimensionality and the latent dimensionality chosen as a hyperparameter. This\nmismatch can negatively contribute to the model performance for representation\nlearning and sample generation tasks. This paper proposes relevance encoding\nnetworks (RENs): a novel probabilistic VAE-based framework that uses the\nautomatic relevance determination (ARD) prior in the latent space to learn the\ndata-specific bottleneck dimensionality. The relevance of each latent dimension\nis directly learned from the data along with the other model parameters using\nstochastic gradient descent and a reparameterization trick adapted to\nnon-Gaussian priors. We leverage the concept of DeepSets to capture permutation\ninvariant statistical properties in both data and latent spaces for relevance\ndetermination. The proposed framework is general and flexible and can be used\nfor the state-of-the-art VAE models that leverage regularizers to impose\nspecific characteristics in the latent space (e.g., disentanglement). With\nextensive experimentation on synthetic and public image datasets, we show that\nthe proposed model learns the relevant latent bottleneck dimensionality without\ncompromising the representation and generation quality of the samples.",
    "descriptor": "",
    "authors": [
      "Krithika Iyer",
      "Riddhish Bhalodia",
      "Shireen Elhabian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13061"
  },
  {
    "id": "arXiv:2205.13064",
    "title": "Urban Rhapsody: Large-scale exploration of urban soundscapes",
    "abstract": "Noise is one of the primary quality-of-life issues in urban environments. In\naddition to annoyance, noise negatively impacts public health and educational\nperformance. While low-cost sensors can be deployed to monitor ambient noise\nlevels at high temporal resolutions, the amount of data they produce and the\ncomplexity of these data pose significant analytical challenges. One way to\naddress these challenges is through machine listening techniques, which are\nused to extract features in attempts to classify the source of noise and\nunderstand temporal patterns of a city's noise situation. However, the\noverwhelming number of noise sources in the urban environment and the scarcity\nof labeled data makes it nearly impossible to create classification models with\nlarge enough vocabularies that capture the true dynamism of urban soundscapes\nIn this paper, we first identify a set of requirements in the yet unexplored\ndomain of urban soundscape exploration. To satisfy the requirements and tackle\nthe identified challenges, we propose Urban Rhapsody, a framework that combines\nstate-of-the-art audio representation, machine learning, and visual analytics\nto allow users to interactively create classification models, understand noise\npatterns of a city, and quickly retrieve and label audio excerpts in order to\ncreate a large high-precision annotated database of urban sound recordings. We\ndemonstrate the tool's utility through case studies performed by domain experts\nusing data generated over the five-year deployment of a one-of-a-kind sensor\nnetwork in New York City.",
    "descriptor": "\nComments: Accepted at EuroVis 2022. Source code available at: this https URL\n",
    "authors": [
      "Joao Rulff",
      "Fabio Miranda",
      "Maryam Hosseini",
      "Marcos Lage",
      "Mark Cartwright",
      "Graham Dove",
      "Juan Bello",
      "Claudio T. Silva"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.13064"
  },
  {
    "id": "arXiv:2205.13066",
    "title": "Semi-supervised Drifted Stream Learning with Short Lookback",
    "abstract": "In many scenarios, 1) data streams are generated in real time; 2) labeled\ndata are expensive and only limited labels are available in the beginning; 3)\nreal-world data is not always i.i.d. and data drift over time gradually; 4) the\nstorage of historical streams is limited and model updating can only be\nachieved based on a very short lookback window. This learning setting limits\nthe applicability and availability of many Machine Learning (ML) algorithms. We\ngeneralize the learning task under such setting as a semi-supervised drifted\nstream learning with short lookback problem (SDSL). SDSL imposes two\nunder-addressed challenges on existing methods in semi-supervised learning,\ncontinuous learning, and domain adaptation: 1) robust pseudo-labeling under\ngradual shifts and 2) anti-forgetting adaptation with short lookback. To tackle\nthese challenges, we propose a principled and generic generation-replay\nframework to solve SDSL. The framework is able to accomplish: 1) robust\npseudo-labeling in the generation step; 2) anti-forgetting adaption in the\nreplay step. To achieve robust pseudo-labeling, we develop a novel pseudo-label\nclassification model to leverage supervised knowledge of previously labeled\ndata, unsupervised knowledge of new data, and, structure knowledge of invariant\nlabel semantics. To achieve adaptive anti-forgetting model replay, we propose\nto view the anti-forgetting adaptation task as a flat region search problem. We\npropose a novel minimax game-based replay objective function to solve the flat\nregion search problem and develop an effective optimization solver. Finally, we\npresent extensive experiments to demonstrate our framework can effectively\naddress the task of anti-forgetting learning in drifted streams with short\nlookback.",
    "descriptor": "\nComments: To appear in KDD 2022\n",
    "authors": [
      "Weijieying Ren",
      "Pengyang Wang",
      "Xiaolin Li",
      "Charles E. Hughes",
      "Yanjie Fu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13066"
  },
  {
    "id": "arXiv:2205.13067",
    "title": "Forecasting Patient Demand at Urgent Care Clinics using Machine Learning",
    "abstract": "Urgent care clinics and emergency departments around the world periodically\nsuffer from extended wait times beyond patient expectations due to inadequate\nstaffing levels. These delays have been linked with adverse clinical outcomes.\nPrevious research into forecasting demand this domain has mostly used a\ncollection of statistical techniques, with machine learning approaches only now\nbeginning to emerge in recent literature. The forecasting problem for this\ndomain is difficult and has also been complicated by the COVID-19 pandemic\nwhich has introduced an additional complexity to this estimation due to typical\ndemand patterns being disrupted. This study explores the ability of machine\nlearning methods to generate accurate patient presentations at two large urgent\ncare clinics located in Auckland, New Zealand. A number of machine learning\nalgorithms were explored in order to determine the most effective technique for\nthis problem domain, with the task of making forecasts of daily patient demand\nthree months in advance. The study also performed an in-depth analysis into the\nmodel behaviour in respect to the exploration of which features are most\neffective at predicting demand and which features are capable of adaptation to\nthe volatility caused by the COVID-19 pandemic lockdowns. The results showed\nthat ensemble-based methods delivered the most accurate and consistent\nsolutions on average, generating improvements in the range of 23%-27% over the\nexisting in-house methods for estimating the daily demand.",
    "descriptor": "",
    "authors": [
      "Paula Maddigan",
      "Teo Susnjak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13067"
  },
  {
    "id": "arXiv:2205.13068",
    "title": "Tight Lower Bounds on Worst-Case Guarantees for Zero-Shot Learning with  Attributes",
    "abstract": "We develop a rigorous mathematical analysis of zero-shot learning with\nattributes. In this setting, the goal is to label novel classes with no\ntraining data, only detectors for attributes and a description of how those\nattributes are correlated with the target classes, called the class-attribute\nmatrix. We develop the first non-trivial lower bound on the worst-case error of\nthe best map from attributes to classes for this setting, even with perfect\nattribute detectors. The lower bound characterizes the theoretical intrinsic\ndifficulty of the zero-shot problem based on the available information -- the\nclass-attribute matrix -- and the bound is practically computable from it. Our\nlower bound is tight, as we show that we can always find a randomized map from\nattributes to classes whose expected error is upper bounded by the value of the\nlower bound. We show that our analysis can be predictive of how standard\nzero-shot methods behave in practice, including which classes will likely be\nconfused with others.",
    "descriptor": "",
    "authors": [
      "Alessio Mazzetto",
      "Cristina Menghini",
      "Andrew Yuan",
      "Eli Upfal",
      "Stephen H. Bach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13068"
  },
  {
    "id": "arXiv:2205.13070",
    "title": "A Stable Weighted Residual Finite Element Formulation for the Simulation  of Linear Moving Conductor Problems",
    "abstract": "The finite element method is one of the widely employed numerical techniques\nin electrical engineering for the study of electric and magnetic fields. When\napplied to the moving conductor problems, the finite element method is known to\nhave numerical oscillations in the solution. To resolve this, the upwinding\ntechniques, which are developed for the transport equation are borrowed and\ndirectly employed for the magnetic induction equation. In this work, an\nalternative weighted residual formulation is explored for the simulation of the\nlinear moving conductor problems. The stability of the formulation is\nanalytically studied for the 1D version of the moving conductor problem. Then\nthe rate of convergence and the accuracy are illustrated with the help of\nseveral test cases in 1D as well as 2D. Subsequently, the stability of the\nformulation is demonstrated with a 3D moving conductor simulation.",
    "descriptor": "",
    "authors": [
      "Sethupathy Subramanian",
      "Sujata Bhowmick"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13070"
  },
  {
    "id": "arXiv:2205.13071",
    "title": "Exploring Map-based Features for Efficient Attention-based Vehicle  Motion Prediction",
    "abstract": "Motion prediction (MP) of multiple agents is a crucial task in arbitrarily\ncomplex environments, from social robots to self-driving cars. Current\napproaches tackle this problem using end-to-end networks, where the input data\nis usually a rendered top-view of the scene and the past trajectories of all\nthe agents; leveraging this information is a must to obtain optimal\nperformance. In that sense, a reliable Autonomous Driving (AD) system must\nproduce reasonable predictions on time, however, despite many of these\napproaches use simple ConvNets and LSTMs, models might not be efficient enough\nfor real-time applications when using both sources of information (map and\ntrajectory history). Moreover, the performance of these models highly depends\non the amount of training data, which can be expensive (particularly the\nannotated HD maps). In this work, we explore how to achieve competitive\nperformance on the Argoverse 1.0 Benchmark using efficient attention-based\nmodels, which take as input the past trajectories and map-based features from\nminimal map information to ensure efficient and reliable MP. These features\nrepresent interpretable information as the driveable area and plausible goal\npoints, in opposition to black-box CNN-based methods for map processing.",
    "descriptor": "\nComments: CVPR MABe 2022 - ICRA FFPFAD 2022 Workshops\n",
    "authors": [
      "Carlos G\u00f3mez-Hu\u00e9lamo",
      "Marcos V. Conde",
      "Miguel Ortiz"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13071"
  },
  {
    "id": "arXiv:2205.13076",
    "title": "Entropy Maximization with Depth: A Variational Principle for Random  Neural Networks",
    "abstract": "To understand the essential role of depth in neural networks, we investigate\na variational principle for depth: Does increasing depth perform an implicit\noptimization for the representations in neural networks? We prove that random\nneural networks equipped with batch normalization maximize the differential\nentropy of representations with depth up to constant factors, assuming that the\nrepresentations are contractive. Thus, representations inherently obey the\n\\textit{principle of maximum entropy} at initialization, in the absence of\ninformation about the learning task. Our variational formulation for neural\nrepresentations characterizes the interplay between representation entropy and\narchitectural components, including depth, width, and non-linear activations,\nthereby potentially inspiring the design of neural architectures.",
    "descriptor": "",
    "authors": [
      "Amir Joudaki",
      "Hadi Daneshmand",
      "Francis Bach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13076"
  },
  {
    "id": "arXiv:2205.13077",
    "title": "Many Sequential Iterative Algorithms Can Be Parallel and (Nearly)  Work-efficient",
    "abstract": "To design efficient parallel algorithms, some recent papers showed that many\nsequential iterative algorithms can be directly parallelized but there are\nstill challenges in achieving work-efficiency and high-parallelism.\nWork-efficiency can be hard for certain problems where the number of\ndependences is asymptotically more than optimal sequential work bound. To\nachieve high-parallelism, we want to process as many objects as possible in\nparallel. The goal is to achieve $\\tilde{O}(D)$ span for a problem with the\ndeepest dependence length $D$. We refer to this property as round-efficiency.\nIn this paper, we show work-efficient and round-efficient algorithms for a\nvariety of classic problems and propose general approaches to do so.\nTo efficiently parallelize many sequential iterative algorithms, we propose\nthe phase-parallel framework. The framework assigns a rank to each object and\nprocesses them accordingly. All objects with the same rank can be processed in\nparallel. To enable work-efficiency and high parallelism, we use two types of\ngeneral techniques. Type 1 algorithms aim to use range queries to extract all\nobjects with the same rank, such that we avoid evaluating all the dependences.\nWe discuss activity selection, unlimited knapsack, and more using Type 1\nframework. Type 2 algorithms aim to wake up an object when the last object it\ndepends on is finished. We discuss activity selection, longest increasing\nsubsequence (LIS), and many other algorithms using Type 2 framework.\nAll of our algorithms are (nearly) work-efficient and round-efficient. Many\nof them improve previous best bounds, and some of them are the first to achieve\nwork-efficiency with round-efficiency. We also implement many of them. On\ninputs with reasonable dependence depth, our algorithms are highly parallelized\nand significantly outperform their sequential counterparts.",
    "descriptor": "",
    "authors": [
      "Zheqi Shen",
      "Zijin Wan",
      "Yan Gu",
      "Yihan Sun"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13077"
  },
  {
    "id": "arXiv:2205.13079",
    "title": "Learning to Query Internet Text for Informing Reinforcement Learning  Agents",
    "abstract": "Generalization to out of distribution tasks in reinforcement learning is a\nchallenging problem. One successful approach improves generalization by\nconditioning policies on task or environment descriptions that provide\ninformation about the current transition or reward functions. Previously, these\ndescriptions were often expressed as generated or crowd sourced text. In this\nwork, we begin to tackle the problem of extracting useful information from\nnatural language found in the wild (e.g. internet forums, documentation, and\nwikis). These natural, pre-existing sources are especially challenging, noisy,\nand large and present novel challenges compared to previous approaches. We\npropose to address these challenges by training reinforcement learning agents\nto learn to query these sources as a human would, and we experiment with how\nand when an agent should query. To address the \\textit{how}, we demonstrate\nthat pretrained QA models perform well at executing zero-shot queries in our\ntarget domain. Using information retrieved by a QA model, we train an agent to\nlearn \\textit{when} it should execute queries. We show that our method\ncorrectly learns to execute queries to maximize reward in a reinforcement\nlearning setting.",
    "descriptor": "",
    "authors": [
      "Kolby Nottingham",
      "Alekhya Pyla",
      "Sameer Singh",
      "Roy Fox"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13079"
  },
  {
    "id": "arXiv:2205.13084",
    "title": "BRIGHT -- Graph Neural Networks in Real-Time Fraud Detection",
    "abstract": "Detecting fraudulent transactions is an essential component to control risk\nin e-commerce marketplaces. Apart from rule-based and machine learning filters\nthat are already deployed in production, we want to enable efficient real-time\ninference with graph neural networks (GNNs), which is useful to catch multihop\nrisk propagation in a transaction graph. However, two challenges arise in the\nimplementation of GNNs in production. First, future information in a dynamic\ngraph should not be considered in message passing to predict the past. Second,\nthe latency of graph query and GNN model inference is usually up to hundreds of\nmilliseconds, which is costly for some critical online services. To tackle\nthese challenges, we propose a Batch and Real-time Inception GrapH Topology\n(BRIGHT) framework to conduct an end-to-end GNN learning that allows efficient\nonline real-time inference. BRIGHT framework consists of a graph transformation\nmodule (Two-Stage Directed Graph) and a corresponding GNN architecture (Lambda\nNeural Network). The Two-Stage Directed Graph guarantees that the information\npassed through neighbors is only from the historical payment transactions. It\nconsists of two subgraphs representing historical relationships and real-time\nlinks, respectively. The Lambda Neural Network decouples inference into two\nstages: batch inference of entity embeddings and real-time inference of\ntransaction prediction. Our experiments show that BRIGHT outperforms the\nbaseline models by >2\\% in average w.r.t.~precision. Furthermore, BRIGHT is\ncomputationally efficient for real-time fraud detection. Regarding end-to-end\nperformance (including neighbor query and inference), BRIGHT can reduce the P99\nlatency by >75\\%. For the inference stage, our speedup is on average\n7.8$\\times$ compared to the traditional GNN.",
    "descriptor": "",
    "authors": [
      "Mingxuan Lu",
      "Zhichao Han",
      "Susie Xi Rao",
      "Zitao Zhang",
      "Yang Zhao",
      "Yinan Shan",
      "Ramesh Raghunathan",
      "Ce Zhang",
      "Jiawei Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13084"
  },
  {
    "id": "arXiv:2205.13087",
    "title": "New Explicit Good Linear Sum-Rank-Metric Codes",
    "abstract": "Sum-rank-metric codes have wide applications in such as universal error\ncorrection and security in multishot network, space-time coding and\nconstruction of partial-MDS codes for repair in distributed storage.\nFundamental properties of sum-rank-metric codes have been studied and some\nexplicit or probabilistic constructions of good sum-rank-metric codes have been\nproposed. In this paper we propose two simple constructions of explicit linear\nsum-rank-metric codes. In finite length regime, numerous good linear\nsum-rank-metric codes from our construction are given. Some of them have better\nparameters than previous constructed sum-rank-metric codes. For example a lot\nof small block size better linear sum-rank-metric codes over ${\\bf F}_q$ of the\nmatrix size $2 \\times 2$ are constructed for $q=2, 3, 4$. Asymptotically our\nconstructed sum-rank-metric codes are closing to or exceeding the\nGilbert-Varshamov-like bound for the sum-rank-metric codes for some parameters.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Hao Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13087"
  },
  {
    "id": "arXiv:2205.13092",
    "title": "Semantic-Aware Representation Blending for Multi-Label Image Recognition  with Partial Labels",
    "abstract": "Despite achieving impressive progress, current multi-label image recognition\n(MLR) algorithms heavily depend on large-scale datasets with complete labels,\nmaking collecting large-scale datasets extremely time-consuming and\nlabor-intensive. Training the multi-label image recognition models with partial\nlabels (MLR-PL) is an alternative way to address this issue, in which merely\nsome labels are known while others are unknown for each image (see Figure 1).\nHowever, current MLP-PL algorithms mainly rely on the pre-trained image\nclassification or similarity models to generate pseudo labels for the unknown\nlabels. Thus, they depend on a certain amount of data annotations and\ninevitably suffer from obvious performance drops, especially when the known\nlabel proportion is low. To address this dilemma, we propose a unified\nsemantic-aware representation blending (SARB) that consists of two crucial\nmodules to blend multi-granularity category-specific semantic representation\nacross different images to transfer information of known labels to complement\nunknown labels. Extensive experiments on the MS-COCO, Visual Genome, and Pascal\nVOC 2007 datasets show that the proposed SARB consistently outperforms current\nstate-of-the-art algorithms on all known label proportion settings. Concretely,\nit obtain the average mAP improvement of 1.9%, 4.5%, 1.0% on the three\nbenchmark datasets compared with the second-best algorithm.",
    "descriptor": "\nComments: Technical Report. arXiv admin note: substantial text overlap with arXiv:2203.02172\n",
    "authors": [
      "Tao Pu",
      "Tianshui Chen",
      "Hefeng Wu",
      "Yongyi Lu",
      "Liang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13092"
  },
  {
    "id": "arXiv:2205.13094",
    "title": "Undersampling is a Minimax Optimal Robustness Intervention in  Nonparametric Classification",
    "abstract": "While a broad range of techniques have been proposed to tackle distribution\nshift, the simple baseline of training on an $\\textit{undersampled}$ dataset\noften achieves close to state-of-the-art-accuracy across several popular\nbenchmarks. This is rather surprising, since undersampling algorithms discard\nexcess majority group data. To understand this phenomenon, we ask if learning\nis fundamentally constrained by a lack of minority group samples. We prove that\nthis is indeed the case in the setting of nonparametric binary classification.\nOur results show that in the worst case, an algorithm cannot outperform\nundersampling unless there is a high degree of overlap between the train and\ntest distributions (which is unlikely to be the case in real-world datasets),\nor if the algorithm leverages additional structure about the distribution\nshift. In particular, in the case of label shift we show that there is always\nan undersampling algorithm that is minimax optimal. While in the case of\ngroup-covariate shift we show that there is an undersampling algorithm that is\nminimax optimal when the overlap between the group distributions is small. We\nalso perform an experimental case study on a label shift dataset and find that\nin line with our theory the test accuracy of robust neural network classifiers\nis constrained by the number of minority samples.",
    "descriptor": "",
    "authors": [
      "Niladri S. Chatterji",
      "Saminul Haque",
      "Tatsunori Hashimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13094"
  },
  {
    "id": "arXiv:2205.13095",
    "title": "VizInspect Pro -- Automated Optical Inspection (AOI) solution",
    "abstract": "Traditional vision based Automated Optical Inspection (referred to as AOI in\npaper) systems present multiple challenges in factory settings including\ninability to scale across multiple product lines, requirement of vendor\nprogramming expertise, little tolerance to variations and lack of cloud\nconnectivity for aggregated insights. The lack of flexibility in these systems\npresents a unique opportunity for a deep learning based AOI system specifically\nfor factory automation. The proposed solution, VizInspect pro is a generic\ncomputer vision based AOI solution built on top of Leo - An edge AI platform.\nInnovative features that overcome challenges of traditional vision systems\ninclude deep learning based image analysis which combines the power of\nself-learning with high speed and accuracy, an intuitive user interface to\nconfigure inspection profiles in minutes without ML or vision expertise and the\nability to solve complex inspection challenges while being tolerant to\ndeviations and unpredictable defects. This solution has been validated by\nmultiple external enterprise customers with confirmed value propositions. In\nthis paper we show you how this solution and platform solved problems around\nmodel development, deployment, scaling multiple inferences and visualizations.",
    "descriptor": "",
    "authors": [
      "Faraz Waseem",
      "Sanjit Menon",
      "Haotian Xu",
      "Debashis Mondal"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13095"
  },
  {
    "id": "arXiv:2205.13098",
    "title": "Optimal Neural Network Approximation of Wasserstein Gradient Direction  via Convex Optimization",
    "abstract": "The computation of Wasserstein gradient direction is essential for posterior\nsampling problems and scientific computing. The approximation of the\nWasserstein gradient with finite samples requires solving a variational\nproblem. We study the variational problem in the family of two-layer networks\nwith squared-ReLU activations, towards which we derive a semi-definite\nprogramming (SDP) relaxation. This SDP can be viewed as an approximation of the\nWasserstein gradient in a broader function family including two-layer networks.\nBy solving the convex SDP, we obtain the optimal approximation of the\nWasserstein gradient direction in this class of functions. Numerical\nexperiments including PDE-constrained Bayesian inference and parameter\nestimation in COVID-19 modeling demonstrate the effectiveness of the proposed\nmethod.",
    "descriptor": "",
    "authors": [
      "Yifei Wang",
      "Peng Chen",
      "Mert Pilanci",
      "Wuchen Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13098"
  },
  {
    "id": "arXiv:2205.13102",
    "title": "Deep-XFCT: Deep learning 3D-mineral liberation analysis with micro X-ray  fluorescence and computed tomography",
    "abstract": "The rapid development of X-ray micro-computed tomography (micro-CT) opens new\nopportunities for 3D analysis of particle and grain-size characterisation,\ndetermination of particle densities and shape factors, estimation of mineral\nassociations and liberation and locking. Current practices in mineral\nliberation analysis are based on 2D representations leading to systematic\nerrors in the extrapolation to volumetric properties. New quantitative methods\nbased on tomographic data are therefore urgently required for characterisation\nof mineral deposits, mineral processing, characterisation of tailings, rock\ntyping, stratigraphic refinement, reservoir characterisation for applications\nin the resource industry, environmental and material sciences. To date, no\nsimple non-destructive method exists for 3D mineral liberation analysis. We\npresent a new development based on combining micro-CT with micro-X-ray\nfluorescence (micro-XRF) using deep learning. We demonstrate successful\nsemi-automated multi-modal analysis of a crystalline magmatic rock where the\nnew technique overcomes the difficult task of differentiating feldspar from\nquartz in micro-CT data set. The approach is universal and can be extended to\nany multi-modal and multi-instrument analysis for further refinement. We\nconclude that the combination of micro-CT and micro-XRF already provides a new\nopportunity for robust 3D mineral liberation analysis in both field and\nlaboratory applications.",
    "descriptor": "\nComments: 24 pages, 10 figures\n",
    "authors": [
      "Patrick Kin Man Tung",
      "Amalia Yunita Halim",
      "Huixin Wang",
      "Anne Rich",
      "Christopher Marjo",
      "Klaus Regenauer-Lieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2205.13102"
  },
  {
    "id": "arXiv:2205.13104",
    "title": "Trainable Weight Averaging for Fast Convergence and Better  Generalization",
    "abstract": "Stochastic gradient descent (SGD) and its variants are commonly considered as\nthe de-facto methods to train deep neural networks (DNNs). While recent\nimprovements to SGD mainly focus on the descent algorithm itself, few works pay\nattention to utilizing the historical solutions -- as an iterative method, SGD\nhas actually gone through substantial explorations before its final\nconvergence. Recently, an interesting attempt is stochastic weight averaging\n(SWA), which significantly improves the generalization by simply averaging the\nsolutions at the tail stage of training. In this paper, we propose to optimize\nthe averaging coefficients, leading to our Trainable Weight Averaging (TWA),\nessentially a novel training method in a reduced subspace spanned by historical\nsolutions. TWA is quite efficient and has good generalization capability as the\ndegree of freedom for training is small. It largely reduces the estimation\nerror from SWA, making it not only further improve the SWA solutions but also\ntake full advantage of the solutions generated in the head of training where\nSWA fails. In the extensive numerical experiments, (i) TWA achieves consistent\nimprovements over SWA with less sensitivity to learning rate; (ii) applying TWA\nin the head stage of training largely speeds up the convergence, resulting in\nover 40% time saving on CIFAR and 30% on ImageNet with improved generalization\ncompared with regular training. The code is released at\nhttps://github.com/nblt/TWA.",
    "descriptor": "",
    "authors": [
      "Tao Li",
      "Zhehao Huang",
      "Qinghua Tao",
      "Yingwen Wu",
      "Xiaolin Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13104"
  },
  {
    "id": "arXiv:2205.13108",
    "title": "Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV  Conversion",
    "abstract": "We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.",
    "descriptor": "\nComments: WIT Workshop @ ACL2022\n",
    "authors": [
      "Seongmin Park",
      "Jihwa Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13108"
  },
  {
    "id": "arXiv:2205.13109",
    "title": "Learning to segment with limited annotations: Self-supervised  pretraining with regression and contrastive loss in MRI",
    "abstract": "Obtaining manual annotations for large datasets for supervised training of\ndeep learning (DL) models is challenging. The availability of large unlabeled\ndatasets compared to labeled ones motivate the use of self-supervised\npretraining to initialize DL models for subsequent segmentation tasks. In this\nwork, we consider two pre-training approaches for driving a DL model to learn\ndifferent representations using: a) regression loss that exploits spatial\ndependencies within an image and b) contrastive loss that exploits semantic\nsimilarity between pairs of images. The effect of pretraining techniques is\nevaluated in two downstream segmentation applications using Magnetic Resonance\n(MR) images: a) liver segmentation in abdominal T2-weighted MR images and b)\nprostate segmentation in T2-weighted MR images of the prostate. We observed\nthat DL models pretrained using self-supervision can be finetuned for\ncomparable performance with fewer labeled datasets. Additionally, we also\nobserved that initializing the DL model using contrastive loss based\npretraining performed better than the regression loss.",
    "descriptor": "\nComments: Presented at the Annual Conference of International Society for Magnetic Resonance in Medicine, London, UK. May 2022\n",
    "authors": [
      "Lavanya Umapathy",
      "Zhiyang Fu",
      "Rohit Philip",
      "Diego Martin",
      "Maria Altbach",
      "Ali Bilgin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13109"
  },
  {
    "id": "arXiv:2205.13113",
    "title": "Hybrid Spherical- and Planar-Wave Channel Modeling and Estimation for  Terahertz Integrated UM-MIMO and IRS Systems",
    "abstract": "Integrated ultra-massive multiple-input multiple-output (UM-MIMO) and\nintelligent reflecting surface (IRS) systems are promising for 6G and beyond\nTerahertz (0.1-10 THz) communications, to effectively bypass the barriers of\nlimited coverage and line-of-sight blockage. However, excessive dimensions of\nUM-MIMO and IRS enlarge the near-field region, while strong THz channel\nsparsity in far-field is detrimental to spatial multiplexing. Moreover, channel\nestimation (CE) requires recovering the large-scale channel from severely\ncompressed observations due to limited RF-chains. To tackle these challenges, a\nhybrid spherical- and planar-wave channel model (HSPM) is developed for the\ncascaded channel of the integrated system. The spatial multiplexing gains under\nnear-field and far-field regions are analyzed, which are found to be limited by\nthe segmented channel with a lower rank. Furthermore, a compressive\nsensing-based CE framework is developed, including a sparse channel\nrepresentation method, a separate-side estimation (SSE) and a\ndictionary-shrinkage estimation (DSE) algorithms. Numerical results verify the\neffectiveness of the HSPM, the capacity of which is only $5\\times10^{-4}$\nbits/s/Hz deviated from that obtained by the ground-truth spherical-wave-model,\nwith 256 elements. While the SSE achieves improved accuracy for CE than\nbenchmark algorithms, the DSE is more attractive in noisy environments, with\n0.8 dB lower normalized-mean-square-error than SSE.",
    "descriptor": "",
    "authors": [
      "Yuhang Chen",
      "Renwang Li",
      "Chong Han",
      "Shu Sun",
      "Meixia Tao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13113"
  },
  {
    "id": "arXiv:2205.13114",
    "title": "Contextual Pandora's Box",
    "abstract": "Pandora's Box is a fundamental stochastic optimization problem, where the\ndecision-maker must find a good alternative while minimizing the search cost of\nexploring the value of each alternative. In the original formulation, it is\nassumed that accurate priors are given for the values of all the alternatives,\nwhile recent work studies the online variant of Pandora's Box where priors are\noriginally unknown. In this work, we extend Pandora's Box to the online\nsetting, while incorporating context. At every round, we are presented with a\nnumber of alternatives each having a context, an exploration cost and an\nunknown value drawn from an unknown prior distribution that may change at every\nround. Our main result is a no-regret algorithm that performs comparably well\nto the optimal algorithm which knows all prior distributions exactly. Our\nalgorithm works even in the bandit setting where the algorithm never learns the\nvalues of the alternatives that were not explored. The key technique that\nenables our result is novel a modification of the realizability condition in\ncontextual bandits that connects a context to the reservation value of the\ncorresponding distribution rather than its mean",
    "descriptor": "",
    "authors": [
      "Alexia Atsidakou",
      "Constantine Caramanis",
      "Evangelia Gergatsouli",
      "Orestis Papadigenopoulos",
      "Christos Tzamos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13114"
  },
  {
    "id": "arXiv:2205.13115",
    "title": "Fine-grained Image Captioning with CLIP Reward",
    "abstract": "Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward",
    "descriptor": "\nComments: NAACL Findings 2022\n",
    "authors": [
      "Jaemin Cho",
      "Seunghyun Yoon",
      "Ajinkya Kale",
      "Franck Dernoncourt",
      "Trung Bui",
      "Mohit Bansal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13115"
  },
  {
    "id": "arXiv:2205.13116",
    "title": "GraphPMU: Event Clustering via Graph Representation Learning Using  Locationally-Scarce Distribution-Level Fundamental and Harmonic PMU  Measurements",
    "abstract": "This paper is concerned with the complex task of identifying the type and\ncause of the events that are captured by distribution-level phasor measurement\nunits (D-PMUs) in order to enhance situational awareness in power distribution\nsystems. Our goal is to address two fundamental challenges in this field: a)\nscarcity in measurement locations due to the high cost of purchasing,\ninstalling, and streaming data from D-PMUs; b) limited prior knowledge about\nthe event signatures due to the fact that the events are diverse, infrequent,\nand inherently unscheduled. To tackle these challenges, we propose an\nunsupervised graph-representation learning method, called GraphPMU, to\nsignificantly improve the performance in event clustering under\nlocationally-scarce data availability by proposing the following two new\ndirections: 1) using the topological information about the relative location of\nthe few available phasor measurement units on the graph of the power\ndistribution network; 2) utilizing not only the commonly used fundamental\nphasor measurements, bus also the less explored harmonic phasor measurements in\nthe process of analyzing the signatures of various events. Through a detailed\nanalysis of several case studies, we show that GraphPMU can highly outperform\nthe prevalent methods in the literature.",
    "descriptor": "\nComments: 10 pages, 8 figures, Submitted to IEEE Trans. on Smart Grid\n",
    "authors": [
      "Armin Aligholian",
      "Hamed Mohsenian-Rad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13116"
  },
  {
    "id": "arXiv:2205.13117",
    "title": "Learn to Cluster Faces via Pairwise Classification",
    "abstract": "Face clustering plays an essential role in exploiting massive unlabeled face\ndata. Recently, graph-based face clustering methods are getting popular for\ntheir satisfying performances. However, they usually suffer from excessive\nmemory consumption especially on large-scale graphs, and rely on empirical\nthresholds to determine the connectivities between samples in inference, which\nrestricts their applications in various real-world scenes. To address such\nproblems, in this paper, we explore face clustering from the pairwise angle.\nSpecifically, we formulate the face clustering task as a pairwise relationship\nclassification task, avoiding the memory-consuming learning on large-scale\ngraphs. The classifier can directly determine the relationship between samples\nand is enhanced by taking advantage of the contextual information. Moreover, to\nfurther facilitate the efficiency of our method, we propose a rank-weighted\ndensity to guide the selection of pairs sent to the classifier. Experimental\nresults demonstrate that our method achieves state-of-the-art performances on\nseveral public clustering benchmarks at the fastest speed and shows a great\nadvantage in comparison with graph-based clustering methods on memory\nconsumption.",
    "descriptor": "\nComments: Accepted by ICCV2021\n",
    "authors": [
      "Junfu Liu",
      "Di Qiu",
      "Pengfei Yan",
      "Xiaolin Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13117"
  },
  {
    "id": "arXiv:2205.13119",
    "title": "Understanding Metrics for Paraphrasing",
    "abstract": "Paraphrase generation is a difficult problem. This is not only because of the\nlimitations in text generation capabilities but also due that to the lack of a\nproper definition of what qualifies as a paraphrase and corresponding metrics\nto measure how good it is. Metrics for evaluation of paraphrasing quality is an\non going research problem. Most of the existing metrics in use having been\nborrowed from other tasks do not capture the complete essence of a good\nparaphrase, and often fail at borderline-cases. In this work, we propose a\nnovel metric $ROUGE_P$ to measure the quality of paraphrases along the\ndimensions of adequacy, novelty and fluency. We also provide empirical evidence\nto show that the current natural language generation metrics are insufficient\nto measure these desired properties of a good paraphrase. We look at paraphrase\nmodel fine-tuning and generation from the lens of metrics to gain a deeper\nunderstanding of what it takes to generate and evaluate a good paraphrase.",
    "descriptor": "\nComments: 19 pages, 7 figures, 12 Tables\n",
    "authors": [
      "Omkar Patil",
      "Rahul Singh",
      "Tarun Joshi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13119"
  },
  {
    "id": "arXiv:2205.13120",
    "title": "Perceptual Learned Source-Channel Coding for High-Fidelity Image  Semantic Transmission",
    "abstract": "As one novel approach to realize end-to-end wireless image semantic\ntransmission, deep learning-based joint source-channel coding (deep JSCC)\nmethod is emerging in both deep learning and communication communities.\nHowever, current deep JSCC image transmission systems are typically optimized\nfor traditional distortion metrics such as peak signal-to-noise ratio (PSNR) or\nmulti-scale structural similarity (MS-SSIM). But for low transmission rates,\ndue to the imperfect wireless channel, these distortion metrics lose\nsignificance as they favor pixel-wise preservation. To account for human visual\nperception in semantic communications, it is of great importance to develop new\ndeep JSCC systems optimized beyond traditional PSNR and MS-SSIM metrics. In\nthis paper, we introduce adversarial losses to optimize deep JSCC, which tends\nto preserve global semantic information and local texture. Our new deep JSCC\narchitecture combines encoder, wireless channel, decoder/generator, and\ndiscriminator, which are jointly learned under both perceptual and adversarial\nlosses. Our method yields human visually much more pleasing results than\nstate-of-the-art engineered image coded transmission systems and traditional\ndeep JSCC systems. A user study confirms that achieving the perceptually\nsimilar end-to-end image transmission quality, the proposed method can save\nabout 50\\% wireless channel bandwidth cost.",
    "descriptor": "",
    "authors": [
      "Jun Wang",
      "Sixian Wang",
      "Jincheng Dai",
      "Zhongwei Si",
      "Dekun Zhou",
      "Kai Niu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13120"
  },
  {
    "id": "arXiv:2205.13121",
    "title": "Cali3F: Calibrated Fast Fair Federated Recommendation System",
    "abstract": "The increasingly stringent regulations on privacy protection have sparked\ninterest in federated learning. As a distributed machine learning framework, it\nbridges isolated data islands by training a global model over devices while\nkeeping data localized. Specific to recommendation systems, many federated\nrecommendation algorithms have been proposed to realize the privacy-preserving\ncollaborative recommendation. However, several constraints remain largely\nunexplored. One big concern is how to ensure fairness between participants of\nfederated learning, that is, to maintain the uniformity of recommendation\nperformance across devices. On the other hand, due to data heterogeneity and\nlimited networks, additional challenges occur in the convergence speed. To\naddress these problems, in this paper, we first propose a personalized\nfederated recommendation system training algorithm to improve the\nrecommendation performance fairness. Then we adopt a clustering-based\naggregation method to accelerate the training process. Combining the two\ncomponents, we proposed Cali3F, a calibrated fast and fair federated\nrecommendation framework. Cali3F not only addresses the convergence problem by\na within-cluster parameter sharing approach but also significantly boosts\nfairness by calibrating local models with the global model. We demonstrate the\nperformance of Cali3F across standard benchmark datasets and explore the\nefficacy in comparison to traditional aggregation approaches.",
    "descriptor": "\nComments: This paper has been accepted by IJCNN2022\n",
    "authors": [
      "Zhitao Zhu",
      "Shijing Si",
      "Jianzong Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13121"
  },
  {
    "id": "arXiv:2205.13124",
    "title": "PixelGame: Infrared small target segmentation as a Nash equilibrium",
    "abstract": "A key challenge of infrared small target segmentation (ISTS) is to balance\nfalse negative pixels (FNs) and false positive pixels (FPs). Traditional\nmethods combine FNs and FPs into a single objective by weighted sum, and the\noptimization process is decided by one actor. Minimizing FNs and FPs with the\nsame strategy leads to antagonistic decisions. To address this problem, we\npropose a competitive game framework (pixelGame) from a novel perspective for\nISTS. In pixelGame, FNs and FPs are controlled by different player whose goal\nis to minimize their own utility function. FNs-player and FPs-player are\ndesigned with different strategies: One is to minimize FNs and the other is to\nminimize FPs. The utility function drives the evolution of the two participants\nin competition. We consider the Nash equilibrium of pixelGame as the optimal\nsolution. In addition, we propose maximum information modulation (MIM) to\nhighlight the tar-get information. MIM effectively focuses on the salient\nregion including small targets. Extensive experiments on two standard public\ndatasets prove the effectiveness of our method. Compared with other\nstate-of-the-art methods, our method achieves better performance in terms of\nF1-measure (F1) and the intersection of union (IoU).",
    "descriptor": "",
    "authors": [
      "Heng Zhou",
      "Chunna Tian",
      "Zhenxi Zhang",
      "Chengyang Li",
      "Yongqiang Xie",
      "Zhongbo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13124"
  },
  {
    "id": "arXiv:2205.13125",
    "title": "Prompt-based Learning for Unpaired Image Captioning",
    "abstract": "Unpaired Image Captioning (UIC) has been developed to learn image\ndescriptions from unaligned vision-language sample pairs. Existing schemes\nusually adopt the visual concept reward of reinforcement learning to obtain the\nalignment between visual concepts and images. However, the cross-domain\nalignment is usually weak that severely constrains the overall performance of\nthese existing schemes. Recent successes of Vision-Language Pre-Trained Models\n(VL-PTMs) have triggered the development of prompt-based learning from VL-PTMs.\nWe present in this paper a novel scheme based on prompt to train the UIC model,\nmaking best use of the powerful generalization ability and abundant\nvision-language prior knowledge learned under VL-PTMs. We adopt the CLIP model\nfor this research in unpaired image captioning. Specifically, the visual images\nare taken as input to the prompt generation module, which contains the\npre-trained model as well as one feed-forward layer for prompt extraction.\nThen, the input images and generated prompts are aggregated for unpaired\nadversarial captioning learning. To further enhance the potential performance\nof the captioning, we designed a high-quality pseudo caption filter guided by\nthe CLIP logits to measure correlations between predicted captions and the\ncorresponding images. This allows us to improve the captioning model in a\nsupervised learning manner. Extensive experiments on the COCO and Flickr30K\ndatasets have been carried out to validate the superiority of the proposed\nmodel. We have achieved the state-of-the-art performance on the COCO dataset,\nwhich outperforms the best UIC model by 1.9% on the BLEU-4 metric. We expect\nthat the proposed prompt-based UIC model will inspire a new line of research\nfor the VL-PTMs based captioning.",
    "descriptor": "",
    "authors": [
      "Peipei Zhu",
      "Xiao Wang",
      "Lin Zhu",
      "Zhenglong Sun",
      "Weishi Zheng",
      "Yaowei Wang",
      "Changwen Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13125"
  },
  {
    "id": "arXiv:2205.13128",
    "title": "Cascading Residual Graph Convolutional Network for Multi-Behavior  Recommendation",
    "abstract": "Multi-behavior recommendation exploits multiple types of user-item\ninteractions to alleviate the data sparsity problem faced by the traditional\nmodels that often utilize only one type of interaction for recommendation. In\nreal scenarios, users often take a sequence of actions to interact with an\nitem, in order to get more information about the item and thus accurately\nevaluate whether an item fits personal preference. Those interaction behaviors\noften obey a certain order, and different behaviors reveal different\ninformation or aspects of user preferences towards the target item. Most\nexisting multi-behavior recommendation methods take the strategy to first\nextract information from different behaviors separately and then fuse them for\nfinal prediction. However, they have not exploited the connections between\ndifferent behaviors to learn user preferences. Besides, they often introduce\ncomplex model structures and more parameters to model multiple behaviors,\nlargely increasing the space and time complexity. In this work, we propose a\nlightweight multi-behavior recommendation model named Cascading Residual Graph\nConvolutional Network (CRGCN for short), which can explicitly exploit the\nconnections between different behaviors into the embedding learning process\nwithout introducing any additional parameters. In particular, we design a\ncascading residual graph convolutional network structure, which enables our\nmodel to learn user preferences by continuously refining user embeddings across\ndifferent types of behaviors. The multi-task learning method is adopted to\njointly optimize our model based on different behaviors. Extensive experimental\nresults on two real-world benchmark datasets show that CRGCN can substantially\noutperform state-of-the-art methods. Further studies also analyze the effects\nof leveraging multi-behaviors in different numbers and orders on the final\nperformance.",
    "descriptor": "",
    "authors": [
      "Mingshi Yan",
      "Zhiyong Cheng",
      "Chen Gao",
      "Jing Sun",
      "Fan Liu",
      "Fuming Sun",
      "Haojie Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.13128"
  },
  {
    "id": "arXiv:2205.13129",
    "title": "Wireless Deep Video Semantic Transmission",
    "abstract": "In this paper, we design a new class of high-efficiency deep joint\nsource-channel coding methods to achieve end-to-end video transmission over\nwireless channels. The proposed methods exploit nonlinear transform and\nconditional coding architecture to adaptively extract semantic features across\nvideo frames, and transmit semantic feature domain representations over\nwireless channels via deep joint source-channel coding. Our framework is\ncollected under the name deep video semantic transmission (DVST). In\nparticular, benefiting from the strong temporal prior provided by the feature\ndomain context, the learned nonlinear transform function becomes temporally\nadaptive, resulting in a richer and more accurate entropy model guiding the\ntransmission of current frame. Accordingly, a novel rate adaptive transmission\nmechanism is developed to customize deep joint source-channel coding for video\nsources. It learns to allocate the limited channel bandwidth within and among\nvideo frames to maximize the overall transmission performance. The whole DVST\ndesign is formulated as an optimization problem whose goal is to minimize the\nend-to-end transmission rate-distortion performance under perceptual quality\nmetrics or machine vision task performance metrics. Across standard video\nsource test sequences and various communication scenarios, experiments show\nthat our DVST can generally surpass traditional wireless video coded\ntransmission schemes. The proposed DVST framework can well support future\nsemantic communications due to its video content-aware and machine vision task\nintegration abilities.",
    "descriptor": "",
    "authors": [
      "Sixian Wang",
      "Jincheng Dai",
      "Zijian Liang",
      "Kai Niu",
      "Zhongwei Si",
      "Chao Dong",
      "Xiaoqi Qin",
      "Ping Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13129"
  },
  {
    "id": "arXiv:2205.13130",
    "title": "RACE: A Reinforcement Learning Framework for Improved Adaptive Control  of NoC Channel Buffers",
    "abstract": "Network-on-chip (NoC) architectures rely on buffers to store flits to cope\nwith contention for router resources during packet switching. Recently,\nreversible multi-function channel (RMC) buffers have been proposed to\nsimultaneously reduce power and enable adaptive NoC buffering between adjacent\nrouters. While adaptive buffering can improve NoC performance by maximizing\nbuffer utilization, controlling the RMC buffer allocations requires a\ncongestion-aware, scalable, and proactive policy. In this work, we present\nRACE, a novel reinforcement learning (RL) framework that utilizes better\nawareness of network congestion and a new reward metric (\"falsefulls\") to help\nguide the RL agent towards better RMC buffer control decisions. We show that\nRACE reduces NoC latency by up to 48.9%, and energy consumption by up to 47.1%\nagainst state-of-the-art NoC buffer control policies.",
    "descriptor": "\nComments: 6 pages, 8 figures, 3 tables\n",
    "authors": [
      "Kamil Khan",
      "Sudeep Pasricha",
      "Ryan Gary Kim"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13130"
  },
  {
    "id": "arXiv:2205.13131",
    "title": "On the Evolution of A.I. and Machine Learning: Towards Measuring and  Understanding Impact, Influence, and Leadership at Premier A.I. Conferences",
    "abstract": "Artificial Intelligence is now recognized as a general-purpose technology\nwith ample impact on human life. In this work, we aim to understand the\nevolution of AI and Machine learning over the years by analyzing researchers'\nimpact, influence, and leadership over the last decades. This work also intends\nto shed new light on the history and evolution of AI by exploring the dynamics\ninvolved in the field's evolution through the lenses of the papers published on\nAI conferences since the first International Joint Conference on Artificial\nIntelligence (IJCAI) in 1969. AI development and evolution have led to\nincreasing research output, reflected in the number of articles published over\nthe last sixty years. We construct comprehensive citation-collaboration and\npaper-author datasets and compute corresponding centrality measures to carry\nout our analyses. These analyses allow a better understanding of how AI has\nreached its current state of affairs in research. Throughout the process, we\ncorrelate these datasets with the work of the ACM Turing Award winners and the\nso-called two AI winters the field has gone through. We also look at\nself-citation trends and new authors' behaviors. Finally, we present a novel\nway to infer the country of affiliation of a paper from its organization.\nTherefore, this work provides a deep analysis of Artificial Intelligence\nhistory from information gathered and analyzed from large technical venues\ndatasets and suggests novel insights that can contribute to understanding and\nmeasuring AI's evolution.",
    "descriptor": "\nComments: 87 pages, 57 figures, 11 tables. Draft\n",
    "authors": [
      "Rafael B. Audibert",
      "Henrique Lemos",
      "Pedro Avelar",
      "Anderson R. Tavares",
      "Lu\u00eds C. Lamb"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13131"
  },
  {
    "id": "arXiv:2205.13133",
    "title": "Coverage Probability Analysis of RIS-Assisted High-Speed Train  Communications",
    "abstract": "Reconfigurable intelligent surface (RIS) has received increasing attention\ndue to its capability of extending cell coverage by reflecting signals toward\nreceivers. This paper considers a RIS-assisted high-speed train (HST)\ncommunication system to improve the coverage probability. We derive the\nclosed-form expression of coverage probability. Moreover, we analyze impacts of\nsome key system parameters, including transmission power, signal-to-noise ratio\nthreshold, and horizontal distance between base station and RIS. Simulation\nresults verify the efficiency of RIS-assisted HST communications in terms of\ncoverage probability.",
    "descriptor": "\nComments: 6 pages, 6 figures,submmited to GlobeCom 2022\n",
    "authors": [
      "Changzhu Liu",
      "Ruisi He",
      "Yong Niu",
      "Bo Ai",
      "Zhu Han",
      "Zhangfeng Ma",
      "Meilin Gao",
      "Zhangdui Zhong",
      "Ning Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13133"
  },
  {
    "id": "arXiv:2205.13134",
    "title": "Symbolic Physics Learner: Discovering governing equations via Monte  Carlo tree search",
    "abstract": "Nonlinear dynamics is ubiquitous in nature and commonly seen in various\nscience and engineering disciplines. Distilling analytical expressions that\ngovern nonlinear dynamics from limited data remains vital but challenging. To\ntackle this fundamental issue, we propose a novel Symbolic Physics Learner\n(SPL) machine to discover the mathematical structure of nonlinear dynamics. The\nkey concept is to interpret mathematical operations and system state variables\nby computational rules and symbols, establish symbolic reasoning of\nmathematical formulas via expression trees, and employ a Monte Carlo tree\nsearch (MCTS) agent to explore optimal expression trees based on measurement\ndata. The MCTS agent obtains an optimistic selection policy through the\ntraversal of expression trees, featuring the one that maps to the arithmetic\nexpression of underlying physics. Salient features of the proposed framework\ninclude search flexibility and enforcement of parsimony for discovered\nequations. The efficacy and superiority of the PSL machine are demonstrated by\nnumerical examples, compared with state-of-the-art baselines.",
    "descriptor": "\nComments: 12 pages for main document; 11 pages for supplementary material\n",
    "authors": [
      "Fangzheng Sun",
      "Yang Liu",
      "Jian-Xun Wang",
      "Hao Sun"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Symbolic Computation (cs.SC)",
      "Chaotic Dynamics (nlin.CD)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13134"
  },
  {
    "id": "arXiv:2205.13135",
    "title": "LAMP 2.0: A Robust Multi-Robot SLAM System for Operation in Challenging  Large-Scale Underground Environments",
    "abstract": "Search and rescue with a team of heterogeneous mobile robots in unknown and\nlarge-scale underground environments requires high-precision localization and\nmapping. This crucial requirement is faced with many challenges in complex and\nperceptually-degraded subterranean environments, as the onboard perception\nsystem is required to operate in off-nominal conditions (poor visibility due to\ndarkness and dust, rugged and muddy terrain, and the presence of self-similar\nand ambiguous scenes). In a disaster response scenario and in the absence of\nprior information about the environment, robots must rely on noisy sensor data\nand perform Simultaneous Localization and Mapping (SLAM) to build a 3D map of\nthe environment and localize themselves and potential survivors. To that end,\nthis paper reports on a multi-robot SLAM system developed by team CoSTAR in the\ncontext of the DARPA Subterranean Challenge. We extend our previous work, LAMP,\nby incorporating a single-robot front-end interface that is adaptable to\ndifferent odometry sources and lidar configurations, a scalable multi-robot\nfront-end to support inter- and intra-robot loop closure detection for large\nscale environments and multi-robot teams, and a robust back-end equipped with\nan outlier-resilient pose graph optimization based on Graduated Non-Convexity.\nWe provide a detailed ablation study on the multi-robot front-end and back-end,\nand assess the overall system performance in challenging real-world datasets\ncollected across mines, power plants, and caves in the United States. We also\nrelease our multi-robot back-end datasets (and the corresponding ground truth),\nwhich can serve as challenging benchmarks for large-scale underground SLAM.",
    "descriptor": "",
    "authors": [
      "Yun Chang",
      "Kamak Ebadi",
      "Christopher E. Denniston",
      "Muhammad Fadhil Ginting",
      "Antoni Rosinol",
      "Andrzej Reinke",
      "Matteo Palieri",
      "Jingnan Shi",
      "Arghya Chatterjee",
      "Benjamin Morrell",
      "Ali-akbar Agha-mohammadi",
      "Luca Carlone"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.13135"
  },
  {
    "id": "arXiv:2205.13137",
    "title": "MixMIM: Mixed and Masked Image Modeling for Efficient Visual  Representation Learning",
    "abstract": "In this study, we propose Mixed and Masked Image Modeling (MixMIM), a simple\nbut efficient MIM method that is applicable to various hierarchical Vision\nTransformers. Existing MIM methods replace a random subset of input tokens with\na special MASK symbol and aim at reconstructing original image tokens from the\ncorrupted image. However, we find that using the MASK symbol greatly slows down\nthe training and causes training-finetuning inconsistency, due to the large\nmasking ratio (e.g., 40% in BEiT). In contrast, we replace the masked tokens of\none image with visible tokens of another image, i.e., creating a mixed image.\nWe then conduct dual reconstruction to reconstruct the original two images from\nthe mixed input, which significantly improves efficiency. While MixMIM can be\napplied to various architectures, this paper explores a simpler but stronger\nhierarchical Transformer, and scales with MixMIM-B, -L, and -H. Empirical\nresults demonstrate that MixMIM can learn high-quality visual representations\nefficiently. Notably, MixMIM-B with 88M parameters achieves 85.1% top-1\naccuracy on ImageNet-1K by pretraining for 600 epochs, setting a new record for\nneural networks with comparable model sizes (e.g., ViT-B) among MIM methods.\nBesides, its transferring performances on the other 6 datasets show MixMIM has\nbetter FLOPs / performance tradeoff than previous MIM methods. Code is\navailable at https://github.com/Sense-X/MixMIM.",
    "descriptor": "\nComments: preprint. Code: this https URL\n",
    "authors": [
      "Jihao Liu",
      "Xin Huang",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13137"
  },
  {
    "id": "arXiv:2205.13139",
    "title": "Unsupervised Reinforcement Adaptation for Class-Imbalanced Text  Classification",
    "abstract": "Class imbalance naturally exists when train and test models in different\ndomains. Unsupervised domain adaptation (UDA) augments model performance with\nonly accessible annotations from the source domain and unlabeled data from the\ntarget domain. However, existing state-of-the-art UDA models learn\ndomain-invariant representations and evaluate primarily on class-balanced data\nacross domains. In this work, we propose an unsupervised domain adaptation\napproach via reinforcement learning that jointly leverages feature variants and\nimbalanced labels across domains. We experiment with the text classification\ntask for its easily accessible datasets and compare the proposed method with\nfive baselines. Experiments on three datasets prove that our proposed method\ncan effectively learn robust domain-invariant representations and successfully\nadapt text classifiers on imbalanced classes over domains. The code is\navailable at https://github.com/woqingdoua/ImbalanceClass.",
    "descriptor": "",
    "authors": [
      "Yuexin Wu",
      "Xiaolei Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13139"
  },
  {
    "id": "arXiv:2205.13145",
    "title": "Towards Syntactic Epistemic Logic",
    "abstract": "Traditionally, Epistemic Logic represents epistemic scenarios using a single\nmodel. This, however, covers only complete descriptions that specify truth\nvalues of all assertions. Indeed, many -- and perhaps most -- epistemic\ndescriptions are not complete. Syntactic Epistemic Logic, SEL, suggests viewing\nan epistemic situation as a set of syntactic conditions rather than as a model.\nThis allows us to naturally capture incomplete descriptions; we discuss a case\nstudy in which our proposal is successful. In Epistemic Game Theory, this\ncloses the conceptual and technical gap, identified by R. Aumann, between the\nsyntactic character of game-descriptions and semantic representations of games.",
    "descriptor": "",
    "authors": [
      "Sergei Artemov"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13145"
  },
  {
    "id": "arXiv:2205.13146",
    "title": "Grasping as Inference: Reactive Grasping in Heavily Cluttered  Environment",
    "abstract": "Although, in the task of grasping via a data-driven method, closed-loop\nfeedback and predicting 6 degrees of freedom (DoF) grasp rather than\nconventionally used 4DoF top-down grasp are demonstrated to improve performance\nindividually, few systems have both. Moreover, the sequential property of that\ntask is hardly dealt with, although the approaching motion necessarily\ngenerates a series of observations. Therefore, this paper synthesizes three\napproaches and suggests a closed-loop framework that can predict the 6DoF grasp\nin a heavily cluttered environment from continuously received vision\nobservations. This can be realized by formulating the grasping problem as\nHidden Markov Model and applying a particle filter to infer grasp.\nAdditionally, we introduce a novel lightweight Convolutional Neural Network\n(CNN) model that evaluates and initializes grasp samples in real-time, making\nthe particle filter process possible. The experiments, which are conducted on a\nreal robot with a heavily cluttered environment, show that our framework not\nonly quantitatively improves the grasping success rate significantly compared\nto the baseline algorithms, but also qualitatively reacts to a dynamic change\nin the environment and cleans up the table.",
    "descriptor": "",
    "authors": [
      "Dongwon Son"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13146"
  },
  {
    "id": "arXiv:2205.13147",
    "title": "Matryoshka Representations for Adaptive Deployment",
    "abstract": "Learned representations are a central component in modern ML systems, serving\na multitude of downstream tasks. When training such representations, it is\noften the case that computational and statistical constraints for each\ndownstream task are unknown. In this context rigid, fixed capacity\nrepresentations can be either over or under-accommodating to the task at hand.\nThis leads us to ask: can we design a flexible representation that can adapt to\nmultiple downstream tasks with varying computational resources? Our main\ncontribution is Matryoshka Representation Learning (MRL) which encodes\ninformation at different granularities and allows a single embedding to adapt\nto the computational constraints of downstream tasks. MRL minimally modifies\nexisting representation learning pipelines and imposes no additional cost\nduring inference and deployment. MRL learns coarse-to-fine representations that\nare at least as accurate and rich as independently trained low-dimensional\nrepresentations. The flexibility within the learned Matryoshka Representations\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\nlong-tail few-shot classification, all while being as robust as the original\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\nare open-sourced at https://github.com/RAIVNLab/MRL.",
    "descriptor": "\nComments: 32 pages, 11 figures\n",
    "authors": [
      "Aditya Kusupati",
      "Gantavya Bhatt",
      "Aniket Rege",
      "Matthew Wallingford",
      "Aditya Sinha",
      "Vivek Ramanujan",
      "William Howard-Snyder",
      "Kaifeng Chen",
      "Sham Kakade",
      "Prateek Jain",
      "Ali Farhadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13147"
  },
  {
    "id": "arXiv:2205.13148",
    "title": "Grammar Detection for Sentiment Analysis through Improved Viterbi  Algorithm",
    "abstract": "Grammar Detection, also referred to as Parts of Speech Tagging of raw text,\nis considered an underlying building block of the various Natural Language\nProcessing pipelines like named entity recognition, question answering, and\nsentiment analysis. In short, forgiven a sentence, Parts of Speech tagging is\nthe task of specifying and tagging each word of a sentence with nouns, verbs,\nadjectives, adverbs, and more. Sentiment Analysis may well be a procedure\naccustomed to determining if a given sentence's emotional tone is neutral,\npositive or negative. To assign polarity scores to the thesis or entities\nwithin phrase, in-text analysis and analytics, machine learning and natural\nlanguage processing, approaches are incorporated. This Sentiment Analysis using\nPOS tagger helps us urge a summary of the broader public over a specific topic.\nFor this, we are using the Viterbi algorithm, Hidden Markov Model, Constraint\nbased Viterbi algorithm for POS tagging. By comparing the accuracies, we select\nthe foremost accurate result of the model for Sentiment Analysis for\ndetermining the character of the sentence.",
    "descriptor": "",
    "authors": [
      "Surya Teja Chavali",
      "Charan Tej Kandavalli",
      "Sugash T M"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13148"
  },
  {
    "id": "arXiv:2205.13150",
    "title": "Semantically Supervised Appearance Decomposition for Virtual Staging  from a Single Panorama",
    "abstract": "We describe a novel approach to decompose a single panorama of an empty\nindoor environment into four appearance components: specular, direct sunlight,\ndiffuse and diffuse ambient without direct sunlight. Our system is weakly\nsupervised by automatically generated semantic maps (with floor, wall, ceiling,\nlamp, window and door labels) that have shown success on perspective views and\nare trained for panoramas using transfer learning without any further\nannotations. A GAN-based approach supervised by coarse information obtained\nfrom the semantic map extracts specular reflection and direct sunlight regions\non the floor and walls. These lighting effects are removed via a similar\nGAN-based approach and a semantic-aware inpainting step. The appearance\ndecomposition enables multiple applications including sun direction estimation,\nvirtual furniture insertion, floor material replacement, and sun direction\nchange, providing an effective tool for virtual home staging. We demonstrate\nthe effectiveness of our approach on a large and recently released dataset of\npanoramas of empty homes.",
    "descriptor": "\nComments: To appear in SIGGRAPH 2022\n",
    "authors": [
      "Tiancheng Zhi",
      "Bowei Chen",
      "Ivaylo Boyadzhiev",
      "Sing Bing Kang",
      "Martial Hebert",
      "Srinivasa G. Narasimhan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13150"
  },
  {
    "id": "arXiv:2205.13152",
    "title": "Transferable Adversarial Attack based on Integrated Gradients",
    "abstract": "The vulnerability of deep neural networks to adversarial examples has drawn\ntremendous attention from the community. Three approaches, optimizing standard\nobjective functions, exploiting attention maps, and smoothing decision\nsurfaces, are commonly used to craft adversarial examples. By tightly\nintegrating the three approaches, we propose a new and simple algorithm named\nTransferable Attack based on Integrated Gradients (TAIG) in this paper, which\ncan find highly transferable adversarial examples for black-box attacks. Unlike\nprevious methods using multiple computational terms or combining with other\nmethods, TAIG integrates the three approaches into one single term. Two\nversions of TAIG that compute their integrated gradients on a straight-line\npath and a random piecewise linear path are studied. Both versions offer strong\ntransferability and can seamlessly work together with the previous methods.\nExperimental results demonstrate that TAIG outperforms the state-of-the-art\nmethods. The code will available at https://github.com/yihuang2016/TAIG",
    "descriptor": "",
    "authors": [
      "Yi Huang",
      "Adams Wai-Kin Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13152"
  },
  {
    "id": "arXiv:2205.13154",
    "title": "Analyzing Image-based Political Propaganda in Referendum Campaigns: From  Elements to Strategies",
    "abstract": "With the increasing popularity of social network services, paradigm-shifting\nhas occurred in political communication. Politicians, candidates, and political\norganizations establish their fan pages to interact with online citizens.\nInitially, they publish text-only content on sites; then, they create\nmultimedia content such as photos, images, and videos to approach more people.\nThis paper takes a first look at image-based political propaganda during a\nnational referendum in Taiwan. Unlike elections, a referendum is a vote on\npolicies. We investigated more than 2,000 images posted on Facebook by the two\nmajor parties to understand the elements of images and the strategies of\npolitical organizations. In addition, we studied the data collection's textual\ncontent, objects, and colors. The results suggest the aspects of propaganda\nmaterials vary with different political organizations. However, the coloring\nstrategies are similar, using representative colors for consolidation and the\nopponent's colors for attacks.",
    "descriptor": "",
    "authors": [
      "Ming-Hung Wang",
      "Wei-Yang Chang",
      "Kuan-Hung Kuo",
      "Kuo-Yu Tsai"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.13154"
  },
  {
    "id": "arXiv:2205.13155",
    "title": "A Large Scale Study and Classification of VirusTotal Reports on Phishing  and Malware URLs",
    "abstract": "VirusTotal (VT) provides aggregated threat intelligence on various entities\nincluding URLs, IP addresses, and binaries. It is widely used by researchers\nand practitioners to collect ground truth and evaluate the maliciousness of\nentities. In this work, we provide a comprehensive analysis of VT URL scanning\nreports containing the results of 95 scanners for 1.577 Billion URLs over two\nyears. Individual VT scanners are known to be noisy in terms of their detection\nand attack type classification. To obtain high quality ground truth of URLs and\nactively take proper actions to mitigate different types of attacks, there are\ntwo challenges: (1) how to decide whether a given URL is malicious given noisy\nreports and (2) how to determine attack types (e.g., phishing or malware\nhosting) that the URL is involved in, given conflicting attack labels from\ndifferent scanners. In this work, we provide a systematic comparative study on\nthe behavior of VT scanners for different attack types of URLs. A common\npractice to decide the maliciousness is to use a cut-off threshold of scanners\nthat report the URL as malicious. However, in this work, we show that using a\nfixed threshold is suboptimal, due to several reasons: (1) correlations between\nscanners; (2) lead/lag behavior; (3) the specialty of scanners; (4) the quality\nand reliability of scanners. A common practice to determine an attack type is\nto use majority voting. However, we show that majority voting could not\naccurately classify the attack type of a URL due to the bias from correlated\nscanners. Instead, we propose a machine learning-based approach to assign an\nattack type to URLs given the VT reports.",
    "descriptor": "",
    "authors": [
      "Euijin Choo",
      "Mohamed Nabeel",
      "Ravindu De Silva",
      "Ting Yu",
      "Issa Khalil"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13155"
  },
  {
    "id": "arXiv:2205.13158",
    "title": "SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned  Distribution Perturbation",
    "abstract": "Data-driven approaches for medium-range weather forecasting are recently\nshown extraordinarily promising for ensemble forecasting for their fast\ninference speed compared to traditional numerical weather prediction (NWP)\nmodels, but their forecast accuracy can hardly match the state-of-the-art\noperational ECMWF Integrated Forecasting System (IFS) model. Previous\ndata-driven attempts achieve ensemble forecast using some simple perturbation\nmethods, like initial condition perturbation and Monte Carlo dropout. However,\nthey mostly suffer unsatisfactory ensemble performance, which is arguably\nattributed to the sub-optimal ways of applying perturbation. We propose a Swin\nTransformer-based Variational Recurrent Neural Network (SwinVRNN), which is a\nstochastic weather forecasting model combining a SwinRNN predictor with a\nperturbation module. SwinRNN is designed as a Swin Transformer-based recurrent\nneural network, which predicts future states deterministically. Furthermore, to\nmodel the stochasticity in prediction, we design a perturbation module\nfollowing the Variational Auto-Encoder paradigm to learn multivariate Gaussian\ndistributions of a time-variant stochastic latent variable from data. Ensemble\nforecasting can be easily achieved by perturbing the model features leveraging\nnoise sampled from the learned distribution. We also compare four categories of\nperturbation methods for ensemble forecasting, i.e. fixed distribution\nperturbation, learned distribution perturbation, MC dropout, and multi model\nensemble. Comparisons on WeatherBench dataset show the learned distribution\nperturbation method using our SwinVRNN model achieves superior forecast\naccuracy and reasonable ensemble spread due to joint optimization of the two\ntargets. More notably, SwinVRNN surpasses operational IFS on surface variables\nof 2-m temperature and 6-hourly total precipitation at all lead times up to\nfive days.",
    "descriptor": "",
    "authors": [
      "Yuan Hu",
      "Lei Chen",
      "Zhibin Wang",
      "Hao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13158"
  },
  {
    "id": "arXiv:2205.13159",
    "title": "HIRL: A General Framework for Hierarchical Image Representation Learning",
    "abstract": "Learning self-supervised image representations has been broadly studied to\nboost various visual understanding tasks. Existing methods typically learn a\nsingle level of image semantics like pairwise semantic similarity or image\nclustering patterns. However, these methods can hardly capture multiple levels\nof semantic information that naturally exists in an image dataset, e.g., the\nsemantic hierarchy of \"Persian cat to cat to mammal\" encoded in an image\ndatabase for species. It is thus unknown whether an arbitrary image\nself-supervised learning (SSL) approach can benefit from learning such\nhierarchical semantics. To answer this question, we propose a general framework\nfor Hierarchical Image Representation Learning (HIRL). This framework aims to\nlearn multiple semantic representations for each image, and these\nrepresentations are structured to encode image semantics from fine-grained to\ncoarse-grained. Based on a probabilistic factorization, HIRL learns the most\nfine-grained semantics by an off-the-shelf image SSL approach and learns\nmultiple coarse-grained semantics by a novel semantic path discrimination\nscheme. We adopt six representative image SSL methods as baselines and study\nhow they perform under HIRL. By rigorous fair comparison, performance gain is\nobserved on all the six methods for diverse downstream tasks, which, for the\nfirst time, verifies the general effectiveness of learning hierarchical image\nsemantics. All source code and model weights are available at\nhttps://github.com/hirl-team/HIRL",
    "descriptor": "\nComments: Research project paper. arXiv v1: all source code and model weights released\n",
    "authors": [
      "Minghao Xu",
      "Yuanfan Guo",
      "Xuanyu Zhu",
      "Jiawen Li",
      "Zhenbang Sun",
      "Jian Tang",
      "Yi Xu",
      "Bingbing Ni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13159"
  },
  {
    "id": "arXiv:2205.13160",
    "title": "Integration of Blockchain and Edge Computing in Internet of Things: A  Survey",
    "abstract": "As an important technology to ensure data security, consistency,\ntraceability, etc., blockchain has been increasingly used in Internet of Things\n(IoT) applications. The integration of blockchain and edge computing can\nfurther improve the resource utilization in terms of network, computing,\nstorage, and security. This paper aims to present a survey on the integration\nof blockchain and edge computing. In particular, we first give an overview of\nblockchain and edge computing. We then present a general architecture of an\nintegration of blockchain and edge computing system. We next study how to\nutilize blockchain to benefit edge computing, as well as how to use edge\ncomputing to benefit blockchain. We also discuss the issues brought by the\nintegration of blockchain and edge computing system and solutions from\nperspectives of resource management, joint optimization, data management,\ncomputation offloading and security mechanism. Finally, we analyze and\nsummarize the existing challenges posed by the integration of blockchain and\nedge computing system and the potential solutions in the future.",
    "descriptor": "",
    "authors": [
      "He Xue",
      "Dajiang Chen",
      "Ning Zhang",
      "Hong-Ning Dai",
      "Keping Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13160"
  },
  {
    "id": "arXiv:2205.13163",
    "title": "Cost-efficient Gaussian Tensor Network Embeddings for Tensor-structured  Inputs",
    "abstract": "This work discusses tensor network embeddings, which are random matrices\n($S$) with tensor network structure. These embeddings have been used to perform\ndimensionality reduction of tensor network structured inputs $x$ and accelerate\napplications such as tensor decomposition and kernel regression. Existing works\nhave designed embeddings for inputs $x$ with specific structures, such that the\ncomputational cost for calculating $Sx$ is efficient. We provide a systematic\nway to design tensor network embeddings consisting of Gaussian random tensors,\nsuch that for inputs with more general tensor network structures, both the\nsketch size (row size of $S$) and the sketching computational cost are low.\nWe analyze general tensor network embeddings that can be reduced to a\nsequence of sketching matrices. We provide a sufficient condition to quantify\nthe accuracy of such embeddings and derive sketching asymptotic cost lower\nbounds using embeddings that satisfy this condition and have a sketch size\nlower than any input dimension. We then provide an algorithm to efficiently\nsketch input data using such embeddings. The sketch size of the embedding used\nin the algorithm has a linear dependence on the number of sketching dimensions\nof the input. Assuming tensor contractions are performed with classical dense\nmatrix multiplication algorithms, this algorithm achieves asymptotic cost\nwithin a factor of $O(\\sqrt{m})$ of our cost lower bound, where $m$ is the\nsketch size. Further, when each tensor in the input has a dimension that needs\nto be sketched, this algorithm yields the optimal sketching asymptotic cost. We\napply our sketching analysis to inexact tensor decomposition optimization\nalgorithms. We provide a sketching algorithm for CP decomposition that is\nasymptotically faster than existing work in multiple regimes, and show\noptimality of an existing algorithm for tensor train rounding.",
    "descriptor": "",
    "authors": [
      "Linjian Ma",
      "Edgar Solomonik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13163"
  },
  {
    "id": "arXiv:2205.13164",
    "title": "Leveraging Dependency Grammar for Fine-Grained Offensive Language  Detection using Graph Convolutional Networks",
    "abstract": "The last few years have witnessed an exponential rise in the propagation of\noffensive text on social media. Identification of this text with high precision\nis crucial for the well-being of society. Most of the existing approaches tend\nto give high toxicity scores to innocuous statements (e.g., \"I am a gay man\").\nThese false positives result from over-generalization on the training data\nwhere specific terms in the statement may have been used in a pejorative sense\n(e.g., \"gay\"). Emphasis on such words alone can lead to discrimination against\nthe classes these systems are designed to protect. In this paper, we address\nthe problem of offensive language detection on Twitter, while also detecting\nthe type and the target of the offence. We propose a novel approach called\nSyLSTM, which integrates syntactic features in the form of the dependency parse\ntree of a sentence and semantic features in the form of word embeddings into a\ndeep learning architecture using a Graph Convolutional Network. Results show\nthat the proposed approach significantly outperforms the state-of-the-art BERT\nmodel with orders of magnitude fewer number of parameters.",
    "descriptor": "\nComments: 10 pages, 2 figures, accepted as a long paper at The 10th International Workshop on Natural Language Processing for Social Media (SocialNLP @ NAACL 2022)\n",
    "authors": [
      "Divyam Goel",
      "Raksha Sharma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13164"
  },
  {
    "id": "arXiv:2205.13165",
    "title": "Light Field Raindrop Removal via 4D Re-sampling",
    "abstract": "The Light Field Raindrop Removal (LFRR) aims to restore the background areas\nobscured by raindrops in the Light Field (LF). Compared with single image, the\nLF provides more abundant information by regularly and densely sampling the\nscene. Since raindrops have larger disparities than the background in the LF,\nthe majority of texture details occluded by raindrops are visible in other\nviews. In this paper, we propose a novel LFRR network by directly utilizing the\ncomplementary pixel information of raindrop-free areas in the input raindrop\nLF, which consists of the re-sampling module and the refinement module.\nSpecifically, the re-sampling module generates a new LF which is less polluted\nby raindrops through re-sampling position predictions and the proposed 4D\ninterpolation. The refinement module improves the restoration of the completely\noccluded background areas and corrects the pixel error caused by 4D\ninterpolation. Furthermore, we carefully build the first real scene LFRR\ndataset for model training and validation. Experiments demonstrate that the\nproposed method can effectively remove raindrops and achieves state-of-the-art\nperformance in both background restoration and view consistency maintenance.",
    "descriptor": "",
    "authors": [
      "Dong Jing",
      "Shuo Zhang",
      "Song Chang",
      "Youfang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13165"
  },
  {
    "id": "arXiv:2205.13169",
    "title": "Revisiting the Efficiency of Asynchronous Multi Party Computation  Against General Adversaries",
    "abstract": "In this paper, we design secure multi-party computation (MPC) protocols in\nthe asynchronous communication setting with optimal resilience. Our protocols\nare secure against a computationally-unbounded malicious adversary,\ncharacterized by an adversary structure $\\mathcal{Z}$, which enumerates all\npossible subsets of potentially corrupt parties. Our protocols incur a\ncommunication of $\\mathcal{O}(|\\mathcal{Z}|^2)$ and\n$\\mathcal{O}(|\\mathcal{Z}|)$ bits per multiplication for perfect and\nstatistical security respectively. These are the first protocols with this\ncommunication complexity, as such protocols were known only in the synchronous\ncommunication setting (Hirt and Tschudi, ASIACRYPT 2013).",
    "descriptor": "",
    "authors": [
      "Ananya Appan",
      "Anirudh Chandramouli",
      "Ashish Choudhury"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13169"
  },
  {
    "id": "arXiv:2205.13170",
    "title": "Distributed Contextual Linear Bandits with Minimax Optimal Communication  Cost",
    "abstract": "We study distributed contextual linear bandits with stochastic contexts,\nwhere $N$ agents act cooperatively to solve a linear bandit-optimization\nproblem with $d$-dimensional features. For this problem, we propose a\ndistributed batch elimination version of the LinUCB algorithm, DisBE-LUCB,\nwhere the agents share information among each other through a central server.\nWe prove that over $T$ rounds ($NT$ actions in total) the communication cost of\nDisBE-LUCB is only $\\tilde{\\mathcal{O}}(dN)$ and its regret is at most\n$\\tilde{\\mathcal{O}}(\\sqrt{dNT})$, which is of the same order as that incurred\nby an optimal single-agent algorithm for $NT$ rounds. Remarkably, we derive an\ninformation-theoretic lower bound on the communication cost of the distributed\ncontextual linear bandit problem with stochastic contexts, and prove that our\nproposed algorithm is nearly minimax optimal in terms of \\emph{both regret and\ncommunication cost}. Finally, we propose DecBE-LUCB, a fully decentralized\nversion of DisBE-LUCB, which operates without a central server, where agents\nshare information with their \\emph{immediate neighbors} through a carefully\ndesigned consensus procedure.",
    "descriptor": "",
    "authors": [
      "Sanae Amani",
      "Tor Lattimore",
      "Andr\u00e1s Gy\u00f6rgy",
      "Lin F. Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13170"
  },
  {
    "id": "arXiv:2205.13176",
    "title": "On Collective Robustness of Bagging Against Data Poisoning",
    "abstract": "Bootstrap aggregating (bagging) is an effective ensemble protocol, which is\nbelieved can enhance robustness by its majority voting mechanism. Recent works\nfurther prove the sample-wise robustness certificates for certain forms of\nbagging (e.g. partition aggregation). Beyond these particular forms, in this\npaper, \\emph{we propose the first collective certification for general bagging\nto compute the tight robustness against the global poisoning attack}.\nSpecifically, we compute the maximum number of simultaneously changed\npredictions via solving a binary integer linear programming (BILP) problem.\nThen we analyze the robustness of vanilla bagging and give the upper bound of\nthe tolerable poison budget. Based on this analysis, \\emph{we propose hash\nbagging} to improve the robustness of vanilla bagging almost for free. This is\nachieved by modifying the random subsampling in vanilla bagging to a hash-based\ndeterministic subsampling, as a way of controlling the influence scope for each\npoisoning sample universally. Our extensive experiments show the notable\nadvantage in terms of applicability and robustness.",
    "descriptor": "\nComments: Accepted by ICML22, code: this https URL\n",
    "authors": [
      "Ruoxin Chen",
      "Zenan Li",
      "Jie Li",
      "Chentao Wu",
      "Junchi Yan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13176"
  },
  {
    "id": "arXiv:2205.13178",
    "title": "Prototyping Next-Generation O-RAN Research Testbeds with SDRs",
    "abstract": "Open RAN (O-RAN) defines an emerging cellular radio access network (RAN)\narchitecture for future 6G wireless networks, emphasizing openness and\nintelligence which are considered the foundations of future 6G wireless\nnetworks. While the inherent complexity and flexibility of the RAN give rise to\nmany new research problems, progress in developing solutions is hampered due to\nthe lack of end-to-end, fully developed platforms that can help in pursuing use\ncases in realistic environments. This has motivated the formation of\nopen-source frameworks available to the wireless community. However, the rapid\nevolution of dedicated platforms and solutions utilizing various software-based\ntechnologies often leaves questions regarding the interoperability and\ninteractions between the components in the framework. This article shows how to\nbuild a software-defined radio testbed featuring an open-source 5G system that\ncan interact with the near-real-time (near-RT) RAN intelligent controller (RIC)\nof the O-RAN architecture through standard interfaces. We focus on the O-RAN E2\ninterface interactions and outline the procedure to enable a RAN system with E2\ncapabilities. We demonstrate the working of two xApps on the testbed with\ndetailed E2 message exchange procedures and their role in controlling\nnext-generation RANs.",
    "descriptor": "\nComments: This manuscript has been submitted to IEEE Vehicular Technology Magazine for possible publication\n",
    "authors": [
      "Pratheek S. Upadhyaya",
      "Aly S. Abdalla",
      "Vuk Marojevic",
      "Jeffrey H. Reed",
      "Vijay K. Shah"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13178"
  },
  {
    "id": "arXiv:2205.13182",
    "title": "Analyzing the Latent Space of GAN through Local Dimension Estimation",
    "abstract": "The impressive success of style-based GANs (StyleGANs) in high-fidelity image\nsynthesis has motivated research to understand the semantic properties of their\nlatent spaces. Recently, a close relationship was observed between the\nsemantically disentangled local perturbations and the local PCA components in\nthe learned latent space $\\mathcal{W}$. However, understanding the number of\ndisentangled perturbations remains challenging. Building upon this observation,\nwe propose a local dimension estimation algorithm for an arbitrary intermediate\nlayer in a pre-trained GAN model. The estimated intrinsic dimension corresponds\nto the number of disentangled local perturbations. In this perspective, we\nanalyze the intermediate layers of the mapping network in StyleGANs. Our\nanalysis clarifies the success of $\\mathcal{W}$-space in StyleGAN and suggests\nan alternative. Moreover, the intrinsic dimension estimation opens the\npossibility of unsupervised evaluation of global-basis-compatibility and\ndisentanglement for a latent space. Our proposed metric, called Distortion,\nmeasures an inconsistency of intrinsic tangent space on the learned latent\nspace. The metric is purely geometric and does not require any additional\nattribute information. Nevertheless, the metric shows a high correlation with\nthe global-basis-compatibility and supervised disentanglement score. Our\nfindings pave the way towards an unsupervised selection of globally\ndisentangled latent space among the intermediate latent spaces in a GAN.",
    "descriptor": "",
    "authors": [
      "Jaewoong Choi",
      "Geonho Hwang",
      "Hyunsoo Cho",
      "Myungjoo Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13182"
  },
  {
    "id": "arXiv:2205.13183",
    "title": "Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach",
    "abstract": "Pre-trained models (PTMs) have lead to great improvements in natural language\ngeneration (NLG). However, it is still unclear how much commonsense knowledge\nthey possess. With the goal of evaluating commonsense knowledge of NLG models,\nrecent work has proposed the problem of generative commonsense reasoning, e.g.,\nto compose a logical sentence given a set of unordered concepts. Existing\napproaches to this problem hypothesize that PTMs lack sufficient parametric\nknowledge for this task, which can be overcome by introducing external\nknowledge or task-specific pre-training objectives. Different from this trend,\nwe argue that PTM's inherent ability for generative commonsense reasoning is\nunderestimated due to the order-agnostic property of its input. In particular,\nwe hypothesize that the order of the input concepts can affect the PTM's\nability to utilize its commonsense knowledge. To this end, we propose a\npre-ordering approach to elaborately manipulate the order of the given concepts\nbefore generation. Experiments show that our approach can outperform the more\nsophisticated models that have access to a lot of external data and resources.",
    "descriptor": "\nComments: NAACL 2022 Findings\n",
    "authors": [
      "Chao Zhao",
      "Faeze Brahman",
      "Tenghao Huang",
      "Snigdha Chaturvedi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13183"
  },
  {
    "id": "arXiv:2205.13187",
    "title": "An Acceleration of Fixed Point Iterations for M/G/1-type Markov Chains  by Means of Relaxation Techniques",
    "abstract": "We present some accelerated variants of fixed point iterations for computing\nthe minimal non-negative solution of the unilateral matrix equation associated\nwith an M/G/1-type Markov chain. These schemes derive from certain staircase\nregular splittings of the block Hessenberg M-matrix associated with the Markov\nchain. By exploiting the staircase profile we introduce a two-step fixed point\niteration. The iteration can be further accelerated by computing a weighted\naverage between the approximations obtained in two consecutive steps. The\nconvergence of the basic two-step fixed point iteration and of its relaxed\nmodification is proved. Our theoretical analysis along with several numerical\nexperiments show that the proposed variants generally outperform the classical\niterations.",
    "descriptor": "",
    "authors": [
      "Luca Gemignani",
      "Beatrice Meini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13187"
  },
  {
    "id": "arXiv:2205.13189",
    "title": "AI for Porosity and Permeability Prediction from Geologic Core X-Ray  Micro-Tomography",
    "abstract": "Geologic cores are rock samples that are extracted from deep under the ground\nduring the well drilling process. They are used for petroleum reservoirs'\nperformance characterization. Traditionally, physical studies of cores are\ncarried out by the means of manual time-consuming experiments. With the\ndevelopment of deep learning, scientists actively started working on developing\nmachine-learning-based approaches to identify physical properties without any\nmanual experiments. Several previous works used machine learning to determine\nthe porosity and permeability of the rocks, but either method was inaccurate or\ncomputationally expensive. We are proposing to use self-supervised pretraining\nof the very small CNN-transformer-based model to predict the physical\nproperties of the rocks with high accuracy in a time-efficient manner. We show\nthat this technique prevents overfitting even for extremely small datasets.",
    "descriptor": "",
    "authors": [
      "Zangir Iklassov",
      "Dmitrii Medvedev",
      "Otabek Nazarov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13189"
  },
  {
    "id": "arXiv:2205.13190",
    "title": "Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via  Role Interactions",
    "abstract": "Role-oriented dialogue summarization is to generate summaries for different\nroles in the dialogue, e.g., merchants and consumers. Existing methods handle\nthis task by summarizing each role's content separately and thus are prone to\nignore the information from other roles. However, we believe that other roles'\ncontent could benefit the quality of summaries, such as the omitted information\nmentioned by other roles. Therefore, we propose a novel role interaction\nenhanced method for role-oriented dialogue summarization. It adopts cross\nattention and decoder self-attention interactions to interactively acquire\nother roles' critical information. The cross attention interaction aims to\nselect other roles' critical dialogue utterances, while the decoder\nself-attention interaction aims to obtain key information from other roles'\nsummaries. Experimental results have shown that our proposed method\nsignificantly outperforms strong baselines on two public role-oriented dialogue\nsummarization datasets. Extensive analyses have demonstrated that other roles'\ncontent could help generate summaries with more complete semantics and correct\ntopic structures.",
    "descriptor": "\nComments: Accepted by ACL 2022 main conference\n",
    "authors": [
      "Haitao Lin",
      "Junnan Zhu",
      "Lu Xiang",
      "Yu Zhou",
      "Jiajun Zhang",
      "Chengqing Zong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13190"
  },
  {
    "id": "arXiv:2205.13191",
    "title": "Orthogonal Stochastic Configuration Networks with Adaptive Construction  Parameter for Data Analytics",
    "abstract": "As a randomized learner model, SCNs are remarkable that the random weights\nand biases are assigned employing a supervisory mechanism to ensure universal\napproximation and fast learning. However, the randomness makes SCNs more likely\nto generate approximate linear correlative nodes that are redundant and low\nquality, thereby resulting in non-compact network structure. In the light of a\nfundamental principle in machine learning, that is, a model with fewer\nparameters holds improved generalization. This paper proposes orthogonal SCN,\ntermed OSCN, to filtrate out the low-quality hidden nodes for network structure\nreduction by incorporating Gram-Schmidt orthogonalization technology. The\nuniversal approximation property of OSCN and an adaptive setting for the key\nconstruction parameters have been presented in details. In addition, an\nincremental updating scheme is developed to dynamically determine the output\nweights, contributing to improved computational efficiency. Finally,\nexperimental results on two numerical examples and several real-world\nregression and classification datasets substantiate the effectiveness and\nfeasibility of the proposed approach.",
    "descriptor": "",
    "authors": [
      "Wei Dai",
      "Chuanfeng Ning",
      "Shiyu Pei",
      "Song Zhu",
      "Xuesong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13191"
  },
  {
    "id": "arXiv:2205.13192",
    "title": "Tree Reconstruction using Topology Optimisation",
    "abstract": "Generating accurate digital tree models from scanned environments is\ninvaluable for forestry, agriculture, and other outdoor industries in tasks\nsuch as identifying biomass, fall hazards and traversability, as well as\ndigital applications such as animation and gaming. Existing methods for tree\nreconstruction rely on feature identification (trunk, crown, etc) to\nheuristically segment a forest into individual trees and generate a branch\nstructure graph, limiting their application to sparse trees and uniform\nforests. However, the natural world is a messy place in which trees present\nwith significant heterogeneity and are frequently encroached upon by the\nsurrounding environment. We present a general method for extracting the branch\nstructure of trees from point cloud data, which estimates the structure of\ntrees by adapting the methods of structural topology optimisation to find the\noptimal material distribution to support wind-loading. We present the results\nof this optimisation over a wide variety of scans, and discuss the benefits and\ndrawbacks of this novel approach to tree structure reconstruction. Despite the\nhigh variability of datasets containing trees, and the high rate of occlusions,\nour method generates detailed and accurate tree structures in most cases.",
    "descriptor": "\nComments: The datasets generated and used in the current study are available in the Tree Reconstructions from Pointclouds Scanned in Pullenvale QLD repository, this https URL\n",
    "authors": [
      "Thomas Lowe",
      "Joshua Pinskier"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13192"
  },
  {
    "id": "arXiv:2205.13196",
    "title": "The Opportunity to Regulate Cybersecurity in the EU (and the World):  Recommendations for the Cybersecurity Resilience Act",
    "abstract": "Safety is becoming cybersecurity under most circumstances. This should be\nreflected in the Cybersecurity Resilience Act when it is proposed and agreed\nupon in the European Union. In this paper, we define a range of principles\nwhich this future Act should build upon, a structure and argue why it should be\nas broad as possible. It is based on what the cybersecurity research community\nfor long have asked for, and on what constitutes clear hard legal rules instead\nof soft. Important areas such as cybersecurity should be taken seriously, by\nregulating it in the same way we see other types of critical infrastructure and\nphysical structures, and be uncompromising and logical, to encompass the risks\nand potential for chaos which its ubiquitous nature entails.\nWe find that principles which regulate cybersecurity systems' life-cycles in\ndetail are needed, as is clearly stating what technology is being used, due to\nKirkhoffs principle, and dismissing the idea of technosolutionism. Furthermore,\ncarefully analysing risks is always necessary, but so is understanding when and\nhow the systems manufacturers may fail or almost fail. We do this through the\nfollowing principles:\nEx ante and Ex post assessment, Safety and Security by Design, Denial of\nObscurity, Dismissal of Infallibility, Systems Acknowledgement, Full\nTransparency, Movement towards a Zero-trust Security Model, Cybersecurity\nResilience, Enforced Circular Risk Management, Dependability, Hazard Analysis\nand mitigation or limitation, liability, A Clear Reporting Regime, Enforcement\nof Certification and Standards, Mandated Verification of Security and\nContinuous Servicing.\nTo this, we suggest that the Act employs similar authorities and mechanisms\nas the GDPR and create strong national authorities to coordinate inspection and\nenforcement in each Member State, with ENISA being the top and coordinating\norgan.",
    "descriptor": "\nComments: 23 pages, 2 figures\n",
    "authors": [
      "Kaspar Rosager Ludvigsen",
      "Shishir Nagaraja"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13196"
  },
  {
    "id": "arXiv:2205.13198",
    "title": "Constellation Design for Non-Coherent Fast-Forward Relays to Mitigate  Full-Duplex Jamming Attacks",
    "abstract": "With potential applications to short-packet communication, we address\ncommunication of low-latency messages in fast-fading channels under the\npresence of a reactive jammer. Unlike a traditional jammer, we assume a\nfull-duplex (FD) jammer capable of detecting pre-existing countermeasures and\nsubsequently changing the target frequency band. To facilitate reliable\ncommunication amidst a strong adversary, we propose non-coherent fast-forward\nfull-duplex relaying scheme wherein the victim uses a helper in its vicinity to\nfast-forward its messages to the base station, in addition to ensuring that the\ncountermeasures are undetected by the FD adversary. Towards designing the\nconstellations for the proposed scheme, we identify that existing non-coherent\nconstellation for fast-fading channels are not applicable owing to the\ncooperative nature of the fast-forward scheme. As a result, we formulate an\noptimization problem of designing the non-coherent constellations at the victim\nand the helper such that the symbol-error-probability at the base station is\nminimized. We theoretically analyze the optimization problem and propose\nseveral strategies to compute near-optimal constellations based on the helper's\ndata-rate and fast-forwarding abilities. We show that the proposed\nconstellations provide near-optimal error performance and help the victim evade\njamming. Finally, we also prove the scheme's efficacy in deceiving the\ncountermeasure detectors at the jammer.",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Communications\n",
    "authors": [
      "Vivek Chaudhary",
      "J. Harshan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13198"
  },
  {
    "id": "arXiv:2205.13199",
    "title": "Decoupled Pyramid Correlation Network for Liver Tumor Segmentation from  CT images",
    "abstract": "Purpose: Automated liver tumor segmentation from Computed Tomography (CT)\nimages is a necessary prerequisite in the interventions of hepatic\nabnormalities and surgery planning. However, accurate liver tumor segmentation\nremains challenging due to the large variability of tumor sizes and\ninhomogeneous texture. Recent advances based on Fully Convolutional Network\n(FCN) for medical image segmentation drew on the success of learning\ndiscriminative pyramid features. In this paper, we propose a Decoupled Pyramid\nCorrelation Network (DPC-Net) that exploits attention mechanisms to fully\nleverage both low- and high-level features embedded in FCN to segment liver\ntumor. Methods: We first design a powerful Pyramid Feature Encoder (PFE) to\nextract multi-level features from input images. Then we decouple the\ncharacteristics of features concerning spatial dimension (i.e., height, width,\ndepth) and semantic dimension (i.e., channel). On top of that, we present two\ntypes of attention modules, Spatial Correlation (SpaCor) and Semantic\nCorrelation (SemCor) modules, to recursively measure the correlation of\nmulti-level features. The former selectively emphasizes global semantic\ninformation in low-level features with the guidance of high-level ones. The\nlatter adaptively enhance spatial details in high-level features with the\nguidance of low-level ones. Results: We evaluate the DPC-Net on MICCAI 2017\nLiTS Liver Tumor Segmentation (LiTS) challenge dataset. Dice Similarity\nCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) are employed\nfor evaluation. The proposed method obtains a DSC of 76.4% and an ASSD of 0.838\nmm for liver tumor segmentation, outperforming the state-of-the-art methods. It\nalso achieves a competitive results with a DSC of 96.0% and an ASSD of 1.636 mm\nfor liver segmentation.",
    "descriptor": "\nComments: 29 pages, 7 figures, 5 tables\n",
    "authors": [
      "Yao Zhang",
      "Jiawei Yang",
      "Yang Liu",
      "Jiang Tian",
      "Siyun Wang",
      "Cheng Zhong",
      "Zhongchao Shi",
      "Yang Zhang",
      "Zhiqiang He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13199"
  },
  {
    "id": "arXiv:2205.13202",
    "title": "More Recent Advances in (Hyper)Graph Partitioning",
    "abstract": "In recent years, significant advances have been made in the design and\nevaluation of balanced (hyper)graph partitioning algorithms. We survey trends\nof the last decade in practical algorithms for balanced (hyper)graph\npartitioning together with future research directions. Our work serves as an\nupdate to a previous survey on the topic. In particular, the survey extends the\nprevious survey by also covering hypergraph partitioning and streaming\nalgorithms, and has an additional focus on parallel algorithms.",
    "descriptor": "",
    "authors": [
      "\u00dcmit V. \u00c7ataly\u00fcrek",
      "Karen D. Devine",
      "Marcelo Fonseca Faraj",
      "Lars Gottesb\u00fcrren",
      "Tobias Heuer",
      "Henning Meyerhenke",
      "Peter Sanders",
      "Sebastian Schlag",
      "Christian Schulz",
      "Daniel Seemaier",
      "Dorothea Wagner"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13202"
  },
  {
    "id": "arXiv:2205.13205",
    "title": "$O(N^2)$ Universal Antisymmetry in Fermionic Neural Networks",
    "abstract": "Fermionic neural network (FermiNet) is a recently proposed wavefunction\nAnsatz, which is used in variational Monte Carlo (VMC) methods to solve the\nmany-electron Schr\\\"odinger equation. FermiNet proposes permutation-equivariant\narchitectures, on which a Slater determinant is applied to induce antisymmetry.\nFermiNet is proved to have universal approximation capability with a single\ndeterminant, namely, it suffices to represent any antisymmetric function given\nsufficient parameters. However, the asymptotic computational bottleneck comes\nfrom the Slater determinant, which scales with $O(N^3)$ for $N$ electrons. In\nthis paper, we substitute the Slater determinant with a pairwise antisymmetry\nconstruction, which is easy to implement and can reduce the computational cost\nto $O(N^2)$. Furthermore, we formally prove that the pairwise construction\nbuilt upon permutation-equivariant architectures can universally represent any\nantisymmetric function.",
    "descriptor": "",
    "authors": [
      "Tianyu Pang",
      "Shuicheng Yan",
      "Min Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13205"
  },
  {
    "id": "arXiv:2205.13206",
    "title": "Populating the Digital Space for Cultural Heritage with Heritage Digital  Twins",
    "abstract": "The present paper concerns the design of the semantic infrastructure of the\ndigital space for cultural heritage as envisaged by the European Commission in\nits recent documents. Due to the complexity of the cultural heritage data and\nof their intrinsic interrelationships, it is necessary to introduce a novel\nontology, yet compliant with existing standards and interoperable with previous\nplatforms used in this context, such as Europeana. The digital space\norganization must be tailored to the methods and the theory of cultural\nheritage, briefly summarized in the introduction. The new ontology is based on\nthe Digital Twin concept, i.e. the digital counterpart of cultural heritage\nassets incorporating all the digital information pertaining to them. This\ncreates a Knowledge Base on the cultural heritage digital space. The paper\noutlines the main features of the proposed Heritage Digital Twin ontology and\nprovides some examples of application. Future work will include completing the\nontology in all its details and testing it in other real cases and with the\nvarious sectors of the cultural heritage community.",
    "descriptor": "\nComments: Submitted to Data - An Open Access Journal from MDPI. 29 pages, 9 figures\n",
    "authors": [
      "Franco Niccolucci",
      "Achille Felicetti",
      "Sorin Hermon"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.13206"
  },
  {
    "id": "arXiv:2205.13209",
    "title": "Sym-NCO: Leveraging Symmetricity for Neural Combinatorial Optimization",
    "abstract": "Deep reinforcement learning (DRL)-based combinatorial optimization (CO)\nmethods (i.e., DRL-NCO) have shown significant merit over the conventional CO\nsolvers as DRL-NCO is capable of learning CO solvers without supervised labels\nattained from the verified solver. This paper presents a novel training scheme,\nSym-NCO, that achieves significant performance increments to existing DRL-NCO\nmethods. Sym-NCO is a regularizer-based training scheme that leverages\nuniversal symmetricities in various CO problems and solutions. Imposing\nsymmetricities such as rotational and reflectional invariance can greatly\nimprove generalization capability of DRL-NCO as symmetricities are invariant\nfeatures shared by certain CO tasks. Our experimental results verify that our\nSym-NCO greatly improves the performance of DRL-NCO methods in four CO tasks,\nincluding traveling salesman problem (TSP), capacitated vehicle routing problem\n(CVRP), prize collecting TSP (PCTSP), and orienteering problem (OP), without\nemploying problem-specific techniques. Remarkably, Sym-NCO outperformed not\nonly the existing DRL-NCO methods but also a competitive conventional solver,\nthe iterative local search (ILS), in PCTSP at 240 times faster speed.",
    "descriptor": "\nComments: 18 pages, 7 figures\n",
    "authors": [
      "Minsu Kim",
      "Junyoung Park",
      "Jinkyoo Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13209"
  },
  {
    "id": "arXiv:2205.13213",
    "title": "Fast Vision Transformers with HiLo Attention",
    "abstract": "Vision Transformers (ViTs) have triggered the most recent and significant\nbreakthroughs in computer vision. Their efficient designs are mostly guided by\nthe indirect metric of computational complexity, i.e., FLOPs, which however has\na clear gap with the direct metric such as throughput. Thus, we propose to use\nthe direct speed evaluation on the target platform as the design principle for\nefficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT\nwhich performs favourably against the existing state-of-the-art methods across\na spectrum of different model sizes with faster speed. At the core of LITv2 is\na novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the\ninsight that high frequencies in an image capture local fine details and low\nfrequencies focus on global structures, whereas a multi-head self-attention\nlayer neglects the characteristic of different frequencies. Therefore, we\npropose to disentangle the high/low frequency patterns in an attention layer by\nseparating the heads into two groups, where one group encodes high frequencies\nvia self-attention within each local window, and another group performs the\nattention to model the global relationship between the average-pooled\nlow-frequency keys from each window and each query position in the input\nfeature map. Benefit from the efficient design for both groups, we show that\nHiLo is superior to the existing attention mechanisms by comprehensively\nbenchmarking on FLOPs, speed and memory consumption on GPUs. Powered by HiLo,\nLITv2 serves as a strong backbone for mainstream vision tasks including image\nclassification, dense detection and segmentation. Code is available at\nhttps://github.com/zip-group/LITv2.",
    "descriptor": "\nComments: Tech report\n",
    "authors": [
      "Zizheng Pan",
      "Jianfei Cai",
      "Bohan Zhuang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13213"
  },
  {
    "id": "arXiv:2205.13214",
    "title": "SymNMF-Net for The Symmetric NMF Problem",
    "abstract": "Recently, many works have demonstrated that Symmetric Non-negative Matrix\nFactorization~(SymNMF) enjoys a great superiority for various clustering tasks.\nAlthough the state-of-the-art algorithms for SymNMF perform well on synthetic\ndata, they cannot consistently obtain satisfactory results with desirable\nproperties and may fail on real-world tasks like clustering. Considering the\nflexibility and strong representation ability of the neural network, in this\npaper, we propose a neural network called SymNMF-Net for the Symmetric NMF\nproblem to overcome the shortcomings of traditional optimization algorithms.\nEach block of SymNMF-Net is a differentiable architecture with an inversion\nlayer, a linear layer and ReLU, which are inspired by a traditional update\nscheme for SymNMF. We show that the inference of each block corresponds to a\nsingle iteration of the optimization. Furthermore, we analyze the constraints\nof the inversion layer to ensure the output stability of the network to a\ncertain extent. Empirical results on real-world datasets demonstrate the\nsuperiority of our SymNMF-Net and confirm the sufficiency of our theoretical\nanalysis.",
    "descriptor": "",
    "authors": [
      "Mingjie Li",
      "Hao Kong",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13214"
  },
  {
    "id": "arXiv:2205.13216",
    "title": "Aggregating Gradients in Encoded Domain for Federated Learning",
    "abstract": "Malicious attackers and an honest-but-curious server can steal private client\ndata from uploaded gradients in federated learning. Although current protection\nmethods (e.g., additive homomorphic cryptosystem) can guarantee the security of\nthe federated learning system, they bring additional computation and\ncommunication costs. To mitigate the cost, we propose the \\texttt{FedAGE}\nframework, which enables the server to aggregate gradients in an encoded domain\nwithout accessing raw gradients of any single client. Thus, \\texttt{FedAGE} can\nprevent the curious server from gradient stealing while maintaining the same\nprediction performance without additional communication costs. Furthermore, we\ntheoretically prove that the proposed encoding-decoding framework is a Gaussian\nmechanism for differential privacy. Finally, we evaluate \\texttt{FedAGE} under\nseveral federated settings, and the results have demonstrated the efficacy of\nthe proposed framework.",
    "descriptor": "",
    "authors": [
      "Dun Zeng",
      "Shiyu Liu",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13216"
  },
  {
    "id": "arXiv:2205.13218",
    "title": "A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental  Learning",
    "abstract": "Real-world applications require the classification model to adapt to new\nclasses without forgetting old ones. Correspondingly, Class-Incremental\nLearning (CIL) aims to train a model with limited memory size to meet this\nrequirement. Typical CIL methods tend to save representative exemplars from\nformer classes to resist forgetting, while recent works find that storing\nmodels from history can substantially boost the performance. However, the\nstored models are not counted into the memory budget, which implicitly results\nin unfair comparisons. We find that when counting the model size into the total\nbudget and comparing methods with aligned memory size, saving models do not\nconsistently work, especially for the case with limited memory budgets. As a\nresult, we need to holistically evaluate different CIL methods at different\nmemory scales and simultaneously consider accuracy and memory size for\nmeasurement. On the other hand, we dive deeply into the construction of the\nmemory buffer for memory efficiency. By analyzing the effect of different\nlayers in the network, we find that shallow and deep layers have different\ncharacteristics in CIL. Motivated by this, we propose a simple yet effective\nbaseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends\nspecialized layers based on the shared generalized representations, efficiently\nextracting diverse representations with modest cost and maintaining\nrepresentative exemplars. Extensive experiments on benchmark datasets validate\nMEMO's competitive performance.",
    "descriptor": "",
    "authors": [
      "Da-Wei Zhou",
      "Qi-Wei Wang",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13218"
  },
  {
    "id": "arXiv:2205.13219",
    "title": "Penalizing Proposals using Classifiers for Semi-Supervised Object  Detection",
    "abstract": "Obtaining gold standard annotated data for object detection is often costly,\ninvolving human-level effort. Semi-supervised object detection algorithms solve\nthe problem with a small amount of gold-standard labels and a large unlabelled\ndataset used to generate silver-standard labels. But training on the silver\nstandard labels does not produce good results, because they are\nmachine-generated annotations. In this work, we design a modified loss function\nto train on large silver standard annotated sets generated by a weak annotator.\nWe include a confidence metric associated with the annotation as an additional\nterm in the loss function, signifying the quality of the annotation. We test\nthe effectiveness of our approach on various test sets and use numerous\nvariations to compare the results with some of the current approaches to object\ndetection. In comparison with the baseline where no confidence metric is used,\nwe achieved a 4\\% gain in mAP with 25\\% labeled data and 10\\% gain in mAP with\n50\\% labeled data by using the proposed confidence metric.",
    "descriptor": "",
    "authors": [
      "Somnath Hazra",
      "Pallab Dasgupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13219"
  },
  {
    "id": "arXiv:2205.13220",
    "title": "DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph",
    "abstract": "Dynamic graph visualization attracts researchers' concentration as it\nrepresents time-varying relationships between entities in multiple domains\n(e.g., social media analysis, academic cooperation analysis, team sports\nanalysis). Integrating visual analytic methods is consequential in presenting,\ncomparing, and reviewing dynamic graphs. Even though dynamic graph\nvisualization is developed for many years, how to effectively visualize\nlarge-scale and time-intensive dynamic graph data with subtle changes is still\nchallenging for researchers. To provide an effective analysis method for this\ntype of dynamic graph data, we propose a snapshot generation algorithm\ninvolving Human-In-Loop to help users divide the dynamic graphs into\nmulti-granularity and hierarchical snapshots for further analysis. In addition,\nwe design a visual analysis prototype system (DGSVis) to assist users in\naccessing the dynamic graph insights effectively. DGSVis integrates a graphical\noperation interface to help users generate snapshots visually and\ninteractively. It is equipped with the overview and details for visualizing\nhierarchical snapshots of the dynamic graph data. To illustrate the usability\nand efficiency of our proposed methods for this type of dynamic graph data, we\nintroduce two case studies based on basketball player networks in a\ncompetition. In addition, we conduct an evaluation and receive exciting\nfeedback from experienced visualization experts.",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Baofeng Chang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13220"
  },
  {
    "id": "arXiv:2205.13222",
    "title": "Friends to Help: Saving Federated Learning from Client Dropout",
    "abstract": "Federated learning (FL) is an outstanding distributed machine learning\nframework due to its benefits on data privacy and communication efficiency.\nSince full client participation in many cases is infeasible due to constrained\nresources, partial participation FL algorithms have been investigated that\nproactively select/sample a subset of clients, aiming to achieve learning\nperformance close to the full participation case. This paper studies a passive\npartial client participation scenario that is much less well understood, where\npartial participation is a result of external events, namely client dropout,\nrather than a decision of the FL algorithm. We cast FL with client dropout as a\nspecial case of a larger class of FL problems where clients can submit\nsubstitute (possibly inaccurate) local model updates. Based on our convergence\nanalysis, we develop a new algorithm FL-FDMS that discovers friends of clients\n(i.e., clients whose data distributions are similar) on-the-fly and uses\nfriends' local updates as substitutes for the dropout clients, thereby reducing\nthe substitution error and improving the convergence performance. A complexity\nreduction mechanism is also incorporated into FL-FDMS, making it both\ntheoretically sound and practically useful. Experiments on MNIST and CIFAR-10\nconfirmed the superior performance of FL-FDMS in handling client dropout in FL.",
    "descriptor": "",
    "authors": [
      "Heqiang Wang",
      "Jie Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13222"
  },
  {
    "id": "arXiv:2205.13225",
    "title": "Collaborative Distillation Meta Learning for Simulation Intensive  Hardware Design",
    "abstract": "This paper proposes a novel collaborative distillation meta learning (CDML)\nframework for simulation intensive hardware design problems. Deep reinforcement\nlearning (DRL) has shown promising performance in various hardware design\nproblems. However, previous works on DRL-based hardware design only dealt with\nproblems with simplified objectives, which are not practical. In fact, the\nobjective evaluation of real-world electrical performance through simulation is\ncostly in terms of both time and computation, making DRL scheme involving\nextensive reward calculations not suitable. In this paper, we apply the CDML\nframework to decoupling capacitor placement problem (DPP), one of the\nsignificant simulation intensive hardware design problems. The CDML framework\nconsists of a context-based meta learner and collaborative distillation scheme\nto produce a reusable solver. The context-based meta learner captures the\nlocation of probing port (i.e., target circuit block) and improves\ngeneralization capability. The collaborative distillation scheme with\nequivariant label transformation imposes the action-permutation\n(AP)-equivariant nature of placement problems, which not only improves sample\nefficiency but also improves generalization capability. Extensive experimental\nresults verified that our CDML outperforms both neural baselines and iterative\nconventional design methods in terms of real-world objective, power integrity,\nwith zero-shot transfer-ability.",
    "descriptor": "\nComments: 29 pages, 19 figures\n",
    "authors": [
      "Haeyeon Kim",
      "Minsu Kim",
      "Joungho Kim",
      "Jinkyoo Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13225"
  },
  {
    "id": "arXiv:2205.13226",
    "title": "Censor-aware Semi-supervised Learning for Survival Time Prediction from  Medical Images",
    "abstract": "Survival time prediction from medical images is important for treatment\nplanning, where accurate estimations can improve healthcare quality. One issue\naffecting the training of survival models is censored data. Most of the current\nsurvival prediction approaches are based on Cox models that can deal with\ncensored data, but their application scope is limited because they output a\nhazard function instead of a survival time. On the other hand, methods that\npredict survival time usually ignore censored data, resulting in an\nunder-utilization of the training set. In this work, we propose a new training\nmethod that predicts survival time using all censored and uncensored data. We\npropose to treat censored data as samples with a lower-bound time to death and\nestimate pseudo labels to semi-supervise a censor-aware survival time\nregressor. We evaluate our method on pathology and x-ray images from the\nTCGA-GM and NLST datasets. Our results establish the state-of-the-art survival\nprediction accuracy on both datasets.",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Renato Hermoza",
      "Gabriel Maicas",
      "Jacinto C. Nascimento",
      "Gustavo Carneiro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13226"
  },
  {
    "id": "arXiv:2205.13229",
    "title": "Symbiotic Child Emotional Support with Social Robots and Temporal  Knowledge Graphs",
    "abstract": "In current youth-care programs, children with needs (mental health, family\nissues, learning disabilities, and autism) receive support from youth and\nfamily experts as one-to-one assistance at schools or hospitals. Occasionally,\nsocial robots have featured in such settings as support roles in a one-to-one\ninteraction with the child. In this paper, we suggest the development of a\nsymbiotic framework for real-time Emotional Support (ES) with social robots\nKnowledge Graphs (KG). By augmenting a domain-specific corpus from the\nliterature on ES for children (between the age of 8 and 12) and providing\nscenario-driven context including the history of events, we suggest developing\nan experimental knowledge-aware ES framework. The framework both guides the\nsocial robot in providing ES statements to the child and assists the expert in\ntracking and interpreting the child's emotional state and related events over\ntime.",
    "descriptor": "\nComments: Human-Centered Design of Symbiotic Hybrid Intelligence Workshop HHAI 2022\n",
    "authors": [
      "Isabella Saccardi",
      "Duygu Sezen Islakoglu",
      "Anouk Neerincx",
      "Federica Lucia Vinella"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13229"
  },
  {
    "id": "arXiv:2205.13231",
    "title": "Giving Back: Contributions Congruent to Library Dependency Changes in a  Software Ecosystem",
    "abstract": "Popular adoption of third-party libraries for contemporary software\ndevelopment has led to the creation of large inter-dependency networks, where\nsustainability issues of a single library can have widespread network effects.\nMaintainers of these libraries are often overworked, relying on the\ncontributions of volunteers to sustain these libraries. In this work, we\nmeasure contributions that are aligned with dependency changes, to understand\nwhere they come from (i.e., non-maintainer, client maintainer, library\nmaintainer, and library and client maintainer), analyze whether they contribute\nto library dormancy (i.e., a lack of activity), and investigate the\nsimilarities between these contributions and developers' typical contributions.\nHence, we leverage socio-technical techniques to measure the\ndependency-contribution congruence (DC congruence), i.e., the degree to which\ncontributions align with dependencies. We conduct a large-scale empirical study\nto measure the DC congruence for the NPM ecosystem using 1.7 million issues,\n970 thousand pull requests (PR), and over 5.3 million commits belonging to\n107,242 NPM packages. At the ecosystem level, we pinpoint in time peaks of\ncongruence with dependency changes (i.e., 16% DC congruence score).\nSurprisingly, these contributions came from the ecosystem itself (i.e.,\nnon-maintainers of either client and library). At the project level, we find\nthat DC congruence shares a statistically significant relationship with the\nlikelihood of a package becoming dormant. Finally, by comparing source code of\ncontributions, we find that congruent contributions are statistically different\nto typical contributions. Our work has implications to encourage and sustain\ncontributions, especially to support library maintainers that require\ndependency changes.",
    "descriptor": "",
    "authors": [
      "Supatsara Wattanakriengkrai",
      "Dong Wang",
      "Raula Gaikovina Kula",
      "Christoph Treude",
      "Patanamon Thongtanunam",
      "Takashi Ishio",
      "Kenichi Matsumoto"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.13231"
  },
  {
    "id": "arXiv:2205.13234",
    "title": "DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees",
    "abstract": "We propose the fully explainable Decision Tree Graph Neural Network (DT+GNN)\narchitecture. In contrast to existing black-box GNNs and post-hoc explanation\nmethods, the reasoning of DT+GNN can be inspected at every step. To achieve\nthis, we first construct a differentiable GNN layer, which uses a categorical\nstate space for nodes and messages. This allows us to convert the trained MLPs\nin the GNN into decision trees. These trees are pruned using our newly proposed\nmethod to ensure they are small and easy to interpret. We can also use the\ndecision trees to compute traditional explanations. We demonstrate on both\nreal-world datasets and synthetic GNN explainability benchmarks that this\narchitecture works as well as traditional GNNs. Furthermore, we leverage the\nexplainability of DT+GNNs to find interesting insights into many of these\ndatasets, with some surprising results. We also provide an interactive web tool\nto inspect DT+GNN's decision making.",
    "descriptor": "",
    "authors": [
      "Peter M\u00fcller",
      "Lukas Faber",
      "Karolis Martinkus",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13234"
  },
  {
    "id": "arXiv:2205.13242",
    "title": "Inertially Assisted Semi-Direct Visual Odometry for Fixed Wing  Autonomous Unmanned Air Vehicles",
    "abstract": "This article proposes a method to diminish the pose (position plus attitude)\ndrift experienced by an SVO (Semi-Direct Visual Odometry) based visual\nnavigation system installed onboard a UAV (Unmanned Air Vehicle) by\nsupplementing its pose estimation non linear optimizations with priors based on\nthe outputs of a GNSS (Global Navigation Satellite System) Denied inertial\nnavigation system. The method is inspired in a PI (Proportional Integral)\ncontrol system, in which the attitude, altitude, and rate of climb inertial\noutputs act as targets to ensure that the visual estimations do not deviate far\nfrom their inertial counterparts. The resulting IA-VNS (Inertially Assisted\nVisual Navigation System) achieves major reductions in the horizontal position\ndrift inherent to the GNSS-Denied navigation of autonomous fixed wing low SWaP\n(Size, Weight, and Power) UAVs. Additionally, the IA-VNS can be considered as a\nvirtual incremental position (ground velocity) sensor capable of providing\nobservations to the inertial filter. Stochastic high fidelity Monte Carlo\nsimulations of two representative scenarios involving the loss of GNSS signals\nare employed to evaluate the results and to analyze their sensitivity to the\nterrain type overflown by the aircraft as well as to the quality of the onboard\nsensors on which the priors are based. The author releases the C ++\nimplementation of both the navigation algorithms and the high fidelity\nsimulation as open-source software.",
    "descriptor": "\nComments: 53 pages, 32 figures\n",
    "authors": [
      "Eduardo Gallo",
      "Antonio Barrientos"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13242"
  },
  {
    "id": "arXiv:2205.13245",
    "title": "Projectively and weakly simultaneously diagonalizable matrices and their  applications",
    "abstract": "Characterizing simultaneously diagonalizable (SD) matrices has been receiving\nconsiderable attention in the recent decades due to its wide applications and\nits role in matrix analysis. However, the notion of SD matrices is arguably\nstill restrictive for wider applications. In this paper, we consider two error\nmeasures related to the simultaneous diagonalization of matrices, and propose\nseveral new variants of SD thereof; in particular, TWSD, TWSD-B, T_{m,n}-SD\n(SDO), DWSD and D_{m,n}-SD (SDO). Those are all weaker forms of SD. We derive\nvarious sufficient and/or necessary conditions of them under different\nassumptions, and show the relationships between these new notions. Finally, we\ndiscuss the applications of these new notions in, e.g., quadratically\nconstrained quadratic programming (QCQP) and independent component analysis\n(ICA).",
    "descriptor": "\nComments: 39 pages\n",
    "authors": [
      "Wentao Ding",
      "Jianze Li",
      "Shuzhong Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13245"
  },
  {
    "id": "arXiv:2205.13248",
    "title": "Constrained Reinforcement Learning for Short Video Recommendation",
    "abstract": "The wide popularity of short videos on social media poses new opportunities\nand challenges to optimize recommender systems on the video-sharing platforms.\nUsers provide complex and multi-faceted responses towards recommendations,\nincluding watch time and various types of interactions with videos. As a\nresult, established recommendation algorithms that concern a single objective\nare not adequate to meet this new demand of optimizing comprehensive user\nexperiences. In this paper, we formulate the problem of short video\nrecommendation as a constrained Markov Decision Process (MDP), where platforms\nwant to optimize the main goal of user watch time in long term, with the\nconstraint of accommodating the auxiliary responses of user interactions such\nas sharing/downloading videos.\nTo solve the constrained MDP, we propose a two-stage reinforcement learning\napproach based on actor-critic framework. At stage one, we learn individual\npolicies to optimize each auxiliary response. At stage two, we learn a policy\nto (i) optimize the main response and (ii) stay close to policies learned at\nthe first stage, which effectively guarantees the performance of this main\npolicy on the auxiliaries. Through extensive simulations, we demonstrate\neffectiveness of our approach over alternatives in both optimizing the main\ngoal as well as balancing the others. We further show the advantage of our\napproach in live experiments of short video recommendations, where it\nsignificantly outperforms other baselines in terms of watch time and\ninteractions from video views. Our approach has been fully launched in the\nproduction system to optimize user experiences on the platform.",
    "descriptor": "",
    "authors": [
      "Qingpeng Cai",
      "Ruohan Zhan",
      "Chi Zhang",
      "Jie Zheng",
      "Guangwei Ding",
      "Pinghua Gong",
      "Dong Zheng",
      "Peng Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.13248"
  },
  {
    "id": "arXiv:2205.13249",
    "title": "DT-SV: A Transformer-based Time-domain Approach for Speaker Verification",
    "abstract": "Speaker verification (SV) aims to determine whether the speaker's identity of\na test utterance is the same as the reference speech. In the past few years,\nextracting speaker embeddings using deep neural networks for SV systems has\ngone mainstream. Recently, different attention mechanisms and Transformer\nnetworks have been explored widely in SV fields. However, utilizing the\noriginal Transformer in SV directly may have frame-level information waste on\noutput features, which could lead to restrictions on capacity and\ndiscrimination of speaker embeddings. Therefore, we propose an approach to\nderive utterance-level speaker embeddings via a Transformer architecture that\nuses a novel loss function named diffluence loss to integrate the feature\ninformation of different Transformer layers. Therein, the diffluence loss aims\nto aggregate frame-level features into an utterance-level representation, and\nit could be integrated into the Transformer expediently. Besides, we also\nintroduce a learnable mel-fbank energy feature extractor named time-domain\nfeature extractor that computes the mel-fbank features more precisely and\nefficiently than the standard mel-fbank extractor. Combining Diffluence loss\nand Time-domain feature extractor, we propose a novel Transformer-based\ntime-domain SV model (DT-SV) with faster training speed and higher accuracy.\nExperiments indicate that our proposed model can achieve better performance in\ncomparison with other models.",
    "descriptor": "\nComments: Accepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\n",
    "authors": [
      "Nan Zhang",
      "Jianzong Wang",
      "Zhenhou Hong",
      "Chendong Zhao",
      "Xiaoyang Qu",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.13249"
  },
  {
    "id": "arXiv:2205.13253",
    "title": "Denial-of-Service Attacks on Learned Image Compression",
    "abstract": "Deep learning techniques have shown promising results in image compression,\nwith competitive bitrate and image reconstruction quality from compressed\nlatent. However, while image compression has progressed towards higher peak\nsignal-to-noise ratio (PSNR) and fewer bits per pixel (bpp), their robustness\nto corner-case images has never received deliberation. In this work, we, for\nthe first time, investigate the robustness of image compression systems where\nimperceptible perturbation of input images can precipitate a significant\nincrease in the bitrate of their compressed latent. To characterize the\nrobustness of state-of-the-art learned image compression, we mount white and\nblack-box attacks. Our results on several image compression models with various\nbitrate qualities show that they are surprisingly fragile, where the white-box\nattack achieves up to 56.326x and black-box 1.947x bpp change. To improve\nrobustness, we propose a novel model which incorporates attention modules and a\nbasic factorized entropy model, resulting in a promising trade-off between the\nPSNR/bpp ratio and robustness to adversarial attacks that surpasses existing\nlearned image compressors.",
    "descriptor": "",
    "authors": [
      "Kang Liu",
      "Di Wu",
      "Yiru Wang",
      "Dan Feng",
      "Benjamin Tan",
      "Siddharth Garg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13253"
  },
  {
    "id": "arXiv:2205.13255",
    "title": "Active Labeling: Streaming Stochastic Gradients",
    "abstract": "The workhorse of machine learning is stochastic gradient descent. To access\nstochastic gradients, it is common to consider iteratively input/output pairs\nof a training dataset. Interestingly, it appears that one does not need full\nsupervision to access stochastic gradients, which is the main motivation of\nthis paper. After formalizing the \"active labeling\" problem, which generalizes\nactive learning based on partial supervision, we provide a streaming technique\nthat provably minimizes the ratio of generalization error over number of\nsamples. We illustrate our technique in depth for robust regression.",
    "descriptor": "\nComments: 38 pages (9 main pages), 9 figures\n",
    "authors": [
      "Vivien Cabannes",
      "Francis Bach",
      "Vianney Perchet",
      "Alessandro Rudi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13255"
  },
  {
    "id": "arXiv:2205.13256",
    "title": "A DLT enabled smart mask system to enable social compliance",
    "abstract": "As Covid-19 remains a cause of concern, especially due to its mutations,\nwearing masks correctly and efficiently remains a priority in order to limit\nthe spread of the disease. In this paper we present a wearable smart-mask\nprototype using concepts from Internet of Things, Control Theory and\nDistributed Ledger Technologies. Its purpose is to encourage people to comply\nwith social distancing norms, through the use of incentives. The smart mask is\ndesigned to monitor Carbon Dioxide and Total Volatile Organic Compounds\nconcentrations. The detected data is appended to a DAG-based DLT, named the\nIOTA Tangle. The IOTA Tangle ensures that the data is secure and immutable and\nacts as a communication backbone for the incentive mechanism. A\nhardware-in-the-loop simulation, based on indoor positioning, is developed to\nvalidate the effectiveness of the designed prototype.",
    "descriptor": "",
    "authors": [
      "Lianna Zhao",
      "Pietro Ferraro",
      "Robert Shorten"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13256"
  },
  {
    "id": "arXiv:2205.13264",
    "title": "Nonlinear Stochastic Trajectory Optimization for Centroidal Momentum  Motion Generation of Legged Robots",
    "abstract": "Generation of robust trajectories for legged robots remains a challenging\ntask due to the underlying nonlinear, hybrid and intrinsically unstable\ndynamics which needs to be stabilized through limited contact forces.\nFurthermore, disturbances arising from unmodelled contact interactions with the\nenvironment and model mismatches can hinder the quality of the planned\ntrajectories leading to unsafe motions. In this work, we propose to use\nstochastic trajectory optimization for generating robust centroidal momentum\ntrajectories to account for additive uncertainties on the model dynamics and\nparametric uncertainties on contact locations. Through an alternation between\nthe robust centroidal and whole-body trajectory optimizations, we generate\nrobust momentum trajectories while being consistent with the whole-body\ndynamics. We perform an extensive set of simulations subject to different\nuncertainties on a quadruped robot showing that our stochastic trajectory\noptimization problem reduces the amount of foot slippage for different gaits\nwhile achieving better performance over deterministic planning.",
    "descriptor": "",
    "authors": [
      "Ahmad Gazar",
      "Majid Khadiv",
      "S\u00e9bastien Kleff",
      "Andrea Del Prete",
      "Ludovic Righetti"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13264"
  },
  {
    "id": "arXiv:2205.13265",
    "title": "Privacy-Preserving Wavelet Wavelet Neural Network with Fully Homomorphic  Encryption",
    "abstract": "The main aim of Privacy-Preserving Machine Learning (PPML) is to protect the\nprivacy and provide security to the data used in building Machine Learning\nmodels. There are various techniques in PPML such as Secure Multi-Party\nComputation, Differential Privacy, and Homomorphic Encryption (HE). The\ntechniques are combined with various Machine Learning models and even Deep\nLearning Networks to protect the data privacy as well as the identity of the\nuser. In this paper, we propose a fully homomorphic encrypted wavelet neural\nnetwork to protect privacy and at the same time not compromise on the\nefficiency of the model. We tested the effectiveness of the proposed method on\nseven datasets taken from the finance and healthcare domains. The results show\nthat our proposed model performs similarly to the unencrypted model.",
    "descriptor": "\nComments: 17 pages; 3 figures, 10 tables\n",
    "authors": [
      "Syed Imtiaz Ahamed",
      "Vadlamani Ravi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13265"
  },
  {
    "id": "arXiv:2205.13266",
    "title": "Logit-Q Learning in Markov Games",
    "abstract": "We present new independent learning dynamics provably converging to an\nefficient equilibrium (also known as optimal equilibrium) maximizing the social\nwelfare in infinite-horizon discounted identical-interest Markov games (MG),\nbeyond the recent concentration of progress on provable convergence to some\n(possibly inefficient) equilibrium. The dynamics are independent in the sense\nthat agents take actions without considering the others' objectives in their\ndecision-making process, and their decisions are consistent with their\nobjectives based on behavioral learning models. Independent and simultaneous\nadaptation of agents in an MG poses the key challenges: i) possible convergence\nto an inefficient equilibrium and ii) possible non-stationarity of the\nenvironment from a single agent's viewpoint. We address the former by\ngeneralizing the log-linear learning dynamics to MG settings and address the\nlatter through the play-in-rounds scheme presented. Particularly, in an MG,\nagents play (normal-form) stage games associated with the state visited based\non their continuation payoff estimates. We let the agents play these stage\ngames in rounds such that their continuation payoff estimates get updated only\nat the end of the round. This makes these stage games stationary within each\nround. Hence, the dynamics approximate the value iteration and the convergence\nto the social optimum of the underlying MG follows.",
    "descriptor": "",
    "authors": [
      "Muhammed O. Sayin",
      "Onur Unlu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13266"
  },
  {
    "id": "arXiv:2205.13267",
    "title": "Task-Customized Self-Supervised Pre-training with Scalable Dynamic  Routing",
    "abstract": "Self-supervised learning (SSL), especially contrastive methods, has raised\nattraction recently as it learns effective transferable representations without\nsemantic annotations. A common practice for self-supervised pre-training is to\nuse as much data as possible. For a specific downstream task, however,\ninvolving irrelevant data in pre-training may degenerate the downstream\nperformance, observed from our extensive experiments. On the other hand, for\nexisting SSL methods, it is burdensome and infeasible to use different\ndownstream-task-customized datasets in pre-training for different tasks. To\naddress this issue, we propose a novel SSL paradigm called Scalable Dynamic\nRouting (SDR), which can be trained once and deployed efficiently to different\ndownstream tasks with task-customized pre-trained models. Specifically, we\nconstruct the SDRnet with various sub-nets and train each sub-net with only one\nsubset of the data by data-aware progressive training. When a downstream task\narrives, we route among all the pre-trained sub-nets to get the best along with\nits corresponding weights. Experiment results show that our SDR can train 256\nsub-nets on ImageNet simultaneously, which provides better transfer performance\nthan a unified model trained on the full ImageNet, achieving state-of-the-art\n(SOTA) averaged accuracy over 11 downstream classification tasks and AP on\nPASCAL VOC detection task.",
    "descriptor": "",
    "authors": [
      "Zhili Liu",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Kai Chen",
      "Chunjing Xu",
      "Zhenguo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13267"
  },
  {
    "id": "arXiv:2205.13268",
    "title": "MemeTector: Enforcing deep focus for meme detection",
    "abstract": "Image memes and specifically their widely-known variation image macros, is a\nspecial new media type that combines text with images and is used in social\nmedia to playfully or subtly express humour, irony, sarcasm and even hate. It\nis important to accurately retrieve image memes from social media to better\ncapture the cultural and social aspects of online phenomena and detect\npotential issues (hate-speech, disinformation). Essentially, the background\nimage of an image macro is a regular image easily recognized as such by humans\nbut cumbersome for the machine to do so due to feature map similarity with the\ncomplete image macro. Hence, accumulating suitable feature maps in such cases\ncan lead to deep understanding of the notion of image memes. To this end, we\npropose a methodology that utilizes the visual part of image memes as instances\nof the regular image class and the initial image memes as instances of the\nimage meme class to force the model to concentrate on the critical parts that\ncharacterize an image meme. Additionally, we employ a trainable attention\nmechanism on top of a standard ViT architecture to enhance the model's ability\nto focus on these critical parts and make the predictions interpretable.\nSeveral training and test scenarios involving web-scraped regular images of\ncontrolled text presence are considered in terms of model robustness and\naccuracy. The findings indicate that light visual part utilization combined\nwith sufficient text presence during training provides the best and most robust\nmodel, surpassing state of the art.",
    "descriptor": "",
    "authors": [
      "Christos Koutlis",
      "Manos Schinas",
      "Symeon Papadopoulos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13268"
  },
  {
    "id": "arXiv:2205.13271",
    "title": "Unsupervised Multi-object Segmentation Using Attention and Soft-argmax",
    "abstract": "We introduce a new architecture for unsupervised object-centric\nrepresentation learning and multi-object detection and segmentation, which uses\nan attention mechanism to associate a feature vector to each object present in\nthe scene and to predict the coordinates of these objects using soft-argmax. A\ntransformer encoder handles occlusions and redundant detections, and a separate\npre-trained background model is in charge of background reconstruction. We show\nthat this architecture significantly outperforms the state of the art on\ncomplex synthetic benchmarks and provide examples of applications to real-world\ntraffic videos.",
    "descriptor": "",
    "authors": [
      "Bruno Sauvalle",
      "Arnaud de La Fortelle"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13271"
  },
  {
    "id": "arXiv:2205.13272",
    "title": "FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for  Constrained Devices",
    "abstract": "IoT devices suffer from resource limitations, such as processor, RAM, and\ndisc storage. These limitations become more evident when handling demanding\napplications, such as deep learning, well-known for their heavy computational\nrequirements. A case in point is robot pose estimation, an application that\npredicts the critical points of the desired image object. One way to mitigate\nprocessing and storage problems is compressing that deep learning application.\nThis paper proposes a new CNN for the pose estimation while applying the\ncompression techniques of pruning and quantization to reduce his demands and\nimprove the response time. While the pruning process reduces the total number\nof parameters required for inference, quantization decreases the precision of\nthe floating-point. We run the approach using a pose estimation task for a\nrobotic arm and compare the results in a high-end device and a constrained\ndevice. As metrics, we consider the number of Floating-point Operations Per\nSecond(FLOPS), the total of mathematical computations, the calculation of\nparameters, the inference time, and the number of video frames processed per\nsecond. In addition, we undertake a qualitative evaluation where we compare the\noutput image predicted for each pruned network with the corresponding original\none. We reduce the originally proposed network to a 70% pruning rate, implying\nan 88.86% reduction in parameters, 94.45% reduction in FLOPS, and for the disc\nstorage, we reduced the requirement in 70% while increasing error by a mere\n$1\\%$. With regard input image processing, this metric increases from 11.71 FPS\nto 41.9 FPS for the Desktop case. When using the constrained device, image\nprocessing augmented from 2.86 FPS to 10.04 FPS. The higher processing rate of\nimage frames achieved by the proposed approach allows a much shorter response\ntime.",
    "descriptor": "",
    "authors": [
      "Marrone Silv\u00e9rio Melo Dantas",
      "Iago Richard Rodrigues",
      "Assis Tiago Oliveira Filho",
      "Gibson Barbosa",
      "Daniel Bezerra",
      "Djamel F. H. Sadok",
      "Judith Kelner",
      "Maria Marquezini",
      "Ricardo Silva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13272"
  },
  {
    "id": "arXiv:2205.13273",
    "title": "Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued  Convolutional Neural Networks",
    "abstract": "This paper features convolutional neural networks defined on hypercomplex\nalgebras applied to classify lymphocytes in blood smear digital microscopic\nimages. Such classification is helpful for the diagnosis of acute lymphoblast\nleukemia (ALL), a type of blood cancer. We perform the classification task\nusing eight hypercomplex-valued convolutional neural networks (HvCNNs) along\nwith real-valued convolutional networks. Our results show that HvCNNs perform\nbetter than the real-valued model, showcasing higher accuracy with a much\nsmaller number of parameters. Moreover, we found that HvCNNs based on Clifford\nalgebras processing HSV-encoded images attained the highest observed\naccuracies. Precisely, our HvCNN yielded an average accuracy rate of 96.6%\nusing the ALL-IDB2 dataset with a 50% train-test split, a value extremely close\nto the state-of-the-art models but using a much simpler architecture with\nsignificantly fewer parameters.",
    "descriptor": "\nComments: Accepted for presentation at 2022 International Joint Conference on Neural Networks (IJCNN 2022), 18-23 July, 2022, Padua, Italy\n",
    "authors": [
      "Guilherme Vieira",
      "Marcos Eduardo Valle"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13273"
  },
  {
    "id": "arXiv:2205.13274",
    "title": "Evaluating Multimodal Interactive Agents",
    "abstract": "Creating agents that can interact naturally with humans is a common goal in\nartificial intelligence (AI) research. However, evaluating these interactions\nis challenging: collecting online human-agent interactions is slow and\nexpensive, yet faster proxy metrics often do not correlate well with\ninteractive evaluation. In this paper, we assess the merits of these existing\nevaluation metrics and present a novel approach to evaluation called the\nStandardised Test Suite (STS). The STS uses behavioural scenarios mined from\nreal human interaction data. Agents see replayed scenario context, receive an\ninstruction, and are then given control to complete the interaction offline.\nThese agent continuations are recorded and sent to human annotators to mark as\nsuccess or failure, and agents are ranked according to the proportion of\ncontinuations in which they succeed. The resulting STS is fast, controlled,\ninterpretable, and representative of naturalistic interactions. Altogether, the\nSTS consolidates much of what is desirable across many of our standard\nevaluation metrics, allowing us to accelerate research progress towards\nproducing agents that can interact naturally with humans.\nhttps://youtu.be/YR1TngGORGQ",
    "descriptor": "",
    "authors": [
      "Josh Abramson",
      "Arun Ahuja",
      "Federico Carnevale",
      "Petko Georgiev",
      "Alex Goldin",
      "Alden Hung",
      "Jessica Landon",
      "Timothy Lillicrap",
      "Alistair Muldal",
      "Blake Richards",
      "Adam Santoro",
      "Tamara von Glehn",
      "Greg Wayne",
      "Nathaniel Wong",
      "Chen Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13274"
  },
  {
    "id": "arXiv:2205.13277",
    "title": "VIDI: A Video Dataset of Incidents",
    "abstract": "Automatic detection of natural disasters and incidents has become more\nimportant as a tool for fast response. There have been many studies to detect\nincidents using still images and text. However, the number of approaches that\nexploit temporal information is rather limited. One of the main reasons for\nthis is that a diverse video dataset with various incident types does not\nexist. To address this need, in this paper we present a video dataset, Video\nDataset of Incidents, VIDI, that contains 4,534 video clips corresponding to 43\nincident categories. Each incident class has around 100 videos with a duration\nof ten seconds on average. To increase diversity, the videos have been searched\nin several languages. To assess the performance of the recent state-of-the-art\napproaches, Vision Transformer and TimeSformer, as well as to explore the\ncontribution of video-based information for incident classification, we\nperformed benchmark experiments on the VIDI and Incidents Dataset. We have\nshown that the recent methods improve the incident classification accuracy. We\nhave found that employing video data is very beneficial for the task. By using\nthe video data, the top-1 accuracy is increased to 76.56% from 67.37%, which\nwas obtained using a single frame. VIDI will be made publicly available.\nAdditional materials can be found at the following link:\nhttps://github.com/vididataset/VIDI.",
    "descriptor": "",
    "authors": [
      "Duygu Sesver",
      "Alp Eren Gen\u00e7o\u011flu",
      "\u00c7a\u011fr\u0131 Emre Y\u0131ld\u0131z",
      "Zehra G\u00fcnindi",
      "Faeze Habibi",
      "Ziya Ata Yaz\u0131c\u0131",
      "Haz\u0131m Kemal Ekenel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13277"
  },
  {
    "id": "arXiv:2205.13278",
    "title": "Semantic Segmentation for Thermal Images: A Comparative Survey",
    "abstract": "Semantic segmentation is a challenging task since it requires excessively\nmore low-level spatial information of the image compared to other computer\nvision problems. The accuracy of pixel-level classification can be affected by\nmany factors, such as imaging limitations and the ambiguity of object\nboundaries in an image. Conventional methods exploit three-channel RGB images\ncaptured in the visible spectrum with deep neural networks (DNN). Thermal\nimages can significantly contribute during the segmentation since thermal\nimaging cameras are capable of capturing details despite the weather and\nillumination conditions. Using infrared spectrum in semantic segmentation has\nmany real-world use cases, such as autonomous driving, medical imaging,\nagriculture, defense industry, etc. Due to this wide range of use cases,\ndesigning accurate semantic segmentation algorithms with the help of infrared\nspectrum is an important challenge. One approach is to use both visible and\ninfrared spectrum images as inputs. These methods can accomplish higher\naccuracy due to enriched input information, with the cost of extra effort for\nthe alignment and processing of multiple inputs. Another approach is to use\nonly thermal images, enabling less hardware cost for smaller use cases. Even\nthough there are multiple surveys on semantic segmentation methods, the\nliterature lacks a comprehensive survey centered explicitly around semantic\nsegmentation using infrared spectrum. This work aims to fill this gap by\npresenting algorithms in the literature and categorizing them by their input\nimages.",
    "descriptor": "\nComments: Accepted to CVPR 2022 Perception Beyond the Visible Spectrum (PBVS) Workshop\n",
    "authors": [
      "Z\u00fclfiye K\u00fct\u00fck",
      "G\u00f6rkem Algan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13278"
  },
  {
    "id": "arXiv:2205.13279",
    "title": "Triangular Contrastive Learning on Molecular Graphs",
    "abstract": "Recent contrastive learning methods have shown to be effective in various\ntasks, learning generalizable representations invariant to data augmentation\nthereby leading to state of the art performances. Regarding the multifaceted\nnature of large unlabeled data used in self-supervised learning while majority\nof real-word downstream tasks use single format of data, a multimodal framework\nthat can train single modality to learn diverse perspectives from other\nmodalities is an important challenge. In this paper, we propose TriCL\n(Triangular Contrastive Learning), a universal framework for trimodal\ncontrastive learning. TriCL takes advantage of Triangular Area Loss, a novel\nintermodal contrastive loss that learns the angular geometry of the embedding\nspace through simultaneously contrasting the area of positive and negative\ntriplets. Systematic observation on embedding space in terms of alignment and\nuniformity showed that Triangular Area Loss can address the line-collapsing\nproblem by discriminating modalities by angle. Our experimental results also\ndemonstrate the outperformance of TriCL on downstream task of molecular\nproperty prediction which implies that the advantages of the embedding space\nindeed benefits the performance on downstream tasks.",
    "descriptor": "",
    "authors": [
      "MinGyu Choi",
      "Wonseok Shin",
      "Yijingxiu Lu",
      "Sun Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13279"
  },
  {
    "id": "arXiv:2205.13280",
    "title": "Objects Matter: Learning Object Relation Graph for Robust Camera  Relocalization",
    "abstract": "Visual relocalization aims to estimate the pose of a camera from one or more\nimages. In recent years deep learning based pose regression methods have\nattracted many attentions. They feature predicting the absolute poses without\nrelying on any prior built maps or stored images, making the relocalization\nvery efficient. However, robust relocalization under environments with complex\nappearance changes and real dynamics remains very challenging. In this paper,\nwe propose to enhance the distinctiveness of the image features by extracting\nthe deep relationship among objects. In particular, we extract objects in the\nimage and construct a deep object relation graph (ORG) to incorporate the\nsemantic connections and relative spatial clues of the objects. We integrate\nour ORG module into several popular pose regression models. Extensive\nexperiments on various public indoor and outdoor datasets demonstrate that our\nmethod improves the performance significantly and outperforms the previous\napproaches.",
    "descriptor": "",
    "authors": [
      "Chengyu Qiao",
      "Zhiyu Xiang",
      "Xinglu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13280"
  },
  {
    "id": "arXiv:2205.13281",
    "title": "Surround-view Fisheye Camera Perception for Automated Driving: Overview,  Survey and Challenges",
    "abstract": "Surround-view fisheye cameras are commonly used for near-field sensing in\nautomated driving. Four fisheye cameras on four sides of the vehicle are\nsufficient to cover 360{\\deg} around the vehicle capturing the entire\nnear-field region. Some primary use cases are automated parking, traffic jam\nassist, and urban driving. There are limited datasets and very little work on\nnear-field perception tasks as the main focus in automotive perception is on\nfar-field perception. In contrast to far-field, surround-view perception poses\nadditional challenges due to high precision object detection requirements of\n10cm and partial visibility of objects. Due to the large radial distortion of\nfisheye cameras, standard algorithms can not be extended easily to the\nsurround-view use case. Thus we are motivated to provide a self-contained\nreference for automotive fisheye camera perception for researchers and\npractitioners. Firstly, we provide a unified and taxonomic treatment of\ncommonly used fisheye camera models. Secondly, we discuss various perception\ntasks and existing literature. Finally, we discuss the challenges and future\ndirection.",
    "descriptor": "",
    "authors": [
      "Varun Ravi Kumar",
      "Ciaran Eising",
      "Christian Witt",
      "Senthil Yogamani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13281"
  },
  {
    "id": "arXiv:2205.13282",
    "title": "On the Eigenvalues of Global Covariance Pooling for Fine-grained Visual  Recognition",
    "abstract": "The Fine-Grained Visual Categorization (FGVC) is challenging because the\nsubtle inter-class variations are difficult to be captured. One notable\nresearch line uses the Global Covariance Pooling (GCP) layer to learn powerful\nrepresentations with second-order statistics, which can effectively model\ninter-class differences. In our previous conference paper, we show that\ntruncating small eigenvalues of the GCP covariance can attain smoother gradient\nand improve the performance on large-scale benchmarks. However, on fine-grained\ndatasets, truncating the small eigenvalues would make the model fail to\nconverge. This observation contradicts the common assumption that the small\neigenvalues merely correspond to the noisy and unimportant information.\nConsequently, ignoring them should have little influence on the performance. To\ndiagnose this peculiar behavior, we propose two attribution methods whose\nvisualizations demonstrate that the seemingly unimportant small eigenvalues are\ncrucial as they are in charge of extracting the discriminative class-specific\nfeatures. Inspired by this observation, we propose a network branch dedicated\nto magnifying the importance of small eigenvalues. Without introducing any\nadditional parameters, this branch simply amplifies the small eigenvalues and\nachieves state-of-the-art performances of GCP methods on three fine-grained\nbenchmarks. Furthermore, the performance is also competitive against other FGVC\napproaches on larger datasets. Code is available at\n\\href{https://github.com/KingJamesSong/DifferentiableSVD}{https://github.com/KingJamesSong/DifferentiableSVD}.",
    "descriptor": "\nComments: Accepted by IEEE T-PAMI\n",
    "authors": [
      "Yue Song",
      "Nicu Sebe",
      "Wei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13282"
  },
  {
    "id": "arXiv:2205.13283",
    "title": "Embedding Principle in Depth for the Loss Landscape Analysis of Deep  Neural Networks",
    "abstract": "Unraveling the general structure underlying the loss landscapes of deep\nneural networks (DNNs) is important for the theoretical study of deep learning.\nInspired by the embedding principle of DNN loss landscape, we prove in this\nwork an embedding principle in depth that loss landscape of an NN \"contains\"\nall critical points of the loss landscapes for shallower NNs. Specifically, we\npropose a critical lifting operator that any critical point of a shallower\nnetwork can be lifted to a critical manifold of the target network while\npreserving the outputs. Through lifting, local minimum of an NN can become a\nstrict saddle point of a deeper NN, which can be easily escaped by first-order\nmethods. The embedding principle in depth reveals a large family of critical\npoints in which layer linearization happens, i.e., computation of certain\nlayers is effectively linear for the training inputs. We empirically\ndemonstrate that, through suppressing layer linearization, batch normalization\nhelps avoid the lifted critical manifolds, resulting in a faster decay of loss.\nWe also demonstrate that increasing training data reduces the lifted critical\nmanifold thus could accelerate the training. Overall, the embedding principle\nin depth well complements the embedding principle (in width), resulting in a\ncomplete characterization of the hierarchical structure of critical\npoints/manifolds of a DNN loss landscape.",
    "descriptor": "",
    "authors": [
      "Zhiwei Bai",
      "Tao Luo",
      "Zhi-Qin John Xu",
      "Yaoyu Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13283"
  },
  {
    "id": "arXiv:2205.13284",
    "title": "YASMIN: Yet Another State MachINe library for ROS 2",
    "abstract": "State machines are a common mechanism for defining behaviors in robots,\ndefining them based on identifiable stages. There are several libraries\navailable for easing the implementation of state machines in ROS 1, as SMACH or\nSMACC, but there are fewer alternatives for ROS 2. YASMIN is yet another\nlibrary specifically designed for ROS 2 for easing the design of robotic\nbehaviors using state machines. It is available in C++ and Python, provides\nsome default states to speed up the development, and a web viewer for\nmonitoring the execution of the system and helping in the debugging.",
    "descriptor": "\nComments: 4 pages, 2 figures, ROSCon FR 2022\n",
    "authors": [
      "Miguel \u00c1ngel Gonz\u00e1lez-Santamarta",
      "Francisco Javier Rodr\u00edguez-Lera",
      "Camino Fern\u00e1ndez Llamas",
      "Francisco Mart\u00edn Rico",
      "Vicente Matell\u00e1n Olivera"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13284"
  },
  {
    "id": "arXiv:2205.13286",
    "title": "Ergodic Capacity Maximization of RIS-assisted Millimeter-Wave MIMO-OFDM  Communication Systems",
    "abstract": "Reconfigurable intelligent surface (RIS) has attracted extensive attentions\nin recent years. However, most research focuses on the scenario of the\nnarrowband and/or instantaneous channel state information (CSI), while wide\nbandwidth with the use of millimeter-wave (mmWave) (including sub-THz) spectrum\nis a major trend in next-generation wireless communications, and statistical\nCSI is more practical to obtain in realistic systems. Thus, we fill the blank\nby looking at the ergodic capacity of RIS-assisted mmWave multiple-input\nmultiple-output (MIMO) orthogonal frequency division multiplexing (OFDM)\ncommunication systems. The widely used Saleh-Valenzuela channel model is\nadopted to characterize the mmWave channels and only the statistical CSI is\navailable. We first derive the approximations of the ergodic capacity by means\nof the majorization theory and Jensen's inequality. Then, an alternating\noptimization based algorithm is proposed to maximize the ergodic capacity by\njointly designing the transmit covariance matrix at the base station and the\nreflection coefficients at the RIS. Specifically, the design of the transmit\ncovariance matrix is transformed into a power allocation problem and solved by\nspatial-frequency water-filling. The reflection coefficients are optimized by\nthe Riemannian conjugate gradient algorithm. Simulation results corroborate the\ncloseness of the derived ergodic capacity approximations and the effectiveness\nof the proposed algorithms.",
    "descriptor": "\nComments: submitted for possible publication\n",
    "authors": [
      "Renwang Li",
      "Shu Sun",
      "Meixia Tao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13286"
  },
  {
    "id": "arXiv:2205.13294",
    "title": "Analytical Interpretation of Latent Codes in InfoGAN with SAR Images",
    "abstract": "Generative Adversarial Networks (GANs) can synthesize abundant\nphoto-realistic synthetic aperture radar (SAR) images. Some recent GANs (e.g.,\nInfoGAN), are even able to edit specific properties of the synthesized images\nby introducing latent codes. It is crucial for SAR image synthesis since the\ntargets in real SAR images are with different properties due to the imaging\nmechanism. Despite the success of InfoGAN in manipulating properties, there\nstill lacks a clear explanation of how these latent codes affect synthesized\nproperties, thus editing specific properties usually relies on empirical\ntrials, unreliable and time-consuming. In this paper, we show that latent codes\nare disentangled to affect the properties of SAR images in a non-linear manner.\nBy introducing some property estimators for latent codes, we are able to\nprovide a completely analytical nonlinear model to decompose the entangled\ncausality between latent codes and different properties. The qualitative and\nquantitative experimental results further reveal that the properties can be\ncalculated by latent codes, inversely, the satisfying latent codes can be\nestimated given desired properties. In this case, properties can be manipulated\nby latent codes as we expect.",
    "descriptor": "\nComments: 13 pages, 14 figures\n",
    "authors": [
      "Zhenpeng Feng",
      "Milos Dakovic",
      "Hongbing Ji",
      "Mingzhe Zhu",
      "Ljubisa Stankovic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13294"
  },
  {
    "id": "arXiv:2205.13296",
    "title": "Social Interpretable Tree for Pedestrian Trajectory Prediction",
    "abstract": "Understanding the multiple socially-acceptable future behaviors is an\nessential task for many vision applications. In this paper, we propose a\ntree-based method, termed as Social Interpretable Tree (SIT), to address this\nmulti-modal prediction task, where a hand-crafted tree is built depending on\nthe prior information of observed trajectory to model multiple future\ntrajectories. Specifically, a path in the tree from the root to leaf represents\nan individual possible future trajectory. SIT employs a coarse-to-fine\noptimization strategy, in which the tree is first built by high-order velocity\nto balance the complexity and coverage of the tree and then optimized greedily\nto encourage multimodality. Finally, a teacher-forcing refining operation is\nused to predict the final fine trajectory. Compared with prior methods which\nleverage implicit latent variables to represent possible future trajectories,\nthe path in the tree can explicitly explain the rough moving behaviors (e.g.,\ngo straight and then turn right), and thus provides better interpretability.\nDespite the hand-crafted tree, the experimental results on ETH-UCY and Stanford\nDrone datasets demonstrate that our method is capable of matching or exceeding\nthe performance of state-of-the-art methods. Interestingly, the experiments\nshow that the raw built tree without training outperforms many prior deep\nneural network based approaches. Meanwhile, our method presents sufficient\nflexibility in long-term prediction and different best-of-$K$ predictions.",
    "descriptor": "\nComments: Accepted by AAAI2022\n",
    "authors": [
      "Liushuai Shi",
      "Le Wang",
      "Chengjiang Long",
      "Sanping Zhou",
      "Fang Zheng",
      "Nanning Zheng",
      "Gang Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13296"
  },
  {
    "id": "arXiv:2205.13299",
    "title": "Federated Split BERT for Heterogeneous Text Classification",
    "abstract": "Pre-trained BERT models have achieved impressive performance in many natural\nlanguage processing (NLP) tasks. However, in many real-world situations,\ntextual data are usually decentralized over many clients and unable to be\nuploaded to a central server due to privacy protection and regulations.\nFederated learning (FL) enables multiple clients collaboratively to train a\nglobal model while keeping the local data privacy. A few researches have\ninvestigated BERT in federated learning setting, but the problem of performance\nloss caused by heterogeneous (e.g., non-IID) data over clients remain\nunder-explored. To address this issue, we propose a framework, FedSplitBERT,\nwhich handles heterogeneous data and decreases the communication cost by\nsplitting the BERT encoder layers into local part and global part. The local\npart parameters are trained by the local client only while the global part\nparameters are trained by aggregating gradients of multiple clients. Due to the\nsheer size of BERT, we explore a quantization method to further reduce the\ncommunication cost with minimal performance loss. Our framework is ready-to-use\nand compatible to many existing federated learning algorithms, including\nFedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of the\nproposed framework, which outperforms baseline methods by a significant margin,\nwhile FedSplitBERT with quantization can reduce the communication cost by\n$11.9\\times$.",
    "descriptor": "\nComments: 8 pages, 6 figures, accepted by IJCNN 2022\n",
    "authors": [
      "Zhengyang Li",
      "Shijing Si",
      "Jianzong Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13299"
  },
  {
    "id": "arXiv:2205.13300",
    "title": "Federated Non-negative Matrix Factorization for Short Texts Topic  Modeling with Mutual Information",
    "abstract": "Non-negative matrix factorization (NMF) based topic modeling is widely used\nin natural language processing (NLP) to uncover hidden topics of short text\ndocuments. Usually, training a high-quality topic model requires large amount\nof textual data. In many real-world scenarios, customer textual data should be\nprivate and sensitive, precluding uploading to data centers. This paper\nproposes a Federated NMF (FedNMF) framework, which allows multiple clients to\ncollaboratively train a high-quality NMF based topic model with locally stored\ndata. However, standard federated learning will significantly undermine the\nperformance of topic models in downstream tasks (e.g., text classification)\nwhen the data distribution over clients is heterogeneous. To alleviate this\nissue, we further propose FedNMF+MI, which simultaneously maximizes the mutual\ninformation (MI) between the count features of local texts and their topic\nweight vectors to mitigate the performance degradation. Experimental results\nshow that our FedNMF+MI methods outperform Federated Latent Dirichlet\nAllocation (FedLDA) and the FedNMF without MI methods for short texts by a\nsignificant margin on both coherence score and classification F1 score.",
    "descriptor": "\nComments: 7 pages, 4 figures, accepted by IJCNN 2022\n",
    "authors": [
      "Shijing Si",
      "Jianzong Wang",
      "Ruiyi Zhang",
      "Qinliang Su",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13300"
  },
  {
    "id": "arXiv:2205.13301",
    "title": "A DPG method for Reissner-Mindlin plates",
    "abstract": "We present a discontinuous Petrov-Galerkin (DPG) method with optimal test\nfunctions for the Reissner-Mindlin plate bending model. Our method is based on\na variational formulation that utilizes a Helmholtz decomposition of the shear\nforce. It produces approximations of the primitive variables and the bending\nmoments. For any canonical selection of boundary conditions the method\nconverges quasi-optimally. In the case of hard-clamped convex plates, we prove\nthat the lowest-order scheme is locking free. Several numerical experiments\nconfirm our results.",
    "descriptor": "",
    "authors": [
      "Thomas F\u00fchrer",
      "Norbert Heuer",
      "Antti H. Niemi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13301"
  },
  {
    "id": "arXiv:2205.13311",
    "title": "SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow  Devices",
    "abstract": "The widely used gene quantisation technique, Lateral Flow Device (LFD), is\nnow commonly used to detect the presence of SARS-CoV-2. It is enabling the\ncontrol and prevention of the spread of the virus. Depending on the viral load,\nLFD have different sensitivity and self-test for normal user present additional\nchallenge to interpret the result. With the evolution of machine learning\nalgorithms, image processing and analysis has seen unprecedented growth. In\nthis interdisciplinary study, we employ novel image analysis methods of\ncomputer vision and machine learning field to study visual features of the\ncontrol region of LFD. Here, we automatically derive results for any image\ncontaining LFD into positive, negative or inconclusive. This will reduce the\nburden of human involvement of health workers and perception bias.",
    "descriptor": "\nComments: 12 pages, 14 figures\n",
    "authors": [
      "Neeraj Vashistha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13311"
  },
  {
    "id": "arXiv:2205.13313",
    "title": "Cross-Architecture Self-supervised Video Representation Learning",
    "abstract": "In this paper, we present a new cross-architecture contrastive learning\n(CACL) framework for self-supervised video representation learning. CACL\nconsists of a 3D CNN and a video transformer which are used in parallel to\ngenerate diverse positive pairs for contrastive learning. This allows the model\nto learn strong representations from such diverse yet meaningful pairs.\nFurthermore, we introduce a temporal self-supervised learning module able to\npredict an Edit distance explicitly between two video sequences in the temporal\norder. This enables the model to learn a rich temporal representation that\ncompensates strongly to the video-level representation learned by the CACL. We\nevaluate our method on the tasks of video retrieval and action recognition on\nUCF101 and HMDB51 datasets, where our method achieves excellent performance,\nsurpassing the state-of-the-art methods such as VideoMoCo and MoCo+BE by a\nlarge margin. The code is made available at https://github.com/guoshengcv/CACL.",
    "descriptor": "\nComments: Accepted to CVPR2022\n",
    "authors": [
      "Sheng Guo",
      "Zihua Xiong",
      "Yujie Zhong",
      "Limin Wang",
      "Xiaobo Guo",
      "Bing Han",
      "Weilin Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13313"
  },
  {
    "id": "arXiv:2205.13315",
    "title": "Arbitrary High Order WENO Finite Volume Scheme with Flux Globalization  for Moving Equilibria Preservation",
    "abstract": "In the context of preserving stationary states, e.g. lake at rest and moving\nequilibria, a new formulation of the shallow water system, called Flux\nGlobalization has been introduced by Cheng et al. (2019). This approach\nconsists in including the integral of the source term in the global flux and\nreconstructing the new global flux rather than the conservative variables. The\nresulting scheme is able to preserve a large family of smooth and discontinuous\nsteady state moving equilibria. In this work, we focus on an arbitrary high\norder WENO Finite Volume (FV) generalization of the global flux approach. The\nmost delicate aspect of the algorithm is the appropriate definition of the\nsource flux (integral of the source term) and the quadrature strategy used to\nmatch it with the WENO reconstruction of the hyperbolic flux. When this\nconstruction is correctly done, one can show that the resulting WENO FV scheme\nadmits exact discrete steady states characterized by constant global fluxes. We\nalso show that, by an appropriate quadrature strategy for the source, we can\nembed exactly some particular steady states, e.g. the lake at rest for the\nshallow water equations. It can be shown that an exact approximation of global\nfluxes leads to a scheme with better convergence properties and improved\nsolutions. The novel method has been tested and validated on classical cases:\nsubcritical, supercritical and transcritical flows.",
    "descriptor": "",
    "authors": [
      "Mirco Ciallella",
      "Davide Torlo",
      "Mario Ricchiuto"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13315"
  },
  {
    "id": "arXiv:2205.13316",
    "title": "Fair Representation Learning through Implicit Path Alignment",
    "abstract": "We consider a fair representation learning perspective, where optimal\npredictors, on top of the data representation, are ensured to be invariant with\nrespect to different sub-groups. Specifically, we formulate this intuition as a\nbi-level optimization, where the representation is learned in the outer-loop,\nand invariant optimal group predictors are updated in the inner-loop. Moreover,\nthe proposed bi-level objective is demonstrated to fulfill the sufficiency\nrule, which is desirable in various practical scenarios but was not commonly\nstudied in the fair learning. Besides, to avoid the high computational and\nmemory cost of differentiating in the inner-loop of bi-level objective, we\npropose an implicit path alignment algorithm, which only relies on the solution\nof inner optimization and the implicit differentiation rather than the exact\noptimization path. We further analyze the error gap of the implicit approach\nand empirically validate the proposed method in both classification and\nregression settings. Experimental results show the consistently better\ntrade-off in prediction performance and fairness measurement.",
    "descriptor": "",
    "authors": [
      "Changjian Shui",
      "Qi Chen",
      "Jiaqi Li",
      "Boyu Wang",
      "Christian Gagn\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13316"
  },
  {
    "id": "arXiv:2205.13317",
    "title": "Capacity Analysis of Molecular Communications with Ratio Shift Keying  Modulation",
    "abstract": "Molecular Communications (MC) is a bio-inspired communication technique that\nuses molecules to encode and transfer information. Many efforts have been\nfocused on developing new modulation techniques for MC by exploiting\ndistinguishable properties of molecules. In this paper, we investigate a\nparticular modulation scheme where the information is encoded into the\nconcentration ratio of two different types of molecules. To evaluate the\nperformance of this so-called Ratio Shift Keying (RSK) modulation, we carry out\nan information theoretical analysis and derive the capacity of the end-to-end\nMC channel where the receiver performs ratio estimation based on\nligand-receptor binding statistics in an optimal or suboptimal manner. The\nnumerical results, obtained for varying similarity between the ligand types\nemployed for ratio-encoding, and number of receptors, indicate that the RSK can\noutperform the concentration shift keying (CSK) modulation, the most common\ntechnique considered in literature, when the transmitter is power-limited. The\nresults also indicate the potential advantages of RSK over other modulation\nmethods under time-varying channel conditions, when the effects of the dynamic\nconditions are invariant to the type of the molecules.",
    "descriptor": "",
    "authors": [
      "M. Serkan Kopuzlu",
      "M. Okan Araz",
      "Ahmet R. Emirdagi",
      "Murat Kuscu"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.13317"
  },
  {
    "id": "arXiv:2205.13320",
    "title": "Towards Learning Universal Hyperparameter Optimizers with Transformers",
    "abstract": "Meta-learning hyperparameter optimization (HPO) algorithms from prior\nexperiments is a promising approach to improve optimization efficiency over\nobjective functions from a similar distribution. However, existing methods are\nrestricted to learning from experiments sharing the same set of\nhyperparameters. In this paper, we introduce the OptFormer, the first\ntext-based Transformer HPO framework that provides a universal end-to-end\ninterface for jointly learning policy and function prediction when trained on\nvast tuning data from the wild. Our extensive experiments demonstrate that the\nOptFormer can imitate at least 7 different HPO algorithms, which can be further\nimproved via its function uncertainty estimates. Compared to a Gaussian\nProcess, the OptFormer also learns a robust prior distribution for\nhyperparameter response functions, and can thereby provide more accurate and\nbetter calibrated predictions. This work paves the path to future extensions\nfor training a Transformer-based model as a general HPO optimizer.",
    "descriptor": "",
    "authors": [
      "Yutian Chen",
      "Xingyou Song",
      "Chansoo Lee",
      "Zi Wang",
      "Qiuyi Zhang",
      "David Dohan",
      "Kazuya Kawakami",
      "Greg Kochanski",
      "Arnaud Doucet",
      "Marc'aurelio Ranzato",
      "Sagi Perel",
      "Nando de Freitas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13320"
  },
  {
    "id": "arXiv:2205.13322",
    "title": "DoS Attacks on Blockchain Ecosystem",
    "abstract": "Denial of Service (DoS) attacks are a growing threat in network services. The\nfrequency and intensity of DoS attacks are rapidly increasing day by day. The\nimmense financial potential of the Cryptocurrency market is a prevalent target\nof the DoS attack. The DoS attack events are kept on happening in\ncryptocurrencies and the blockchain ecosystem. To the best of our knowledge,\nthere has not been any study on the DoS attack on the blockchain ecosystem. In\nthis paper, we identify ten entities in the blockchain ecosystem and we\nscrutinize the DoS attacks on them. We also present the DoS mitigation\ntechniques applicable to the blockchain services. Additionally, we propose a\nDoS mitigation technique by the use of verifiable delay function (VDF).",
    "descriptor": "\nComments: Accepted at 4TH INTERNATIONAL WORKSHOP ON FUTURE PERSPECTIVE OF DECENTRALIZED APPLICATIONS (FPDAPP), Euro-Par 2021: Parallel Processing Workshops\n",
    "authors": [
      "Mayank Raikwar",
      "Danilo Gligoroski"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13322"
  },
  {
    "id": "arXiv:2205.13323",
    "title": "The Effect of Task Ordering in Continual Learning",
    "abstract": "We investigate the effect of task ordering on continual learning performance.\nWe conduct an extensive series of empirical experiments on synthetic and\nnaturalistic datasets and show that reordering tasks significantly affects the\namount of catastrophic forgetting. Connecting to the field of curriculum\nlearning, we show that the effect of task ordering can be exploited to modify\ncontinual learning performance, and present a simple approach for doing so. Our\nmethod computes the distance between all pairs of tasks, where distance is\ndefined as the source task curvature of a gradient step toward the target task.\nUsing statistically rigorous methods and sound experimental design, we show\nthat task ordering is an important aspect of continual learning that can be\nmodified for improved performance.",
    "descriptor": "",
    "authors": [
      "Samuel J. Bell",
      "Neil D. Lawrence"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13323"
  },
  {
    "id": "arXiv:2205.13324",
    "title": "Stochastic Geometry Analysis of Spectrum Sharing Among Multiple Seller  and Buyer Mobile Operators",
    "abstract": "Sharing the licensed frequency spectrum among multiple mobile network\noperators (MNOs) is a promising approach to improve licensed spectrum\nutilization. In this paper, we model and analyze a non-orthogonal spectrum\nsharing system consisting of multiple seller and multiple buyer MNOs where\nbuyer MNOs lease several licensed sub-bands from different seller MNOs. All\nbase stations (BSs) owned by a buyer MNO can also utilize various licensed\nsub-bands simultaneously, which are also used by other buyer MNOs. To reduce\nthe interference that a buyer MNO imposes on one seller MNO sharing its\nlicensed sub-band, this buyer MNO has a limitation on the maximum interference\ncaused to the corresponding seller MNO's users. We assume each MNO owns its BSs\nand users whose locations are modeled as two independent homogeneous Poisson\npoint processes. Applying stochastic geometry, we derive expressions for the\ndownlink signal-to-interference-plus-noise ratio coverage probability and the\naverage rate of both seller and buyer networks. The numerical results validate\nour analysis with simulation and illustrate the effect of the maximum\ninterference threshold on the total sum-rate of the network.",
    "descriptor": "\nComments: 6 pages, 4 figures, 2022 IEEE Wireless Communications and Networking Conference (WCNC)\n",
    "authors": [
      "Elaheh Ataeebojd",
      "Mehdi Rasti",
      "Hossein Pedram",
      "Pedro H. J. Nardelli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13324"
  },
  {
    "id": "arXiv:2205.13326",
    "title": "SHREC 2022: pothole and crack detection in the road pavement using  images and RGB-D data",
    "abstract": "This paper describes the methods submitted for evaluation to the SHREC 2022\ntrack on pothole and crack detection in the road pavement. A total of 7\ndifferent runs for the semantic segmentation of the road surface are compared,\n6 from the participants plus a baseline method. All methods exploit Deep\nLearning techniques and their performance is tested using the same environment\n(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic\nsegmentation image/mask pairs and 797 RGB-D video clips collected with the\nlatest depth cameras was made available to the participants. The methods are\nthen evaluated on the 496 image/mask pairs in the validation set, on the 504\npairs in the test set and finally on 8 video clips. The analysis of the results\nis based on quantitative metrics for image segmentation and qualitative\nanalysis of the video clips. The participation and the results show that the\nscenario is of great interest and that the use of RGB-D data is still\nchallenging in this context.",
    "descriptor": "",
    "authors": [
      "Elia Moscoso Thompson",
      "Andrea Ranieri",
      "Silvia Biasotti",
      "Miguel Chicchon",
      "Ivan Sipiran",
      "Minh-Khoi Pham",
      "Thang-Long Nguyen-Ho",
      "Hai-Dang Nguyen",
      "Minh-Triet Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13326"
  },
  {
    "id": "arXiv:2205.13328",
    "title": "How Powerful are K-hop Message Passing Graph Neural Networks",
    "abstract": "The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop\nmessage passing -- aggregating features from 1-hop neighbors repeatedly.\nHowever, the expressive power of 1-hop message passing is bounded by the\nWeisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message\npassing to K-hop message passing by aggregating information from K-hop\nneighbors of nodes simultaneously. However, there is no work on analyzing the\nexpressive power of K-hop message passing. In this work, we theoretically\ncharacterize the expressive power of K-hop message passing. Specifically, we\nfirst formally differentiate two kinds of kernels of K-hop message passing\nwhich are often misused in previous works. We then characterize the expressive\npower of K-hop message passing by showing that it is more powerful than 1-hop\nmessage passing. Despite the higher expressive power, we show that K-hop\nmessage passing still cannot distinguish some simple regular graphs. To further\nenhance its expressive power, we introduce a KP-GNN framework, which improves\nK-hop message passing by leveraging the peripheral subgraph information in each\nhop. We prove that KP-GNN can distinguish almost all regular graphs including\nsome distance regular graphs which could not be distinguished by previous\ndistance encoding methods. Experimental results verify the expressive power and\neffectiveness of KP-GNN. KP-GNN achieves competitive results across all\nbenchmark datasets.",
    "descriptor": "",
    "authors": [
      "Jiarui Feng",
      "Yixin Chen",
      "Fuhai Li",
      "Anindya Sarkar",
      "Muhan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13328"
  },
  {
    "id": "arXiv:2205.13330",
    "title": "Analysis of a Learning Based Algorithm for Budget Pacing",
    "abstract": "In this paper, we analyze a natural learning algorithm for uniform pacing of\nadvertising budgets, equipped to adapt to varying ad sale platform conditions.\nOn the demand side, advertisers face a fundamental technical challenge in\nautomating bidding in a way that spreads their allotted budget across a given\ncampaign subject to hidden, and potentially dynamic, \"spent amount\" functions.\nThis automation and calculation must be done in runtime, implying a necessary\nlow computational cost for the high frequency auction rate. Advertisers are\nadditionally expected to exhaust nearly all of their sub-interval (by the hour\nor minute) budgets to maintain budgeting quotas in the long run. Our study\nanalyzes a simple learning algorithm that adapts to the latent spent amount\nfunction of the market and learns the optimal average bidding value for a\nperiod of auctions in a small fraction of the total campaign time, allowing for\nsmooth budget pacing in real-time. We prove our algorithm is robust to changes\nin the auction mechanism, and exhibits a fast convergence to a stable average\nbidding strategy. The algorithm not only guarantees that budgets are nearly\nspent in their entirety, but also smoothly paces bidding to prevent early exit\nfrom the campaign and a loss of the opportunity to bid on potentially lucrative\nimpressions later in the period.",
    "descriptor": "\nComments: 10 pages, 1 figure\n",
    "authors": [
      "MohammadTaghi Hajiaghayi",
      "Max Springer"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13330"
  },
  {
    "id": "arXiv:2205.13331",
    "title": "TransBoost: Improving the Best ImageNet Performance using Deep  Transduction",
    "abstract": "This paper deals with deep transductive learning, and proposes TransBoost as\na procedure for fine-tuning any deep neural model to improve its performance on\nany (unlabeled) test set provided at training time. TransBoost is inspired by a\nlarge margin principle and is efficient and simple to use. The ImageNet\nclassification performance is consistently and significantly improved with\nTransBoost on many architectures such as ResNets, MobileNetV3-L,\nEfficientNetB0, ViT-S, and ConvNext-T. Additionally we show that TransBoost is\neffective on a wide variety of image classification datasets.",
    "descriptor": "",
    "authors": [
      "Omer Belhasin",
      "Guy Bar-Shalom",
      "Ran El-Yaniv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13331"
  },
  {
    "id": "arXiv:2205.13333",
    "title": "SoK: Decentralized Randomness Beacon Protocols",
    "abstract": "The scientific interest in the area of Decentralized Randomness Beacon (DRB)\nprotocols has been thriving recently. Partially that interest is due to the\nsuccess of the disruptive technologies introduced by modern cryptography, such\nas cryptocurrencies, blockchain technologies, and decentralized finances, where\nthere is an enormous need for a public, reliable, trusted, verifiable, and\ndistributed source of randomness. On the other hand, recent advancements in the\ndevelopment of new cryptographic primitives brought a huge interest in\nconstructing a plethora of DRB protocols differing in design and underlying\nprimitives.\nTo the best of our knowledge, no systematic and comprehensive work\nsystematizes and analyzes the existing DRB protocols. Therefore, we present a\nSystematization of Knowledge (SoK) intending to structure the multi-faced body\nof research on DRB protocols. In this SoK, we delineate the DRB protocols along\nthe following axes: their underlying primitive, properties, and security. This\nSoK tries to fill that gap by providing basic standard definitions and\nrequirements for DRB protocols, such as Unpredictability, Bias-resistance,\nAvailability (or Liveness), and Public Verifiability. We classify DRB protocols\naccording to the nature of interactivity among protocol participants. We also\nhighlight the most significant features of DRB protocols such as scalability,\ncomplexity, and performance along with a brief discussion on its improvement.\nWe present future research directions along with a few interesting research\nproblems.",
    "descriptor": "\nComments: Accepted at the 27th Australasian Conference on Information Security and Privacy (ACISP 2022)\n",
    "authors": [
      "Mayank Raikwar",
      "Danilo Gligoroski"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13333"
  },
  {
    "id": "arXiv:2205.13339",
    "title": "Target-aware Abstractive Related Work Generation with Contrastive  Learning",
    "abstract": "The related work section is an important component of a scientific paper,\nwhich highlights the contribution of the target paper in the context of the\nreference papers. Authors can save their time and effort by using the\nautomatically generated related work section as a draft to complete the final\nrelated work. Most of the existing related work section generation methods rely\non extracting off-the-shelf sentences to make a comparative discussion about\nthe target work and the reference papers. However, such sentences need to be\nwritten in advance and are hard to obtain in practice. Hence, in this paper, we\npropose an abstractive target-aware related work generator (TAG), which can\ngenerate related work sections consisting of new sentences. Concretely, we\nfirst propose a target-aware graph encoder, which models the relationships\nbetween reference papers and the target paper with target-centered attention\nmechanisms. In the decoding process, we propose a hierarchical decoder that\nattends to the nodes of different levels in the graph with keyphrases as\nsemantic indicators. Finally, to generate a more informative related work, we\npropose multi-level contrastive optimization objectives, which aim to maximize\nthe mutual information between the generated related work with the references\nand minimize that with non-references. Extensive experiments on two public\nscholar datasets show that the proposed model brings substantial improvements\nover several strong baselines in terms of automatic and tailored human\nevaluations.",
    "descriptor": "\nComments: 11 pages, 7 figures, SIGIR 2022\n",
    "authors": [
      "Xiuying Chen",
      "Hind Alamro",
      "Mingzhe Li",
      "Shen Gao",
      "Rui Yan",
      "Xin Gao",
      "Xiangliang Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13339"
  },
  {
    "id": "arXiv:2205.13340",
    "title": "Deep Active Learning with Noise Stability",
    "abstract": "Uncertainty estimation for unlabeled data is crucial to active learning. With\na deep neural network employed as the backbone model, the data selection\nprocess is highly challenging due to the potential over-confidence of the model\ninference. Existing methods resort to special learning fashions (e.g.\nadversarial) or auxiliary models to address this challenge. This tends to\nresult in complex and inefficient pipelines, which would render the methods\nimpractical. In this work, we propose a novel algorithm that leverages noise\nstability to estimate data uncertainty in a Single-Training Multi-Inference\nfashion. The key idea is to measure the output derivation from the original\nobservation when the model parameters are randomly perturbed by noise. We\nprovide theoretical analyses by leveraging the small Gaussian noise theory and\ndemonstrate that our method favors a subset with large and diverse gradients.\nDespite its simplicity, our method outperforms the state-of-the-art active\nlearning baselines in various tasks, including computer vision, natural\nlanguage processing, and structural data analysis.",
    "descriptor": "",
    "authors": [
      "Xingjian Li",
      "Pengkun Yang",
      "Tianyang Wang",
      "Xueying Zhan",
      "Min Xu",
      "Dejing Dou",
      "Chengzhong Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13340"
  },
  {
    "id": "arXiv:2205.13341",
    "title": "QUICK-FL: Quick Unbiased Compression for Federated Learning",
    "abstract": "Distributed Mean Estimation (DME) is a fundamental building block in\ncommunication efficient federated learning. In DME, clients communicate their\nlossily compressed gradients to the parameter server, which estimates the\naverage and updates the model. State of the art DME techniques apply either\nunbiased quantization methods, resulting in large estimation errors, or biased\nquantization methods, where unbiasing the result requires that the server\ndecodes each gradient individually, which markedly slows the aggregation time.\nIn this paper, we propose QUIC-FL, a DME algorithm that achieves the best of\nall worlds. QUIC-FL is unbiased, offers fast aggregation time, and is\ncompetitive with the most accurate (slow aggregation) DME techniques. To\nachieve this, we formalize the problem in a novel way that allows us to use\nstandard solvers to design near-optimal unbiased quantization schemes.",
    "descriptor": "",
    "authors": [
      "Ran Ben Basat",
      "Shay Vargaftik",
      "Amit Portnoy",
      "Gil Einziger",
      "Yaniv Ben-Itzhak",
      "Michael Mitzenmacher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.13341"
  },
  {
    "id": "arXiv:2205.13342",
    "title": "Leveraging Causal Inference for Explainable Automatic Program Repair",
    "abstract": "Deep learning models have made significant progress in automatic program\nrepair. However, the black-box nature of these methods has restricted their\npractical applications. To address this challenge, this paper presents an\ninterpretable approach for program repair based on sequence-to-sequence models\nwith causal inference and our method is called CPR, short for causal program\nrepair. Our CPR can generate explanations in the process of decision making,\nwhich consists of groups of causally related input-output tokens. Firstly, our\nmethod infers these relations by querying the model with inputs disturbed by\ndata augmentation. Secondly, it generates a graph over tokens from the\nresponses and solves a partitioning problem to select the most relevant\ncomponents. The experiments on four programming languages (Java, C, Python, and\nJavaScript) show that CPR can generate causal graphs for reasonable\ninterpretations and boost the performance of bug fixing in automatic program\nrepair.",
    "descriptor": "\nComments: This paper has been accepted by IJCNN2022. arXiv admin note: text overlap with arXiv:1707.01943, arXiv:2103.11626 by other authors\n",
    "authors": [
      "Jianzong Wang",
      "Shijing Si",
      "Zhitao Zhu",
      "Xiaoyang Qu",
      "Zhenhou Hong",
      "Jing Xiao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13342"
  },
  {
    "id": "arXiv:2205.13343",
    "title": "Sliding mode control with a neural network compensation scheme for  electro-hydraulic systems",
    "abstract": "Electro-hydraulic servo-systems are widely employed in industrial\napplications such as robotic manipulators, active suspensions, precision\nmachine tools and aerospace systems. They provide many advantages over electric\nmotors, including high force to weight ratio, fast response time and compact\nsize. However, precise control of electro-hydraulic systems, due to their\ninherent nonlinear characteristics, cannot be easily obtained with conventional\nlinear controllers. Most flow control valves can also exhibit some hard\nnonlinearities such as dead-zone due to valve spool overlap. This work\ndescribes the development of a sliding mode controller with a neural network\ncompensation scheme for electro-hydraulic systems subject to an unknown\ndead-zone input. The boundedness and convergence properties of the closed-loop\nsignals are proven using Lyapunov stability theory. Numerical results are\npresented in order to demonstrate the control system performance.",
    "descriptor": "",
    "authors": [
      "Josiane Maria de Macedo Fernandes",
      "Marcelo Costa Tanaka",
      "Wallace Moreira Bessa"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13343"
  },
  {
    "id": "arXiv:2205.13344",
    "title": "A neural network based controller for underwater robotic vehicles",
    "abstract": "Due to the enormous technological improvements obtained in the last decades\nit is possible to use robotic vehicles for underwater exploration. This work\ndescribes the development of a dynamic positioning system for remotely operated\nunderwater vehicles based. The adopted approach is developed using Lyapunov\nStability Theory and enhanced by a neural network based algorithm for\nuncertainty and disturbance compensation. The performance of the proposed\ncontrol scheme is evaluated by means of numerical simulations.",
    "descriptor": "",
    "authors": [
      "Josiane Maria Macedo Fernandes",
      "Marcelo Costa Tanaka",
      "Raimundo Carlos Silv\u00e9rio Freire J\u00fanior",
      "Wallace Moreira Bessa"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13344"
  },
  {
    "id": "arXiv:2205.13345",
    "title": "Properties of the Hebrew Calendar",
    "abstract": "We describe an ACL2 program that implements the Hebrew calendar and the\nformal verification of several of its properties, including the critical result\nthat the algorithm that determines the placement of the new year ensures that\nthe length of every year belongs to a small set of admissible values. These\nproperties have been accepted for many centuries without the benefit of\nexplicit proof, in spite of subtleties in the underlying arguments. For the\nsake of accessibility to a broad audience, the program is coded in Restricted\nAlgorithmic C (RAC), a simple language consisting of the most basic constructs\nof C, for which an automatic translator to the ACL2 logic has been implemented.\nWhile RAC is primarily intended for modeling arithmetic hardware designs, this\nnovel application provides a relatively simple illustration of the language and\nthe translator.",
    "descriptor": "\nComments: In Proceedings ACL2 2022, arXiv:2205.11103\n",
    "authors": [
      "David M. Russinoff"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13345"
  },
  {
    "id": "arXiv:2205.13346",
    "title": "Keywords and Instances: A Hierarchical Contrastive Learning Framework  Unifying Hybrid Granularities for Text Generation",
    "abstract": "Contrastive learning has achieved impressive success in generation tasks to\nmilitate the \"exposure bias\" problem and discriminatively exploit the different\nquality of references. Existing works mostly focus on contrastive learning on\nthe instance-level without discriminating the contribution of each word, while\nkeywords are the gist of the text and dominant the constrained mapping\nrelationships. Hence, in this work, we propose a hierarchical contrastive\nlearning mechanism, which can unify hybrid granularities semantic meaning in\nthe input text. Concretely, we first propose a keyword graph via contrastive\ncorrelations of positive-negative pairs to iteratively polish the keyword\nrepresentations. Then, we construct intra-contrasts within instance-level and\nkeyword-level, where we assume words are sampled nodes from a sentence\ndistribution. Finally, to bridge the gap between independent contrast levels\nand tackle the common contrast vanishing problem, we propose an inter-contrast\nmechanism that measures the discrepancy between contrastive keyword nodes\nrespectively to the instance distribution. Experiments demonstrate that our\nmodel outperforms competitive baselines on paraphrasing, dialogue generation,\nand storytelling tasks.",
    "descriptor": "\nComments: Accepted by ACL2022\n",
    "authors": [
      "Mingzhe Li",
      "XieXiong Lin",
      "Xiuying Chen",
      "Jinxiong Chang",
      "Qishen Zhang",
      "Feng Wang",
      "Taifeng Wang",
      "Zhongyi Liu",
      "Wei Chu",
      "Dongyan Zhao",
      "Rui Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13346"
  },
  {
    "id": "arXiv:2205.13347",
    "title": "A Formalization of Finite Group Theory",
    "abstract": "Previous formulations of group theory in ACL2 and Nqthm, based on either\n\"encapsulate\" or \"defn-sk\", have been limited by their failure to provide a\npath to proof by induction on the order of a group, which is required for most\ninteresting results in this domain beyond Lagrange's Theorem (asserting the\ndivisibility of the order of a group by that of a subgroup). We describe an\nalternative approach to finite group theory that remedies this deficiency,\nbased on an explicit representation of a group as an operation table. We define\na \"defgroup\" macro for generating parametrized families of groups, which we\napply to the additive and multiplicative groups of integers modulo n, the\nsymmetric groups, arbitrary quotient groups, and cyclic subgroups. In addition\nto a proof of Lagrange's Theorem, we present an inductive proof of the abelian\ncase of a theorem of Cauchy: If the order of a group G is divisible by a prime\np, then G has an element of order p.",
    "descriptor": "\nComments: In Proceedings ACL2 2022, arXiv:2205.11103\n",
    "authors": [
      "David M. Russinoff"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13347"
  },
  {
    "id": "arXiv:2205.13349",
    "title": "Learning What and Where -- Unsupervised Disentangling Location and  Identity Tracking",
    "abstract": "Our brain can almost effortlessly decompose visual data streams into\nbackground and salient objects. Moreover, it can track the objects and\nanticipate their motion and interactions. In contrast, recent object reasoning\ndatasets, such as CATER, have revealed fundamental shortcomings of current\nvision-based AI systems, particularly when targeting explicit object encodings,\nobject permanence, and object reasoning. We introduce an unsupervised\ndisentangled LOCation and Identity tracking system (Loci), which excels on the\nCATER tracking challenge. Inspired by the dorsal-ventral pathways in the brain,\nLoci tackles the what-and-where binding problem by means of a self-supervised\nsegregation mechanism. Our autoregressive neural network partitions and\ndistributes the visual input stream across separate, identically-parameterized\nand autonomously recruited neural network modules. Each module binds what with\nwhere, that is, compressed Gestalt encodings with locations. On the deep latent\nencoding levels interaction dynamics are processed. Besides exhibiting superior\nperformance in current benchmarks, we propose that Loci may set the stage for\ndeeper, explanation-oriented video processing -- akin to some deeper networked\nprocesses in the brain that appear to integrate individual entity and\nspatiotemporal interaction dynamics into event structures.",
    "descriptor": "",
    "authors": [
      "Manuel Traub",
      "Sebastian Otte",
      "Tobias Menge",
      "Matthias Karlbauer",
      "Jannik Th\u00fcmmel",
      "Martin V. Butz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13349"
  },
  {
    "id": "arXiv:2205.13350",
    "title": "On the interface matrix for fluid-structure interaction problems with  fictitious domain approach",
    "abstract": "We study a recent formulation for fluid-structure interaction problems based\non the use of a distributed Lagrange multiplier in the spirit of the fictitious\ndomain approach. In this paper, we focus our attention on a crucial\ncomputational aspect regarding the interface matrix for the finite element\ndiscretization: it involves integration of functions supported on two different\nmeshes. Several numerical tests show that accurate computation of the interface\nmatrix has to be performed in order to ensure the optimal convergence of the\nmethod.",
    "descriptor": "\nComments: 30 pages, 18 figures\n",
    "authors": [
      "Daniele Boffi",
      "Fabio Credali",
      "Lucia Gastaldi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13350"
  },
  {
    "id": "arXiv:2205.13351",
    "title": "LeiBi@COLIEE 2022: Aggregating Tuned Lexical Models with a  Cluster-driven BERT-based Model for Case Law Retrieval",
    "abstract": "This paper summarizes our approaches submitted to the case law retrieval task\nin the Competition on Legal Information Extraction/Entailment (COLIEE) 2022.\nOur methodology consists of four steps; in detail, given a legal case as a\nquery, we reformulate it by extracting various meaningful sentences or n-grams.\nThen, we utilize the pre-processed query case to retrieve an initial set of\npossible relevant legal cases, which we further re-rank. Lastly, we aggregate\nthe relevance scores obtained by the first stage and the re-ranking models to\nimprove retrieval effectiveness. In each step of our methodology, we explore\nvarious well-known and novel methods. In particular, to reformulate the query\ncases aiming to make them shorter, we extract unigrams using three different\nstatistical methods: KLI, PLM, IDF-r, as well as models that leverage\nembeddings (e.g., KeyBERT). Moreover, we investigate if automatic summarization\nusing Longformer-Encoder-Decoder (LED) can produce an effective query\nrepresentation for this retrieval task. Furthermore, we propose a novel\nre-ranking cluster-driven approach, which leverages Sentence-BERT models that\nare pre-tuned on large amounts of data for embedding sentences from query and\ncandidate documents. Finally, we employ a linear aggregation method to combine\nthe relevance scores obtained by traditional IR models and neural-based models,\naiming to incorporate the semantic understanding of neural models and the\nstatistically measured topical relevance. We show that aggregating these\nrelevance scores can improve the overall retrieval effectiveness.",
    "descriptor": "\nComments: Accepted at the COLIEE Workshop in Proceedings of JURISIN 2022. Please cite the published version\n",
    "authors": [
      "Arian Askari",
      "Georgios Peikos",
      "Gabriella Pasi",
      "Suzan Verberne"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.13351"
  },
  {
    "id": "arXiv:2205.13355",
    "title": "Single-pass Nystr\u00f6m approximation in mixed precision",
    "abstract": "Low rank matrix approximations appear in a number of scientific computing\napplications. We consider the Nystr\\\"{o}m method for approximating a positive\nsemidefinite matrix $A$. The computational cost of its single-pass version can\nbe decreased by running it in mixed precision, where the expensive products\nwith $A$ are computed in a lower than the working precision. We bound the extra\nfinite precision error which is compared to the error of the Nystr\\\"{o}m\napproximation in exact arithmetic and identify when the approximation quality\nis not affected by the low precision computations. The mixed precision\nNystr\\\"{o}m method can be used to inexpensively construct a limited memory\npreconditioner for the conjugate gradient method. We bound the condition number\nof the preconditioned coefficient matrix, and experimentally show that such\npreconditioner can be effective.",
    "descriptor": "\nComments: 22 pages, 7 figures\n",
    "authors": [
      "Erin Carson",
      "Ieva Dau\u017eickait\u0117"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13355"
  },
  {
    "id": "arXiv:2205.13357",
    "title": "The Document Vectors Using Cosine Similarity Revisited",
    "abstract": "The current state-of-the-art test accuracy (97.42\\%) on the IMDB movie\nreviews dataset was reported by \\citet{thongtan-phienthrakul-2019-sentiment}\nand achieved by the logistic regression classifier trained on the Document\nVectors using Cosine Similarity (DV-ngrams-cosine) proposed in their paper and\nthe Bag-of-N-grams (BON) vectors scaled by Naive Bayesian weights. While large\npre-trained Transformer-based models have shown SOTA results across many\ndatasets and tasks, the aforementioned model has not been surpassed by them,\ndespite being much simpler and pre-trained on the IMDB dataset only.\nIn this paper, we describe an error in the evaluation procedure of this\nmodel, which was found when we were trying to analyze its excellent performance\non the IMDB dataset. We further show that the previously reported test accuracy\nof 97.42\\% is invalid and should be corrected to 93.68\\%. We also analyze the\nmodel performance with different amounts of training data (subsets of the IMDB\ndataset) and compare it to the Transformer-based RoBERTa model. The results\nshow that while RoBERTa has a clear advantage for larger training sets, the\nDV-ngrams-cosine performs better than RoBERTa when the labelled training set is\nvery small (10 or 20 documents). Finally, we introduce a sub-sampling scheme\nbased on Naive Bayesian weights for the training process of the\nDV-ngrams-cosine, which leads to faster training and better quality.",
    "descriptor": "",
    "authors": [
      "Zhang Bingyu",
      "Nikolay Arefyev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13357"
  },
  {
    "id": "arXiv:2205.13358",
    "title": "Transfer and Share: Semi-Supervised Learning from Long-Tailed Data",
    "abstract": "Long-Tailed Semi-Supervised Learning (LTSSL) aims to learn from\nclass-imbalanced data where only a few samples are annotated. Existing\nsolutions typically require substantial cost to solve complex optimization\nproblems, or class-balanced undersampling which can result in information loss.\nIn this paper, we present the TRAS (TRAnsfer and Share) to effectively utilize\nlong-tailed semi-supervised data. TRAS transforms the imbalanced pseudo-label\ndistribution of a traditional SSL model via a delicate function to enhance the\nsupervisory signals for minority classes. It then transfers the distribution to\na target model such that the minority class will receive significant attention.\nInterestingly, TRAS shows that more balanced pseudo-label distribution can\nsubstantially benefit minority-class training, instead of seeking to generate\naccurate pseudo-labels as in previous works. To simplify the approach, TRAS\nmerges the training of the traditional SSL model and the target model into a\nsingle procedure by sharing the feature extractor, where both classifiers help\nimprove the representation learning. According to extensive experiments, TRAS\ndelivers much higher accuracy than state-of-the-art methods in the entire set\nof classes as well as minority classes.",
    "descriptor": "\nComments: paper under review\n",
    "authors": [
      "Tong Wei",
      "Qian-Yu Liu",
      "Jiang-Xin Shi",
      "Wei-Wei Tu",
      "Lan-Zhe Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13358"
  },
  {
    "id": "arXiv:2205.13359",
    "title": "Feature Forgetting in Continual Representation Learning",
    "abstract": "In continual and lifelong learning, good representation learning can help\nincrease performance and reduce sample complexity when learning new tasks.\nThere is evidence that representations do not suffer from \"catastrophic\nforgetting\" even in plain continual learning, but little further fact is known\nabout its characteristics. In this paper, we aim to gain more understanding\nabout representation learning in continual learning, especially on the feature\nforgetting problem. We devise a protocol for evaluating representation in\ncontinual learning, and then use it to present an overview of the basic trends\nof continual representation learning, showing its consistent deficiency and\npotential issues. To study the feature forgetting problem, we create a\nsynthetic dataset to identify and visualize the prevalence of feature\nforgetting in neural networks. Finally, we propose a simple technique using\ngating adapters to mitigate feature forgetting. We conclude by discussing that\nimproving representation learning benefits both old and new tasks in continual\nlearning.",
    "descriptor": "",
    "authors": [
      "Xiao Zhang",
      "Dejing Dou",
      "Ji Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13359"
  },
  {
    "id": "arXiv:2205.13362",
    "title": "Multi-fidelity power flow solver",
    "abstract": "We propose a multi-fidelity neural network (MFNN) tailored for rapid\nhigh-dimensional grid power flow simulations and contingency analysis with\nscarce high-fidelity contingency data. The proposed model comprises two\nnetworks -- the first one trained on DC approximation as low-fidelity data and\ncoupled to a high-fidelity neural net trained on both low- and high-fidelity\npower flow data. Each network features a latent module which parametrizes the\nmodel by a discrete grid topology vector for generalization (e.g., $n$ power\nlines with $k$ disconnections or contingencies, if any), and the targeted\nhigh-fidelity output is a weighted sum of linear and nonlinear functions. We\ntested the model on 14- and 118-bus test cases and evaluated its performance\nbased on the $n-k$ power flow prediction accuracy with respect to imbalanced\ncontingency data and high-to-low-fidelity sample ratio. The results presented\nherein demonstrate MFNN's potential and its limits with up to two orders of\nmagnitude faster and more accurate power flow solutions than DC approximation.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Sam Yang",
      "Bjorn Vaagensmith",
      "Deepika Patra",
      "Ryan Hruska",
      "Tyler Phillips"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13362"
  },
  {
    "id": "arXiv:2205.13366",
    "title": "Minimization of THD in Nine Level Cascaded H-Bridge Inverter Using  Artificial Neural Network",
    "abstract": "Multilevel inverter converts different level DC voltage to AC voltage. It has\nwide interest in power industry especially in high power applications. In power\nelectronic equipment the major drawback is the harmonics. Several control\nstrategies are available to reduce the harmonic content and the most widely\nused measure of Total Harmonic Distortion (THD). In this project, the\ncomparison has been made for the open loop and closed loop PI controller and\nneural network that predict the switching angle in order to reduce the\nharmonics. The mapping between Modulation Index and Switching angles are\nplotted for the forward neural network. After the prediction of switching\nangles the neural network topologies are executed for better result. This\ntechnique is applied for any type of multilevel inverter, Cascaded H-Bridge\nmultilevel inverter is chosen. A nine level Cascaded H-Bridge multilevel\ninverter power circuit is simulated in MATLAB 8.3 simulink with sinusoidal PWM\ntechnique. The comparison results reveal that the THD is reduced to about 3%\nwith neural network control compared to open loop control. The results are\npresented and analyzed.",
    "descriptor": "",
    "authors": [
      "Manoj Mathews",
      "B. Ramesh",
      "T. Sreedhar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Instrumentation and Detectors (physics.ins-det)"
    ],
    "url": "https://arxiv.org/abs/2205.13366"
  },
  {
    "id": "arXiv:2205.13368",
    "title": "One-Shot Face Reenactment on Megapixels",
    "abstract": "The goal of face reenactment is to transfer a target expression and head pose\nto a source face while preserving the source identity. With the popularity of\nface-related applications, there has been much research on this topic. However,\nthe results of existing methods are still limited to low-resolution and lack\nphotorealism. In this work, we present a one-shot and high-resolution face\nreenactment method called MegaFR. To be precise, we leverage StyleGAN by using\n3DMM-based rendering images and overcome the lack of high-quality video\ndatasets by designing a loss function that works without high-quality videos.\nAlso, we apply iterative refinement to deal with extreme poses and/or\nexpressions. Since the proposed method controls source images through 3DMM\nparameters, we can explicitly manipulate source images. We apply MegaFR to\nvarious applications such as face frontalization, eye in-painting, and talking\nhead generation. Experimental results show that our method successfully\ndisentangles identity from expression and head pose, and outperforms\nconventional methods.",
    "descriptor": "\nComments: 29 pages, 19 figures\n",
    "authors": [
      "Wonjun Kang",
      "Geonsu Lee",
      "Hyung Il Koo",
      "Nam Ik Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13368"
  },
  {
    "id": "arXiv:2205.13370",
    "title": "Prismal view of ethics",
    "abstract": "We shall have a hard look at ethics and try to extract insights in the form\nof abstract properties that might become tools. We want to connect ethics to\ngames, talk about the performance of ethics, introduce curiosity into the\ninterplay between competing and coordinating in well-performing ethics, and\noffer a view of possible developments that could unify increasing aggregates of\nentities. All this is under a long shadow cast by computational complexity that\nis quite negative about games. This analysis is the first step toward finding\nmodeling aspects that might be used in AI ethics for integrating modern AI\nsystems into human society.",
    "descriptor": "\nComments: 18 pages, 2 figures, 84 references\n",
    "authors": [
      "Sarah Isufi",
      "Kristijan Poje",
      "Igor Vukobratovic",
      "Mario Brcic"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13370"
  },
  {
    "id": "arXiv:2205.13371",
    "title": "A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical  Representation Learning",
    "abstract": "We present a rotated hyperbolic wrapped normal distribution (RoWN), a simple\nyet effective alteration of a hyperbolic wrapped normal distribution (HWN). The\nHWN expands the domain of probabilistic modeling from Euclidean to hyperbolic\nspace, where a tree can be embedded with arbitrary low distortion in theory. In\nthis work, we analyze the geometric properties of the diagonal HWN, a standard\nchoice of distribution in probabilistic modeling. The analysis shows that the\ndistribution is inappropriate to represent the data points at the same\nhierarchy level through their angular distance with the same norm in the\nPoincar\\'e disk model. We then empirically verify the presence of limitations\nof HWN, and show how RoWN, the newly proposed distribution, can alleviate the\nlimitations on various hierarchical datasets, including noisy synthetic binary\ntree, WordNet, and Atari 2600 Breakout.",
    "descriptor": "",
    "authors": [
      "Seunghyuk Cho",
      "Juyong Lee",
      "Jaesik Park",
      "Dongwoo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13371"
  },
  {
    "id": "arXiv:2205.13375",
    "title": "Embedded System Evolution in IoT System Development Based on MAPE-K Loop  Mechanism",
    "abstract": "Embedded systems including IoT devices are designed for specialized\nfunctions; thus, changes in functions are not considered following their\nrelease. For this reason, changing functions to satisfy the requirements of IoT\nsystems is difficult. In this study, we focus on updating existing embedded\nsystems without modifying them. We investigate the design of new functions and\ntheir implementation with limited resources. This paper describes an evolution\nmechanism for updating the functionalities of existing embedded systems. The\nevolution mechanism uses a control unit that is deployed outside the embedded\nsystem. To guide the steady implementation of the evolution mechanism, we\ndefine an evolution process that effectively uses the state machine diagram at\nthe design time and runtime to update the embedded systems. The programming\nframework implemented in this study supports the evolution process. We evaluate\nthe evolution mechanism based on the results from two experiments. The first\nexperiment involved applying the evolution mechanism to a cleaning robot, this\ndemonstrated that the evolution mechanism systematically enables the injection\nof new functions into an embedded system in the real world. The second\nexperiment, on the probabilistic model checking, demonstrated that the\nmechanism provides almost the same performance as the ordinary embedded system\nwith an improved robustness.",
    "descriptor": "",
    "authors": [
      "Hiroyuki Nakagawa",
      "Shinya Tsuchida",
      "Emiliano Tramontana",
      "Andrea Fornaia",
      "Tatsuhiro Tsuchiya"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.13375"
  },
  {
    "id": "arXiv:2205.13383",
    "title": "BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural  Networks via Image Quantization and Contrastive Adversarial Learning",
    "abstract": "Deep neural networks are vulnerable to Trojan attacks. Existing attacks use\nvisible patterns (e.g., a patch or image transformations) as triggers, which\nare vulnerable to human inspection. In this paper, we propose stealthy and\nefficient Trojan attacks, BppAttack. Based on existing biology literature on\nhuman visual systems, we propose to use image quantization and dithering as the\nTrojan trigger, making imperceptible changes. It is a stealthy and efficient\nattack without training auxiliary models. Due to the small changes made to\nimages, it is hard to inject such triggers during training. To alleviate this\nproblem, we propose a contrastive learning based approach that leverages\nadversarial attacks to generate negative sample pairs so that the learned\ntrigger is precise and accurate. The proposed method achieves high attack\nsuccess rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and\nCelebA. It also effectively bypasses existing Trojan defenses and human\ninspection. Our code can be found in\nhttps://github.com/RU-System-Software-and-Security/BppAttack.",
    "descriptor": "",
    "authors": [
      "Zhenting Wang",
      "Juan Zhai",
      "Shiqing Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13383"
  },
  {
    "id": "arXiv:2205.13384",
    "title": "Continual Learning for Visual Search with Backward Consistent Feature  Embedding",
    "abstract": "In visual search, the gallery set could be incrementally growing and added to\nthe database in practice. However, existing methods rely on the model trained\non the entire dataset, ignoring the continual updating of the model. Besides,\nas the model updates, the new model must re-extract features for the entire\ngallery set to maintain compatible feature space, imposing a high computational\ncost for a large gallery set. To address the issues of long-term visual search,\nwe introduce a continual learning (CL) approach that can handle the\nincrementally growing gallery set with backward embedding consistency. We\nenforce the losses of inter-session data coherence, neighbor-session model\ncoherence, and intra-session discrimination to conduct a continual learner. In\naddition to the disjoint setup, our CL solution also tackles the situation of\nincreasingly adding new classes for the blurry boundary without assuming all\ncategories known in the beginning and during model update. To our knowledge,\nthis is the first CL method both tackling the issue of backward-consistent\nfeature embedding and allowing novel classes to occur in the new sessions.\nExtensive experiments on various benchmarks show the efficacy of our approach\nunder a wide range of setups.",
    "descriptor": "\nComments: 15 pages with supplementary material; accepted to CVPR 2022\n",
    "authors": [
      "Timmy S. T. Wan",
      "Jun-Cheng Chen",
      "Tzer-Yi Wu",
      "Chu-Song Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13384"
  },
  {
    "id": "arXiv:2205.13387",
    "title": "Coalgebraic Fuzzy geometric logic",
    "abstract": "The paper aims to develop a framework for the coalgebraic fuzzy geometric\nlogic by adding modalities to the language of fuzzy geometric logic. Using the\nmethods of coalgebra, we introduce modal operators to the language of fuzzy\ngeometric logic. We introduce a notion of fuzzy open predicate lifting to\ndefine modal operators. Models of fuzzy geometric logic are defined on\ncoalgebras for an endofunctor $T$ on a category \\textbf{Fuzzy-Top} of fuzzy\ntopological spaces and fuzzy continuous maps. Bisimulations for the defined\nmodels have been discussed in this paper.",
    "descriptor": "",
    "authors": [
      "Litan Kumar Das",
      "Kumar Sankar Ray",
      "Prakash Chandra Mali"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13387"
  },
  {
    "id": "arXiv:2205.13394",
    "title": "Dynamic Interventions for Networked Contagions",
    "abstract": "We study the problem of designing dynamic intervention policies for\nminimizing networked defaults in financial networks. Formally, we consider a\ndynamic version of the celebrated Eisenberg-Noe model of financial network\nliabilities, and use this to study the design of external intervention\npolicies. Our controller has a fixed resource budget in each round, and can use\nthis to minimize the effect of demand/supply shocks in the network. We\nformulate the optimal intervention problem as a Markov Decision Process, and\nshow how we can leverage the problem structure to efficiently compute optimal\nintervention policies with continuous interventions, and constant-factor\napproximations with discrete interventions. Going beyond financial networks, we\nargue that our model captures dynamic network intervention in a much broader\nclass of dynamic demand/supply settings with networked inter-dependencies. To\ndemonstrate this, we apply our intervention algorithms to a wide variety of\napplication domains, including ridesharing, online transaction platforms, and\nfinancial networks with agent mobility; in each case, we study the relationship\nbetween node centrality and intervention strength, as well as fairness\nproperties of the optimal interventions.",
    "descriptor": "",
    "authors": [
      "Marios Papachristou",
      "Siddhartha Banerjee",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13394"
  },
  {
    "id": "arXiv:2205.13398",
    "title": "Looking for Out-of-Distribution Environments in Critical Care: A case  study with the eICU Database",
    "abstract": "Generalizing to new populations and domains in machine learning is still an\nopen problem which has seen increased interest recently. In particular,\nclinical models show a significant performance drop when tested in settings not\nseen during training, e.g., new hospitals or population demographics. Recently\nproposed models for domain generalisation promise to alleviate this problem by\nlearning invariant characteristics across environments, however, there is still\nscepticism about whether they improve over traditional training. In this work,\nwe take a principled approach to identifying Out of Distribution (OoD)\nenvironments, motivated by the problem of cross-hospital generalization in\ncritical care. We propose model-based and heuristic approaches to identify OoD\nenvironments and systematically compare models with different levels of\nheld-out information. In particular, based on the assumption that models with\naccess to OoD data should outperform other models, we train models across a\nrange of experimental setups that include leave-one-hospital-out training and\ncross-sectional feature splits. We find that access to OoD data does not\ntranslate to increased performance, pointing to inherent limitations in\ndefining potential OoD environments in the eICU Database potentially due to\ndata harmonisation and sampling. Echoing similar results with other popular\nclinical benchmarks in the literature, new approaches are required to evaluate\nrobust models in critical care.",
    "descriptor": "",
    "authors": [
      "Dimitris Spathis",
      "Stephanie L. Hyland"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13398"
  },
  {
    "id": "arXiv:2205.13399",
    "title": "Multi-objective QUBO Solver: Bi-objective Quadratic Assignment",
    "abstract": "Quantum and quantum-inspired optimisation algorithms are designed to solve\nproblems represented in binary, quadratic and unconstrained form. Combinatorial\noptimisation problems are therefore often formulated as Quadratic Unconstrained\nBinary Optimisation Problems (QUBO) to solve them with these algorithms.\nMoreover, these QUBO solvers are often implemented using specialised hardware\nto achieve enormous speedups, e.g. Fujitsu's Digital Annealer (DA) and D-Wave's\nQuantum Annealer. However, these are single-objective solvers, while many\nreal-world problems feature multiple conflicting objectives. Thus, a common\npractice when using these QUBO solvers is to scalarise such multi-objective\nproblems into a sequence of single-objective problems. Due to design trade-offs\nof these solvers, formulating each scalarisation may require more time than\nfinding a local optimum. We present the first attempt to extend the algorithm\nsupporting a commercial QUBO solver as a multi-objective solver that is not\nbased on scalarisation. The proposed multi-objective DA algorithm is validated\non the bi-objective Quadratic Assignment Problem. We observe that algorithm\nperformance significantly depends on the archiving strategy adopted, and that\ncombining DA with non-scalarisation methods to optimise multiple objectives\noutperforms the current scalarised version of the DA in terms of final solution\nquality.",
    "descriptor": "\nComments: The Genetic and Evolutionary Computation Conference 2022 (GECCO22)\n",
    "authors": [
      "Mayowa Ayodele",
      "Richard Allmendinger",
      "Manuel L\u00f3pez-Ib\u00e1\u00f1ez",
      "Matthieu Parizy"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13399"
  },
  {
    "id": "arXiv:2205.13401",
    "title": "Your Transformer May Not be as Powerful as You Expect",
    "abstract": "Relative Positional Encoding (RPE), which encodes the relative distance\nbetween any pair of tokens, is one of the most successful modifications to the\noriginal Transformer. As far as we know, theoretical understanding of the\nRPE-based Transformers is largely unexplored. In this work, we mathematically\nanalyze the power of RPE-based Transformers regarding whether the model is\ncapable of approximating any continuous sequence-to-sequence functions. One may\nnaturally assume the answer is in the affirmative -- RPE-based Transformers are\nuniversal function approximators. However, we present a negative result by\nshowing there exist continuous sequence-to-sequence functions that RPE-based\nTransformers cannot approximate no matter how deep and wide the neural network\nis. One key reason lies in that most RPEs are placed in the softmax attention\nthat always generates a right stochastic matrix. This restricts the network\nfrom capturing positional information in the RPEs and limits its capacity. To\novercome the problem and make the model more powerful, we first present\nsufficient conditions for RPE-based Transformers to achieve universal function\napproximation. With the theoretical guidance, we develop a novel attention\nmodule, called Universal RPE-based (URPE) Attention, which satisfies the\nconditions. Therefore, the corresponding URPE-based Transformers become\nuniversal function approximators. Extensive experiments covering typical\narchitectures and tasks demonstrate that our model is parameter-efficient and\ncan achieve superior performance to strong baselines in a wide range of\napplications.",
    "descriptor": "\nComments: Preprint. Work in Progress\n",
    "authors": [
      "Shengjie Luo",
      "Shanda Li",
      "Shuxin Zheng",
      "Tie-Yan Liu",
      "Liwei Wang",
      "Di He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13401"
  },
  {
    "id": "arXiv:2205.13407",
    "title": "Tight Memory-Independent Parallel Matrix Multiplication Communication  Lower Bounds",
    "abstract": "Communication lower bounds have long been established for matrix\nmultiplication algorithms. However, most methods of asymptotic analysis have\neither ignored the constant factors or not obtained the tightest possible\nvalues. Recent work has demonstrated that more careful analysis improves the\nbest known constants for some classical matrix multiplication lower bounds and\nhelps to identify more efficient algorithms that match the leading-order terms\nin the lower bounds exactly and improve practical performance. The main result\nof this work is the establishment of memory-independent communication lower\nbounds with tight constants for parallel matrix multiplication. Our constants\nimprove on previous work in each of three cases that depend on the relative\nsizes of the aspect ratios of the matrices.",
    "descriptor": "",
    "authors": [
      "Hussam Al Daas",
      "Grey Ballard",
      "Laura Grigori",
      "Suraj Kumar",
      "Kathryn Rouse"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13407"
  },
  {
    "id": "arXiv:2205.13411",
    "title": "Exponential Random Graph Models for Dynamic Signed Networks: An  Application to International Relations",
    "abstract": "Substantive research in the Social Sciences regularly investigates signed\nnetworks, where edges between actors are either positive or negative. For\ninstance, schoolchildren can be friends or rivals, just as countries can\ncooperate or fight each other. This research often builds on structural balance\ntheory, one of the earliest and most prominent network theories, making signed\nnetworks one of the most frequently studied matters in social network analysis.\nWhile the theorization and description of signed networks have thus made\nsignificant progress, the inferential study of tie formation within them\nremains limited in the absence of appropriate statistical models. In this paper\nwe fill this gap by proposing the Signed Exponential Random Graph Model\n(SERGM), extending the well-known Exponential Random Graph Model (ERGM) to\nnetworks where ties are not binary but negative or positive if a tie exists.\nSince most networks are dynamically evolving systems, we specify the model for\nboth cross-sectional and dynamic networks. Based on structural hypotheses\nderived from structural balance theory, we formulate interpretable signed\nnetwork statistics, capturing dynamics such as \"the enemy of my enemy is my\nfriend\". In our empirical application, we use the SERGM to analyze cooperation\nand conflict between countries within the international state system.",
    "descriptor": "",
    "authors": [
      "Cornelius Fritz",
      "Marius Mehrl",
      "Paul W. Thurner",
      "G\u00f6ran kauermann"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.13411"
  },
  {
    "id": "arXiv:2205.13412",
    "title": "A Physical-World Adversarial Attack Against 3D Face Recognition",
    "abstract": "3D face recognition systems have been widely employed in intelligent\nterminals, among which structured light imaging is a common method to measure\nthe 3D shape. However, this method could be easily attacked, leading to\ninaccurate 3D face recognition. In this paper, we propose a novel,\nphysically-achievable attack on the fringe structured light system, named\nstructured light attack. The attack utilizes a projector to project optical\nadversarial fringes on faces to generate point clouds with well-designed\nnoises. We firstly propose a 3D transform-invariant loss function to enhance\nthe robustness of 3D adversarial examples in the physical-world attack. Then we\nreverse the 3D adversarial examples to the projector's input to place noises on\nphase-shift images, which models the process of structured light imaging. A\nreal-world structured light system is constructed for the attack and several\nstate-of-the-art 3D face recognition neural networks are tested. Experiments\nshow that our method can attack the physical system successfully and only needs\nminor modifications of projected images.",
    "descriptor": "\nComments: 10 pages, 5 figures, Submit to NeurIPS 2022\n",
    "authors": [
      "Yanjie Li",
      "Yiquan Li",
      "Bin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13412"
  },
  {
    "id": "arXiv:2205.13415",
    "title": "A Fair Federated Learning Framework With Reinforcement Learning",
    "abstract": "Federated learning (FL) is a paradigm where many clients collaboratively\ntrain a model under the coordination of a central server, while keeping the\ntraining data locally stored. However, heterogeneous data distributions over\ndifferent clients remain a challenge to mainstream FL algorithms, which may\ncause slow convergence, overall performance degradation and unfairness of\nperformance across clients. To address these problems, in this study we propose\na reinforcement learning framework, called PG-FFL, which automatically learns a\npolicy to assign aggregation weights to clients. Additionally, we propose to\nutilize Gini coefficient as the measure of fairness for FL. More importantly,\nwe apply the Gini coefficient and validation accuracy of clients in each\ncommunication round to construct a reward function for the reinforcement\nlearning. Our PG-FFL is also compatible to many existing FL algorithms. We\nconduct extensive experiments over diverse datasets to verify the effectiveness\nof our framework. The experimental results show that our framework can\noutperform baseline methods in terms of overall performance, fairness and\nconvergence speed.",
    "descriptor": "",
    "authors": [
      "Yaqi Sun",
      "Shijing Si",
      "Jianzong Wang",
      "Yuhan Dong",
      "Zhitao Zhu",
      "Jing Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13415"
  },
  {
    "id": "arXiv:2205.13419",
    "title": "The way we cite: common metadata used across disciplines for defining  bibliographic references",
    "abstract": "Current citation practices observed in articles are very noisy, confusing,\nand not standardised at all, making the identification of the cited works\nproblematic for humans and any reference extraction software. In this work, we\nwant to investigate such citation practices for referencing different types of\nentities and, in particular, for understanding what the most used metadata in\nbibliographic references are. We identified 36 different types of cited\nentities (the most cited ones were articles, books, and proceeding papers)\nwithin the 34,140 bibliographic references extracted from a huge set of journal\narticles on 27 different subject areas. The analysis of such bibliographic\nreferences, grouped by the particular type of cited entities, enabled us to\nhighlight the most used metadata for defining bibliographic references across\nthe subject areas. However, we also noticed that, in some cases, bibliographic\nreferences did not provide the essential elements to easily identify the work\nthey refer to.",
    "descriptor": "",
    "authors": [
      "Erika Alves dos Santos",
      "Silvio Peroni",
      "Marcos Luiz Mucheroni"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2205.13419"
  },
  {
    "id": "arXiv:2205.13421",
    "title": "Machine Learning Models Are Not Necessarily Biased When Constructed  Properly: Evidence from Neuroimaging Studies",
    "abstract": "Despite the great promise that machine learning has offered in many fields of\nmedicine, it has also raised concerns about potential biases and poor\ngeneralization across genders, age distributions, races and ethnicities,\nhospitals, and data acquisition equipment and protocols. In the current study,\nand in the context of three brain diseases, we provide experimental data which\nsupport that when properly trained, machine learning models can generalize well\nacross diverse conditions and do not suffer from biases. Specifically, by using\nmulti-study magnetic resonance imaging consortia for diagnosing Alzheimer's\ndisease, schizophrenia, and autism spectrum disorder, we find that, the\naccuracy of well-trained models is consistent across different subgroups\npertaining to attributes such as gender, age, and racial groups, as also\ndifferent clinical studies. We find that models that incorporate multi-source\ndata from demographic, clinical, genetic factors and cognitive scores are also\nunbiased. These models have better predictive accuracy across subgroups than\nthose trained only with structural measures in some cases but there are also\nsituations when these additional features do not help.",
    "descriptor": "",
    "authors": [
      "Rongguang Wang",
      "Pratik Chaudhari",
      "Christos Davatzikos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13421"
  },
  {
    "id": "arXiv:2205.13422",
    "title": "Opinion Spam Detection: A New Approach Using Machine Learning and  Network-Based Algorithms",
    "abstract": "E-commerce is the fastest-growing segment of the economy. Online reviews play\na crucial role in helping consumers evaluate and compare products and services.\nAs a result, fake reviews (opinion spam) are becoming more prevalent and\nnegatively impacting customers and service providers. There are many reasons\nwhy it is hard to identify opinion spammers automatically, including the\nabsence of reliable labeled data. This limitation precludes an off-the-shelf\napplication of a machine learning pipeline. We propose a new method for\nclassifying reviewers as spammers or benign, combining machine learning with a\nmessage-passing algorithm that capitalizes on the users' graph structure to\ncompensate for the possible scarcity of labeled data. We devise a new way of\nsampling the labels for the training step (active learning), replacing the\ntypical uniform sampling. Experiments on three large real-world datasets from\nYelp.com show that our method outperforms state-of-the-art active learning\napproaches and also machine learning methods that use a much larger set of\nlabeled data for training.",
    "descriptor": "",
    "authors": [
      "Kiril Danilchenko",
      "Michael Segal",
      "Dan Vilenchik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13422"
  },
  {
    "id": "arXiv:2205.13425",
    "title": "Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation",
    "abstract": "Action classification has made great progress, but segmenting and recognizing\nactions from long untrimmed videos remains a challenging problem. Most\nstate-of-the-art methods focus on designing temporal convolution-based models,\nbut the limitations on modeling long-term temporal dependencies and\ninflexibility of temporal convolutions limit the potential of these models.\nRecently, Transformer-based models with flexible and strong sequence modeling\nability have been applied in various tasks. However, the lack of inductive bias\nand the inefficiency of handling long video sequences limit the application of\nTransformer in action segmentation. In this paper, we design a pure\nTransformer-based model without temporal convolutions by incorporating the\nU-Net architecture. The U-Transformer architecture reduces complexity while\nintroducing an inductive bias that adjacent frames are more likely to belong to\nthe same class, but the introduction of coarse resolutions results in the\nmisclassification of boundaries. We observe that the similarity distribution\nbetween a boundary frame and its neighboring frames depends on whether the\nboundary frame is the start or end of an action segment. Therefore, we further\npropose a boundary-aware loss based on the distribution of similarity scores\nbetween frames from attention modules to enhance the ability to recognize\nboundaries. Extensive experiments show the effectiveness of our model.",
    "descriptor": "",
    "authors": [
      "Dazhao Du",
      "Bing Su",
      "Yu Li",
      "Zhongang Qi",
      "Lingyu Si",
      "Ying Shan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13425"
  },
  {
    "id": "arXiv:2205.13426",
    "title": "AntiBenford Subgraphs: Unsupervised Anomaly Detection in Financial  Networks",
    "abstract": "Benford's law describes the distribution of the first digit of numbers\nappearing in a wide variety of numerical data, including tax records, and\nelection outcomes, and has been used to raise \"red flags\" about potential\nanomalies in the data such as tax evasion. In this work, we ask the following\nnovel question: given a large transaction or financial graph, how do we find a\nset of nodes that perform many transactions among each other that also deviate\nsignificantly from Benford's law?\nWe propose the AntiBenford subgraph framework that is founded on\nwell-established statistical principles. Furthermore, we design an efficient\nalgorithm that finds AntiBenford subgraphs in near-linear time on real data. We\nevaluate our framework on both real and synthetic data against a variety of\ncompetitors. We show empirically that our proposed framework enables the\ndetection of anomalous subgraphs in cryptocurrency transaction networks that go\nundetected by state-of-the-art graph-based anomaly detection methods. Our\nempirical findings show that our \\ab framework is able to mine anomalous\nsubgraphs, and provide novel insights into financial transaction data.\nThe code and the datasets are available at\n\\url{https://github.com/tsourakakis-lab/antibenford-subgraphs}.",
    "descriptor": "\nComments: Accepted at KDD'22\n",
    "authors": [
      "Tianyi Chen",
      "Charalampos E. Tsourakakis"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.13426"
  },
  {
    "id": "arXiv:2205.13428",
    "title": "QBF Merge Resolution is powerful but unnatural",
    "abstract": "The Merge Resolution proof system (M-Res) for QBFs, proposed by Beyersdorff\net al. in 2019, explicitly builds partial strategies inside refutations. The\noriginal motivation for this approach was to overcome the limitations\nencountered in long-distance Q-Resolution proof system (LD-Q-Res), where the\nsyntactic side-conditions, while prohibiting all unsound resolutions, also end\nup prohibiting some sound resolutions. However, while the advantage of M-Res\nover many other resolution-based QBF proof systems was already demonstrated, a\ncomparison with LD-Q-Res itself had remained open. In this paper, we settle\nthis question. We show that M-Res has an exponential advantage over not only\nLD-Q-Res, but even over LQU$^+$-Res and IRM, the most powerful among currently\nknown resolution-based QBF proof systems. Combining this with results from\nBeyersdorff et al. 2020, we conclude that M-Res is incomparable with LQU-Res\nand LQU$^+$-Res.\nOur proof method reveals two additional and curious features about M-Res: (i)\nMRes is not closed under restrictions, and is hence not a natural proof system,\nand (ii) weakening axiom clauses with existential variables provably yields an\nexponential advantage over M-Res without weakening. We further show that in the\ncontext of regular derivations, weakening axiom clauses with universal\nvariables provably yields an exponential advantage over M-Res without\nweakening. These results suggest that MRes is better used with weakening,\nthough whether M-Res with weakening is closed under restrictions remains open.\nWe note that even with weakening, M-Res continues to be simulated by eFrege $+$\n$\\forall$red (the simulation of ordinary M-Res was shown recently by Chew and\nSlivovsky).",
    "descriptor": "",
    "authors": [
      "Meena Mahajan",
      "Gaurav Sood"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13428"
  },
  {
    "id": "arXiv:2205.13430",
    "title": "GNOLL: Efficient Software for Real-World Dice Notation and Extensions",
    "abstract": "GNOLL (\"GNOLL's Not *OLL\") is a software library for dice notation. Unlike\nprevious papers, GNOLL's dice notation syntax is focused on parsing a language\nthat tabletop role-players and board gamers are already used to for specifying\ndice rolls in many popular software applications. Existing implementations of\nsuch a syntax are either incomplete, fragile, or proprietary, meaning that\nanyone hoping to use such syntax in their application likely needs to write\ntheir own solution. GNOLL is an open-source project using the compilation tool\n'YACC' and lexical tool 'LEX' which can be integrated into many applications\nwith relative ease. This paper explores GNOLL's extended dice notation syntax\nand its competitive performance.",
    "descriptor": "\nComments: 11 pages, 12 figures, Under Review for MPLR '22\n",
    "authors": [
      "Ian Frederick Vigogne Goodbody Hunter"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.13430"
  },
  {
    "id": "arXiv:2205.13431",
    "title": "Sub-Rate Linear Network Coding",
    "abstract": "Increasing network utilization is often considered as the holy grail of\ncommunications. In this article, the concept of sub-rate coding and decoding in\nthe framework of linear network coding (LNC) is discussed for single-source\nmultiple-sinks finite acyclic networks. Sub-rate coding offers an add-on to\nexisting LNC. It allows sinks whose max-flow is smaller than the source\nmessage-rate, termed \\emph{sub-rate sinks}, to decode a portion of the\ntransmitted message without degrading the maximum achievable rate of LNC sinks\nwhose max-flow is equal (or greater) than the rate of the source node. The\narticle studies theoretical aspects of sub-rate coding by formulating the\nconditions a node (and indeed the network) must fulfill so as to qualify as a\nlegitimate sub-rate sink.",
    "descriptor": "",
    "authors": [
      "Ben Grinboim",
      "Itay Shrem",
      "Ofer Amrani"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13431"
  },
  {
    "id": "arXiv:2205.13434",
    "title": "Jointly Learning Span Extraction and Sequence Labeling for Information  Extraction from Business Documents",
    "abstract": "This paper introduces a new information extraction model for business\ndocuments. Different from prior studies which only base on span extraction or\nsequence labeling, the model takes into account advantage of both span\nextraction and sequence labeling. The combination allows the model to deal with\nlong documents with sparse information (the small amount of extracted\ninformation). The model is trained end-to-end to jointly optimize the two tasks\nin a unified manner. Experimental results on four business datasets in English\nand Japanese show that the model achieves promising results and is\nsignificantly faster than the normal span-based extraction method. The code is\nalso available.",
    "descriptor": "\nComments: Accepted to IJCNN 2022\n",
    "authors": [
      "Nguyen Hong Son",
      "Hieu M. Vu",
      "Tuan-Anh D. Nguyen",
      "Minh-Tien Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13434"
  },
  {
    "id": "arXiv:2205.13439",
    "title": "Automatic parameter selection for the TGV regularizer in image  restoration under Poisson noise",
    "abstract": "We address the image restoration problem under Poisson noise corruption. The\nKullback-Leibler divergence, which is typically adopted in the variational\nframework as data fidelity term in this case, is coupled with the second-order\nTotal Generalized Variation (TGV$^2$). The TGV$^2$ regularizer is known to be\ncapable of preserving both smooth and piece-wise constant features in the\nimage, however its behavior is subject to a suitable setting of the parameters\narising in its expression. We propose a hierarchical Bayesian formulation of\nthe original problem coupled with a Maximum A Posteriori estimation approach,\naccording to which the unknown image and parameters can be jointly and\nautomatically estimated by minimizing a given cost functional. The minimization\nproblem is tackled via a scheme based on the Alternating Direction Method of\nMultipliers, which also incorporates a procedure for the automatic selection of\nthe regularization parameter by means of a popular discrepancy principle.\nComputational results show the effectiveness of our proposal.",
    "descriptor": "",
    "authors": [
      "Daniela di Serafino",
      "Monica Pragliola"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13439"
  },
  {
    "id": "arXiv:2205.13440",
    "title": "The Neuro-Symbolic Brain",
    "abstract": "Neural networks promote a distributed representation with no clear place for\nsymbols. Despite this, we propose that symbols are manufactured simply by\ntraining a sparse random noise as a self-sustaining attractor in a feedback\nspiking neural network. This way, we can generate many of what we shall call\nprime attractors, and the networks that support them are like registers holding\na symbolic value, and we call them registers. Like symbols, prime attractors\nare atomic and devoid of any internal structure. Moreover, the winner-take-all\nmechanism naturally implemented by spiking neurons enables registers to recover\na prime attractor within a noisy signal. Using this faculty, when considering\ntwo connected registers, an input one and an output one, it is possible to bind\nin one shot using a Hebbian rule the attractor active on the output to the\nattractor active on the input. Thus, whenever an attractor is active on the\ninput, it induces its bound attractor on the output; even though the signal\ngets blurrier with more bindings, the winner-take-all filtering faculty can\nrecover the bound prime attractor. However, the capacity is still limited. It\nis also possible to unbind in one shot, restoring the capacity taken by that\nbinding. This mechanism serves as a basis for working memory, turning prime\nattractors into variables. Also, we use a random second-order network to\namalgamate the prime attractors held by two registers to bind the prime\nattractor held by a third register to them in one shot, de facto implementing a\nhash table. Furthermore, we introduce the register switch box composed of\nregisters to move the content of one register to another. Then, we use spiking\nneurons to build a toy symbolic computer based on the above. The technics used\nsuggest ways to design extrapolating, reusable, sample-efficient deep learning\nnetworks at the cost of structural priors.",
    "descriptor": "\nComments: 32 pages, 11 figures\n",
    "authors": [
      "Robert Liz\u00e9e"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13440"
  },
  {
    "id": "arXiv:2205.13441",
    "title": "Deep Reinforcement Learning with Adaptive Hierarchical Reward for  MultiMulti-Phase Multi Multi-Objective Dexterous Manipulation",
    "abstract": "Dexterous manipulation tasks usually have multiple objectives, and the\npriorities of these objectives may vary at different phases of a manipulation\ntask. Varying priority makes a robot hardly or even failed to learn an optimal\npolicy with a deep reinforcement learning (DRL) method. To solve this problem,\nwe develop a novel Adaptive Hierarchical Reward Mechanism (AHRM) to guide the\nDRL agent to learn manipulation tasks with multiple prioritized objectives. The\nAHRM can determine the objective priorities during the learning process and\nupdate the reward hierarchy to adapt to the changing objective priorities at\ndifferent phases. The proposed method is validated in a multi-objective\nmanipulation task with a JACO robot arm in which the robot needs to manipulate\na target with obstacles surrounded. The simulation and physical experiment\nresults show that the proposed method improved robot learning in task\nperformance and learning efficiency.",
    "descriptor": "\nComments: Revision submitted to Journal of Intelligent & Robotic Systems\n",
    "authors": [
      "Lingfeng Tao",
      "Jiucai Zhang",
      "Xiaoli Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13441"
  },
  {
    "id": "arXiv:2205.13444",
    "title": "Principled Knowledge Extrapolation with GANs",
    "abstract": "Human can extrapolate well, generalize daily knowledge into unseen scenarios,\nraise and answer counterfactual questions. To imitate this ability via\ngenerative models, previous works have extensively studied explicitly encoding\nStructural Causal Models (SCMs) into architectures of generator networks. This\nmethodology, however, limits the flexibility of the generator as they must be\ncarefully crafted to follow the causal graph, and demands a ground truth SCM\nwith strong ignorability assumption as prior, which is a nontrivial assumption\nin many real scenarios. Thus, many current causal GAN methods fail to generate\nhigh fidelity counterfactual results as they cannot easily leverage\nstate-of-the-art generative models. In this paper, we propose to study\ncounterfactual synthesis from a new perspective of knowledge extrapolation,\nwhere a given knowledge dimension of the data distribution is extrapolated, but\nthe remaining knowledge is kept indistinguishable from the original\ndistribution. We show that an adversarial game with a closed-form discriminator\ncan be used to address the knowledge extrapolation problem, and a novel\nprincipal knowledge descent method can efficiently estimate the extrapolated\ndistribution through the adversarial game. Our method enjoys both elegant\ntheoretical guarantees and superior performance in many scenarios.",
    "descriptor": "",
    "authors": [
      "Ruili Feng",
      "Jie Xiao",
      "Kecheng Zheng",
      "Deli Zhao",
      "Jingren Zhou",
      "Qibin Sun",
      "Zheng-Jun Zha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13444"
  },
  {
    "id": "arXiv:2205.13445",
    "title": "Mutual Information Divergence: A Unified Metric for Multimodal  Generative Models",
    "abstract": "Text-to-image generation and image captioning are recently emerged as a new\nexperimental paradigm to assess machine intelligence. They predict continuous\nquantity accompanied by their sampling techniques in the generation, making\nevaluation complicated and intractable to get marginal distributions. Based on\na recent trend that multimodal generative evaluations exploit a\nvison-and-language pre-trained model, we propose the negative Gaussian\ncross-mutual information using the CLIP features as a unified metric, coined by\nMutual Information Divergence (MID). To validate, we extensively compare it\nwith competing metrics using carefully-generated or human-annotated judgments\nin text-to-image generation and image captioning tasks. The proposed MID\nsignificantly outperforms the competitive methods by having consistency across\nbenchmarks, sample parsimony, and robustness toward the exploited CLIP model.\nWe look forward to seeing the underrepresented implications of the Gaussian\ncross-mutual information in multimodal representation learning and the future\nworks based on this novel proposition.",
    "descriptor": "",
    "authors": [
      "Jin-Hwa Kim",
      "Yunji Kim",
      "Jiyoung Lee",
      "Kang Min Yoo",
      "Sang-Woo Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13445"
  },
  {
    "id": "arXiv:2205.13446",
    "title": "Optimal Repair/Access MDS Array Codes with Multiple Repair Degrees",
    "abstract": "In the literature, most of the known high-rate $(n,k)$ MDS array codes with\nthe optimal repair property only support a single repair degree (i.e., the\nnumber of helper nodes contacted during a repair process) $d$, where $k\\le d\\le\nn-1$. However, in practical storage systems, the number of available nodes\nchanges frequently. Thus, it is preferred to construct $(n,k)$ MDS array codes\nwith multiple repair degrees and the optimal repair property for all nodes. To\nthe best of our knowledge, only two MDS array codes have such properties in the\nliterature, which were proposed by Ye and Barg (IEEE Trans. Inform. Theory,\n63(10), 2001-2014, 2017). However, their sub-packetization levels are\nrelatively large. In this paper, we present a generic construction method that\ncan convert some MDS array codes with a single repair degree into the ones with\nmultiple repair degrees and optimal repair property for a set of nodes, while\nthe repair efficiency/degrees of the remaining nodes can be kept. As an\napplication of the generic construction method, an explicit construction of\nhigh-rate MDS array code with multiple repair degrees and the optimal access\nproperty for all nodes is obtained over a small finite field. Especially, the\nsub-packetization level is much smaller than that of the two codes proposed by\nYe and Barg concerning the same parameters $n$ and $k$.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Y. Liu",
      "J. Li",
      "X.H. Tang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13446"
  },
  {
    "id": "arXiv:2205.13447",
    "title": "Probabilistic failure mechanisms via Monte Carlo simulations of complex  microstructures",
    "abstract": "A probabilistic approach to phase-field brittle and ductile fracture with\nrandom material and geometric properties is proposed within this work. In the\nmacroscopic failure mechanics, materials properties and exactness of spatial\nquantities (of different phases in the geometrical domain) are assumed to be\nhomogeneous and deterministic. This is unlike the lower-scale with strong\nfluctuation in the material and geometrical properties. Such a response is\napproximated through some uncertainty in the model problem. The presented\ncontribution is devoted to providing a mathematical framework for modeling\nuncertainty through stochastic analysis of a microstructure undergoing\nbrittle/ductile failure. Hereby, the proposed model employs various\nrepresentative volume elements with random distribution of stiff-inclusions and\nvoids within the composite structure. We develop an allocating strategy to\nallocate the heterogeneities and generate the corresponding meshes in two- and\nthree-dimensional cases. Then the Monte Carlo finite element technique is\nemployed for solving the stochastic PDE-based model and approximate the\nexpectation and the variance of the solution field of brittle/ductile failure\nby evaluating a large number of samples. For the prediction of failure\nmechanisms, we rely on the phase-field approach which is a widely adopted\nframework for modeling and computing the fracture phenomena in solids.\nIncremental perturbed minimization principles for a class of gradient-type\ndissipative materials are used to derive the perturbed governing equations.\nThis analysis enables us to study the highly heterogeneous microstructure and\nmonitor the uncertainty in failure mechanics. Several numerical examples are\ngiven to examine the efficiency of the proposed method.",
    "descriptor": "",
    "authors": [
      "Nima Noii",
      "Amirreza Khodadadian",
      "Fadi Aldakheel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13447"
  },
  {
    "id": "arXiv:2205.13448",
    "title": "Validated Objects: Specification, Implementation, and Applications",
    "abstract": "Guaranteeing the validity of concurrent operations on distributed objects is\na key property for ensuring reliability and consistency in distributed systems.\nUsually, the methods for validating these operations, if present, are wired in\nthe object implementation. In this work, we formalize the notion of a {\\em\nvalidated object}, decoupling the object operations and properties from the\nvalidation procedure. We consider two types of objects, satisfying different\nlevels of consistency: the validated {\\em totally-ordered} object, offering a\ntotal ordering of its operations, and its weaker variant, the validated {\\em\nregular} object. We provide conditions under which it is possible to implement\nthese objects. In particular, we show that crash-tolerant implementations of\nvalidated regular objects are always possible in an asynchronous system with a\nmajority of correct processes. However, for validated totally-ordered objects,\nconsensus is always required if a property of the object we introduce in this\nwork, {\\em persistent validity,} does not hold. Persistent validity combined\nwith another new property, {\\em persistent execution}, allows consensus-free\ncrash-tolerant implementations of validated totally-ordered objects. We\ndemonstrate the utility of validated objects by considering several\napplications conforming to our formalism.",
    "descriptor": "",
    "authors": [
      "Antonio Fern\u00e1ndez Anta",
      "Chryssis Georgiou",
      "Nicolas Nicolaou",
      "Antonio Russo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13448"
  },
  {
    "id": "arXiv:2205.13450",
    "title": "Variance-Aware Sparse Linear Bandits",
    "abstract": "It is well-known that the worst-case minimax regret for sparse linear bandits\nis $\\widetilde{\\Theta}\\left(\\sqrt{dT}\\right)$ where $d$ is the ambient\ndimension and $T$ is the number of time steps (ignoring the dependency on\nsparsity). On the other hand, in the benign setting where there is no noise and\nthe action set is the unit sphere, one can use divide-and-conquer to achieve an\n$\\widetilde{\\mathcal O}(1)$ regret, which is (nearly) independent of $d$ and\n$T$. In this paper, we present the first variance-aware regret guarantee for\nsparse linear bandits: $\\widetilde{\\mathcal O}\\left(\\sqrt{d\\sum_{t=1}^T\n\\sigma_t^2} + 1\\right)$, where $\\sigma_t^2$ is the variance of the noise at the\n$t$-th time step. This bound naturally interpolates the regret bounds for the\nworst-case constant-variance regime ($\\sigma_t = \\Omega(1)$) and the benign\ndeterministic regimes ($\\sigma_t = 0$). To achieve this variance-aware regret\nguarantee, we develop a general framework that converts any variance-aware\nlinear bandit algorithm to a variance-aware algorithm for sparse linear bandits\nin a ``black-box'' manner. Specifically, we take two recent algorithms as black\nboxes to illustrate that the claimed bounds indeed hold, where the first\nalgorithm can handle unknown-variance cases and the second one is more\nefficient.",
    "descriptor": "",
    "authors": [
      "Yan Dai",
      "Ruosong Wang",
      "Simon S. Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13450"
  },
  {
    "id": "arXiv:2205.13451",
    "title": "Follow-the-Perturbed-Leader for Adversarial Markov Decision Processes  with Bandit Feedback",
    "abstract": "We consider regret minimization for Adversarial Markov Decision Processes\n(AMDPs), where the loss functions are changing over time and adversarially\nchosen, and the learner only observes the losses for the visited state-action\npairs (i.e., bandit feedback). While there has been a surge of studies on this\nproblem using Online-Mirror-Descent (OMD) methods, very little is known about\nthe Follow-the-Perturbed-Leader (FTPL) methods, which are usually\ncomputationally more efficient and also easier to implement since it only\nrequires solving an offline planning problem. Motivated by this, we take a\ncloser look at FTPL for learning AMDPs, starting from the standard episodic\nfinite-horizon setting. We find some unique and intriguing difficulties in the\nanalysis and propose a workaround to eventually show that FTPL is also able to\nachieve near-optimal regret bounds in this case. More importantly, we then find\ntwo significant applications: First, the analysis of FTPL turns out to be\nreadily generalizable to delayed bandit feedback with order-optimal regret,\nwhile OMD methods exhibit extra difficulties (Jin et al., 2022). Second, using\nFTPL, we also develop the first no-regret algorithm for learning communicating\nAMDPs in the infinite-horizon setting with bandit feedback and stochastic\ntransitions. Our algorithm is efficient assuming access to an offline planning\noracle, while even for the easier full-information setting, the only existing\nalgorithm (Chandrasekaran and Tewari, 2021) is computationally inefficient.",
    "descriptor": "",
    "authors": [
      "Yan Dai",
      "Haipeng Luo",
      "Liyu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13451"
  },
  {
    "id": "arXiv:2205.13452",
    "title": "Continual evaluation for lifelong learning: Identifying the stability  gap",
    "abstract": "Introducing a time dependency on the data generating distribution has proven\nto be difficult for gradient-based training of neural networks, as the greedy\nupdates result in catastrophic forgetting of previous timesteps. Continual\nlearning aims to overcome the greedy optimization to enable continuous\naccumulation of knowledge over time. The data stream is typically divided into\nlocally stationary distributions, called tasks, allowing task-based evaluation\non held-out data from the training tasks. Contemporary evaluation protocols and\nmetrics in continual learning are task-based and quantify the trade-off between\nstability and plasticity only at task transitions. However, our empirical\nevidence suggests that between task transitions significant, temporary\nforgetting can occur, remaining unidentified in task-based evaluation.\nTherefore, we propose a framework for continual evaluation that establishes\nper-iteration evaluation and define a new set of metrics that enables\nidentifying the worst-case performance of the learner over its lifetime.\nPerforming continual evaluation, we empirically identify that replay suffers\nfrom a stability gap: upon learning a new task, there is a substantial but\ntransient decrease in performance on past tasks. Further conceptual and\nempirical analysis suggests not only replay-based, but also\nregularization-based continual learning methods are prone to the stability gap.",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Matthias De Lange",
      "Gido van de Ven",
      "Tinne Tuytelaars"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13452"
  },
  {
    "id": "arXiv:2205.13457",
    "title": "AutoTSG: Learning and Synthesis for Incident Troubleshooting",
    "abstract": "Incident management is a key aspect of operating large-scale cloud services.\nTo aid with faster and efficient resolution of incidents, engineering teams\ndocument frequent troubleshooting steps in the form of Troubleshooting Guides\n(TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed,\nunstructured, and often incomplete, requiring developers to manually understand\nand execute necessary steps. This results in a plethora of issues such as\non-call fatigue, reduced productivity, and human errors. In this work, we\nconduct a large-scale empirical study of over 4K+ TSGs mapped to 1000s of\nincidents and find that TSGs are widely used and help significantly reduce\nmitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and\npropose a taxonomy of issues that highlights significant gaps in TSG quality.\nTo alleviate these gaps, we investigate the automation of TSGs and propose\nAutoTSG -- a novel framework for automation of TSGs to executable workflows by\ncombining machine learning and program synthesis. Our evaluation of AutoTSG on\n50 TSGs shows the effectiveness in both identifying TSG statements (accuracy\n0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly,\nwe survey ten Microsoft engineers and show the importance of TSG automation and\nthe usefulness of AutoTSG.",
    "descriptor": "",
    "authors": [
      "Manish Shetty",
      "Chetan Bansal",
      "Sai Pramod Upadhyayula",
      "Arjun Radhakrishna",
      "Anurag Gupta"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.13457"
  },
  {
    "id": "arXiv:2205.13459",
    "title": "SigMaNet: One Laplacian to Rule Them All",
    "abstract": "This paper introduces SigMaNet, a generalized Graph Convolutional Network\n(GCN) capable of handling both undirected and directed graphs with weights not\nrestricted in sign and magnitude. The cornerstone of SigMaNet is the\nintroduction of a generalized Laplacian matrix: the Sign-Magnetic Laplacian\n($L^\\sigma$). The adoption of such a matrix allows us to bridge a gap in the\ncurrent literature by extending the theory of spectral GCNs to directed graphs\nwith both positive and negative weights. $L^{\\sigma}$ exhibits several\ndesirable properties not enjoyed by the traditional Laplacian matrices on which\nseveral state-of-the-art architectures are based. In particular, $L^\\sigma$ is\ncompletely parameter-free, which is not the case of Laplacian operators such as\nthe Magnetic Laplacian $L^{(q)}$, where the calibration of the parameter q is\nan essential yet problematic component of the operator. $L^\\sigma$ simplifies\nthe approach, while also allowing for a natural interpretation of the signs of\nthe edges in terms of their directions. The versatility of the proposed\napproach is amply demonstrated experimentally; the proposed network SigMaNet\nturns out to be competitive in all the tasks we considered, regardless of the\ngraph structure.",
    "descriptor": "\nComments: 18 pages, 5 tables\n",
    "authors": [
      "Stefano Fiorini",
      "Stefano Coniglio",
      "Michele Ciavotta",
      "Enza Messina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13459"
  },
  {
    "id": "arXiv:2205.13462",
    "title": "FedAug: Reducing the Local Learning Bias Improves Federated Learning on  Heterogeneous Data",
    "abstract": "Federated Learning (FL) is a machine learning paradigm that learns from data\nkept locally to safeguard the privacy of clients, whereas local SGD is\ntypically employed on the clients' devices to improve communication efficiency.\nHowever, such a scheme is currently constrained by the slow and unstable\nconvergence induced by clients' heterogeneous data. In this work, we identify\nthree under-explored phenomena of the biased local learning that may explain\nthese challenges caused by local updates in supervised FL. As a remedy, we\npropose FedAug, a novel unified algorithm that reduces the local learning bias\non features and classifiers to tackle these challenges. FedAug consists of two\ncomponents: AugMean and AugCA. AugMean alleviates the bias in the local\nclassifiers by balancing the output distribution of models. AugCA learns client\ninvariant features that are close to global features but considerably distinct\nfrom those learned from other input distributions. In a series of experiments,\nwe show that FedAug consistently outperforms other SOTA FL and domain\ngeneralization (DG) baselines, in which both two components (i.e., AugMean and\nAugCA) have individual performance gains.",
    "descriptor": "",
    "authors": [
      "Yongxin Guo",
      "Tao Lin",
      "Xiaoying Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13462"
  },
  {
    "id": "arXiv:2205.13471",
    "title": "Characterising Research Areas in the field of AI",
    "abstract": "Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it\nis crucial to support researchers and organisations in understanding where AI\nresearch is heading. In this study, we conducted a bibliometric analysis on\n257K articles in AI, retrieved from OpenAlex. We identified the main conceptual\nthemes by performing clustering analysis on the co-occurrence network of\ntopics. Finally, we observed how such themes evolved over time. The results\nhighlight the growing academic interest in research themes like deep learning,\nmachine learning, and internet of things.",
    "descriptor": "\nComments: paper presented at SIS2022 - 51ST SCIENTIFIC MEETING OF THE ITALIAN STATISTICAL SOCIETY\n",
    "authors": [
      "Alessandra Belfiore",
      "Angelo Salatino",
      "Francesco Osborne"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.13471"
  },
  {
    "id": "arXiv:2205.13474",
    "title": "2D versus 3D Convolutional Spiking Neural Networks Trained with  Unsupervised STDP for Human Action Recognition",
    "abstract": "Current advances in technology have highlighted the importance of video\nanalysis in the domain of computer vision. However, video analysis has\nconsiderably high computational costs with traditional artificial neural\nnetworks (ANNs). Spiking neural networks (SNNs) are third generation\nbiologically plausible models that process the information in the form of\nspikes. Unsupervised learning with SNNs using the spike timing dependent\nplasticity (STDP) rule has the potential to overcome some bottlenecks of\nregular artificial neural networks, but STDP-based SNNs are still immature and\ntheir performance is far behind that of ANNs. In this work, we study the\nperformance of SNNs when challenged with the task of human action recognition,\nbecause this task has many real-time applications in computer vision, such as\nvideo surveillance. In this paper we introduce a multi-layered 3D convolutional\nSNN model trained with unsupervised STDP. We compare the performance of this\nmodel to those of a 2D STDP-based SNN when challenged with the KTH and Weizmann\ndatasets. We also compare single-layer and multi-layer versions of these models\nin order to get an accurate assessment of their performance. We show that\nSTDP-based convolutional SNNs can learn motion patterns using 3D kernels, thus\nenabling motion-based recognition from videos. Finally, we give evidence that\n3D convolution is superior to 2D convolution with STDP-based SNNs, especially\nwhen dealing with long video sequences.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2105.14740 by other authors\n",
    "authors": [
      "Mireille El-Assal",
      "Pierre Tirilly",
      "Ioan Marius Bilasco"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13474"
  },
  {
    "id": "arXiv:2205.13476",
    "title": "Embed to Control Partially Observed Systems: Representation Learning  with Provable Sample Efficiency",
    "abstract": "Reinforcement learning in partially observed Markov decision processes\n(POMDPs) faces two challenges. (i) It often takes the full history to predict\nthe future, which induces a sample complexity that scales exponentially with\nthe horizon. (ii) The observation and state spaces are often continuous, which\ninduces a sample complexity that scales exponentially with the extrinsic\ndimension. Addressing such challenges requires learning a minimal but\nsufficient representation of the observation and state histories by exploiting\nthe structure of the POMDP.\nTo this end, we propose a reinforcement learning algorithm named Embed to\nControl (ETC), which learns the representation at two levels while optimizing\nthe policy.~(i) For each step, ETC learns to represent the state with a\nlow-dimensional feature, which factorizes the transition kernel. (ii) Across\nmultiple steps, ETC learns to represent the full history with a low-dimensional\nembedding, which assembles the per-step feature. We integrate (i) and (ii) in a\nunified framework that allows a variety of estimators (including maximum\nlikelihood estimators and generative adversarial networks). For a class of\nPOMDPs with a low-rank structure in the transition kernel, ETC attains an\n$O(1/\\epsilon^2)$ sample complexity that scales polynomially with the horizon\nand the intrinsic dimension (that is, the rank). Here $\\epsilon$ is the\noptimality gap. To our best knowledge, ETC is the first sample-efficient\nalgorithm that bridges representation learning and policy optimization in\nPOMDPs with infinite observation and state spaces.",
    "descriptor": "",
    "authors": [
      "Lingxiao Wang",
      "Qi Cai",
      "Zhuoran Yang",
      "Zhaoran Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13476"
  },
  {
    "id": "arXiv:2205.13479",
    "title": "Learning to Reconstruct Missing Data from Spatiotemporal Graphs with  Sparse Observations",
    "abstract": "Modeling multivariate time series as temporal signals over a (possibly\ndynamic) graph is an effective representational framework that allows for\ndeveloping models for time series analysis. In fact, discrete sequences of\ngraphs can be processed by autoregressive graph neural networks to recursively\nlearn representations at each discrete point in time and space. Spatiotemporal\ngraphs are often highly sparse, with time series characterized by multiple,\nconcurrent, and even long sequences of missing data, e.g., due to the\nunreliable underlying sensor network. In this context, autoregressive models\ncan be brittle and exhibit unstable learning dynamics. The objective of this\npaper is, then, to tackle the problem of learning effective models to\nreconstruct, i.e., impute, missing data points by conditioning the\nreconstruction only on the available observations. In particular, we propose a\nnovel class of attention-based architectures that, given a set of highly sparse\ndiscrete observations, learn a representation for points in time and space by\nexploiting a spatiotemporal diffusion architecture aligned with the imputation\ntask. Representations are trained end-to-end to reconstruct observations w.r.t.\nthe corresponding sensor and its neighboring nodes. Compared to the state of\nthe art, our model handles sparse data without propagating prediction errors or\nrequiring a bidirectional model to encode forward and backward time\ndependencies. Empirical results on representative benchmarks show the\neffectiveness of the proposed method.",
    "descriptor": "",
    "authors": [
      "Ivan Marisca",
      "Andrea Cini",
      "Cesare Alippi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13479"
  },
  {
    "id": "arXiv:2205.13481",
    "title": "DeepJoint: Robust Survival Modelling Under Clinical Presence Shift",
    "abstract": "Observational data in medicine arise as a result of the complex interaction\nbetween patients and the healthcare system. The sampling process is often\nhighly irregular and itself constitutes an informative process. When using such\ndata to develop prediction models, this phenomenon is often ignored, leading to\nsub-optimal performance and generalisability of models when practices evolve.\nWe propose a multi-task recurrent neural network which models three clinical\npresence dimensions -- namely the longitudinal, the inter-observation and the\nmissingness processes -- in parallel to the survival outcome. On a prediction\ntask using MIMIC III laboratory tests, explicit modelling of these three\nprocesses showed improved performance in comparison to state-of-the-art\npredictive models (C-index at 1 day horizon: 0.878). More importantly, the\nproposed approach was more robust to change in the clinical presence setting,\ndemonstrated by performance comparison between patients admitted on weekdays\nand weekends. This analysis demonstrates the importance of studying and\nleveraging clinical presence to improve performance and create more\ntransportable clinical models.",
    "descriptor": "",
    "authors": [
      "Vincent Jeanselme",
      "Glen Martin",
      "Niels Peek",
      "Matthew Sperrin",
      "Brian Tom",
      "Jessica Barrett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13481"
  },
  {
    "id": "arXiv:2205.13482",
    "title": "CMA-ES with Margin: Lower-Bounding Marginal Probability for  Mixed-Integer Black-Box Optimization",
    "abstract": "This study targets the mixed-integer black-box optimization (MI-BBO) problem\nwhere continuous and integer variables should be optimized simultaneously. The\nCMA-ES, our focus in this study, is a population-based stochastic search method\nthat samples solution candidates from a multivariate Gaussian distribution\n(MGD), which shows excellent performance in continuous BBO. The parameters of\nMGD, mean and (co)variance, are updated based on the evaluation value of\ncandidate solutions in the CMA-ES. If the CMA-ES is applied to the MI-BBO with\nstraightforward discretization, however, the variance corresponding to the\ninteger variables becomes much smaller than the granularity of the\ndiscretization before reaching the optimal solution, which leads to the\nstagnation of the optimization. In particular, when binary variables are\nincluded in the problem, this stagnation more likely occurs because the\ngranularity of the discretization becomes wider, and the existing modification\nto the CMA-ES does not address this stagnation. To overcome these limitations,\nwe propose a simple modification of the CMA-ES based on lower-bounding the\nmarginal probabilities associated with the generation of integer variables in\nthe MGD. The numerical experiments on the MI-BBO benchmark problems demonstrate\nthe efficiency and robustness of the proposed method.",
    "descriptor": "\nComments: Accepted at Genetic and Evolutionary Computation Conference (GECCO) 2022. The code is available at this https URL\n",
    "authors": [
      "Ryoki Hamano",
      "Shota Saito",
      "Masahiro Nomura",
      "Shinichi Shirakawa"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13482"
  },
  {
    "id": "arXiv:2205.13485",
    "title": "Benchmarking of Deep Learning models on 2D Laminar Flow behind Cylinder",
    "abstract": "The rapidly advancing field of Fluid Mechanics has recently employed Deep\nLearning to solve various problems within that field. In that same spirit we\ntry to perform Direct Numerical Simulation(DNS) which is one of the tasks in\nComputational Fluid Dynamics, using three fundamental architectures in the\nfield of Deep Learning that were each used to solve various high dimensional\nproblems. We train these three models in an autoencoder manner, for this the\ndataset is treated like sequential frames given to the model as input. We\nobserve that recently introduced architecture called Transformer significantly\noutperforms its counterparts on the selected dataset.Furthermore, we conclude\nthat using Transformers for doing DNS in the field of CFD is an interesting\nresearch area worth exploring.",
    "descriptor": "\nComments: 5 figures, 8 pages\n",
    "authors": [
      "Mritunjay Musale",
      "Vaibhav Vasani"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.13485"
  },
  {
    "id": "arXiv:2205.13489",
    "title": "Measuring Perceptual Color Differences of Smartphone Photography",
    "abstract": "Measuring perceptual color differences (CDs) is of great importance in modern\nsmartphone photography. Despite the long history, most CD measures have been\nconstrained by psychophysical data of homogeneous color patches or a limited\nnumber of simplistic natural images. It is thus questionable whether existing\nCD measures generalize in the age of smartphone photography characterized by\ngreater content complexities and learning-based image signal processors. In\nthis paper, we put together so far the largest image dataset for perceptual CD\nassessment, in which the natural images are 1) captured by six flagship\nsmartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of\nthe smartphones, and 4) reproduced with incorrect color profiles. We then\nconduct a large-scale psychophysical experiment to gather perceptual CDs of\n30,000 image pairs in a carefully controlled laboratory environment. Based on\nthe newly established dataset, we make one of the first attempts to construct\nan end-to-end learnable CD formula based on a lightweight neural network, as a\ngeneralization of several previous metrics. Extensive experiments demonstrate\nthat the optimized formula outperforms 28 existing CD measures by a large\nmargin, offers reasonable local CD maps without the use of dense supervision,\ngeneralizes well to color patch data, and empirically behaves as a proper\nmetric in the mathematical sense.",
    "descriptor": "\nComments: 10 figures, 6 tables, 14 pages\n",
    "authors": [
      "Zhihua Wang",
      "Keshuo Xu",
      "Yang Yang",
      "Jianlei Dong",
      "Shuhang Gu",
      "Lihao Xu",
      "Yuming Fang",
      "Kede Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13489"
  },
  {
    "id": "arXiv:2205.13490",
    "title": "SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation",
    "abstract": "Conventional point cloud semantic segmentation methods usually employ an\nencoder-decoder architecture, where mid-level features are locally aggregated\nto extract geometric information. However, the over-reliance on these\nclass-agnostic local geometric representations may raise confusion between\nlocal parts from different categories that are similar in appearance or\nspatially adjacent. To address this issue, we argue that mid-level features can\nbe further enhanced with semantic information, and propose semantic-affine\ntransformation that transforms features of mid-level points belonging to\ndifferent categories with class-specific affine parameters. Based on this\ntechnique, we propose SemAffiNet for point cloud semantic segmentation, which\nutilizes the attention mechanism in the Transformer module to implicitly and\nexplicitly capture global structural knowledge within local parts for overall\ncomprehension of each category. We conduct extensive experiments on the\nScanNetV2 and NYUv2 datasets, and evaluate semantic-affine transformation on\nvarious 3D point cloud and 2D image segmentation baselines, where both\nqualitative and quantitative results demonstrate the superiority and\ngeneralization ability of our proposed approach. Code is available at\nhttps://github.com/wangzy22/SemAffiNet.",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Ziyi Wang",
      "Yongming Rao",
      "Xumin Yu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13490"
  },
  {
    "id": "arXiv:2205.13492",
    "title": "Sparse Graph Learning for Spatiotemporal Time Series",
    "abstract": "Outstanding achievements of graph neural networks for spatiotemporal time\nseries prediction show that relational constraints introduce a positive\ninductive bias into neural forecasting architectures. Often, however, the\nrelational information characterizing the underlying data generating process is\nunavailable; the practitioner is then left with the problem of inferring from\ndata which relational graph to use in the subsequent processing stages. We\npropose novel, principled -- yet practical -- probabilistic methods that learn\nthe relational dependencies by modeling distributions over graphs while\nmaximizing, at the same time, end-to-end the forecasting accuracy. Our novel\ngraph learning approach, based on consolidated variance reduction techniques\nfor Monte Carlo score-based gradient estimation, is theoretically grounded and\neffective. We show that tailoring the gradient estimators to the graph learning\nproblem allows us also for achieving state-of-the-art forecasting performance\nwhile controlling, at the same time, both the sparsity of the learned graph and\nthe computational burden. We empirically assess the effectiveness of the\nproposed method on synthetic and real-world benchmarks, showing that the\nproposed solution can be used as a stand-alone graph identification procedure\nas well as a learned component of an end-to-end forecasting architecture.",
    "descriptor": "",
    "authors": [
      "Andrea Cini",
      "Daniele Zambon",
      "Cesare Alippi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13492"
  },
  {
    "id": "arXiv:2205.13502",
    "title": "An Analytic Framework for Robust Training of Artificial Neural Networks",
    "abstract": "The reliability of a learning model is key to the successful deployment of\nmachine learning in various industries. Creating a robust model, particularly\none unaffected by adversarial attacks, requires a comprehensive understanding\nof the adversarial examples phenomenon. However, it is difficult to describe\nthe phenomenon due to the complicated nature of the problems in machine\nlearning. Consequently, many studies investigate the phenomenon by proposing a\nsimplified model of how adversarial examples occur and validate it by\npredicting some aspect of the phenomenon. While these studies cover many\ndifferent characteristics of the adversarial examples, they have not reached a\nholistic approach to the geometric and analytic modeling of the phenomenon.\nThis paper propose a formal framework to study the phenomenon in learning\ntheory and make use of complex analysis and holomorphicity to offer a robust\nlearning rule for artificial neural networks. With the help of complex\nanalysis, we can effortlessly move between geometric and analytic perspectives\nof the phenomenon and offer further insights on the phenomenon by revealing its\nconnection with harmonic functions. Using our model, we can explain some of the\nmost intriguing characteristics of adversarial examples, including\ntransferability of adversarial examples, and pave the way for novel approaches\nto mitigate the effects of the phenomenon.",
    "descriptor": "",
    "authors": [
      "Ramin Barati",
      "Reza Safabakhsh",
      "Mohammad Rahmati"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13502"
  },
  {
    "id": "arXiv:2205.13503",
    "title": "Multi-layer State Evolution Under Random Convolutional Design",
    "abstract": "Signal recovery under generative neural network priors has emerged as a\npromising direction in statistical inference and computational imaging.\nTheoretical analysis of reconstruction algorithms under generative priors is,\nhowever, challenging. For generative priors with fully connected layers and\nGaussian i.i.d. weights, this was achieved by the multi-layer approximate\nmessage (ML-AMP) algorithm via a rigorous state evolution. However, practical\ngenerative priors are typically convolutional, allowing for computational\nbenefits and inductive biases, and so the Gaussian i.i.d. weight assumption is\nvery limiting. In this paper, we overcome this limitation and establish the\nstate evolution of ML-AMP for random convolutional layers. We prove in\nparticular that random convolutional layers belong to the same universality\nclass as Gaussian matrices. Our proof technique is of an independent interest\nas it establishes a mapping between convolutional matrices and spatially\ncoupled sensing matrices used in coding theory.",
    "descriptor": "\nComments: Initial version\n",
    "authors": [
      "Max Daniels",
      "C\u00e9dric Gerbelot",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13503"
  },
  {
    "id": "arXiv:2205.13504",
    "title": "Are Transformers Effective for Time Series Forecasting?",
    "abstract": "Recently, there has been a surge of Transformer-based solutions for the time\nseries forecasting (TSF) task, especially for the challenging long-term TSF\nproblem. Transformer architecture relies on self-attention mechanisms to\neffectively extract the semantic correlations between paired elements in a long\nsequence, which is permutation-invariant and anti-ordering to some extent.\nHowever, in time series modeling, we are to extract the temporal relations\namong an ordering set of continuous points. Consequently, whether\nTransformer-based techniques are the right solutions for long-term time series\nforecasting is an interesting problem to investigate, despite the performance\nimprovements shown in these studies. In this work, we question the validity of\nTransformer-based TSF solutions. In their experiments, the compared\n(non-Transformer) baselines are mainly autoregressive forecasting solutions,\nwhich usually have a poor long-term prediction capability due to inevitable\nerror accumulation effects. In contrast, we use an embarrassingly simple\narchitecture named DLinear that conducts direct multi-step (DMS) forecasting\nfor comparison. DLinear decomposes the time series into a trend and a remainder\nseries and employs two one-layer linear networks to model these two series for\nthe forecasting task. Surprisingly, it outperforms existing complex\nTransformer-based models in most cases by a large margin. Therefore, we\nconclude that the relatively higher long-term forecasting accuracy of\nTransformer-based TSF solutions shown in existing works has little to do with\nthe temporal relation extraction capabilities of the Transformer architecture.\nInstead, it is mainly due to the non-autoregressive DMS forecasting strategy\nused in them. We hope this study also advocates revisiting the validity of\nTransformer-based solutions for other time series analysis tasks (e.g., anomaly\ndetection) in the future.",
    "descriptor": "\nComments: Code is available at \\url{this https URL}\n",
    "authors": [
      "Ailing Zeng",
      "Muxi Chen",
      "Lei Zhang",
      "Qiang Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13504"
  },
  {
    "id": "arXiv:2205.13505",
    "title": "Flipping the Script on Criminal Justice Risk Assessment: An actuarial  model for assessing the risk the federal sentencing system poses to  defendants",
    "abstract": "In the criminal justice system, algorithmic risk assessment instruments are\nused to predict the risk a defendant poses to society; examples include the\nrisk of recidivating or the risk of failing to appear at future court dates.\nHowever, defendants are also at risk of harm from the criminal justice system.\nTo date, there exists no risk assessment instrument that considers the risk the\nsystem poses to the individual. We develop a risk assessment instrument that\n\"flips the script.\" Using data about U.S. federal sentencing decisions, we\nbuild a risk assessment instrument that predicts the likelihood an individual\nwill receive an especially lengthy sentence given factors that should be\nlegally irrelevant to the sentencing decision. To do this, we develop a\ntwo-stage modeling approach. Our first-stage model is used to determine which\nsentences were \"especially lengthy.\" We then use a second-stage model to\npredict the defendant's risk of receiving a sentence that is flagged as\nespecially lengthy given factors that should be legally irrelevant. The factors\nthat should be legally irrelevant include, for example, race, court location,\nand other socio-demographic information about the defendant. Our instrument\nachieves comparable predictive accuracy to risk assessment instruments used in\npretrial and parole contexts. We discuss the limitations of our modeling\napproach and use the opportunity to highlight how traditional risk assessment\ninstruments in various criminal justice settings also suffer from many of the\nsame limitations and embedded value systems of their creators.",
    "descriptor": "\nComments: Conference on Fairness, Accountability, and Transparency (FAccT 2022)\n",
    "authors": [
      "Mikaela Meyer",
      "Aaron Horowitz",
      "Erica Marshall",
      "Kristian Lum"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.13505"
  },
  {
    "id": "arXiv:2205.13507",
    "title": "A framework for overparameterized learning",
    "abstract": "An explanation for the success of deep neural networks is a central question\nin theoretical machine learning. According to classical statistical learning,\nthe overparameterized nature of such models should imply a failure to\ngeneralize. Many argue that good empirical performance is due to the implicit\nregularization of first order optimization methods. In particular, the\nPolyak-{\\L}ojasiewicz condition leads to gradient descent finding a global\noptimum that is close to initialization. In this work, we propose a framework\nconsisting of a prototype learning problem, which is general enough to cover\nmany popular problems and even the cases of infinitely wide neural networks and\ninfinite data. We then perform an analysis from the perspective of the\nPolyak-{\\L}ojasiewicz condition. We obtain theoretical results of independent\ninterest, concerning gradient descent on a composition $(f \\circ F): G \\to\n\\mathbb{R}$ of functions $F: G \\to H$ and $f: H \\to \\mathbb{R}$ with $G, H$\nbeing Hilbert spaces. Building on these results, we determine the properties\nthat have to be satisfied by the components of the prototype problem for\ngradient descent to find a global optimum that is close to initialization. We\nthen demonstrate that supervised learning, variational autoencoders and\ntraining with gradient penalty can be translated to the prototype problem.\nFinally, we lay out a number of directions for future research.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "D\u00e1vid Terj\u00e9k",
      "Diego Gonz\u00e1lez-S\u00e1nchez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13507"
  },
  {
    "id": "arXiv:2205.13508",
    "title": "Pick up the PACE: Fast and Simple Domain Adaptation via Ensemble  Pseudo-Labeling",
    "abstract": "Domain Adaptation (DA) has received widespread attention from deep learning\nresearchers in recent years because of its potential to improve test accuracy\nwith out-of-distribution labeled data. Most state-of-the-art DA algorithms\nrequire an extensive amount of hyperparameter tuning and are computationally\nintensive due to the large batch sizes required. In this work, we propose a\nfast and simple DA method consisting of three stages: (1) domain alignment by\ncovariance matching, (2) pseudo-labeling, and (3) ensembling. We call this\nmethod $\\textbf{PACE}$, for $\\textbf{P}$seudo-labels, $\\textbf{A}$lignment of\n$\\textbf{C}$ovariances, and $\\textbf{E}$nsembles. PACE is trained on top of\nfixed features extracted from an ensemble of modern pretrained backbones. PACE\nexceeds previous state-of-the-art by $\\textbf{5 - 10 \\%}$ on most benchmark\nadaptation tasks without training a neural network. PACE reduces training time\nand hyperparameter tuning time by $82\\%$ and $97\\%$, respectively, when\ncompared to state-of-the-art DA methods. Code is released here:\nhttps://github.com/Chris210634/PACE-Domain-Adaptation",
    "descriptor": "",
    "authors": [
      "Christopher Liao",
      "Theodoros Tsiligkaridis",
      "Brian Kulis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13508"
  },
  {
    "id": "arXiv:2205.13515",
    "title": "Green Hierarchical Vision Transformer for Masked Image Modeling",
    "abstract": "We present an efficient approach for Masked Image Modeling (MIM) with\nhierarchical Vision Transformers (ViTs), e.g., Swin Transformer, allowing the\nhierarchical ViTs to discard masked patches and operate only on the visible\nones. Our approach consists of two key components. First, for the window\nattention, we design a Group Window Attention scheme following the\nDivide-and-Conquer strategy. To mitigate the quadratic complexity of the\nself-attention w.r.t. the number of patches, group attention encourages a\nuniform partition that visible patches within each local window of arbitrary\nsize can be grouped with equal size, where masked self-attention is then\nperformed within each group. Second, we further improve the grouping strategy\nvia the Dynamic Programming algorithm to minimize the overall computation cost\nof the attention on the grouped patches. As a result, MIM now can work on\nhierarchical ViTs in a green and efficient way. For example, we can train the\nhierarchical ViTs about 2.7$\\times$ faster and reduce the GPU memory usage by\n70%, while still enjoying competitive performance on ImageNet classification\nand the superiority on downstream COCO object detection benchmarks. Code and\npre-trained models have been made publicly available at\nhttps://github.com/LayneH/GreenMIM.",
    "descriptor": "\nComments: 16 pages, 7 figures, 3 tables, 3 algorithms\n",
    "authors": [
      "Lang Huang",
      "Shan You",
      "Mingkai Zheng",
      "Fei Wang",
      "Chen Qian",
      "Toshihiko Yamasaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13515"
  },
  {
    "id": "arXiv:2205.13516",
    "title": "The boundedness and zero isolation problems for weighted automata over  nonnegative rationals",
    "abstract": "We consider linear cost-register automata (equivalent to weighted automata)\nover the semiring of nonnegative rationals, which generalise probabilistic\nautomata. The two problems of boundedness and zero isolation ask whether there\nis a sequence of words that converge to infinity and to zero, respectively. In\nthe general model both problems are undecidable so we focus on the copyless\nlinear restriction. There, we show that the boundedness problem is decidable.\nAs for the zero isolation problem we need to further restrict the class. We\nobtain a model, where zero isolation becomes equivalent to universal\ncoverability of orthant vector addition systems (OVAS), a new model in the VAS\nfamily interesting on its own. In standard VAS runs are considered only in the\npositive orthant, while in OVAS every orthant has its own set of vectors that\ncan be applied in that orthant. Assuming Schanuel's conjecture is true, we\nprove decidability of universal coverability for three-dimensional OVAS, which\nimplies decidability of zero isolation in a model with at most three\nindependent registers.",
    "descriptor": "\nComments: Full versions of the LICS 2022 paper\n",
    "authors": [
      "Wojciech Czerwi\u0144ski",
      "Engel Lefaucheux",
      "Filip Mazowiecki",
      "David Purser",
      "Markus A. Whiteland"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13516"
  },
  {
    "id": "arXiv:2205.13521",
    "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining  Near Optimality",
    "abstract": "Finding different solutions to the same problem is a key aspect of\nintelligence associated with creativity and adaptation to novel situations. In\nreinforcement learning, a set of diverse policies can be useful for\nexploration, transfer, hierarchy, and robustness. We propose DOMiNO, a method\nfor Diversity Optimization Maintaining Near Optimality. We formalize the\nproblem as a Constrained Markov Decision Process where the objective is to find\ndiverse policies, measured by the distance between the state occupancies of the\npolicies in the set, while remaining near-optimal with respect to the extrinsic\nreward. We demonstrate that the method can discover diverse and meaningful\nbehaviors in various domains, such as different locomotion patterns in the\nDeepMind Control Suite. We perform extensive analysis of our approach, compare\nit with other multi-objective baselines, demonstrate that we can control both\nthe quality and the diversity of the set via interpretable hyperparameters, and\nshow that the discovered set is robust to perturbations.",
    "descriptor": "",
    "authors": [
      "Tom Zahavy",
      "Yannick Schroecker",
      "Feryal Behbahani",
      "Kate Baumli",
      "Sebastian Flennerhag",
      "Shaobo Hou",
      "Satinder Singh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13521"
  },
  {
    "id": "arXiv:2205.13522",
    "title": "Dynamically Relative Position Encoding-Based Transformer for Automatic  Code Edit",
    "abstract": "Adapting Deep Learning (DL) techniques to automate non-trivial coding\nactivities, such as code documentation and defect detection, has been\nintensively studied recently. Learning to predict code changes is one of the\npopular and essential investigations. Prior studies have shown that DL\ntechniques such as Neural Machine Translation (NMT) can benefit meaningful code\nchanges, including bug fixing and code refactoring. However, NMT models may\nencounter bottleneck when modeling long sequences, thus are limited in\naccurately predicting code changes. In this work, we design a Transformer-based\napproach, considering that Transformer has proven effective in capturing\nlong-term dependencies. Specifically, we propose a novel model named DTrans.\nFor better incorporating the local structure of code, i.e., statement-level\ninformation in this paper, DTrans is designed with dynamically relative\nposition encoding in the multi-head attention of Transformer. Experiments on\nbenchmark datasets demonstrate that DTrans can more accurately generate patches\nthan the state-of-the-art methods, increasing the performance by at least\n5.45\\%-46.57\\% in terms of the exact match metric on different datasets.\nMoreover, DTrans can locate the lines to change with 1.75\\%-24.21\\% higher\naccuracy than the existing methods.",
    "descriptor": "",
    "authors": [
      "Shiyi Qi",
      "Yaoxian Li",
      "Cuiyun Gao",
      "Xiaohong Su",
      "Shuzheng Gao",
      "Zibin Zheng",
      "Chuanyi Liu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.13522"
  },
  {
    "id": "arXiv:2205.13523",
    "title": "PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using  Adversarial Perturbations",
    "abstract": "Federated Learning (FL) enables numerous participants to train deep learning\nmodels collaboratively without exposing their personal, potentially sensitive\ndata, making it a promising solution for data privacy in collaborative\ntraining. The distributed nature of FL and unvetted data, however, makes it\ninherently vulnerable to backdoor attacks: In this scenario, an adversary\ninjects backdoor functionality into the centralized model during training,\nwhich can be triggered to cause the desired misclassification for a specific\nadversary-chosen input. A range of prior work establishes successful backdoor\ninjection in an FL system; however, these backdoors are not demonstrated to be\nlong-lasting. The backdoor functionality does not remain in the system if the\nadversary is removed from the training process since the centralized model\nparameters continuously mutate during successive FL training rounds. Therefore,\nin this work, we propose PerDoor, a persistent-by-construction backdoor\ninjection technique for FL, driven by adversarial perturbation and targeting\nparameters of the centralized model that deviate less in successive FL rounds\nand contribute the least to the main task accuracy. An exhaustive evaluation\nconsidering an image classification scenario portrays on average $10.5\\times$\npersistence over multiple FL rounds compared to traditional backdoor attacks.\nThrough experiments, we further exhibit the potency of PerDoor in the presence\nof state-of-the-art backdoor prevention techniques in an FL system.\nAdditionally, the operation of adversarial perturbation also assists PerDoor in\ndeveloping non-uniform trigger patterns for backdoor inputs compared to uniform\ntriggers (with fixed patterns and locations) of existing backdoor techniques,\nwhich are prone to be easily mitigated.",
    "descriptor": "",
    "authors": [
      "Manaar Alam",
      "Esha Sarkar",
      "Michail Maniatakos"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13523"
  },
  {
    "id": "arXiv:2205.13524",
    "title": "PREF: Phasorial Embedding Fields for Compact Neural Representations",
    "abstract": "We present a phasorial embedding field \\emph{PREF} as a compact\nrepresentation to facilitate neural signal modeling and reconstruction tasks.\nPure multi-layer perceptron (MLP) based neural techniques are biased towards\nlow frequency signals and have relied on deep layers or Fourier encoding to\navoid losing details. PREF instead employs a compact and physically explainable\nencoding field based on the phasor formulation of the Fourier embedding space.\nWe conduct a comprehensive theoretical analysis to demonstrate the advantages\nof PREF over the latest spatial embedding techniques. We then develop a highly\nefficient frequency learning framework using an approximated inverse Fourier\ntransform scheme for PREF along with a novel Parseval regularizer. Extensive\nexperiments show our compact PREF-based neural signal processing technique is\non par with the state-of-the-art in 2D image completion, 3D SDF surface\nregression, and 5D radiance field reconstruction.",
    "descriptor": "",
    "authors": [
      "Binbin Huang",
      "Xinhao Yan",
      "Anpei Chen",
      "Shenghua Gao",
      "Jingyi Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13524"
  },
  {
    "id": "arXiv:2205.13525",
    "title": "Kernel Ridgeless Regression is Inconsistent for Low Dimensions",
    "abstract": "We show that kernel interpolation for a large class of shift-invariant\nkernels is inconsistent in fixed dimension, even with bandwidth adaptive to the\ntraining set.",
    "descriptor": "",
    "authors": [
      "Daniel Beaglehole",
      "Mikhail Belkin",
      "Parthe Pandit"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13525"
  },
  {
    "id": "arXiv:2205.13528",
    "title": "TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement  Learning",
    "abstract": "Efficient exploration is a crucial challenge in deep reinforcement learning.\nSeveral methods, such as behavioral priors, are able to leverage offline data\nin order to efficiently accelerate reinforcement learning on complex tasks.\nHowever, if the task at hand deviates excessively from the demonstrated task,\nthe effectiveness of such methods is limited. In our work, we propose to learn\nfeatures from offline data that are shared by a more diverse range of tasks,\nsuch as correlation between actions and directedness. Therefore, we introduce\nstate-independent temporal priors, which directly model temporal consistency in\ndemonstrated trajectories, and are capable of driving exploration in complex\ntasks, even when trained on data collected on simpler tasks. Furthermore, we\nintroduce a novel integration scheme for action priors in off-policy\nreinforcement learning by dynamically sampling actions from a probabilistic\nmixture of policy and action prior. We compare our approach against strong\nbaselines and provide empirical evidence that it can accelerate reinforcement\nlearning in long-horizon continuous control tasks under sparse reward settings.",
    "descriptor": "",
    "authors": [
      "Marco Bagatella",
      "Sammy Christen",
      "Otmar Hilliges"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13528"
  },
  {
    "id": "arXiv:2205.13530",
    "title": "Semantic Parsing of Interpage Relations",
    "abstract": "Page-level analysis of documents has been a topic of interest in digitization\nefforts, and multimodal approaches have been applied to both classification and\npage stream segmentation. In this work, we focus on capturing finer semantic\nrelations between pages of a multi-page document. To this end, we formalize the\ntask as semantic parsing of interpage relations and we propose an end-to-end\napproach for interpage dependency extraction, inspired by the dependency\nparsing literature. We further design a multi-task training approach to jointly\noptimize for page embeddings to be used in segmentation, classification, and\nparsing of the page dependencies using textual and visual features extracted\nfrom the pages. Moreover, we also combine the features from two modalities to\nobtain multimodal page embeddings. To the best of our knowledge, this is the\nfirst study to extract rich semantic interpage relations from multi-page\ndocuments. Our experimental results show that the proposed method increased LAS\nby 41 percentage points for semantic parsing, increased accuracy by 33\npercentage points for page stream segmentation, and 45 percentage points for\npage classification over a naive baseline.",
    "descriptor": "\nComments: Accepted to 26th International Conference on Pattern Recognition (ICPR 2022)\n",
    "authors": [
      "Mehmet Arif Demirta\u015f",
      "Berke Oral",
      "Mehmet Yasin Akp\u0131nar",
      "Onur Deniz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13530"
  },
  {
    "id": "arXiv:2205.13531",
    "title": "Training ReLU networks to high uniform accuracy is intractable",
    "abstract": "Statistical learning theory provides bounds on the necessary number of\ntraining samples needed to reach a prescribed accuracy in a learning problem\nformulated over a given target class. This accuracy is typically measured in\nterms of a generalization error, that is, an expected value of a given loss\nfunction. However, for several applications -- for example in a\nsecurity-critical context or for problems in the computational sciences --\naccuracy in this sense is not sufficient. In such cases, one would like to have\nguarantees for high accuracy on every input value, that is, with respect to the\nuniform norm. In this paper we precisely quantify the number of training\nsamples needed for any conceivable training algorithm to guarantee a given\nuniform accuracy on any learning problem formulated over target classes\ncontaining (or consisting of) ReLU neural networks of a prescribed\narchitecture. We prove that, under very general assumptions, the minimal number\nof training samples for this task scales exponentially both in the depth and\nthe input dimension of the network architecture. As a corollary we conclude\nthat the training of ReLU neural networks to high uniform accuracy is\nintractable. In a security-critical context this points to the fact that deep\nlearning based systems are prone to being fooled by a possible adversary. We\ncorroborate our theoretical findings by numerical results.",
    "descriptor": "",
    "authors": [
      "Julius Berner",
      "Philipp Grohs",
      "Felix Voigtlaender"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13531"
  },
  {
    "id": "arXiv:2205.13532",
    "title": "Selective Classification Via Neural Network Training Dynamics",
    "abstract": "Selective classification is the task of rejecting inputs a model would\npredict incorrectly on through a trade-off between input space coverage and\nmodel accuracy. Current methods for selective classification impose constraints\non either the model architecture or the loss function; this inhibits their\nusage in practice. In contrast to prior work, we show that state-of-the-art\nselective classification performance can be attained solely from studying the\n(discretized) training dynamics of a model. We propose a general framework\nthat, for a given test input, monitors metrics capturing the disagreement with\nthe final predicted label over intermediate models obtained during training; we\nthen reject data points exhibiting too much disagreement at late stages in\ntraining. In particular, we instantiate a method that tracks when the label\npredicted during training stops disagreeing with the final predicted label. Our\nexperimental evaluation shows that our method achieves state-of-the-art\naccuracy/coverage trade-offs on typical selective classification benchmarks.\nFor example, we improve coverage on CIFAR-10/SVHN by 10.1%/1.5% respectively at\na fixed target error of 0.5%.",
    "descriptor": "",
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Kimia Hamidieh",
      "Adam Dziedzic",
      "Nicolas Papernot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13532"
  },
  {
    "id": "arXiv:2205.13535",
    "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual  Recognition",
    "abstract": "Although the pre-trained Vision Transformers (ViTs) achieved great success in\ncomputer vision, adapting a ViT to various image and video tasks is challenging\nbecause of its heavy computation and storage burdens, where each model needs to\nbe independently and comprehensively fine-tuned to different tasks, limiting\nits transferability in different domains. To address this challenge, we propose\nan effective adaptation approach for Transformer, namely AdaptFormer, which can\nadapt the pre-trained ViTs into many different image and video tasks\nefficiently. It possesses several benefits more appealing than prior arts.\nFirstly, AdaptFormer introduces lightweight modules that only add less than 2%\nextra parameters to a ViT, while it is able to increase the ViT's\ntransferability without updating its original pre-trained parameters,\nsignificantly outperforming the existing 100% fully fine-tuned models on action\nrecognition benchmarks. Secondly, it can be plug-and-play in different\nTransformers and scalable to many visual tasks. Thirdly, extensive experiments\non five image and video datasets show that AdaptFormer largely improves ViTs in\nthe target domains. For example, when updating just 1.5% extra parameters, it\nachieves about 10% and 19% relative improvement compared to the fully\nfine-tuned models on Something-Something~v2 and HMDB51, respectively. Project\npage: this http URL",
    "descriptor": "\nComments: Technical report. Project page: this http URL\n",
    "authors": [
      "Shoufa Chen",
      "Chongjian Ge",
      "Zhan Tong",
      "Jiangliu Wang",
      "Yibing Song",
      "Jue Wang",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13535"
  },
  {
    "id": "arXiv:2205.13536",
    "title": "Verifying Learning-Based Robotic Navigation Systems",
    "abstract": "Deep reinforcement learning (DRL) has become a dominant deep-learning\nparadigm for various tasks in which complex policies are learned within\nreactive systems. In parallel, there has recently been significant research on\nverifying deep neural networks. However, to date, there has been little work\ndemonstrating the use of modern verification tools on real, DRL-controlled\nsystems.\nIn this case-study paper, we attempt to begin bridging this gap, and focus on\nthe important task of mapless robotic navigation -- a classic robotics problem,\nin which a robot, usually controlled by a DRL agent, needs to efficiently and\nsafely navigate through an unknown arena towards a desired target. We\ndemonstrate how modern verification engines can be used for effective model\nselection, i.e., the process of selecting the best available policy for the\nrobot in question from a pool of candidate policies. Specifically, we use\nverification to detect and rule out policies that may demonstrate suboptimal\nbehavior, such as collisions and infinite loops. We also apply verification to\nidentify models with overly conservative behavior, thus allowing users to\nchoose superior policies that are better at finding an optimal, shorter path to\na target.\nTo validate our work, we conducted extensive experiments on an actual robot,\nand confirmed that the suboptimal policies detected by our method were indeed\nflawed. We also compared our verification-driven approach to state-of-the-art\ngradient attacks, and our results demonstrate that gradient-based methods are\ninadequate in this setting.\nOur work is the first to demonstrate the use of DNN verification backends for\nrecognizing suboptimal DRL policies in real-world robots, and for filtering out\nunwanted policies. We believe that the methods presented in this work can be\napplied to a large range of application domains that incorporate\ndeep-learning-based agents.",
    "descriptor": "",
    "authors": [
      "Guy Amir",
      "Davide Corsi",
      "Raz Yerushalmi",
      "Luca Marzari",
      "David Harel",
      "Alessandro Farinelli",
      "Guy Katz"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13536"
  },
  {
    "id": "arXiv:2205.13538",
    "title": "On the separation of correlation-assisted sum capacities of multiple  access channels",
    "abstract": "The capacity of a channel characterizes the maximum rate at which information\ncan be transmitted through the channel asymptotically faithfully. For a channel\nwith multiple senders and a single receiver, computing its sum capacity is\npossible in theory, but challenging in practice because of the nonconvex\noptimization involved. In this work, we study the sum capacity of a family of\nmultiple access channels (MACs) obtained from nonlocal games. For any MAC in\nthis family, we obtain an upper bound on the sum rate that depends only on the\nproperties of the game when allowing assistance from an arbitrary set of\ncorrelations between the senders. This approach can be used to prove\nseparations between sum capacities when the senders are allowed to share\ndifferent sets of correlations, such as classical, quantum or no-signalling\ncorrelations. We also construct a specific nonlocal game to show that the usual\napproach of bounding the sum capacity by relaxing the nonconvex optimization\ncan give arbitrarily loose bounds. Towards a potential solution to this\nproblem, we first prove a Lipschitz-like property for the mutual information.\nUsing a modification of existing algorithms for optimizing Lipschitz-continuous\nfunctions, we then show that it is possible to compute the sum capacity of an\narbitrary two-sender MAC to a fixed additive precision in quasi-polynomial\ntime. We showcase our method by efficiently computing the sum capacity of a\nfamily of two-sender MACs for which one of the input alphabets has size two.\nFurthermore, we demonstrate with an example that our algorithm may compute the\nsum capacity to a higher precision than using the convex relaxation.",
    "descriptor": "\nComments: 64 pages, 2 figures\n",
    "authors": [
      "Akshay Seshadri",
      "Felix Leditzky",
      "Vikesh Siddhu",
      "Graeme Smith"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13538"
  },
  {
    "id": "arXiv:2205.13542",
    "title": "BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View  Representation",
    "abstract": "Multi-sensor fusion is essential for an accurate and reliable autonomous\ndriving system. Recent approaches are based on point-level fusion: augmenting\nthe LiDAR point cloud with camera features. However, the camera-to-LiDAR\nprojection throws away the semantic density of camera features, hindering the\neffectiveness of such methods, especially for semantic-oriented tasks (such as\n3D scene segmentation). In this paper, we break this deeply-rooted convention\nwith BEVFusion, an efficient and generic multi-task multi-sensor fusion\nframework. It unifies multi-modal features in the shared bird's-eye view (BEV)\nrepresentation space, which nicely preserves both geometric and semantic\ninformation. To achieve this, we diagnose and lift key efficiency bottlenecks\nin the view transformation with optimized BEV pooling, reducing latency by more\nthan 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports\ndifferent 3D perception tasks with almost no architectural changes. It\nestablishes the new state of the art on nuScenes, achieving 1.3% higher mAP and\nNDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with\n1.9x lower computation cost.",
    "descriptor": "\nComments: The first two authors contributed equally to this work. Project page: this https URL\n",
    "authors": [
      "Zhijian Liu",
      "Haotian Tang",
      "Alexander Amini",
      "Xinyu Yang",
      "Huizi Mao",
      "Daniela Rus",
      "Song Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13542"
  },
  {
    "id": "arXiv:2205.13543",
    "title": "Revealing the Dark Secrets of Masked Image Modeling",
    "abstract": "Masked image modeling (MIM) as pre-training is shown to be effective for\nnumerous vision downstream tasks, but how and where MIM works remain unclear.\nIn this paper, we compare MIM with the long-dominant supervised pre-trained\nmodels from two perspectives, the visualizations and the experiments, to\nuncover their key representational differences. From the visualizations, we\nfind that MIM brings locality inductive bias to all layers of the trained\nmodels, but supervised models tend to focus locally at lower layers but more\nglobally at higher layers. That may be the reason why MIM helps Vision\nTransformers that have a very large receptive field to optimize. Using MIM, the\nmodel can maintain a large diversity on attention heads in all layers. But for\nsupervised models, the diversity on attention heads almost disappears from the\nlast three layers and less diversity harms the fine-tuning performance. From\nthe experiments, we find that MIM models can perform significantly better on\ngeometric and motion tasks with weak semantics or fine-grained classification\ntasks, than their supervised counterparts. Without bells and whistles, a\nstandard MIM pre-trained SwinV2-L could achieve state-of-the-art performance on\npose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth\nestimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object\ntracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the\ncategories are sufficiently covered by the supervised pre-training, MIM models\ncan still achieve highly competitive transfer performance. With a deeper\nunderstanding of MIM, we hope that our work can inspire new and solid research\nin this direction.",
    "descriptor": "",
    "authors": [
      "Zhenda Xie",
      "Zigang Geng",
      "Jingcheng Hu",
      "Zheng Zhang",
      "Han Hu",
      "Yue Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13543"
  },
  {
    "id": "arXiv:2205.12995",
    "title": "Black holes and cryptocurrencies",
    "abstract": "It has been proposed in the literature that the volume of Einstein-Rosen\nbridge is equal to complexity of state preparation (\"Complexity=Volume\"\nconjecture). Taking this statement outside the horizon, one might be tempted to\npropose \"Complexity=Time\" correspondence. In this Essay we argue that in a\nblockchain protocol, which is the foundation of all modern cryptocurrencies,\ntime is emergent and it is defined according to a version of \"Complexity=Time\".",
    "descriptor": "\nComments: 15 pages. Essay written for the Gravity Research Foundation 2022 Awards for Essays on Gravitation (extended version)\n",
    "authors": [
      "Alexey Milekhin"
    ],
    "subjectives": [
      "High Energy Physics - Theory (hep-th)",
      "Computational Complexity (cs.CC)",
      "General Relativity and Quantum Cosmology (gr-qc)"
    ],
    "url": "https://arxiv.org/abs/2205.12995"
  },
  {
    "id": "arXiv:2205.12997",
    "title": "An Algorithmic Approach to Emergence",
    "abstract": "We suggest a quantitative and objective notion of emergence. Our proposal\nuses algorithmic information theory as a basis for an objective framework in\nwhich a bit string encodes observational data. Plurality of drops in the\nKolmogorov structure function of such a string is seen as the hallmark of\nemergence. Our definition offers some theoretical results, in addition to\nextending the notions of coarse-graining and boundary conditions. Finally, we\nconfront our proposal with applications to dynamical systems and\nthermodynamics.",
    "descriptor": "\nComments: 39 pages, 11 figures\n",
    "authors": [
      "Charles Alexandre B\u00e9dard",
      "Geoffroy Bergeron"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.12997"
  },
  {
    "id": "arXiv:2205.13051",
    "title": "Online Deep Equilibrium Learning for Regularization by Denoising",
    "abstract": "Plug-and-Play Priors (PnP) and Regularization by Denoising (RED) are\nwidely-used frameworks for solving imaging inverse problems by computing\nfixed-points of operators combining physical measurement models and learned\nimage priors. While traditional PnP/RED formulations have focused on priors\nspecified using image denoisers, there is a growing interest in learning\nPnP/RED priors that are end-to-end optimal. The recent Deep Equilibrium Models\n(DEQ) framework has enabled memory-efficient end-to-end learning of PnP/RED\npriors by implicitly differentiating through the fixed-point equations without\nstoring intermediate activation values. However, the dependence of the\ncomputational/memory complexity of the measurement models in PnP/RED on the\ntotal number of measurements leaves DEQ impractical for many imaging\napplications. We propose ODER as a new strategy for improving the efficiency of\nDEQ through stochastic approximations of the measurement models. We\ntheoretically analyze ODER giving insights into its convergence and ability to\napproximate the traditional DEQ approach. Our numerical results suggest the\npotential improvements in training/testing complexity due to ODER on three\ndistinct imaging applications.",
    "descriptor": "\nComments: 28 pages, 8 figures\n",
    "authors": [
      "Jiaming Liu",
      "Xiaojian Xu",
      "Weijie Gan",
      "Shirin Shoushtari",
      "Ulugbek S. Kamilov"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13051"
  },
  {
    "id": "arXiv:2205.13055",
    "title": "Complexity-Optimal and Curvature-Free First-Order Methods for Finding  Stationary Points of Composite Optimization Problems",
    "abstract": "This paper develops and analyzes an accelerated proximal descent method for\nfinding stationary points of nonconvex composite optimization problems. The\nobjective function is of the form $f+h$ where $h$ is a proper closed convex\nfunction, $f$ is a differentiable function on the domain of $h$, and $\\nabla f$\nis Lipschitz continuous on the domain of $h$. The main advantage of this method\nis that it is \"curvature-free\" in the sense that it does not require knowledge\nof the Lipschitz constant of $\\nabla f$ or of any global topological properties\nof $f$. It is shown that the proposed method can obtain a $\\rho$-approximate\nstationary point with iteration complexity bounds that are optimal, up to\nlogarithmic terms over $\\rho$, in both the convex and nonconvex settings. Some\ndiscussion is also given about how the proposed method can be leveraged in\nother existing optimization frameworks, such as min-max smoothing and penalty\nframeworks for constrained programming, to create more specialized\ncurvature-free methods. Finally, numerical experiments on a set of nonconvex\nquadratic semidefinite programming problems are given to support the practical\nviability of the method.",
    "descriptor": "",
    "authors": [
      "Weiwei Kong"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13055"
  },
  {
    "id": "arXiv:2205.13056",
    "title": "Efficient and Near-Optimal Smoothed Online Learning for Generalized  Linear Functions",
    "abstract": "Due to the drastic gap in complexity between sequential and batch statistical\nlearning, recent work has studied a smoothed sequential learning setting, where\nNature is constrained to select contexts with density bounded by 1/{\\sigma}\nwith respect to a known measure {\\mu}. Unfortunately, for some function\nclasses, there is an exponential gap between the statistically optimal regret\nand that which can be achieved efficiently. In this paper, we give a\ncomputationally efficient algorithm that is the first to enjoy the\nstatistically optimal log(T/{\\sigma}) regret for realizable K-wise linear\nclassification. We extend our results to settings where the true classifier is\nlinear in an over-parameterized polynomial featurization of the contexts, as\nwell as to a realizable piecewise-regression setting assuming access to an\nappropriate ERM oracle. Somewhat surprisingly, standard disagreement-based\nanalyses are insufficient to achieve regret logarithmic in 1/{\\sigma}. Instead,\nwe develop a novel characterization of the geometry of the disagreement region\ninduced by generalized linear classifiers. Along the way, we develop numerous\ntechnical tools of independent interest, including a general anti-concentration\nbound for the determinant of certain matrix averages.",
    "descriptor": "",
    "authors": [
      "Adam Block",
      "Max Simchowitz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13056"
  },
  {
    "id": "arXiv:2205.13080",
    "title": "Factorized Structured Regression for Large-Scale Varying Coefficient  Models",
    "abstract": "Recommender Systems (RS) pervade many aspects of our everyday digital life.\nProposed to work at scale, state-of-the-art RS allow the modeling of thousands\nof interactions and facilitate highly individualized recommendations.\nConceptually, many RS can be viewed as instances of statistical regression\nmodels that incorporate complex feature effects and potentially non-Gaussian\noutcomes. Such structured regression models, including time-aware varying\ncoefficients models, are, however, limited in their applicability to\ncategorical effects and inclusion of a large number of interactions. Here, we\npropose Factorized Structured Regression (FaStR) for scalable varying\ncoefficient models. FaStR overcomes limitations of general regression models\nfor large-scale data by combining structured additive regression and\nfactorization approaches in a neural network-based model implementation. This\nfusion provides a scalable framework for the estimation of statistical models\nin previously infeasible data settings. Empirical results confirm that the\nestimation of varying coefficients of our approach is on par with\nstate-of-the-art regression techniques, while scaling notably better and also\nbeing competitive with other time-aware RS in terms of prediction performance.\nWe illustrate FaStR's performance and interpretability on a large-scale\nbehavioral study with smartphone user data.",
    "descriptor": "",
    "authors": [
      "David R\u00fcgamer",
      "Andreas Bender",
      "Simon Wiegrebe",
      "Daniel Racek",
      "Bernd Bischl",
      "Christian L. M\u00fcller",
      "Clemens Stachl"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.13080"
  },
  {
    "id": "arXiv:2205.13085",
    "title": "Identifying Patient-Specific Root Causes with the Heteroscedastic Noise  Model",
    "abstract": "Complex diseases are caused by a multitude of factors that may differ between\npatients even within the same diagnostic category. A few underlying root causes\nmay nevertheless initiate the development of disease within each patient. We\ntherefore focus on identifying patient-specific root causes of disease, which\nwe equate to the sample-specific predictivity of the exogenous error terms in a\nstructural equation model. We generalize from the linear setting to the\nheteroscedastic noise model where $Y = m(X) + \\varepsilon\\sigma(X)$ with\nnon-linear functions $m(X)$ and $\\sigma(X)$ representing the conditional mean\nand mean absolute deviation, respectively. This model preserves identifiability\nbut introduces non-trivial challenges that require a customized algorithm\ncalled Generalized Root Causal Inference (GRCI) to extract the error terms\ncorrectly. GRCI recovers patient-specific root causes more accurately than\nexisting alternatives.",
    "descriptor": "",
    "authors": [
      "Eric V. Strobl",
      "Thomas A. Lasko"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.13085"
  },
  {
    "id": "arXiv:2205.13122",
    "title": "To image, or not to image: Class-specific diffractive cameras with  all-optical erasure of undesired objects",
    "abstract": "Privacy protection is a growing concern in the digital era, with machine\nvision techniques widely used throughout public and private settings. Existing\nmethods address this growing problem by, e.g., encrypting camera images or\nobscuring/blurring the imaged information through digital algorithms. Here, we\ndemonstrate a camera design that performs class-specific imaging of target\nobjects with instantaneous all-optical erasure of other classes of objects.\nThis diffractive camera consists of transmissive surfaces structured using deep\nlearning to perform selective imaging of target classes of objects positioned\nat its input field-of-view. After their fabrication, the thin diffractive\nlayers collectively perform optical mode filtering to accurately form images of\nthe objects that belong to a target data class or group of classes, while\ninstantaneously erasing objects of the other data classes at the output\nfield-of-view. Using the same framework, we also demonstrate the design of\nclass-specific permutation cameras, where the objects of a target data class\nare pixel-wise permuted for all-optical class-specific encryption, while the\nother objects are irreversibly erased from the output image. The success of\nclass-specific diffractive cameras was experimentally demonstrated using\nterahertz (THz) waves and 3D-printed diffractive layers that selectively imaged\nonly one class of the MNIST handwritten digit dataset, all-optically erasing\nthe other handwritten digits. This diffractive camera design can be scaled to\ndifferent parts of the electromagnetic spectrum, including, e.g., the visible\nand infrared wavelengths, to provide transformative opportunities for\nprivacy-preserving digital cameras and task-specific data-efficient imaging.",
    "descriptor": "\nComments: 31 Pages, 7 Figures\n",
    "authors": [
      "Bijie Bai",
      "Yi Luo",
      "Tianyi Gan",
      "Jingtian Hu",
      "Yuhang Li",
      "Yifan Zhao",
      "Deniz Mengu",
      "Mona Jarrahi",
      "Aydogan Ozcan"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13122"
  },
  {
    "id": "arXiv:2205.13166",
    "title": "On Learning Mixture of Linear Regressions in the Non-Realizable Setting",
    "abstract": "While mixture of linear regressions (MLR) is a well-studied topic, prior\nworks usually do not analyze such models for prediction error. In fact, {\\em\nprediction} and {\\em loss} are not well-defined in the context of mixtures. In\nthis paper, first we show that MLR can be used for prediction where instead of\npredicting a label, the model predicts a list of values (also known as {\\em\nlist-decoding}). The list size is equal to the number of components in the\nmixture, and the loss function is defined to be minimum among the losses\nresulted by all the component models. We show that with this definition, a\nsolution of the empirical risk minimization (ERM) achieves small probability of\nprediction error. This begs for an algorithm to minimize the empirical risk for\nMLR, which is known to be computationally hard. Prior algorithmic works in MLR\nfocus on the {\\em realizable} setting, i.e., recovery of parameters when data\nis probabilistically generated by a mixed linear (noisy) model. In this paper\nwe show that a version of the popular alternating minimization (AM) algorithm\nfinds the best fit lines in a dataset even when a realizable model is not\nassumed, under some regularity conditions on the dataset and the initial\npoints, and thereby provides a solution for the ERM. We further provide an\nalgorithm that runs in polynomial time in the number of datapoints, and\nrecovers a good approximation of the best fit lines. The two algorithms are\nexperimentally compared.",
    "descriptor": "\nComments: To appear in ICML 2022\n",
    "authors": [
      "Avishek Ghosh",
      "Arya Mazumdar",
      "Soumyabrata Pal",
      "Rajat Sen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13166"
  },
  {
    "id": "arXiv:2205.13221",
    "title": "QSpeech: Low-Qubit Quantum Speech Application Toolkit",
    "abstract": "Quantum devices with low qubits are common in the Noisy Intermediate-Scale\nQuantum (NISQ) era. However, Quantum Neural Network (QNN) running on low-qubit\nquantum devices would be difficult since it is based on Variational Quantum\nCircuit (VQC), which requires many qubits. Therefore, it is critical to make\nQNN with VQC run on low-qubit quantum devices. In this study, we propose a\nnovel VQC called the low-qubit VQC. VQC requires numerous qubits based on the\ninput dimension; however, the low-qubit VQC with linear transformation can\nliberate this condition. Thus, it allows the QNN to run on low-qubit quantum\ndevices for speech applications. Furthermore, as compared to the VQC, our\nproposed low-qubit VQC can stabilize the training process more. Based on the\nlow-qubit VQC, we implement QSpeech, a library for quick prototyping of hybrid\nquantum-classical neural networks in the speech field. It has numerous quantum\nneural layers and QNN models for speech applications. Experiments on Speech\nCommand Recognition and Text-to-Speech show that our proposed low-qubit VQC\noutperforms VQC and is more stable.",
    "descriptor": "\nComments: Accepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks). QSpeech code available at this https URL\n",
    "authors": [
      "Zhenhou Hong",
      "Jianzong Wang",
      "Xiaoyang Qu",
      "Chendong Zhao",
      "Wei Tao",
      "Jing Xiao"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13221"
  },
  {
    "id": "arXiv:2205.13270",
    "title": "Computing homomorphisms in hereditary graph classes: the peculiar case  of the 5-wheel and graphs with no long claws",
    "abstract": "For graphs $G$ and $H$, an $H$-coloring of $G$ is an edge-preserving mapping\nfrom $V(G)$ to $V(H)$. In the $H$-Coloring problem the graph $H$ is fixed and\nwe ask whether an instance graph $G$ admits an $H$-coloring. A generalization\nof this problem is $H$-ColoringExt, where some vertices of $G$ are already\nmapped to vertices of $H$ and we ask if this partial mapping can be extended to\nan $H$-coloring.\nWe study the complexity of variants of $H$-Coloring in $F$-free graphs, i.e.,\ngraphs excluding a fixed graph $F$ as an induced subgraph. For integers $a,b,c\n\\geq 1$, by $S_{a,b,c}$ we denote the graph obtained by identifying one\nendvertex of three paths on $a+1$, $b+1$, and $c+1$ vertices, respectively. For\nodd $k \\geq 5$, by $W_k$ we denote the graph obtained from the $k$-cycle by\nadding a universal vertex.\nAs our main algorithmic result we show that $W_5$-ColoringExt is\npolynomial-time solvable in $S_{2,1,1}$-free graphs. This result exhibits an\ninteresting non-monotonicity of $H$-ColoringExt with respect to taking induced\nsubgraphs of $H$. Indeed, $W_5$ contains a triangle, and $K_3$-Coloring, i.e.,\nclassical 3-coloring, is NP-hard already in claw-free (i.e., $S_{1,1,1}$-free)\ngraphs.\nOur algorithm is based on two main observations:\n1. $W_5$-ColoringExt in $S_{2,1,1}$-free graphs can be in polynomial time\nreduced to a variant of the problem of finding an independent set intersecting\nall triangles, and\n2. the latter problem can be solved in polynomial time in $S_{2,1,1}$-free\ngraphs.\nWe complement this algorithmic result with several negative ones. In\nparticular, we show that $W_5$-ColoringExt is NP-hard in $S_{3,3,3}$-free\ngraphs. This is again uncommon, as usually problems that are NP-hard in\n$S_{a,b,c}$-free graphs for some constant $a,b,c$ are already hard in claw-free\ngraphs.",
    "descriptor": "",
    "authors": [
      "Micha\u0142 D\u0119bski",
      "Zbigniew Lonc",
      "Karolina Okrasa",
      "Marta Piecyk",
      "Pawe\u0142 Rz\u0105\u017cewski"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.13270"
  },
  {
    "id": "arXiv:2205.13285",
    "title": "The Babylonian Graph",
    "abstract": "The Babylonian graph B has the positive integers as vertices and connects two\nif they define a Pythagorean triple. Triangular subgraphs correspond to Euler\nbricks. What are the properties of this graph? Are there tetrahedral subgraphs\ncorresponding to Euler tesseracts? Is there only one infinite connected\ncomponent? Are there two Euler bricks in the graph that are disconnected? Do\nthe number of edges or triangles in the subgraph generated by the first n\nvertices grow like of the order n W(n), where n is the product log? We prove\nhere some first results like the threshold where B(n) becomes non-planar. In an\nappendix, we include handout from a talk on Euler cuboids given in the year\n2009.",
    "descriptor": "\nComments: 23 pages, 15 figures\n",
    "authors": [
      "Oliver Knill"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2205.13285"
  },
  {
    "id": "arXiv:2205.13293",
    "title": "Joint Training of Speech Enhancement and Self-supervised Model for  Noise-robust ASR",
    "abstract": "Speech enhancement (SE) is usually required as a front end to improve the\nspeech quality in noisy environments, while the enhanced speech might not be\noptimal for automatic speech recognition (ASR) systems due to speech\ndistortion. On the other hand, it was shown that self-supervised pre-training\nenables the utilization of a large amount of unlabeled noisy data, which is\nrather beneficial for the noise robustness of ASR. However, the potential of\nthe (optimal) integration of SE and self-supervised pre-training still remains\nunclear. In order to find an appropriate combination and reduce the impact of\nspeech distortion caused by SE, in this paper we therefore propose a joint\npre-training approach for the SE module and the self-supervised model. First,\nin the pre-training phase the original noisy waveform or the waveform obtained\nby SE is fed into the self-supervised model to learn the contextual\nrepresentation, where the quantified clean speech acts as the target. Second,\nwe propose a dual-attention fusion method to fuse the features of noisy and\nenhanced speeches, which can compensate the information loss caused by\nseparately using individual modules. Due to the flexible exploitation of\nclean/noisy/enhanced branches, the proposed method turns out to be a\ngeneralization of some existing noise-robust ASR models, e.g., enhanced\nwav2vec2.0. Finally, experimental results on both synthetic and real noisy\ndatasets show that the proposed joint training approach can improve the ASR\nperformance under various noisy settings, leading to a stronger noise\nrobustness.",
    "descriptor": "\nComments: submitted to IEEE/ACM TASLP. arXiv admin note: text overlap with arXiv:2201.08930\n",
    "authors": [
      "Qiu-Shi Zhu",
      "Jie Zhang",
      "Zi-Qiang Zhang",
      "Li-Rong Dai"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.13293"
  },
  {
    "id": "arXiv:2205.13297",
    "title": "DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment  of CT Images",
    "abstract": "Reliably detecting diseases using relevant biological information is crucial\nfor real-world applicability of deep learning techniques in medical imaging. We\ndebias deep learning models during training against unknown bias - without\npreprocessing/filtering the input beforehand or assuming specific knowledge\nabout its distribution or precise nature in the dataset. We use control regions\nas surrogates that carry information regarding the bias, employ the classifier\nmodel to extract features, and suppress biased intermediate features with our\ncustom, modular DecorreLayer. We evaluate our method on a dataset of 952 lung\ncomputed tomography scans by introducing simulated biases w.r.t. reconstruction\nkernel and noise level and propose including an adversarial test set in\nevaluations of bias reduction techniques. In a moderately sized model\narchitecture, applying the proposed method to learn from data exhibiting a\nstrong bias, it near-perfectly recovers the classification performance observed\nwhen training with corresponding unbiased data.",
    "descriptor": "",
    "authors": [
      "Simon Langer",
      "Oliver Taubmann",
      "Felix Denzinger",
      "Andreas Maier",
      "Alexander M\u00fchlberg"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13297"
  },
  {
    "id": "arXiv:2205.13303",
    "title": "Gaussian Universality of Linear Classifiers with Random Labels in  High-Dimension",
    "abstract": "While classical in many theoretical settings, the assumption of Gaussian\ni.i.d. inputs is often perceived as a strong limitation in the analysis of\nhigh-dimensional learning. In this study, we redeem this line of work in the\ncase of generalized linear classification with random labels. Our main\ncontribution is a rigorous proof that data coming from a range of generative\nmodels in high-dimensions have the same minimum training loss as Gaussian data\nwith corresponding data covariance. In particular, our theorem covers data\ncreated by an arbitrary mixture of homogeneous Gaussian clouds, as well as\nmulti-modal generative neural networks. In the limit of vanishing\nregularization, we further demonstrate that the training loss is independent of\nthe data covariance. Finally, we show that this universality property is\nobserved in practice with real datasets and random labels.",
    "descriptor": "",
    "authors": [
      "Federica Gerace",
      "Florent Krzakala",
      "Bruno Loureiro",
      "Ludovic Stephan",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13303"
  },
  {
    "id": "arXiv:2205.13325",
    "title": "Learning the spatio-temporal relationship between wind and significant  wave height using deep learning",
    "abstract": "Ocean wave climate has a significant impact on near-shore and off-shore human\nactivities, and its characterisation can help in the design of ocean structures\nsuch as wave energy converters and sea dikes. Therefore, engineers need long\ntime series of ocean wave parameters. Numerical models are a valuable source of\nocean wave data; however, they are computationally expensive. Consequently,\nstatistical and data-driven approaches have gained increasing interest in\nrecent decades. This work investigates the spatio-temporal relationship between\nNorth Atlantic wind and significant wave height (Hs) at an off-shore location\nin the Bay of Biscay, using a two-stage deep learning model. The first step\nuses convolutional neural networks (CNNs) to extract the spatial features that\ncontribute to Hs. Then, long short-term memory (LSTM) is used to learn the\nlong-term temporal dependencies between wind and waves.",
    "descriptor": "",
    "authors": [
      "Said Obakrim",
      "Val\u00e9rie Monbet",
      "Nicolas Raillard",
      "Pierre Ailliot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.13325"
  },
  {
    "id": "arXiv:2205.13409",
    "title": "On stochastic stabilization via non-smooth control Lyapunov functions",
    "abstract": "Control Lyapunov function is a central tool in stabilization. It generalizes\nan abstract energy function -- a Lyapunov function -- to the case of controlled\nsystems. It is a known fact that most control Lyapunov functions are non-smooth\n-- so is the case in non-holonomic systems, like wheeled robots and cars.\nFrameworks for stabilization using non-smooth control Lyapunov functions exist,\nlike Dini aiming and steepest descent. This work generalizes the related\nresults to the stochastic case. As the groundwork, sampled control scheme is\nchosen in which control actions are computed at discrete moments in time using\ndiscrete measurements of the system state. In such a setup, special attention\nshould be paid to the sample-to-sample behavior of the control Lyapunov\nfunction. A particular challenge here is a random noise acting on the system.\nThe central result of this work is a theorem that states, roughly, that if\nthere is a, generally non-smooth, control Lyapunov function, the given\nstochastic dynamical system can be practically stabilized in the\nsample-and-hold mode meaning that the control actions are held constant within\nsampling time steps. A particular control method chosen is based on\nMoreau-Yosida regularization, in other words, inf-convolution of the control\nLyapunov function, but the overall framework is extendable to further control\nschemes. It is assumed that the system noise be bounded almost surely, although\nthe case of unbounded noise is briefly addressed.",
    "descriptor": "\nComments: IEEE Transactions on Automatic Control\n",
    "authors": [
      "Pavel Osinenko",
      "Grigory Yaremenko",
      "Georgiy Malaniya"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.13409"
  },
  {
    "id": "arXiv:2205.13418",
    "title": "Avoiding Barren Plateaus with Classical Deep Neural Networks",
    "abstract": "Variational quantum algorithms (VQAs) are among the most promising algorithms\nin the era of Noisy Intermediate Scale Quantum Devices. The VQAs are applied to\na variety of tasks, such as in chemistry simulations, optimization problems,\nand quantum neural networks. Such algorithms are constructed using a\nparameterization U($\\pmb{\\theta}$) with a classical optimizer that updates the\nparameters $\\pmb{\\theta}$ in order to minimize a cost function $C$. For this\ntask, in general the gradient descent method, or one of its variants, is used.\nThis is a method where the circuit parameters are updated iteratively using the\ncost function gradient. However, several works in the literature have shown\nthat this method suffers from a phenomenon known as the Barren Plateaus (BP).\nThis phenomenon is characterized by the exponentially flattening of the cost\nfunction landscape, so that the number of times the function must be evaluated\nto perform the optimization grows exponentially as the number of qubits and\nparameterization depth increase. In this article, we report on how the use of a\nclassical neural networks in the VQAs input parameters can alleviate the BP\nphenomenon.",
    "descriptor": "",
    "authors": [
      "Lucas Friedrich",
      "Jonas Maziero"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13418"
  },
  {
    "id": "arXiv:2205.13461",
    "title": "Communicating with Anecdotes",
    "abstract": "We study a communication game between a sender and receiver where the sender\nhas access to a set of informative signals about a state of the world. The\nsender chooses one of her signals, called an ``anecdote'' and communicates it\nto the receiver. The receiver takes an action, yielding a utility for both\nplayers. Sender and receiver both care about the state of the world but are\nalso influenced by a personal preference so that their ideal actions differ. We\ncharacterize perfect Bayesian equilibria when the sender cannot commit to a\nparticular communication scheme. In this setting the sender faces ``persuasion\ntemptation'': she is tempted to select a more biased anecdote to influence the\nreceiver's action. Anecdotes are still informative to the receiver but\npersuasion comes at the cost of precision. This gives rise to ``informational\nhomophily'' where the receiver prefers to listen to like-minded senders because\nthey provide higher-precision signals. In particular, we show that a sender\nwith access to many anecdotes will essentially send the minimum or maximum\nanecdote even though with high probability she has access to an anecdote close\nto the state of the world that would almost perfectly reveal it to the\nreceiver. In contrast to the classic Crawford-Sobel model, full revelation is a\nknife-edge equilibrium and even small differences in personal preferences will\ninduce highly polarized communication and a loss in utility for any\nequilibrium. We show that for fat-tailed anecdote distributions the receiver\nmight even prefer to talk to poorly informed senders with aligned preferences\nrather than a knowledgeable expert whose preferences may differ from her own.\nWe also show that under commitment differences in personal preferences no\nlonger affect communication and the sender will generally report the most\nrepresentative anecdote closest to the posterior mean for common distributions.",
    "descriptor": "\nComments: A preliminary version of tis paper appeared under the title \"Persuading with Anecdotes\" as an NBER working paper\n",
    "authors": [
      "Nika Haghtalab",
      "Nicole Immorlica",
      "Brendan Lucier",
      "Markus Mobius",
      "Divyarthi Mohan"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13461"
  },
  {
    "id": "arXiv:2205.13493",
    "title": "Mesoscopic modeling of hidden spiking neurons",
    "abstract": "Can we use spiking neural networks (SNN) as generative models of\nmulti-neuronal recordings, while taking into account that most neurons are\nunobserved? Modeling the unobserved neurons with large pools of hidden spiking\nneurons leads to severely underconstrained problems that are hard to tackle\nwith maximum likelihood estimation. In this work, we use coarse-graining and\nmean-field approximations to derive a bottom-up, neuronally-grounded latent\nvariable model (neuLVM), where the activity of the unobserved neurons is\nreduced to a low-dimensional mesoscopic description. In contrast to previous\nlatent variable models, neuLVM can be explicitly mapped to a recurrent,\nmulti-population SNN, giving it a transparent biological interpretation. We\nshow, on synthetic spike trains, that a few observed neurons are sufficient for\nneuLVM to perform efficient model inversion of large SNNs, in the sense that it\ncan recover connectivity parameters, infer single-trial latent population\nactivity, reproduce ongoing metastable dynamics, and generalize when subjected\nto perturbations mimicking photo-stimulation.",
    "descriptor": "\nComments: 22 pages, 7 figures\n",
    "authors": [
      "Shuqi Wang",
      "Valentin Schmutz",
      "Guillaume Bellec",
      "Wulfram Gerstner"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13493"
  },
  {
    "id": "arXiv:2205.13496",
    "title": "Censored Quantile Regression Neural Networks",
    "abstract": "This paper considers doing quantile regression on censored data using neural\nnetworks (NNs). This adds to the survival analysis toolkit by allowing direct\nprediction of the target variable, along with a distribution-free\ncharacterisation of uncertainty, using a flexible function approximator. We\nbegin by showing how an algorithm popular in linear models can be applied to\nNNs. However, the resulting procedure is inefficient, requiring sequential\noptimisation of an individual NN at each desired quantile. Our major\ncontribution is a novel algorithm that simultaneously optimises a grid of\nquantiles output by a single NN. To offer theoretical insight into our\nalgorithm, we show firstly that it can be interpreted as a form of\nexpectation-maximisation, and secondly that it exhibits a desirable\n`self-correcting' property. Experimentally, the algorithm produces quantiles\nthat are better calibrated than existing methods on 10 out of 12 real datasets.",
    "descriptor": "",
    "authors": [
      "Tim Pearce",
      "Jong-Hyeon Jeong",
      "Yichen Jia",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13496"
  },
  {
    "id": "arXiv:2205.13519",
    "title": "Transfer learning driven design optimization for inertial confinement  fusion",
    "abstract": "Transfer learning is a promising approach to creating predictive models that\nincorporate simulation and experimental data into a common framework. In this\ntechnique, a neural network is first trained on a large database of\nsimulations, then partially retrained on sparse sets of experimental data to\nadjust predictions to be more consistent with reality. Previously, this\ntechnique has been used to create predictive models of Omega and NIF inertial\nconfinement fusion (ICF) experiments that are more accurate than simulations\nalone. In this work, we conduct a transfer learning driven hypothetical ICF\ncampaign in which the goal is to maximize experimental neutron yield via\nBayesian optimization. The transfer learning model achieves yields within 5% of\nthe maximum achievable yield in a modest-sized design space in fewer than 20\nexperiments. Furthermore, we demonstrate that this method is more efficient at\noptimizing designs than traditional model calibration techniques commonly\nemployed in ICF design. Such an approach to ICF design could enable robust\noptimization of experimental performance under uncertainty.",
    "descriptor": "",
    "authors": [
      "K. D. Humbird",
      "J. L. Peterson"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13519"
  },
  {
    "id": "arXiv:2205.13527",
    "title": "Subspace clustering in high-dimensions: Phase transitions \\&  Statistical-to-Computational gap",
    "abstract": "A simple model to study subspace clustering is the high-dimensional\n$k$-Gaussian mixture model where the cluster means are sparse vectors. Here we\nprovide an exact asymptotic characterization of the statistically optimal\nreconstruction error in this model in the high-dimensional regime with\nextensive sparsity, i.e. when the fraction of non-zero components of the\ncluster means $\\rho$, as well as the ratio $\\alpha$ between the number of\nsamples and the dimension are fixed, while the dimension diverges. We identify\nthe information-theoretic threshold below which obtaining a positive\ncorrelation with the true cluster means is statistically impossible.\nAdditionally, we investigate the performance of the approximate message passing\n(AMP) algorithm analyzed via its state evolution, which is conjectured to be\noptimal among polynomial algorithm for this task. We identify in particular the\nexistence of a statistical-to-computational gap between the algorithm that\nrequire a signal-to-noise ratio $\\lambda_{\\text{alg}} \\ge k / \\sqrt{\\alpha} $\nto perform better than random, and the information theoretic threshold at\n$\\lambda_{\\text{it}} \\approx \\sqrt{-k \\rho \\log{\\rho}} / \\sqrt{\\alpha}$.\nFinally, we discuss the case of sub-extensive sparsity $\\rho$ by comparing the\nperformance of the AMP with other sparsity-enhancing algorithms, such as\nsparse-PCA and diagonal thresholding.",
    "descriptor": "",
    "authors": [
      "Luca Pesce",
      "Bruno Loureiro",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13527"
  },
  {
    "id": "arXiv:2205.13539",
    "title": "Mitigating barren plateaus of variational quantum eigensolvers",
    "abstract": "Variational quantum algorithms (VQAs) are expected to establish valuable\napplications on near-term quantum computers. However, recent works have pointed\nout that the performance of VQAs greatly relies on the capability of the\nansatzes and is seriously limited by optimization issues such as barren\nplateaus (i.e., vanishing gradients). This work proposes the state efficient\nansatz (SEA) for accurate quantum dynamics simulations with improved\ntrainability. First, we show that SEA can generate an arbitrary pure state with\nmuch fewer parameters than a universal ansatz, making it efficient for tasks\nlike ground state estimation. It also has the flexibility in adjusting the\nentanglement of the prepared state, which could be applied to further improve\nthe efficiency of simulating weak entanglement. Second, we show that SEA is not\na unitary 2-design even if it has universal wavefunction expressibility and\nthus has great potential to improve the trainability by avoiding the zone of\nbarren plateaus. We further investigate a plethora of examples in ground state\nestimation and notably obtain significant improvements in the variances of\nderivatives and the overall optimization behaviors. This result indicates that\nSEA can mitigate barren plateaus by sacrificing the redundant expressibility\nfor the target problem.",
    "descriptor": "\nComments: 18 pages including appendix\n",
    "authors": [
      "Xia Liu",
      "Geng Liu",
      "Jiaxin Huang",
      "Xin Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13539"
  },
  {
    "id": "arXiv:1309.1555",
    "title": "A New Chase-type Soft-decision Decoding Algorithm for Reed-Solomon Codes",
    "abstract": "A New Chase-type Soft-decision Decoding Algorithm for Reed-Solomon Codes",
    "descriptor": "",
    "authors": [
      "Siyun Tang",
      "Suihua Cai",
      "Xiao Ma Member"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1309.1555"
  },
  {
    "id": "arXiv:1811.11664",
    "title": "Dynamic Competitive Persuasion",
    "abstract": "Dynamic Competitive Persuasion",
    "descriptor": "",
    "authors": [
      "Mark Whitmeyer"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Computer Science and Game Theory (cs.GT)",
      "General Economics (econ.GN)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/1811.11664"
  },
  {
    "id": "arXiv:1906.10019",
    "title": "Machine Learning Construction: implications to cybersecurity",
    "abstract": "Machine Learning Construction: implications to cybersecurity",
    "descriptor": "",
    "authors": [
      "Waleed A. Yousef"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1906.10019"
  },
  {
    "id": "arXiv:1907.12851",
    "title": "Machine Learning Assessment: implications to cybersecurity",
    "abstract": "Machine Learning Assessment: implications to cybersecurity",
    "descriptor": "",
    "authors": [
      "Waleed A. Yousef"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1907.12851"
  },
  {
    "id": "arXiv:1909.10958",
    "title": "On Communication Complexity of Fixed Point Computation",
    "abstract": "On Communication Complexity of Fixed Point Computation",
    "descriptor": "",
    "authors": [
      "Anat Ganor",
      "Karthik C. S.",
      "D\u00f6m\u00f6t\u00f6r P\u00e1lv\u00f6lgyi"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Computational Geometry (cs.CG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/1909.10958"
  },
  {
    "id": "arXiv:2006.12963",
    "title": "PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural  Networks Acceleration",
    "abstract": "PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural  Networks Acceleration",
    "descriptor": "",
    "authors": [
      "Jianrong Xu",
      "Boyu Diao",
      "Bifeng Cui",
      "Kang Yang",
      "Chao Li",
      "Yongjun Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2006.12963"
  },
  {
    "id": "arXiv:2007.08307",
    "title": "A Type Theory for Strictly Unital $\\infty$-Categories",
    "abstract": "Comments: 46 pages",
    "descriptor": "\nComments: 46 pages\n",
    "authors": [
      "Eric Finster",
      "David Reutter",
      "Alex Rice",
      "Jamie Vicary"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2007.08307"
  },
  {
    "id": "arXiv:2009.04742",
    "title": "Backtracking algorithms for constructing the Hamiltonian decomposition  of a 4-regular multigraph",
    "abstract": "Comments: English translation",
    "descriptor": "\nComments: English translation\n",
    "authors": [
      "Alexander V. Korostil",
      "Andrei V. Nikolaev"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2009.04742"
  },
  {
    "id": "arXiv:2010.05025",
    "title": "Historical Credibility for Movie Reviews and Its Application to Weakly  Supervised Classification",
    "abstract": "Historical Credibility for Movie Reviews and Its Application to Weakly  Supervised Classification",
    "descriptor": "",
    "authors": [
      "Min-Seon Kim",
      "Bo-Young Lim",
      "Han-Sub Shin",
      "Hyuk-Yoon Kwon"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2010.05025"
  },
  {
    "id": "arXiv:2010.09165",
    "title": "Optimal Descartes' Rule of Signs for Circuits",
    "abstract": "Comments: 21 pages, 5 figures. We improved the proof of Theorem 2.4 by adding Proposition 2.7",
    "descriptor": "\nComments: 21 pages, 5 figures. We improved the proof of Theorem 2.4 by adding Proposition 2.7\n",
    "authors": [
      "Fr\u00e9d\u00e9ric Bihan",
      "Alicia Dickenstein",
      "Jens Forsg\u00e5rd"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2010.09165"
  },
  {
    "id": "arXiv:2011.00464",
    "title": "What is the T-Algorithm? A case study to evaluate a new University",
    "abstract": "What is the T-Algorithm? A case study to evaluate a new University",
    "descriptor": "",
    "authors": [
      "Jose Berengueres"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2011.00464"
  },
  {
    "id": "arXiv:2011.13307",
    "title": "Polygon-free: Unconstrained Scene Text Detection with Box Annotations",
    "abstract": "Polygon-free: Unconstrained Scene Text Detection with Box Annotations",
    "descriptor": "",
    "authors": [
      "Weijia Wu",
      "Enze Xie",
      "Ruimao Zhang",
      "Wenhai Wang",
      "Hong Zhou",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.13307"
  },
  {
    "id": "arXiv:2011.15081",
    "title": "DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes",
    "abstract": "DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes",
    "descriptor": "",
    "authors": [
      "Albert Matveev",
      "Ruslan Rakhimov",
      "Alexey Artemov",
      "Gleb Bobrovskikh",
      "Vage Egiazarian",
      "Emil Bogomolov",
      "Daniele Panozzo",
      "Denis Zorin",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2011.15081"
  },
  {
    "id": "arXiv:2012.00278",
    "title": "Convergence analysis of a fully discrete energy-stable numerical scheme  for the Q-tensor flow of liquid crystals",
    "abstract": "Convergence analysis of a fully discrete energy-stable numerical scheme  for the Q-tensor flow of liquid crystals",
    "descriptor": "",
    "authors": [
      "Varun M. Gudibanda",
      "Franziska Weber",
      "Yukun Yue"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2012.00278"
  },
  {
    "id": "arXiv:2012.03384",
    "title": "Linear Reduced Order Model Predictive Control",
    "abstract": "Comments: This work has been submitted to the IEEE Transactions on Automatic Control for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE Transactions on Automatic Control for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Joseph Lorenzetti",
      "Andrew McClellan",
      "Charbel Farhat",
      "Marco Pavone"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2012.03384"
  },
  {
    "id": "arXiv:2012.15643",
    "title": "CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse  Relations",
    "abstract": "CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse  Relations",
    "descriptor": "",
    "authors": [
      "Changlong Yu",
      "Hongming Zhang",
      "Yangqiu Song",
      "Wilfred Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2012.15643"
  },
  {
    "id": "arXiv:2102.05013",
    "title": "Spherical Message Passing for 3D Graph Networks",
    "abstract": "Comments: The paper has been accepted by ICLR 2022. The updated title is \"Spherical Message Passing for 3D Molecular Graphs\". You can also cite the conference version",
    "descriptor": "\nComments: The paper has been accepted by ICLR 2022. The updated title is \"Spherical Message Passing for 3D Molecular Graphs\". You can also cite the conference version\n",
    "authors": [
      "Yi Liu",
      "Limei Wang",
      "Meng Liu",
      "Xuan Zhang",
      "Bora Oztekin",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05013"
  },
  {
    "id": "arXiv:2102.09717",
    "title": "Continual Learning for Blind Image Quality Assessment",
    "abstract": "Comments: Accepted to IEEE TPAMI",
    "descriptor": "\nComments: Accepted to IEEE TPAMI\n",
    "authors": [
      "Weixia Zhang",
      "Dingquan Li",
      "Chao Ma",
      "Guangtao Zhai",
      "Xiaokang Yang",
      "Kede Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2102.09717"
  },
  {
    "id": "arXiv:2103.00944",
    "title": "A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate  Spiking Neural Network from Convolutional Neural Network",
    "abstract": "A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate  Spiking Neural Network from Convolutional Neural Network",
    "descriptor": "",
    "authors": [
      "Dengyu Wu",
      "Xinping Yi",
      "Xiaowei Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.00944"
  },
  {
    "id": "arXiv:2103.11802",
    "title": "Forest Fire Clustering for Single-cell Sequencing with Iterative Label  Propagation and Parallelized Monte Carlo Simulation",
    "abstract": "Comments: 30 pages, 6 figures",
    "descriptor": "\nComments: 30 pages, 6 figures\n",
    "authors": [
      "Zhanlin Chen",
      "Jeremy Goldwasser",
      "Philip Tuckman",
      "Jason Liu",
      "Jing Zhang",
      "Mark Gerstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.11802"
  },
  {
    "id": "arXiv:2104.06591",
    "title": "Zero-Resource Multi-Dialectal Arabic Natural Language Understanding",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2101.04758",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2101.04758\n",
    "authors": [
      "Muhammad Khalifa",
      "Hesham Hassan",
      "Aly Fahmy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.06591"
  },
  {
    "id": "arXiv:2104.08570",
    "title": "Crossing the Conversational Chasm: A Primer on Natural Language  Processing for Multilingual Task-Oriented Dialogue Systems",
    "abstract": "Crossing the Conversational Chasm: A Primer on Natural Language  Processing for Multilingual Task-Oriented Dialogue Systems",
    "descriptor": "",
    "authors": [
      "Evgeniia Razumovskaia",
      "Goran Glava\u0161",
      "Olga Majewska",
      "Edoardo M. Ponti",
      "Anna Korhonen",
      "Ivan Vuli\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.08570"
  },
  {
    "id": "arXiv:2104.12717",
    "title": "TrustyAI Explainability Toolkit",
    "abstract": "TrustyAI Explainability Toolkit",
    "descriptor": "",
    "authors": [
      "Rob Geada",
      "Tommaso Teofili",
      "Rui Vieira",
      "Rebecca Whitworth",
      "Daniele Zonca"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.12717"
  },
  {
    "id": "arXiv:2104.14860",
    "title": "Summarization, Simplification, and Generation: The Case of Patents",
    "abstract": "Summarization, Simplification, and Generation: The Case of Patents",
    "descriptor": "",
    "authors": [
      "Silvia Casola",
      "Alberto Lavelli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.14860"
  },
  {
    "id": "arXiv:2104.15079",
    "title": "Ranking the information content of distance measures",
    "abstract": "Ranking the information content of distance measures",
    "descriptor": "",
    "authors": [
      "Aldo Glielmo",
      "Claudio Zeni",
      "Bingqing Cheng",
      "Gabor Csanyi",
      "Alessandro Laio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.15079"
  },
  {
    "id": "arXiv:2105.03266",
    "title": "Comparison of Traditional and Hybrid Time Series Models for Forecasting  COVID-19 Cases",
    "abstract": "Comparison of Traditional and Hybrid Time Series Models for Forecasting  COVID-19 Cases",
    "descriptor": "",
    "authors": [
      "Samyak Prajapati",
      "Aman Swaraj",
      "Ronak Lalwani",
      "Akhil Narwal",
      "Karan Verma"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03266"
  },
  {
    "id": "arXiv:2105.07459",
    "title": "Formal Security Analysis on dBFT Protocol of NEO",
    "abstract": "Comments: Extended version",
    "descriptor": "\nComments: Extended version\n",
    "authors": [
      "Qin Wang",
      "Rujia Li",
      "Shiping Chen",
      "Yang Xiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.07459"
  },
  {
    "id": "arXiv:2105.14108",
    "title": "Efficient and robust multi-task learning in the brain with modular  latent primitives",
    "abstract": "Comments: *Shared senior authorship",
    "descriptor": "\nComments: *Shared senior authorship\n",
    "authors": [
      "Christian David M\u00e1rton",
      "L\u00e9o Gagnon",
      "Guillaume Lajoie",
      "Kanaka Rajan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2105.14108"
  },
  {
    "id": "arXiv:2106.03374",
    "title": "MixR: Data Mixing Augmentation for Regression",
    "abstract": "Comments: 13 pages, 8 figures, 8 tables",
    "descriptor": "\nComments: 13 pages, 8 figures, 8 tables\n",
    "authors": [
      "Seong-Hyeon Hwang",
      "Steven Euijong Whang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03374"
  },
  {
    "id": "arXiv:2107.03996",
    "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with  Cross-Modal Transformers",
    "abstract": "Comments: Our project page with videos is at this https URL",
    "descriptor": "\nComments: Our project page with videos is at this https URL\n",
    "authors": [
      "Ruihan Yang",
      "Minghao Zhang",
      "Nicklas Hansen",
      "Huazhe Xu",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2107.03996"
  },
  {
    "id": "arXiv:2107.04519",
    "title": "Partial Matchings Induced by Morphisms between Persistence Modules",
    "abstract": "Partial Matchings Induced by Morphisms between Persistence Modules",
    "descriptor": "",
    "authors": [
      "R. Gonzalez-Diaz",
      "M. Soriano-Trigueros",
      "A. Torras-Casas"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2107.04519"
  },
  {
    "id": "arXiv:2107.04647",
    "title": "Global sensitivity analysis of asymmetric energy harvesters",
    "abstract": "Global sensitivity analysis of asymmetric energy harvesters",
    "descriptor": "",
    "authors": [
      "Jo\u00e3o Pedro Norenberg",
      "Americo Cunha Jr",
      "Samuel da Silva",
      "Paulo S\u00e9rgio Varoto"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Dynamical Systems (math.DS)",
      "Classical Physics (physics.class-ph)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2107.04647"
  },
  {
    "id": "arXiv:2107.07875",
    "title": "A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic  Treatment Regimens",
    "abstract": "A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic  Treatment Regimens",
    "descriptor": "",
    "authors": [
      "Palash Ghosh",
      "Trikay Nalamada",
      "Shruti Agarwal",
      "Maria Jahja",
      "Bibhas Chakraborty"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.07875"
  },
  {
    "id": "arXiv:2107.08924",
    "title": "Epistemic Neural Networks",
    "abstract": "Epistemic Neural Networks",
    "descriptor": "",
    "authors": [
      "Ian Osband",
      "Zheng Wen",
      "Seyed Mohammad Asghari",
      "Vikranth Dwaracherla",
      "Morteza Ibrahimi",
      "Xiuyuan Lu",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.08924"
  },
  {
    "id": "arXiv:2107.13721",
    "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$",
    "abstract": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$",
    "descriptor": "",
    "authors": [
      "Zhengwu Zhang",
      "Bayan Saparbayeva"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2107.13721"
  },
  {
    "id": "arXiv:2108.00505",
    "title": "DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in  Highways",
    "abstract": "Comments: Published in IEEE Transactions on Intelligent Transportation Systems",
    "descriptor": "\nComments: Published in IEEE Transactions on Intelligent Transportation Systems\n",
    "authors": [
      "Vinit Katariya",
      "Mohammadreza Baharani",
      "Nichole Morris",
      "Omidreza Shoghli",
      "Hamed Tabkhi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2108.00505"
  },
  {
    "id": "arXiv:2108.02848",
    "title": "Construction and application of provable positive and exact cubature  formulas",
    "abstract": "Comments: 33 pages",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Jan Glaubitz"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2108.02848"
  },
  {
    "id": "arXiv:2108.11486",
    "title": "\"Look! It's a Computer Program! It's an Algorithm! It's AI!\": Does  Terminology Affect Human Perceptions and Evaluations of Algorithmic  Decision-Making Systems?",
    "abstract": "Comments: Preregistrations for the studies included in this paper are available under this https URL and this https URL",
    "descriptor": "\nComments: Preregistrations for the studies included in this paper are available under this https URL and this https URL\n",
    "authors": [
      "Markus Langer",
      "Tim Hunsicker",
      "Tina Feldkamp",
      "Cornelius J. K\u00f6nig",
      "Nina Grgi\u0107-Hla\u010da"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2108.11486"
  },
  {
    "id": "arXiv:2108.11489",
    "title": "The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer  Linear Networks",
    "abstract": "The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer  Linear Networks",
    "descriptor": "",
    "authors": [
      "Niladri S. Chatterji",
      "Philip M. Long",
      "Peter L. Bartlett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2108.11489"
  },
  {
    "id": "arXiv:2108.12537",
    "title": "An Anytime Hierarchical Approach for Stochastic Task and Motion Planning",
    "abstract": "An Anytime Hierarchical Approach for Stochastic Task and Motion Planning",
    "descriptor": "",
    "authors": [
      "Naman Shah",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2108.12537"
  },
  {
    "id": "arXiv:2109.02427",
    "title": "A Markov chain approximation of switched Fokker-Planck equations for a  model of on-off intermittency in the postural control during quiet standing",
    "abstract": "Comments: 49 pages, 6 figures",
    "descriptor": "\nComments: 49 pages, 6 figures\n",
    "authors": [
      "Yasuyuki Suzuki",
      "Keigo Togame",
      "Akihiro Nakamura",
      "Taishin Nomura"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.02427"
  },
  {
    "id": "arXiv:2109.03780",
    "title": "Bayesian Over-The-Air Computation",
    "abstract": "Comments: Multi-tier computing, over-the-air computation, Bayesian estimation, sum-product algorithm. 17 pages, 11 figures. arXiv admin note: text overlap with arXiv:2102.13604",
    "descriptor": "\nComments: Multi-tier computing, over-the-air computation, Bayesian estimation, sum-product algorithm. 17 pages, 11 figures. arXiv admin note: text overlap with arXiv:2102.13604\n",
    "authors": [
      "Yulin Shao",
      "Deniz Gunduz",
      "Soung Chang Liew"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2109.03780"
  },
  {
    "id": "arXiv:2109.08745",
    "title": "Sublinear-Time Computation in the Presence of Online Erasures",
    "abstract": "Sublinear-Time Computation in the Presence of Online Erasures",
    "descriptor": "",
    "authors": [
      "Iden Kalemaj",
      "Sofya Raskhodnikova",
      "Nithin Varma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2109.08745"
  },
  {
    "id": "arXiv:2109.10705",
    "title": "On Crossing-Families in Planar Point Sets",
    "abstract": "On Crossing-Families in Planar Point Sets",
    "descriptor": "",
    "authors": [
      "Oswin Aichholzer",
      "Jan Kyn\u010dl",
      "Manfred Scheucher",
      "Birgit Vogtenhuber",
      "Pavel Valtr"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2109.10705"
  },
  {
    "id": "arXiv:2109.12485",
    "title": "On the convergence to local limit of nonlocal models with approximated  interaction neighborhoods",
    "abstract": "On the convergence to local limit of nonlocal models with approximated  interaction neighborhoods",
    "descriptor": "",
    "authors": [
      "Qiang Du",
      "Hehu Xie",
      "Xiaobo Yin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.12485"
  },
  {
    "id": "arXiv:2109.14026",
    "title": "Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual  Observations",
    "abstract": "Comments: Video summary can be found at this https URL",
    "descriptor": "\nComments: Video summary can be found at this https URL\n",
    "authors": [
      "Fernando Acero",
      "Kai Yuan",
      "Zhibin Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.14026"
  },
  {
    "id": "arXiv:2110.01167",
    "title": "Trustworthy AI: From Principles to Practices",
    "abstract": "Trustworthy AI: From Principles to Practices",
    "descriptor": "",
    "authors": [
      "Bo Li",
      "Peng Qi",
      "Bo Liu",
      "Shuai Di",
      "Jingen Liu",
      "Jiquan Pei",
      "Jinfeng Yi",
      "Bowen Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01167"
  },
  {
    "id": "arXiv:2110.04629",
    "title": "The Neural Testbed: Evaluating Joint Predictions",
    "abstract": "The Neural Testbed: Evaluating Joint Predictions",
    "descriptor": "",
    "authors": [
      "Ian Osband",
      "Zheng Wen",
      "Seyed Mohammad Asghari",
      "Vikranth Dwaracherla",
      "Botao Hao",
      "Morteza Ibrahimi",
      "Dieterich Lawson",
      "Xiuyuan Lu",
      "Brendan O'Donoghue",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.04629"
  },
  {
    "id": "arXiv:2110.07514",
    "title": "Advances in Scaling Community Discovery Methods for Signed Graph  Networks",
    "abstract": "Comments: 42 pages, 14 figures, 13 tables",
    "descriptor": "\nComments: 42 pages, 14 figures, 13 tables\n",
    "authors": [
      "Maria Tomasso",
      "Lucas Rusnak",
      "Jelena Te\u0161i\u0107"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.07514"
  },
  {
    "id": "arXiv:2110.08750",
    "title": "TIP: Task-Informed Motion Prediction for Intelligent Vehicles",
    "abstract": "Comments: 9 pages, 5 figures, 5 tables",
    "descriptor": "\nComments: 9 pages, 5 figures, 5 tables\n",
    "authors": [
      "Xin Huang",
      "Guy Rosman",
      "Ashkan Jasour",
      "Stephen G. McGill",
      "John J. Leonard",
      "Brian C. Williams"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08750"
  },
  {
    "id": "arXiv:2110.09022",
    "title": "Mitigating Memorization of Noisy Labels via Regularization between  Representations",
    "abstract": "Mitigating Memorization of Noisy Labels via Regularization between  Representations",
    "descriptor": "",
    "authors": [
      "Hao Cheng",
      "Zhaowei Zhu",
      "Xing Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.09022"
  },
  {
    "id": "arXiv:2110.09167",
    "title": "RKHS-SHAP: Shapley Values for Kernel Methods",
    "abstract": "RKHS-SHAP: Shapley Values for Kernel Methods",
    "descriptor": "",
    "authors": [
      "Siu Lun Chau",
      "Robert Hu",
      "Javier Gonzalez",
      "Dino Sejdinovic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.09167"
  },
  {
    "id": "arXiv:2110.12087",
    "title": "Gaussian Process Sampling and Optimization with Approximate Upper and  Lower Bounds",
    "abstract": "Comments: 21 pages",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Vu Nguyen",
      "Marc Peter Deisenroth",
      "Michael A. Osborne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.12087"
  },
  {
    "id": "arXiv:2111.00743",
    "title": "Towards the Generalization of Contrastive Self-Supervised Learning",
    "abstract": "Towards the Generalization of Contrastive Self-Supervised Learning",
    "descriptor": "",
    "authors": [
      "Weiran Huang",
      "Mingyang Yi",
      "Xuyang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.00743"
  },
  {
    "id": "arXiv:2111.03199",
    "title": "Concurrent multiscale analysis without meshing: Microscale  representation with CutFEM and micro/macro model blending",
    "abstract": "Concurrent multiscale analysis without meshing: Microscale  representation with CutFEM and micro/macro model blending",
    "descriptor": "",
    "authors": [
      "Ehsan Mikaeili",
      "Susanne Claus",
      "Pierre Kerfriden"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2111.03199"
  },
  {
    "id": "arXiv:2111.03343",
    "title": "Lattice Packings of Cross-polytopes from Reed-Solomon Codes and Sidon  Sets",
    "abstract": "Comments: 7 pages. v2: Section 2 and the discussion after the proof of Theorem 3.1 are added, v3: minor changes. To appear in the Bulletin of the London Mathematical Society",
    "descriptor": "\nComments: 7 pages. v2: Section 2 and the discussion after the proof of Theorem 3.1 are added, v3: minor changes. To appear in the Bulletin of the London Mathematical Society\n",
    "authors": [
      "Mladen Kova\u010devi\u0107"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Information Theory (cs.IT)",
      "Metric Geometry (math.MG)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2111.03343"
  },
  {
    "id": "arXiv:2111.03394",
    "title": "Coherent Probabilistic Aggregate Queries on Long-horizon Forecasts",
    "abstract": "Comments: 7 pages, 1 figure, 1 table, 1 algorithm",
    "descriptor": "\nComments: 7 pages, 1 figure, 1 table, 1 algorithm\n",
    "authors": [
      "Prathamesh Deshpande",
      "Sunita Sarawagi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.03394"
  },
  {
    "id": "arXiv:2111.04699",
    "title": "Automated pharyngeal phase detection and bolus localization in  videofluoroscopic swallowing study: Killing two birds with one stone?",
    "abstract": "Automated pharyngeal phase detection and bolus localization in  videofluoroscopic swallowing study: Killing two birds with one stone?",
    "descriptor": "",
    "authors": [
      "Andrea Bandini",
      "Sana Smaoui",
      "Catriona M. Steele"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.04699"
  },
  {
    "id": "arXiv:2111.07183",
    "title": "Reliably-stabilizing piecewise-affine neural network controllers",
    "abstract": "Reliably-stabilizing piecewise-affine neural network controllers",
    "descriptor": "",
    "authors": [
      "Filippo Fabiani",
      "Paul J. Goulart"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2111.07183"
  },
  {
    "id": "arXiv:2111.07695",
    "title": "Joint Synthesis of Safety Certificate and Safe Control Policy using  Constrained Reinforcement Learning",
    "abstract": "Comments: 24 pages, 8 figures, accepted for oral presentation at L4DC 2022",
    "descriptor": "\nComments: 24 pages, 8 figures, accepted for oral presentation at L4DC 2022\n",
    "authors": [
      "Haitong Ma",
      "Changliu Liu",
      "Shengbo Eben Li",
      "Sifa Zheng",
      "Jianyu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.07695"
  },
  {
    "id": "arXiv:2111.09046",
    "title": "Motion Planning of Multi-Robots Object Transport with Deformable Sheet",
    "abstract": "Comments: 8 pages, 10 figures, a Submission for RAL and CASE 2022",
    "descriptor": "\nComments: 8 pages, 10 figures, a Submission for RAL and CASE 2022\n",
    "authors": [
      "Jiawei Hu",
      "Wenhang Liu",
      "Heng Zhang",
      "Jingang Yi",
      "Zhenhua Xiong"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2111.09046"
  },
  {
    "id": "arXiv:2111.12292",
    "title": "Improved Fine-Tuning by Better Leveraging Pre-Training Data",
    "abstract": "Improved Fine-Tuning by Better Leveraging Pre-Training Data",
    "descriptor": "",
    "authors": [
      "Ziquan Liu",
      "Yi Xu",
      "Yuanhong Xu",
      "Qi Qian",
      "Hao Li",
      "Xiangyang Ji",
      "Antoni Chan",
      "Rong Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.12292"
  },
  {
    "id": "arXiv:2111.13401",
    "title": "A Learned-SVD approach for Regularization in Diffuse Optical Tomography",
    "abstract": "A Learned-SVD approach for Regularization in Diffuse Optical Tomography",
    "descriptor": "",
    "authors": [
      "Alessandro Benfenati",
      "Giuseppe Bisazza",
      "Paola Causin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.13401"
  },
  {
    "id": "arXiv:2111.14655",
    "title": "FedHM: Efficient Federated Learning for Heterogeneous Models via  Low-rank Factorization",
    "abstract": "FedHM: Efficient Federated Learning for Heterogeneous Models via  Low-rank Factorization",
    "descriptor": "",
    "authors": [
      "Dezhong Yao",
      "Wanning Pan",
      "Michael J O'Neill",
      "Yutong Dai",
      "Yao Wan",
      "Hai Jin",
      "Lichao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2111.14655"
  },
  {
    "id": "arXiv:2112.00573",
    "title": "Power law decay at criticality for the q-state antiferromagnetic Potts  model on regular trees",
    "abstract": "Comments: 22 pages, 1 figure",
    "descriptor": "\nComments: 22 pages, 1 figure\n",
    "authors": [
      "Chenlin Gu",
      "Wei Wu",
      "Kuan Yang"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Discrete Mathematics (cs.DM)",
      "Mathematical Physics (math-ph)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2112.00573"
  },
  {
    "id": "arXiv:2112.00673",
    "title": "Robustly Self-Ordered Graphs: Constructions and Applications to Property  Testing",
    "abstract": "Comments: Slightly modified and revised version of a CCC 2021 paper that also appeared on ECCC 27: 149 (2020)",
    "descriptor": "\nComments: Slightly modified and revised version of a CCC 2021 paper that also appeared on ECCC 27: 149 (2020)\n",
    "authors": [
      "Oded Goldreich",
      "Avi Wigderson"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2112.00673"
  },
  {
    "id": "arXiv:2112.02157",
    "title": "A Web of Confocal Parabolas in a Grid of Hexagons",
    "abstract": "Comments: 18 pages, 21 figures, 3 video links",
    "descriptor": "\nComments: 18 pages, 21 figures, 3 video links\n",
    "authors": [
      "Peter Moses",
      "Dan Reznik"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)",
      "Graphics (cs.GR)",
      "Robotics (cs.RO)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2112.02157"
  },
  {
    "id": "arXiv:2112.02478",
    "title": "Classification of COVID-19 on chest X-Ray images using Deep Learning  model with Histogram Equalization and Lungs Segmentation",
    "abstract": "Comments: Total number of words of the manuscript- 6577 The number of words of the abstract- 238 The number of figures- 8 The number of tables- 10",
    "descriptor": "\nComments: Total number of words of the manuscript- 6577 The number of words of the abstract- 238 The number of figures- 8 The number of tables- 10\n",
    "authors": [
      "Hitendra Singh Bhadouria",
      "Krishan Kumar",
      "Aman Swaraj",
      "Karan Verma"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.02478"
  },
  {
    "id": "arXiv:2112.04684",
    "title": "Trajectory-Constrained Deep Latent Visual Attention for Improved Local  Planning in Presence of Heterogeneous Terrain",
    "abstract": "Comments: Published in International Conference on Intelligent Robots and Systems (IROS) 2021 proceedings. Project website: this https URL",
    "descriptor": "\nComments: Published in International Conference on Intelligent Robots and Systems (IROS) 2021 proceedings. Project website: this https URL\n",
    "authors": [
      "Stefan Wapnick",
      "Travis Manderson",
      "David Meger",
      "Gregory Dudek"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.04684"
  },
  {
    "id": "arXiv:2112.07995",
    "title": "Domain-informed neural networks for interaction localization within  astroparticle experiments",
    "abstract": "Comments: 18 pages, 10 figures. Updated to the published version",
    "descriptor": "\nComments: 18 pages, 10 figures. Updated to the published version\n",
    "authors": [
      "Shixiao Liang",
      "Aaron Higuera",
      "Christina Peters",
      "Venkat Roy",
      "Waheed U. Bajwa",
      "Hagit Shatkay",
      "Christopher D. Tunnell"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.07995"
  },
  {
    "id": "arXiv:2112.09118",
    "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "abstract": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Gautier Izacard",
      "Mathilde Caron",
      "Lucas Hosseini",
      "Sebastian Riedel",
      "Piotr Bojanowski",
      "Armand Joulin",
      "Edouard Grave"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.09118"
  },
  {
    "id": "arXiv:2112.10961",
    "title": "Nonlinear Transform Source-Channel Coding for Semantic Communications",
    "abstract": "Comments: To appear in IEEE JSAC Series on Machine Learning in Communications and Networks",
    "descriptor": "\nComments: To appear in IEEE JSAC Series on Machine Learning in Communications and Networks\n",
    "authors": [
      "Jincheng Dai",
      "Sixian Wang",
      "Kailin Tan",
      "Zhongwei Si",
      "Xiaoqi Qin",
      "Kai Niu",
      "Ping Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.10961"
  },
  {
    "id": "arXiv:2112.14135",
    "title": "Empowering Base Stations with Co-Site Intelligent Reflecting Surfaces:  User Association, Channel Estimation and Reflection Optimization",
    "abstract": "Comments: Accepted by IEEE Transactions on Communications",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Communications\n",
    "authors": [
      "Yuwei Huang",
      "Weidong Mei",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.14135"
  },
  {
    "id": "arXiv:2112.15085",
    "title": "Feature Extraction, Classification and Prediction for Hand Hygiene  Gestures with KNN Algorithm",
    "abstract": "Feature Extraction, Classification and Prediction for Hand Hygiene  Gestures with KNN Algorithm",
    "descriptor": "",
    "authors": [
      "Rashmi Bakshi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.15085"
  },
  {
    "id": "arXiv:2201.00393",
    "title": "ros2_tracing: Multipurpose Low-Overhead Framework for Real-Time Tracing  of ROS 2",
    "abstract": "Comments: 8 pages, 8 figures, 3 tables",
    "descriptor": "\nComments: 8 pages, 8 figures, 3 tables\n",
    "authors": [
      "Christophe B\u00e9dard",
      "Ingo L\u00fctkebohle",
      "Michel Dagenais"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.00393"
  },
  {
    "id": "arXiv:2201.01080",
    "title": "Towards Understanding and Harnessing the Effect of Image Transformation  in Adversarial Detection",
    "abstract": "Towards Understanding and Harnessing the Effect of Image Transformation  in Adversarial Detection",
    "descriptor": "",
    "authors": [
      "Hui Liu",
      "Bo Zhao",
      "Yuefeng Peng",
      "Weidong Li",
      "Peng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.01080"
  },
  {
    "id": "arXiv:2201.01601",
    "title": "FedBalancer: Data and Pace Control for Efficient Federated Learning on  Heterogeneous Clients",
    "abstract": "Comments: Accepted to the 20th ACM International Conference on Mobile Systems, Applications, and Services (MobiSys 2022)",
    "descriptor": "\nComments: Accepted to the 20th ACM International Conference on Mobile Systems, Applications, and Services (MobiSys 2022)\n",
    "authors": [
      "Jaemin Shin",
      "Yuanchun Li",
      "Yunxin Liu",
      "Sung-Ju Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.01601"
  },
  {
    "id": "arXiv:2201.02311",
    "title": "Joint Routing and Charging Problem of Electric Vehicles with  Incentive-aware Customers Considering Spatio-temporal Charging Prices",
    "abstract": "Comments: Submitted to TRC",
    "descriptor": "\nComments: Submitted to TRC\n",
    "authors": [
      "Canqi Yao",
      "Shibo Chen",
      "Mauro Salazar",
      "Zaiyue Yang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.02311"
  },
  {
    "id": "arXiv:2201.02867",
    "title": "Deep Generative Modeling for Volume Reconstruction in Cryo-Electron  Microscopy",
    "abstract": "Deep Generative Modeling for Volume Reconstruction in Cryo-Electron  Microscopy",
    "descriptor": "",
    "authors": [
      "Claire Donnat",
      "Axel Levy",
      "Frederic Poitevin",
      "Ellen Zhong",
      "Nina Miolane"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2201.02867"
  },
  {
    "id": "arXiv:2201.06993",
    "title": "Spiker: an FPGA-optimized Hardware acceleration for Spiking Neural  Networks",
    "abstract": "Comments: 6 pages, 3 figures, 4 tables",
    "descriptor": "\nComments: 6 pages, 3 figures, 4 tables\n",
    "authors": [
      "Alessio Carpegna",
      "Alessandro Savino",
      "Stefano Di Carlo"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.06993"
  },
  {
    "id": "arXiv:2201.07598",
    "title": "Near-Optimal Sparse Allreduce for Distributed Deep Learning",
    "abstract": "Comments: Published in Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'22), April 2-6, 2022, Pages 135-149, this https URL",
    "descriptor": "\nComments: Published in Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'22), April 2-6, 2022, Pages 135-149, this https URL\n",
    "authors": [
      "Shigang Li",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.07598"
  },
  {
    "id": "arXiv:2201.08054",
    "title": "VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine  Translation",
    "abstract": "Comments: Accepted by LREC2022",
    "descriptor": "\nComments: Accepted by LREC2022\n",
    "authors": [
      "Yihang Li",
      "Shuichiro Shimizu",
      "Weiqi Gu",
      "Chenhui Chu",
      "Sadao Kurohashi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.08054"
  },
  {
    "id": "arXiv:2201.08770",
    "title": "Evaluating Generalization in Classical and Quantum Generative Models",
    "abstract": "Comments: 27 pages, 17 figures. The main text and appendix have been revised and expanded. We include more details about the numerical simulations and the trained generative models. Also, we added new results comparing the TNBM and GAN models against a random sampling model baseline",
    "descriptor": "\nComments: 27 pages, 17 figures. The main text and appendix have been revised and expanded. We include more details about the numerical simulations and the trained generative models. Also, we added new results comparing the TNBM and GAN models against a random sampling model baseline\n",
    "authors": [
      "Kaitlin Gili",
      "Marta Mauri",
      "Alejandro Perdomo-Ortiz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.08770"
  },
  {
    "id": "arXiv:2201.09113",
    "title": "Predicting Physics in Mesh-reduced Space with Temporal Attention",
    "abstract": "Predicting Physics in Mesh-reduced Space with Temporal Attention",
    "descriptor": "",
    "authors": [
      "Xu Han",
      "Han Gao",
      "Tobias Pfaff",
      "Jian-Xun Wang",
      "Li-Ping Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.09113"
  },
  {
    "id": "arXiv:2201.09490",
    "title": "Dual Preference Distribution Learning for Item Recommendation",
    "abstract": "Comments: 11 pages, 5 figures. This manuscript has been submitted to ACM TOIS",
    "descriptor": "\nComments: 11 pages, 5 figures. This manuscript has been submitted to ACM TOIS\n",
    "authors": [
      "Xue Dong",
      "Xuemeng Song",
      "Na Zheng",
      "Yinwei Wei",
      "Zhongzhou Zhao",
      "Hongjun Dai"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.09490"
  },
  {
    "id": "arXiv:2201.12096",
    "title": "Mask-based Latent Reconstruction for Reinforcement Learning",
    "abstract": "Comments: A latent-space masked modeling method for improving RL sample efficiency",
    "descriptor": "\nComments: A latent-space masked modeling method for improving RL sample efficiency\n",
    "authors": [
      "Tao Yu",
      "Zhizheng Zhang",
      "Cuiling Lan",
      "Yan Lu",
      "Zhibo Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12096"
  },
  {
    "id": "arXiv:2201.12151",
    "title": "Unsupervised Learning From Incomplete Measurements for Inverse Problems",
    "abstract": "Unsupervised Learning From Incomplete Measurements for Inverse Problems",
    "descriptor": "",
    "authors": [
      "Juli\u00e1n Tachella",
      "Dongdong Chen",
      "Mike Davies"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12151"
  },
  {
    "id": "arXiv:2201.12825",
    "title": "Lorentzian Fully Hyperbolic Generative Adversarial Network",
    "abstract": "Lorentzian Fully Hyperbolic Generative Adversarial Network",
    "descriptor": "",
    "authors": [
      "Eric Qu",
      "Dongmian Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12825"
  },
  {
    "id": "arXiv:2201.13008",
    "title": "Communication-Efficient Distributed Multiple Testing for Large-Scale  Inference",
    "abstract": "Comments: Accepted to the 2022 IEEE International Symposium on Information Theory (ISIT)",
    "descriptor": "\nComments: Accepted to the 2022 IEEE International Symposium on Information Theory (ISIT)\n",
    "authors": [
      "Mehrdad Pournaderi",
      "Yu Xiang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.13008"
  },
  {
    "id": "arXiv:2202.00645",
    "title": "Generalization Analysis of Message Passing Neural Networks on Large  Random Graphs",
    "abstract": "Comments: Preprint in Review",
    "descriptor": "\nComments: Preprint in Review\n",
    "authors": [
      "Sohir Maskey",
      "Ron Levie",
      "Yunseok Lee",
      "Gitta Kutyniok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2202.00645"
  },
  {
    "id": "arXiv:2202.00762",
    "title": "Extending FEniCS to Work in Higher Dimensions Using Tensor Product  Finite Elements",
    "abstract": "Extending FEniCS to Work in Higher Dimensions Using Tensor Product  Finite Elements",
    "descriptor": "",
    "authors": [
      "Mark Loveland",
      "Eirik Valseth",
      "Matt Lukac",
      "Clint Dawson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2202.00762"
  },
  {
    "id": "arXiv:2202.04064",
    "title": "Parallel Contests for Crowdsourcing Reviews: Existence and Quality of  Equilibria",
    "abstract": "Parallel Contests for Crowdsourcing Reviews: Existence and Quality of  Equilibria",
    "descriptor": "",
    "authors": [
      "Georgios Birmpas",
      "Lyudmila Kovalchuk",
      "Philip Lazos",
      "Roman Oliynykov"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2202.04064"
  },
  {
    "id": "arXiv:2202.04541",
    "title": "Sparse superposition codes under VAMP decoding with generic rotational  invariant coding matrices",
    "abstract": "Comments: Submitted to the 2022 IEEE International Symposium on Information Theory (ISIT)",
    "descriptor": "\nComments: Submitted to the 2022 IEEE International Symposium on Information Theory (ISIT)\n",
    "authors": [
      "TianQi Hou",
      "YuHao Liu",
      "Teng Fu",
      "Jean Barbier"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)"
    ],
    "url": "https://arxiv.org/abs/2202.04541"
  },
  {
    "id": "arXiv:2202.04598",
    "title": "Reproducibility in Optimization: Theoretical Framework and Limits",
    "abstract": "Comments: 51 pages; Version 2 improved presentation and polished writing",
    "descriptor": "\nComments: 51 pages; Version 2 improved presentation and polished writing\n",
    "authors": [
      "Kwangjun Ahn",
      "Prateek Jain",
      "Ziwei Ji",
      "Satyen Kale",
      "Praneeth Netrapalli",
      "Gil I. Shamir"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04598"
  },
  {
    "id": "arXiv:2202.04977",
    "title": "Needs-aware Artificial Intelligence: AI that 'serves [human] needs'",
    "abstract": "Comments: 3-10-2022 Reference #6 updates with arXiv link, 5-15-22 final version for publication in AI & Ethics",
    "descriptor": "\nComments: 3-10-2022 Reference #6 updates with arXiv link, 5-15-22 final version for publication in AI & Ethics\n",
    "authors": [
      "Ryan Watkins",
      "Soheil Human"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.04977"
  },
  {
    "id": "arXiv:2202.05240",
    "title": "ChemicalX: A Deep Learning Library for Drug Pair Scoring",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Benedek Rozemberczki",
      "Charles Tapley Hoyt",
      "Anna Gogleva",
      "Piotr Grabowski",
      "Klas Karis",
      "Andrej Lamov",
      "Andriy Nikolov",
      "Sebastian Nilsson",
      "Michael Ughetto",
      "Yu Wang",
      "Tyler Derr",
      "Benjamin M Gyori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.05240"
  },
  {
    "id": "arXiv:2202.05564",
    "title": "A Partial Reciprocity-based Channel Prediction Framework for FDD Massive  MIMO with High Mobility",
    "abstract": "Comments: 9 figures, 15 pages, to appear in IEEE Transactions on Wireless Communications",
    "descriptor": "\nComments: 9 figures, 15 pages, to appear in IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Ziao Qin",
      "Haifan Yin",
      "Yandi Cao",
      "Weidong Li",
      "David Gesbert"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2202.05564"
  },
  {
    "id": "arXiv:2202.05594",
    "title": "The Shapley Value in Machine Learning",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Benedek Rozemberczki",
      "Lauren Watson",
      "P\u00e9ter Bayer",
      "Hao-Tsung Yang",
      "Oliv\u00e9r Kiss",
      "Sebastian Nilsson",
      "Rik Sarkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2202.05594"
  },
  {
    "id": "arXiv:2202.06775",
    "title": "A structure-preserving finite element approximation of surface diffusion  for curve networks and surface clusters",
    "abstract": "Comments: 29 figures",
    "descriptor": "\nComments: 29 figures\n",
    "authors": [
      "Weizhu Bao",
      "Harald Garcke",
      "Robert N\u00fcrnberg",
      "Quan Zhao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2202.06775"
  },
  {
    "id": "arXiv:2202.09745",
    "title": "RDP-Net: Region Detail Preserving Network for Change Detection",
    "abstract": "Comments: 9 pages, 10 figures, 46 references",
    "descriptor": "\nComments: 9 pages, 10 figures, 46 references\n",
    "authors": [
      "Hongjia Chen",
      "Fangling Pu",
      "Rui Yang",
      "Rui Tang",
      "Xin Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.09745"
  },
  {
    "id": "arXiv:2202.10295",
    "title": "Single-Query Verifiable Proof-of-Sequential-Work",
    "abstract": "Single-Query Verifiable Proof-of-Sequential-Work",
    "descriptor": "",
    "authors": [
      "Souvik Sur"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2202.10295"
  },
  {
    "id": "arXiv:2202.10894",
    "title": "Quantum Internet Protocol Stack: a Comprehensive Survey",
    "abstract": "Quantum Internet Protocol Stack: a Comprehensive Survey",
    "descriptor": "",
    "authors": [
      "Jessica Illiano",
      "Marcello Caleffi",
      "Antonio Manzalini",
      "Angela Sara Cacciapuoti"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2202.10894"
  },
  {
    "id": "arXiv:2202.10970",
    "title": "$\\mathbf{VDF} = \\mathbf{PSPACE}$",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2112.05997",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.05997\n",
    "authors": [
      "Souvik Sur"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2202.10970"
  },
  {
    "id": "arXiv:2202.12116",
    "title": "Motion-driven Visual Tempo Learning for Video-based Action Recognition",
    "abstract": "Comments: Accpted by IEEE Transactions on Image Processing(TIP), 2022",
    "descriptor": "\nComments: Accpted by IEEE Transactions on Image Processing(TIP), 2022\n",
    "authors": [
      "Yuanzhong Liu",
      "Junsong Yuan",
      "Zhigang Tu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.12116"
  },
  {
    "id": "arXiv:2202.12595",
    "title": "Evolutionary scheduling of university activities based on consumption  forecasts to minimise electricity costs",
    "abstract": "Comments: Accepted to the 2022 IEEE Congress on Evolutionary Computation",
    "descriptor": "\nComments: Accepted to the 2022 IEEE Congress on Evolutionary Computation\n",
    "authors": [
      "Julian Ruddick",
      "Evgenii Genov",
      "Luis Ramirez Camargo",
      "Thierry Coosemans",
      "Maarten Messagie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2202.12595"
  },
  {
    "id": "arXiv:2202.12916",
    "title": "Incremental Inference on Higher-Order Probabilistic Graphical Models  Applied to Constraint Satisfaction Problems",
    "abstract": "Comments: PhD thesis, Stellenbosch University, 2022",
    "descriptor": "\nComments: PhD thesis, Stellenbosch University, 2022\n",
    "authors": [
      "Simon Streicher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12916"
  },
  {
    "id": "arXiv:2203.00523",
    "title": "Towards Creativity Characterization of Generative Models via Group-based  Subset Scanning",
    "abstract": "Comments: Accepted to IJCAI 2022 - Creativity Track - Extended version from Synthetic Data Generation Workshop at ICLR'21 submission (arXiv:2104.00479). arXiv admin note: text overlap with arXiv:2105.12479",
    "descriptor": "\nComments: Accepted to IJCAI 2022 - Creativity Track - Extended version from Synthetic Data Generation Workshop at ICLR'21 submission (arXiv:2104.00479). arXiv admin note: text overlap with arXiv:2105.12479\n",
    "authors": [
      "Celia Cintas",
      "Payel Das",
      "Brian Quanz",
      "Girmaw Abebe Tadesse",
      "Skyler Speakman",
      "Pin-Yu Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.00523"
  },
  {
    "id": "arXiv:2203.01012",
    "title": "Continual Feature Selection: Spurious Features in Continual Learning",
    "abstract": "Continual Feature Selection: Spurious Features in Continual Learning",
    "descriptor": "",
    "authors": [
      "Timoth\u00e9e Lesort"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.01012"
  },
  {
    "id": "arXiv:2203.05291",
    "title": "On Robustness in Optimization-Based Constrained Iterative Learning  Control",
    "abstract": "On Robustness in Optimization-Based Constrained Iterative Learning  Control",
    "descriptor": "",
    "authors": [
      "Dominic Liao-McPherson",
      "Efe C. Balta",
      "Alisa Rupenyan",
      "John Lygeros"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.05291"
  },
  {
    "id": "arXiv:2203.06749",
    "title": "Decontextualized I3D ConvNet for ultra-distance runners performance  analysis at a glance",
    "abstract": "Comments: Accepted at 21st International Conference on Image Analysis and Processing (ICIAP 2021)",
    "descriptor": "\nComments: Accepted at 21st International Conference on Image Analysis and Processing (ICIAP 2021)\n",
    "authors": [
      "David Freire-Obreg\u00f3n",
      "Javier Lorenzo-Navarro",
      "Modesto Castrill\u00f3n-Santana"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.06749"
  },
  {
    "id": "arXiv:2203.06791",
    "title": "HDPView: Differentially Private Materialized View for Exploring High  Dimensional Relational Data",
    "abstract": "Comments: accepted at VLDB 2022",
    "descriptor": "\nComments: accepted at VLDB 2022\n",
    "authors": [
      "Fumiyuki Kato",
      "Tsubasa Takahashi",
      "Shun Takagi",
      "Yang Cao",
      "Seng Pei Liew",
      "Masatoshi Yoshikawa"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2203.06791"
  },
  {
    "id": "arXiv:2203.10477",
    "title": "A Non-iterative Overlapping Schwarz Waveform Relaxation Algorithm for  Wave Equation",
    "abstract": "Comments: 12 pages, 4 figures, submitted",
    "descriptor": "\nComments: 12 pages, 4 figures, submitted\n",
    "authors": [
      "Fei Wei",
      "Anna Zhao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2203.10477"
  },
  {
    "id": "arXiv:2203.16325",
    "title": "A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification",
    "abstract": "A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification",
    "descriptor": "",
    "authors": [
      "Jingjun Yi",
      "Beichen Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.16325"
  },
  {
    "id": "arXiv:2203.16408",
    "title": "Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker  SVS by Learning from Singing Teacher",
    "abstract": "Comments: Submitted to INTERSPEECH 2022",
    "descriptor": "\nComments: Submitted to INTERSPEECH 2022\n",
    "authors": [
      "Heyang Xue",
      "Xinsheng Wang",
      "Yongmao Zhang",
      "Lei Xie",
      "Pengcheng Zhu",
      "Mengxiao Bi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.16408"
  },
  {
    "id": "arXiv:2203.16432",
    "title": "Long-term Dynamics of Fairness Intervention in Connection Recommender  Systems",
    "abstract": "Comments: Conference on Artificial Intelligence, Ethics, and Society (AIES 2022)",
    "descriptor": "\nComments: Conference on Artificial Intelligence, Ethics, and Society (AIES 2022)\n",
    "authors": [
      "Nil-Jana Akpinar",
      "Cyrus DiCiccio",
      "Preetam Nandy",
      "Kinjal Basu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2203.16432"
  },
  {
    "id": "arXiv:2204.00746",
    "title": "What to look at and where: Semantic and Spatial Refined Transformer for  detecting human-object interactions",
    "abstract": "Comments: CVPR 2022 Oral",
    "descriptor": "\nComments: CVPR 2022 Oral\n",
    "authors": [
      "A S M Iftekhar",
      "Hao Chen",
      "Kaustav Kundu",
      "Xinyu Li",
      "Joseph Tighe",
      "Davide Modolo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.00746"
  },
  {
    "id": "arXiv:2204.00747",
    "title": "RFID-Based Indoor Spatial Query Evaluation with Bayesian Filtering  Techniques",
    "abstract": "RFID-Based Indoor Spatial Query Evaluation with Bayesian Filtering  Techniques",
    "descriptor": "",
    "authors": [
      "Bo Hui",
      "Wenlu Wang",
      "Jiao Yu",
      "Zhitao Gong",
      "Wei-Shinn Ku",
      "Min-Te Sun",
      "Hua Lu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.00747"
  },
  {
    "id": "arXiv:2204.01705",
    "title": "Learning to Accelerate by the Methods of Step-size Planning",
    "abstract": "Learning to Accelerate by the Methods of Step-size Planning",
    "descriptor": "",
    "authors": [
      "Hengshuai Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2204.01705"
  },
  {
    "id": "arXiv:2204.02559",
    "title": "X-CAR: An Experimental Vehicle Platform for Connected Autonomy Research  Powered by CARMA",
    "abstract": "Comments: \\copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "descriptor": "\nComments: \\copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Goodarz Mehr",
      "Prasenjit Ghorai",
      "Ce Zhang",
      "Anshul Nayak",
      "Darshit Patel",
      "Shathushan Sivashangaran",
      "Azim Eskandarian"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.02559"
  },
  {
    "id": "arXiv:2204.03394",
    "title": "Towards Comparing Performance of Algorithms in Hardware and Software",
    "abstract": "Towards Comparing Performance of Algorithms in Hardware and Software",
    "descriptor": "",
    "authors": [
      "Maja H. Kirkeby",
      "Martin Schoeberl"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2204.03394"
  },
  {
    "id": "arXiv:2204.04773",
    "title": "Worst-case Performance of Greedy Policies in Bandits with Imperfect  Context Observations",
    "abstract": "Comments: 14 pages, 2figures",
    "descriptor": "\nComments: 14 pages, 2figures\n",
    "authors": [
      "Hongju Park",
      "Mohamad Kazem Shirani Faradonbeh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.04773"
  },
  {
    "id": "arXiv:2204.05184",
    "title": "Domain Adversarial Graph Convolutional Network Based on RSSI and  Crowdsensing for Indoor Localization",
    "abstract": "Domain Adversarial Graph Convolutional Network Based on RSSI and  Crowdsensing for Indoor Localization",
    "descriptor": "",
    "authors": [
      "Mingxin Zhang",
      "Zipei Fan",
      "Ryosuke Shibasaki",
      "Xuan Song"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2204.05184"
  },
  {
    "id": "arXiv:2204.09280",
    "title": "Reinforced Structured State-Evolution for Vision-Language Navigation",
    "abstract": "Comments: Accepted to CVPR 2022",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Jinyu Chen",
      "Chen Gao",
      "Erli Meng",
      "Qiong Zhang",
      "Si Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.09280"
  },
  {
    "id": "arXiv:2204.11032",
    "title": "Heterogeneous Separation Consistency Training for Adaptation of  Unsupervised Speech Separation",
    "abstract": "Heterogeneous Separation Consistency Training for Adaptation of  Unsupervised Speech Separation",
    "descriptor": "",
    "authors": [
      "Jiangyu Han",
      "Yanhua Long"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.11032"
  },
  {
    "id": "arXiv:2205.00583",
    "title": "Accelerated Algorithms for a Class of Optimization Problems with  Constraints",
    "abstract": "Comments: 6 pages",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Anjali Parashar",
      "Priyank Srivastava",
      "Anuradha M. Annaswamy"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.00583"
  },
  {
    "id": "arXiv:2205.00690",
    "title": "From Noisy Prediction to True Label: Noisy Prediction Calibration via  Generative Model",
    "abstract": "Comments: 21 pages, 9 figures. International Conference on Machine Learning (ICML 2022), Baltimore, Jul 17, 2022",
    "descriptor": "\nComments: 21 pages, 9 figures. International Conference on Machine Learning (ICML 2022), Baltimore, Jul 17, 2022\n",
    "authors": [
      "HeeSun Bae",
      "Seungjae Shin",
      "Byeonghu Na",
      "JoonHo Jang",
      "Kyungwoo Song",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00690"
  },
  {
    "id": "arXiv:2205.01411",
    "title": "On the Utility of Prediction Sets in Human-AI Teams",
    "abstract": "Comments: Accepted at IJCAI 2022",
    "descriptor": "\nComments: Accepted at IJCAI 2022\n",
    "authors": [
      "Varun Babbar",
      "Umang Bhatt",
      "Adrian Weller"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.01411"
  },
  {
    "id": "arXiv:2205.02584",
    "title": "Study on the ERP Implementation Methodologies on SAP, Oracle NetSuite,  and Microsoft Dynamics 365: A Review",
    "abstract": "Study on the ERP Implementation Methodologies on SAP, Oracle NetSuite,  and Microsoft Dynamics 365: A Review",
    "descriptor": "",
    "authors": [
      "Madabattula Archana",
      "Dr VijayaKumar Varadarajan",
      "Sai Sravan Medicherla"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.02584"
  },
  {
    "id": "arXiv:2205.04262",
    "title": "Discontinuous Galerkin approximation of the fully-coupled  thermo-poroelastic problem",
    "abstract": "Discontinuous Galerkin approximation of the fully-coupled  thermo-poroelastic problem",
    "descriptor": "",
    "authors": [
      "Paola F. Antonietti",
      "Stefano Bonetti",
      "Michele Botti"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.04262"
  },
  {
    "id": "arXiv:2205.04464",
    "title": "Differentiable Electron Microscopy Simulation: Methods and Applications  for Visualization",
    "abstract": "Comments: Version 2: Page 10: Fix the rendering problem in in Line 12 of Algorithm 2 Page 12: Table 2: Fix wrong data entries in the table",
    "descriptor": "\nComments: Version 2: Page 10: Fix the rendering problem in in Line 12 of Algorithm 2 Page 12: Table 2: Fix wrong data entries in the table\n",
    "authors": [
      "Ngan Nguyen",
      "Feng Liang",
      "Dominik Engel",
      "Ciril Bohak",
      "Peter Wonka",
      "Timo Ropinski",
      "Ivan Viola"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.04464"
  },
  {
    "id": "arXiv:2205.04596",
    "title": "When does dough become a bagel? Analyzing the remaining mistakes on  ImageNet",
    "abstract": "Comments: Data and analysis available at this https URL",
    "descriptor": "\nComments: Data and analysis available at this https URL\n",
    "authors": [
      "Vijay Vasudevan",
      "Benjamin Caine",
      "Raphael Gontijo-Lopes",
      "Sara Fridovich-Keil",
      "Rebecca Roelofs"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.04596"
  },
  {
    "id": "arXiv:2205.05800",
    "title": "Stochastic first-order methods for average-reward Markov decision  processes",
    "abstract": "Stochastic first-order methods for average-reward Markov decision  processes",
    "descriptor": "",
    "authors": [
      "Tianjiao Li",
      "Feiyang Wu",
      "Guanghui Lan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.05800"
  },
  {
    "id": "arXiv:2205.06457",
    "title": "ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language  Generation",
    "abstract": "Comments: NAACL SRW 2022. arXiv admin note: text overlap with arXiv:2110.04257",
    "descriptor": "\nComments: NAACL SRW 2022. arXiv admin note: text overlap with arXiv:2110.04257\n",
    "authors": [
      "Long Phan",
      "Hieu Tran",
      "Hieu Nguyen",
      "Trieu H. Trinh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.06457"
  },
  {
    "id": "arXiv:2205.06515",
    "title": "An Information-theoretic Method for Collaborative Distributed Learning  with Limited Communication",
    "abstract": "An Information-theoretic Method for Collaborative Distributed Learning  with Limited Communication",
    "descriptor": "",
    "authors": [
      "Xinyi Tong",
      "Jian Xu",
      "Shao-Lun Huang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.06515"
  },
  {
    "id": "arXiv:2205.06523",
    "title": "Deterministic Identification over Channels without CSI",
    "abstract": "Deterministic Identification over Channels without CSI",
    "descriptor": "",
    "authors": [
      "Yuan Li",
      "Xianbin Wang",
      "Huazi Zhang",
      "Jun Wang",
      "Wen Tong",
      "Guiying Yan",
      "Zhiming Ma"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.06523"
  },
  {
    "id": "arXiv:2205.06854",
    "title": "An Approach for Automatic Construction of an Algorithmic Knowledge Graph  from Textual Resources",
    "abstract": "Comments: 12 pages, 7 figures, 2 tables",
    "descriptor": "\nComments: 12 pages, 7 figures, 2 tables\n",
    "authors": [
      "Jyotima Patel",
      "Biswanath Dutta"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.06854"
  },
  {
    "id": "arXiv:2205.06918",
    "title": "Representation learning with function call graph transformations for  malware open set recognition",
    "abstract": "Representation learning with function call graph transformations for  malware open set recognition",
    "descriptor": "",
    "authors": [
      "Jingyun Jia",
      "Philip K. Chan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.06918"
  },
  {
    "id": "arXiv:2205.07159",
    "title": "Exponentially Stable Observer-based Controller for VTOL-UAVs without  Velocity Measurements",
    "abstract": "Exponentially Stable Observer-based Controller for VTOL-UAVs without  Velocity Measurements",
    "descriptor": "",
    "authors": [
      "Hashim A. Hashim"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.07159"
  },
  {
    "id": "arXiv:2205.07178",
    "title": "Optimal Congestion-aware Routing and Offloading in Collaborative Edge  Computing",
    "abstract": "Comments: Withdrawal because this version is meant to be a replacement (revised version) of another submitted paper(2205.00714). Will submit this version as a replacement for 2205.00714",
    "descriptor": "\nComments: Withdrawal because this version is meant to be a replacement (revised version) of another submitted paper(2205.00714). Will submit this version as a replacement for 2205.00714\n",
    "authors": [
      "Jinkun Zhang",
      "Yuezhou Liu",
      "Edmund Yeh"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.07178"
  },
  {
    "id": "arXiv:2205.07208",
    "title": "Fine-tuning Pre-trained Language Models for Few-shot Intent Detection:  Supervised Pre-training and Isotropization",
    "abstract": "Comments: NAACL 2022",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Haode Zhang",
      "Haowen Liang",
      "Yuwei Zhang",
      "Liming Zhan",
      "Xiao-Ming Wu",
      "Xiaolei Lu",
      "Albert Y.S. Lam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.07208"
  },
  {
    "id": "arXiv:2205.07292",
    "title": "A Computational Framework of Cortical Microcircuits Approximates  Sign-concordant Random Backpropagation",
    "abstract": "A Computational Framework of Cortical Microcircuits Approximates  Sign-concordant Random Backpropagation",
    "descriptor": "",
    "authors": [
      "Yukun Yang",
      "Peng Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.07292"
  },
  {
    "id": "arXiv:2205.07410",
    "title": "TNN7: A Custom Macro Suite for Implementing Highly Optimized Designs of  Neuromorphic TNNs",
    "abstract": "Comments: To be published in ISVLSI 2022",
    "descriptor": "\nComments: To be published in ISVLSI 2022\n",
    "authors": [
      "Harideep Nair",
      "Prabhu Vellaisamy",
      "Santha Bhasuthkar",
      "John Paul Shen"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.07410"
  },
  {
    "id": "arXiv:2205.08712",
    "title": "CARNet: A Dynamic Autoencoder for Learning Latent Dynamics in Autonomous  Driving Tasks",
    "abstract": "Comments: 13 pages, 14 figures, 8 tables, removed submission info, bios",
    "descriptor": "\nComments: 13 pages, 14 figures, 8 tables, removed submission info, bios\n",
    "authors": [
      "Andrey Pak",
      "Hemanth Manjunatha",
      "Dimitar Filev",
      "Panagiotis Tsiotras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.08712"
  },
  {
    "id": "arXiv:2205.09348",
    "title": "Analyzing Echo-state Networks Using Fractal Dimension",
    "abstract": "Comments: IEEE WCCI 2022, Padua. Copyright is with IEEE",
    "descriptor": "\nComments: IEEE WCCI 2022, Padua. Copyright is with IEEE\n",
    "authors": [
      "Norbert Michael Mayer",
      "Oliver Obst"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.09348"
  },
  {
    "id": "arXiv:2205.09534",
    "title": "IFTT-PIN: A PIN-Entry Method Leveraging the Self-Calibration Paradigm",
    "abstract": "IFTT-PIN: A PIN-Entry Method Leveraging the Self-Calibration Paradigm",
    "descriptor": "",
    "authors": [
      "Jonathan Grizou"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09534"
  },
  {
    "id": "arXiv:2205.10517",
    "title": "Pre-training Data Quality and Quantity for a Low-Resource Language: New  Corpus and BERT Models for Maltese",
    "abstract": "Comments: DeepLo 2022 camera-ready version",
    "descriptor": "\nComments: DeepLo 2022 camera-ready version\n",
    "authors": [
      "Kurt Micallef",
      "Albert Gatt",
      "Marc Tanti",
      "Lonneke van der Plas",
      "Claudia Borg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10517"
  },
  {
    "id": "arXiv:2205.10724",
    "title": "Learnable Visual Words for Interpretable Image Recognition",
    "abstract": "Learnable Visual Words for Interpretable Image Recognition",
    "descriptor": "",
    "authors": [
      "Wenxiao Xiao",
      "Zhengming Ding",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10724"
  },
  {
    "id": "arXiv:2205.10760",
    "title": "CNNs are Myopic",
    "abstract": "Comments: Added ablation study in appendix",
    "descriptor": "\nComments: Added ablation study in appendix\n",
    "authors": [
      "Vamshi C. Madala",
      "Shivkumar Chandrasekaran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10760"
  },
  {
    "id": "arXiv:2205.10890",
    "title": "Nonparametric likelihood-free inference with Jensen-Shannon divergence  for simulator-based models with categorical output",
    "abstract": "Comments: 61 pages, 14 figures",
    "descriptor": "\nComments: 61 pages, 14 figures\n",
    "authors": [
      "Jukka Corander",
      "Ulpu Remes",
      "Ida Holopainen",
      "Timo Koski"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10890"
  },
  {
    "id": "arXiv:2205.10955",
    "title": "Investigating classification learning curves for automatically generated  and labelled plant images",
    "abstract": "Investigating classification learning curves for automatically generated  and labelled plant images",
    "descriptor": "",
    "authors": [
      "Michael A. Beck",
      "Christopher P. Bidinosti",
      "Christopher J. Henry",
      "Manisha Ajmani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10955"
  },
  {
    "id": "arXiv:2205.10956",
    "title": "CIRCLE: Continual Repair across Programming Languages",
    "abstract": "Comments: This paper was accepted by ISSTA2022",
    "descriptor": "\nComments: This paper was accepted by ISSTA2022\n",
    "authors": [
      "Wei Yuan",
      "Quanjun Zhang",
      "Tieke He",
      "Chunrong Fang",
      "Nguyen Quoc Viet Hung",
      "Xiaodong Hao",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10956"
  },
  {
    "id": "arXiv:2205.10968",
    "title": "Eigenvalue bounds of the Kirchhoff Laplacian",
    "abstract": "Comments: 9 pages, Added more references, a figure and reorganize",
    "descriptor": "\nComments: 9 pages, Added more references, a figure and reorganize\n",
    "authors": [
      "Oliver Knill"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.10968"
  },
  {
    "id": "arXiv:2205.11264",
    "title": "Adaptive Fairness-Aware Online Meta-Learning for Changing Environments",
    "abstract": "Comments: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2022. arXiv admin note: text overlap with arXiv:2108.09435",
    "descriptor": "\nComments: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2022. arXiv admin note: text overlap with arXiv:2108.09435\n",
    "authors": [
      "Chen Zhao",
      "Feng Mi",
      "Xintao Wu",
      "Kai Jiang",
      "Latifur Khan",
      "Feng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11264"
  },
  {
    "id": "arXiv:2205.11397",
    "title": "Super Vision Transformer",
    "abstract": "Super Vision Transformer",
    "descriptor": "",
    "authors": [
      "Mingbao Lin",
      "Mengzhao Chen",
      "Yuxin Zhang",
      "Ke Li",
      "Yunhang Shen",
      "Chunhua Shen",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11397"
  },
  {
    "id": "arXiv:2205.11508",
    "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods",
    "abstract": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods",
    "descriptor": "",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Spectral Theory (math.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11508"
  },
  {
    "id": "arXiv:2205.11736",
    "title": "Towards a Defense against Backdoor Attacks in Continual Federated  Learning",
    "abstract": "Towards a Defense against Backdoor Attacks in Continual Federated  Learning",
    "descriptor": "",
    "authors": [
      "Shuaiqi Wang",
      "Jonathan Hayase",
      "Giulia Fanti",
      "Sewoong Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11736"
  },
  {
    "id": "arXiv:2205.11738",
    "title": "Adaptive Few-Shot Learning Algorithm for Rare Sound Event Detection",
    "abstract": "Comments: Accepted to IJCNN 2022",
    "descriptor": "\nComments: Accepted to IJCNN 2022\n",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Leilai Li",
      "Xiaoyang Qu",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.11738"
  },
  {
    "id": "arXiv:2205.11900",
    "title": "Flying-Qubit Control via a Three-level Atom with Tunable Waveguide  Couplings",
    "abstract": "Comments: 13 pages, 10 figures",
    "descriptor": "\nComments: 13 pages, 10 figures\n",
    "authors": [
      "Wenlong Li",
      "Xue Dong",
      "Guofeng Zhang",
      "Re-Bing Wu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11900"
  },
  {
    "id": "arXiv:2205.12022",
    "title": "Improving Human Image Synthesis with Residual Fast Fourier  Transformation and Wasserstein Distance",
    "abstract": "Comments: This paper is accepted by IJCNN2022",
    "descriptor": "\nComments: This paper is accepted by IJCNN2022\n",
    "authors": [
      "Jianhan Wu",
      "Shijing Si",
      "Jianzong Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12022"
  },
  {
    "id": "arXiv:2205.12098",
    "title": "COVID-19: An exploration of consecutive systemic barriers to  pathogen-related data sharing during a pandemic",
    "abstract": "Comments: 30 pages including references, no figures. To be submitted to Data and Policy",
    "descriptor": "\nComments: 30 pages including references, no figures. To be submitted to Data and Policy\n",
    "authors": [
      "Yo Yehudi",
      "Lukas Hughes-Noehrer",
      "Carole Goble",
      "Caroline Jay"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.12098"
  },
  {
    "id": "arXiv:2205.12318",
    "title": "ColdGuess: A General and Effective Relational Graph Convolutional  Network to Tackle Cold Start Cases",
    "abstract": "ColdGuess: A General and Effective Relational Graph Convolutional  Network to Tackle Cold Start Cases",
    "descriptor": "",
    "authors": [
      "Bo He",
      "Xiang Song",
      "Vincent Gao",
      "Christos Faloutsos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12318"
  },
  {
    "id": "arXiv:2205.12331",
    "title": "Certified Robustness Against Natural Language Attacks by Causal  Intervention",
    "abstract": "Certified Robustness Against Natural Language Attacks by Causal  Intervention",
    "descriptor": "",
    "authors": [
      "Haiteng Zhao",
      "Chang Ma*",
      "Xinshuai Dong",
      "Anh Tuan Luu",
      "Zhi-Hong Deng",
      "Hanwang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12331"
  },
  {
    "id": "arXiv:2205.12354",
    "title": "Optimal Entanglement Distribution using Satellite Based Quantum Networks",
    "abstract": "Optimal Entanglement Distribution using Satellite Based Quantum Networks",
    "descriptor": "",
    "authors": [
      "Nitish K. Panigrahy",
      "Prajit Dhara",
      "Don Towsley",
      "Saikat Guha",
      "Leandros Tassiulas"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12354"
  },
  {
    "id": "arXiv:2205.12377",
    "title": "Hardness of Maximum Likelihood Learning of DPPs",
    "abstract": "Hardness of Maximum Likelihood Learning of DPPs",
    "descriptor": "",
    "authors": [
      "Elena Grigorescu",
      "Brendan Juba",
      "Karl Wimmer",
      "Ning Xie"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12377"
  },
  {
    "id": "arXiv:2205.12408",
    "title": "Using user's local context to support local news",
    "abstract": "Using user's local context to support local news",
    "descriptor": "",
    "authors": [
      "Payam Pourashraf",
      "Bamshad Mobasher"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12408"
  },
  {
    "id": "arXiv:2205.12461",
    "title": "Augmentation-induced Consistency Regularization for Classification",
    "abstract": "Comments: This paper is accepted by IJCNN2022",
    "descriptor": "\nComments: This paper is accepted by IJCNN2022\n",
    "authors": [
      "Jianhan Wu",
      "Shijing Si",
      "Jianzong Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12461"
  },
  {
    "id": "arXiv:2205.12493",
    "title": "Federated Self-supervised Learning for Heterogeneous Clients",
    "abstract": "Federated Self-supervised Learning for Heterogeneous Clients",
    "descriptor": "",
    "authors": [
      "Disha Makhija",
      "Nhat Ho",
      "Joydeep Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12493"
  },
  {
    "id": "arXiv:2205.12635",
    "title": "MoCoViT: Mobile Convolutional Vision Transformer",
    "abstract": "Comments: After evaluation, the relevant technical details are temporarily inconvenient to be disclosed, so the manuscript is temporarily withdrawn. We will wait for the right time to reopen",
    "descriptor": "\nComments: After evaluation, the relevant technical details are temporarily inconvenient to be disclosed, so the manuscript is temporarily withdrawn. We will wait for the right time to reopen\n",
    "authors": [
      "Hailong Ma",
      "Xin Xia",
      "Xing Wang",
      "Xuefeng Xiao",
      "Jiashi Li",
      "Min Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12635"
  },
  {
    "id": "arXiv:2205.12646",
    "title": "UniInst: Unique Representation for End-to-End Instance Segmentation",
    "abstract": "Comments: This work is in the revision phase of the journal Neurocomputing. Codes will be available upon publication",
    "descriptor": "\nComments: This work is in the revision phase of the journal Neurocomputing. Codes will be available upon publication\n",
    "authors": [
      "Yimin Ou",
      "Rui Yang",
      "Lufan Ma",
      "Yong Liu",
      "Jiangpeng Yan",
      "Shang Xu",
      "Chengjie Wang",
      "Xiu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12646"
  },
  {
    "id": "arXiv:2205.12665",
    "title": "QAMPARI: : An Open-domain Question Answering Benchmark for Questions  with Many Answers from Multiple Paragraphs",
    "abstract": "QAMPARI: : An Open-domain Question Answering Benchmark for Questions  with Many Answers from Multiple Paragraphs",
    "descriptor": "",
    "authors": [
      "Samuel Joseph Amouyal",
      "Ohad Rubin",
      "Ori Yoran",
      "Tomer Wolfson",
      "Jonathan Herzig",
      "Jonathan Berant"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12665"
  },
  {
    "id": "arXiv:2205.12857",
    "title": "Structure Unbiased Adversarial Model for Medical Image Segmentation",
    "abstract": "Comments: Will revise the paper and resubmit",
    "descriptor": "\nComments: Will revise the paper and resubmit\n",
    "authors": [
      "Tianyang Zhang",
      "Shaoming Zheng",
      "Jun Cheng",
      "Xi Jia",
      "Joseph Bartlett",
      "Huazhu Fu",
      "Zhaowen Qiu",
      "Jiang Liu",
      "Jinming Duan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12857"
  },
  {
    "id": "arXiv:2205.12940",
    "title": "Conformal Prediction Intervals with Temporal Dependence",
    "abstract": "Comments: 15 pages (main paper, including references) + 4 pages (supplementary material)",
    "descriptor": "\nComments: 15 pages (main paper, including references) + 4 pages (supplementary material)\n",
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.12940"
  },
  {
    "id": "arXiv:2205.12956",
    "title": "Inception Transformer",
    "abstract": "Comments: Code and models will be released at this https URL",
    "descriptor": "\nComments: Code and models will be released at this https URL\n",
    "authors": [
      "Chenyang Si",
      "Weihao Yu",
      "Pan Zhou",
      "Yichen Zhou",
      "Xinchao Wang",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12956"
  }
]