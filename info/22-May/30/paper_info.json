[
  {
    "id": "arXiv:2205.13554",
    "title": "Training and Inference on Any-Order Autoregressive Models the Right Way",
    "abstract": "Conditional inference on arbitrary subsets of variables is a core problem in\nprobabilistic inference with important applications such as masked language\nmodeling and image inpainting. In recent years, the family of Any-Order\nAutoregressive Models (AO-ARMs) -- which includes popular models such as XLNet\n-- has shown breakthrough performance in arbitrary conditional tasks across a\nsweeping range of domains. But, in spite of their success, in this paper we\nidentify significant improvements to be made to previous formulations of\nAO-ARMs. First, we show that AO-ARMs suffer from redundancy in their\nprobabilistic model, i.e., they define the same distribution in multiple\ndifferent ways. We alleviate this redundancy by training on a smaller set of\nunivariate conditionals that still maintains support for efficient arbitrary\nconditional inference. Second, we upweight the training loss for univariate\nconditionals that are evaluated more frequently during inference. Our method\nleads to improved performance with no compromises on tractability, giving\nstate-of-the-art likelihoods in arbitrary conditional modeling on text (Text8),\nimage (CIFAR10, ImageNet32), and continuous tabular data domains.",
    "descriptor": "",
    "authors": [
      "Andy Shih",
      "Dorsa Sadigh",
      "Stefano Ermon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13554"
  },
  {
    "id": "arXiv:2205.13559",
    "title": "HashPIM: High-Throughput SHA-3 via Memristive Digital  Processing-in-Memory",
    "abstract": "Recent research has sought to accelerate cryptographic hash functions as they\nare at the core of modern cryptography. Traditional designs, however, suffer\nfrom the von Neumann bottleneck that originates from the separation of\nprocessing and memory units. An emerging solution to overcome this bottleneck\nis processing-in-memory (PIM): performing logic within the same devices\nresponsible for memory to eliminate data-transfer and simultaneously provide\nmassive computational parallelism. In this paper, we seek to vastly accelerate\nthe state-of-the-art SHA-3 cryptographic function using the memristive memory\nprocessing unit (mMPU), a general-purpose memristive PIM architecture. To that\nend, we propose a novel in-memory algorithm for variable rotation, and utilize\nan efficient mapping of the SHA-3 state vector for memristive crossbar arrays\nto efficiently exploit PIM parallelism. We demonstrate a massive energy\nefficiency of 1,422 Gbps/W, improving a state-of-the-art memristive SHA-3\naccelerator (SHINE-2) by 4.6x.",
    "descriptor": "\nComments: Accepted to International Conference on Modern Circuits and Systems Technologies (MOCAST) 2022\n",
    "authors": [
      "Batel Oved",
      "Orian Leitersdorf",
      "Ronny Ronen",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13559"
  },
  {
    "id": "arXiv:2205.13561",
    "title": "Physics-Guided Hierarchical Reward Mechanism for LearningBased  Multi-Finger Object Grasping",
    "abstract": "Autonomous grasping is challenging due to the high computational cost caused\nby multi-fingered robotic hands and their interactions with objects. Various\nanalytical methods have been developed yet their high computational cost limits\nthe adoption in real-world applications. Learning-based grasping can afford\nreal-time motion planning thanks to its high computational efficiency. However,\nit needs to explore large search spaces during its learning process. The search\nspace causes low learning efficiency, which has been the main barrier to its\npractical adoption. In this work, we develop a novel Physics-Guided Deep\nReinforcement Learning with a Hierarchical Reward Mechanism, which combines the\nbenefits of both analytical methods and learning-based methods for autonomous\ngrasping. Different from conventional observation-based grasp learning,\nphysics-informed metrics are utilized to convey correlations between features\nassociated with hand structures and objects to improve learning efficiency and\nlearning outcomes. Further, a hierarchical reward mechanism is developed to\nenable the robot to learn the grasping task in a prioritized way. It is\nvalidated in a grasping task with a MICO robot arm in simulation and physical\nexperiments. The results show that our method outperformed the baseline in task\nperformance by 48% and learning efficiency by 40%.",
    "descriptor": "",
    "authors": [
      "Yunsik Jung",
      "Lingfeng Tao",
      "Michael Bowman",
      "Jiucai Zhang",
      "Xiaoli Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13561"
  },
  {
    "id": "arXiv:2205.13562",
    "title": "Analysis of a Direct Separation Method Based on Adaptive Chirplet  Transform for Signals with Crossover Instantaneous Frequencies",
    "abstract": "In many applications, it is necessary to retrieve the sub-signal building\nblocks of a multi-component signal, which is usually non-stationary in\nreal-world and real-life applications. Empirical mode decomposition (EMD),\nsynchrosqueezing transform (SST), signal separation operation (SSO), and\niterative filtering decomposition (IFD) have been proposed and developed for\nthis purpose. However, these computational methods are restricted by the\nspecification of well-separation of the sub-signal frequency curves for\nmulti-component signals. On the other hand, the chirplet transform-based signal\nseparation scheme (CT3S) that extends SSO from the two-dimensional\n\"time-frequency\" plane to the three-dimensional \"time-frequency-chirp rate\"\nspace was recently proposed in our recent work to remove the\nfrequency-separation specification, and thereby allowing \"frequency crossing\".\nThe main objective of this present paper is to carry out an in-depth error\nanalysis study of instantaneous frequency estimation and component recovery for\nthe CT3S method.",
    "descriptor": "",
    "authors": [
      "Charles K. Chui",
      "Qingtang Jiang",
      "Lin Li",
      "Jian Lu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13562"
  },
  {
    "id": "arXiv:2205.13565",
    "title": "Unequal Covariance Awareness for Fisher Discriminant Analysis and Its  Variants in Classification",
    "abstract": "Fisher Discriminant Analysis (FDA) is one of the essential tools for feature\nextraction and classification. In addition, it motivates the development of\nmany improved techniques based on the FDA to adapt to different problems or\ndata types. However, none of these approaches make use of the fact that the\nassumption of equal covariance matrices in FDA is usually not satisfied in\npractical situations. Therefore, we propose a novel classification rule for the\nFDA that accounts for this fact, mitigating the effect of unequal covariance\nmatrices in the FDA. Furthermore, since we only modify the classification rule,\nthe same can be applied to many FDA variants, improving these algorithms\nfurther. Theoretical analysis reveals that the new classification rule allows\nthe implicit use of the class covariance matrices while increasing the number\nof parameters to be estimated by a small amount compared to going from FDA to\nQuadratic Discriminant Analysis. We illustrate our idea via experiments, which\nshow the superior performance of the modified algorithms based on our new\nclassification rule compared to the original ones.",
    "descriptor": "",
    "authors": [
      "Thu Nguyen",
      "Quang M. Le",
      "Son N.T. Tu",
      "Binh T. Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13565"
  },
  {
    "id": "arXiv:2205.13566",
    "title": "Exploration, Exploitation, and Engagement in Multi-Armed Bandits with  Abandonment",
    "abstract": "Multi-armed bandit (MAB) is a classic model for understanding the\nexploration-exploitation trade-off. The traditional MAB model for\nrecommendation systems assumes the user stays in the system for the entire\nlearning horizon. In new online education platforms such as ALEKS or new video\nrecommendation systems such as TikTok and YouTube Shorts, the amount of time a\nuser spends on the app depends on how engaging the recommended contents are.\nUsers may temporarily leave the system if the recommended items cannot engage\nthe users. To understand the exploration, exploitation, and engagement in these\nsystems, we propose a new model, called MAB-A where \"A\" stands for abandonment\nand the abandonment probability depends on the current recommended item and the\nuser's past experience (called state). We propose two algorithms, ULCB and\nKL-ULCB, both of which do more exploration (being optimistic) when the user\nlikes the previous recommended item and less exploration (being pessimistic)\nwhen the user does not like the previous item. We prove that both ULCB and\nKL-ULCB achieve logarithmic regret, $O(\\log K)$, where $K$ is the number of\nvisits (or episodes). Furthermore, the regret bound under KL-ULCB is\nasymptotically sharp. We also extend the proposed algorithms to the\ngeneral-state setting. Simulation results confirm our theoretical analysis and\nshow that the proposed algorithms have significantly lower regrets than the\ntraditional UCB and KL-UCB, and Q-learning-based algorithms.",
    "descriptor": "",
    "authors": [
      "Zixian Yang",
      "Xin Liu",
      "Lei Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13566"
  },
  {
    "id": "arXiv:2205.13568",
    "title": "Learning Dialogue Representations from Consecutive Utterances",
    "abstract": "Learning high-quality dialogue representations is essential for solving a\nvariety of dialogue-oriented tasks, especially considering that dialogue\nsystems often suffer from data scarcity. In this paper, we introduce Dialogue\nSentence Embedding (DSE), a self-supervised contrastive learning method that\nlearns effective dialogue representations suitable for a wide range of dialogue\ntasks. DSE learns from dialogues by taking consecutive utterances of the same\ndialogue as positive pairs for contrastive learning. Despite its simplicity,\nDSE achieves significantly better representation capability than other dialogue\nrepresentation and universal sentence representation models. We evaluate DSE on\nfive downstream dialogue tasks that examine dialogue representation at\ndifferent semantic granularities. Experiments in few-shot and zero-shot\nsettings show that DSE outperforms baselines by a large margin. For example, it\nachieves 13 average performance improvement over the strongest unsupervised\nbaseline in 1-shot intent classification on 6 datasets. We also provide\nanalyses on the benefits and limitations of our model.",
    "descriptor": "\nComments: NAACL 2022 main conference\n",
    "authors": [
      "Zhihan Zhou",
      "Dejiao Zhang",
      "Wei Xiao",
      "Nicholas Dingwall",
      "Xiaofei Ma",
      "Andrew O. Arnold",
      "Bing Xiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13568"
  },
  {
    "id": "arXiv:2205.13570",
    "title": "ClinicalPath: a Visualization tool to Improve the Evaluation of  Electronic Health Records in Clinical Decision-Making",
    "abstract": "Physicians work at a very tight schedule and need decision-making support\ntools to help on improving and doing their work in a timely and dependable\nmanner. Examining piles of sheets with test results and using systems with\nlittle visualization support to provide diagnostics is daunting, but that is\nstill the usual way for the physicians' daily procedure, especially in\ndeveloping countries. Electronic Health Records systems have been designed to\nkeep the patients' history and reduce the time spent analyzing the patient's\ndata. However, better tools to support decision-making are still needed. In\nthis paper, we propose ClinicalPath, a visualization tool for users to track a\npatient's clinical path through a series of tests and data, which can aid in\ntreatments and diagnoses. Our proposal is focused on patient's data analysis,\npresenting the test results and clinical history longitudinally. Both the\nvisualization design and the system functionality were developed in close\ncollaboration with experts in the medical domain to ensure a right fit of the\ntechnical solutions and the real needs of the professionals. We validated the\nproposed visualization based on case studies and user assessments through tasks\nbased on the physician's daily activities. Our results show that our proposed\nsystem improves the physicians' experience in decision-making tasks, made with\nmore confidence and better usage of the physicians' time, allowing them to take\nother needed care for the patients.",
    "descriptor": "\nComments: 14 pages and 8 figures\n",
    "authors": [
      "Claudio D. G. Linhares",
      "Daniel M. Lima",
      "Jean R. Ponciano",
      "Mauro M. Olivatto",
      "Marco A. Gutierrez",
      "Jorge Poco",
      "Caetano Traina Jr.",
      "Agma J. M. Traina"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13570"
  },
  {
    "id": "arXiv:2205.13571",
    "title": "Low-rank lottery tickets: finding efficient low-rank neural networks via  matrix differential equations",
    "abstract": "Neural networks have achieved tremendous success in a large variety of\napplications. However, their memory footprint and computational demand can\nrender them impractical in application settings with limited hardware or energy\nresources. In this work, we propose a novel algorithm to find efficient\nlow-rank subnetworks. Remarkably, these subnetworks are determined and adapted\nalready during the training phase and the overall time and memory resources\nrequired by both training and evaluating them is significantly reduced. The\nmain idea is to restrict the weight matrices to a low-rank manifold and to\nupdate the low-rank factors rather than the full matrix during training. To\nderive training updates that are restricted to the prescribed manifold, we\nemploy techniques from dynamic model order reduction for matrix differential\nequations. Moreover, our method automatically and dynamically adapts the ranks\nduring training to achieve a desired approximation accuracy. The efficiency of\nthe proposed method is demonstrated through a variety of numerical experiments\non fully-connected and convolutional networks.",
    "descriptor": "",
    "authors": [
      "Steffen Schotth\u00f6fer",
      "Emanuele Zangrando",
      "Jonas Kusch",
      "Gianluca Ceruti",
      "Francesco Tudisco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13571"
  },
  {
    "id": "arXiv:2205.13572",
    "title": "Clinical Dialogue Transcription Error Correction using Seq2Seq Models",
    "abstract": "Good communication is critical to good healthcare. Clinical dialogue is a\nconversation between health practitioners and their patients, with the explicit\ngoal of obtaining and sharing medical information. This information contributes\nto medical decision-making regarding the patient and plays a crucial role in\ntheir healthcare journey. The reliance on note taking and manual scribing\nprocesses are extremely inefficient and leads to manual transcription errors\nwhen digitizing notes. Automatic Speech Recognition (ASR) plays a significant\nrole in speech-to-text applications, and can be directly used as a text\ngenerator in conversational applications. However, recording clinical dialogue\npresents a number of general and domain-specific challenges. In this paper, we\npresent a seq2seq learning approach for ASR transcription error correction of\nclinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)\nDataset which was gathered by healthcare professionals from a NHS Inflammatory\nBowel Disease clinic and use this in a comparative study with four commercial\nASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on\na mask-filling task using a domain-specific PubMed dataset which we have shared\npublicly for future research. The BART model fine-tuned for mask-filling was\nable to correct transcription errors and achieve lower word error rates for\nthree out of four commercial ASR outputs.",
    "descriptor": "\nComments: Pre-print of the paper accepted at the 6th International Workshop on Health Intelligence (W3PHIAI-22)\n",
    "authors": [
      "Gayani Nanayakkara",
      "Nirmalie Wiratunga",
      "David Corsar",
      "Kyle Martin",
      "Anjana Wijekoon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13572"
  },
  {
    "id": "arXiv:2205.13573",
    "title": "Efficient Approximation of Gromov-Wasserstein Distance using Importance  Sparsification",
    "abstract": "As a valid metric of metric-measure spaces, Gromov-Wasserstein (GW) distance\nhas shown the potential for the matching problems of structured data like point\nclouds and graphs. However, its application in practice is limited due to its\nhigh computational complexity. To overcome this challenge, we propose a novel\nimportance sparsification method, called Spar-GW, to approximate GW distance\nefficiently. In particular, instead of considering a dense coupling matrix, our\nmethod leverages a simple but effective sampling strategy to construct a sparse\ncoupling matrix and update it with few computations. We demonstrate that the\nproposed Spar-GW method is applicable to the GW distance with arbitrary ground\ncost, and it reduces the complexity from $\\mathcal{O}(n^4)$ to\n$\\mathcal{O}(n^{2+\\delta})$ for an arbitrary small $\\delta>0$. In addition,\nthis method can be extended to approximate the variants of GW distance,\nincluding the entropic GW distance, the fused GW distance, and the unbalanced\nGW distance. Experiments show the superiority of our Spar-GW to\nstate-of-the-art methods in both synthetic and real-world tasks.",
    "descriptor": "\nComments: 24 pages, 7 figures\n",
    "authors": [
      "Mengyu Li",
      "Jun Yu",
      "Hongteng Xu",
      "Cheng Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13573"
  },
  {
    "id": "arXiv:2205.13574",
    "title": "Pruning has a disparate impact on model accuracy",
    "abstract": "Network pruning is a widely-used compression technique that is able to\nsignificantly scale down overparameterized models with minimal loss of\naccuracy. This paper shows that pruning may create or exacerbate disparate\nimpacts. The paper sheds light on the factors to cause such disparities,\nsuggesting differences in gradient norms and distance to decision boundary\nacross groups to be responsible for this critical issue. It analyzes these\nfactors in detail, providing both theoretical and empirical support, and\nproposes a simple, yet effective, solution that mitigates the disparate impacts\ncaused by pruning.",
    "descriptor": "",
    "authors": [
      "Cuong Tran",
      "Ferdinando Fioretto",
      "Jung-Eun Kim",
      "Rakshit Naidu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13574"
  },
  {
    "id": "arXiv:2205.13575",
    "title": "Predictor-corrector algorithms for stochastic optimization under gradual  distribution shift",
    "abstract": "Time-varying stochastic optimization problems frequently arise in machine\nlearning practice (e.g. gradual domain shift, object tracking, strategic\nclassification). Although most problems are solved in discrete time, the\nunderlying process is often continuous in nature. We exploit this underlying\ncontinuity by developing predictor-corrector algorithms for time-varying\nstochastic optimizations. We provide error bounds for the iterates, both in\npresence of pure and noisy access to the queries from the relevant derivatives\nof the loss function. Furthermore, we show (theoretically and empirically in\nseveral examples) that our method outperforms non-predictor corrector methods\nthat do not exploit the underlying continuous process.",
    "descriptor": "",
    "authors": [
      "Subha Maity",
      "Debarghya Mukherjee",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.13575"
  },
  {
    "id": "arXiv:2205.13576",
    "title": "Factors Impacting Resilience of Internet of Things Systems in Critical  Infrastructure",
    "abstract": "Internet of Things (IoT) systems are recently being employed in various types\nof critical infrastructure, including integrated rescue systems, healthcare,\ndefence, energy and other fields. Recently, the security and safety of IoT\nsystems, in general, has been questioned by a number of studies. Raised\nconcerns do not relate to the IoT technology in principle but to poor\nengineering practices that are mostly preventable. In critical infrastructure,\ndemand for safety and security is strongly present and justifies a discussion\nabout the general resilience of IoT systems. In this context, resilience\nincludes system resistance to cyberattacks and its stability to operating\nconditions and system reliability and safety in terms of present flaws. In this\npaper, we discuss relevant factors impacting the resilience of IoT systems in\nthe critical infrastructure and suggest possible countermeasures and actions\nmitigate the potential effects of these factors. Contrary to the previous work,\nan unique critical system Model-based Testing viewpoint is taken in this\nanalysis.",
    "descriptor": "\nComments: Paper accepted at IEEE AIIoT 2022, Seattle, USA, 6-9 June 2022\n",
    "authors": [
      "Miroslav Bures",
      "Pavel Blazek",
      "Jiri Nema",
      "Hynek Schvach"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13576"
  },
  {
    "id": "arXiv:2205.13577",
    "title": "Understanding new tasks through the lens of training data via  exponential tilting",
    "abstract": "Deploying machine learning models to new tasks is a major challenge despite\nthe large size of the modern training datasets. However, it is conceivable that\nthe training data can be reweighted to be more representative of the new\n(target) task. We consider the problem of reweighing the training samples to\ngain insights into the distribution of the target task. Specifically, we\nformulate a distribution shift model based on the exponential tilt assumption\nand learn train data importance weights minimizing the KL divergence between\nlabeled train and unlabeled target datasets. The learned train data weights can\nthen be used for downstream tasks such as target performance evaluation,\nfine-tuning, and model selection. We demonstrate the efficacy of our method on\nWaterbirds and Breeds benchmarks.",
    "descriptor": "",
    "authors": [
      "Subha Maity",
      "Mikhail Yurochkin",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13577"
  },
  {
    "id": "arXiv:2205.13578",
    "title": "Dynamic Network Reconfiguration for Entropy Maximization using Deep  Reinforcement Learning",
    "abstract": "A key problem in network theory is how to reconfigure a graph in order to\noptimize a quantifiable objective. Given the ubiquity of networked systems,\nsuch work has broad practical applications in a variety of situations, ranging\nfrom drug and material design to telecommunications. The large decision space\nof possible reconfigurations, however, makes this problem computationally\nintensive. In this paper, we cast the problem of network rewiring for\noptimizing a specified structural property as a Markov Decision Process (MDP),\nin which a decision-maker is given a budget of modifications that are performed\nsequentially. We then propose a general approach based on the Deep Q-Network\n(DQN) algorithm and graph neural networks (GNNs) that can efficiently learn\nstrategies for rewiring networks. We then discuss a cybersecurity case study,\ni.e., an application to the computer network reconfiguration problem for\nintrusion protection. In a typical scenario, an attacker might have a (partial)\nmap of the system they plan to penetrate; if the network is effectively\n\"scrambled\", they would not be able to navigate it since their prior knowledge\nwould become obsolete. This can be viewed as an entropy maximization problem,\nin which the goal is to increase the surprise of the network. Indeed, entropy\nacts as a proxy measurement of the difficulty of navigating the network\ntopology. We demonstrate the general ability of the proposed method to obtain\nbetter entropy gains than random rewiring on synthetic and real-world graphs\nwhile being computationally inexpensive, as well as being able to generalize to\nlarger graphs than those seen during training. Simulations of attack scenarios\nconfirm the effectiveness of the learned rewiring strategies.",
    "descriptor": "\nComments: 9 pages, 4 figures, 2 appendices\n",
    "authors": [
      "Christoffel Doorman",
      "Victor-Alexandru Darvariu",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13578"
  },
  {
    "id": "arXiv:2205.13579",
    "title": "CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal  Assignment and Pseudo-Label Refinement",
    "abstract": "Recent works on unsupervised domain adaptation (UDA) focus on the selection\nof good pseudo-labels as surrogates for the missing labels in the target data.\nHowever, source domain bias that deteriorates the pseudo-labels can still exist\nsince the shared network of the source and target domains are typically used\nfor the pseudo-label selections. The suboptimal feature space source-to-target\ndomain alignment can also result in unsatisfactory performance. In this paper,\nwe propose CA-UDA to improve the quality of the pseudo-labels and UDA results\nwith optimal assignment, a pseudo-label refinement strategy and class-aware\ndomain alignment. We use an auxiliary network to mitigate the source domain\nbias for pseudo-label refinement. Our intuition is that the underlying\nsemantics in the target domain can be fully exploited to help refine the\npseudo-labels that are inferred from the source features under domain shift.\nFurthermore, our optimal assignment can optimally align features in the\nsource-to-target domains and our class-aware domain alignment can\nsimultaneously close the domain gap while preserving the classification\ndecision boundaries. Extensive experiments on several benchmark datasets show\nthat our method can achieve state-of-the-art performance in the image\nclassification task.",
    "descriptor": "",
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13579"
  },
  {
    "id": "arXiv:2205.13582",
    "title": "On the Construction of New Toric Quantum Codes and Quantum Burst-Error  Correcting Codes",
    "abstract": "A toric quantum error-correcting code construction procedure is presented in\nthis work. A new class of an infinite family of toric quantum codes is provided\nby constructing a classical cyclic code on the square lattice\n$\\mathbb{Z}_{q}\\times \\mathbb{Z}_{q}$ for all odd integers $q\\geq 5$ and,\nconsequently, new toric quantum codes are constructed on such square lattices\nregardless of whether $q$ can be represented as a sum of two squares.\nFurthermore this work supplies for each $q$ the polyomino shapes that\ntessellate the corresponding square lattices and, consequently, tile the\nlattice $\\mathbb{Z}^{2}$. The channel without memory to be considered for these\nconstructed toric quantum codes is symmetric, since the\n$\\mathbb{Z}^{2}$-lattice is autodual. Moreover, we propose a quantum\ninterleaving technique by using the constructed toric quantum codes which shows\nthat the code rate and the coding gain of the interleaved toric quantum codes\nare better than the code rate and the coding gain of Kitaev's toric quantum\ncodes for $q=2n+1$, where $n\\geq 2$, and of an infinite class of Bombin and\nMartin-Delgado's toric quantum codes. In addition to the proposed quantum\ninterleaving technique improves such parameters, it can be used for burst-error\ncorrection in errors which are located, quantum data stored and quantum\nchannels with memory.",
    "descriptor": "\nComments: Submitted to \"Journal of Algebra, Combinatorics, Discrete Structures and Applications\"\n",
    "authors": [
      "Cibele Cristina Trinca",
      "J. Carmelo Interlando",
      "Reginaldo Palazzo Jr.",
      "Antonio Aparecido de Andrade",
      "Ricardo Augusto Watanabe"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.13582"
  },
  {
    "id": "arXiv:2205.13583",
    "title": "Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for  the Diagnosis of Eosinophilic Esophagitis",
    "abstract": "Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition\nof the esophagus associated with elevated esophageal eosinophils. Second only\nto gastroesophageal reflux disease, EoE is one of the leading causes of chronic\nrefractory dysphagia in adults and children. EoE diagnosis requires enumerating\nthe density of esophageal eosinophils in esophageal biopsies, a somewhat\nsubjective task that is time-consuming, thus reducing the ability to process\nthe complex tissue structure. Previous artificial intelligence (AI) approaches\nthat aimed to improve histology-based diagnosis focused on recapitulating\nidentification and quantification of the area of maximal eosinophil density.\nHowever, this metric does not account for the distribution of eosinophils or\nother histological features, over the whole slide image. Here, we developed an\nartificial intelligence platform that infers local and spatial biomarkers based\non semantic segmentation of intact eosinophils and basal zone distributions.\nBesides the maximal density of eosinophils (referred to as Peak Eosinophil\nCount [PEC]) and a maximal basal zone fraction, we identify two additional\nmetrics that reflect the distribution of eosinophils and basal zone fractions.\nThis approach enables a decision support system that predicts EoE activity and\nclassifies the histological severity of EoE patients. We utilized a cohort that\nincludes 1066 biopsy slides from 400 subjects to validate the system's\nperformance and achieved a histological severity classification accuracy of\n86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach\nhighlights the importance of systematically analyzing the distribution of\nbiopsy features over the entire slide and paves the way towards a personalized\ndecision support system that will assist not only in counting cells but can\nalso potentially improve diagnosis and provide treatment prediction.",
    "descriptor": "\nComments: AL, EA, and ND have contributed equally to this work and share first authorship. YS is the corresponding author, e-mail: yoni.savir@technion.ac.il\n",
    "authors": [
      "Ariel Larey",
      "Eliel Aknin",
      "Nati Daniel",
      "Garrett A. Osswald",
      "Julie M. Caldwell",
      "Mark Rochman",
      "Tanya Wasserman",
      "Margaret H. Collins",
      "Nicoleta C. Arva",
      "Guang-Yu Yang",
      "Marc E. Rothenberg",
      "Yonatan Savir"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.13583"
  },
  {
    "id": "arXiv:2205.13585",
    "title": "Learning in Feedback-driven Recurrent Spiking Neural Networks using  full-FORCE Training",
    "abstract": "Feedback-driven recurrent spiking neural networks (RSNNs) are powerful\ncomputational models that can mimic dynamical systems. However, the presence of\na feedback loop from the readout to the recurrent layer de-stabilizes the\nlearning mechanism and prevents it from converging. Here, we propose a\nsupervised training procedure for RSNNs, where a second network is introduced\nonly during the training, to provide hint for the target dynamics. The proposed\ntraining procedure consists of generating targets for both recurrent and\nreadout layers (i.e., for a full RSNN system). It uses the recursive least\nsquare-based First-Order and Reduced Control Error (FORCE) algorithm to fit the\nactivity of each layer to its target. The proposed full-FORCE training\nprocedure reduces the amount of modifications needed to keep the error between\nthe output and target close to zero. These modifications control the feedback\nloop, which causes the training to converge. We demonstrate the improved\nperformance and noise robustness of the proposed full-FORCE training procedure\nto model 8 dynamical systems using RSNNs with leaky integrate and fire (LIF)\nneurons and rate coding. For energy-efficient hardware implementation, an\nalternative time-to-first-spike (TTFS) coding is implemented for the full-\nFORCE training procedure. Compared to rate coding, full-FORCE with TTFS coding\ngenerates fewer spikes and facilitates faster convergence to the target\ndynamics.",
    "descriptor": "\nComments: Accepted at IJCNN 2022\n",
    "authors": [
      "Ankita Paul",
      "Stefan Wagner",
      "Anup Das"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.13585"
  },
  {
    "id": "arXiv:2205.13586",
    "title": "Comparing the Digital Annealer with Classical Evolutionary Algorithm",
    "abstract": "In more recent years, there has been increasing research interest in\nexploiting the use of application specific hardware for solving optimisation\nproblems. Examples of solvers that use specialised hardware are IBM's Quantum\nSystem One and D-wave's Quantum Annealer (QA) and Fujitsu's Digital Annealer\n(DA). These solvers have been developed to optimise problems faster than\ntraditional meta-heuristics implemented on general purpose machines. Previous\nresearch has shown that these solvers (can optimise many problems much quicker\nthan exact solvers such as GUROBI and CPLEX. Such conclusions have not been\nmade when comparing hardware solvers with classical evolutionary algorithms.\nMaking a fair comparison between traditional evolutionary algorithms, such as\nGenetic Algorithm (GA), and the DA (or other similar solvers) is challenging\nbecause the later benefits from the use of application specific hardware while\nevolutionary algorithms are often implemented on generation purpose machines.\nMoreover, quantum or quantum-inspired solvers are limited to solving problems\nin a specific format. A common formulation used is Quadratic Unconstrained\nBinary Optimisation (QUBO). Many optimisation problems are however constrained\nand have natural representations that are non-binary. Converting such problems\nto QUBO can lead to more problem difficulty and/or larger search space.\nThe question addressed in this paper is whether quantum or quantum-inspired\nsolvers can optimise QUBO transformations of combinatorial optimisation\nproblems faster than classical evolutionary algorithms applied to the same\nproblems in their natural representations. We show that the DA often presents\nbetter average objective function values than GA on Travelling Salesman,\nQuadratic Assignment and Multi-dimensional Knapsack Problem instances.",
    "descriptor": "",
    "authors": [
      "Mayowa Ayodele"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13586"
  },
  {
    "id": "arXiv:2205.13587",
    "title": "Evolution of beliefs in social networks",
    "abstract": "Evolution of beliefs of a society are a product of interactions between\npeople (horizontal transmission) in the society over generations (vertical\ntransmission). Researchers have studied both horizontal and vertical\ntransmission separately. Extending prior work, we propose a new theoretical\nframework which allows application of tools from Markov chain theory to the\nanalysis of belief evolution via horizontal and vertical transmission. We\nanalyze three cases: static network, randomly changing network, and\nhomophily-based dynamic network. Whereas the former two assume network\nstructure is independent of beliefs, the latter assumes that people tend to\ncommunicate with those who have similar beliefs. We prove under general\nconditions that both static and randomly changing networks converge to a single\nset of beliefs among all individuals along with the rate of convergence. We\nprove that homophily-based network structures do not in general converge to a\nsingle set of beliefs shared by all and prove lower bounds on the number of\ndifferent limiting beliefs as a function of initial beliefs. We conclude by\ndiscussing implications for prior theories and directions for future work.",
    "descriptor": "",
    "authors": [
      "Pushpi Paranamana",
      "Pei Wang",
      "Patrick Shafto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13587"
  },
  {
    "id": "arXiv:2205.13589",
    "title": "Pessimism in the Face of Confounders: Provably Efficient Offline  Reinforcement Learning in Partially Observable Markov Decision Processes",
    "abstract": "We study offline reinforcement learning (RL) in partially observable Markov\ndecision processes. In particular, we aim to learn an optimal policy from a\ndataset collected by a behavior policy which possibly depends on the latent\nstate. Such a dataset is confounded in the sense that the latent state\nsimultaneously affects the action and the observation, which is prohibitive for\nexisting offline RL algorithms. To this end, we propose the \\underline{P}roxy\nvariable \\underline{P}essimistic \\underline{P}olicy \\underline{O}ptimization\n(\\texttt{P3O}) algorithm, which addresses the confounding bias and the\ndistributional shift between the optimal and behavior policies in the context\nof general function approximation. At the core of \\texttt{P3O} is a coupled\nsequence of pessimistic confidence regions constructed via proximal causal\ninference, which is formulated as minimax estimation. Under a partial coverage\nassumption on the confounded dataset, we prove that \\texttt{P3O} achieves a\n$n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the\ndataset. To our best knowledge, \\texttt{P3O} is the first provably efficient\noffline RL algorithm for POMDPs with a confounded dataset.",
    "descriptor": "",
    "authors": [
      "Miao Lu",
      "Yifei Min",
      "Zhaoran Wang",
      "Zhuoran Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13589"
  },
  {
    "id": "arXiv:2205.13593",
    "title": "Block Ciphers Substitution Box Generation Based on Natural Randomness in  Underwater Acoustics and Knights Tour Chain",
    "abstract": "The protection of confidential information is a global issue and block\nencryption algorithms are the most reliable option. The famous information\ntheorist, Claude Shannon has given two desirable characteristics that should\nexist in a strong cipher which are substitution and permutation in their\nfundamental research on Communication Theory of Secrecy Systems. block ciphers\nstrictly follow the substitution and permutation principle to generate a\nciphertext. The actual strength of the block ciphers against several attacks is\nentirely based on its substitution characteristic, which is gained by using the\nS-Box. In the current literature, algebraic structure-based and chaos-based\ntechniques are highly used for the construction of S-boxes because both these\ntechniques have favourable features for S-box construction, but also various\nattacks of these techniques have been identified. True randomness has been\nuniversally recognized as the ideal method for cipher primitives design because\ntrue random numbers are unpredictable, irreversible, and unreproducible. The\nbasic concept of the proposed technique is the extraction of true random bits\nfrom underwater acoustic waves and to design a novel technique for the dynamic\ngeneration of S-boxes using the chain of knights tour. The proposed method\nsatisfies all standard evaluation tests of S-boxes construction and true random\nnumbers generation. Two million bits have been analyzed using the NIST\nrandomness test suite, and the results show that underwater sound waves are an\nimpeccable entropy source for true randomness. Additionally, our dynamically\ngenerated S-boxes have better or equal strength, over the latest published\nS-boxes (2020 to 2021). According to our knowledge first time, this type of\nresearch has been done, in which natural randomness of underwater acoustic\nwaves has been used for the construction of block cipher's S-Box",
    "descriptor": "\nComments: 17 pages, 5 figures, Journal\n",
    "authors": [
      "Muhammad Fahad Khan",
      "Khalid Saleem",
      "Tariq Shah",
      "Mohammad Mazyad Hazzazi",
      "Ismail Bahkali",
      "Piyush Kumar Shukla"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13593"
  },
  {
    "id": "arXiv:2205.13594",
    "title": "DRLComplex: Reconstruction of protein quaternary structures using deep  reinforcement learning",
    "abstract": "Predicted inter-chain residue-residue contacts can be used to build the\nquaternary structure of protein complexes from scratch. However, only a small\nnumber of methods have been developed to reconstruct protein quaternary\nstructures using predicted inter-chain contacts. Here, we present an\nagent-based self-learning method based on deep reinforcement learning\n(DRLComplex) to build protein complex structures using inter-chain contacts as\ndistance constraints. We rigorously tested DRLComplex on two standard datasets\nof homodimeric and heterodimeric protein complexes (i.e., the CASP-CAPRI\nhomodimer and Std_32 heterodimer datasets) using both true and predicted\ninterchain contacts as inputs. Utilizing true contacts as input, DRLComplex\nachieved high average TM-scores of 0.9895 and 0.9881 and a low average\ninterface RMSD (I_RMSD) of 0.2197 and 0.92 on the two datasets, respectively.\nWhen predicted contacts are used, the method achieves TM-scores of 0.73 and\n0.76 for homodimers and heterodimers, respectively. Our experiments find that\nthe accuracy of reconstructed quaternary structures depends on the accuracy of\nthe contact predictions. Compared to other optimization methods for\nreconstructing quaternary structures from inter-chain contacts, DRLComplex\nperforms similar to an advanced gradient descent method and better than a\nMarkov Chain Monte Carlo simulation method and a simulated annealing-based\nmethod, validating the effectiveness of DRLComplex for quaternary\nreconstruction of protein complexes.",
    "descriptor": "\nComments: 20 pages, 8 figures, 12 tables. Under review\n",
    "authors": [
      "Elham Soltanikazemi",
      "Raj S. Roy",
      "Farhan Quadir",
      "Nabin Giri",
      "Alex Morehead",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.13594"
  },
  {
    "id": "arXiv:2205.13598",
    "title": "Multiwinner Elections under Minimax Chamberlin-Courant Rule in Euclidean  Space",
    "abstract": "We consider multiwinner elections in Euclidean space using the minimax\nChamberlin-Courant rule. In this setting, voters and candidates are embedded in\na $d$-dimensional Euclidean space, and the goal is to choose a committee of $k$\ncandidates so that the rank of any voter's most preferred candidate in the\ncommittee is minimized. (The problem is also equivalent to the ordinal version\nof the classical $k$-center problem.) We show that the problem is NP-hard in\nany dimension $d \\geq 2$, and also provably hard to approximate. Our main\nresults are three polynomial-time approximation schemes, each of which finds a\ncommittee with provably good minimax score. In all cases, we show that our\napproximation bounds are tight or close to tight. We mainly focus on the\n$1$-Borda rule but some of our results also hold for the more general\n$r$-Borda.",
    "descriptor": "\nComments: Accepted for IJCAI-ECAI 2022\n",
    "authors": [
      "Chinmay Sonar",
      "Subhash Suri",
      "Jie Xue"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.13598"
  },
  {
    "id": "arXiv:2205.13599",
    "title": "VectorAdam for Rotation Equivariant Geometry Optimization",
    "abstract": "The rise of geometric problems in machine learning has necessitated the\ndevelopment of equivariant methods, which preserve their output under the\naction of rotation or some other transformation. At the same time, the Adam\noptimization algorithm has proven remarkably effective across machine learning\nand even traditional tasks in geometric optimization. In this work, we observe\nthat naively applying Adam to optimize vector-valued data is not rotation\nequivariant, due to per-coordinate moment updates, and in fact this leads to\nsignificant artifacts and biases in practice. We propose to resolve this\ndeficiency with VectorAdam, a simple modification which makes Adam\nrotation-equivariant by accounting for the vector structure of optimization\nvariables. We demonstrate this approach on problems in machine learning and\ntraditional geometric optimization, showing that equivariant VectorAdam\nresolves the artifacts and biases of traditional Adam when applied to\nvector-valued data, with equivalent or even improved rates of convergence.",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Selena Ling",
      "Nicholas Sharp",
      "Alec Jacobson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13599"
  },
  {
    "id": "arXiv:2205.13600",
    "title": "MyoSuite -- A contact-rich simulation suite for musculoskeletal motor  control",
    "abstract": "Embodied agents in continuous control domains have had limited exposure to\ntasks allowing to explore musculoskeletal properties that enable agile and\nnimble behaviors in biological beings. The sophistication behind\nneuro-musculoskeletal control can pose new challenges for the motor learning\ncommunity. At the same time, agents solving complex neural control problems\nallow impact in fields such as neuro-rehabilitation, as well as\ncollaborative-robotics. Human biomechanics underlies complex\nmulti-joint-multi-actuator musculoskeletal systems. The sensory-motor system\nrelies on a range of sensory-contact rich and proprioceptive inputs that define\nand condition muscle actuation required to exhibit intelligent behaviors in the\nphysical world. Current frameworks for musculoskeletal control do not support\nphysiological sophistication of the musculoskeletal systems along with physical\nworld interaction capabilities. In addition, they are neither embedded in\ncomplex and skillful motor tasks nor are computationally effective and scalable\nto study large-scale learning paradigms. Here, we present MyoSuite -- a suite\nof physiologically accurate biomechanical models of elbow, wrist, and hand,\nwith physical contact capabilities, which allow learning of complex and\nskillful contact-rich real-world tasks. We provide diverse motor-control\nchallenges: from simple postural control to skilled hand-object interactions\nsuch as turning a key, twirling a pen, rotating two balls in one hand, etc. By\nsupporting physiological alterations in musculoskeletal geometry (tendon\ntransfer), assistive devices (exoskeleton assistance), and muscle contraction\ndynamics (muscle fatigue, sarcopenia), we present real-life tasks with temporal\nchanges, thereby exposing realistic non-stationary conditions in our tasks\nwhich most continuous control benchmarks lack.",
    "descriptor": "",
    "authors": [
      "Vittorio Caggiano",
      "Huawei Wang",
      "Guillaume Durandau",
      "Massimo Sartori",
      "Vikash Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13600"
  },
  {
    "id": "arXiv:2205.13603",
    "title": "Tensor Program Optimization with Probabilistic Programs",
    "abstract": "Automatic optimization for tensor programs becomes increasingly important as\nwe deploy deep learning in various environments, and efficient optimization\nrelies on a rich search space and effective search. Most existing efforts adopt\na search space which lacks the ability to efficiently enable domain experts to\ngrow the search space. This paper introduces MetaSchedule, a domain-specific\nprobabilistic programming language abstraction to construct a rich search space\nof tensor programs. Our abstraction allows domain experts to analyze the\nprogram, and easily propose stochastic choices in a modular way to compose\nprogram transformation accordingly. We also build an end-to-end learning-driven\nframework to find an optimized program for a given search space. Experimental\nresults show that MetaSchedule can cover the search space used in the\nstate-of-the-art tensor program optimization frameworks in a modular way.\nAdditionally, it empowers domain experts to conveniently grow the search space\nand modularly enhance the system, which brings 48% speedup on end-to-end deep\nlearning workloads.",
    "descriptor": "",
    "authors": [
      "Junru Shao",
      "Xiyou Zhou",
      "Siyuan Feng",
      "Bohan Hou",
      "Ruihang Lai",
      "Hongyi Jin",
      "Wuwei Lin",
      "Masahiro Masuda",
      "Cody Hao Yu",
      "Tianqi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13603"
  },
  {
    "id": "arXiv:2205.13607",
    "title": "Self-supervised Pretraining and Transfer Learning Enable Flu and  COVID-19 Predictions in Small Mobile Sensing Datasets",
    "abstract": "Detailed mobile sensing data from phones, watches, and fitness trackers offer\nan unparalleled opportunity to quantify and act upon previously unmeasurable\nbehavioral changes in order to improve individual health and accelerate\nresponses to emerging diseases. Unlike in natural language processing and\ncomputer vision, deep representation learning has yet to broadly impact this\ndomain, in which the vast majority of research and clinical applications still\nrely on manually defined features and boosted tree models or even forgo\npredictive modeling altogether due to insufficient accuracy. This is due to\nunique challenges in the behavioral health domain, including very small\ndatasets (~10^1 participants), which frequently contain missing data, consist\nof long time series with critical long-range dependencies (length>10^4), and\nextreme class imbalances (>10^3:1).",
    "descriptor": "",
    "authors": [
      "Mike A. Merrill",
      "Tim Althoff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13607"
  },
  {
    "id": "arXiv:2205.13613",
    "title": "Circumventing Backdoor Defenses That Are Based on Latent Separability",
    "abstract": "Deep learning models are vulnerable to backdoor poisoning attacks. In\nparticular, adversaries can embed hidden backdoors into a model by only\nmodifying a very small portion of its training data. On the other hand, it has\nalso been commonly observed that backdoor poisoning attacks tend to leave a\ntangible signature in the latent space of the backdoored model i.e. poison\nsamples and clean samples form two separable clusters in the latent space.\nThese observations give rise to the popularity of latent separability\nassumption, which states that the backdoored DNN models will learn separable\nlatent representations for poison and clean populations. A number of popular\ndefenses (e.g. Spectral Signature, Activation Clustering, SCAn, etc.) are\nexactly built upon this assumption. However, in this paper, we show that the\nlatent separation can be significantly suppressed via designing adaptive\nbackdoor poisoning attacks with more sophisticated poison strategies, which\nconsequently render state-of-the-art defenses based on this assumption less\neffective (and often completely fail). More interestingly, we find that our\nadaptive attacks can even evade some other typical backdoor defenses that do\nnot explicitly build on this separability assumption. Our results show that\nadaptive backdoor poisoning attacks that can breach the latent separability\nassumption should be seriously considered for evaluating existing and future\ndefenses.",
    "descriptor": "",
    "authors": [
      "Xiangyu Qi",
      "Tinghao Xie",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13613"
  },
  {
    "id": "arXiv:2205.13616",
    "title": "Fight Poison with Poison: Detecting Backdoor Poison Samples via  Decoupling Benign Correlations",
    "abstract": "In this work, we study poison samples detection for defending against\nbackdoor poisoning attacks on deep neural networks (DNNs). A principled idea\nunderlying prior arts on this problem is to utilize the backdoored models'\ndistinguishable behaviors on poison and clean populations to distinguish\nbetween these two different populations themselves and remove the identified\npoison. Many prior arts build their detectors upon a latent separability\nassumption, which states that backdoored models trained on the poisoned dataset\nwill learn separable latent representations for backdoor and clean samples.\nAlthough such separation behaviors empirically exist for many existing attacks,\nthere is no control on the separability and the extent of separation can vary a\nlot across different poison strategies, datasets, as well as the training\nconfigurations of backdoored models. Worse still, recent adaptive poison\nstrategies can greatly reduce the \"distinguishable behaviors\" and consequently\nrender most prior arts less effective (or completely fail). We point out that\nthese limitations directly come from the passive reliance on some\ndistinguishable behaviors that are not controlled by defenders. To mitigate\nsuch limitations, in this work, we propose the idea of active defense -- rather\nthan passively assuming backdoored models will have certain distinguishable\nbehaviors on poison and clean samples, we propose to actively enforce the\ntrained models to behave differently on these two different populations.\nSpecifically, we introduce confusion training as a concrete instance of active\ndefense.",
    "descriptor": "",
    "authors": [
      "Xiangyu Qi",
      "Tinghao Xie",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13616"
  },
  {
    "id": "arXiv:2205.13617",
    "title": "Approximate Q-learning and SARSA(0) under the $\u03b5$-greedy Policy:  a Differential Inclusion Analysis",
    "abstract": "Q-learning and SARSA(0) with linear function approximation, under\n$\\epsilon$-greedy exploration, are leading methods to estimate the optimal\npolicy in Reinforcement Learning (RL). It has been empirically known that the\ndiscontinuous nature of the greedy policies causes these algorithms to exhibit\ncomplex phenomena such as i.) instability, ii.) policy oscillation and\nchattering, iii.) multiple attractors, and iv.) worst policy convergence.\nHowever, the literature lacks a formal recipe to explain these behaviors and\nthis has been a long-standing open problem (Sutton, 1999). Our work addresses\nthis by building the necessary mathematical framework using stochastic\nrecursive inclusions and Differential Inclusions (DIs). From this novel\nviewpoint, our main result states that these approximate algorithms\nasymptotically converge to suitable invariant sets of DIs instead of\ndifferential equations, as is common elsewhere in RL. Furthermore, the nature\nof these deterministic DIs completely governs the limiting behaviors of these\nalgorithms.",
    "descriptor": "\nComments: 16 pages, 3 figures\n",
    "authors": [
      "Aditya Gopalan",
      "Gugan Thoppe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13617"
  },
  {
    "id": "arXiv:2205.13618",
    "title": "Denial-of-Service Attack on Object Detection Model Using Universal  Adversarial Perturbation",
    "abstract": "Adversarial attacks against deep learning-based object detectors have been\nstudied extensively in the past few years. The proposed attacks aimed solely at\ncompromising the models' integrity (i.e., trustworthiness of the model's\nprediction), while adversarial attacks targeting the models' availability, a\ncritical aspect in safety-critical domains such as autonomous driving, have not\nbeen explored by the machine learning research community. In this paper, we\npropose NMS-Sponge, a novel approach that negatively affects the decision\nlatency of YOLO, a state-of-the-art object detector, and compromises the\nmodel's availability by applying a universal adversarial perturbation (UAP). In\nour experiments, we demonstrate that the proposed UAP is able to increase the\nprocessing time of individual frames by adding \"phantom\" objects while\npreserving the detection of the original objects.",
    "descriptor": "",
    "authors": [
      "Avishag Shapira",
      "Alon Zolfi",
      "Luca Demetrio",
      "Battista Biggio",
      "Asaf Shabtai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13618"
  },
  {
    "id": "arXiv:2205.13619",
    "title": "Fairness in Recommendation: A Survey",
    "abstract": "As one of the most pervasive applications of machine learning, recommender\nsystems are playing an important role on assisting human decision making. The\nsatisfaction of users and the interests of platforms are closely related to the\nquality of the generated recommendation results. However, as a highly\ndata-driven system, recommender system could be affected by data or algorithmic\nbias and thus generate unfair results, which could weaken the reliance of the\nsystems. As a result, it is crucial to address the potential unfairness\nproblems in recommendation settings. Recently, there has been growing attention\non fairness considerations in recommender systems with more and more literature\non approaches to promote fairness in recommendation. However, the studies are\nrather fragmented and lack a systematic organization, thus making it difficult\nto penetrate for new researchers to the domain. This motivates us to provide a\nsystematic survey of existing works on fairness in recommendation. This survey\nfocuses on the foundations for fairness in recommendation literature. It first\npresents a brief introduction about fairness in basic machine learning tasks\nsuch as classification and ranking in order to provide a general overview of\nfairness research, as well as introduce the more complex situations and\nchallenges that need to be considered when studying fairness in recommender\nsystems. After that, the survey will introduce fairness in recommendation with\na focus on the taxonomies of current fairness definitions, the typical\ntechniques for improving fairness, as well as the datasets for fairness studies\nin recommendation. The survey also talks about the challenges and opportunities\nin fairness research with the hope of promoting the fair recommendation\nresearch area and beyond.",
    "descriptor": "",
    "authors": [
      "Yunqi Li",
      "Hanxiong Chen",
      "Shuyuan Xu",
      "Yingqiang Ge",
      "Juntao Tan",
      "Yongfeng Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13619"
  },
  {
    "id": "arXiv:2205.13621",
    "title": "Differentially Private Decoding in Large Language Models",
    "abstract": "Recent large-scale natural language processing (NLP) systems use a\npre-trained Large Language Model (LLM) on massive and diverse corpora as a\nheadstart. In practice, the pre-trained model is adapted to a wide array of\ntasks via fine-tuning on task-specific datasets. LLMs, while effective, have\nbeen shown to memorize instances of training data thereby potentially revealing\nprivate information processed during pre-training. The potential leakage might\nfurther propagate to the downstream tasks for which LLMs are fine-tuned. On the\nother hand, privacy-preserving algorithms usually involve retraining from\nscratch, which is prohibitively expensive for LLMs. In this work, we propose a\nsimple, easy to interpret, and computationally lightweight perturbation\nmechanism to be applied to an already trained model at the decoding stage. Our\nperturbation mechanism is model-agnostic and can be used in conjunction with\nany LLM. We provide theoretical analysis showing that the proposed mechanism is\ndifferentially private, and experimental results showing a privacy-utility\ntrade-off.",
    "descriptor": "",
    "authors": [
      "Jimit Majmudar",
      "Christophe Dupuy",
      "Charith Peris",
      "Sami Smaili",
      "Rahul Gupta",
      "Richard Zemel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13621"
  },
  {
    "id": "arXiv:2205.13623",
    "title": "A Hybrid Neural Autoencoder for Sensory Neuroprostheses and Its  Applications in Bionic Vision",
    "abstract": "Sensory neuroprostheses are emerging as a promising technology to restore\nlost sensory function or augment human capacities. However, sensations elicited\nby current devices often appear artificial and distorted. Although current\nmodels can often predict the neural or perceptual response to an electrical\nstimulus, an optimal stimulation strategy solves the inverse problem: what is\nthe required stimulus to produce a desired response? Here we frame this as an\nend-to-end optimization problem, where a deep neural network encoder is trained\nto invert a known, fixed forward model that approximates the underlying\nbiological system. As a proof of concept, we demonstrate the effectiveness of\nour hybrid neural autoencoder (HNA) on the use case of visual neuroprostheses.\nWe found that HNA is able to produce high-fidelity stimuli from the MNIST and\nCOCO datasets that outperform conventional encoding strategies and surrogate\ntechniques across all tested conditions. Overall this is an important step\ntowards the long-standing challenge of restoring high-quality vision to people\nliving with incurable blindness and may prove a promising solution for a\nvariety of neuroprosthetic technologies.",
    "descriptor": "",
    "authors": [
      "Jacob Granley",
      "Lucas Relic",
      "Michael Beyeler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13623"
  },
  {
    "id": "arXiv:2205.13624",
    "title": "Faster Optimization on Sparse Graphs via Neural Reparametrization",
    "abstract": "In mathematical optimization, second-order Newton's methods generally\nconverge faster than first-order methods, but they require the inverse of the\nHessian, hence are computationally expensive. However, we discover that on\nsparse graphs, graph neural networks (GNN) can implement an efficient\nQuasi-Newton method that can speed up optimization by a factor of 10-100x. Our\nmethod, neural reparametrization, modifies the optimization parameters as the\noutput of a GNN to reshape the optimization landscape. Using a precomputed\nHessian as the propagation rule, the GNN can effectively utilize the\nsecond-order information, reaching a similar effect as adaptive gradient\nmethods. As our method solves optimization through architecture design, it can\nbe used in conjunction with any optimizers such as Adam and RMSProp. We show\nthe application of our method on scientifically relevant problems including\nheat diffusion, synchronization and persistent homology.",
    "descriptor": "",
    "authors": [
      "Nima Dehmamy",
      "Csaba Both",
      "Jianzhi Long",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13624"
  },
  {
    "id": "arXiv:2205.13627",
    "title": "Experimental Design for Linear Functionals in Reproducing Kernel Hilbert  Spaces",
    "abstract": "Optimal experimental design seeks to determine the most informative\nallocation of experiments to infer an unknown statistical quantity. In this\nwork, we investigate the optimal design of experiments for {\\em estimation of\nlinear functionals in reproducing kernel Hilbert spaces (RKHSs)}. This problem\nhas been extensively studied in the linear regression setting under an\nestimability condition, which allows estimating parameters without bias. We\ngeneralize this framework to RKHSs, and allow for the linear functional to be\nonly approximately inferred, i.e., with a fixed bias. This scenario captures\nmany important modern applications, such as estimation of gradient maps,\nintegrals, and solutions to differential equations. We provide algorithms for\nconstructing bias-aware designs for linear functionals. We derive\nnon-asymptotic confidence sets for fixed and adaptive designs under\nsub-Gaussian noise, enabling us to certify estimation with bounded error with\nhigh probability.",
    "descriptor": "",
    "authors": [
      "Mojm\u00edr Mutn\u00fd",
      "Andreas Krause"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13627"
  },
  {
    "id": "arXiv:2205.13629",
    "title": "Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic  Segmentation",
    "abstract": "Robust environment perception for autonomous vehicles is a tremendous\nchallenge, which makes a diverse sensor set with e.g. camera, lidar and radar\ncrucial. In the process of understanding the recorded sensor data, 3D semantic\nsegmentation plays an important role. Therefore, this work presents a\npyramid-based deep fusion architecture for lidar and camera to improve 3D\nsemantic segmentation of traffic scenes. Individual sensor backbones extract\nfeature maps of camera images and lidar point clouds. A novel Pyramid Fusion\nBackbone fuses these feature maps at different scales and combines the\nmultimodal features in a feature pyramid to compute valuable multimodal,\nmulti-scale features. The Pyramid Fusion Head aggregates these pyramid features\nand further refines them in a late fusion step, incorporating the final\nfeatures of the sensor backbones. The approach is evaluated on two challenging\noutdoor datasets and different fusion strategies and setups are investigated.\nIt outperforms recent range view based lidar approaches as well as all so far\nproposed fusion strategies and architectures.",
    "descriptor": "\nComments: conditionally accepted at IEEE IV 2022, 7 pages, 4 figures, 5 tables\n",
    "authors": [
      "Hannah Schieber",
      "Fabian Duerr",
      "Torsten Schoen",
      "J\u00fcrgen Beyerer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13629"
  },
  {
    "id": "arXiv:2205.13634",
    "title": "BagFlip: A Certified Defense against Data Poisoning",
    "abstract": "Machine learning models are vulnerable to data-poisoning attacks, in which an\nattacker maliciously modifies the training set to change the prediction of a\nlearned model. In a trigger-less attack, the attacker can modify the training\nset but not the test inputs, while in a backdoor attack the attacker can also\nmodify test inputs. Existing model-agnostic defense approaches either cannot\nhandle backdoor attacks or do not provide effective certificates (i.e., a proof\nof a defense). We present BagFlip, a model-agnostic certified approach that can\neffectively defend against both trigger-less and backdoor attacks. We evaluate\nBagFlip on image classification and malware detection datasets. BagFlip is\nequal to or more effective than the state-of-the-art approaches for\ntrigger-less attacks and more effective than the state-of-the-art approaches\nfor backdoor attacks.",
    "descriptor": "",
    "authors": [
      "Yuhao Zhang",
      "Aws Albarghouthi",
      "Loris D'Antoni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13634"
  },
  {
    "id": "arXiv:2205.13635",
    "title": "RIGID: Robust Linear Regression with Missing Data",
    "abstract": "We present a robust framework to perform linear regression with missing\nentries in the features. By considering an elliptical data distribution, and\nspecifically a multivariate normal model, we are able to conditionally\nformulate a distribution for the missing entries and present a robust\nframework, which minimizes the worst case error caused by the uncertainty about\nthe missing data. We show that the proposed formulation, which naturally takes\ninto account the dependency between different variables, ultimately reduces to\na convex program, for which a customized and scalable solver can be delivered.\nIn addition to a detailed analysis to deliver such solver, we also asymptoticly\nanalyze the behavior of the proposed framework, and present technical\ndiscussions to estimate the required input parameters. We complement our\nanalysis with experiments performed on synthetic, semi-synthetic, and real\ndata, and show how the proposed formulation improves the prediction accuracy\nand robustness, and outperforms the competing techniques.",
    "descriptor": "",
    "authors": [
      "Alireza Aghasi",
      "MohammadJavad Feizollahi",
      "Saeed Ghadimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13635"
  },
  {
    "id": "arXiv:2205.13636",
    "title": "Quark: Controllable Text Generation with Reinforced Unlearning",
    "abstract": "Large-scale language models often learn behaviors that are misaligned with\nuser expectations. Generated text may contain offensive or toxic language,\ncontain significant repetition, or be of a different sentiment than desired by\nthe user. We consider the task of unlearning these misalignments by fine-tuning\nthe language model on signals of what not to do. We introduce Quantized Reward\nKonditioning (Quark), an algorithm for optimizing a reward function that\nquantifies an (un)wanted property, while not straying too far from the original\nmodel. Quark alternates between (i) collecting samples with the current\nlanguage model, (ii) sorting them into quantiles based on reward, with each\nquantile identified by a reward token prepended to the language model's input,\nand (iii) using a standard language modeling loss on samples from each quantile\nconditioned on its reward token, while remaining nearby the original language\nmodel via a KL-divergence penalty. By conditioning on a high-reward token at\ngeneration time, the model generates text that exhibits less of the unwanted\nproperty. For unlearning toxicity, negative sentiment, and repetition, our\nexperiments show that Quark outperforms both strong baselines and\nstate-of-the-art reinforcement learning methods like PPO (Schulman et al.\n2017), while relying only on standard language modeling primitives.",
    "descriptor": "",
    "authors": [
      "Ximing Lu",
      "Sean Welleck",
      "Liwei Jiang",
      "Jack Hessel",
      "Lianhui Qin",
      "Peter West",
      "Prithviraj Ammanabrolu",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13636"
  },
  {
    "id": "arXiv:2205.13639",
    "title": "A Model Predictive Control Functional Continuous Time Bayesian Network  for Self-Management of Multiple Chronic Conditions",
    "abstract": "Multiple chronic conditions (MCC) are one of the biggest challenges of modern\ntimes. The evolution of MCC follows a complex stochastic process that is\ninfluenced by a variety of risk factors, ranging from pre-existing conditions\nto modifiable lifestyle behavioral factors (e.g. diet, exercise habits, tobacco\nuse, alcohol use, etc.) to non-modifiable socio-demographic factors (e.g., age,\ngender, education, marital status, etc.). People with MCC are at an increased\nrisk of new chronic conditions and mortality. This paper proposes a model\npredictive control functional continuous time Bayesian network, an online\nrecursive method to examine the impact of various lifestyle behavioral changes\non the emergence trajectories of MCC and generate strategies to minimize the\nrisk of progression of chronic conditions in individual patients. The proposed\nmethod is validated based on the Cameron county Hispanic cohort (CCHC) dataset,\nwhich has a total of 385 patients. The dataset examines the emergence of 5\nchronic conditions (diabetes, obesity, cognitive impairment, hyperlipidemia,\nand hypertension) based on four modifiable risk factors representing lifestyle\nbehaviors (diet, exercise habits, tobacco use, alcohol use) and four\nnon-modifiable risk factors, including socio-demographic information (age,\ngender, education, marital status). The proposed method is tested under\ndifferent scenarios (e.g., age group, the prior existence of MCC),\ndemonstrating the effective intervention strategies for improving the lifestyle\nbehavioral risk factors to offset MCC evolution.",
    "descriptor": "\nComments: Submitted for review in Artificial Intelligence in Medicine\n",
    "authors": [
      "Syed Hasib Akhter Faruqui",
      "Adel Alaeddini",
      "Jing Wang",
      "Susan P Fisher-Hoch",
      "Joseph B Mccormick",
      "Julian Carvajal Rico"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.13639"
  },
  {
    "id": "arXiv:2205.13640",
    "title": "Spatio-temporally separable non-linear latent factor learning: an  application to somatomotor cortex fMRI data",
    "abstract": "Functional magnetic resonance imaging (fMRI) data contain complex\nspatiotemporal dynamics, thus researchers have developed approaches that reduce\nthe dimensionality of the signal while extracting relevant and interpretable\ndynamics. Models of fMRI data that can perform whole-brain discovery of\ndynamical latent factors are understudied. The benefits of approaches such as\nlinear independent component analysis models have been widely appreciated,\nhowever, nonlinear extensions of these models present challenges in terms of\nidentification. Deep learning methods provide a way forward, but new methods\nfor efficient spatial weight-sharing are critical to deal with the high\ndimensionality of the data and the presence of noise. Our approach generalizes\nweight sharing to non-Euclidean neuroimaging data by first performing spectral\nclustering based on the structural and functional similarity between voxels.\nThe spectral clusters and their assignments can then be used as patches in an\nadapted multi-layer perceptron (MLP)-mixer model to share parameters among\ninput points. To encourage temporally independent latent factors, we use an\nadditional total correlation term in the loss. Our approach is evaluated on\ndata with multiple motor sub-tasks to assess whether the model captures\ndisentangled latent factors that correspond to each sub-task. Then, to assess\nthe latent factors we find further, we compare the spatial location of each\nlatent factor to the motor homunculus. Finally, we show that our approach\ncaptures task effects better than the current gold standard of source signal\nseparation, independent component analysis (ICA).",
    "descriptor": "\nComments: 12 pages, 3 figures\n",
    "authors": [
      "Eloy Geenjaar",
      "Amrit Kashyap",
      "Noah Lewis",
      "Robyn Miller",
      "Vince Calhoun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13640"
  },
  {
    "id": "arXiv:2205.13643",
    "title": "Differentiable solver for time-dependent deformation problems with  contact",
    "abstract": "We introduce a general differentiable solver for time-dependent deformation\nproblems with contact. Our approach uses a finite element discretization with a\nhigh-order time integrator coupled with the recently proposed incremental\npotential contact method for handling contact and friction forces to solve PDE-\nand ODE-constrained optimization problems on scenes with a complex geometry. It\nsupport static and dynamic problems, it support differentiation with respect to\nall physical parameters involved in the physical problem description, which\ninclude shape, material parameters, friction parameters and initial conditions.\nOur analytically derived adjoint formulation is efficient, with an overhead of\nnot more than 2 times the forward simulation, and shares many similarities with\nthe forward problem, allowing reusing large parts of the code of an existing\nforward simulator code.\nWe implement our approach on top of the open-source PolyFEM FE library, and\ndemonstrate the applicability of our solver to shape design, initial condition\noptimization, and material estimation on both simulated results and in physical\nvalidations.",
    "descriptor": "",
    "authors": [
      "Arvi Gjoka",
      "Zizhou Huang",
      "Davi Colli Tozoni",
      "Zachary Ferguson",
      "Teseo Schneider",
      "Daniele Panozzo",
      "Denis Zorin"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13643"
  },
  {
    "id": "arXiv:2205.13646",
    "title": "Machine and Deep Learning Applications to Mouse Dynamics for Continuous  User Authentication",
    "abstract": "Static authentication methods, like passwords, grow increasingly weak with\nadvancements in technology and attack strategies. Continuous authentication has\nbeen proposed as a solution, in which users who have gained access to an\naccount are still monitored in order to continuously verify that the user is\nnot an imposter who had access to the user credentials. Mouse dynamics is the\nbehavior of a users mouse movements and is a biometric that has shown great\npromise for continuous authentication schemes. This article builds upon our\nprevious published work by evaluating our dataset of 40 users using three\nmachine learning and deep learning algorithms. Two evaluation scenarios are\nconsidered: binary classifiers are used for user authentication, with the top\nperformer being a 1-dimensional convolutional neural network with a peak\naverage test accuracy of 85.73% across the top 10 users. Multi class\nclassification is also examined using an artificial neural network which\nreaches an astounding peak accuracy of 92.48% the highest accuracy we have seen\nfor any classifier on this dataset.",
    "descriptor": "",
    "authors": [
      "Nyle Siddiqui",
      "Rushit Dave",
      "Naeem Seliya",
      "Mounika Vanamala"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13646"
  },
  {
    "id": "arXiv:2205.13647",
    "title": "Learning to Reason with Neural Networks: Generalization, Unseen Data and  Boolean Measures",
    "abstract": "This paper considers the Pointer Value Retrieval (PVR) benchmark introduced\nin [ZRKB21], where a 'reasoning' function acts on a string of digits to produce\nthe label. More generally, the paper considers the learning of logical\nfunctions with gradient descent (GD) on neural networks. It is first shown that\nin order to learn logical functions with gradient descent on symmetric neural\nnetworks, the generalization error can be lower-bounded in terms of the\nnoise-stability of the target function, supporting a conjecture made in\n[ZRKB21]. It is then shown that in the distribution shift setting, when the\ndata withholding corresponds to freezing a single feature (referred to as\ncanonical holdout), the generalization error of gradient descent admits a tight\ncharacterization in terms of the Boolean influence for several relevant\narchitectures. This is shown on linear models and supported experimentally on\nother models such as MLPs and Transformers. In particular, this puts forward\nthe hypothesis that for such architectures and for learning logical functions\nsuch as PVR functions, GD tends to have an implicit bias towards low-degree\nrepresentations, which in turn gives the Boolean influence for the\ngeneralization error under quadratic loss.",
    "descriptor": "\nComments: 28 pages, 8 figures\n",
    "authors": [
      "Emmanuel Abbe",
      "Samy Bengio",
      "Elisabetta Cornacchia",
      "Jon Kleinberg",
      "Aryo Lotfi",
      "Maithra Raghu",
      "Chiyuan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13647"
  },
  {
    "id": "arXiv:2205.13648",
    "title": "A Unified Analysis of Federated Learning with Arbitrary Client  Participation",
    "abstract": "Federated learning (FL) faces challenges of intermittent client availability\nand computation/communication efficiency. As a result, only a small subset of\nclients can participate in FL at a given time. It is important to understand\nhow partial client participation affects convergence, but most existing works\nhave either considered idealized participation patterns or obtained results\nwith non-zero optimality error for generic patterns. In this paper, we provide\na unified convergence analysis for FL with arbitrary client participation. We\nfirst introduce a generalized version of federated averaging (FedAvg) that\namplifies parameter updates at an interval of multiple FL rounds. Then, we\npresent a novel analysis that captures the effect of client participation in a\nsingle term. By analyzing this term, we obtain convergence upper bounds for a\nwide range of participation patterns, including both non-stochastic and\nstochastic cases, which match either the lower bound of stochastic gradient\ndescent (SGD) or the state-of-the-art results in specific settings. We also\ndiscuss various insights, recommendations, and experimental results.",
    "descriptor": "",
    "authors": [
      "Shiqiang Wang",
      "Mingyue Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13648"
  },
  {
    "id": "arXiv:2205.13655",
    "title": "Mixed Federated Learning: Joint Decentralized and Centralized Learning",
    "abstract": "Federated learning (FL) enables learning from decentralized privacy-sensitive\ndata, with computations on raw data confined to take place at edge clients.\nThis paper introduces mixed FL, which incorporates an additional loss term\ncalculated at the coordinating server (while maintaining FL's private data\nrestrictions). There are numerous benefits. For example, additional datacenter\ndata can be leveraged to jointly learn from centralized (datacenter) and\ndecentralized (federated) training data and better match an expected inference\ndata distribution. Mixed FL also enables offloading some intensive computations\n(e.g., embedding regularization) to the server, greatly reducing communication\nand client computation load. For these and other mixed FL use cases, we present\nthree algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY\nGRADIENT TRANSFER. We state convergence bounds for each, and give intuition on\nwhich are suited to particular mixed FL problems. Finally we perform extensive\nexperiments on three tasks, demonstrating that mixed FL can blend training data\nto achieve an oracle's accuracy on an inference distribution, and can reduce\ncommunication and computation overhead by over 90%. Our experiments confirm\ntheoretical predictions of how algorithms perform under different mixed FL\nproblem settings.",
    "descriptor": "\nComments: 36 pages, 12 figures\n",
    "authors": [
      "Sean Augenstein",
      "Andrew Hard",
      "Lin Ning",
      "Karan Singhal",
      "Satyen Kale",
      "Kurt Partridge",
      "Rajiv Mathews"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13655"
  },
  {
    "id": "arXiv:2205.13656",
    "title": "Finite difference schemes for the parabolic $p$-Laplace equation",
    "abstract": "We propose a new finite difference scheme for the degenerate parabolic\nequation \\[ \\partial_t u - \\mbox{div}(|\\nabla u|^{p-2}\\nabla u) =f, \\quad p\\geq\n2. \\] Under the assumption that the data is H\\\"older continuous, we establish\nthe convergence of the explicit-in-time scheme for the Cauchy problem provided\na suitable stability type CFL-condition. An important advantage of our\napproach, is that the CFL-condition makes use of the regularity provided by the\nscheme to reduce the computational cost. In particular, for Lipschitz data, the\nCFL-condition is of the same order as for the heat equation and independent of\n$p$.",
    "descriptor": "\nComments: 17 pages, 1 figure\n",
    "authors": [
      "F\u00e9lix del Teso",
      "Erik Lindgren"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.13656"
  },
  {
    "id": "arXiv:2205.13658",
    "title": "On the Effect of Triadic Closure on Network Segregation",
    "abstract": "The tendency for individuals to form social ties with others who are similar\nto themselves, known as homophily, is one of the most robust sociological\nprinciples. Since this phenomenon can lead to patterns of interactions that\nsegregate people along different demographic dimensions, it can also lead to\ninequalities in access to information, resources, and opportunities. As we\nconsider potential interventions that might alleviate the effects of\nsegregation, we face the challenge that homophily constitutes a pervasive and\norganic force that is difficult to push back against. Designing effective\ninterventions can therefore benefit from identifying counterbalancing social\nprocesses that might be harnessed to work in opposition to segregation.\nIn this work, we show that triadic closure -- another common phenomenon that\nposits that individuals with a mutual connection are more likely to be\nconnected to one another -- can be one such process. In doing so, we challenge\na long-held belief that triadic closure and homophily work in tandem. By\nanalyzing several fundamental network models using popular integration\nmeasures, we demonstrate the desegregating potential of triadic closure. We\nfurther empirically investigate this effect on real-world dynamic networks,\nsurfacing observations that mirror our theoretical findings. We leverage these\ninsights to discuss simple interventions that can help reduce segregation in\nsettings that exhibit an interplay between triadic closure and homophily. We\nconclude with a discussion on qualitative implications for the design of\ninterventions in settings where individuals arrive in an online fashion, and\nthe designer can influence the initial set of connections.",
    "descriptor": "\nComments: To Appear in Proceedings of the 23rd ACM Conference on Economics and Computation (EC'22)\n",
    "authors": [
      "Rediet Abebe",
      "Nicole Immorlica",
      "Jon Kleinberg",
      "Brendan Lucier",
      "Ali Shirali"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2205.13658"
  },
  {
    "id": "arXiv:2205.13659",
    "title": "Backward Euler method for stochastic differential equations with  non-Lipschitz coefficients",
    "abstract": "We study the traditional backward Euler method for $m$-dimensional stochastic\ndifferential equations driven by fractional Brownian motion with Hurst\nparameter $H > 1/2$ whose drift coefficient satisfies the one-sided Lipschitz\ncondition. The backward Euler scheme is proved to be of order $1$ and this rate\nis optimal by showing the asymptotic error distribution result. Two numerical\nexperiments are performed to validate our claims about the optimality of the\nrate of convergence.",
    "descriptor": "",
    "authors": [
      "Hao Zhou",
      "Yaozhong Hu",
      "Yanghui Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13659"
  },
  {
    "id": "arXiv:2205.13660",
    "title": "Contextual Adapters for Personalized Speech Recognition in Neural  Transducers",
    "abstract": "Personal rare word recognition in end-to-end Automatic Speech Recognition\n(E2E ASR) models is a challenge due to the lack of training data. A standard\nway to address this issue is with shallow fusion methods at inference time.\nHowever, due to their dependence on external language models and the\ndeterministic approach to weight boosting, their performance is limited. In\nthis paper, we propose training neural contextual adapters for personalization\nin neural transducer based ASR models. Our approach can not only bias towards\nuser-defined words, but also has the flexibility to work with pretrained ASR\nmodels. Using an in-house dataset, we demonstrate that contextual adapters can\nbe applied to any general purpose pretrained ASR model to improve\npersonalization. Our method outperforms shallow fusion, while retaining\nfunctionality of the pretrained models by not altering any of the model\nweights. We further show that the adapter style training is superior to\nfull-fine-tuning of the ASR models on datasets with user-defined content.",
    "descriptor": "\nComments: Accepted at ICASSP 2022\n",
    "authors": [
      "Kanthashree Mysore Sathyendra",
      "Thejaswi Muniyappa",
      "Feng-Ju Chang",
      "Jing Liu",
      "Jinru Su",
      "Grant P. Strimel",
      "Athanasios Mouchtaris",
      "Siegfried Kunzmann"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13660"
  },
  {
    "id": "arXiv:2205.13669",
    "title": "An adaptive fuzzy sliding mode controller for nonlinear systems with  non-symmetric dead-zone and its application to an electro-hydraulic system",
    "abstract": "The dead-zone is one of the most common hard nonlinearities in industrial\nactuators and its presence may drastically compromise control systems stability\nand performance. In this work, an adaptive variable structure controller is\nproposed to deal with a class of uncertain nonlinear systems subject to a\nnon-symmetric dead-zone input. The adopted approach is primarily based on the\nsliding mode control methodology but enhanced by an adaptive fuzzy algorithm to\ncompensate the dead-zone. Using Lyapunov stability theory and Barbalat's lemma,\nthe convergence properties of the closed-loop system are analytically proven.\nIn order to illustrate the controller design methodology, an application of the\nproposed scheme to an electro-hydraulic system is introduced. The performance\nof the control system is evaluated by means of numerical simulations.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.13343\n",
    "authors": [
      "Wallace Moreira Bessa"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13669"
  },
  {
    "id": "arXiv:2205.13671",
    "title": "Transformer for Partial Differential Equations' Operator Learning",
    "abstract": "Data-driven learning of partial differential equations' solution operators\nhas recently emerged as a promising paradigm for approximating the underlying\nsolutions. The solution operators are usually parameterized by deep learning\nmodels that are built upon problem-specific inductive biases. An example is a\nconvolutional or a graph neural network that exploits the local grid structure\nwhere functions' values are sampled. The attention mechanism, on the other\nhand, provides a flexible way to implicitly exploit the patterns within inputs,\nand furthermore, relationship between arbitrary query locations and inputs. In\nthis work, we present an attention-based framework for data-driven operator\nlearning, which we term Operator Transformer (OFormer). Our framework is built\nupon self-attention, cross-attention, and a set of point-wise multilayer\nperceptrons (MLPs), and thus it makes few assumptions on the sampling pattern\nof the input function or query locations. We show that the proposed framework\nis competitive on standard benchmark problems and can flexibly be adapted to\nrandomly sampled input.",
    "descriptor": "",
    "authors": [
      "Zijie Li",
      "Kazem Meidani",
      "Amir Barati Farimani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13671"
  },
  {
    "id": "arXiv:2205.13673",
    "title": "Diffusion of Community Fact-Checked Misinformation on Twitter",
    "abstract": "The spread of misinformation on social media is a pressing societal problem\nthat platforms, policymakers, and researchers continue to grapple with. As a\ncountermeasure, recent works have proposed to employ non-expert fact-checkers\nin the crowd to fact-check social media content. While experimental studies\nsuggest that crowds might be able to accurately assess the veracity of social\nmedia content, an understanding of how crowd fact-checked (mis-)information\nspreads is missing. In this work, we empirically analyze the spread of\nmisleading vs. not misleading community fact-checked posts on social media. For\nthis purpose, we employ a unique dataset of community-created fact-checks from\nTwitter's \"Birdwatch\" platform and map them to resharing cascades on Twitter.\nDifferent from earlier studies analyzing the spread of misinformation listed on\nthird-party fact-checking websites (e.g., snopes.com), we find that community\nfact-checked misinformation is less viral than the truth. Specifically,\nmisleading posts are estimated to receive 36.85% fewer retweets than not\nmisleading posts. A possible explanation lies in differences in the\nfact-checking targets: community fact-checkers tend to fact-check posts from\ninfluential user accounts with many followers, while expert fact-checks tend to\ntarget posts that are shared by less influential users. We further demonstrate\nthat there are significant differences in virality across different sub-types\nof misinformation (e.g., factual errors, missing context, manipulated media).\nAltogether, our findings offer insights into how misleading vs. not misleading\nposts spread and highlight the crucial role of sample selection when studying\nmisinformation on social media.",
    "descriptor": "",
    "authors": [
      "Chiara Drolsbach",
      "Nicolas Pr\u00f6llochs"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.13673"
  },
  {
    "id": "arXiv:2205.13674",
    "title": "Global Normalization for Streaming Speech Recognition in a Modular  Framework",
    "abstract": "We introduce the Globally Normalized Autoregressive Transducer (GNAT) for\naddressing the label bias problem in streaming speech recognition. Our solution\nadmits a tractable exact computation of the denominator for the sequence-level\nnormalization. Through theoretical and empirical results, we demonstrate that\nby switching to a globally normalized model, the word error rate gap between\nstreaming and non-streaming speech-recognition models can be greatly reduced\n(by more than 50\\% on the Librispeech dataset). This model is developed in a\nmodular framework which encompasses all the common neural speech recognition\nmodels. The modularity of this framework enables controlled comparison of\nmodelling choices and creation of new models.",
    "descriptor": "",
    "authors": [
      "Ehsan Variani",
      "Ke Wu",
      "Michael Riley",
      "David Rybach",
      "Matt Shannon",
      "Cyril Allauzen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13674"
  },
  {
    "id": "arXiv:2205.13675",
    "title": "Reinforcement Learning Approach for Mapping Applications to  Dataflow-Based Coarse-Grained Reconfigurable Array",
    "abstract": "The Streaming Engine (SE) is a Coarse-Grained Reconfigurable Array which\nprovides programming flexibility and high-performance with energy efficiency.\nAn application program to be executed on the SE is represented as a combination\nof Synchronous Data Flow (SDF) graphs, where every instruction is represented\nas a node. Each node needs to be mapped to the right slot and array in the SE\nto ensure the correct execution of the program. This creates an optimization\nproblem with a vast and sparse search space for which finding a mapping\nmanually is impractical because it requires expertise and knowledge of the SE\nmicro-architecture. In this work we propose a Reinforcement Learning framework\nwith Global Graph Attention (GGA) module and output masking of invalid\nplacements to find and optimize instruction schedules. We use Proximal Policy\nOptimization in order to train a model which places operations into the SE\ntiles based on a reward function that models the SE device and its constraints.\nThe GGA module consists of a graph neural network and an attention module. The\ngraph neural network creates embeddings of the SDFs and the attention block is\nused to model sequential operation placement. We show results on how certain\nworkloads are mapped to the SE and the factors affecting mapping quality. We\nfind that the addition of GGA, on average, finds 10% better instruction\nschedules in terms of total clock cycles taken and masking improves reward\nobtained by 20%.",
    "descriptor": "\nComments: 10 pages, 12 figures\n",
    "authors": [
      "Andre Xian Ming Chang",
      "Parth Khopkar",
      "Bashar Romanous",
      "Abhishek Chaurasia",
      "Patrick Estep",
      "Skyler Windh",
      "Doug Vanesko",
      "Sheik Dawood Beer Mohideen",
      "Eugenio Culurciello"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13675"
  },
  {
    "id": "arXiv:2205.13676",
    "title": "Fast variable selection makes scalable Gaussian process BSS-ANOVA a  speedy and accurate choice for tabular and time series regression",
    "abstract": "Gaussian processes (GPs) are non-parametric regression engines with a long\nhistory. They are often overlooked in modern machine learning contexts because\nof scalability issues: regression for traditional GP kernels are\n$\\mathcal{O}(N^3)$ where $N$ is the size of the dataset. One of a number of\nscalable GP approaches is the Karhunen-Lo\\'eve (KL) decomposed kernel\nBSS-ANOVA, developed in 2009. It is $\\mathcal{O}(NP)$ in training and\n$\\mathcal{O}(P)$ per point in prediction, where $P$ is the number of terms in\nthe ANOVA / KL expansion. A new method of forward variable selection, quickly\nand effectively limits the number of terms, yielding a method with competitive\naccuracies, training and inference times for large tabular datasets. The new\nalgorithm balances model fidelity with model complexity using Bayesian and\nAkaike information criteria (BIC/AIC). The inference speed and accuracy makes\nthe method especially useful for modeling dynamic systems in a model-free\nmanner, by modeling the derivative in a dynamic system as a static problem,\nthen integrating the learned dynamics using a high-order scheme. The methods\nare demonstrated on a `Susceptible, Infected, Recovered' (SIR) toy problem,\nwith the transmissibility used as forcing function, along with the `Cascaded\nTanks' benchmark dataset. Comparisons on the static prediction of derivatives\nare made with a Random Forest and Residual Neural Network, while for the\ntimeseries prediction comparisons are made with LSTM and GRU recurrent neural\nnetworks. The GP outperforms the other methods in all modeling tasks on\naccuracy, while (in the case of the neural networks) performing many orders of\nmagnitude fewer calculations. For the SIR test, which involved prediction for a\nset of forcing functions qualitatively different from those appearing in the\ntraining set, the GP captured the correct dynamics while the neural networks\nfailed to do so.",
    "descriptor": "",
    "authors": [
      "David S. Mebane",
      "Kyle Hayes",
      "Ali Baheri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13676"
  },
  {
    "id": "arXiv:2205.13679",
    "title": "SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching",
    "abstract": "Recently, there have been significant interests in designing Graph Neural\nNetworks (GNNs) for seeded graph matching, which aims to match two (unlabeled)\ngraphs using only topological information and a small set of seeds. However,\nmost previous GNN architectures for seeded graph matching employ a\nsemi-supervised approach, which learns from only the seed set in a single pair\nof graphs, and therefore does not attempt to learn from many training\nexamples/graphs to best match future unseen graphs. In contrast, this paper is\nthe first to propose a supervised approach for seeded graph matching, which had\nso far only been used for seedless graph matching. Our proposed SeedGNN\narchitecture employs a number of novel design choices that are inspired by\ntheoretical studies of seeded graph matching. First, SeedGNN can easily learn\nthe capability of counting and using witnesses of different hops, in a way that\ncan be generalized to graphs with different sizes. Second, SeedGNN can use\neasily-matched pairs as new seeds to percolate and match other nodes. We\nevaluate SeedGNN on both synthetic and real graphs, and demonstrate significant\nperformance improvement over both non-learning and learning algorithms in the\nexisting literature. Further, our experiments confirm that the knowledge\nlearned by SeedGNN from training graphs can be generalized to test graphs with\ndifferent sizes and categories.",
    "descriptor": "",
    "authors": [
      "Liren Yu",
      "Jiaming Xu",
      "Xiaojun Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13679"
  },
  {
    "id": "arXiv:2205.13680",
    "title": "Membership Inference Attack Using Self Influence Functions",
    "abstract": "Member inference (MI) attacks aim to determine if a specific data sample was\nused to train a machine learning model. Thus, MI is a major privacy threat to\nmodels trained on private sensitive data, such as medical records. In MI\nattacks one may consider the black-box settings, where the model's parameters\nand activations are hidden from the adversary, or the white-box case where they\nare available to the attacker. In this work, we focus on the latter and present\na novel MI attack for it that employs influence functions, or more specifically\nthe samples' self-influence scores, to perform the MI prediction. We evaluate\nour attack on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, using versatile\narchitectures such as AlexNet, ResNet, and DenseNet. Our attack method achieves\nnew state-of-the-art results for both training with and without data\naugmentations. Code is available at\nhttps://github.com/giladcohen/sif_mi_attack.",
    "descriptor": "",
    "authors": [
      "Gilad Cohen",
      "Raja Giryes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13680"
  },
  {
    "id": "arXiv:2205.13681",
    "title": "Sequential Nature of Recommender Systems Disrupts the Evaluation Process",
    "abstract": "Datasets are often generated in a sequential manner, where the previous\nsamples and intermediate decisions or interventions affect subsequent samples.\nThis is especially prominent in cases where there are significant human-AI\ninteractions, such as in recommender systems. To characterize the importance of\nthis relationship across samples, we propose to use adversarial attacks on\npopular evaluation processes. We present sequence-aware boosting attacks and\nprovide a lower bound on the amount of extra information that can be exploited\nfrom a confidential test set solely based on the order of the observed data. We\nuse real and synthetic data to test our methods and show that the evaluation\nprocess on the MovieLense-100k dataset can be affected by $\\sim1\\%$ which is\nimportant when considering the close competition. Codes are publicly available.",
    "descriptor": "\nComments: To Appear in Third International Workshop on Algorithmic Bias in Search and Recommendation (Bias 2022)\n",
    "authors": [
      "Ali Shirali"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13681"
  },
  {
    "id": "arXiv:2205.13682",
    "title": "ANISE: Assembly-based Neural Implicit Surface rEconstruction",
    "abstract": "We present ANISE, a method that reconstructs a 3D shape from partial\nobservations (images or sparse point clouds) using a part-aware neural implicit\nshape representation. It is formulated as an assembly of neural implicit\nfunctions, each representing a different shape part. In contrast to previous\napproaches, the prediction of this representation proceeds in a coarse-to-fine\nmanner. Our network first predicts part transformations which are associated\nwith part neural implicit functions conditioned on those transformations. The\npart implicit functions can then be combined into a single, coherent shape,\nenabling part-aware shape reconstructions from images and point clouds. Those\nreconstructions can be obtained in two ways: (i) by directly decoding combining\nthe refined part implicit functions; or (ii) by using part latents to query\nsimilar parts in a part database and assembling them in a single shape. We\ndemonstrate that, when performing reconstruction by decoding part\nrepresentations into implicit functions, our method achieves state-of-the-art\npart-aware reconstruction results from both images and sparse point clouds.\nWhen reconstructing shapes by assembling parts queried from a dataset, our\napproach significantly outperforms traditional shape retrieval methods even\nwhen significantly restricting the size of the shape database. We present our\nresults in well-known sparse point cloud reconstruction and single-view\nreconstruction benchmarks.",
    "descriptor": "\nComments: 8 pages, 5 figures, 4 tables\n",
    "authors": [
      "Dmitry Petrov",
      "Matheus Gadelha",
      "Radomir Mech",
      "Evangelos Kalogerakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13682"
  },
  {
    "id": "arXiv:2205.13683",
    "title": "Looking at Creative ML Blindspots with a Sociological Lens",
    "abstract": "How can researchers from the creative ML/AI community and sociology of\nculture engage in fruitful collaboration? How do researchers from both fields\nthink (differently) about creativity and the production of creative work? While\nthe ML community considers creativity as a matter of technical expertise and\nacumen, social scientists have emphasized the role of embeddedness in cultural\nproduction. This perspective aims to bridge both disciplines and proposes a\nconceptual and methodological toolkit for collaboration. We provide a\nsystematic review of recent research in both fields and offer three\nperspectives around which to structure interdisciplinary research on cultural\nproduction: people, processes, and products. We thereby provide necessary\ngrounding work to support multidisciplinary researchers to navigate conceptual\nand methodological hurdles in their collaboration. Our research will be of\ninterest to ML researchers and sociologists interested in creativity that aim\nto conduct innovative research by bridging both fields.",
    "descriptor": "",
    "authors": [
      "Katharina Burgdorf",
      "Negar Rostamzadeh",
      "Ramya Srinivasan",
      "Jennifer Lena"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.13683"
  },
  {
    "id": "arXiv:2205.13685",
    "title": "Adversarial attacks and defenses in Speaker Recognition Systems: A  survey",
    "abstract": "Speaker recognition has become very popular in many application scenarios,\nsuch as smart homes and smart assistants, due to ease of use for remote control\nand economic-friendly features. The rapid development of SRSs is inseparable\nfrom the advancement of machine learning, especially neural networks. However,\nprevious work has shown that machine learning models are vulnerable to\nadversarial attacks in the image domain, which inspired researchers to explore\nadversarial attacks and defenses in Speaker Recognition Systems (SRS).\nUnfortunately, existing literature lacks a thorough review of this topic. In\nthis paper, we fill this gap by performing a comprehensive survey on\nadversarial attacks and defenses in SRSs. We first introduce the basics of SRSs\nand concepts related to adversarial attacks. Then, we propose two sets of\ncriteria to evaluate the performance of attack methods and defense methods in\nSRSs, respectively. After that, we provide taxonomies of existing attack\nmethods and defense methods, and further review them by employing our proposed\ncriteria. Finally, based on our review, we find some open issues and further\nspecify a number of future directions to motivate the research of SRSs\nsecurity.",
    "descriptor": "\nComments: 38pages, 2 figures, 2 tables. Journal of Systems Architecture,2022\n",
    "authors": [
      "Jiahe Lan",
      "Rui Zhang",
      "Zheng Yan",
      "Jie Wang",
      "Yu Chen",
      "Ronghui Hou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.13685"
  },
  {
    "id": "arXiv:2205.13689",
    "title": "Safety Aware Changepoint Detection for Piecewise i.i.d. Bandits",
    "abstract": "In this paper, we consider the setting of piecewise i.i.d. bandits under a\nsafety constraint. In this piecewise i.i.d. setting, there exists a finite\nnumber of changepoints where the mean of some or all arms change\nsimultaneously. We introduce the safety constraint studied in\n\\citet{wu2016conservative} to this setting such that at any round the\ncumulative reward is above a constant factor of the default action reward. We\npropose two actively adaptive algorithms for this setting that satisfy the\nsafety constraint, detect changepoints, and restart without the knowledge of\nthe number of changepoints or their locations. We provide regret bounds for our\nalgorithms and show that the bounds are comparable to their counterparts from\nthe safe bandit and piecewise i.i.d. bandit literature. We also provide the\nfirst matching lower bounds for this setting. Empirically, we show that our\nsafety-aware algorithms perform similarly to the state-of-the-art actively\nadaptive algorithms that do not satisfy the safety constraint.",
    "descriptor": "\nComments: Accepted for the 38th Conference on Uncertainty in Artificial Intelligence (UAI 2022)\n",
    "authors": [
      "Subhojyoti Mukherjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13689"
  },
  {
    "id": "arXiv:2205.13692",
    "title": "FedAvg with Fine Tuning: Local Updates Lead to Representation Learning",
    "abstract": "The Federated Averaging (FedAvg) algorithm, which consists of alternating\nbetween a few local stochastic gradient updates at client nodes, followed by a\nmodel averaging update at the server, is perhaps the most commonly used method\nin Federated Learning. Notwithstanding its simplicity, several empirical\nstudies have illustrated that the output model of FedAvg, after a few\nfine-tuning steps, leads to a model that generalizes well to new unseen tasks.\nThis surprising performance of such a simple method, however, is not fully\nunderstood from a theoretical point of view. In this paper, we formally\ninvestigate this phenomenon in the multi-task linear representation setting. We\nshow that the reason behind generalizability of the FedAvg's output is its\npower in learning the common data representation among the clients' tasks, by\nleveraging the diversity among client data distributions via local updates. We\nformally establish the iteration complexity required by the clients for proving\nsuch result in the setting where the underlying shared representation is a\nlinear map. To the best of our knowledge, this is the first such result for any\nsetting. We also provide empirical evidence demonstrating FedAvg's\nrepresentation learning ability in federated image classification with\nheterogeneous data.",
    "descriptor": "",
    "authors": [
      "Liam Collins",
      "Hamed Hassani",
      "Aryan Mokhtari",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13692"
  },
  {
    "id": "arXiv:2205.13697",
    "title": "FedFormer: Contextual Federation with Attention in Reinforcement  Learning",
    "abstract": "A core issue in federated reinforcement learning is defining how to aggregate\ninsights from multiple agents into one. This is commonly done by taking the\naverage of each participating agent's model weights into one common model\n(FedAvg). We instead propose FedFormer, a novel federation strategy that\nutilizes Transformer Attention to contextually aggregate embeddings from models\noriginating from different learner agents. In so doing, we attentively weigh\ncontributions of other agents with respect to the current agent's environment\nand learned relationships, thus providing more effective and efficient\nfederation. We evaluate our methods on the Meta-World environment and find that\nour approach yields significant improvements over FedAvg and non-federated Soft\nActor Critique single agent methods. Our results compared to Soft Actor\nCritique show that FedFormer performs better while still abiding by the privacy\nconstraints of federated learning. In addition, we demonstrate nearly linear\nimprovements in effectiveness with increased agent pools in certain tasks. This\nis contrasted by FedAvg, which fails to make noticeable improvements when\nscaled.",
    "descriptor": "\nComments: Our source code can be found at this https URL Submitted to NeurIPS2022\n",
    "authors": [
      "Liam Hebert",
      "Lukasz Golab",
      "Pascal Poupart",
      "Robin Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13697"
  },
  {
    "id": "arXiv:2205.13699",
    "title": "Maximum Likelihood Training of Implicit Nonlinear Diffusion Models",
    "abstract": "Whereas diverse variations of diffusion models exist, expanding the linear\ndiffusion into a nonlinear diffusion process is investigated only by a few\nworks. The nonlinearity effect has been hardly understood, but intuitively,\nthere would be more promising diffusion patterns to optimally train the\ngenerative distribution towards the data distribution. This paper introduces\nsuch a data-adaptive and nonlinear diffusion process for score-based diffusion\nmodels. The proposed Implicit Nonlinear Diffusion Model (INDM) learns the\nnonlinear diffusion process by combining a normalizing flow and a diffusion\nprocess. Specifically, INDM implicitly constructs a nonlinear diffusion on the\n\\textit{data space} by leveraging a linear diffusion on the \\textit{latent\nspace} through a flow network. This flow network is the key to forming a\nnonlinear diffusion as the nonlinearity fully depends on the flow network. This\nflexible nonlinearity is what improves the learning curve of INDM to nearly MLE\ntraining, compared against the non-MLE training of DDPM++, which turns out to\nbe a special case of INDM with the identity flow. Also, training the nonlinear\ndiffusion empirically yields a sampling-friendly latent diffusion that the\nsample trajectory of INDM is closer to an optimal transport than the\ntrajectories of previous research. In experiments, INDM achieves the\nstate-of-the-art FID on CelebA.",
    "descriptor": "",
    "authors": [
      "Dongjun Kim",
      "Byeonghu Na",
      "Se Jung Kwon",
      "Dongsoo Lee",
      "Wanmo Kang",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13699"
  },
  {
    "id": "arXiv:2205.13700",
    "title": "ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge  Splitting",
    "abstract": "Graph Neural Networks (GNNs) have achieved enormous success in tackling\nanalytical problems on graph data. Most GNNs interpret nearly all the node\nconnections as inductive bias with feature smoothness, and implicitly assume\nstrong homophily on the observed graph. However, real-world networks are not\nalways homophilic, but sometimes exhibit heterophilic patterns where adjacent\nnodes share dissimilar attributes and distinct labels. Therefore,GNNs smoothing\nthe node proximity holistically may aggregate inconsistent information arising\nfrom both task-relevant and irrelevant connections. In this paper, we propose a\nnovel edge splitting GNN (ES-GNN) framework, which generalizes GNNs beyond\nhomophily by jointly partitioning network topology and disentangling node\nfeatures. Specifically, the proposed framework employs an interpretable\noperation to adaptively split the set of edges of the original graph into two\nexclusive sets indicating respectively the task-relevant and irrelevant\nrelations among nodes. The node features are then aggregated separately on\nthese two partial edge sets to produce disentangled representations, based on\nwhich a more accurate edge splitting can be attained later. Theoretically, we\nshow that our ES-GNN can be regarded as a solution to a graph denoising problem\nwith a disentangled smoothness assumption, which further illustrates our\nmotivations and interprets the improved generalization. Extensive experiments\nover 8 benchmark and 1 synthetic datasets demonstrate that ES-GNN not only\noutperforms the state-of-the-arts (including 8 GNN baselines), but also can be\nmore robust to adversarial graphs and alleviate the over-smoothing problem.",
    "descriptor": "",
    "authors": [
      "Jingwei Guo",
      "Kaizhu Huang",
      "Xinping Yi",
      "Rui Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13700"
  },
  {
    "id": "arXiv:2205.13702",
    "title": "R-HTDetector: Robust Hardware-Trojan Detection Based on Adversarial  Training",
    "abstract": "Hardware Trojans (HTs) have become a serious problem, and extermination of\nthem is strongly required for enhancing the security and safety of integrated\ncircuits. An effective solution is to identify HTs at the gate level via\nmachine learning techniques. However, machine learning has specific\nvulnerabilities, such as adversarial examples. In reality, it has been reported\nthat adversarial modified HTs greatly degrade the performance of a machine\nlearning-based HT detection method. Therefore, we propose a robust HT detection\nmethod using adversarial training (R-HTDetector). We formally describe the\nrobustness of R-HTDetector in modifying HTs. Our work gives the world-first\nadversarial training for HT detection with theoretical backgrounds. We show\nthrough experiments with Trust-HUB benchmarks that R-HTDetector overcomes\nadversarial examples while maintaining its original accuracy.",
    "descriptor": "",
    "authors": [
      "Kento Hasegawa",
      "Seira Hidano",
      "Kohei Nozawa",
      "Shinsaku Kiyomoto",
      "Nozomu Togawa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13702"
  },
  {
    "id": "arXiv:2205.13703",
    "title": "Why So Pessimistic? Estimating Uncertainties for Offline RL through  Ensembles, and Why Their Independence Matters",
    "abstract": "Motivated by the success of ensembles for uncertainty estimation in\nsupervised learning, we take a renewed look at how ensembles of $Q$-functions\ncan be leveraged as the primary source of pessimism for offline reinforcement\nlearning (RL). We begin by identifying a critical flaw in a popular algorithmic\nchoice used by many ensemble-based RL algorithms, namely the use of shared\npessimistic target values when computing each ensemble member's Bellman error.\nThrough theoretical analyses and construction of examples in toy MDPs, we\ndemonstrate that shared pessimistic targets can paradoxically lead to value\nestimates that are effectively optimistic. Given this result, we propose MSG, a\npractical offline RL algorithm that trains an ensemble of $Q$-functions with\nindependently computed targets based on completely separate networks, and\noptimizes a policy with respect to the lower confidence bound of predicted\naction values. Our experiments on the popular D4RL and RL Unplugged offline RL\nbenchmarks demonstrate that on challenging domains such as antmazes, MSG with\ndeep ensembles surpasses highly well-tuned state-of-the-art methods by a wide\nmargin. Additionally, through ablations on benchmarks domains, we verify the\ncritical significance of using independently trained $Q$-functions, and study\nthe role of ensemble size. Finally, as using separate networks per ensemble\nmember can become computationally costly with larger neural network\narchitectures, we investigate whether efficient ensemble approximations\ndeveloped for supervised learning can be similarly effective, and demonstrate\nthat they do not match the performance and robustness of MSG with separate\nnetworks, highlighting the need for new efforts into efficient uncertainty\nestimation directed at RL.",
    "descriptor": "\nComments: Our codebase can be found at this https URL\n",
    "authors": [
      "Seyed Kamyar Seyed Ghasemipour",
      "Shixiang Shane Gu",
      "Ofir Nachum"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13703"
  },
  {
    "id": "arXiv:2205.13705",
    "title": "A Decentralized Collaborative Learning Framework Across Heterogeneous  Devices for Personalized Predictive Analytics",
    "abstract": "In this paper, we propose a Similarity-based Decentralized Knowledge\nDistillation (SD-Dist) framework for collaboratively learning heterogeneous\ndeep models on decentralized devices. By introducing a preloaded reference\ndataset, SD-Dist enables all participant devices to identify similar users and\ndistil knowledge from them without any assumptions on a fixed model\narchitecture. In addition, none of these operations will reveal any sensitive\ninformation like personal data and model parameters. Extensive experimental\nresults on three real-life datasets show that SD-Dist can achieve competitive\nperformance with less compute resources, while ensuring model heterogeneity and\nprivacy. As revealed in our experiments, our framework also enhances the\nresultant models' robustness when users' data is sparse and diverse.",
    "descriptor": "",
    "authors": [
      "Guanhua Ye",
      "Hongzhi Yin",
      "Tong Chen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13705"
  },
  {
    "id": "arXiv:2205.13707",
    "title": "A Hybrid Josephson Transmission Line and Passive Transmission Line  Routing Framework for Single Flux Quantum Logic",
    "abstract": "The Single Flux Quantum (SFQ) logic family is a novel digital logic as it\nprovides ultra-fast and energy-efficient circuits. For large-scale SFQ circuit\ndesign, specialized electronic design automation (EDA) tools are required due\nto the differences in logic type, timing constraints and circuit architecture,\nin contrast to the CMOS logic. In order to improve the overall performance of\nan SFQ circuit, an efficient routing algorithm should be applied during the\nlayout design to perform accurate timing adjustment for fixing hold violations\nand optimizing critical paths. Thus, a hybrid Josephson transmission line and\npassive transmission line routing framework is proposed. It consists of four\nmain modules and an exploration of the potential timing performance based on\nthe given layout placement. The proposed routing tool is demonstrated on seven\ntestbench circuits. The obtained results demonstrate that the operating\nfrequency is greatly improved, and all the hold violations are eliminated for\neach circuit.",
    "descriptor": "",
    "authors": [
      "Shucheng Yang",
      "Xiaoping Gao",
      "Ruoting Yang",
      "Jie Ren",
      "Zhen Wang"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.13707"
  },
  {
    "id": "arXiv:2205.13708",
    "title": "HiJoNLP at SemEval-2022 Task 2: Detecting Idiomaticity of Multiword  Expressions using Multilingual Pretrained Language Models",
    "abstract": "This paper describes an approach to detect idiomaticity only from the\ncontextualized representation of a MWE over multilingual pretrained language\nmodels. Our experiments find that larger models are usually more effective in\nidiomaticity detection. However, using a higher layer of the model may not\nguarantee a better performance. In multilingual scenarios, the convergence of\ndifferent languages are not consistent and rich-resource languages have big\nadvantages over other languages.",
    "descriptor": "",
    "authors": [
      "Minghuan Tan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13708"
  },
  {
    "id": "arXiv:2205.13709",
    "title": "DP-PCA: Statistically Optimal and Differentially Private PCA",
    "abstract": "We study the canonical statistical task of computing the principal component\nfrom $n$ i.i.d.~data in $d$ dimensions under\n$(\\varepsilon,\\delta)$-differential privacy. Although extensively studied in\nliterature, existing solutions fall short on two key aspects: ($i$) even for\nGaussian data, existing private algorithms require the number of samples $n$ to\nscale super-linearly with $d$, i.e., $n=\\Omega(d^{3/2})$, to obtain non-trivial\nresults while non-private PCA requires only $n=O(d)$, and ($ii$) existing\ntechniques suffer from a non-vanishing error even when the randomness in each\ndata point is arbitrarily small. We propose DP-PCA, which is a single-pass\nalgorithm that overcomes both limitations. It is based on a private minibatch\ngradient ascent method that relies on {\\em private mean estimation}, which adds\nminimal noise required to ensure privacy by adapting to the variance of a given\nminibatch of gradients. For sub-Gaussian data, we provide nearly optimal\nstatistical error rates even for $n=\\tilde O(d)$. Furthermore, we provide a\nlower bound showing that sub-Gaussian style assumption is necessary in\nobtaining the optimal error rate.",
    "descriptor": "",
    "authors": [
      "Xiyang Liu",
      "Weihao Kong",
      "Prateek Jain",
      "Sewoong Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13709"
  },
  {
    "id": "arXiv:2205.13710",
    "title": "Privacy of Noisy Stochastic Gradient Descent: More Iterations without  More Privacy Loss",
    "abstract": "A central issue in machine learning is how to train models on sensitive user\ndata. Industry has widely adopted a simple algorithm: Stochastic Gradient\nDescent with noise (a.k.a. Stochastic Gradient Langevin Dynamics). However,\nfoundational theoretical questions about this algorithm's privacy loss remain\nopen -- even in the seemingly simple setting of smooth convex losses over a\nbounded domain. Our main result resolves these questions: for a large range of\nparameters, we characterize the differential privacy up to a constant factor.\nThis result reveals that all previous analyses for this setting have the wrong\nqualitative behavior. Specifically, while previous privacy analyses increase ad\ninfinitum in the number of iterations, we show that after a small burn-in\nperiod, running SGD longer leaks no further privacy.\nOur analysis departs completely from previous approaches based on fast\nmixing, instead using techniques based on optimal transport (namely, Privacy\nAmplification by Iteration) and the Sampled Gaussian Mechanism (namely, Privacy\nAmplification by Sampling). Our techniques readily extend to other settings,\ne.g., strongly convex losses, non-uniform stepsizes, arbitrary batch sizes, and\nrandom or cyclic choice of batches.",
    "descriptor": "",
    "authors": [
      "Jason M. Altschuler",
      "Kunal Talwar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13710"
  },
  {
    "id": "arXiv:2205.13711",
    "title": "Incorporating the Barzilai-Borwein Adaptive Step Size into Sugradient  Methods for Deep Network Training",
    "abstract": "In this paper, we incorporate the Barzilai-Borwein step size into gradient\ndescent methods used to train deep networks. This allows us to adapt the\nlearning rate using a two-point approximation to the secant equation which\nquasi-Newton methods are based upon. Moreover, the adaptive learning rate\nmethod presented here is quite general in nature and can be applied to widely\nused gradient descent approaches such as Adagrad and RMSprop. We evaluate our\nmethod using standard example network architectures on widely available\ndatasets and compare against alternatives elsewhere in the literature. In our\nexperiments, our adaptive learning rate shows a smoother and faster convergence\nthan that exhibited by the alternatives, with better or comparable performance.",
    "descriptor": "",
    "authors": [
      "Antonio Robles-Kelly",
      "Asef Nazari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13711"
  },
  {
    "id": "arXiv:2205.13713",
    "title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences",
    "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension\nwhile exhibiting regularities and order in the temporal dimension. Therefore,\nexisting grid based convolutions for conventional video processing cannot be\ndirectly applied to spatio-temporal modeling of raw point cloud sequences. In\nthis paper, we propose a point spatio-temporal (PST) convolution to achieve\ninformative representations of point cloud sequences. The proposed PST\nconvolution first disentangles space and time in point cloud sequences. Then, a\nspatial convolution is employed to capture the local structure of points in the\n3D space, and a temporal convolution is used to model the dynamics of the\nspatial regions along the time dimension. Furthermore, we incorporate the\nproposed PST convolution into a deep network, namely PSTNet, to extract\nfeatures of point cloud sequences in a hierarchical manner. Extensive\nexperiments on widely-used 3D action recognition and 4D semantic segmentation\ndatasets demonstrate the effectiveness of PSTNet to model point cloud\nsequences.",
    "descriptor": "\nComments: Accepted to ICLR2021\n",
    "authors": [
      "Hehe Fan",
      "Xin Yu",
      "Yuhang Ding",
      "Yi Yang",
      "Mohan Kankanhalli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13713"
  },
  {
    "id": "arXiv:2205.13714",
    "title": "Distributed Gaussian Process Based Cooperative Visual Pursuit Control  for Drone Networks",
    "abstract": "In this paper, we propose a control law for camera-equipped drone networks to\npursue a target rigid body with unknown motion based on distributed Gaussian\nprocess. First, we consider the situation where each drone has its own dataset,\nand learned the unknown target motion in a distributed manner. Second, we\npropose a control law using the distributed Gaussian processes, and show that\nthe estimation and control errors are ultimately bounded. Furthermore, the\neffectiveness of the proposed method is verified by using simulations and\nexperiments with actual drones.",
    "descriptor": "\nComments: This work has been accepted to IFAC for publication under a Creative Commons Licence CC-BY-NC-ND\n",
    "authors": [
      "Makoto Saito",
      "Junya Yamauchi",
      "Tesshu Fujinami",
      "Marco Omainska",
      "Masayuki Fujita"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13714"
  },
  {
    "id": "arXiv:2205.13717",
    "title": "Hazard Gradient Penalty for Survival Analysis",
    "abstract": "Survival analysis appears in various fields such as medicine, economics,\nengineering, and business. Recent studies showed that the Ordinary Differential\nEquation (ODE) modeling framework unifies many existing survival models while\nthe framework is flexible and widely applicable. However, naively applying the\nODE framework to survival analysis problems may model fiercely changing density\nfunction which may worsen the model's performance. Though we can apply L1 or L2\nregularizers to the ODE model, their effect on the ODE modeling framework is\nbarely known. In this paper, we propose hazard gradient penalty (HGP) to\nenhance the performance of a survival analysis model. Our method imposes\nconstraints on local data points by regularizing the gradient of hazard\nfunction with respect to the data point. Our method applies to any survival\nanalysis model including the ODE modeling framework and is easy to implement.\nWe theoretically show that our method is related to minimizing the KL\ndivergence between the density function at a data point and that of the\nneighborhood points. Experimental results on three public benchmarks show that\nour approach outperforms other regularization methods.",
    "descriptor": "\nComments: 9 pages, 2 figures\n",
    "authors": [
      "Seungjae Jung",
      "Kyung-Min Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13717"
  },
  {
    "id": "arXiv:2205.13718",
    "title": "Off-Beat Multi-Agent Reinforcement Learning",
    "abstract": "We investigate model-free multi-agent reinforcement learning (MARL) in\nenvironments where off-beat actions are prevalent, i.e., all actions have\npre-set execution durations. During execution durations, the environment\nchanges are influenced by, but not synchronised with, action execution. Such a\nsetting is ubiquitous in many real-world problems. However, most MARL methods\nassume actions are executed immediately after inference, which is often\nunrealistic and can lead to catastrophic failure for multi-agent coordination\nwith off-beat actions. In order to fill this gap, we develop an algorithmic\nframework for MARL with off-beat actions. We then propose a novel episodic\nmemory, LeGEM, for model-free MARL algorithms. LeGEM builds agents' episodic\nmemories by utilizing agents' individual experiences. It boosts multi-agent\nlearning by addressing the challenging temporal credit assignment problem\nraised by the off-beat actions via our novel reward redistribution scheme,\nalleviating the issue of non-Markovian reward. We evaluate LeGEM on various\nmulti-agent scenarios with off-beat actions, including Stag-Hunter Game, Quarry\nGame, Afforestation Game, and StarCraft II micromanagement tasks. Empirical\nresults show that LeGEM significantly boosts multi-agent coordination and\nachieves leading performance and improved sample efficiency.",
    "descriptor": "",
    "authors": [
      "Wei Qiu",
      "Weixun Wang",
      "Rundong Wang",
      "Bo An",
      "Yujing Hu",
      "Svetlana Obraztsova",
      "Zinovi Rabinovich",
      "Jianye Hao",
      "Yingfeng Chen",
      "Changjie Fan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13718"
  },
  {
    "id": "arXiv:2205.13720",
    "title": "Effective Abstract Reasoning with Dual-Contrast Network",
    "abstract": "As a step towards improving the abstract reasoning capability of machines, we\naim to solve Raven's Progressive Matrices (RPM) with neural networks, since\nsolving RPM puzzles is highly correlated with human intelligence. Unlike\nprevious methods that use auxiliary annotations or assume hidden rules to\nproduce appropriate feature representation, we only use the ground truth answer\nof each question for model learning, aiming for an intelligent agent to have a\nstrong learning capability with a small amount of supervision. Based on the RPM\nproblem formulation, the correct answer filled into the missing entry of the\nthird row/column has to best satisfy the same rules shared between the first\ntwo rows/columns. Thus we design a simple yet effective Dual-Contrast Network\n(DCNet) to exploit the inherent structure of RPM puzzles. Specifically, a rule\ncontrast module is designed to compare the latent rules between the filled\nrow/column and the first two rows/columns; a choice contrast module is designed\nto increase the relative differences between candidate choices. Experimental\nresults on the RAVEN and PGM datasets show that DCNet outperforms the\nstate-of-the-art methods by a large margin of 5.77%. Further experiments on few\ntraining samples and model generalization also show the effectiveness of DCNet.\nCode is available at https://github.com/visiontao/dcnet.",
    "descriptor": "\nComments: Published on ICLR 2021\n",
    "authors": [
      "Tao Zhuo",
      "Mohan Kankanhalli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13720"
  },
  {
    "id": "arXiv:2205.13722",
    "title": "Can Foundation Models Help Us Achieve Perfect Secrecy?",
    "abstract": "A key promise of machine learning is the ability to assist users with\npersonal tasks. Because the personal context required to make accurate\npredictions is often sensitive, we require systems that protect privacy. A gold\nstandard privacy-preserving system will satisfy perfect secrecy, meaning that\ninteractions with the system provably reveal no additional private information\nto adversaries. This guarantee should hold even as we perform multiple personal\ntasks over the same underlying data. However, privacy and quality appear to be\nin tension in existing systems for personal tasks. Neural models typically\nrequire lots of training to perform well, while individual users typically hold\na limited scale of data, so the systems propose to learn from the aggregate\ndata of multiple users. This violates perfect secrecy and instead, in the last\nfew years, academics have defended these solutions using statistical notions of\nprivacy -- i.e., the probability of learning private information about a user\nshould be reasonably low. Given the vulnerabilities of these solutions, we\nexplore whether the strong perfect secrecy guarantee can be achieved using\nrecent zero-to-few sample adaptation techniques enabled by foundation models.\nIn response, we propose FOCUS, a framework for personal tasks. Evaluating on\npopular privacy benchmarks, we find the approach, satisfying perfect secrecy,\ncompetes with strong collaborative learning baselines on 6 of 7 tasks. We\nempirically analyze the proposal, highlighting the opportunities and\nlimitations across task types, and model inductive biases and sizes.",
    "descriptor": "\nComments: Keywords: privacy, large models, few-sample adaptation\n",
    "authors": [
      "Simran Arora",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13722"
  },
  {
    "id": "arXiv:2205.13723",
    "title": "DLTTA: Dynamic Learning Rate for Test-time Adaptation on Cross-domain  Medical Images",
    "abstract": "Test-time adaptation (TTA) has increasingly been an important topic to\nefficiently tackle the cross-domain distribution shift at test time for medical\nimages from different institutions. Previous TTA methods have a common\nlimitation of using a fixed learning rate for all the test samples. Such a\npractice would be sub-optimal for TTA, because test data may arrive\nsequentially therefore the scale of distribution shift would change frequently.\nTo address this problem, we propose a novel dynamic learning rate adjustment\nmethod for test-time adaptation, called DLTTA, which dynamically modulates the\namount of weights update for each test image to account for the differences in\ntheir distribution shift. Specifically, our DLTTA is equipped with a memory\nbank based estimation scheme to effectively measure the discrepancy of a given\ntest sample. Based on this estimated discrepancy, a dynamic learning rate\nadjustment strategy is then developed to achieve a suitable degree of\nadaptation for each test sample. The effectiveness and general applicability of\nour DLTTA is extensively demonstrated on three tasks including retinal optical\ncoherence tomography (OCT) segmentation, histopathological image\nclassification, and prostate 3D MRI segmentation. Our method achieves effective\nand fast test-time adaptation with consistent performance improvement over\ncurrent state-of-the-art test-time adaptation methods. Code is available at:\nhttps://github.com/med-air/DLTTA.",
    "descriptor": "",
    "authors": [
      "Hongzheng Yang",
      "Cheng Chen",
      "Meirui Jiang",
      "Quande Liu",
      "Jianfeng Cao",
      "Pheng Ann Heng",
      "Qi Dou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13723"
  },
  {
    "id": "arXiv:2205.13724",
    "title": "V-Doc : Visual questions answers with Documents",
    "abstract": "We propose V-Doc, a question-answering tool using document images and PDF,\nmainly for researchers and general non-deep learning experts looking to\ngenerate, process, and understand the document visual question answering tasks.\nThe V-Doc supports generating and using both extractive and abstractive\nquestion-answer pairs using documents images. The extractive QA selects a\nsubset of tokens or phrases from the document contents to predict the answers,\nwhile the abstractive QA recognises the language in the content and generates\nthe answer based on the trained model. Both aspects are crucial to\nunderstanding the documents, especially in an image format. We include a\ndetailed scenario of question generation for the abstractive QA task. V-Doc\nsupports a wide range of datasets and models, and is highly extensible through\na declarative, framework-agnostic platform.",
    "descriptor": "\nComments: Accepted by CVPR 2022 demo track; The first two authors contributed equally to this work\n",
    "authors": [
      "Yihao Ding",
      "Zhe Huang",
      "Runlin Wang",
      "Yanhang Zhang",
      "Xianru Chen",
      "Yuzhong Ma",
      "Hyunsuk Chung",
      "Soyeon Caren Han"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13724"
  },
  {
    "id": "arXiv:2205.13725",
    "title": "Low-Degree Polynomials Extract from Local Sources",
    "abstract": "We continue a line of work on extracting random bits from weak sources that\nare generated by simple processes. We focus on the model of locally samplable\nsources, where each bit in the source depends on a small number of (hidden)\nuniformly random input bits. Also known as local sources, this model was\nintroduced by De and Watson (TOCT 2012) and Viola (SICOMP 2014), and is closely\nrelated to sources generated by $\\mathsf{AC}^0$ circuits and bounded-width\nbranching programs. In particular, extractors for local sources also work for\nsources generated by these classical computational models.\nDespite being introduced a decade ago, little progress has been made on\nimproving the entropy requirement for extracting from local sources. The\ncurrent best explicit extractors require entropy $n^{1/2}$, and follow via a\nreduction to affine extractors. To start, we prove a barrier showing that one\ncannot hope to improve this entropy requirement via a black-box reduction of\nthis form. In particular, new techniques are needed.\nIn our main result, we seek to answer whether low-degree polynomials (over\n$\\mathbb{F}_2$) hold potential for breaking this barrier. We answer this\nquestion in the positive, and fully characterize the power of low-degree\npolynomials as extractors for local sources. More precisely, we show that a\nrandom degree $r$ polynomial is a low-error extractor for $n$-bit local sources\nwith min-entropy $\\Omega(r(n\\log n)^{1/r})$, and we show that this is tight.\nOur result leverages several new ingredients, which may be of independent\ninterest. Our existential result relies on a new reduction from local sources\nto a more structured family, known as local non-oblivious bit-fixing sources.\nTo show its tightness, we prove a \"local version\" of a structural result by\nCohen and Tal (RANDOM 2015), which relies on a new \"low-weight\"\nChevalley-Warning theorem.",
    "descriptor": "\nComments: 44 pages\n",
    "authors": [
      "Omar Alrabiah",
      "Eshan Chattopadhyay",
      "Jesse Goodman",
      "Xin Li",
      "Jo\u00e3o Ribeiro"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.13725"
  },
  {
    "id": "arXiv:2205.13726",
    "title": "A Robust, Multiple Control Barrier Function Framework for Input  Constrained Systems",
    "abstract": "We propose a novel (Type-II) zeroing control barrier function (ZCBF) for\nsafety-critical control, which generalizes the original ZCBF approach. Our\nmethod allows for applications to a larger class of systems (e.g.\npassivity-based) while still ensuring robustness, for which the construction of\nconventional ZCBFs is difficult. We also propose a locally Lipschitz continuous\ncontrol law that handles multiple ZCBFs, while respecting input constraints,\nwhich is not currently possible with existing ZCBF methods. We apply the\nproposed concept for unicycle navigation in an obstacle-rich environment.",
    "descriptor": "",
    "authors": [
      "Wenceslao Shaw Cortez",
      "Xiao Tan",
      "Dimos V. Dimarogonas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13726"
  },
  {
    "id": "arXiv:2205.13727",
    "title": "High-Order Incremental Potential Contact for Elastodynamic Simulation on  Curved Meshes",
    "abstract": "We introduce a high-order finite element formulation (high-order basis) for\nelastodynamic simulation on high-order (curved) meshes based on the recently\nproposed Incremental Potential Contact model.\nHigh-order meshes provide a more accurate geometrical approximation of the\nobject boundary (where stress usually concentrates, especially in the presence\nof contacts) than linear elements, for a negligible additional cost when used\nin a finite element simulation. High-order bases provide major advantages over\nlinear ones in terms of efficiency, as they provide higher accuracy for the\nsame running time, and reliability, as they are less affected by locking\nartifacts and mesh quality.\nOur approach is based on the observation that each IPC optimization step used\nto minimize the elasticity, contact, and friction potentials leads to linear\ntrajectories even in the presence of non-linear meshes or non-linear FE basis.\nIt is thus possible to retain the strong non-penetration guarantees and large\ntime steps of the original formulation while benefiting from the high-order\nbasis and high-order geometry. Additionally, we show that collision proxies can\nbe naturally incorporated into this formulation.\nWe demonstrate the effectiveness of our approach in a selection of problems\nfrom graphics, computational fabrication, and scientific computing.",
    "descriptor": "",
    "authors": [
      "Zachary Ferguson",
      "Pranav Jain",
      "Denis Zorin",
      "Teseo Schneider",
      "Daniele Panozzo"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13727"
  },
  {
    "id": "arXiv:2205.13728",
    "title": "GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic  Synthesis",
    "abstract": "Despite achieving superior performance in human-level control problems,\nunlike humans, deep reinforcement learning (DRL) lacks high-order intelligence\n(e.g., logic deduction and reuse), thus it behaves ineffectively than humans\nregarding learning and generalization in complex problems. Previous works\nattempt to directly synthesize a white-box logic program as the DRL policy,\nmanifesting logic-driven behaviors. However, most synthesis methods are built\non imperative or declarative programming, and each has a distinct limitation,\nrespectively. The former ignores the cause-effect logic during synthesis,\nresulting in low generalizability across tasks. The latter is strictly\nproof-based, thus failing to synthesize programs with complex hierarchical\nlogic. In this paper, we combine the above two paradigms together and propose a\nnovel Generalizable Logic Synthesis (GALOIS) framework to synthesize\nhierarchical and strict cause-effect logic programs. GALOIS leverages the\nprogram sketch and defines a new sketch-based hybrid program language for\nguiding the synthesis. Based on that, GALOIS proposes a sketch-based program\nsynthesis method to automatically generate white-box programs with\ngeneralizable and interpretable cause-effect logic. Extensive evaluations on\nvarious decision-making tasks with complex logic demonstrate the superiority of\nGALOIS over mainstream baselines regarding the asymptotic performance,\ngeneralizability, and great knowledge reusability across different\nenvironments.",
    "descriptor": "",
    "authors": [
      "Yushi Cao",
      "Zhiming Li",
      "Tianpei Yang",
      "Hao Zhang",
      "Yan Zheng",
      "Yi Li",
      "Jianye Hao",
      "Yang Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13728"
  },
  {
    "id": "arXiv:2205.13730",
    "title": "Understanding Long Programming Languages with Structure-Aware Sparse  Attention",
    "abstract": "Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have\nachieved great success in many downstream code-related tasks. Since the memory\nand computational complexity of self-attention in the Transformer grow\nquadratically with the sequence length, PPLMs typically limit the code length\nto 512. However, codes in real-world applications are generally long, such as\ncode searches, which cannot be processed efficiently by existing PPLMs. To\nsolve this problem, in this paper, we present SASA, a Structure-Aware Sparse\nAttention mechanism, which reduces the complexity and improves performance for\nlong code understanding tasks. The key components in SASA are top-$k$ sparse\nattention and Abstract Syntax Tree (AST)-based structure-aware attention. With\ntop-$k$ sparse attention, the most crucial attention relation can be obtained\nwith a lower computational cost. As the code structure represents the logic of\nthe code statements, which is a complement to the code sequence\ncharacteristics, we further introduce AST structures into attention. Extensive\nexperiments on CodeXGLUE tasks show that SASA achieves better performance than\nthe competing baselines.",
    "descriptor": "\nComments: sigir 2022 accepted, code will be available at this https URL\n",
    "authors": [
      "Tingting Liu",
      "Chengyu Wang",
      "Cen Chen",
      "Ming Gao",
      "Aoying Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13730"
  },
  {
    "id": "arXiv:2205.13733",
    "title": "On Consistency in Graph Neural Network Interpretation",
    "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has\nreceived increasing attention over recent years. Instance-level GNN explanation\naims to discover critical input elements, like nodes or edges, that the target\nGNN relies upon for making predictions. These identified sub-structures can\nprovide interpretations of GNN's behavior. Though various algorithms are\nproposed, most of them formalize this task by searching the minimal subgraph\nwhich can preserve original predictions. An inductive bias is deep-rooted in\nthis framework: the same output cannot guarantee that two inputs are processed\nunder the same rationale. Consequently, they have the danger of providing\nspurious explanations and fail to provide consistent explanations. Applying\nthem to explain weakly-performed GNNs would further amplify these issues. To\naddress the issues, we propose to obtain more faithful and consistent\nexplanations of GNNs. After a close examination on predictions of GNNs from the\ncausality perspective, we attribute spurious explanations to two typical\nreasons: confounding effect of latent variables like distribution shift, and\ncausal factors distinct from the original input. Motivated by the observation\nthat both confounding effects and diverse causal rationales are encoded in\ninternal representations, we propose a simple yet effective countermeasure by\naligning embeddings. This new objective can be incorporated into existing GNN\nexplanation algorithms with no effort. We implement both a simplified version\nbased on absolute distance and a distribution-aware version based on anchors.\nExperiments on $5$ datasets validate its effectiveness, and theoretical\nanalysis shows that it is in effect optimizing a more faithful explanation\nobjective in design, which further justifies the proposed approach.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Tianxiang Zhao",
      "Dongsheng Luo",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.13733"
  },
  {
    "id": "arXiv:2205.13738",
    "title": "Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network  with Mixed Multi-layer Attention",
    "abstract": "Image super-resolution reconstruction achieves better results than\ntraditional methods with the help of the powerful nonlinear representation\nability of convolution neural network. However, some existing algorithms also\nhave some problems, such as insufficient utilization of phased features,\nignoring the importance of early phased feature fusion to improve network\nperformance, and the inability of the network to pay more attention to\nhigh-frequency information in the reconstruction process. To solve these\nproblems, we propose a multi-branch feature multiplexing fusion network with\nmixed multi-layer attention (MBMFN), which realizes the multiple utilization of\nfeatures and the multistage fusion of different levels of features. To further\nimprove the networks performance, we propose a lightweight enhanced residual\nchannel attention (LERCA), which can not only effectively avoid the loss of\nchannel information but also make the network pay more attention to the key\nchannel information and benefit from it. Finally, the attention mechanism is\nintroduced into the reconstruction process to strengthen the restoration of\nedge texture and other details. A large number of experiments on several\nbenchmark sets show that, compared with other advanced reconstruction\nalgorithms, our algorithm produces highly competitive objective indicators and\nrestores more image detail texture information.",
    "descriptor": "",
    "authors": [
      "Yuxi Cai",
      "Huicheng Lai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.13738"
  },
  {
    "id": "arXiv:2205.13740",
    "title": "Subverting machines, fluctuating identities: Re-learning human  categorization",
    "abstract": "Most machine learning systems that interact with humans construct some notion\nof a person's \"identity,\" yet the default paradigm in AI research envisions\nidentity with essential attributes that are discrete and static. In stark\ncontrast, strands of thought within critical theory present a conception of\nidentity as malleable and constructed entirely through interaction; a doing\nrather than a being. In this work, we distill some of these ideas for machine\nlearning practitioners and introduce a theory of identity as autopoiesis,\ncircular processes of formation and function. We argue that the default\nparadigm of identity used by the field immobilizes existing identity categories\nand the power differentials that co$\\unicode{x2010}$occur, due to the absence\nof iterative feedback to our models. This includes a critique of emergent AI\nfairness practices that continue to impose the default paradigm. Finally, we\napply our theory to sketch approaches to autopoietic identity through\nmultilevel optimization and relational learning. While these ideas raise many\nopen questions, we imagine the possibilities of machines that are capable of\nexpressing human identity as a relationship perpetually in flux.",
    "descriptor": "\nComments: In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea. First two authors contributed equally to this work\n",
    "authors": [
      "Christina Lu",
      "Jackie Kay",
      "Kevin R. McKee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.13740"
  },
  {
    "id": "arXiv:2205.13741",
    "title": "Group GAN",
    "abstract": "Generating multivariate time series is a promising approach for sharing\nsensitive data in many medical, financial, and IoT applications. A common type\nof multivariate time series originates from a single source such as the\nbiometric measurements from a medical patient. This leads to complex dynamical\npatterns between individual time series that are hard to learn by typical\ngeneration models such as GANs. There is valuable information in those patterns\nthat machine learning models can use to better classify, predict or perform\nother downstream tasks. We propose a novel framework that takes time series'\ncommon origin into account and favors inter-channel relationship preservation.\nThe two key points of our method are: 1) the individual time series are\ngenerated from a common point in latent space and 2) a central discriminator\nfavors the preservation of inter-channel dynamics. We demonstrate empirically\nthat our method helps preserve channel correlations and that our synthetic data\nperforms very well downstream tasks with medical and financial data.",
    "descriptor": "\nComments: 18 pages, 16 figures\n",
    "authors": [
      "Ali Seyfi",
      "Jean-Francois Rajotte",
      "Raymond T. Ng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13741"
  },
  {
    "id": "arXiv:2205.13743",
    "title": "Generating personalized counterfactual interventions for algorithmic  recourse by eliciting user preferences",
    "abstract": "Counterfactual interventions are a powerful tool to explain the decisions of\na black-box decision process, and to enable algorithmic recourse. They are a\nsequence of actions that, if performed by a user, can overturn an unfavourable\ndecision made by an automated decision system. However, most of the current\nmethods provide interventions without considering the user's preferences. For\nexample, a user might prefer doing certain actions with respect to others. In\nthis work, we present the first human-in-the-loop approach to perform\nalgorithmic recourse by eliciting user preferences. We introduce a polynomial\nprocedure to ask choice-set questions which maximize the Expected Utility of\nSelection (EUS), and use it to iteratively refine our cost estimates in a\nBayesian setting. We integrate this preference elicitation strategy into a\nreinforcement learning agent coupled with Monte Carlo Tree Search for efficient\nexploration, so as to provide personalized interventions achieving algorithmic\nrecourse. An experimental evaluation on synthetic and real-world datasets shows\nthat a handful of queries allows to achieve a substantial reduction in the cost\nof interventions with respect to user-independent alternatives.",
    "descriptor": "",
    "authors": [
      "Giovanni De Toni",
      "Paolo Viappiani",
      "Bruno Lepri",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13743"
  },
  {
    "id": "arXiv:2205.13744",
    "title": "Learning Instance Representation Banks for Aerial Scene Classification",
    "abstract": "Aerial scenes are more complicated in terms of object distribution and\nspatial arrangement than natural scenes due to the bird view, and thus remain\nchallenging to learn discriminative scene representation. Recent solutions\ndesign \\textit{local semantic descriptors} so that region of interests (RoIs)\ncan be properly highlighted. However, each local descriptor has limited\ndescription capability and the overall scene representation remains to be\nrefined. In this paper, we solve this problem by designing a novel\nrepresentation set named \\textit{instance representation bank} (IRB), which\nunifies multiple local descriptors under the multiple instance learning (MIL)\nformulation. This unified framework is not trivial as all the local semantic\ndescriptors can be aligned to the same scene scheme, enhancing the scene\nrepresentation capability. Specifically, our IRB learning framework consists of\na backbone, an instance representation bank, a semantic fusion module and a\nscene scheme alignment loss function. All the components are organized in an\nend-to-end manner. Extensive experiments on three aerial scene benchmarks\ndemonstrate that our proposed method outperforms the state-of-the-art\napproaches by a large margin.",
    "descriptor": "",
    "authors": [
      "Jingjun Yi",
      "Beichen Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13744"
  },
  {
    "id": "arXiv:2205.13745",
    "title": "DeepSAT: An EDA-Driven Learning Framework for SAT",
    "abstract": "We present DeepSAT, a novel end-to-end learning framework for the Boolean\nsatisfiability (SAT) problem. Unlike existing solutions trained on random SAT\ninstances with relatively weak supervisions, we propose applying the knowledge\nof the well-developed electronic design automation (EDA) field for SAT solving.\nSpecifically, we first resort to advanced logic synthesis algorithms to\npre-process SAT instances into optimized and-inverter graphs (AIGs). By doing\nso, our training and test sets have a unified distribution, thus the learned\nmodel can generalize well to test sets of various sources of SAT instances.\nNext, we regard the distribution of SAT solutions being a product of\nconditional Bernoulli distributions. Based on this observation, we approximate\nthe SAT solving procedure with a conditional generative model, leveraging a\ndirected acyclic graph neural network with two polarity prototypes for\nconditional SAT modeling. To effectively train the generative model, with the\nhelp of logic simulation tools, we obtain the probabilities of nodes in the AIG\nbeing logic '1' as rich supervision. We conduct extensive experiments on\nvarious SAT instances. DeepSAT achieves significant accuracy improvements over\nstate-of-the-art learning-based SAT solutions, especially when generalized to\nSAT instances that are large or with diverse distributions.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Min Li",
      "Zhengyuan Shi",
      "Qiuxia Lai",
      "Sadaf Khan",
      "Qiang Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13745"
  },
  {
    "id": "arXiv:2205.13748",
    "title": "Auto-PINN: Understanding and Optimizing Physics-Informed Neural  Architecture",
    "abstract": "Physics-informed neural networks (PINNs) are revolutionizing science and\nengineering practice by bringing together the power of deep learning to bear on\nscientific computation. In forward modeling problems, PINNs are meshless\npartial differential equation (PDE) solvers that can handle irregular,\nhigh-dimensional physical domains. Naturally, the neural architecture\nhyperparameters have a large impact on the efficiency and accuracy of the PINN\nsolver. However, this remains an open and challenging problem because of the\nlarge search space and the difficulty of identifying a proper search objective\nfor PDEs. Here, we propose Auto-PINN, the first systematic, automated\nhyperparameter optimization approach for PINNs, which employs Neural\nArchitecture Search (NAS) techniques to PINN design. Auto-PINN avoids manually\nor exhaustively searching the hyperparameter space associated with PINNs. A\ncomprehensive set of pre-experiments using standard PDE benchmarks allows us to\nprobe the structure-performance relationship in PINNs. We find that the\ndifferent hyperparameters can be decoupled, and that the training loss function\nof PINNs is a good search objective. Comparison experiments with baseline\nmethods demonstrate that Auto-PINN produces neural architectures with superior\nstability and accuracy over alternative baselines.",
    "descriptor": "",
    "authors": [
      "Yicheng Wang",
      "Xiaotian Han",
      "Chia-Yuan Chang",
      "Daochen Zha",
      "Ulisses Braga-Neto",
      "Xia Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13748"
  },
  {
    "id": "arXiv:2205.13750",
    "title": "Attention Awareness Multiple Instance Neural Network",
    "abstract": "Multiple instance learning is qualified for many pattern recognition tasks\nwith weakly annotated data. The combination of artificial neural network and\nmultiple instance learning offers an end-to-end solution and has been widely\nutilized. However, challenges remain in two-folds. Firstly, current MIL pooling\noperators are usually pre-defined and lack flexibility to mine key instances.\nSecondly, in current solutions, the bag-level representation can be inaccurate\nor inaccessible. To this end, we propose an attention awareness multiple\ninstance neural network framework in this paper. It consists of an\ninstance-level classifier, a trainable MIL pooling operator based on spatial\nattention and a bag-level classification layer. Exhaustive experiments on a\nseries of pattern recognition tasks demonstrate that our framework outperforms\nmany state-of-the-art MIL methods and validates the effectiveness of our\nproposed attention MIL pooling operators.",
    "descriptor": "",
    "authors": [
      "Jingjun Yi",
      "Beichen Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13750"
  },
  {
    "id": "arXiv:2205.13753",
    "title": "HOUDINI: Escaping from Moderately Constrained Saddles",
    "abstract": "We give the first polynomial time algorithms for escaping from\nhigh-dimensional saddle points under a moderate number of constraints. Given\ngradient access to a smooth function $f \\colon \\mathbb R^d \\to \\mathbb R$ we\nshow that (noisy) gradient descent methods can escape from saddle points under\na logarithmic number of inequality constraints. This constitutes the first\ntangible progress (without reliance on NP-oracles or altering the definitions\nto only account for certain constraints) on the main open question of the\nbreakthrough work of Ge et al. who showed an analogous result for unconstrained\nand equality-constrained problems. Our results hold for both regular and\nstochastic gradient descent.",
    "descriptor": "",
    "authors": [
      "Dmitrii Avdiukhin",
      "Grigory Yaroslavtsev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13753"
  },
  {
    "id": "arXiv:2205.13754",
    "title": "NLU for Game-based Learning in Real: Initial Evaluations",
    "abstract": "Intelligent systems designed for play-based interactions should be\ncontextually aware of the users and their surroundings. Spoken Dialogue Systems\n(SDS) are critical for these interactive agents to carry out effective\ngoal-oriented communication with users in real-time. For the real-world (i.e.,\nin-the-wild) deployment of such conversational agents, improving the Natural\nLanguage Understanding (NLU) module of the goal-oriented SDS pipeline is\ncrucial, especially with limited task-specific datasets. This study explores\nthe potential benefits of a recently proposed transformer-based multi-task NLU\narchitecture, mainly to perform Intent Recognition on small-size\ndomain-specific educational game datasets. The evaluation datasets were\ncollected from children practicing basic math concepts via play-based\ninteractions in game-based learning settings. We investigate the NLU\nperformances on the initial proof-of-concept game datasets versus the\nreal-world deployment datasets and observe anticipated performance drops\nin-the-wild. We have shown that compared to the more straightforward baseline\napproaches, Dual Intent and Entity Transformer (DIET) architecture is robust\nenough to handle real-world data to a large extent for the Intent Recognition\ntask on these domain-specific in-the-wild game datasets.",
    "descriptor": "\nComments: Proceedings of the Games and Natural Language Processing Workshop at LREC 2022\n",
    "authors": [
      "Eda Okur",
      "Saurav Sahay",
      "Lama Nachman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13754"
  },
  {
    "id": "arXiv:2205.13756",
    "title": "NOMA-ISAC: Performance Analysis and Rate Region Characterization",
    "abstract": "This paper analyzes the performance of a multiuser integrated sensing and\ncommunications (ISAC) system, where nonorthogonal multiple access (NOMA) is\nexploited to mitigate inter-user interference. Closed-form expressions are\nderived to evaluate the outage probability, ergodic communication rate, and\nsensing rate. Furthermore, asymptotic analyses are carried out to unveil\ndiversity orders and high signal-to-noise ratio (SNR) slopes of the considered\nNOMA-ISAC system. As the further advance, the achievable sensing-communication\nrate region of ISAC is characterized. It is proved that ISAC system is capable\nof achieving a larger rate region than the conventional frequency-division\nsensing and communications (FDSAC) system.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Chongjun Ouyang",
      "Yuanwei Liu",
      "Hongwen Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13756"
  },
  {
    "id": "arXiv:2205.13758",
    "title": "CIGMO: Categorical invariant representations in a deep generative  framework",
    "abstract": "Data of general object images have two most common structures: (1) each\nobject of a given shape can be rendered in multiple different views, and (2)\nshapes of objects can be categorized in such a way that the diversity of shapes\nis much larger across categories than within a category. Existing deep\ngenerative models can typically capture either structure, but not both. In this\nwork, we introduce a novel deep generative model, called CIGMO, that can learn\nto represent category, shape, and view factors from image data. The model is\ncomprised of multiple modules of shape representations that are each\nspecialized to a particular category and disentangled from view representation,\nand can be learned using a group-based weakly supervised learning method. By\nempirical investigation, we show that our model can effectively discover\ncategories of object shapes despite large view variation and quantitatively\nsupersede various previous methods including the state-of-the-art invariant\nclustering algorithm. Further, we show that our approach using\ncategory-specialization can enhance the learned shape representation to better\nperform down-stream tasks such as one-shot object identification as well as\nshape-view disentanglement.",
    "descriptor": "",
    "authors": [
      "Haruo Hosoya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13758"
  },
  {
    "id": "arXiv:2205.13760",
    "title": "Tranception: protein fitness prediction with autoregressive transformers  and inference-time retrieval",
    "abstract": "The ability to accurately model the fitness landscape of protein sequences is\ncritical to a wide range of applications, from quantifying the effects of human\nvariants on disease likelihood, to predicting immune-escape mutations in\nviruses and designing novel biotherapeutic proteins. Deep generative models of\nprotein sequences trained on multiple sequence alignments have been the most\nsuccessful approaches so far to address these tasks. The performance of these\nmethods is however contingent on the availability of sufficiently deep and\ndiverse alignments for reliable training. Their potential scope is thus limited\nby the fact many protein families are hard, if not impossible, to align. Large\nlanguage models trained on massive quantities of non-aligned protein sequences\nfrom diverse families address these problems and show potential to eventually\nbridge the performance gap. We introduce Tranception, a novel transformer\narchitecture leveraging autoregressive predictions and retrieval of homologous\nsequences at inference to achieve state-of-the-art fitness prediction\nperformance. Given its markedly higher performance on multiple mutants,\nrobustness to shallow alignments and ability to score indels, our approach\noffers significant gain of scope over existing approaches. To enable more\nrigorous model testing across a broader range of protein families, we develop\nProteinGym -- an extensive set of multiplexed assays of variant effects,\nsubstantially increasing both the number and diversity of assays compared to\nexisting benchmarks.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Pascal Notin",
      "Mafalda Dias",
      "Jonathan Frazer",
      "Javier Marchena-Hurtado",
      "Aidan Gomez",
      "Debora S. Marks",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13760"
  },
  {
    "id": "arXiv:2205.13763",
    "title": "Tutorial on Course-of-Action (COA) Attack Search Methods in Computer  Networks",
    "abstract": "In the literature of modern network security research, deriving effective and\nefficient course-of-action (COA) attach search methods are of interests in\nindustry and academia. As the network size grows, the traditional COA attack\nsearch methods can suffer from the limitations to computing and communication\nresources. Therefore, various methods have been developed to solve these\nproblems, and reinforcement learning (RL)-based intelligent algorithms are one\nof the most effective solutions. Therefore, we review the RL-based COA attack\nsearch methods for network attack scenarios in terms of the trends and their\ncontrib",
    "descriptor": "",
    "authors": [
      "Seok Bin Son",
      "Soohyun Park",
      "Haemin Lee",
      "Joongheon Kim",
      "Soyi Jung",
      "Donghwa Kim"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13763"
  },
  {
    "id": "arXiv:2205.13764",
    "title": "Fully Convolutional One-Stage 3D Object Detection on LiDAR Range Images",
    "abstract": "We present a simple yet effective fully convolutional one-stage 3D object\ndetector for LiDAR point clouds of autonomous driving scenes, termed\nFCOS-LiDAR. Unlike the dominant methods that use the bird-eye view (BEV), our\nproposed detector detects objects from the range view (RV, a.k.a. range image)\nof the LiDAR points. Due to the range view's compactness and compatibility with\nthe LiDAR sensors' sampling process on self-driving cars, the range view-based\nobject detector can be realized by solely exploiting the vanilla 2D\nconvolutions, departing from the BEV-based methods which often involve\ncomplicated voxelization operations and sparse convolutions.\nFor the first time, we show that an RV-based 3D detector with standard 2D\nconvolutions alone can achieve comparable performance to state-of-the-art\nBEV-based detectors while being significantly faster and simpler. More\nimportantly, almost all previous range view-based detectors only focus on\nsingle-frame point clouds, since it is challenging to fuse multi-frame point\nclouds into a single range view. In this work, we tackle this challenging issue\nwith a novel range view projection mechanism, and for the first time\ndemonstrate the benefits of fusing multi-frame point clouds for a range-view\nbased detector. Extensive experiments on nuScenes show the superiority of our\nproposed method and we believe that our work can be strong evidence that an\nRV-based 3D detector can compare favourably with the current mainstream\nBEV-based detectors.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Zhi Tian",
      "Xiangxiang Chu",
      "Xiaoming Wang",
      "Xiaolin Wei",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13764"
  },
  {
    "id": "arXiv:2205.13765",
    "title": "Machine Learning-based Ransomware Detection Using Low-level Memory  Access Patterns Obtained From Live-forensic Hypervisor",
    "abstract": "Since modern anti-virus software mainly depends on a signature-based static\nanalysis, they are not suitable for coping with the rapid increase in malware\nvariants. Moreover, even worse, many vulnerabilities of operating systems\nenable attackers to evade such protection mechanisms. We, therefore, developed\na thin and lightweight live-forensic hypervisor to create an additional\nprotection layer under a conventional protection layer of operating systems\nwith supporting ransomware detection using dynamic behavioral features. The\ndeveloped live-forensic hypervisor collects low-level memory access patterns\ninstead of high-level information such as process IDs and API calls that modern\nVirtual Machine Introspection techniques have employed. We then created the\nlow-level memory access patterns dataset of three ransomware samples, one wiper\nmalware sample, and four benign applications. We confirmed that our best\nmachine learning classifier using only low-level memory access patterns\nachieved an $F_1$ score of 0.95 in detecting ransomware and wiper malware.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Manabu Hirano",
      "Ryotaro Kobayashi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13765"
  },
  {
    "id": "arXiv:2205.13766",
    "title": "Block-coordinate Frank-Wolfe algorithm and convergence analysis for  semi-relaxed optimal transport problem",
    "abstract": "The optimal transport (OT) problem has been used widely for machine learning.\nIt is necessary for computation of an OT problem to solve linear programming\nwith tight mass-conservation constraints. These constraints prevent its\napplication to large-scale problems. To address this issue, loosening such\nconstraints enables us to propose the relaxed-OT method using a faster\nalgorithm. This approach has demonstrated its effectiveness for applications.\nHowever, it remains slow. As a superior alternative, we propose a fast\nblock-coordinate Frank-Wolfe (BCFW) algorithm for a convex semi-relaxed OT.\nSpecifically, we prove their upper bounds of the worst convergence iterations,\nand equivalence between the linearization duality gap and the Lagrangian\nduality gap. Additionally, we develop two fast variants of the proposed BCFW.\nNumerical experiments have demonstrated that our proposed algorithms are\neffective for color transfer and surpass state-of-the-art algorithms. This\nreport presents a short version of arXiv:2103.05857.",
    "descriptor": "\nComments: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP2022). arXiv admin note: substantial text overlap with arXiv:2103.05857\n",
    "authors": [
      "Takumi Fukunaga",
      "Hiroyuki Kasai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13766"
  },
  {
    "id": "arXiv:2205.13769",
    "title": "Semantic-aware Dense Representation Learning for Remote Sensing Image  Change Detection",
    "abstract": "Training deep learning-based change detection (CD) model heavily depends on\nlabeled data. Contemporary transfer learning-based methods to alleviate the CD\nlabel insufficiency mainly upon ImageNet pre-training. A recent trend is using\nremote sensing (RS) data to obtain in-domain representations via supervised or\nself-supervised learning (SSL). Here, different from traditional supervised\npre-training that learns the mapping from image to label, we leverage semantic\nsupervision in a contrastive manner. There are typically multiple objects of\ninterest (e.g., buildings) distributed in varying locations in RS images. We\npropose dense semantic-aware pre-training for RS image CD via sampling multiple\nclass-balanced points. Instead of manipulating image-level representations that\nlack spatial information, we constrain pixel-level cross-view consistency and\ncross-semantic discrimination to learn spatially-sensitive features, thus\nbenefiting downstream dense CD. Apart from learning illumination invariant\nfeatures, we fulfill consistent foreground features insensitive to irrelevant\nbackground changes via a synthetic view using background swapping. We\nadditionally achieve discriminative representations to distinguish foreground\nland-covers and others. We collect large-scale image-mask pairs freely\navailable in the RS community for pre-training. Extensive experiments on three\nCD datasets verify the effectiveness of our method. Ours significantly\noutperforms ImageNet, in-domain supervision, and several SSL methods. Empirical\nresults indicate ours well alleviates data insufficiency in CD. Notably, we\nachieve competitive results using only 20% training data than baseline (random)\nusing 100% data. Both quantitative and qualitative results demonstrate the\ngeneralization ability of our pre-trained model to downstream images even\nremaining domain gaps with the pre-training data. Our Code will make public.",
    "descriptor": "\nComments: Submitted to IEEE for possible publication. 17 pages, 7 figures\n",
    "authors": [
      "Hao Chen",
      "Wenyuan Li",
      "Song Chen",
      "Zhenwei Shi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13769"
  },
  {
    "id": "arXiv:2205.13770",
    "title": "LEAF + AIO: Edge-Assisted Energy-Aware Object Detection for Mobile  Augmented Reality",
    "abstract": "Today very few deep learning-based mobile augmented reality (MAR)\napplications are applied in mobile devices because they are significantly\nenergy-guzzling. In this paper, we design an edge-based energy-aware MAR system\nthat enables MAR devices to dynamically change their configurations, such as\nCPU frequency, computation model size, and image offloading frequency based on\nuser preferences, camera sampling rates, and available radio resources. Our\nproposed dynamic MAR configuration adaptations can minimize the per frame\nenergy consumption of multiple MAR clients without degrading their preferred\nMAR performance metrics, such as latency and detection accuracy. To thoroughly\nanalyze the interactions among MAR configurations, user preferences, camera\nsampling rate, and energy consumption, we propose, to the best of our\nknowledge, the first comprehensive analytical energy model for MAR devices.\nBased on the proposed analytical model, we design a LEAF optimization algorithm\nto guide the MAR configuration adaptation and server radio resource allocation.\nAn image offloading frequency orchestrator, coordinating with the LEAF, is\ndeveloped to adaptively regulate the edge-based object detection invocations\nand to further improve the energy efficiency of MAR devices. Extensive\nevaluations are conducted to validate the performance of the proposed\nanalytical model and algorithms.",
    "descriptor": "\nComments: This is a personal copy of the authors. Not for redistribution. The final version of this paper was accepted by IEEE Transactions on Mobile Computing\n",
    "authors": [
      "Haoxin Wang",
      "BaekGyu Kim",
      "Jiang Xie",
      "Zhu Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.13770"
  },
  {
    "id": "arXiv:2205.13771",
    "title": "IGLU 2022: Interactive Grounded Language Understanding in a  Collaborative Environment at NeurIPS 2022",
    "abstract": "Human intelligence has the remarkable ability to adapt to new tasks and\nenvironments quickly. Starting from a very young age, humans acquire new skills\nand learn how to solve new tasks either by imitating the behavior of others or\nby following provided natural language instructions. To facilitate research in\nthis direction, we propose IGLU: Interactive Grounded Language Understanding in\na Collaborative Environment. The primary goal of the competition is to approach\nthe problem of how to develop interactive embodied agents that learn to solve a\ntask while provided with grounded natural language instructions in a\ncollaborative environment. Understanding the complexity of the challenge, we\nsplit it into sub-tasks to make it feasible for participants.\nThis research challenge is naturally related, but not limited, to two fields\nof study that are highly relevant to the NeurIPS community: Natural Language\nUnderstanding and Generation (NLU/G) and Reinforcement Learning (RL).\nTherefore, the suggested challenge can bring two communities together to\napproach one of the crucial challenges in AI. Another critical aspect of the\nchallenge is the dedication to perform a human-in-the-loop evaluation as a\nfinal evaluation for the agents developed by contestants.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2110.06536\n",
    "authors": [
      "Julia Kiseleva",
      "Alexey Skrynnik",
      "Artem Zholus",
      "Shrestha Mohanty",
      "Negar Arabzadeh",
      "Marc-Alexandre C\u00f4t\u00e9",
      "Mohammad Aliannejadi",
      "Milagro Teruel",
      "Ziming Li",
      "Mikhail Burtsev",
      "Maartje ter Hoeve",
      "Zoya Volovikova",
      "Aleksandr Panov",
      "Yuxuan Sun",
      "Kavya Srinet",
      "Arthur Szlam",
      "Ahmed Awadallah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13771"
  },
  {
    "id": "arXiv:2205.13775",
    "title": "A Survey on Long-Tailed Visual Recognition",
    "abstract": "The heavy reliance on data is one of the major reasons that currently limit\nthe development of deep learning. Data quality directly dominates the effect of\ndeep learning models, and the long-tailed distribution is one of the factors\naffecting data quality. The long-tailed phenomenon is prevalent due to the\nprevalence of power law in nature. In this case, the performance of deep\nlearning models is often dominated by the head classes while the learning of\nthe tail classes is severely underdeveloped. In order to learn adequately for\nall classes, many researchers have studied and preliminarily addressed the\nlong-tailed problem. In this survey, we focus on the problems caused by\nlong-tailed data distribution, sort out the representative long-tailed visual\nrecognition datasets and summarize some mainstream long-tailed studies.\nSpecifically, we summarize these studies into ten categories from the\nperspective of representation learning, and outline the highlights and\nlimitations of each category. Besides, we have studied four quantitative\nmetrics for evaluating the imbalance, and suggest using the Gini coefficient to\nevaluate the long-tailedness of a dataset. Based on the Gini coefficient, we\nquantitatively study 20 widely-used and large-scale visual datasets proposed in\nthe last decade, and find that the long-tailed phenomenon is widespread and has\nnot been fully studied. Finally, we provide several future directions for the\ndevelopment of long-tailed learning to provide more ideas for readers.",
    "descriptor": "\nComments: Accepted for publication in International Journal of Computer Vision (IJCV)\n",
    "authors": [
      "Lu Yang",
      "He Jiang",
      "Qing Song",
      "Jun Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13775"
  },
  {
    "id": "arXiv:2205.13776",
    "title": "PrivacyDates: A Framework for More Privacy-Preserving Timestamp Data  Type",
    "abstract": "Case studies of application software data models indicate that timestamps are\nexcessively used in connection with user activity. This contradicts the\nprinciple of data minimisation which demands a limitation to data necessary for\na given purpose. Prior work has also identified common purposes of timestamps\nthat can be realised by more privacy-preserving alternatives like counters and\ndates with purpose-oriented precision. In this paper, we follow up by\ndemonstrating the real-world applicability of those alternatives. We design and\nimplement three timestamp alternatives for the popular web development\nframework Django and evaluate their practicality by replacing conventional\ntimestamps in the project management application Taiga. We find that our\nalternatives could be adopted without impairing the functionality of Taiga.",
    "descriptor": "\nComments: Accepted and presented at the conference GI Sicherheit 2022\n",
    "authors": [
      "Christian Burkert",
      "Jonathan Balack",
      "Hannes Federrath"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13776"
  },
  {
    "id": "arXiv:2205.13780",
    "title": "Text-Based Automatic Personality Prediction Using KGrAt-Net; A Knowledge  Graph Attention Network Classifier",
    "abstract": "Nowadays, a tremendous amount of human communications take place on the\nInternet-based communication infrastructures, like social networks, email,\nforums, organizational communication platforms, etc. Indeed, the automatic\nprediction or assessment of individuals' personalities through their written or\nexchanged text would be advantageous to ameliorate the relationships among\nthem. To this end, this paper aims to propose KGrAt-Net which is a Knowledge\nGraph Attention Network text classifier. For the first time, it applies the\nknowledge graph attention network to perform Automatic Personality Prediction\n(APP), according to the Big Five personality traits. After performing some\npreprocessing activities, first, it tries to acquire a knowingful\nrepresentation of the knowledge behind the concepts in the input text through\nbuilding its equivalent knowledge graph. A knowledge graph is a graph-based\ndata model that formally represents the semantics of the existing concepts in\nthe input text and models the knowledge behind them. Then, applying the\nattention mechanism, it efforts to pay attention to the most relevant parts of\nthe graph to predict the personality traits of the input text. The results\ndemonstrated that KGrAt-Net considerably improved the personality prediction\naccuracies. Furthermore, KGrAt-Net also uses the knowledge graphs' embeddings\nto enrich the classification, which makes it even more accurate in APP.",
    "descriptor": "",
    "authors": [
      "Majid Ramezani",
      "Mohammad-Reza Feizi-Derakhshi",
      "Mohammad-Ali Balafar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13780"
  },
  {
    "id": "arXiv:2205.13790",
    "title": "BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework",
    "abstract": "Fusing the camera and LiDAR information has become a de-facto standard for 3D\nobject detection tasks. Current methods rely on point clouds from the LiDAR\nsensor as queries to leverage the feature from the image space. However, people\ndiscover that this underlying assumption makes the current fusion framework\ninfeasible to produce any prediction when there is a LiDAR malfunction,\nregardless of minor or major. This fundamentally limits the deployment\ncapability to realistic autonomous driving scenarios. In contrast, we propose a\nsurprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera\nstream does not depend on the input of LiDAR data, thus addressing the downside\nof previous methods. We empirically show that our framework surpasses the\nstate-of-the-art methods under the normal training settings. Under the\nrobustness training settings that simulate various LiDAR malfunctions, our\nframework significantly surpasses the state-of-the-art methods by 15.7% to\n28.9% mAP. To the best of our knowledge, we are the first to handle realistic\nLiDAR malfunction and can be deployed to realistic scenarios without any\npost-processing procedure. The code is available at\nhttps://github.com/ADLab-AutoDrive/BEVFusion.",
    "descriptor": "",
    "authors": [
      "Tingting Liang",
      "Hongwei Xie",
      "Kaicheng Yu",
      "Zhongyu Xia",
      "Zhiwei Lin",
      "Yongtao Wang",
      "Tao Tang",
      "Bing Wang",
      "Zhi Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13790"
  },
  {
    "id": "arXiv:2205.13792",
    "title": "Nearest Neighbor Zero-Shot Inference",
    "abstract": "We introduce kNN-Prompt, a simple and effective technique to use k-nearest\nneighbor (kNN) retrieval augmentation (Khandelwal et al., 2021) for zero-shot\ninference with language models (LMs). Key to our approach is the introduction\nof fuzzy verbalizers which leverage the sparse kNN distribution for downstream\ntasks by automatically associating each classification label with a set of\nnatural language tokens. Across eleven diverse end-tasks (spanning text\nclassification, fact retrieval and question answering), using kNN-Prompt with\nGPT-2 Large yields significant performance boosts over zero-shot baselines (14%\nabsolute improvement over the base LM on average). Extensive experiments show\nthat kNN-Prompt is effective for domain adaptation with no further training,\nand that the benefits of retrieval increase with the size of the model used for\nkNN retrieval. Overall, we show that augmenting a language model with retrieval\ncan bring significant gains for zero-shot inference, with the possibility that\nlarger retrieval models may yield even greater benefits.",
    "descriptor": "",
    "authors": [
      "Weijia Shi",
      "Julian Michael",
      "Suchin Gururangan",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13792"
  },
  {
    "id": "arXiv:2205.13793",
    "title": "Dither Signal Design for PAPR Reduction in OFDM-IM over a Rayleigh  Fading Channel",
    "abstract": "Orthogonal frequency division multiplexing with index modulation (OFDM-IM) is\na novel scheme where the information bits are conveyed through the subcarrier\nactivation pattern (SAP) and the symbols on the active subcarriers.\nUnfortunately, OFDM-IM inherits the high peak-to-average power ratio (PAPR)\nproblem from the classical OFDM. The OFDMIM signal with high PAPR induces\nin-band distortion and out-of-band radiation when it passes through high power\namplifier (HPA). There are attempts to reduce PAPR by adding dither signals in\nthe idle subcarriers, where the dither signals can have various amplitude\nconstraints according to the characteristic of the corresponding OFDM-IM\nsubblock. But, there is no result for the specific amplitude constraint for the\ndither signals over a Rayleigh fading channel. In this letter, based on\npairwise error probability (PEP) analysis, a specific constraint for the dither\nsignals is derived over a Rayleigh fading channel.",
    "descriptor": "",
    "authors": [
      "Kee-Hoon Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13793"
  },
  {
    "id": "arXiv:2205.13795",
    "title": "Improving Item Cold-start Recommendation via Model-agnostic Conditional  Variational Autoencoder",
    "abstract": "Embedding & MLP has become a paradigm for modern large-scale recommendation\nsystem. However, this paradigm suffers from the cold-start problem which will\nseriously compromise the ecological health of recommendation systems. This\npaper attempts to tackle the item cold-start problem by generating enhanced\nwarmed-up ID embeddings for cold items with historical data and limited\ninteraction records. From the aspect of industrial practice, we mainly focus on\nthe following three points of item cold-start: 1) How to conduct cold-start\nwithout additional data requirements and make strategy easy to be deployed in\nonline recommendation scenarios. 2) How to leverage both historical records and\nconstantly emerging interaction data of new items. 3) How to model the\nrelationship between item ID and side information stably from interaction data.\nTo address these problems, we propose a model-agnostic Conditional Variational\nAutoencoder based Recommendation(CVAR) framework with some advantages including\ncompatibility on various backbones, no extra requirements for data, utilization\nof both historical data and recent emerging interactions. CVAR uses latent\nvariables to learn a distribution over item side information and generates\ndesirable item ID embeddings using a conditional decoder. The proposed method\nis evaluated by extensive offline experiments on public datasets and online A/B\ntests on Tencent News recommendation platform, which further illustrate the\nadvantages and robustness of CVAR.",
    "descriptor": "\nComments: 6 pages, 2 figures, accepted as SIGIR 2022 short paper\n",
    "authors": [
      "Xu Zhao",
      "Yi Ren",
      "Ying Du",
      "Shenzheng Zhang",
      "Nian Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.13795"
  },
  {
    "id": "arXiv:2205.13796",
    "title": "Face Morphing: Fooling a Face Recognition System Is Simple!",
    "abstract": "State-of-the-art face recognition (FR) approaches have shown remarkable\nresults in predicting whether two faces belong to the same identity, yielding\naccuracies between 92% and 100% depending on the difficulty of the protocol.\nHowever, the accuracy drops substantially when exposed to morphed faces,\nspecifically generated to look similar to two identities. To generate morphed\nfaces, we integrate a simple pretrained FR model into a generative adversarial\nnetwork (GAN) and modify several loss functions for face morphing. In contrast\nto previous works, our approach and analyses are not limited to pairs of\nfrontal faces with the same ethnicity and gender. Our qualitative and\nquantitative results affirm that our approach achieves a seamless change\nbetween two faces even in unconstrained scenarios. Despite using features from\na simpler FR model for face morphing, we demonstrate that even recent FR\nsystems struggle to distinguish the morphed face from both identities obtaining\nan accuracy of only 55-70%. Besides, we provide further insights into how\nknowing the FR system makes it particularly vulnerable to face morphing\nattacks.",
    "descriptor": "",
    "authors": [
      "Stefan H\u00f6rmann",
      "Tianlin Kong",
      "Torben Teepe",
      "Fabian Herzog",
      "Martin Knoche",
      "Gerhard Rigoll"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13796"
  },
  {
    "id": "arXiv:2205.13797",
    "title": "AsyncFedED: Asynchronous Federated Learning with Euclidean Distance  based Adaptive Weight Aggregation",
    "abstract": "In an asynchronous federated learning framework, the server updates the\nglobal model once it receives an update from a client instead of waiting for\nall the updates to arrive as in the synchronous setting. This allows\nheterogeneous devices with varied computing power to train the local models\nwithout pausing, thereby speeding up the training process. However, it\nintroduces the stale model problem, where the newly arrived update was\ncalculated based on a set of stale weights that are older than the current\nglobal model, which may hurt the convergence of the model. In this paper, we\npresent an asynchronous federated learning framework with a proposed adaptive\nweight aggregation algorithm, referred to as AsyncFedED. To the best of our\nknowledge this aggregation method is the first to take the staleness of the\narrived gradients, measured by the Euclidean distance between the stale model\nand the current global model, and the number of local epochs that have been\nperformed, into account. Assuming general non-convex loss functions, we prove\nthe convergence of the proposed method theoretically. Numerical results\nvalidate the effectiveness of the proposed AsyncFedED in terms of the\nconvergence rate and model accuracy compared to the existing methods for three\nconsidered tasks.",
    "descriptor": "",
    "authors": [
      "Qiyuan Wang",
      "Qianqian Yang",
      "Shibo He",
      "Zhiguo Shui",
      "Jiming Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13797"
  },
  {
    "id": "arXiv:2205.13799",
    "title": "Generalization Bounds for Gradient Methods via Discrete and Continuous  Prior",
    "abstract": "Proving algorithm-dependent generalization error bounds for gradient-type\noptimization methods has attracted significant attention recently in learning\ntheory. However, most existing trajectory-based analyses require either\nrestrictive assumptions on the learning rate (e.g., fast decreasing learning\nrate), or continuous injected noise (such as the Gaussian noise in Langevin\ndynamics). In this paper, we introduce a new discrete data-dependent prior to\nthe PAC-Bayesian framework, and prove a high probability generalization bound\nof order $O(\\frac{1}{n}\\cdot\n\\sum_{t=1}^T(\\gamma_t/\\varepsilon_t)^2\\left\\|{\\mathbf{g}_t}\\right\\|^2)$ for\nFloored GD (i.e. a version of gradient descent with precision level\n$\\varepsilon_t$), where $n$ is the number of training samples, $\\gamma_t$ is\nthe learning rate at step $t$, $\\mathbf{g}_t$ is roughly the difference of the\ngradient computed using all samples and that using only prior samples.\n$\\left\\|{\\mathbf{g}_t}\\right\\|$ is upper bounded by and and typical much\nsmaller than the gradient norm $\\left\\|{\\nabla f(W_t)}\\right\\|$. We remark that\nour bound holds for nonconvex and nonsmooth scenarios. Moreover, our\ntheoretical results provide numerically favorable upper bounds of testing\nerrors (e.g., $0.037$ on MNIST). Using a similar technique, we can also obtain\nnew generalization bounds for certain variants of SGD. Furthermore, we study\nthe generalization bounds for gradient Langevin Dynamics (GLD). Using the same\nframework with a carefully constructed continuous prior, we show a new high\nprobability generalization bound of order $O(\\frac{1}{n} +\n\\frac{L^2}{n^2}\\sum_{t=1}^T(\\gamma_t/\\sigma_t)^2)$ for GLD. The new $1/n^2$\nrate is due to the concentration of the difference between the gradient of\ntraining samples and that of the prior.",
    "descriptor": "",
    "authors": [
      "Jian Li",
      "Xuanyuan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13799"
  },
  {
    "id": "arXiv:2205.13801",
    "title": "Quantitative and Qualitative Assessment of Indoor Exploration Algorithms  for Autonomous UAVs",
    "abstract": "Indoor exploration is an important task in disaster relief, emergency\nresponse scenarios, and Search And Rescue (SAR) missions. Unmanned Aerial\nVehicle (UAV) systems can aid first responders by maneuvering autonomously in\nareas inside buildings dangerous for humans to traverse, exploring the\ninterior, and providing an accurate and reliable indoor map before the\nemergency response team takes action. Due to the challenging conditions in such\nscenarios and the inherent battery limitations and time constraints, we\ninvestigate 2D autonomous exploration strategies (e.g., based on 2D LiDAR) for\nmapping 3D indoor environments. First, we introduce a battery consumption model\nto consider the battery life aspect for the first time as a critical factor for\nevaluating the flight endurance of exploration strategies. Second, we perform\nextensive simulation experiments in diverse indoor environments using various\nstate-of-the-art 2D LiDAR-based exploration strategies. We evaluate our\nfindings in terms of various quantitative and qualitative performance\nindicators, suggesting that these strategies behave differently depending on\nthe complexity of the environment and initial conditions, e.g., the entry\npoint.",
    "descriptor": "\nComments: 2022 International Conference on Unmanned Aircraft Systems (ICUAS), June 21-24, 2022, Dubrovnik, Croatia (accepted)\n",
    "authors": [
      "Adil Farooq",
      "Christos Laoudias",
      "Panayiotis S. Kolios",
      "Theocharis Theocharides"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13801"
  },
  {
    "id": "arXiv:2205.13803",
    "title": "Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object  Interactions",
    "abstract": "A significant gap remains between today's visual pattern recognition models\nand human-level visual cognition especially when it comes to few-shot learning\nand compositional reasoning of novel concepts. We introduce Bongard-HOI, a new\nvisual reasoning benchmark that focuses on compositional learning of\nhuman-object interactions (HOIs) from natural images. It is inspired by two\ndesirable characteristics from the classical Bongard problems (BPs): 1)\nfew-shot concept learning, and 2) context-dependent reasoning. We carefully\ncurate the few-shot instances with hard negatives, where positive and negative\nimages only disagree on action labels, making mere recognition of object\ncategories insufficient to complete our benchmarks. We also design multiple\ntest sets to systematically study the generalization of visual learning models,\nwhere we vary the overlap of the HOI concepts between the training and test\nsets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a\nsubstantial challenge to today's visual recognition models. The\nstate-of-the-art HOI detection model achieves only 62% accuracy on few-shot\nbinary prediction while even amateur human testers on MTurk have 91% accuracy.\nWith the Bongard-HOI benchmark, we hope to further advance research efforts in\nvisual reasoning, especially in holistic perception-reasoning systems and\nbetter representation learning.",
    "descriptor": "\nComments: CVPR 2022 (oral); Code: this https URL\n",
    "authors": [
      "Huaizu Jiang",
      "Xiaojian Ma",
      "Weili Nie",
      "Zhiding Yu",
      "Yuke Zhu",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13803"
  },
  {
    "id": "arXiv:2205.13804",
    "title": "End-to-End Learning of Hybrid Inverse Dynamics Models for Precise and  Compliant Impedance Control",
    "abstract": "It is well-known that inverse dynamics models can improve tracking\nperformance in robot control. These models need to precisely capture the robot\ndynamics, which consist of well-understood components, e.g., rigid body\ndynamics, and effects that remain challenging to capture, e.g., stick-slip\nfriction and mechanical flexibilities. Such effects exhibit hysteresis and\npartial observability, rendering them, particularly challenging to model.\nHence, hybrid models, which combine a physical prior with data-driven\napproaches are especially well-suited in this setting. We present a novel\nhybrid model formulation that enables us to identify fully physically\nconsistent inertial parameters of a rigid body dynamics model which is paired\nwith a recurrent neural network architecture, allowing us to capture unmodeled\npartially observable effects using the network memory. We compare our approach\nagainst state-of-the-art inverse dynamics models on a 7 degree of freedom\nmanipulator. Using data sets obtained through an optimal experiment design\napproach, we study the accuracy of offline torque prediction and generalization\ncapabilities of joint learning methods. In control experiments on the real\nsystem, we evaluate the model as a feed-forward term for impedance control and\nshow the feedback gains can be drastically reduced to achieve a given tracking\naccuracy.",
    "descriptor": "\nComments: Accepted for publication at Robotics: Science and System XVIII (RSS), year 2022. Paper length is 13 pages (i.e. 9 pages of technical content, 1 page of the Bibliography/References and 3 pages of Appendix)\n",
    "authors": [
      "Moritz Reuss",
      "Niels van Duijkeren",
      "Robert Krug",
      "Philipp Becker",
      "Vaisakh Shaj",
      "Gerhard Neumann"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13804"
  },
  {
    "id": "arXiv:2205.13805",
    "title": "X-ViT: High Performance Linear Vision Transformer without Softmax",
    "abstract": "Vision transformers have become one of the most important models for computer\nvision tasks. Although they outperform prior works, they require heavy\ncomputational resources on a scale that is quadratic to the number of tokens,\n$N$. This is a major drawback of the traditional self-attention (SA) algorithm.\nHere, we propose the X-ViT, ViT with a novel SA mechanism that has linear\ncomplexity. The main approach of this work is to eliminate nonlinearity from\nthe original SA. We factorize the matrix multiplication of the SA mechanism\nwithout complicated linear approximation. By modifying only a few lines of code\nfrom the original SA, the proposed models outperform most transformer-based\nmodels on image classification and dense prediction tasks on most capacity\nregimes.",
    "descriptor": "",
    "authors": [
      "Jeonggeun Song",
      "Heung-Chang Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13805"
  },
  {
    "id": "arXiv:2205.13807",
    "title": "fakeWeather: Adversarial Attacks for Deep Neural Networks Emulating  Weather Conditions on the Camera Lens of Autonomous Systems",
    "abstract": "Recently, Deep Neural Networks (DNNs) have achieved remarkable performances\nin many applications, while several studies have enhanced their vulnerabilities\nto malicious attacks. In this paper, we emulate the effects of natural weather\nconditions to introduce plausible perturbations that mislead the DNNs. By\nobserving the effects of such atmospheric perturbations on the camera lenses,\nwe model the patterns to create different masks that fake the effects of rain,\nsnow, and hail. Even though the perturbations introduced by our attacks are\nvisible, their presence remains unnoticed due to their association with natural\nevents, which can be especially catastrophic for fully-autonomous and unmanned\nvehicles. We test our proposed fakeWeather attacks on multiple Convolutional\nNeural Network and Capsule Network models, and report noticeable accuracy drops\nin the presence of such adversarial perturbations. Our work introduces a new\nsecurity threat for DNNs, which is especially severe for safety-critical\napplications and autonomous systems.",
    "descriptor": "\nComments: To appear at the 2022 International Joint Conference on Neural Networks (IJCNN), at the 2022 IEEE World Congress on Computational Intelligence (WCCI), July 2022, Padua, Italy\n",
    "authors": [
      "Alberto Marchisio",
      "Giovanni Caramia",
      "Maurizio Martina",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13807"
  },
  {
    "id": "arXiv:2205.13808",
    "title": "Hide and Seek -- Preserving Location Privacy and Utility in the Remote  Identification of Unmanned Aerial Vehicles",
    "abstract": "Due to the frequent unauthorized access by commercial drones to Critical\nInfrastructures (CIs) such as airports and oil refineries, the US-based Federal\nAvionics Administration (FAA) recently published a new specification, namely\nRemoteID. The aforementioned rule mandates that all Unmanned Aerial Vehicles\n(UAVs) have to broadcast information about their identity and location\nwirelessly to allow for immediate invasion attribution. However, the\nenforcement of such a rule poses severe concerns on UAV operators, especially\nin terms of location privacy and tracking threats, to name a few. Indeed, by\nsimply eavesdropping on the wireless channel, an adversary could know the\nprecise location of the UAV and track it, as well as obtaining sensitive\ninformation on path source and destination of the UAV. In this paper, we\ninvestigate the trade-off between location privacy and data utility that can be\nprovided to UAVs when obfuscating the broadcasted location through differential\nprivacy techniques. Leveraging the concept of Geo-Indistinguishability\n(Geo-Ind), already adopted in the context of Location-Based Services (LBS), we\nshow that it is possible to enhance the privacy of the UAVs without preventing\nCI operators to timely detect unauthorized invasions. In particular, our\nexperiments showed that when the location of an UAV is obfuscated with an\naverage distance of 1.959 km, a carefully designed UAV detection system can\ndetect 97.9% of invasions, with an average detection delay of 303.97 msec. The\nUAVs have to trade-off such enhanced location privacy with a non-negligible\nprobability of false positives, i.e., being detected as invading while not\nreally invading the no-fly zone. UAVs and CI operators can solve such ambiguous\nsituations later on through the help of the FAA, being this latter the only one\nthat can unveil the actual location of the UAV.",
    "descriptor": "",
    "authors": [
      "Alessandro Brighente",
      "Mauro Conti",
      "Savio Sciancalepore"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13808"
  },
  {
    "id": "arXiv:2205.13814",
    "title": "Global Convergence of Over-parameterized Deep Equilibrium Models",
    "abstract": "A deep equilibrium model (DEQ) is implicitly defined through an equilibrium\npoint of an infinite-depth weight-tied model with an input-injection. Instead\nof infinite computations, it solves an equilibrium point directly with\nroot-finding and computes gradients with implicit differentiation. The training\ndynamics of over-parameterized DEQs are investigated in this study. By\nsupposing a condition on the initial equilibrium point, we show that the unique\nequilibrium point always exists during the training process, and the gradient\ndescent is proved to converge to a globally optimal solution at a linear\nconvergence rate for the quadratic loss function. In order to show that the\nrequired initial condition is satisfied via mild over-parameterization, we\nperform a fine-grained analysis on random DEQs. We propose a novel\nprobabilistic framework to overcome the technical difficulty in the\nnon-asymptotic analysis of infinite-depth weight-tied models.",
    "descriptor": "",
    "authors": [
      "Zenan Ling",
      "Xingyu Xie",
      "Qiuhao Wang",
      "Zongpeng Zhang",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13814"
  },
  {
    "id": "arXiv:2205.13817",
    "title": "Isolating and Leveraging Controllable and Noncontrollable Visual  Dynamics in World Models",
    "abstract": "World models learn the consequences of actions in vision-based interactive\nsystems. However, in practical scenarios such as autonomous driving, there\ncommonly exists noncontrollable dynamics independent of the action signals,\nmaking it difficult to learn effective world models. To tackle this problem, we\npresent a novel reinforcement learning approach named Iso-Dream, which improves\nthe Dream-to-Control framework in two aspects. First, by optimizing the inverse\ndynamics, we encourage the world model to learn controllable and\nnoncontrollable sources of spatiotemporal changes on isolated state transition\nbranches. Second, we optimize the behavior of the agent on the decoupled latent\nimaginations of the world model. Specifically, to estimate state values, we\nroll-out the noncontrollable states into the future and associate them with the\ncurrent controllable state. In this way, the isolation of dynamics sources can\ngreatly benefit long-horizon decision-making of the agent, such as a\nself-driving car that can avoid potential risks by anticipating the movement of\nother vehicles. Experiments show that Iso-Dream is effective in decoupling the\nmixed dynamics and remarkably outperforms existing approaches in a wide range\nof visual control and prediction domains.",
    "descriptor": "",
    "authors": [
      "Minting Pan",
      "Xiangming Zhu",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13817"
  },
  {
    "id": "arXiv:2205.13821",
    "title": "A Look at Improving Robustness in Visual-inertial SLAM by Moment  Matching",
    "abstract": "The fusion of camera sensor and inertial data is a leading method for\nego-motion tracking in autonomous and smart devices. State estimation\ntechniques that rely on non-linear filtering are a strong paradigm for solving\nthe associated information fusion task. The de facto inference method in this\nspace is the celebrated extended Kalman filter (EKF), which relies on\nfirst-order linearizations of both the dynamical and measurement model. This\npaper takes a critical look at the practical implications and limitations posed\nby the EKF, especially under faulty visual feature associations and the\npresence of strong confounding noise. As an alternative, we revisit the assumed\ndensity formulation of Bayesian filtering and employ a moment matching\n(unscented Kalman filtering) approach to both visual-inertial odometry and\nvisual SLAM. Our results highlight important aspects in robustness both in\ndynamics propagation and visual measurement updates, and we show\nstate-of-the-art results on EuRoC MAV drone data benchmark.",
    "descriptor": "\nComments: 8 pages, to appear in Proceedings of FUSION 2022\n",
    "authors": [
      "Arno Solin",
      "Rui Li",
      "Andrea Pilzer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13821"
  },
  {
    "id": "arXiv:2205.13826",
    "title": "Multivariate Probabilistic Forecasting of Intraday Electricity Prices  using Normalizing Flows",
    "abstract": "Electricity is traded on various markets with different time horizons and\nregulations. Short-term trading becomes increasingly important due to higher\npenetration of renewables. In Germany, the intraday electricity price typically\nfluctuates around the day-ahead price of the EPEX spot markets in a distinct\nhourly pattern. This work proposes a probabilistic modeling approach that\nmodels the intraday price difference to the day-ahead contracts. The model\ncaptures the emerging hourly pattern by considering the four 15 min intervals\nin each day-ahead price interval as a four-dimensional joint distribution. The\nresulting nontrivial, multivariate price difference distribution is learned\nusing a normalizing flow, i.e., a deep generative model that combines\nconditional multivariate density estimation and probabilistic regression. The\nnormalizing flow is compared to a selection of historical data, a Gaussian\ncopula, and a Gaussian regression model. Among the different models, the\nnormalizing flow identifies the trends most accurately and has the narrowest\nprediction intervals. Notably, the normalizing flow is the only approach that\nidentifies rare price peaks. Finally, this work discusses the influence of\ndifferent external impact factors and finds that, individually, most of these\nfactors have negligible impact. Only the immediate history of the price\ndifference realization and the combination of all input factors lead to notable\nimprovements in the forecasts.",
    "descriptor": "\nComments: manuscript (16 pages, 9 figures, 3 tables), supporting information (3 pages, 5 figures)\n",
    "authors": [
      "Eike Cramer",
      "Dirk Witthaut",
      "Alexander Mitsos",
      "Manuel Dahmen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13826"
  },
  {
    "id": "arXiv:2205.13832",
    "title": "Counterfactual Analysis in Dynamic Models: Copulas and Bounds",
    "abstract": "We provide an explicit model of the causal mechanism in a structural causal\nmodel (SCM) with the goal of estimating counterfactual quantities of interest\n(CQIs). We propose some standard dependence structures, i.e. copulas, as base\ncases for the causal mechanism. While these base cases can be used to construct\nmore interesting copulas, there are uncountably many copulas in general and so\nwe formulate optimization problems for bounding the CQIs. As our ultimate goal\nis counterfactual reasoning in dynamic models which may have latent-states, we\nshow by way of example that filtering / smoothing / sampling methods for these\nmodels can be integrated with our modeling of the causal mechanism.\nSpecifically, we consider the \"cheating-at-the-casino\" application of a hidden\nMarkov model and use linear programming (LP) to construct lower and upper\nbounds on the casino's winnings due to cheating. These bounds are considerably\ntighter when we constrain the copulas in the LPs to be time-independent. We can\ncharacterize the entire space of SCMs obeying counterfactual stability (CS),\nand we use it to negatively answer the open question of Oberst and Sontag [18]\nregarding the uniqueness of the Gumbel-max mechanism for modeling CS. Our work\nhas applications in epidemiology and legal reasoning, and more generally in\ncounterfactual off-policy evaluation, a topic of increasing interest in the\nreinforcement learning community.",
    "descriptor": "\nComments: 20 pages, 5 figures\n",
    "authors": [
      "Martin Haugh",
      "Raghav Singal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13832"
  },
  {
    "id": "arXiv:2205.13833",
    "title": "A Decentralised Control Strategy for Secondary Voltage Regulation",
    "abstract": "This paper proposes a decentralised secondary voltage control strategy that\nhas several benefits over the existing centralised strategies. For that, a new\nstructure for the control is proposed in terms of an inner and outer loops for\neach generator. The individual generators of a particular zone participate in\nthe secondary voltage control by aligning their reactive powers with respect to\nthe pilot point voltage reference. The decentralised nature of the proposed\ncontrol strategy enables plug and play operation: run with different numbers of\ngenerators without any need of regulators reconfiguration, resilience in case\nof generator failure. This allows one to use such control to integrate\nrenewable generators to existing secondary regulations alongside with the\nclassic generators. The proposed control strategy is implemented using a\nmodel-free control scheme that does not require a higher order complex model of\nthe power grid. The deployment of a discrete-time intelligent proportional\ncontroller simplifies the tuning of the controller gains. The proposed strategy\nis validated on a four generator power system model in MATLAB/\nSimulink/Simelectrical environment. Simulation results are presented to show\nthe effectiveness of the proposed strategy along with different case studies\nsuch as load perturbation, transmission line perturbation, generator\ndisconnection and delay in pilot point voltage measurement to highlight its\nrobustness.",
    "descriptor": "",
    "authors": [
      "Jitendra Kumar Goyal",
      "Vinu Thomas",
      "Bogdan Marinescu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13833"
  },
  {
    "id": "arXiv:2205.13834",
    "title": "Improving Bidding and Playing Strategies in the Trick-Taking game Wizard  using Deep Q-Networks",
    "abstract": "In this work, the trick-taking game Wizard with a separate bidding and\nplaying phase is modeled by two interleaved partially observable Markov\ndecision processes (POMDP). Deep Q-Networks (DQN) are used to empower\nself-improving agents, which are capable of tackling the challenges of a highly\nnon-stationary environment. To compare algorithms between each other, the\naccuracy between bid and trick count is monitored, which strongly correlates\nwith the actual rewards and provides a well-defined upper and lower performance\nbound. The trained DQN agents achieve accuracies between 66% and 87% in\nself-play, leaving behind both a random baseline and a rule-based heuristic.\nThe conducted analysis also reveals a strong information asymmetry concerning\nplayer positions during bidding. To overcome the missing Markov property of\nimperfect-information games, a long short-term memory (LSTM) network is\nimplemented to integrate historic information into the decision-making process.\nAdditionally, a forward-directed tree search is conducted by sampling a state\nof the environment and thereby turning the game into a perfect information\nsetting. To our surprise, both approaches do not surpass the performance of the\nbasic DQN agent.",
    "descriptor": "\nComments: Accepted to IEEE CoG 2022, 8 pages, 17 figures\n",
    "authors": [
      "Jonas Schumacher",
      "Marco Pleines"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13834"
  },
  {
    "id": "arXiv:2205.13836",
    "title": "Feudal Multi-Agent Reinforcement Learning with Adaptive Network  Partition for Traffic Signal Control",
    "abstract": "Multi-agent reinforcement learning (MARL) has been applied and shown great\npotential in multi-intersections traffic signal control, where multiple agents,\none for each intersection, must cooperate together to optimize traffic flow. To\nencourage global cooperation, previous work partitions the traffic network into\nseveral regions and learns policies for agents in a feudal structure. However,\nstatic network partition fails to adapt to dynamic traffic flow, which will\nchanges frequently over time. To address this, we propose a novel feudal MARL\napproach with adaptive network partition. Specifically, we first partition the\nnetwork into several regions according to the traffic flow. To do this, we\npropose two approaches: one is directly to use graph neural network (GNN) to\ngenerate the network partition, and the other is to use Monte-Carlo tree search\n(MCTS) to find the best partition with criteria computed by GNN. Then, we\ndesign a variant of Qmix using GNN to handle various dimensions of input, given\nby the dynamic network partition. Finally, we use a feudal hierarchy to manage\nagents in each partition and promote global cooperation. By doing so, agents\nare able to adapt to the traffic flow as required in practice. We empirically\nevaluate our method both in a synthetic traffic grid and real-world traffic\nnetworks of three cities, widely used in the literature. Our experimental\nresults confirm that our method can achieve better performance, in terms of\naverage travel time and queue length, than several leading methods for traffic\nsignal control.",
    "descriptor": "",
    "authors": [
      "Jinming Ma",
      "Feng Wu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13836"
  },
  {
    "id": "arXiv:2205.13838",
    "title": "Adaptive Random Forests for Energy-Efficient Inference on  Microcontrollers",
    "abstract": "Random Forests (RFs) are widely used Machine Learning models in low-power\nembedded devices, due to their hardware friendly operation and high accuracy on\npractically relevant tasks. The accuracy of a RF often increases with the\nnumber of internal weak learners (decision trees), but at the cost of a\nproportional increase in inference latency and energy consumption. Such costs\ncan be mitigated considering that, in most applications, inputs are not all\nequally difficult to classify. Therefore, a large RF is often necessary only\nfor (few) hard inputs, and wasteful for easier ones. In this work, we propose\nan early-stopping mechanism for RFs, which terminates the inference as soon as\na high-enough classification confidence is reached, reducing the number of weak\nlearners executed for easy inputs. The early-stopping confidence threshold can\nbe controlled at runtime, in order to favor either energy saving or accuracy.\nWe apply our method to three different embedded classification tasks, on a\nsingle-core RISC-V microcontroller, achieving an energy reduction from 38% to\nmore than 90% with a drop of less than 0.5% in accuracy. We also show that our\napproach outperforms previous adaptive ML methods for RFs.",
    "descriptor": "\nComments: Published in: 2021 IFIP/IEEE 29th International Conference on Very Large Scale Integration (VLSI-SoC), 2021\n",
    "authors": [
      "Francesco Daghero",
      "Alessio Burrello",
      "Chen Xie",
      "Luca Benini",
      "Andrea Calimera",
      "Enrico Macii",
      "Massimo Poncino",
      "Daniele Jahier Pagliari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13838"
  },
  {
    "id": "arXiv:2205.13842",
    "title": "Krylov subspace restarting for matrix Laplace transforms",
    "abstract": "A common way to approximate $F(A)b$ -- the action of a matrix function on a\nvector -- is to use the Arnoldi approximation. Since a new vector needs to be\ngenerated and stored in every iteration, one is often forced to rely on restart\nalgorithms which are either not efficient, not stable or only applicable to\nrestricted classes of functions. We present a new representation of the error\nof the Arnoldi iterates if the function $F$ is given as a Laplace transform.\nBased on this representation we build an efficient and stable restart\nalgorithm. In doing so we extend earlier work for the class of Stieltjes\nfunctions which are special Laplace transforms. We report several numerical\nexperiments including comparisons with the restart method for Stieltjes\nfunctions.",
    "descriptor": "",
    "authors": [
      "Andreas Frommer",
      "Karsten Kahl",
      "Marcel Schweitzer",
      "Manuel Tsolakis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13842"
  },
  {
    "id": "arXiv:2205.13845",
    "title": "Raising the Bar in Graph-level Anomaly Detection",
    "abstract": "Graph-level anomaly detection has become a critical topic in diverse areas,\nsuch as financial fraud detection and detecting anomalous activities in social\nnetworks. While most research has focused on anomaly detection for visual data\nsuch as images, where high detection accuracies have been obtained, existing\ndeep learning approaches for graphs currently show considerably worse\nperformance. This paper raises the bar on graph-level anomaly detection, i.e.,\nthe task of detecting abnormal graphs in a set of graphs. By drawing on ideas\nfrom self-supervised learning and transformation learning, we present a new\ndeep learning approach that significantly improves existing deep one-class\napproaches by fixing some of their known problems, including hypersphere\ncollapse and performance flip. Experiments on nine real-world data sets\ninvolving nine techniques reveal that our method achieves an average\nperformance improvement of 11.8% AUC compared to the best existing approach.",
    "descriptor": "\nComments: To appear in IJCAI-ECAI 2022\n",
    "authors": [
      "Chen Qiu",
      "Marius Kloft",
      "Stephan Mandt",
      "Maja Rudolph"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13845"
  },
  {
    "id": "arXiv:2205.13846",
    "title": "On the Convergence of Semi-Relaxed Sinkhorn with Marginal Constraint and  OT Distance Gaps",
    "abstract": "This paper presents consideration of the Semi-Relaxed Sinkhorn (SR-Sinkhorn)\nalgorithm for the semi-relaxed optimal transport (SROT) problem, which relaxes\none marginal constraint of the standard OT problem. For evaluation of how the\nconstraint relaxation affects the algorithm behavior and solution, it is\nvitally necessary to present the theoretical convergence analysis in terms not\nonly of the functional value gap, but also of the marginal constraint gap as\nwell as the OT distance gap. However, no existing work has addressed all\nanalyses simultaneously. To this end, this paper presents a comprehensive\nconvergence analysis for SR-Sinkhorn. After presenting the\n$\\epsilon$-approximation of the functional value gap based on a new proof\nstrategy and exploiting this proof strategy, we give the upper bound of the\nmarginal constraint gap. We also provide its convergence to the\n$\\epsilon$-approximation when two distributions are in the probability simplex.\nFurthermore, the convergence analysis of the OT distance gap to the\n$\\epsilon$-approximation is given as assisted by the obtained marginal\nconstraint gap. The latter two theoretical results are the first results\npresented in the literature related to the SROT problem.",
    "descriptor": "",
    "authors": [
      "Takumi Fukunaga",
      "Hiroyuki Kasai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13846"
  },
  {
    "id": "arXiv:2205.13857",
    "title": "TrackNet: A Triplet metric-based method for Multi-Target Multi-Camera  Vehicle Tracking",
    "abstract": "We present TrackNet, a method for Multi-Target Multi-Camera (MTMC) vehicle\ntracking from traffic video sequences. Cross-camera vehicle tracking has proved\nto be a challenging task due to perspective, scale and speed variance, as well\nocclusions and noise conditions. Our method is based on a modular approach that\nfirst detects vehicles frame-by-frame using Faster R-CNN, then tracks\ndetections through single camera using Kalman filter, and finally matches\ntracks by a triplet metric learning strategy. We conduct experiments on\nTrackNet within the AI City Challenge framework, and present competitive IDF1\nresults of 0.4733.",
    "descriptor": "\nComments: 4 pages, 2 figures\n",
    "authors": [
      "David Serrano",
      "Francesc Net",
      "Juan Antonio Rodr\u00edguez",
      "Igor Ugarte"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13857"
  },
  {
    "id": "arXiv:2205.13858",
    "title": "Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word  Embeddings",
    "abstract": "Word embeddings have advanced the state of the art in NLP across numerous\ntasks. Understanding the contents of dense neural representations is of utmost\ninterest to the computational semantics community. We propose to focus on\nrelating these opaque word vectors with human-readable definitions, as found in\ndictionaries. This problem naturally divides into two subtasks: converting\ndefinitions into embeddings, and converting embeddings into definitions. This\ntask was conducted in a multilingual setting, using comparable sets of\nembeddings trained homogeneously.",
    "descriptor": "",
    "authors": [
      "Timothee Mickus",
      "Kees van Deemter",
      "Mathieu Constant",
      "Denis Paperno"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13858"
  },
  {
    "id": "arXiv:2205.13863",
    "title": "Why Robust Generalization in Deep Learning is Difficult: Perspective of  Expressive Power",
    "abstract": "It is well-known that modern neural networks are vulnerable to adversarial\nexamples. To mitigate this problem, a series of robust learning algorithms have\nbeen proposed. However, although the robust training error can be near zero via\nsome methods, all existing algorithms lead to a high robust generalization\nerror. In this paper, we provide a theoretical understanding of this puzzling\nphenomenon from the perspective of expressive power for deep neural networks.\nSpecifically, for binary classification problems with well-separated data, we\nshow that, for ReLU networks, while mild over-parameterization is sufficient\nfor high robust training accuracy, there exists a constant robust\ngeneralization gap unless the size of the neural network is exponential in the\ndata dimension $d$. Even if the data is linear separable, which means achieving\nlow clean generalization error is easy, we can still prove an\n$\\exp({\\Omega}(d))$ lower bound for robust generalization. Moreover, we\nestablish an improved upper bound of $\\exp({\\mathcal{O}}(k))$ for the network\nsize to achieve low robust generalization error when the data lies on a\nmanifold with intrinsic dimension $k$ ($k \\ll d$). Nonetheless, we also have a\nlower bound that grows exponentially with respect to $k$ -- the curse of\ndimensionality is inevitable. By demonstrating an exponential separation\nbetween the network size for achieving low robust training and generalization\nerror, our results reveal that the hardness of robust generalization may stem\nfrom the expressive power of practical models.",
    "descriptor": "",
    "authors": [
      "Binghui Li",
      "Jikai Jin",
      "Han Zhong",
      "John E. Hopcroft",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13863"
  },
  {
    "id": "arXiv:2205.13866",
    "title": "Task Offloading with Multi-Tier Computing Resources in Next Generation  Wireless Networks",
    "abstract": "With the development of next-generation wireless networks, the Internet of\nThings (IoT) is evolving towards the intelligent IoT (iIoT), where intelligent\napplications usually have stringent delay and jitter requirements. In order to\nprovide low-latency services to heterogeneous users in the emerging iIoT,\nmulti-tier computing was proposed by effectively combining edge computing and\nfog computing. More specifically, multi-tier computing systems compensate for\ncloud computing through task offloading and dispersing computing tasks to\nmulti-tier nodes along the continuum from the cloud to things. In this paper,\nwe investigate key techniques and directions for wireless communications and\nresource allocation approaches to enable task offloading in multi-tier\ncomputing systems. A multi-tier computing model, with its main functionality\nand optimization methods, is presented in details. We hope that this paper will\nserve as a valuable reference and guide to the theoretical, algorithmic, and\nsystematic opportunities of multi-tier computing towards next-generation\nwireless networks.",
    "descriptor": "",
    "authors": [
      "Kunlun Wang",
      "Jiong Jin",
      "Yang Yang",
      "Tao Zhang",
      "Arumugam Nallanathan",
      "Chintha Tellambura",
      "Bijan Jabbari"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.13866"
  },
  {
    "id": "arXiv:2205.13867",
    "title": "Energy-efficient transmission policies for the linear quadratic control  of scalar systems",
    "abstract": "This paper considers controlled scalar systems relying on a lossy wireless\nfeedback channel. In contrast with the existing literature, the focus is not on\nthe system controller but on the wireless transmit power controller that is\nimplemented at the system side for reporting the state to the controller. Such\na problem may be of interest, \\emph{e.g.}, for the remote control of drones,\nwhere communication costs may have to be considered. Determining the power\ncontrol policy that minimizes the combination of the dynamical system cost and\nthe wireless transmission energy is shown to be a non-trivial optimization\nproblem. It turns out that the recursive structure of the problem can be\nexploited to determine the optimal power control policy. As illustrated in the\nnumerical performance analysis, in the scenario of a dynamics without\nperturbations, the optimal power control policy consists in decreasing the\ntransmit power at the right pace. This allows a significant performance gain\ncompared to conventional policies such as the full transmit power policy or the\nopen-loop policy.",
    "descriptor": "",
    "authors": [
      "Yifei Sun",
      "Samson Lasaulce",
      "Michel Kieffer",
      "Romain Postoyan",
      "Dragan Ne\u0161i\u0107"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13867"
  },
  {
    "id": "arXiv:2205.13869",
    "title": "MissDAG: Causal Discovery in the Presence of Missing Data with  Continuous Additive Noise Models",
    "abstract": "State-of-the-art causal discovery methods usually assume that the\nobservational data is complete. However, the missing data problem is pervasive\nin many practical scenarios such as clinical trials, economics, and biology.\nOne straightforward way to address the missing data problem is first to impute\nthe data using off-the-shelf imputation methods and then apply existing causal\ndiscovery methods. However, such a two-step method may suffer from\nsuboptimality, as the imputation algorithm is unaware of the causal discovery\nstep. In this paper, we develop a general method, which we call MissDAG, to\nperform causal discovery from data with incomplete observations. Focusing\nmainly on the assumptions of ignorable missingness and the identifiable\nadditive noise models (ANMs), MissDAG maximizes the expected likelihood of the\nvisible part of observations under the expectation-maximization (EM) framework.\nIn the E-step, in cases where computing the posterior distributions of\nparameters in closed-form is not feasible, Monte Carlo EM is leveraged to\napproximate the likelihood. In the M-step, MissDAG leverages the density\ntransformation to model the noise distributions with simpler and specific\nformulations by virtue of the ANMs and uses a likelihood-based causal discovery\nalgorithm with directed acyclic graph prior as an inductive bias. We\ndemonstrate the flexibility of MissDAG for incorporating various causal\ndiscovery algorithms and its efficacy through extensive simulations and real\ndata experiments.",
    "descriptor": "",
    "authors": [
      "Erdun Gao",
      "Ignavier Ng",
      "Mingming Gong",
      "Li Shen",
      "Wei Huang",
      "Tongliang Liu",
      "Kun Zhang",
      "Howard Bondell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13869"
  },
  {
    "id": "arXiv:2205.13870",
    "title": "Move and Time Optimal Arbitrary Pattern Formation by Asynchronous Robots  on Infinite Grid",
    "abstract": "The \\textsc{Arbitrary Pattern Formation} (\\textsc{Apf}) is a widely studied\nin distributed computing for swarm robots. This problem asks to design a\ndistributed algorithm that allows a team of identical, autonomous mobile robots\nto form any arbitrary pattern given as input. This paper considers that the\nrobots are operating on a two-dimensional infinite grid. Robots are initially\npositioned on distinct grid points forming an asymmetric configuration (no two\nrobots have the same snapshot). They operate under a fully asynchronous\nscheduler and do not have any access to a global coordinate system, but they\nwill align the axes of their local coordinate systems along the grid lines. The\nprevious work dealing with \\textsc{Apf} problem solved it in\n$O(\\mathcal{D}^2k)$ robot movements under similar conditions, where\n$\\mathcal{D}$ is the side of the smallest square that can contain both initial\nand target configuration and, $k$ is the number of robots. Let\n$\\mathcal{D}'=\\max\\{\\mathcal{D},k\\}$. This paper presents two algorithms of\n\\textsc{Apf} on an infinite grid. The first algorithm solves the \\textsc{Apf}\nproblem using $O(\\mathcal{D}')$ asymptotically move optimal. The second\nalgorithm solves the \\textsc{Apf} problem in $O(\\mathcal{D}')$ epochs, which we\nshow is asymptotically optimal.",
    "descriptor": "",
    "authors": [
      "Satakshi Ghosh",
      "Pritam Goswami",
      "Avisek Sharma",
      "Buddhadeb Sau"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.13870"
  },
  {
    "id": "arXiv:2205.13871",
    "title": "Probabilistic Systems with Hidden State and Unobservable Transitions",
    "abstract": "We consider probabilistic systems with hidden state and unobservable\ntransitions, an extension of Hidden Markov Models (HMMs) that in particular\nadmits unobservable {\\epsilon}-transitions (also called null transitions),\nallowing state changes of which the observer is unaware. Due to the presence of\n{\\epsilon}-loops this additional feature complicates the theory and requires to\ncarefully set up the corresponding probability space and random variables. In\nparticular we present an algorithm for determining the most probable\nexplanation given an observation (a generalization of the Viterbi algorithm for\nHMMs) and a method for parameter learning that adapts the probabilities of a\ngiven model based on an observation (a generalization of the Baum-Welch\nalgorithm). The latter algorithm guarantees that the given observation has a\nhigher (or equal) probability after adjustment of the parameters and its\ncorrectness can be derived directly from the so-called EM algorithm.",
    "descriptor": "",
    "authors": [
      "Rebecca Bernemann",
      "Barbara K\u00f6nig",
      "Matthias Schaffeld",
      "Torben Weis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13871"
  },
  {
    "id": "arXiv:2205.13872",
    "title": "Self-Admitted Technical Debt in the Embedded Systems Industry: An  Exploratory Case Study",
    "abstract": "Technical debt denotes shortcuts taken during software development, mostly\nfor the sake of expedience. When such shortcuts are admitted explicitly by\ndevelopers (e.g., writing a TODO/Fixme comment), they are termed as\nSelf-Admitted Technical Debt or SATD. There has been a fair amount of work\nstudying SATD management in Open Source projects, but SATD in industry is\nrelatively unexplored. At the same time, there is no work focusing on\ndevelopers' perspectives towards SATD and its management. To address this, we\nconducted an exploratory case study in cooperation with an industrial partner\nto study how they think of SATD and how they manage it. Specifically, we\ncollected data by identifying and characterizing SATD in different sources\n(issues, source code comments and commits) and carried out a series of\ninterviews with 12 software practitioners. The results show: 1) the core\ncharacteristics of SATD in industrial projects; 2) developers' attitudes\ntowards identified SATD and statistics; 3) triggers for practitioners to\nintroduce and repay SATD; 4) relations between SATD in different sources; 5)\npractices used to manage SATD; 6) challenges and tooling ideas for SATD\nmanagement.",
    "descriptor": "",
    "authors": [
      "Yikun Li",
      "Mohamed Soliman",
      "Paris Avgeriou",
      "Lou Somers"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.13872"
  },
  {
    "id": "arXiv:2205.13874",
    "title": "Comparison of Deep Learning Segmentation and Multigrader-annotated  Mandibular Canals of Multicenter CBCT scans",
    "abstract": "Deep learning approach has been demonstrated to automatically segment the\nbilateral mandibular canals from CBCT scans, yet systematic studies of its\nclinical and technical validation are scarce. To validate the mandibular canal\nlocalization accuracy of a deep learning system (DLS) we trained it with 982\nCBCT scans and evaluated using 150 scans of five scanners from clinical\nworkflow patients of European and Southeast Asian Institutes, annotated by four\nradiologists. The interobserver variability was compared to the variability\nbetween the DLS and the radiologists. In addition, the generalization of DLS to\nCBCT scans from scanners not used in the training data was examined to evaluate\nthe out-of-distribution generalization capability. The DLS had lower\nvariability to the radiologists than the interobserver variability between them\nand it was able to generalize to three new devices. For the radiologists'\nconsensus segmentation, used as gold standard, the DLS had a symmetric mean\ncurve distance of 0.39 mm compared to those of the individual radiologists with\n0.62 mm, 0.55 mm, 0.47 mm, and 0.42 mm. The DLS showed comparable or slightly\nbetter performance in the segmentation of the mandibular canal with the\nradiologists and generalization capability to new scanners.",
    "descriptor": "",
    "authors": [
      "Jorma J\u00e4rnstedt",
      "Jaakko Sahlsten",
      "Joel Jaskari",
      "Kimmo Kaski",
      "Helena Mehtonen",
      "Ziyuan Lin",
      "Ari Hietanen",
      "Osku Sundqvist",
      "Vesa Varjonen",
      "Vesa Mattila",
      "Sangsom Prapayasotok",
      "Sakarat Nalampang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13874"
  },
  {
    "id": "arXiv:2205.13879",
    "title": "MIMII DG: Sound Dataset for Malfunctioning Industrial Machine  Investigation and Inspection for Domain Generalization Task",
    "abstract": "We present a machine sound dataset to benchmark domain generalization\ntechniques for anomalous sound detection (ASD). To handle performance\ndegradation caused by domain shifts that are difficult to detect or too\nfrequent to adapt, domain generalization techniques are preferred. However,\ncurrently available datasets have difficulties in evaluating these techniques,\nsuch as limited number of values for parameters that cause domain shifts\n(domain shift parameters). In this paper, we present the first ASD dataset for\nthe domain generalization techniques, called MIMII DG. The dataset consists of\nfive machine types and three domain shift scenarios for each machine type. We\nprepared at least two values for the domain shift parameters in the source\ndomain. Also, we introduced domain shifts that can be difficult to notice.\nExperimental results using two baseline systems indicate that the dataset\nreproduces the domain shift scenarios and is useful for benchmarking domain\ngeneralization techniques.",
    "descriptor": "",
    "authors": [
      "Kota Dohi",
      "Tomoya Nishida",
      "Harsh Purohit",
      "Ryo Tanabe",
      "Takashi Endo",
      "Masaaki Yamamoto",
      "Yuki Nikaido",
      "Yohei Kawaguchi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.13879"
  },
  {
    "id": "arXiv:2205.13880",
    "title": "TraClets: Harnessing the power of computer vision for trajectory  classification",
    "abstract": "Due to the advent of new mobile devices and tracking sensors in recent years,\nhuge amounts of data are being produced every day. Therefore, novel\nmethodologies need to emerge that dive through this vast sea of information and\ngenerate insights and meaningful information. To this end, researchers have\ndeveloped several trajectory classification algorithms over the years that are\nable to annotate tracking data. Similarly, in this research, a novel\nmethodology is presented that exploits image representations of trajectories,\ncalled TraClets, in order to classify trajectories in an intuitive humans way,\nthrough computer vision techniques. Several real-world datasets are used to\nevaluate the proposed approach and compare its classification performance to\nother state-of-the-art trajectory classification algorithms. Experimental\nresults demonstrate that TraClets achieves a classification performance that is\ncomparable to, or in most cases, better than the state-of-the-art, acting as a\nuniversal, high-accuracy approach for trajectory classification.",
    "descriptor": "",
    "authors": [
      "Ioannis Kontopoulos",
      "Antonios Makris",
      "Konstantinos Tserpes",
      "Vania Bogorny"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13880"
  },
  {
    "id": "arXiv:2205.13881",
    "title": "Automated Dynamic Algorithm Configuration",
    "abstract": "The performance of an algorithm often critically depends on its parameter\nconfiguration. While a variety of automated algorithm configuration methods\nhave been proposed to relieve users from the tedious and error-prone task of\nmanually tuning parameters, there is still a lot of untapped potential as the\nlearned configuration is static, i.e., parameter settings remain fixed\nthroughout the run. However, it has been shown that some algorithm parameters\nare best adjusted dynamically during execution, e.g., to adapt to the current\npart of the optimization landscape. Thus far, this is most commonly achieved\nthrough hand-crafted heuristics. A promising recent alternative is to\nautomatically learn such dynamic parameter adaptation policies from data. In\nthis article, we give the first comprehensive account of this new field of\nautomated dynamic algorithm configuration (DAC), present a series of recent\nadvances, and provide a solid foundation for future research in this field.\nSpecifically, we (i) situate DAC in the broader historical context of AI\nresearch; (ii) formalize DAC as a computational problem; (iii) identify the\nmethods used in prior-art to tackle this problem; (iv) conduct empirical case\nstudies for using DAC in evolutionary optimization, AI planning, and machine\nlearning.",
    "descriptor": "",
    "authors": [
      "Steven Adriaensen",
      "Andr\u00e9 Biedenkapp",
      "Gresa Shala",
      "Noor Awad",
      "Theresa Eimer",
      "Marius Lindauer",
      "Frank Hutter"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.13881"
  },
  {
    "id": "arXiv:2205.13882",
    "title": "How to Peel a Million: Validating and Expanding Bitcoin Clusters",
    "abstract": "One of the defining features of Bitcoin and the thousands of cryptocurrencies\nthat have been derived from it is a globally visible transaction ledger. While\nBitcoin uses pseudonyms as a way to hide the identity of its participants, a\nlong line of research has demonstrated that Bitcoin is not anonymous. This has\nbeen perhaps best exemplified by the development of clustering heuristics,\nwhich have in turn given rise to the ability to track the flow of bitcoins as\nthey are sent from one entity to another.\nIn this paper, we design a new heuristic that is designed to track a certain\ntype of flow, called a peel chain, that represents many transactions performed\nby the same entity; in doing this, we implicitly cluster these transactions and\ntheir associated pseudonyms together. We then use this heuristic to both\nvalidate and expand the results of existing clustering heuristics. We also\ndevelop a machine learning-based validation method and, using a ground-truth\ndataset, evaluate all our approaches and compare them with the state of the\nart. Ultimately, our goal is to not only enable more powerful tracking\ntechniques but also call attention to the limits of anonymity in these systems.",
    "descriptor": "",
    "authors": [
      "George Kappos",
      "Haaroon Yousaf",
      "Rainer St\u00fctz",
      "Sofia Rollet",
      "Bernhard Haslhofer",
      "Sarah Meiklejohn"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13882"
  },
  {
    "id": "arXiv:2205.13883",
    "title": "Efficient Semantic Summary Graphs for Querying Large Knowledge Graphs",
    "abstract": "Knowledge Graphs (KGs) integrate heterogeneous data, but one challenge is the\ndevelopment of efficient tools for allowing end users to extract useful\ninsights from these sources of knowledge. In such a context, reducing the size\nof a Resource Description Framework (RDF) graph while preserving all\ninformation can speed up query engines by limiting data shuffle, especially in\na distributed setting. This paper presents two algorithms for RDF graph\nsummarization: Grouping Based Summarization (GBS) and Query Based Summarization\n(QBS). The latter is an optimized and lossless approach for the former method.\nWe empirically study the effectiveness of the proposed lossless RDF graph\nsummarization to retrieve complete data, by rewriting an RDF Query Language\ncalled SPARQL query with fewer triple patterns using a semantic similarity. We\nconduct our experimental study in instances of four datasets with different\nsizes. Compared with the state-of-the-art query engine Sparklify executed over\nthe original RDF graphs as a baseline, QBS query execution time is reduced by\nup to 80% and the summarized RDF graph is decreased by up to 99%.",
    "descriptor": "",
    "authors": [
      "Emetis Niazmand",
      "Gezim Sejdiu",
      "Damien Graux",
      "Maria-Esther Vidal"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.13883"
  },
  {
    "id": "arXiv:2205.13884",
    "title": "Learning to Automate Follow-up Question Generation using Process  Knowledge for Depression Triage on Reddit Posts",
    "abstract": "Conversational Agents (CAs) powered with deep language models (DLMs) have\nshown tremendous promise in the domain of mental health. Prominently, the CAs\nhave been used to provide informational or therapeutic services to patients.\nHowever, the utility of CAs to assist in mental health triaging has not been\nexplored in the existing work as it requires a controlled generation of\nfollow-up questions (FQs), which are often initiated and guided by the mental\nhealth professionals (MHPs) in clinical settings. In the context of depression,\nour experiments show that DLMs coupled with process knowledge in a mental\nhealth questionnaire generate 12.54% and 9.37% better FQs based on similarity\nand longest common subsequence matches to questions in the PHQ-9 dataset\nrespectively, when compared with DLMs without process knowledge support.\nDespite coupling with process knowledge, we find that DLMs are still prone to\nhallucination, i.e., generating redundant, irrelevant, and unsafe FQs. We\ndemonstrate the challenge of using existing datasets to train a DLM for\ngenerating FQs that adhere to clinical process knowledge. To address this\nlimitation, we prepared an extended PHQ-9 based dataset, PRIMATE, in\ncollaboration with MHPs. PRIMATE contains annotations regarding whether a\nparticular question in the PHQ-9 dataset has already been answered in the\nuser's initial description of the mental health condition. We used PRIMATE to\ntrain a DLM in a supervised setting to identify which of the PHQ-9 questions\ncan be answered directly from the user's post and which ones would require more\ninformation from the user. Using performance analysis based on MCC scores, we\nshow that PRIMATE is appropriate for identifying questions in PHQ-9 that could\nguide generative DLMs towards controlled FQ generation suitable for aiding\ntriaging. Dataset created as a part of this research:\nhttps://github.com/primate-mh/Primate2022",
    "descriptor": "",
    "authors": [
      "Shrey Gupta",
      "Anmol Agarwal",
      "Manas Gaur",
      "Kaushik Roy",
      "Vignesh Narayanan",
      "Ponnurangam Kumaraguru",
      "Amit Sheth"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13884"
  },
  {
    "id": "arXiv:2205.13885",
    "title": "YouTubers Not madeForKids: Detecting Channels Sharing Inappropriate  Videos Targeting Children",
    "abstract": "In the last years, hundreds of new Youtube channels have been creating and\nsharing videos targeting children, with themes related to animation, superhero\nmovies, comics, etc. Unfortunately, many of these videos are inappropriate for\nconsumption by their target audience, due to disturbing, violent, or sexual\nscenes. In this paper, we study YouTube channels found to post suitable or\ndisturbing videos targeting kids in the past. We identify a clear discrepancy\nbetween what YouTube assumes and flags as inappropriate content and channel,\nvs. what is found to be disturbing content and still available on the platform,\ntargeting kids. In particular, we find that almost 60\\% of videos that were\nmanually annotated and classified as disturbing by an older study in 2019 (a\ncollection bootstrapped with Elsa and other keywords related to children\nvideos), are still available on YouTube in mid 2021. In the meantime, 44% of\nchannels that uploaded such disturbing videos, have yet to be suspended and\ntheir videos to be removed. For the first time in literature, we also study the\n\"madeForKids\" flag, a new feature that YouTube introduced in the end of 2019,\nand compare its application to the channels that shared disturbing videos, as\nflagged from the previous study. Apparently, these channels are less likely to\nbe set as \"madeForKids\" than those sharing suitable content. In addition,\nchannels posting disturbing videos utilize their channel features such as\nkeywords, description, topics, posts, etc., to appeal to kids (e.g., using\ngame-related keywords). Finally, we use a collection of such channel and\ncontent features to train ML classifiers able to detect, at channel creation\ntime, when a channel will be related to disturbing content uploads. These\nclassifiers can help YouTube moderators reduce such incidences, pointing to\npotentially suspicious accounts without analyzing actual videos.",
    "descriptor": "\nComments: 12 pages, 10 Tables, 23 Figures. In Proceedings of 14th ACM Web Science Conference 2022, Barcelona, Spain\n",
    "authors": [
      "Myrsini Gkolemi",
      "Panagiotis Papadopoulos",
      "Evangelos P. Markatos",
      "Nicolas Kourtellis"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.13885"
  },
  {
    "id": "arXiv:2205.13888",
    "title": "Bertrand-Game-Based On-Device Federated Learning Framework",
    "abstract": "Federated learning (FL) has been proposed as a popular learning framework to\nprotect the users' data privacy but it has difficulties in motivating the users\nto participate in task training. This paper proposes a Bertrand-game-based\nframework for FL in wireless networks, where the model server as a resource\nbuyer can issue an FL task, whereas the employed user equipment (UEs) as the\nresource sellers can help train the model by using their local data. Specially,\nthe influence of time-varying \\textit{task load} and \\textit{channel quality}\non UE's motivation to participate in FL is considered. Firstly, we adopt the\nfinite-state discrete-time Markov chain (FSDT-MC) method to predict the\n\\textit{existing task load} and \\textit{channel gain} of a UE during a FL task.\nDepending on the performance metrics set by the model server and the estimated\noverall energy cost for engaging in the FL task, each UE seeks the best price\nto maximize its own profit in the game. To this end, the Nash equilibrium (NE)\nof the game is obtained in closed form, and a distributed iterative algorithm\nis also developed to find the NE. Simulation result verifies the effectiveness\nof the proposed approach.",
    "descriptor": "",
    "authors": [
      "Jiawei Liu",
      "Guopeng Zhang",
      "Kezhi Wang",
      "Kun Yang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13888"
  },
  {
    "id": "arXiv:2205.13891",
    "title": "Transformers from an Optimization Perspective",
    "abstract": "Deep learning models such as the Transformer are often constructed by\nheuristics and experience. To provide a complementary foundation, in this work\nwe study the following problem: Is it possible to find an energy function\nunderlying the Transformer model, such that descent steps along this energy\ncorrespond with the Transformer forward pass? By finding such a function, we\ncan reinterpret Transformers as the unfolding of an interpretable optimization\nprocess across iterations. This unfolding perspective has been frequently\nadopted in the past to elucidate more straightforward deep models such as MLPs\nand CNNs; however, it has thus far remained elusive obtaining a similar\nequivalence for more complex models with self-attention mechanisms like the\nTransformer. To this end, we first outline several major obstacles before\nproviding companion techniques to at least partially address them,\ndemonstrating for the first time a close association between energy function\nminimization and deep layers with self-attention. This interpretation\ncontributes to our intuition and understanding of Transformers, while\npotentially laying the ground-work for new model designs.",
    "descriptor": "",
    "authors": [
      "Yongyi Yang",
      "Zengfeng Huang",
      "David Wipf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13891"
  },
  {
    "id": "arXiv:2205.13892",
    "title": "EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural  Networks",
    "abstract": "Graph Neural Networks (GNNs) have received extensive research attention for\ntheir promising performance in graph machine learning. Despite their\nextraordinary predictive accuracy, existing approaches, such as GCN and GPRGNN,\nare not robust in the face of homophily changes on test graphs, rendering these\nmodels vulnerable to graph structural attacks and with limited capacity in\ngeneralizing to graphs of varied homophily levels. Although many methods have\nbeen proposed to improve the robustness of GNN models, most of these techniques\nare restricted to the spatial domain and employ complicated defense mechanisms,\nsuch as learning new graph structures or calculating edge attentions. In this\npaper, we study the problem of designing simple and robust GNN models in the\nspectral domain. We propose EvenNet, a spectral GNN corresponding to an\neven-polynomial graph filter. Based on our theoretical analysis in both spatial\nand spectral domains, we demonstrate that EvenNet outperforms full-order models\nin generalizing across homophilic and heterophilic graphs, implying that\nignoring odd-hop neighbors improves the robustness of GNNs. We conduct\nexperiments on both synthetic and real-world datasets to demonstrate the\neffectiveness of EvenNet. Notably, EvenNet outperforms existing defense models\nagainst structural attacks without introducing additional computational costs\nand maintains competitiveness in traditional node classification tasks on\nhomophilic and heterophilic graphs.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Runlin Lei",
      "Zhen Wang",
      "Yaliang Li",
      "Bolin Ding",
      "Zhewei Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13892"
  },
  {
    "id": "arXiv:2205.13900",
    "title": "How Tempering Fixes Data Augmentation in Bayesian Neural Networks",
    "abstract": "While Bayesian neural networks (BNNs) provide a sound and principled\nalternative to standard neural networks, an artificial sharpening of the\nposterior usually needs to be applied to reach comparable performance. This is\nin stark contrast to theory, dictating that given an adequate prior and a\nwell-specified model, the untempered Bayesian posterior should achieve optimal\nperformance. Despite the community's extensive efforts, the observed gains in\nperformance still remain disputed with several plausible causes pointing at its\norigin. While data augmentation has been empirically recognized as one of the\nmain drivers of this effect, a theoretical account of its role, on the other\nhand, is largely missing. In this work we identify two interlaced factors\nconcurrently influencing the strength of the cold posterior effect, namely the\ncorrelated nature of augmentations and the degree of invariance of the employed\nmodel to such transformations. By theoretically analyzing simplified settings,\nwe prove that tempering implicitly reduces the misspecification arising from\nmodeling augmentations as i.i.d. data. The temperature mimics the role of the\neffective sample size, reflecting the gain in information provided by the\naugmentations. We corroborate our theoretical findings with extensive empirical\nevaluations, scaling to realistic BNNs. By relying on the framework of group\nconvolutions, we experiment with models of varying inherent degree of\ninvariance, confirming its hypothesized relationship with the optimal\ntemperature.",
    "descriptor": "",
    "authors": [
      "Gregor Bachmann",
      "Lorenzo Noci",
      "Thomas Hofmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13900"
  },
  {
    "id": "arXiv:2205.13901",
    "title": "Bias Reduction via Cooperative Bargaining in Synthetic Graph Dataset  Generation",
    "abstract": "In general, to draw robust conclusions from a dataset, all the analyzed\npopulation must be represented on said dataset. Having a dataset that does not\nfulfill this condition normally leads to selection bias. Additionally, graphs\nhave been used to model a wide variety of problems. Although synthetic graphs\ncan be used to augment available real graph datasets to overcome selection\nbias, the generation of unbiased synthetic datasets is complex with current\ntools. In this work, we propose a method to find a synthetic graph dataset that\nhas an even representation of graphs with different metrics. The resulting\ndataset can then be used, among others, for benchmarking graph processing\ntechniques as the accuracy of different Graph Neural Network (GNN) models or\nthe speedups obtained by different graph processing acceleration frameworks.",
    "descriptor": "",
    "authors": [
      "Axel Wassington",
      "Sergi Abadal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13901"
  },
  {
    "id": "arXiv:2205.13902",
    "title": "Sample-Efficient Optimisation with Probabilistic Transformer Surrogates",
    "abstract": "Faced with problems of increasing complexity, recent research in Bayesian\nOptimisation (BO) has focused on adapting deep probabilistic models as flexible\nalternatives to Gaussian Processes (GPs). In a similar vein, this paper\ninvestigates the feasibility of employing state-of-the-art probabilistic\ntransformers in BO. Upon further investigation, we observe two drawbacks\nstemming from their training procedure and loss definition, hindering their\ndirect deployment as proxies in black-box optimisation. First, we notice that\nthese models are trained on uniformly distributed inputs, which impairs\npredictive accuracy on non-uniform data - a setting arising from any typical BO\nloop due to exploration-exploitation trade-offs. Second, we realise that\ntraining losses (e.g., cross-entropy) only asymptotically guarantee accurate\nposterior approximations, i.e., after arriving at the global optimum, which\ngenerally cannot be ensured. At the stationary points of the loss function,\nhowever, we observe a degradation in predictive performance especially in\nexploratory regions of the input space. To tackle these shortcomings we\nintroduce two components: 1) a BO-tailored training prior supporting\nnon-uniformly distributed points, and 2) a novel approximate posterior\nregulariser trading-off accuracy and input sensitivity to filter favourable\nstationary points for improved predictive performance. In a large panel of\nexperiments, we demonstrate, for the first time, that one transformer\npre-trained on data sampled from random GP priors produces competitive results\non 16 benchmark black-boxes compared to GP-based BO. Since our model is only\npre-trained once and used in all tasks without any retraining and/or\nfine-tuning, we report an order of magnitude time-reduction, while matching and\nsometimes outperforming GPs.",
    "descriptor": "",
    "authors": [
      "Alexandre Maraval",
      "Matthieu Zimmer",
      "Antoine Grosnit",
      "Rasul Tutunov",
      "Jun Wang",
      "Haitham Bou Ammar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13902"
  },
  {
    "id": "arXiv:2205.13904",
    "title": "Secrecy Capacity Maximization for a Hybrid Relay-RIS Scheme in mmWave  MIMO Networks",
    "abstract": "The hybrid relay-reflecting intelligent surface (HR-RIS) has been recently\nintroduced as an efficient solution to overcome the double path loss and\nlimited beamforming diversity of the conventional fully passive reflecting\nsurface. This motivates us to investigate the application of the HR-RIS in\nimproving the secrecy capacity of millimeter wave\nmultiple-input-multiple-output (MIMO) systems with the presence of\nmulti-antenna eavesdropper. The joint optimization of the transmit beamformer\nand the relay-reflecting coefficients at RIS is tackled via alternating\noptimization. In the proposed solution, a closed-form expression for the\noptimal transmit beamformer at the transmitter is derived, and a metaheuristic\nsolution based on particle swarm optimization is proposed to optimize the\nactive and passive elements at the HR-RIS. The simulation results verify that\nunder various scenarios, the HR-RIS provides significant improvement in the\nsecrecy capacity with respect to the conventional passive reflecting surface.",
    "descriptor": "",
    "authors": [
      "Edson Nobuyuki Egashira",
      "Diana Pamela Moya Osorio",
      "Nhan Thanh Nguyen",
      "Markku Juntti"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.13904"
  },
  {
    "id": "arXiv:2205.13908",
    "title": "EmoInHindi: A Multi-label Emotion and Intensity Annotated Dataset in  Hindi for Emotion Recognition in Dialogues",
    "abstract": "The long-standing goal of Artificial Intelligence (AI) has been to create\nhuman-like conversational systems. Such systems should have the ability to\ndevelop an emotional connection with the users, hence emotion recognition in\ndialogues is an important task. Emotion detection in dialogues is a challenging\ntask because humans usually convey multiple emotions with varying degrees of\nintensities in a single utterance. Moreover, emotion in an utterance of a\ndialogue may be dependent on previous utterances making the task more complex.\nEmotion recognition has always been in great demand. However, most of the\nexisting datasets for multi-label emotion and intensity detection in\nconversations are in English. To this end, we create a large conversational\ndataset in Hindi named EmoInHindi for multi-label emotion and intensity\nrecognition in conversations containing 1,814 dialogues with a total of 44,247\nutterances. We prepare our dataset in a Wizard-of-Oz manner for mental health\nand legal counselling of crime victims. Each utterance of the dialogue is\nannotated with one or more emotion categories from the 16 emotion classes\nincluding neutral, and their corresponding intensity values. We further propose\nstrong contextual baselines that can detect emotion(s) and the corresponding\nintensity of an utterance given the conversational context.",
    "descriptor": "\nComments: This paper is accepted at LREC 2022\n",
    "authors": [
      "Gopendra Vikram Singh",
      "Priyanshu Priya",
      "Mauajama Firdaus",
      "Asif Ekbal",
      "Pushpak Bhattacharyya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13908"
  },
  {
    "id": "arXiv:2205.13909",
    "title": "(De-)Randomized Smoothing for Decision Stump Ensembles",
    "abstract": "Tree-based models are used in many high-stakes application domains such as\nfinance and medicine, where robustness and interpretability are of utmost\nimportance. Yet, methods for improving and certifying their robustness are\nseverely under-explored, in contrast to those focusing on neural networks.\nTargeting this important challenge, we propose deterministic smoothing for\ndecision stump ensembles. Whereas most prior work on randomized smoothing\nfocuses on evaluating arbitrary base models approximately under input\nrandomization, the key insight of our work is that decision stump ensembles\nenable exact yet efficient evaluation via dynamic programming. Importantly, we\nobtain deterministic robustness certificates, even jointly over numerical and\ncategorical features, a setting ubiquitous in the real world. Further, we\nderive an MLE-optimal training method for smoothed decision stumps under\nrandomization and propose two boosting approaches to improve their provable\nrobustness. An extensive experimental evaluation shows that our approach yields\nsignificantly higher certified accuracies than the state-of-the-art for\ntree-based models. We release all code and trained models at ANONYMIZED.",
    "descriptor": "",
    "authors": [
      "Mikl\u00f3s Z. Horv\u00e1th",
      "Mark Niklas M\u00fcller",
      "Marc Fischer",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13909"
  },
  {
    "id": "arXiv:2205.13913",
    "title": "Dynamic Domain Generalization",
    "abstract": "Domain generalization (DG) is a fundamental yet very challenging research\ntopic in machine learning. The existing arts mainly focus on learning\ndomain-invariant features with limited source domains in a static model.\nUnfortunately, there is a lack of training-free mechanism to adjust the model\nwhen generalized to the agnostic target domains. To tackle this problem, we\ndevelop a brand-new DG variant, namely Dynamic Domain Generalization (DDG), in\nwhich the model learns to twist the network parameters to adapt the data from\ndifferent domains. Specifically, we leverage a meta-adjuster to twist the\nnetwork parameters based on the static model with respect to different data\nfrom different domains. In this way, the static model is optimized to learn\ndomain-shared features, while the meta-adjuster is designed to learn\ndomain-specific features. To enable this process, DomainMix is exploited to\nsimulate data from diverse domains during teaching the meta-adjuster to adapt\nto the upcoming agnostic target domains. This learning mechanism urges the\nmodel to generalize to different agnostic target domains via adjusting the\nmodel without training. Extensive experiments demonstrate the effectiveness of\nour proposed method. Code is available at: https://github.com/MetaVisionLab/DDG",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Zhishu Sun",
      "Zhifeng Shen",
      "Luojun Lin",
      "Yuanlong Yu",
      "Zhifeng Yang",
      "Shicai Yang",
      "Weijie Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13913"
  },
  {
    "id": "arXiv:2205.13914",
    "title": "3DILG: Irregular Latent Grids for 3D Generative Modeling",
    "abstract": "We propose a new representation for encoding 3D shapes as neural fields. The\nrepresentation is designed to be compatible with the transformer architecture\nand to benefit both shape reconstruction and shape generation. Existing works\non neural fields are grid-based representations with latents defined on a\nregular grid. In contrast, we define latents on irregular grids, enabling our\nrepresentation to be sparse and adaptive. In the context of shape\nreconstruction from point clouds, our shape representation built on irregular\ngrids improves upon grid-based methods in terms of reconstruction accuracy. For\nshape generation, our representation promotes high-quality shape generation\nusing auto-regressive probabilistic models. We show different applications that\nimprove over the current state of the art. First, we show results for\nprobabilistic shape reconstruction from a single higher resolution image.\nSecond, we train a probabilistic model conditioned on very low resolution\nimages. Third, we apply our model to category-conditioned generation. All\nprobabilistic experiments confirm that we are able to generate detailed and\nhigh quality shapes to yield the new state of the art in generative 3D shape\nmodeling.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Biao Zhang",
      "Matthias Nie\u00dfner",
      "Peter Wonka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13914"
  },
  {
    "id": "arXiv:2205.13919",
    "title": "Fast Causal Orientation Learning in Directed Acyclic Graphs",
    "abstract": "Causal relationships among a set of variables are commonly represented by a\ndirected acyclic graph. The orientations of some edges in the causal DAG can be\ndiscovered from observational/interventional data. Further edges can be\noriented by iteratively applying so-called Meek rules. Inferring edges'\norientations from some previously oriented edges, which we call Causal\nOrientation Learning (COL), is a common problem in various causal discovery\ntasks. In these tasks, it is often required to solve multiple COL problems and\ntherefore applying Meek rules could be time-consuming. Motivated by Meek rules,\nwe introduce Meek functions that can be utilized in solving COL problems. In\nparticular, we show that these functions have some desirable properties,\nenabling us to speed up the process of applying Meek rules. In particular, we\npropose a dynamic programming (DP) based method to apply Meek functions.\nMoreover, based on the proposed DP method, we present a lower bound on the\nnumber of edges that can be oriented as a result of intervention. We also\npropose a method to check whether some oriented edges belong to a causal DAG.\nExperimental results show that the proposed methods can outperform previous\nwork in several causal discovery tasks in terms of running-time.",
    "descriptor": "",
    "authors": [
      "Ramin Safaeian",
      "Saber Salehkaleybar",
      "Mahmoud Tabandeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.13919"
  },
  {
    "id": "arXiv:2205.13921",
    "title": "ProtoFSSL: Federated Semi-Supervised Learning with Prototype-based  Consistency Regularization",
    "abstract": "With the increasing computing power of edge devices, Federated Learning (FL)\nemerges to enable model training without privacy concerns. The majority of\nexisting studies assume the data are fully labeled on the client side. In\npractice, however, the amount of labeled data is often limited. Recently,\nfederated semi-supervised learning (FSSL) is explored as a way to effectively\nutilize unlabeled data during training. In this work, we propose ProtoFSSL, a\nnovel FSSL approach based on prototypical networks. In ProtoFSSL, clients share\nknowledge with each other via lightweight prototypes, which prevents the local\nmodels from diverging. For computing loss on unlabeled data, each client\ncreates accurate pseudo-labels based on shared prototypes. Jointly with labeled\ndata, the pseudo-labels provide training signals for local prototypes. Compared\nto a FSSL approach based on weight sharing, the prototype-based inter-client\nknowledge sharing significantly reduces both communication and computation\ncosts, enabling more frequent knowledge sharing between more clients for better\naccuracy. In multiple datasets, ProtoFSSL results in higher accuracy compared\nto the recent FSSL methods with and without knowledge sharing, such as\nFixMatch, FedRGD, and FedMatch. On SVHN dataset, ProtoFSSL performs comparably\nto fully supervised FL methods.",
    "descriptor": "",
    "authors": [
      "Woojung Kim",
      "Keondo Park",
      "Kihyuk Sohn",
      "Raphael Shu",
      "Hyung-Sin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13921"
  },
  {
    "id": "arXiv:2205.13922",
    "title": "CREAM: Weakly Supervised Object Localization via Class RE-Activation  Mapping",
    "abstract": "Weakly Supervised Object Localization (WSOL) aims to localize objects with\nimage-level supervision. Existing works mainly rely on Class Activation Mapping\n(CAM) derived from a classification model. However, CAM-based methods usually\nfocus on the most discriminative parts of an object (i.e., incomplete\nlocalization problem). In this paper, we empirically prove that this problem is\nassociated with the mixup of the activation values between less discriminative\nforeground regions and the background. To address it, we propose Class\nRE-Activation Mapping (CREAM), a novel clustering-based approach to boost the\nactivation values of the integral object regions. To this end, we introduce\nclass-specific foreground and background context embeddings as cluster\ncentroids. A CAM-guided momentum preservation strategy is developed to learn\nthe context embeddings during training. At the inference stage, the\nre-activation mapping is formulated as a parameter estimation problem under\nGaussian Mixture Model, which can be solved by deriving an unsupervised\nExpectation-Maximization based soft-clustering algorithm. By simply integrating\nCREAM into various WSOL approaches, our method significantly improves their\nperformance. CREAM achieves the state-of-the-art performance on CUB, ILSVRC and\nOpenImages benchmark datasets. Code will be available at\nhttps://github.com/Jazzcharles/CREAM.",
    "descriptor": "\nComments: 10 pages, CVPR 2022\n",
    "authors": [
      "Jilan Xu",
      "Junlin Hou",
      "Yuejie Zhang",
      "Rui Feng",
      "Rui-Wei Zhao",
      "Tao Zhang",
      "Xuequan Lu",
      "Shang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13922"
  },
  {
    "id": "arXiv:2205.13924",
    "title": "Lifting the Information Ratio: An Information-Theoretic Analysis of  Thompson Sampling for Contextual Bandits",
    "abstract": "We study the Bayesian regret of the renowned Thompson Sampling algorithm in\ncontextual bandits with binary losses and adversarially-selected contexts. We\nadapt the information-theoretic perspective of Russo and Van Roy [2016] to the\ncontextual setting by introducing a new concept of information ratio based on\nthe mutual information between the unknown model parameter and the observed\nloss. This allows us to bound the regret in terms of the entropy of the prior\ndistribution through a remarkably simple proof, and with no structural\nassumptions on the likelihood or the prior. The extension to priors with\ninfinite entropy only requires a Lipschitz assumption on the log-likelihood. An\ninteresting special case is that of logistic bandits with d-dimensional\nparameters, K actions, and Lipschitz logits, for which we provide a\n$\\widetilde{O}(\\sqrt{dKT})$ regret upper-bound that does not depend on the\nsmallest slope of the sigmoid link function.",
    "descriptor": "",
    "authors": [
      "Gergely Neu",
      "Julia Olkhovskaya",
      "Matteo Papini",
      "Ludovic Schwartz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13924"
  },
  {
    "id": "arXiv:2205.13925",
    "title": "Client Selection in Nonconvex Federated Learning: Improved Convergence  Analysis for Optimal Unbiased Sampling Strategy",
    "abstract": "Federated learning (FL) is a distributed machine learning paradigm that\nselects a subset of clients to participate in training to reduce communication\nburdens. However, partial client participation in FL causes \\emph{objective\ninconsistency}, which can hinder the convergence, while this objective\ninconsistency has not been analyzed in existing studies on sampling methods. To\ntackle this issue, we propose an improved analysis method that focuses on the\nconvergence behavior of the practical participated client's objective.\nMoreover, based on our convergence analysis, we give a novel unbiased sampling\nstrategy, i.e., FedSRC-D, whose sampling probability is proportional to the\nclient's gradient diversity and local variance. FedSRC-D is provable the\noptimal unbiased sampling in non-convex settings for non-IID FL with respect to\nthe given bounds. Specifically, FedSRC-D achieves\n$\\mathop{O}(\\frac{G^2}{\\epsilon^2}+\\frac{1}{\\epsilon^{2/3}})$ higher than SOTA\nconvergence rate of FedAvg, and $\\mathop{O}(\\frac{G^2}{\\epsilon^2})$ higher\nthan other unbiased sampling methods. We corroborate our results with\nexperiments on both synthetic and real data sets.",
    "descriptor": "",
    "authors": [
      "Lin Wang",
      "YongXin Guo",
      "Tao Lin",
      "Xiaoying Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13925"
  },
  {
    "id": "arXiv:2205.13927",
    "title": "Probabilistic Transformer: Modelling Ambiguities and Distributions for  RNA Folding and Molecule Design",
    "abstract": "Our world is ambiguous and this is reflected in the data we use to train our\nalgorithms. This is especially true when we try to model natural processes\nwhere collected data is affected by noisy measurements and differences in\nmeasurement techniques. Sometimes, the process itself can be ambiguous, such as\nin the case of RNA folding, where a single nucleotide sequence can fold into\nmultiple structures. This ambiguity suggests that a predictive model should\nhave similar probabilistic characteristics to match the data it models.\nTherefore, we propose a hierarchical latent distribution to enhance one of the\nmost successful deep learning models, the Transformer, to accommodate\nambiguities and data distributions. We show the benefits of our approach on a\nsynthetic task, with state-of-the-art results in RNA folding, and demonstrate\nits generative capabilities on property-based molecule design, outperforming\nexisting work.",
    "descriptor": "\nComments: Preprint, currently under review\n",
    "authors": [
      "J\u00f6rg K. H. Franke",
      "Frederic Runge",
      "Frank Hutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13927"
  },
  {
    "id": "arXiv:2205.13928",
    "title": "Commonsense and Named Entity Aware Knowledge Grounded Dialogue  Generation",
    "abstract": "Grounding dialogue on external knowledge and interpreting linguistic patterns\nin dialogue history context, such as ellipsis, anaphora, and co-references is\ncritical for dialogue comprehension and generation. In this paper, we present a\nnovel open-domain dialogue generation model which effectively utilizes the\nlarge-scale commonsense and named entity based knowledge in addition to the\nunstructured topic-specific knowledge associated with each utterance. We\nenhance the commonsense knowledge with named entity-aware structures using\nco-references. Our proposed model utilizes a multi-hop attention layer to\npreserve the most accurate and critical parts of the dialogue history and the\nassociated knowledge. In addition, we employ a Commonsense and Named Entity\nEnhanced Attention Module, which starts with the extracted triples from various\nsources and gradually finds the relevant supporting set of triples using\nmulti-hop attention with the query vector obtained from the interactive\ndialogue-knowledge module. Empirical results on two benchmark dataset\ndemonstrate that our model significantly outperforms the state-of-the-art\nmethods in terms of both automatic evaluation metrics and human judgment. Our\ncode is publicly available at\n\\href{https://github.com/deekshaVarshney/CNTF}{https://github.com/deekshaVarshney/CNTF};\n\\href{https://www.iitp.ac.in/~ai-nlp-ml/resources/codes/CNTF.zip}{https://www.iitp.ac.in/-ai-nlp-ml/resources/\ncodes/CNTF.zip}.",
    "descriptor": "",
    "authors": [
      "Deeksha Varshney",
      "Akshara Prabhakar",
      "Asif Ekbal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13928"
  },
  {
    "id": "arXiv:2205.13930",
    "title": "Fairness and Welfare Quantification for Regret in Multi-Armed Bandits",
    "abstract": "We extend the notion of regret with a welfarist perspective. Focussing on the\nclassic multi-armed bandit (MAB) framework, the current work quantifies the\nperformance of bandit algorithms by applying a fundamental welfare function,\nnamely the Nash social welfare (NSW) function. This corresponds to equating\nalgorithm's performance to the geometric mean of its expected rewards and leads\nus to the study of Nash regret, defined as the difference between the -- a\npriori unknown -- optimal mean (among the arms) and the algorithm's\nperformance. Since NSW is known to satisfy fairness axioms, our approach\ncomplements the utilitarian considerations of average (cumulative) regret,\nwherein the algorithm is evaluated via the arithmetic mean of its expected\nrewards.\nThis work develops an algorithm that, given the horizon of play $T$, achieves\na Nash regret of $O \\left( \\sqrt{\\frac{{k \\log T}}{T}} \\right)$, here $k$\ndenotes the number of arms in the MAB instance. Since, for any algorithm, the\nNash regret is at least as much as its average regret (the AM-GM inequality),\nthe known lower bound on average regret holds for Nash regret as well.\nTherefore, our Nash regret guarantee is essentially tight. In addition, we\ndevelop an anytime algorithm with a Nash regret guarantee of $O \\left(\n\\sqrt{\\frac{{k\\log T}}{T}} \\log T \\right)$.",
    "descriptor": "\nComments: 32 pages\n",
    "authors": [
      "Siddharth Barman",
      "Arindam Khan",
      "Arnab Maiti",
      "Ayush Sawarni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.13930"
  },
  {
    "id": "arXiv:2205.13933",
    "title": "Standalone Neural ODEs with Sensitivity Analysis",
    "abstract": "This paper presents the Standalone Neural ODE (sNODE), a continuous-depth\nneural ODE model capable of describing a full deep neural network. This uses a\nnovel nonlinear conjugate gradient (NCG) descent optimization scheme for\ntraining, where the Sobolev gradient can be incorporated to improve smoothness\nof model weights. We also present a general formulation of the neural\nsensitivity problem and show how it is used in the NCG training. The\nsensitivity analysis provides a reliable measure of uncertainty propagation\nthroughout a network, and can be used to study model robustness and to generate\nadversarial attacks. Our evaluations demonstrate that our novel formulations\nlead to increased robustness and performance as compared to ResNet models, and\nthat it opens up for new opportunities for designing and developing machine\nlearning with improved explainability.",
    "descriptor": "\nComments: 25 pages, 15 figures\n",
    "authors": [
      "Rym Jaroudi",
      "Luk\u00e1\u0161 Mal\u00fd",
      "Gabriel Eilertsen",
      "Tomas B. Johansson",
      "Jonas Unger",
      "George Baravdish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13933"
  },
  {
    "id": "arXiv:2205.13937",
    "title": "Deep face recognition with clustering based domain adaptation",
    "abstract": "Despite great progress in face recognition tasks achieved by deep convolution\nneural networks (CNNs), these models often face challenges in real world tasks\nwhere training images gathered from Internet are different from test images\nbecause of different lighting condition, pose and image quality. These factors\nincrease domain discrepancy between training (source domain) and testing\n(target domain) database and make the learnt models degenerate in application.\nMeanwhile, due to lack of labeled target data, directly fine-tuning the\npre-learnt models becomes intractable and impractical. In this paper, we\npropose a new clustering-based domain adaptation method designed for face\nrecognition task in which the source and target domain do not share any\nclasses. Our method effectively learns the discriminative target feature by\naligning the feature domain globally, and, at the meantime, distinguishing the\ntarget clusters locally. Specifically, it first learns a more reliable\nrepresentation for clustering by minimizing global domain discrepancy to reduce\ndomain gaps, and then applies simplified spectral clustering method to generate\npseudo-labels in the domain-invariant feature space, and finally learns\ndiscriminative target representation. Comprehensive experiments on widely-used\nGBU, IJB-A/B/C and RFW databases clearly demonstrate the effectiveness of our\nnewly proposed approach. State-of-the-art performance of GBU data set is\nachieved by only unsupervised adaptation from the target training data.",
    "descriptor": "\nComments: Accepted by Neurocomputing\n",
    "authors": [
      "Mei Wang",
      "Weihong Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13937"
  },
  {
    "id": "arXiv:2205.13939",
    "title": "Asymptotically constant-free and polynomial-degree-robust a posteriori  estimates for space discretizations of the wave equation",
    "abstract": "We derive an equilibrated a posteriori error estimator for the space (semi)\ndiscretization of the scalar wave equation by finite elements. In the idealized\nsetting where time discretization is ignored and the simulation time is large,\nwe provide fully-guaranteed upper bounds that are asymptotically constant-free\nand show that the proposed estimator is efficient and polynomial-degree-robust,\nmeaning that the efficiency constant does not deteriorate as the approximation\norder is increased. To the best of our knowledge, this work is the first to\nderive provably efficient error estimates for the wave equation. We also\nexplain, without analysis, how the estimator is adapted to cover time\ndiscretization by an explicit time integration scheme. Numerical examples\nillustrate the theory and suggest that it is sharp.",
    "descriptor": "",
    "authors": [
      "T. Chaumont-Frelet"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13939"
  },
  {
    "id": "arXiv:2205.13941",
    "title": "Auditing Differential Privacy in High Dimensions with the Kernel Quantum  R\u00e9nyi Divergence",
    "abstract": "Differential privacy (DP) is the de facto standard for private data release\nand private machine learning. Auditing black-box DP algorithms and mechanisms\nto certify whether they satisfy a certain DP guarantee is challenging,\nespecially in high dimension. We propose relaxations of differential privacy\nbased on new divergences on probability distributions: the kernel R\\'enyi\ndivergence and its regularized version. We show that the regularized kernel\nR\\'enyi divergence can be estimated from samples even in high dimensions,\ngiving rise to auditing procedures for $\\varepsilon$-DP,\n$(\\varepsilon,\\delta)$-DP and $(\\alpha,\\varepsilon)$-R\\'enyi DP.",
    "descriptor": "\nComments: Code at this https URL\n",
    "authors": [
      "Carles Domingo-Enrich",
      "Youssef Mroueh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13941"
  },
  {
    "id": "arXiv:2205.13943",
    "title": "Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
    "abstract": "Masked image modeling (MIM), an emerging self-supervised pre-training method,\nhas shown impressive success across numerous downstream vision tasks with\nVision transformers (ViT). Its underlying idea is simple: a portion of the\ninput image is randomly masked out and then reconstructed via the pre-text\ntask. However, why MIM works well is not well explained, and previous studies\ninsist that MIM primarily works for the Transformer family but is incompatible\nwith CNNs. In this paper, we first study interactions among patches to\nunderstand what knowledge is learned and how it is acquired via the MIM task.\nWe observe that MIM essentially teaches the model to learn better middle-level\ninteractions among patches and extract more generalized features. Based on this\nfact, we propose an Architecture-Agnostic Masked Image Modeling framework\n(A$^2$MIM), which is compatible with not only Transformers but also CNNs in a\nunified way. Extensive experiments on popular benchmarks show that our A$^2$MIM\nlearns better representations and endows the backbone model with the stronger\ncapability to transfer to various downstream tasks for both Transformers and\nCNNs.",
    "descriptor": "\nComments: A preprint version. The source code will be released in this https URL\n",
    "authors": [
      "Siyuan Li",
      "Di Wu",
      "Fang Wu",
      "Zelin Zang",
      "Kai Wang",
      "Lei Shang",
      "Baigui Sun",
      "Hao Li",
      "Stan.Z.Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13943"
  },
  {
    "id": "arXiv:2205.13944",
    "title": "Deep Reinforcement Learning for Distributed and Uncoordinated Cognitive  Radios Resource Allocation",
    "abstract": "This paper presents a novel deep reinforcement learning-based resource\nallocation technique for the multi-agent environment presented by a cognitive\nradio network where the interactions of the agents during learning may lead to\na non-stationary environment. The resource allocation technique presented in\nthis work is distributed, not requiring coordination with other agents. It is\nshown by considering aspects specific to deep reinforcement learning that the\npresented algorithm converges in an arbitrarily long time to equilibrium\npolicies in a non-stationary multi-agent environment that results from the\nuncoordinated dynamic interaction between radios through the shared wireless\nenvironment. Simulation results show that the presented technique achieves a\nfaster learning performance compared to an equivalent table-based Q-learning\nalgorithm and is able to find the optimal policy in 99% of cases for a\nsufficiently long learning time. In addition, simulations show that our DQL\napproach requires less than half the number of learning steps to achieve the\nsame performance as an equivalent table-based implementation. Moreover, it is\nshown that the use of a standard single-agent deep reinforcement learning\napproach may not achieve convergence when used in an uncoordinated interacting\nmulti-radio scenario",
    "descriptor": "\nComments: Submitted to IEEE Journal of Open Journal of the Communications Society\n",
    "authors": [
      "Ankita Tondwalkar",
      "Andres Kwasinski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13944"
  },
  {
    "id": "arXiv:2205.13945",
    "title": "Nighthawk: Fully Automated Localizing UI Display Issues via Visual  Understanding",
    "abstract": "Graphical User Interface (GUI) provides a visual bridge between a software\napplication and end users, through which they can interact with each other.\nWith the upgrading of mobile devices and the development of aesthetics, the\nvisual effects of the GUI are more and more attracting, and users pay more\nattention to the accessibility and usability of applications. However, such GUI\ncomplexity posts a great challenge to the GUI implementation. According to our\npilot study of crowdtesting bug reports, display issues such as text overlap,\ncomponent occlusion, missing image always occur during GUI rendering on\ndifferent devices due to the software or hardware compatibility. They\nnegatively influence the app usability, resulting in poor user experience. To\ndetect these issues, we propose a fully automated approach, Nighthawk, based on\ndeep learning for modelling visual information of the GUI screenshot. Nighthawk\ncan detect GUIs with display issues and also locate the detailed region of the\nissue in the given GUI for guiding developers to fix the bug. At the same time,\ntraining the model needs a large amount of labeled buggy screenshots, which\nrequires considerable manual effort to prepare them. We therefore propose a\nheuristic-based training data auto-generation method to automatically generate\nthe labeled training data. The evaluation demonstrates that our Nighthawk can\nachieve average 0.84 precision and 0.84 recall in detecting UI display issues,\naverage 0.59 AP and 0.60 AR in localizing these issues. We also evaluate\nNighthawk with popular Android apps on Google Play and F-Droid, and\nsuccessfully uncover 151 previously-undetected UI display issues with 75 of\nthem being confirmed or fixed so far.",
    "descriptor": "\nComments: Accepted by IEEE TRANSACTIONS ON SOFTWARE ENGINEERING(TSE 22). arXiv admin note: substantial text overlap with arXiv:2009.01417\n",
    "authors": [
      "Zhe Liu",
      "Chunyang Chen",
      "Junjie Wang",
      "Yuekai Huang",
      "Jun Hu",
      "Qing Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.13945"
  },
  {
    "id": "arXiv:2205.13947",
    "title": "Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge  Transfer",
    "abstract": "Spatio-temporal graph learning is a key method for urban computing tasks,\nsuch as traffic flow, taxi demand and air quality forecasting. Due to the high\ncost of data collection, some developing cities have few available data, which\nmakes it infeasible to train a well-performed model. To address this challenge,\ncross-city knowledge transfer has shown its promise, where the model learned\nfrom data-sufficient cities is leveraged to benefit the learning process of\ndata-scarce cities. However, the spatio-temporal graphs among different cities\nshow irregular structures and varied features, which limits the feasibility of\nexisting Few-Shot Learning (\\emph{FSL}) methods. Therefore, we propose a\nmodel-agnostic few-shot learning framework for spatio-temporal graph called\nST-GFSL. Specifically, to enhance feature extraction by transfering cross-city\nknowledge, ST-GFSL proposes to generate non-shared parameters based on\nnode-level meta knowledge. The nodes in target city transfer the knowledge via\nparameter matching, retrieving from similar spatio-temporal characteristics.\nFurthermore, we propose to reconstruct the graph structure during\nmeta-learning. The graph reconstruction loss is defined to guide\nstructure-aware learning, avoiding structure deviation among different\ndatasets. We conduct comprehensive experiments on four traffic speed prediction\nbenchmarks and the results demonstrate the effectiveness of ST-GFSL compared\nwith state-of-the-art methods.",
    "descriptor": "\nComments: Accepted by KDD2022\n",
    "authors": [
      "Bin Lu",
      "Xiaoying Gan",
      "Weinan Zhang",
      "Huaxiu Yao",
      "Luoyi Fu",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13947"
  },
  {
    "id": "arXiv:2205.13948",
    "title": "Evolution as a Service: A Privacy-Preserving Genetic Algorithm for  Combinatorial Optimization",
    "abstract": "Evolutionary algorithms (EAs), such as the genetic algorithm (GA), offer an\nelegant way to handle combinatorial optimization problems (COPs). However,\nlimited by expertise and resources, most users do not have enough capability to\nimplement EAs to solve COPs. An intuitive and promising solution is to\noutsource evolutionary operations to a cloud server, whilst it suffers from\nprivacy concerns. To this end, this paper proposes a novel computing paradigm,\nevolution as a service (EaaS), where a cloud server renders evolutionary\ncomputation services for users without sacrificing users' privacy. Inspired by\nthe idea of EaaS, this paper designs PEGA, a novel privacy-preserving GA for\nCOPs. Specifically, PEGA enables users outsourcing COPs to the cloud server\nholding a competitive GA and approximating the optimal solution in a\nprivacy-preserving manner. PEGA features the following characteristics. First,\nany user without expertise and enough resources can solve her COPs. Second,\nPEGA does not leak contents of optimization problems, i.e., users' privacy.\nThird, PEGA has the same capability as the conventional GA to approximate the\noptimal solution. We implements PEGA falling in a twin-server architecture and\nevaluates it in the traveling salesman problem (TSP, a widely known COP).\nParticularly, we utilize encryption cryptography to protect users' privacy and\ncarefully design a suit of secure computing protocols to support evolutionary\noperators of GA on encrypted data. Privacy analysis demonstrates that PEGA does\nnot disclose the contents of the COP to the cloud server. Experimental\nevaluation results on four TSP datasets show that PEGA is as effective as the\nconventional GA in approximating the optimal solution.",
    "descriptor": "",
    "authors": [
      "Bowen Zhao",
      "Wei-Neng Chen",
      "Feng-Feng Wei",
      "Ximeng Liu",
      "Qingqi Pei",
      "Jun Zhang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.13948"
  },
  {
    "id": "arXiv:2205.13950",
    "title": "Non-Markovian policies occupancy measures",
    "abstract": "A central object of study in Reinforcement Learning (RL) is the Markovian\npolicy, in which an agent's actions are chosen from a memoryless probability\ndistribution, conditioned only on its current state. The family of Markovian\npolicies is broad enough to be interesting, yet simple enough to be amenable to\nanalysis. However, RL often involves more complex policies: ensembles of\npolicies, policies over options, policies updated online, etc. Our main\ncontribution is to prove that the occupancy measure of any non-Markovian\npolicy, i.e., the distribution of transition samples collected with it, can be\nequivalently generated by a Markovian policy.\nThis result allows theorems about the Markovian policy class to be directly\nextended to its non-Markovian counterpart, greatly simplifying proofs, in\nparticular those involving replay buffers and datasets. We provide various\nexamples of such applications to the field of Reinforcement Learning.",
    "descriptor": "\nComments: 9p+sup. mat\n",
    "authors": [
      "Romain Laroche",
      "Remi Tachet des Combes",
      "Jacob Buckman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13950"
  },
  {
    "id": "arXiv:2205.13954",
    "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype  Representation",
    "abstract": "With the tremendous expansion of graphs data, node classification shows its\ngreat importance in many real-world applications. Existing graph neural network\nbased methods mainly focus on classifying unlabeled nodes within fixed classes\nwith abundant labeling. However, in many practical scenarios, graph evolves\nwith emergence of new nodes and edges. Novel classes appear incrementally along\nwith few labeling due to its newly emergence or lack of exploration. In this\npaper, we focus on this challenging but practical graph few-shot\nclass-incremental learning (GFSCIL) problem and propose a novel method called\nGeometer. Instead of replacing and retraining the fully connected neural\nnetwork classifer, Geometer predicts the label of a node by finding the nearest\nclass prototype. Prototype is a vector representing a class in the metric\nspace. With the pop-up of novel classes, Geometer learns and adjusts the\nattention-based prototypes by observing the geometric proximity, uniformity and\nseparability. Teacher-student knowledge distillation and biased sampling are\nfurther introduced to mitigate catastrophic forgetting and unbalanced labeling\nproblem respectively. Experimental results on four public datasets demonstrate\nthat Geometer achieves a substantial improvement of 9.46% to 27.60% over\nstate-of-the-art methods.",
    "descriptor": "\nComments: Accepted by KDD2022\n",
    "authors": [
      "Bin Lu",
      "Xiaoying Gan",
      "Lina Yang",
      "Weinan Zhang",
      "Luoyi Fu",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13954"
  },
  {
    "id": "arXiv:2205.13955",
    "title": "Hybridizing Waterborne Transport: Modeling and Simulation of  Low-Emissions Hybrid Waterbuses for the City of Venice",
    "abstract": "Hybrid-electric powertrains are among the most promising technologies for\nabating emissions from marine vessels in sensitive areas. However, their\neffectiveness strongly depends on the context they operate into. This paper\nattempts to evaluate the potential impact on air quality of hybridizing the\ndiesel-powered waterbuses that currently operate in the city of Venice as part\nof the local public transportation network. Simulation models for conventional,\nseries hybrid and parallel hybrid marine powertrains were developed and applied\nto the typical operational mission of one of these waterbuses. For the hybrid\npowertrains, an Energy Management Strategy is also obtained using a Dynamic\nProgramming - based optimization algorithm. The results show that both hybrid\narchitectures have high emission-reducing potential, with the series hybrid\noffering the greatest benefits.",
    "descriptor": "",
    "authors": [
      "Federico Miretti",
      "Daniela Misul",
      "Giulio Gennaro",
      "Antonio Ferrari"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13955"
  },
  {
    "id": "arXiv:2205.13956",
    "title": "Guided Exploration of Data Summaries",
    "abstract": "Data summarization is the process of producing interpretable and\nrepresentative subsets of an input dataset. It is usually performed following a\none-shot process with the purpose of finding the best summary. A useful summary\ncontains k individually uniform sets that are collectively diverse to be\nrepresentative. Uniformity addresses interpretability and diversity addresses\nrepresentativity. Finding such as summary is a difficult task when data is\nhighly diverse and large. We examine the applicability of Exploratory Data\nAnalysis (EDA) to data summarization and formalize Eda4Sum, the problem of\nguided exploration of data summaries that seeks to sequentially produce\nconnected summaries with the goal of maximizing their cumulative utility.\nEdA4Sum generalizes one-shot summarization. We propose to solve it with one of\ntwo approaches: (i) Top1Sum which chooses the most useful summary at each step;\n(ii) RLSum which trains a policy with Deep Reinforcement Learning that rewards\nan agent for finding a diverse and new collection of uniform sets at each step.\nWe compare these approaches with one-shot summarization and top-performing EDA\nsolutions. We run extensive experiments on three large datasets. Our results\ndemonstrate the superiority of our approaches for summarizing very large data,\nand the need to provide guidance to domain experts.",
    "descriptor": "",
    "authors": [
      "Brit Youngmann",
      "Sihem Amer-Yahia",
      "Aur\u00e9lien Personnaz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.13956"
  },
  {
    "id": "arXiv:2205.13957",
    "title": "Cycle Label-Consistent Networks for Unsupervised Domain Adaptation",
    "abstract": "Domain adaptation aims to leverage a labeled source domain to learn a\nclassifier for the unlabeled target domain with a different distribution.\nPrevious methods mostly match the distribution between two domains by global or\nclass alignment. However, global alignment methods cannot achieve a\nfine-grained class-to-class overlap; class alignment methods supervised by\npseudo-labels cannot guarantee their reliability. In this paper, we propose a\nsimple yet efficient domain adaptation method, i.e. Cycle Label-Consistent\nNetwork (CLCN), by exploiting the cycle consistency of classification label,\nwhich applies dual cross-domain nearest centroid classification procedures to\ngenerate a reliable self-supervised signal for the discrimination in the target\ndomain. The cycle label-consistent loss reinforces the consistency between\nground-truth labels and pseudo-labels of source samples leading to\nstatistically similar latent representations between source and target domains.\nThis new loss can easily be added to any existing classification network with\nalmost no computational overhead. We demonstrate the effectiveness of our\napproach on MNIST-USPS-SVHN, Office-31, Office-Home and Image CLEF-DA\nbenchmarks. Results validate that the proposed method can alleviate the\nnegative influence of falsely-labeled samples and learn more discriminative\nfeatures, leading to the absolute improvement over source-only model by 9.4% on\nOffice-31 and 6.3% on Image CLEF-DA.",
    "descriptor": "\nComments: Accepted by Neurocomputing\n",
    "authors": [
      "Mei Wang",
      "Weihong Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13957"
  },
  {
    "id": "arXiv:2205.13958",
    "title": "Machine Learning-Based User Scheduling in Integrated  Satellite-HAPS-Ground Networks",
    "abstract": "Integrated space-air-ground networks promise to offer a valuable solution\nspace for empowering next generation of communication networks (6G),\nparticularly in the context of connecting the unconnected and ultraconnecting\nthe connected. Such digital inclusion thrive makes the resource management\nproblem of particular interest. However, the classical model-based optimization\nmethods cannot meet the real-time processing and user's QoS needs, due to the\nhigh heterogeneity of the space-air-ground networks and the complexity of its\nassociated resource allocation problems. Given the premises of artificial\nintelligence at automating wireless networks design, this paper focuses on\nshowcasing the prospects of machine learning in the context of user scheduling\nin integrated space-air-ground communications. The paper first overviews the\nmost relevant state-of-the art in the context of machine learning applications\nto the resource allocation problems in integrated space-air-ground networks.\nThe paper then proposes, and shows the benefit of, one specific use-case that\nadopts ensembling deep neural network for optimizing the user scheduling\npolicies in space-high altitude platform station (HAPS)-ground networks.\nFinally, the paper presents some challenges and sheds light on several open\nissues in the context of machine learning applications in space-air-ground\nnetworks, namely, power limit, imperfect channel state information, multi-HAPSs\nscenarios and flying taxis-empowered systems.",
    "descriptor": "",
    "authors": [
      "Shasha Liu",
      "Hayssam Dahrouj",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13958"
  },
  {
    "id": "arXiv:2205.13959",
    "title": "Robust Stutter Bisimulation for Abstraction and Controller Synthesis  with Disturbance: Proofs",
    "abstract": "This paper proposes a method to synthesise controllers for cyber-physical\nsystems such that the controlled systems satisfy specifications given as linear\ntemporal logic formulas. The focus is on systems with disturbance, where future\nstates cannot be predicted exactly due to uncertainty in the environment. The\napproach used to solve this problem is to first construct a finite-state\nabstraction of the original system and then synthesise a controller for the\nabstract system. For this approach, the robust stutter bisimulation relation is\nintroduced, which preserves the existence of controllers for any given linear\ntemporal logic formula. States are related by the robust stutter bisimulation\nrelation if the same target sets can be guaranteed to be reached or avoided\nunder control of some controllers, thereby ensuring that disturbances have\nsimilar effect on paths that start in related states. This paper presents an\nalgorithm to construct the corresponding robust stutter bisimulation quotient\nto solve the abstraction problem, and it is shown, by explicit construction,\nthat there exists a controller enforcing a linear temporal logic formula for\nthe original system if and only if a corresponding controller exists for the\nquotient system. Lastly, the result of the algorithm and the controller\nconstruction are demonstrated by application to an example of robot navigation.",
    "descriptor": "\nComments: 21 pages, 5 figures, Preprint submitted to Automatica, plus appendices with proofs of theorems\n",
    "authors": [
      "Jonas Krook",
      "Robi Malik",
      "Sahar Mohajerani",
      "Martin Fabian"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13959"
  },
  {
    "id": "arXiv:2205.13961",
    "title": "Punctuation Restoration in Spanish Customer Support Transcripts using  Transfer Learning",
    "abstract": "Automatic Speech Recognition (ASR) systems typically produce unpunctuated\ntranscripts that have poor readability. In addition, building a punctuation\nrestoration system is challenging for low-resource languages, especially for\ndomain-specific applications. In this paper, we propose a Spanish punctuation\nrestoration system designed for a real-time customer support transcription\nservice. To address the data sparsity of Spanish transcripts in the customer\nsupport domain, we introduce two transfer-learning-based strategies: 1) domain\nadaptation using out-of-domain Spanish text data; 2) cross-lingual transfer\nlearning leveraging in-domain English transcript data. Our experiment results\nshow that these strategies improve the accuracy of the Spanish punctuation\nrestoration system.",
    "descriptor": "\nComments: Accepted to DeepLo 2022 at NAACL 2022\n",
    "authors": [
      "Xiliang Zhu",
      "Shayna Gardiner",
      "David Rossouw",
      "Tere Rold\u00e1n",
      "Simon Corston-Oliver"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.13961"
  },
  {
    "id": "arXiv:2205.13963",
    "title": "Exploring Techniques for the Analysis of Spontaneous Asynchronicity in  MPI-Parallel Applications",
    "abstract": "This paper studies the utility of using data analytics and machine learning\ntechniques for identifying, classifying, and characterizing the dynamics of\nlarge-scale parallel (MPI) programs. To this end, we run microbenchmarks and\nrealistic proxy applications with the regular compute-communicate structure on\ntwo different supercomputing platforms and choose the per-process performance\nand MPI time per time step as relevant observables. Using principal component\nanalysis, clustering techniques, correlation functions, and a new \"phase space\nplot,\" we show how desynchronization patterns (or lack thereof) can be readily\nidentified from a data set that is much smaller than a full MPI trace. Our\nmethods also lead the way towards a more general classification of parallel\nprogram dynamics.",
    "descriptor": "\nComments: 12 pages, 9 figures, 1 table\n",
    "authors": [
      "Ayesha Afzal",
      "Georg Hager",
      "Gerhard Wellein",
      "Stefano Markidis"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.13963"
  },
  {
    "id": "arXiv:2205.13970",
    "title": "The Internet of People: A human and data-centric paradigm for the Next  Generation Internet",
    "abstract": "The cyber-physical convergence, the fast expansion of the Internet at its\nedge, and tighter interactions between human users and their personal mobile\ndevices push towards a data-centric Internet where the human user becomes more\ncentral than ever. We argue that this will profoundly impact primarily on the\nway data should be handled in the Next Generation Internet. It will require a\nradical change of the Internet data-management paradigm, from the current\nplatform-centric to a human-centric model. In this paper we present a new\nparadigm for Internet data management that we name Internet of People (IoP)\nbecause it embeds human behavior models in its algorithms. To this end, IoP\nalgorithms exploit quantitative models of the humans' individual and social\nbehavior, from sociology, anthropology, psychology, economics, physics. IoP is\nnot a replacement of the current Internet networking infrastructure, but it\nexploits legacy Internet services as (reliable) primitives to achieve\nend-to-end connectivity on a global-scale. In this opinion paper, we first\ndiscuss the key features of the IoP paradigm along with the underlying research\nissues and challenges. Then, we present emerging data-management paradigms that\nare anticipating IoP.",
    "descriptor": "",
    "authors": [
      "Marco Conti",
      "Andrea Passarella"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.13970"
  },
  {
    "id": "arXiv:2205.13972",
    "title": "Counterfactual Fairness with Partially Known Causal Graph",
    "abstract": "Fair machine learning aims to avoid treating individuals or sub-populations\nunfavourably based on \\textit{sensitive attributes}, such as gender and race.\nThose methods in fair machine learning that are built on causal inference\nascertain discrimination and bias through causal effects. Though\ncausality-based fair learning is attracting increasing attention, current\nmethods assume the true causal graph is fully known. This paper proposes a\ngeneral method to achieve the notion of counterfactual fairness when the true\ncausal graph is unknown. To be able to select features that lead to\ncounterfactual fairness, we derive the conditions and algorithms to identify\nancestral relations between variables on a \\textit{Partially Directed Acyclic\nGraph (PDAG)}, specifically, a class of causal DAGs that can be learned from\nobservational data combined with domain knowledge. Interestingly, we find that\ncounterfactual fairness can be achieved as if the true causal graph were fully\nknown, when specific background knowledge is provided: the sensitive attributes\ndo not have ancestors in the causal graph. Results on both simulated and\nreal-world datasets demonstrate the effectiveness of our method.",
    "descriptor": "",
    "authors": [
      "Aoqi Zuo",
      "Susan Wei",
      "Tongliang Liu",
      "Bo Han",
      "Kun Zhang",
      "Mingming Gong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13972"
  },
  {
    "id": "arXiv:2205.13973",
    "title": "A General approach to Ammann bars for aperiodic tilings",
    "abstract": "Ammann bars are formed by segments (decorations) on the tiles of a tiling\nsuch that forming straight lines with them while tiling forces non-periodicity.\nOnly a few cases are known, starting with Robert Ammann's observations on\nPenrose tiles, but there is no general explanation or construction. In this\narticle we propose a general method for cut and project tilings based on the\nnotion of \\emph{subperiods} and we illustrate it with an aperiodic set of 36\ndecorated prototiles related to what we called \\emph{Cyrenaic tilings}.",
    "descriptor": "\nComments: sagemath code as an ancillary file\n",
    "authors": [
      "Thomas Fernique",
      "Carole Porrier"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.13973"
  },
  {
    "id": "arXiv:2205.13974",
    "title": "DLMP of Competitive Markets in Active Distribution Networks: Models,  Solutions, Applications, and Visions",
    "abstract": "Traditionally, the electric distribution system operates with uniform energy\nprices across all system nodes. However, as the adoption of distributed energy\nresources (DERs) propels a shift from passive to active distribution network\n(ADN) operation, a distribution-level electricity market has been proposed to\nmanage new complexities efficiently. In addition, distribution locational\nmarginal price (DLMP) has been established in the literature as the primary\npricing mechanism. The DLMP inherits the LMP concept in the transmission-level\nwholesale market, but incorporates characteristics of the distribution system,\nsuch as high R/X ratios and power losses, system imbalance, and voltage\nregulation needs. The DLMP provides a solution that can be essential for\ncompetitive market operation in future distribution systems. This paper first\nprovides an overview of the current distribution-level market architectures and\ntheir early implementations. Next, the general clearing model, model\nrelaxations, and DLMP formulation are comprehensively reviewed. The\nstate-of-the-art solution methods for distribution market clearing are\nsummarized and categorized into centralized, distributed, and decentralized\nmethods. Then, DLMP applications for the operation and planning of DERs and\ndistribution system operators (DSOs) are discussed in detail. Finally, visions\nof future research directions and possible barriers and challenges are\npresented.",
    "descriptor": "",
    "authors": [
      "Xiaofei Wang",
      "Fangxing Li",
      "Linquan Bai",
      "Xin Fang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13974"
  },
  {
    "id": "arXiv:2205.13977",
    "title": "Robust Distributed Control within a Curve Virtual Tube for a Robotic  Swarm under Self-Localization Drift and Precise Relative Navigation",
    "abstract": "To guide the movement of a robotic swarm in a corridor-like environment, a\ncurve virtual tube with no obstacle inside is designed in our previous work.\nThis paper generalizes the controller design to the condition that all robots\nhave self-localization drifts and precise relative navigation, where the\nflocking algorithm is introduced to reduce the negative impact of the\nself-localization drift. It is shown that the cohesion behavior and the\nvelocity alignment behavior are able to reduce the influence of the position\nmeasurement drift and the velocity measurement error, respectively. For the\nconvenience in practical use, a modified vector field controller with five\ncontrol terms is put forward. Finally, the effectiveness of the proposed method\nis validated by numerical simulations and real experiments.",
    "descriptor": "\nComments: 8 pages, 10 figures\n",
    "authors": [
      "Yan Gao",
      "Chenggang Bai",
      "Quan Quan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13977"
  },
  {
    "id": "arXiv:2205.13978",
    "title": "Exact bounds on the amplitude and phase of the interval discrete Fourier  transform in polynomial time",
    "abstract": "We elucidate why an interval algorithm that computes the exact bounds on the\namplitude and phase of the discrete Fourier transform can run in polynomial\ntime. We address this question from a formal perspective to provide the\nmathematical foundations underpinning such an algorithm. We show that the\nprocedure set out by the algorithm fully addresses the dependency problem of\ninterval arithmetic, making it usable in a variety of applications involving\nthe discrete Fourier transform. For example when analysing signals with poor\nprecision, signals with missing data, and for automatic error propagation and\nverified computations.",
    "descriptor": "\nComments: Manuscript submitted to the journal of Reliable Computing on 17 December 2021\n",
    "authors": [
      "Marco de Angelis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13978"
  },
  {
    "id": "arXiv:2205.13980",
    "title": "The Structure of Online Social Networks Mirror Those in the Offline  World",
    "abstract": "We use data on frequencies of bi-directional posts to define edges (or\nrelationships) in two Facebook datasets and a Twitter dataset and use these to\ncreate ego-centric social networks. We explore the internal structure of these\nnetworks to determine whether they have the same kind of layered structure as\nhas been found in offline face-to-face networks (which have a distinctively\nscaled structure with successively inclusive layers at 5, 15, 50 and 150\nalters). The two Facebook datasets are best described by a four-layer structure\nand the Twitter dataset by a five-layer structure. The absolute sizes of these\nlayers and the mean frequencies of contact with alters within each layer match\nvery closely the observed values from offline networks. In addition, all three\ndatasets reveal the existence of an innermost network layer at ~1.5 alters. Our\nanalyses thus confirm the existence of the layered structure of ego-centric\nsocial networks with a very much larger sample (in total, >185,000 egos) than\nthose previously used to describe them, as well as identifying the existence of\nan additional network layer whose existence was only hypothesised in offline\nsocial networks. In addition, our analyses indicate that online communities\nhave very similar structural characteristics to offline face-to-face networks.",
    "descriptor": "",
    "authors": [
      "R.I.M. Dunbar",
      "Valerio Arnaboldi",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.13980"
  },
  {
    "id": "arXiv:2205.13981",
    "title": "$\\mathbb{Z}_p\\mathbb{Z}_{p^2}$-linear codes: rank and kernel",
    "abstract": "A code $C$ is called $\\Z_p\\Z_{p^2}$-linear if it is the Gray image of a\n$\\Z_p\\Z_{p^2}$-additive code, where $p>2$ is prime. In this paper, the rank and\nthe dimension of the kernel of $\\Z_p\\Z_{p^2}$-linear codes are studied. Two\nbounds of the rank of a $\\Z_3\\Z_{9}$-linear code and the dimension of the\nkernel of a $\\Z_p\\Z_{p^2}$-linear code are given, respectively. For each value\nof these bounds, we give detailed construction of the corresponding code.\nFinally, pairs of rank and the dimension of the kernel of $\\Z_3\\Z_{9}$-linear\ncodes are also considered.",
    "descriptor": "",
    "authors": [
      "Minjia Shi",
      "Shukai Wang",
      "Xiaoxiao Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13981"
  },
  {
    "id": "arXiv:2205.13984",
    "title": "Information geometry of the Tojo-Yoshino's exponential family on the  Poincar\u00e9 upper plane",
    "abstract": "We study the dually flat information geometry of the Tojo-Yoshino exponential\nfamily with has sample space the Poincar\\'e upper plane and parameter space the\nopen convex cone of $2\\times 2$ symmetric positive-definite matrices. Using the\nframework of Eaton's maximal invariant, we prove that all $f$-divergences\nbetween Tojo-Yoshino Poincar\\'e distributions are functions of $3$ simple\ndeterminant/trace terms. We report closed-form formula for the Fisher\ninformation matrix, the differential entropy and the Kullback-Leibler\ndivergence and Bhattacharyya distance between such distributions.",
    "descriptor": "",
    "authors": [
      "Frank Nielsen",
      "Kazuki Okamura"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13984"
  },
  {
    "id": "arXiv:2205.13988",
    "title": "Deep Ensembles for Graphs with Higher-order Dependencies",
    "abstract": "Graph neural networks (GNNs) continue to achieve state-of-the-art performance\non many graph learning tasks, but rely on the assumption that a given graph is\na sufficient approximation of the true neighborhood structure. In the presence\nof higher-order sequential dependencies, we show that the tendency of\ntraditional graph representations to underfit each node's neighborhood causes\nexisting GNNs to generalize poorly. To address this, we propose a novel Deep\nGraph Ensemble (DGE), which captures neighborhood variance by training an\nensemble of GNNs on different neighborhood subspaces of the same node within a\nhigher-order network structure. We show that DGE consistently outperforms\nexisting GNNs on semisupervised and supervised tasks on four real-world data\nsets with known higher-order dependencies, even under a similar parameter\nbudget. We demonstrate that learning diverse and accurate base classifiers is\ncentral to DGE's success, and discuss the implications of these findings for\nfuture work on GNNs.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Steven J. Krieg",
      "William C. Burgis",
      "Patrick M. Soga",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13988"
  },
  {
    "id": "arXiv:2205.13992",
    "title": "NaviDroid: A Tool for Guiding Manual Android Testing via Hint Moves",
    "abstract": "Manual testing, as a complement to automated GUI testing, is the last line of\ndefense for app quality especially in spotting usability and accessibility\nissues. However, the repeated actions and easy missing of some functionalities\nmake manual testing time-consuming, labor-extensive and inefficient. Inspired\nby the game candy crush with flashy candies as hint moves for players, we\ndevelop a tool named NaviDroid for navigating human testers via highlighted\nnext operations for more effective and efficient testing. Within NaviDroid, it\nconstructs an enriched state transition graph (STG) with the trigger actions as\nthe edges for two involved states. Based on the STG, NaviDroid utilizes the\ndynamic programming algorithm to plan the exploration path, and augment the\nrun-time GUI with visualized hint moves for testers to quickly explore untested\nstates and avoid duplication. The automated experiments demonstrate the high\ncoverage and efficient path planning of NaviDroid. A user study further\nconfirms its usefulness in the participants covering more states and\nactivities, detecting more bugs within less time compared with the control\ngroup. NaviDroid demo video: https://youtu.be/lShFyg_nTA0.",
    "descriptor": "\nComments: Accepted by ICSE 2022. arXiv admin note: substantial text overlap with arXiv:2201.12085\n",
    "authors": [
      "Zhe Liu",
      "Chunyang Chen",
      "Junjie Wang",
      "Yuhui Su",
      "Qing Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.13992"
  },
  {
    "id": "arXiv:2205.13994",
    "title": "A framework for robotic arm pose estimation and movement prediction  based on deep and extreme learning models",
    "abstract": "Human-robot collaboration has gained a notable prominence in Industry 4.0, as\nthe use of collaborative robots increases efficiency and productivity in the\nautomation process. However, it is necessary to consider the use of mechanisms\nthat increase security in these environments, as the literature reports that\nrisk situations may exist in the context of human-robot collaboration. One of\nthe strategies that can be adopted is the visual recognition of the\ncollaboration environment using machine learning techniques, which can\nautomatically identify what is happening in the scene and what may happen in\nthe future. In this work, we are proposing a new framework that is capable of\ndetecting robotic arm keypoints commonly used in Industry 4.0. In addition to\ndetecting, the proposed framework is able to predict the future movement of\nthese robotic arms, thus providing relevant information that can be considered\nin the recognition of the human-robot collaboration scenario. The proposed\nframework is based on deep and extreme learning machine techniques. Results\nshow that the proposed framework is capable of detecting and predicting with\nlow error, contributing to the mitigation of risks in human-robot\ncollaboration.",
    "descriptor": "\nComments: To submit to the journal of supercomputing (Springer)\n",
    "authors": [
      "Iago Richard Rodrigues",
      "Marrone Dantas",
      "Assis Oliveira Filho",
      "Gibson Barbosa",
      "Daniel Bezerra",
      "Ricardo Souza",
      "Maria Val\u00e9ria Marquezini",
      "Patricia Takako Endo",
      "Judith Kelner",
      "Djamel H. Sadok"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.13994"
  },
  {
    "id": "arXiv:2205.13996",
    "title": "Video2StyleGAN: Disentangling Local and Global Variations in a Video",
    "abstract": "Image editing using a pretrained StyleGAN generator has emerged as a powerful\nparadigm for facial editing, providing disentangled controls over age,\nexpression, illumination, etc. However, the approach cannot be directly adopted\nfor video manipulations. We hypothesize that the main missing ingredient is the\nlack of fine-grained and disentangled control over face location, face pose,\nand local facial expressions. In this work, we demonstrate that such a\nfine-grained control is indeed achievable using pretrained StyleGAN by working\nacross multiple (latent) spaces (namely, the positional space, the W+ space,\nand the S space) and combining the optimization results across the multiple\nspaces. Building on this enabling component, we introduce Video2StyleGAN that\ntakes a target image and driving video(s) to reenact the local and global\nlocations and expressions from the driving video in the identity of the target\nimage. We evaluate the effectiveness of our method over multiple challenging\nscenarios and demonstrate clear improvements over alternative approaches.",
    "descriptor": "\nComments: Video : this https URL\n",
    "authors": [
      "Rameen Abdal",
      "Peihao Zhu",
      "Niloy J. Mitra",
      "Peter Wonka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13996"
  },
  {
    "id": "arXiv:2205.13997",
    "title": "Prototype Based Classification from Hierarchy to Fairness",
    "abstract": "Artificial neural nets can represent and classify many types of data but are\noften tailored to particular applications -- e.g., for \"fair\" or \"hierarchical\"\nclassification. Once an architecture has been selected, it is often difficult\nfor humans to adjust models for a new task; for example, a hierarchical\nclassifier cannot be easily transformed into a fair classifier that shields a\nprotected field. Our contribution in this work is a new neural network\narchitecture, the concept subspace network (CSN), which generalizes existing\nspecialized classifiers to produce a unified model capable of learning a\nspectrum of multi-concept relationships. We demonstrate that CSNs reproduce\nstate-of-the-art results in fair classification when enforcing concept\nindependence, may be transformed into hierarchical classifiers, or even\nreconcile fairness and hierarchy within a single classifier. The CSN is\ninspired by existing prototype-based classifiers that promote interpretability.",
    "descriptor": "",
    "authors": [
      "Mycal Tucker",
      "Julie Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13997"
  },
  {
    "id": "arXiv:2205.14003",
    "title": "Choiceless Polynomial Time with Witnessed Symmetric Choice",
    "abstract": "We extend Choiceless Polynomial Time (CPT), the currently only remaining\npromising candidate in the quest for a logic capturing PTime, so that this\nextended logic has the following property: for every class of structures for\nwhich isomorphism is definable, the logic automatically captures PTime.\nFor the construction of this logic we extend CPT by a witnessed symmetric\nchoice operator. This operator allows for choices from definable orbits. But,\nto ensure polynomial time evaluation, automorphisms have to be provided to\ncertify that the choice set is indeed an orbit.\nWe argue that, in this logic, definable isomorphism implies definable\ncanonization. Thereby, our construction removes the non-trivial step of\nextending isomorphism definability results to canonization. This step was a\npart of proofs that show that CPT or other logics capture PTime on a particular\nclass of structures. The step typically required substantial extra effort.",
    "descriptor": "\nComments: 65 pages. Full version of a paper to appear at LICS 22\n",
    "authors": [
      "Moritz Lichter",
      "Pascal Schweitzer"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14003"
  },
  {
    "id": "arXiv:2205.14005",
    "title": "RecipeRec: A Heterogeneous Graph Learning Model for Recipe  Recommendation",
    "abstract": "Recipe recommendation systems play an essential role in helping people decide\nwhat to eat. Existing recipe recommendation systems typically focused on\ncontent-based or collaborative filtering approaches, ignoring the higher-order\ncollaborative signal such as relational structure information among users,\nrecipes and food items. In this paper, we formalize the problem of recipe\nrecommendation with graphs to incorporate the collaborative signal into recipe\nrecommendation through graph modeling. In particular, we first present\nURI-Graph, a new and large-scale user-recipe-ingredient graph. We then propose\nRecipeRec, a novel heterogeneous graph learning model for recipe\nrecommendation. The proposed model can capture recipe content and collaborative\nsignal through a heterogeneous graph neural network with hierarchical attention\nand an ingredient set transformer. We also introduce a graph contrastive\naugmentation strategy to extract informative graph knowledge in a\nself-supervised manner. Finally, we design a joint objective function of\nrecommendation and contrastive learning to optimize the model. Extensive\nexperiments demonstrate that RecipeRec outperforms state-of-the-art methods for\nrecipe recommendation. Dataset and codes are available at\nhttps://github.com/meettyj/RecipeRec.",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Yijun Tian",
      "Chuxu Zhang",
      "Zhichun Guo",
      "Chao Huang",
      "Ronald Metoyer",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14005"
  },
  {
    "id": "arXiv:2205.14006",
    "title": "The Utility of Synthetic Reflexes and Haptic Feedback for Upper-Limb  Prostheses in a Dexterous Task Without Direct Vision",
    "abstract": "Individuals who use myoelectric upper-limb prostheses often rely heavily on\nvision to complete their daily activities. They thus struggle in situations\nwhere vision is overloaded, such as multitasking, or unavailable, such as poor\nlighting conditions. Non-amputees can easily accomplish such tasks due to\ntactile reflexes and haptic sensation guiding their upper-limb motor\ncoordination. Based on these principles, we developed and tested two novel\nprosthesis systems that incorporate autonomous controllers and provide the user\nwith touch-location feedback through either vibration or distributed pressure.\nThese capabilities were made possible by installing a custom contact-location\nsensor on thefingers of a commercial prosthetic hand, along with a custom\npressure sensor on the thumb. We compared the performance of the two systems\nagainst a standard myoelectric prosthesis and a myoelectric prosthesis with\nonly autonomous controllers in a difficult reach-to-pick-and-place task\nconducted without direct vision. Results from 40 non-amputee participants in\nthis between-subjects study indicated that vibrotactile feedback combined with\nsynthetic reflexes proved significantly more advantageous than the standard\nprosthesis in several of the task milestones. In addition, vibrotactile\nfeedback and synthetic reflexes improved grasp placement compared to only\nsynthetic reflexes or pressure feedback combined with synthetic reflexes. These\nresults indicate that both autonomous controllers and haptic feedback\nfacilitate success in dexterous tasks without vision, and that the type of\nhaptic display matters.",
    "descriptor": "",
    "authors": [
      "Neha Thomas",
      "Farimah Fazlollahi",
      "Katherine J. Kuchenbecker",
      "Jeremy D. Brown"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14006"
  },
  {
    "id": "arXiv:2205.14014",
    "title": "What Dense Graph Do You Need for Self-Attention?",
    "abstract": "Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.",
    "descriptor": "\nComments: Accepted by ICML 2022. Code is available at this https URL\n",
    "authors": [
      "Yuxing Wang",
      "Chu-Tak Lee",
      "Qipeng Guo",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14014"
  },
  {
    "id": "arXiv:2205.14017",
    "title": "BASALISC: Flexible Asynchronous Hardware Accelerator for Fully  Homomorphic Encryption",
    "abstract": "Fully Homomorphic Encryption (FHE) allows for secure computation on encrypted\ndata. We present BASALISC, an architecture family of FHE hardware accelerators\nthat aims to substantially accelerate FHE computations in the cloud. BASALISC\nimplements the BGV scheme, targets a range of parameter sets, and directly\nsupports and implements BGV bootstrapping. We propose a new generalized version\nof bootstrapping that can be implemented with optimized Montgomery multipliers\nthat cost 46% less in silicon area and 40% less in power consumption. BASALISC\nis a RISC architecture with a four-layer memory hierarchy, including a\ntwo-dimensional conflict-free inner memory layer that enables 32 Tb/s radix-256\nNTT computations without pipeline stalls. Our conflict-resolution data\npermutation hardware is re-used to compute BGV automorphisms without additional\nhardware and without throughput penalty. BASALISC additionally includes a\ncustom multiply-accumulate unit familiar in DSP architectures, with which we\naccelerate tight BGV key switching loops. The BASALISC computation units and\ninner memory layers are designed in asynchronous logic, allowing them to run at\ndifferent speeds to optimize each function. BASALISC is designed for ASIC\nimplementation with a 1 GHz operational frequency, and is already underway\ntoward tape-out with a 150mm2 die size in a 12nm Global Foundries process.The\nBASALISC toolchain comprises both a custom compiler and a joint performance and\ncorrectness simulator. We evaluate BASALISC in multiple ways: we study its\nphysical realizability; we emulate and formally verify its core functional\nunits; and we study its performance on a single iteration of logistic\nregression training over encrypted data. For this application, comprising from\nup to 900K high-level BASALISC instructions down to 27B low-level instructions,\nwe show a speedup of at least 2,025x over HElib.",
    "descriptor": "",
    "authors": [
      "Robin Geelen",
      "Michiel Van Beirendonck",
      "Hilder V. L. Pereira",
      "Brian Huffman",
      "Tynan McAuley",
      "Ben Selfridge",
      "Daniel Wagner",
      "Georgios Dimou",
      "Ingrid Verbauwhede",
      "Frederik Vercauteren",
      "David W. Archer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.14017"
  },
  {
    "id": "arXiv:2205.14018",
    "title": "Synchronizable functions on integers",
    "abstract": "For all natural numbers a,b and d > 0, we consider the function f_{a,b,d}\nwhich associates n/d to any integer n when it is a multiple of d, and an + b\notherwise; in particular f_{3,1,2} is the Collatz function. Coding in base a >\n1 with b < a, we realize these functions by input-deterministic\nletter-to-letter transducers with additional output final words. This\nparticular form allows to explicit, for any integer n, the composition n times\nof such a transducer to compute f^n_{a,b,d}. We even realize the closure under\ncomposition f^*_{a,b,d by an infinite input-deterministic letter-to-letter\ntransducer with a regular set of initial states and a length recurrent terminal\nfunction.",
    "descriptor": "\nComments: 23 pages, 15 figures\n",
    "authors": [
      "Didier Caucal",
      "Chlo\u00e9 Rispal"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.14018"
  },
  {
    "id": "arXiv:2205.14022",
    "title": "Future Transformer for Long-term Action Anticipation",
    "abstract": "The task of predicting future actions from a video is crucial for a\nreal-world agent interacting with others. When anticipating actions in the\ndistant future, we humans typically consider long-term relations over the whole\nsequence of actions, i.e., not only observed actions in the past but also\npotential actions in the future. In a similar spirit, we propose an end-to-end\nattention model for action anticipation, dubbed Future Transformer (FUTR), that\nleverages global attention over all input frames and output tokens to predict a\nminutes-long sequence of future actions. Unlike the previous autoregressive\nmodels, the proposed method learns to predict the whole sequence of future\nactions in parallel decoding, enabling more accurate and fast inference for\nlong-term anticipation. We evaluate our method on two standard benchmarks for\nlong-term action anticipation, Breakfast and 50 Salads, achieving\nstate-of-the-art results.",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Dayoung Gong",
      "Joonseok Lee",
      "Manjin Kim",
      "Seong Jong Ha",
      "Minsu Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14022"
  },
  {
    "id": "arXiv:2205.14026",
    "title": "Locally Authenticated Privacy-preserving Voice Input",
    "abstract": "Increasing use of our biometrics (e.g., fingerprints, faces, or voices) to\nunlock access to and interact with online services raises concerns about the\ntrade-offs between convenience, privacy, and security. Service providers must\nauthenticate their users, although individuals may wish to maintain privacy and\nlimit the disclosure of sensitive attributes beyond the authentication step,\n\\eg~when interacting with Voice User Interfaces (VUIs). Preserving privacy\nwhile performing authentication is challenging, particularly where adversaries\ncan use biometric data to train transformation tools (e.g.,`deepfaked' speech)\nand use the faked output to defeat existing authentication systems. In this\npaper, we take a step towards understanding security and privacy requirements\nto establish the threat and defense boundaries. We introduce a secure, flexible\nprivacy-preserving system to capture and store an on-device fingerprint of the\nusers' raw signals (i.e., voice) for authentication instead of sending/sharing\nthe raw biometric signals. We then analyze this fingerprint using different\npredictors, each evaluating its legitimacy from a different perspective (e.g.,\ntarget identity claim, spoofing attempt, and liveness). We fuse multiple\npredictors' decisions to make a final decision on whether the user input is\nlegitimate or not. Validating legitimate users yields an accuracy rate of\n98.68% after cross-validation using our verification technique. The pipeline\nruns in tens of milliseconds when tested on a CPU and a single-core ARM\nprocessor, without specialized hardware.",
    "descriptor": "",
    "authors": [
      "Ranya Aloufi",
      "Andreas Nautsch",
      "Hamed Haddadi",
      "David Boyle"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.14026"
  },
  {
    "id": "arXiv:2205.14027",
    "title": "Learning Dynamical Systems via Koopman Operator Regression in  Reproducing Kernel Hilbert Spaces",
    "abstract": "We study a class of dynamical systems modelled as Markov chains that admit an\ninvariant distribution via the corresponding transfer, or Koopman, operator.\nWhile data-driven algorithms to reconstruct such operators are well known,\ntheir relationship with statistical learning is largely unexplored. We\nformalize a framework to learn the Koopman operator from finite data\ntrajectories of the dynamical system. We consider the restriction of this\noperator to a reproducing kernel Hilbert space and introduce a notion of risk,\nfrom which different estimators naturally arise. We link the risk with the\nestimation of the spectral decomposition of the Koopman operator. These\nobservations motivate a reduced-rank operator regression (RRR) estimator. We\nderive learning bounds for the proposed estimator, holding both in i.i.d. and\nnon i.i.d. settings, the latter in terms of mixing coefficients. Our results\nsuggest RRR might be beneficial over other widely used estimators as confirmed\nin numerical experiments both for forecasting and mode decomposition.",
    "descriptor": "\nComments: Main text: 9 pages, 1 figure, 1 table. Supplementary informations: 15 pages, 3 figures, 1 table\n",
    "authors": [
      "Vladimir Kostic",
      "Pietro Novelli",
      "Andreas Maurer",
      "Carlo Ciliberto",
      "Lorenzo Rosasco",
      "Massimiliano Pontil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14027"
  },
  {
    "id": "arXiv:2205.14028",
    "title": "A new discretization technique for initial value problems based on a  variational principle",
    "abstract": "Motivated by the fact that both the classical and quantum description of\nnature rest on causality and a variational principle, we develop a novel and\nhighly versatile discretization prescription for classical initial value\nproblems (IVPs). It is based on an optimization functional with doubled degrees\nof freedom, which is discretized using a single regularized summation-by-parts\n(SBP) operator. The variational principle provides a straight forward recipe to\nformulate the corresponding optimization functional for a large class of\ndifferential equations. The novel regularization we develop here is inspired by\nthe weak imposition of initial data, often deployed in the modern treatment of\nIVPs and is implemented using affine coordinates. We demonstrate numerically\nthe stability, accuracy and convergence properties of our approach in systems\nwith classical equations of motion featuring both first and second order\nderivatives in time.",
    "descriptor": "\nComments: 26 pages, 16 figures\n",
    "authors": [
      "Alexander Rothkopf",
      "Jan Nordstr\u00f6m"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "High Energy Physics - Lattice (hep-lat)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14028"
  },
  {
    "id": "arXiv:2205.14032",
    "title": "Ontology Design Facilitating Wikibase Integration -- and a Worked  Example for Historical Data",
    "abstract": "Wikibase -- which is the software underlying Wikidata -- is a powerful\nplatform for knowledge graph creation and management. However, it has been\ndeveloped with a crowd-sourced knowledge graph creation scenario in mind, which\nin particular means that it has not been designed for use case scenarios in\nwhich a tightly controlled high-quality schema, in the form of an ontology, is\nto be imposed, and indeed, independently developed ontologies do not\nnecessarily map seamlessly to the Wikibase approach. In this paper, we provide\nthe key ingredients needed in order to combine traditional ontology modeling\nwith use of the Wikibase platform, namely a set of \\emph{axiom} patterns that\nbridge the paradigm gap, together with usage instructions and a worked example\nfor historical data.",
    "descriptor": "",
    "authors": [
      "Cogan Shimizu",
      "Andrew Eells",
      "Seila Gonzalez",
      "Lu Zhou",
      "Pascal Hitzler",
      "Alicia Sheill",
      "Catherine Foley",
      "Dean Rehberger"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14032"
  },
  {
    "id": "arXiv:2205.14035",
    "title": "Learning to Control Linear Systems can be Hard",
    "abstract": "In this paper, we study the statistical difficulty of learning to control\nlinear systems. We focus on two standard benchmarks, the sample complexity of\nstabilization, and the regret of the online learning of the Linear Quadratic\nRegulator (LQR). Prior results state that the statistical difficulty for both\nbenchmarks scales polynomially with the system state dimension up to\nsystem-theoretic quantities. However, this does not reveal the whole picture.\nBy utilizing minimax lower bounds for both benchmarks, we prove that there\nexist non-trivial classes of systems for which learning complexity scales\ndramatically, i.e. exponentially, with the system dimension. This situation\narises in the case of underactuated systems, i.e. systems with fewer inputs\nthan states. Such systems are structurally difficult to control and their\nsystem theoretic quantities can scale exponentially with the system dimension\ndominating learning complexity. Under some additional structural assumptions\n(bounding systems away from uncontrollability), we provide qualitatively\nmatching upper bounds. We prove that learning complexity can be at most\nexponential with the controllability index of the system, that is the degree of\nunderactuation.",
    "descriptor": "\nComments: Accepted to COLT 2022\n",
    "authors": [
      "Anastasios Tsiamis",
      "Ingvar Ziemann",
      "Manfred Morari",
      "Nikolai Matni",
      "George J. Pappas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14035"
  },
  {
    "id": "arXiv:2205.14036",
    "title": "StereoKG: Data-Driven Knowledge Graph Construction for Cultural  Knowledge and Stereotypes",
    "abstract": "Analyzing ethnic or religious bias is important for improving fairness,\naccountability, and transparency of natural language processing models.\nHowever, many techniques rely on human-compiled lists of bias terms, which are\nexpensive to create and are limited in coverage. In this study, we present a\nfully data-driven pipeline for generating a knowledge graph (KG) of cultural\nknowledge and stereotypes. Our resulting KG covers 5 religious groups and 5\nnationalities and can easily be extended to include more entities. Our human\nevaluation shows that the majority (59.2%) of non-singleton entries are\ncoherent and complete stereotypes. We further show that performing intermediate\nmasked language model training on the verbalized KG leads to a higher level of\ncultural awareness in the model and has the potential to increase\nclassification performance on knowledge-crucial samples on a related task,\ni.e., hate speech detection.",
    "descriptor": "\nComments: 12 pages, 2 figures, accepted as a long paper at WOAH at NAACL 2022\n",
    "authors": [
      "Awantee Deshpande",
      "Dana Ruiter",
      "Marius Mosbach",
      "Dietrich Klakow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14036"
  },
  {
    "id": "arXiv:2205.14039",
    "title": "Group-invariant max filtering",
    "abstract": "Given a real inner product space $V$ and a group $G$ of linear isometries, we\nconstruct a family of $G$-invariant real-valued functions on $V$ that we call\nmax filters. In the case where $V=\\mathbb{R}^d$ and $G$ is finite, a suitable\nmax filter bank separates orbits, and is even bilipschitz in the quotient\nmetric. In the case where $V=L^2(\\mathbb{R}^d)$ and $G$ is the group of\ntranslation operators, a max filter exhibits stability to diffeomorphic\ndistortion like that of the scattering transform introduced by Mallat. We\nestablish that max filters are well suited for various classification tasks,\nboth in theory and in practice.",
    "descriptor": "",
    "authors": [
      "Jameson Cahill",
      "Joseph W. Iverson",
      "Dustin G. Mixon",
      "Daniel Packer"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)"
    ],
    "url": "https://arxiv.org/abs/2205.14039"
  },
  {
    "id": "arXiv:2205.14040",
    "title": "Intelligent Transportation Systems' Orchestration: Lessons Learned &  Potential Opportunities",
    "abstract": "The growing deployment efforts of 5G networks globally has led to the\nacceleration of the businesses/services' digital transformation. This growth\nhas led to the need for new communication technologies that will promote this\ntransformation. 6G is being proposed as the set of technologies and\narchitectures that will achieve this target. Among the main use cases that have\nemerged for 5G networks and will continue to play a pivotal role in 6G networks\nis that of Intelligent Transportation Systems (ITSs). With all the projected\nbenefits of developing and deploying efficient and effective ITSs comes a group\nof unique challenges that need to be addressed. One prominent challenge is ITS\norchestration due to the various supporting technologies and heterogeneous\nnetworks used to offer the desired ITS applications/services. To that end, this\npaper focuses on the ITS orchestration challenge in detail by highlighting the\nrelated previous works from the literature and listing the lessons learned from\ncurrent ITS deployment orchestration efforts. It also presents multiple\npotential data-driven research opportunities in which paradigms such as\nreinforcement learning and federated learning can be deployed to offer\neffective and efficient ITS orchestration.",
    "descriptor": "\nComments: 6 pages, 3 figures, accepted and presented in 1320th International Conference on Recent Innovations in Engineering and Technology (ICRIET-2022)\n",
    "authors": [
      "Abdallah Moubayed",
      "Abdallah Shami",
      "Abbas Ibrahim"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14040"
  },
  {
    "id": "arXiv:2205.14041",
    "title": "Double Deep Q Networks for Sensor Management in Space Situational  Awareness",
    "abstract": "We present a novel Double Deep Q Network (DDQN) application to a sensor\nmanagement problem in space situational awareness (SSA). Frequent launches of\nsatellites into Earth orbit pose a significant sensor management challenge,\nwhereby a limited number of sensors are required to detect and track an\nincreasing number of objects. In this paper, we demonstrate the use of\nreinforcement learning to develop a sensor management policy for SSA. We\nsimulate a controllable Earth-based telescope, which is trained to maximise the\nnumber of satellites tracked using an extended Kalman filter. The estimated\nstate covariance matrices for satellites observed under the DDQN policy are\ngreatly reduced compared to those generated by an alternate (random) policy.\nThis work provides the basis for further advancements and motivates the use of\nreinforcement learning for SSA.",
    "descriptor": "\nComments: 6 pages, 8 figures, accepted for publication\n",
    "authors": [
      "Benedict Oakes",
      "Dominic Richards",
      "Jordi Barr",
      "Jason F. Ralph"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14041"
  },
  {
    "id": "arXiv:2205.14042",
    "title": "Reinforced Pedestrian Attribute Recognition with Group Optimization  Reward",
    "abstract": "Pedestrian Attribute Recognition (PAR) is a challenging task in intelligent\nvideo surveillance. Two key challenges in PAR include complex alignment\nrelations between images and attributes, and imbalanced data distribution.\nExisting approaches usually formulate PAR as a recognition task. Different from\nthem, this paper addresses it as a decision-making task via a reinforcement\nlearning framework. Specifically, PAR is formulated as a Markov decision\nprocess (MDP) by designing ingenious states, action space, reward function and\nstate transition. To alleviate the inter-attribute imbalance problem, we apply\nan Attribute Grouping Strategy (AGS) by dividing all attributes into subgroups\naccording to their region and category information. Then we employ an agent to\nrecognize each group of attributes, which is trained with Deep Q-learning\nalgorithm. We also propose a Group Optimization Reward (GOR) function to\nalleviate the intra-attribute imbalance problem. Experimental results on the\nthree benchmark datasets of PETA, RAP and PA100K illustrate the effectiveness\nand competitiveness of the proposed approach and demonstrate that the\napplication of reinforcement learning to PAR is a valuable research direction.",
    "descriptor": "",
    "authors": [
      "Zhong Ji",
      "Zhenfei Hu",
      "Yaodong Wang",
      "Shengjia Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14042"
  },
  {
    "id": "arXiv:2205.14050",
    "title": "MIMO Integrated Sensing and Communication with Extended Target: CRB-Rate  Tradeoff",
    "abstract": "This paper studies a multiple-input multiple-output (MIMO) integrated sensing\nand communication (ISAC) system, in which a multi-antenna base station (BS)\nsends unified wireless signals to estimate an extended target and communicate\nwith a multi-antenna communication user (CU) at the same time. We investigate\nthe fundamental tradeoff between the estimation Cram\\'er-Rao bound (CRB) for\nsensing and the data rate for communication, by characterizing the Pareto\nboundary of the achievable CRB-rate (C-R) region. Towards this end, we\nformulate a new MIMO rate maximization problem by optimizing the transmit\ncovariance matrix at the BS, subject to a new form of maximum CRB constraint\ntogether with a maximum transmit power constraint. We derive the optimal\ntransmit covariance solution in a semi-closed form, by first implementing the\nsingular-value decomposition (SVD) to diagonalize the communication channel and\nthen properly allocating the transmit power over these subchannels for\ncommunication and other orthogonal subchannels (if any) for dedicated sensing.\nIt is shown that the optimal transmit covariance is of full rank, which unifies\nthe conventional rate maximization design with water-filling power allocation\nand the CRB minimization design with isotropic transmission. Numerical results\nare provided to validate the performance achieved by our proposed optimal\ndesign, in comparison with other benchmark schemes.",
    "descriptor": "",
    "authors": [
      "Haocheng Hua",
      "Xianxin Song",
      "Yuan Fang",
      "Tony Xiao Han",
      "Jie Xu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.14050"
  },
  {
    "id": "arXiv:2205.14051",
    "title": "Fine-tuning deep learning models for stereo matching using results from  semi-global matching",
    "abstract": "Deep learning (DL) methods are widely investigated for stereo image matching\ntasks due to their reported high accuracies. However, their\ntransferability/generalization capabilities are limited by the instances seen\nin the training data. With satellite images covering large-scale areas with\nvariances in locations, content, land covers, and spatial patterns, we expect\ntheir performances to be impacted. Increasing the number and diversity of\ntraining data is always an option, but with the ground-truth disparity being\nlimited in remote sensing due to its high cost, it is almost impossible to\nobtain the ground-truth for all locations. Knowing that classical stereo\nmatching methods such as Census-based semi-global-matching (SGM) are widely\nadopted to process different types of stereo data, we therefore, propose a\nfinetuning method that takes advantage of disparity maps derived from SGM on\ntarget stereo data. Our proposed method adopts a simple scheme that uses the\nenergy map derived from the SGM algorithm to select high confidence disparity\nmeasurements, at the same utilizing the images to limit these selected\ndisparity measurements on texture-rich regions. Our approach aims to\ninvestigate the possibility of improving the transferability of current DL\nmethods to unseen target data without having their ground truth as a\nrequirement. To perform a comprehensive study, we select 20 study-sites around\nthe world to cover a variety of complexities and densities. We choose\nwell-established DL methods like geometric and context network (GCNet), pyramid\nstereo matching network (PSMNet), and LEAStereo for evaluation. Our results\nindicate an improvement in the transferability of the DL methods across\ndifferent regions visually and numerically.",
    "descriptor": "\nComments: 6 figures\n",
    "authors": [
      "Hessah Albanwan",
      "Rongjun Qin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14051"
  },
  {
    "id": "arXiv:2205.14054",
    "title": "Contrastive Siamese Network for Semi-supervised Speech Recognition",
    "abstract": "This paper introduces contrastive siamese (c-siam) network, an architecture\nfor leveraging unlabeled acoustic data in speech recognition. c-siam is the\nfirst network that extracts high-level linguistic information from speech by\nmatching outputs of two identical transformer encoders. It contains augmented\nand target branches which are trained by: (1) masking inputs and matching\noutputs with a contrastive loss, (2) incorporating a stop gradient operation on\nthe target branch, (3) using an extra learnable transformation on the augmented\nbranch, (4) introducing new temporal augment functions to prevent the shortcut\nlearning problem. We use the Libri-light 60k unsupervised data and the\nLibriSpeech 100hrs/960hrs supervised data to compare c-siam and other\nbest-performing systems. Our experiments show that c-siam provides 20% relative\nword error rate improvement over wav2vec baselines. A c-siam network with 450M\nparameters achieves competitive results compared to the state-of-the-art\nnetworks with 600M parameters.",
    "descriptor": "",
    "authors": [
      "Soheil Khorram",
      "Jaeyoung Kim",
      "Anshuman Tripathi",
      "Han Lu",
      "Qian Zhang",
      "Hasim Sak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14054"
  },
  {
    "id": "arXiv:2205.14055",
    "title": "Benign Overparameterization in Membership Inference with Early Stopping",
    "abstract": "Does a neural network's privacy have to be at odds with its accuracy? In this\nwork, we study the effects the number of training epochs and parameters have on\na neural network's vulnerability to membership inference (MI) attacks, which\naim to extract potentially private information about the training data. We\nfirst demonstrate how the number of training epochs and parameters individually\ninduce a privacy-utility trade-off: more of either improves generalization\nperformance at the expense of lower privacy. However, remarkably, we also show\nthat jointly tuning both can eliminate this privacy-utility trade-off.\nSpecifically, with careful tuning of the number of training epochs, more\noverparameterization can increase model privacy for fixed generalization error.\nTo better understand these phenomena theoretically, we develop a powerful new\nleave-one-out analysis tool to study the asymptotic behavior of linear\nclassifiers and apply it to characterize the sample-specific loss threshold MI\nattack in high-dimensional logistic regression. For practitioners, we introduce\na low-overhead procedure to estimate MI risk and tune the number of training\nepochs to guard against MI attacks.",
    "descriptor": "\nComments: 28 pages, 12 figures\n",
    "authors": [
      "Jasper Tan",
      "Daniel LeJeune",
      "Blake Mason",
      "Hamid Javadi",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14055"
  },
  {
    "id": "arXiv:2205.14056",
    "title": "Dual Convexified Convolutional Neural Networks",
    "abstract": "We propose the framework of dual convexified convolutional neural networks\n(DCCNNs). In this framework, we first introduce a primal learning problem\nmotivated from convexified convolutional neural networks (CCNNs), and then\nconstruct the dual convex training program through careful analysis of the\nKarush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach\nreduces the memory overhead of constructing a large kernel matrix and\neliminates the ambiguity of factorizing the matrix. Due to the low-rank\nstructure in CCNNs and the related subdifferential of nuclear norms, there is\nno closed-form expression to recover the primal solution from the dual\nsolution. To overcome this, we propose a highly novel weight recovery\nalgorithm, which takes the dual solution and the kernel information as the\ninput, and recovers the linear and convolutional weights of a CCNN.\nFurthermore, our recovery algorithm exploits the low-rank structure and imposes\na small number of filters indirectly, which reduces the parameter size. As a\nresult, DCCNNs inherit all the statistical benefits of CCNNs, while enjoying a\nmore formal and efficient workflow.",
    "descriptor": "",
    "authors": [
      "Site Bai",
      "Chuyang Ke",
      "Jean Honorio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14056"
  },
  {
    "id": "arXiv:2205.14057",
    "title": "General Optimization Framework for Recurrent Reachability Objectives",
    "abstract": "We consider the mobile robot path planning problem for a class of recurrent\nreachability objectives. These objectives are parameterized by the expected\ntime needed to visit one position from another, the expected square of this\ntime, and also the frequency of moves between two neighboring locations. We\ndesign an efficient strategy synthesis algorithm for recurrent reachability\nobjectives and demonstrate its functionality on non-trivial instances.",
    "descriptor": "",
    "authors": [
      "David Kla\u0161ka",
      "Anton\u00edn Ku\u010dera",
      "V\u00edt Musil",
      "Vojt\u011bch \u0158eh\u00e1k"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.14057"
  },
  {
    "id": "arXiv:2205.14058",
    "title": "Image Harmonization with Region-wise Contrastive Learning",
    "abstract": "Image harmonization task aims at harmonizing different composite foreground\nregions according to specific background image. Previous methods would rather\nfocus on improving the reconstruction ability of the generator by some internal\nenhancements such as attention, adaptive normalization and light adjustment,\n$etc.$. However, they pay less attention to discriminating the foreground and\nbackground appearance features within a restricted generator, which becomes a\nnew challenge in image harmonization task. In this paper, we propose a novel\nimage harmonization framework with external style fusion and region-wise\ncontrastive learning scheme. For the external style fusion, we leverage the\nexternal background appearance from the encoder as the style reference to\ngenerate harmonized foreground in the decoder. This approach enhances the\nharmonization ability of the decoder by external background guidance. Moreover,\nfor the contrastive learning scheme, we design a region-wise contrastive loss\nfunction for image harmonization task. Specifically, we first introduce a\nstraight-forward samples generation method that selects negative samples from\nthe output harmonized foreground region and selects positive samples from the\nground-truth background region. Our method attempts to bring together\ncorresponding positive and negative samples by maximizing the mutual\ninformation between the foreground and background styles, which desirably makes\nour harmonization network more robust to discriminate the foreground and\nbackground style features when harmonizing composite images. Extensive\nexperiments on the benchmark datasets show that our method can achieve a clear\nimprovement in harmonization quality and demonstrate the good generalization\ncapability in real-scenario applications.",
    "descriptor": "",
    "authors": [
      "Jingtang Liang",
      "Chi-Man Pun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14058"
  },
  {
    "id": "arXiv:2205.14065",
    "title": "Simple Unsupervised Object-Centric Learning for Complex and Naturalistic  Videos",
    "abstract": "Unsupervised object-centric learning aims to represent the modular,\ncompositional, and causal structure of a scene as a set of object\nrepresentations and thereby promises to resolve many critical limitations of\ntraditional single-vector representations such as poor systematic\ngeneralization. Although there have been many remarkable advances in recent\nyears, one of the most critical problems in this direction has been that\nprevious methods work only with simple and synthetic scenes but not with\ncomplex and naturalistic images or videos. In this paper, we propose STEVE, an\nunsupervised model for object-centric learning in videos. Our proposed model\nmakes a significant advancement by demonstrating its effectiveness on various\ncomplex and naturalistic videos unprecedented in this line of research.\nInterestingly, this is achieved by neither adding complexity to the model\narchitecture nor introducing a new objective or weak supervision. Rather, it is\nachieved by a surprisingly simple architecture that uses a transformer-based\nimage decoder conditioned on slots and the learning objective is simply to\nreconstruct the observation. Our experiment results on various complex and\nnaturalistic videos show significant improvements compared to the previous\nstate-of-the-art.",
    "descriptor": "",
    "authors": [
      "Gautam Singh",
      "Yi-Fu Wu",
      "Sungjin Ahn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14065"
  },
  {
    "id": "arXiv:2205.14068",
    "title": "Interleaved Prange: A New Generic Decoder for Interleaved Codes",
    "abstract": "Due to the recent challenges in post-quantum cryptography, several new\napproaches for code-based cryptography have been proposed. For example, a\nvariant of the McEliece cryptosystem based on interleaved codes was proposed.\nIn order to deem such new settings secure, we first need to understand and\nanalyze the complexity of the underlying problem, in this case the problem of\ndecoding a random interleaved code. A simple approach to decode such codes,\nwould be to randomly choose a vector in the row span of the received matrix and\nrun a classical information set decoding algorithm on this erroneous codeword.\nIn this paper, we propose a new generic decoder for interleaved codes, which is\nan adaption of the classical idea of information set decoding by Prange and\nperfectly fits the interleaved setting. We then analyze the cost of the new\nalgorithm and a comparison to the simple approach described above shows the\nsuperiority of Interleaved Prange.",
    "descriptor": "",
    "authors": [
      "Anmoal Porwal",
      "Lukas Holzbaur",
      "Hedongliang Liu",
      "Julian Renner",
      "Antonia Wachter-Zeh",
      "Violetta Weger"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.14068"
  },
  {
    "id": "arXiv:2205.14069",
    "title": "Deep Coding Patterns Design for Compressive Near-Infrared Spectral  Classification",
    "abstract": "Compressive spectral imaging (CSI) has emerged as an attractive compression\nand sensing technique, primarily to sense spectral regions where traditional\nsystems result in highly costly such as in the near-infrared spectrum.\nRecently, it has been shown that spectral classification can be performed\ndirectly in the compressive domain, considering the amount of spectral\ninformation embedded in the measurements, skipping the reconstruction step.\nConsequently, the classification quality directly depends on the set of coding\npatterns employed in the sensing step. Therefore, this work proposes an\nend-to-end approach to jointly design the coding patterns used in CSI and the\nnetwork parameters to perform spectral classification directly from the\nembedded near-infrared compressive measurements. Extensive simulation on the\nthree-dimensional coded aperture snapshot spectral imaging (3D-CASSI) system\nvalidates that the proposed design outperforms traditional and random design in\nup to 10% of classification accuracy.",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Jorge Bacca",
      "Alejandra Hernandez-Rojas",
      "Henry Arguello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14069"
  },
  {
    "id": "arXiv:2205.14070",
    "title": "Multi-criteria Decision-making of Intelligent Vehicles under Fault  Condition Enhancing Public-private Partnership",
    "abstract": "With the development of vehicular technologies on automation,\nelectrification, and digitalization, vehicles are becoming more intelligent\nwhile being exposed to more complex, uncertain, and frequently occurring\nfaults. In this paper, we look into the maintenance planning of an operating\nvehicle under fault condition and formulate it as a multi-criteria\ndecision-making problem. The maintenance decisions are generated by route\nsearching in road networks and evaluated based on risk assessment considering\nthe uncertainty of vehicle breakdowns. Particularly, we consider two criteria,\nnamely the risk of public time loss and the risk of mission delay, representing\nthe concerns of the public sector and the private sector, respectively. A\npublic time loss model is developed to evaluate the traffic congestion caused\nby a vehicle breakdown and the corresponding towing process. The Pareto optimal\nset of non-dominated decisions is derived by evaluating the risk of the\ndecisions. We demonstrate the relevance of the problem and the effectiveness of\nthe proposed method by numerical experiments derived from real-world scenarios.\nThe experiments show that neglecting the risk of vehicle breakdown on public\nroads can cause a high risk of public time loss in dense traffic flow. With the\nproposed method, alternate decisions can be derived to reduce the risks of\npublic time loss significantly with a low increase in the risk of mission\ndelay. This study aims at catalyzing public-private partnership through\ncollaborative decision-making between the private sector and the public sector,\nthus archiving a more sustainable transportation system in the future.",
    "descriptor": "",
    "authors": [
      "Xin Tao",
      "Mladen \u010ci\u010di\u0107",
      "Jonas M\u00e5rtensson"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14070"
  },
  {
    "id": "arXiv:2205.14071",
    "title": "A Mechanically Assisted Examination of Vacuity and Question Begging in  Anselm's Ontological Argument",
    "abstract": "I use mechanized verification to examine several first- and higher-order\nformalizations of Anselm's Ontological Argument against the charge of begging\nthe question. I propose three different but related criteria for a premise to\nbeg the question in fully formal proofs and find that one or another applies to\nall the formalizations examined. I also show that all these formalizations\nentail variants that are vacuous, in the sense that they apply no\ninterpretation to \"than which there is no greater\" and are therefore vulnerable\nto Gaunilo's refutation. My purpose is to demonstrate that mechanized\nverification provides an effective and reliable technique to perform these\nanalyses; readers may decide whether the forms of question begging and vacuity\nso identified affect their interest in the Argument or its various\nformalizations. This version updates the paper that originally appeared as\nChapter 13 in \"Beyond Faith and Rationality: Essays on Logic, Religion and\nPhilosophy\" published by Springer to respond to criticisms by Oppenheimer and\nZalta.",
    "descriptor": "",
    "authors": [
      "John Rushby"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14071"
  },
  {
    "id": "arXiv:2205.14076",
    "title": "How to Tame Multiple Spending in Decentralized Cryptocurrencies",
    "abstract": "The last decade has seen a variety of Asset-Transfer systems designed for\ndecentralized environments. To address the problem of double-spending, these\nsystems inherently make strong model assumptions and spend a lot of resources.\nIn this paper, we take a non-orthodox approach to the double-spending problem\nthat might suit better realistic environments in which these systems are to be\ndeployed. We consider the decentralized trust setting, where each user may\nindependently choose who to trust by forming its local quorums. In this\nsetting, we define $k$-Spending Asset Transfer, a relaxed version of asset\ntransfer which bounds the number of times the same asset can be spent. We\nestablish a precise relationship between the decentralized trust assumptions\nand $k$, the optimal spending number of the system.",
    "descriptor": "",
    "authors": [
      "Jo\u00e3o Paulo Bezerra",
      "Petr Kuznetsov"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14076"
  },
  {
    "id": "arXiv:2205.14077",
    "title": "ARKODE: A flexible IVP solver infrastructure for one-step methods",
    "abstract": "We describe the ARKODE library of one-step time integration methods for\ninitial-value problems (IVPs). In addition to providing standard explicit and\ndiagonally-implicit Runge--Kutta methods, ARKODE also supports one-step methods\ndesigned to treat additive splittings of the IVP, including implicit-explicit\n(ImEx) additive Runge--Kutta methods and multirate infinitesimal (MRI) methods.\nWe present the role of ARKODE within the SUNDIALS suite of time integration and\nnonlinear solver libraries, the core ARKODE infrastructure for utilities common\nto large classes of one-step methods, as well as its use of \"time stepper\"\nmodules enabling easy incorporation of novel algorithms into the library.\nNumerical results show example problems of increasing complexity, highlighting\nthe algorithmic flexibility afforded through this infrastructure, and end with\na larger multiphysics application leveraging multiple algorithmic features from\nARKODE and SUNDIALS.",
    "descriptor": "",
    "authors": [
      "Daniel R. Reynolds",
      "David J. Gardner",
      "Carol S. Woodward",
      "Rujeko Chinomona"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14077"
  },
  {
    "id": "arXiv:2205.14079",
    "title": "Haptic Shared Control Improves Neural Efficiency During Myoelectric  Prosthesis Use",
    "abstract": "Clinical myoelectric prostheses lack the sensory feedback and sufficient\ndexterity required to complete activities of daily living efficiently and\naccurately. Providing haptic feedback of relevant environmental cues to the\nuser or imbuing the prosthesis with autonomous control authority have been\nseparately shown to improve prosthesis utility. Few studies, however, have\ninvestigated the effect of combining these two approaches in a shared control\nparadigm, and none have evaluated such an approach from the perspective of\nneural efficiency (the relationship between task performance and mental effort\nmeasured directly from the brain). In this work, we analyzed the neural\nefficiency of 30 non-amputee participants in a grasp-and-lift task of a brittle\nobject. Here, a myoelectric prosthesis featuring vibrotactile feedback of grip\nforce and autonomous control of grasping was compared with a standard\nmyoelectric prosthesis with and without vibrotactile feedback. As a measure of\nmental effort, we captured the prefrontal cortex activity changes using\nfunctional near infrared spectroscopy during the experiment. Results showed\nthat only the haptic shared control system enabled users to achieve high neural\nefficiency, and that vibrotactile feedback was important for grasping with the\nappropriate grip force. These results indicate that the haptic shared control\nsystem synergistically combines the benefits of haptic feedback and autonomous\ncontrollers, and is well-poised to inform such hybrid advancements in\nmyoelectric prosthesis technology.",
    "descriptor": "",
    "authors": [
      "Neha Thomas",
      "Alexandra J. Miller",
      "Hasan Ayaz",
      "Jeremy D. Brown"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14079"
  },
  {
    "id": "arXiv:2205.14082",
    "title": "AANG: Automating Auxiliary Learning",
    "abstract": "When faced with data-starved or highly complex end-tasks, it is commonplace\nfor machine learning practitioners to introduce auxiliary objectives as\nsupplementary learning signals. Whilst much work has been done to formulate\nuseful auxiliary objectives, their construction is still an art which proceeds\nby slow and tedious hand-design. Intuitions about how and when these objectives\nimprove end-task performance have also had limited theoretical backing. In this\nwork, we present an approach for automatically generating a suite of auxiliary\nobjectives. We achieve this by deconstructing existing objectives within a\nnovel unified taxonomy, identifying connections between them, and generating\nnew ones based on the uncovered structure. Next, we theoretically formalize\nwidely-held intuitions about how auxiliary learning improves generalization of\nthe end-task. This leads us to a principled and efficient algorithm for\nsearching the space of generated objectives to find those most useful to a\nspecified end-task. With natural language processing (NLP) as our domain of\nstudy, we empirically verify that our automated auxiliary learning pipeline\nleads to strong improvements over competitive baselines across continued\ntraining experiments on a pre-trained model on 5 NLP end-tasks.",
    "descriptor": "\nComments: 18 pages, 9 tables and 4 figures\n",
    "authors": [
      "Lucio M. Dery",
      "Paul Michel",
      "Mikhail Khodak",
      "Graham Neubig",
      "Ameet Talwalkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14082"
  },
  {
    "id": "arXiv:2205.14083",
    "title": "Sharpness-Aware Training for Free",
    "abstract": "Modern deep neural networks (DNNs) have achieved state-of-the-art\nperformances but are typically over-parameterized. The over-parameterization\nmay result in undesirably large generalization error in the absence of other\ncustomized training strategies. Recently, a line of research under the name of\nSharpness-Aware Minimization (SAM) has shown that minimizing a sharpness\nmeasure, which reflects the geometry of the loss landscape, can significantly\nreduce the generalization error. However, SAM-like methods incur a two-fold\ncomputational overhead of the given base optimizer (e.g. SGD) for approximating\nthe sharpness measure. In this paper, we propose Sharpness-Aware Training for\nFree, or SAF, which mitigates the sharp landscape at almost zero additional\ncomputational cost over the base optimizer. Intuitively, SAF achieves this by\navoiding sudden drops in the loss in the sharp local minima throughout the\ntrajectory of the updates of the weights. Specifically, we suggest a novel\ntrajectory loss, based on the KL-divergence between the outputs of DNNs with\nthe current weights and past weights, as a replacement of the SAM's sharpness\nmeasure. This loss captures the rate of change of the training loss along the\nmodel's update trajectory. By minimizing it, SAF ensures the convergence to a\nflat minimum with improved generalization capabilities. Extensive empirical\nresults show that SAF minimizes the sharpness in the same way that SAM does,\nyielding better results on the ImageNet dataset with essentially the same\ncomputational cost as the base optimizer.",
    "descriptor": "",
    "authors": [
      "Jiawei Du",
      "Daquan Zhou",
      "Jiashi Feng",
      "Vincent Y.F. Tan",
      "Joey Tianyi Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14083"
  },
  {
    "id": "arXiv:2205.14084",
    "title": "UAlberta at SemEval 2022 Task 2: Leveraging Glosses and Translations for  Multilingual Idiomaticity Detection",
    "abstract": "We describe the University of Alberta systems for the SemEval-2022 Task 2 on\nmultilingual idiomaticity detection. Working under the assumption that\nidiomatic expressions are noncompositional, our first method integrates\ninformation on the meanings of the individual words of an expression into a\nbinary classifier. Further hypothesizing that literal and idiomatic expressions\ntranslate differently, our second method translates an expression in context,\nand uses a lexical knowledge base to determine if the translation is literal.\nOur approaches are grounded in linguistic phenomena, and leverage existing\nsources of lexical knowledge. Our results offer support for both approaches,\nparticularly the former.",
    "descriptor": "\nComments: To be published in the proceedings of SemEval 2022\n",
    "authors": [
      "Bradley Hauer",
      "Seeratpal Jaura",
      "Talgat Omarov",
      "Grzegorz Kondrak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14084"
  },
  {
    "id": "arXiv:2205.14086",
    "title": "Patching Leaks in the Charformer for Efficient Character-Level  Generation",
    "abstract": "Character-based representations have important advantages over subword-based\nones for morphologically rich languages. They come with increased robustness to\nnoisy input and do not need a separate tokenization step. However, they also\nhave a crucial disadvantage: they notably increase the length of text\nsequences. The GBST method from Charformer groups (aka downsamples) characters\nto solve this, but allows information to leak when applied to a Transformer\ndecoder. We solve this information leak issue, thereby enabling character\ngrouping in the decoder. We show that Charformer downsampling has no apparent\nbenefits in NMT over previous downsampling methods in terms of translation\nquality, however it can be trained roughly 30% faster. Promising performance on\nEnglish--Turkish translation indicate the potential of character-level models\nfor morphologically-rich languages.",
    "descriptor": "",
    "authors": [
      "Lukas Edman",
      "Antonio Toral",
      "Gertjan van Noord"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14086"
  },
  {
    "id": "arXiv:2205.14087",
    "title": "OpenCalib: A multi-sensor calibration toolbox for autonomous driving",
    "abstract": "Accurate sensor calibration is a prerequisite for multi-sensor perception and\nlocalization systems for autonomous vehicles. The intrinsic parameter\ncalibration of the sensor is to obtain the mapping relationship inside the\nsensor, and the extrinsic parameter calibration is to transform two or more\nsensors into a unified spatial coordinate system. Most sensors need to be\ncalibrated after installation to ensure the accuracy of sensor measurements. To\nthis end, we present OpenCalib, a calibration toolbox that contains a rich set\nof various sensor calibration methods. OpenCalib covers manual calibration\ntools, automatic calibration tools, factory calibration tools, and online\ncalibration tools for different application scenarios. At the same time, to\nevaluate the calibration accuracy and subsequently improve the accuracy of the\ncalibration algorithm, we released a corresponding benchmark dataset. This\npaper introduces various features and calibration methods of this toolbox. To\nour knowledge, this is the first open-sourced calibration codebase containing\nthe full set of autonomous-driving-related calibration approaches in this area.\nWe wish that the toolbox could be helpful to autonomous driving researchers. We\nhave open-sourced our code on GitHub to benefit the community. Code is\navailable at https://github.com/PJLab-ADG/SensorsCalibration.",
    "descriptor": "",
    "authors": [
      "Guohang Yan",
      "Liu Zhuochun",
      "Chengjie Wang",
      "Chunlei Shi",
      "Pengjin Wei",
      "Xinyu Cai",
      "Tao Ma",
      "Zhizheng Liu",
      "Zebin Zhong",
      "Yuqian Liu",
      "Ming Zhao",
      "Zheng Ma",
      "Yikang Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14087"
  },
  {
    "id": "arXiv:2205.14091",
    "title": "Measuring Equality and Hierarchical Mobility on Abstract Complex  Networks",
    "abstract": "The centrality of a node within a network, however it is measured, is a vital\nproxy for the importance or influence of that node, and the differences in node\ncentrality generate hierarchies and inequalities. If the network is evolving in\ntime, the influence of each node changes in time as well, and the corresponding\nhierarchies are modified accordingly. However, there is still a lack of\nsystematic study into the ways in which the centrality of a node evolves when a\ngraph changes. In this paper we introduce a taxonomy of metrics of equality and\nhierarchical mobility in networks that evolve in time. We propose an indicator\nof equality based on the classical Gini Coefficient from economics, and we\nquantify the hierarchical mobility of nodes, that is, how and to what extent\nthe centrality of a node and its neighbourhood change over time. These measures\nare applied to a corpus of thirty time evolving network data sets from\ndifferent domains. We show that the proposed taxonomy measures can discriminate\nbetween networks from different fields. We also investigate correlations\nbetween different taxonomy measures, and demonstrate that some of them have\nconsistently strong correlations (or anti-correlations) across the entire\ncorpus. The mobility and equality measures developed here constitute a useful\ntoolbox for investigating the nature of network evolution, and also for\ndiscriminating between different artificial models hypothesised to explain that\nevolution.",
    "descriptor": "",
    "authors": [
      "Matthew Russell Barnes",
      "Vincenzo Nicosia",
      "Richard G. Clegg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.14091"
  },
  {
    "id": "arXiv:2205.14092",
    "title": "Capturing Graphs with Hypo-Elliptic Diffusions",
    "abstract": "Convolutional layers within graph neural networks operate by aggregating\ninformation about local neighbourhood structures; one common way to encode such\nsubstructures is through random walks. The distribution of these random walks\nevolves according to a diffusion equation defined using the graph Laplacian. We\nextend this approach by leveraging classic mathematical results about\nhypo-elliptic diffusions. This results in a novel tensor-valued graph operator,\nwhich we call the hypo-elliptic graph Laplacian. We provide theoretical\nguarantees and efficient low-rank approximation algorithms. In particular, this\ngives a structured approach to capture long-range dependencies on graphs that\nis robust to pooling. Besides the attractive theoretical properties, our\nexperiments show that this method competes with graph transformers on datasets\nrequiring long-range reasoning but scales only linearly in the number of edges\nas opposed to quadratically in nodes.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Csaba Toth",
      "Darrick Lee",
      "Celia Hacker",
      "Harald Oberhauser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.14092"
  },
  {
    "id": "arXiv:2205.14094",
    "title": "Failure Detection in Medical Image Classification: A Reality Check and  Benchmarking Testbed",
    "abstract": "Failure detection in automated image classification is a critical safeguard\nfor clinical deployment. Detected failure cases can be referred to human\nassessment, ensuring patient safety in computer-aided clinical decision making.\nDespite its paramount importance, there is insufficient evidence about the\nability of state-of-the-art confidence scoring methods to detect test-time\nfailures of classification models in the context of medical imaging. This paper\nprovides a reality check, establishing the performance of in-domain\nmisclassification detection methods, benchmarking 9 confidence scores on 6\nmedical imaging datasets with different imaging modalities, in multiclass and\nbinary classification settings. Our experiments show that the problem of\nfailure detection is far from being solved. We found that none of the\nbenchmarked advanced methods proposed in the computer vision and machine\nlearning literature can consistently outperform a simple softmax baseline. Our\ndeveloped testbed facilitates future work in this important area.",
    "descriptor": "",
    "authors": [
      "Melanie Bernhardt",
      "Fabio De Sousa Ribeiro",
      "Ben Glocker"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14094"
  },
  {
    "id": "arXiv:2205.14098",
    "title": "Solving infinite-horizon POMDPs with memoryless stochastic policies in  state-action space",
    "abstract": "Reward optimization in fully observable Markov decision processes is\nequivalent to a linear program over the polytope of state-action frequencies.\nTaking a similar perspective in the case of partially observable Markov\ndecision processes with memoryless stochastic policies, the problem was\nrecently formulated as the optimization of a linear objective subject to\npolynomial constraints. Based on this we present an approach for Reward\nOptimization in State-Action space (ROSA). We test this approach experimentally\nin maze navigation tasks. We find that ROSA is computationally efficient and\ncan yield stability improvements over other existing methods.",
    "descriptor": "\nComments: Accepted as an extended abstract at RLDM 2022, 5 pages, 2 figures\n",
    "authors": [
      "Johannes M\u00fcller",
      "Guido Mont\u00fafar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14098"
  },
  {
    "id": "arXiv:2205.14099",
    "title": "BURG-Toolkit: Robot Grasping Experiments in Simulation and the Real  World",
    "abstract": "This paper presents BURG-Toolkit, a set of open-source tools for Benchmarking\nand Understanding Robotic Grasping. Our tools allow researchers to: (1) create\nvirtual scenes for generating training data and performing grasping in\nsimulation; (2) recreate the scene by arranging the corresponding objects\naccurately in the physical world for real robot experiments, supporting an\nanalysis of the sim-to-real gap; and (3) share the scenes with other\nresearchers to foster comparability and reproducibility of experimental\nresults. We explain how to use our tools by describing some potential use\ncases. We further provide proof-of-concept experimental results quantifying the\nsim-to-real gap for robot grasping in some example scenes. The tools are\navailable at: https://mrudorfer.github.io/burg-toolkit/",
    "descriptor": "\nComments: presented at ICRA 2022 Workshop on Releasing Robots into the Wild: Simulations, Benchmarks, and Deployment. Project page: this https URL\n",
    "authors": [
      "Martin Rudorfer",
      "Markus Suchi",
      "Mohan Sridharan",
      "Markus Vincze",
      "Ale\u0161 Leonardis"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.14099"
  },
  {
    "id": "arXiv:2205.14100",
    "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
    "abstract": "In this paper, we design and train a Generative Image-to-text Transformer,\nGIT, to unify vision-language tasks such as image/video captioning and question\nanswering. While generative models provide a consistent network architecture\nbetween pre-training and fine-tuning, existing work typically contains complex\nstructures (uni/multi-modal encoder/decoder) and depends on external modules\nsuch as object detectors/taggers and optical character recognition (OCR). In\nGIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data\nand the model size to boost the model performance. Without bells and whistles,\nour GIT establishes new state of the arts on 12 challenging benchmarks with a\nlarge margin. For instance, our model surpasses the human performance for the\nfirst time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a\nnew scheme of generation-based image classification and scene text recognition,\nachieving decent performance on standard benchmarks.",
    "descriptor": "",
    "authors": [
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Xiaowei Hu",
      "Linjie Li",
      "Kevin Lin",
      "Zhe Gan",
      "Zicheng Liu",
      "Ce Liu",
      "Lijuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14100"
  },
  {
    "id": "arXiv:2205.14101",
    "title": "Adaptive Massively Parallel Algorithms for Cut Problems",
    "abstract": "We study the Weighted Min Cut problem in the Adaptive Massively Parallel\nComputation (AMPC) model. In 2019, Behnezhad et al. [3] introduced the AMPC\nmodel as an extension of the Massively Parallel Computation (MPC) model. In the\npast decade, research on highly scalable algorithms has had significant impact\non many massive systems. The MPC model, introduced in 2010 by Karloff et al.\n[16], which is an abstraction of famous practical frameworks such as MapReduce,\nHadoop, Flume, and Spark, has been at the forefront of this research. While\ngreat strides have been taken to create highly efficient MPC algorithms for a\nrange of problems, recent progress has been limited by the 1-vs-2 Cycle\nConjecture [20], which postulates that the simple problem of distinguishing\nbetween one and two cycles requires $\\Omega(\\log n)$ MPC rounds. In the AMPC\nmodel, each machine has adaptive read access to a distributed hash table even\nwhen communication is restricted (i.e., in the middle of a round). While\nremaining practical [4], this gives algorithms the power to bypass limitations\nlike the 1-vs-2 Cycle Conjecture.\nWe give the first sublogarithmic AMPC algorithm, requiring $O(\\log\\log n)$\nrounds, for $(2+\\epsilon)$-approximate weighted Min Cut. Our algorithm is\ninspired by the divide and conquer approach of Ghaffari and Nowicki [11], which\nsolves the $(2+\\epsilon)$-approximate weighted Min Cut problem in $O(\\log\nn\\log\\log n)$ rounds of MPC using the classic result of Karger and Stein [15].\nOur work is fully-scalable in the sense that the local memory of each machine\nis $O(n^\\epsilon)$ for any constant $0 < \\epsilon < 1$. There are no $o(\\log\nn)$-round MPC algorithms for Min Cut in this memory regime assuming the 1-vs-2\nCycle Conjecture holds. The exponential speedup in AMPC is the result of\ndecoupling the different layers of the divide and conquer algorithm and solving\nall layers in $O(1)$ rounds.",
    "descriptor": "",
    "authors": [
      "MohammadTaghi Hajiaghayi",
      "Marina Knittel",
      "Jan Olkowski",
      "Hamed Saleh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14101"
  },
  {
    "id": "arXiv:2205.14102",
    "title": "Generalizing Brain Decoding Across Subjects with Deep Learning",
    "abstract": "Decoding experimental variables from brain imaging data is gaining\npopularity, with applications in brain-computer interfaces and the study of\nneural representations. Decoding is typically subject-specific and does not\ngeneralise well over subjects. Here, we investigate ways to achieve\ncross-subject decoding. We used magnetoencephalography (MEG) data where 15\nsubjects viewed 118 different images, with 30 examples per image. Training on\nthe entire 1s window following the presentation of each image, we experimented\nwith an adaptation of the WaveNet architecture for classification. We also\ninvestigated the use of subject embedding to aid learning of subject\nvariability in the group model. We show that deep learning and subject\nembedding are crucial to closing the performance gap between subject and\ngroup-level models. Importantly group models outperform subject models when\ntested on an unseen subject with little available data. The potential of such\ngroup modelling is even higher with bigger datasets. Furthermore, we\ndemonstrate the use of permutation feature importance to gain insight into the\nspatio-temporal and spectral information encoded in the models, enabling better\nphysiological interpretation. All experimental code is available at\nhttps://github.com/ricsinaruto/MEG-group-decode.",
    "descriptor": "\nComments: 17 pages, 11 figures\n",
    "authors": [
      "Richard Csaky",
      "Mats Van Es",
      "Oiwi Parker Jones",
      "Mark Woolrich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.14102"
  },
  {
    "id": "arXiv:2205.14104",
    "title": "Efficient Forecasting of Large Scale Hierarchical Time Series via  Multilevel Clustering",
    "abstract": "We propose a novel approach to the problem of clustering hierarchically\naggregated time-series data, which has remained an understudied problem though\nit has several commercial applications. We first group time series at each\naggregated level, while simultaneously leveraging local and global information.\nThe proposed method can cluster hierarchical time series (HTS) with different\nlengths and structures. For common two-level hierarchies, we employ a combined\nobjective for local and global clustering over spaces of discrete probability\nmeasures, using Wasserstein distance coupled with Soft-DTW divergence. For\nmulti-level hierarchies, we present a bottom-up procedure that progressively\nleverages lower-level information for higher-level clustering. Our final goal\nis to improve both the accuracy and speed of forecasts for a larger number of\nHTS needed for a real-world application. To attain this goal, each time series\nis first assigned the forecast for its cluster representative, which can be\nconsidered as a \"shrinkage prior\" for the set of time series it represents.\nThen this base forecast can be quickly fine-tuned to adjust to the specifics of\nthat time series. We empirically show that our method substantially improves\nperformance in terms of both speed and accuracy for large-scale forecasting\ntasks involving much HTS.",
    "descriptor": "\nComments: 17 pages, 3 figures, 4 tables\n",
    "authors": [
      "Xing Han",
      "Tongzheng Ren",
      "Jing Hu",
      "Joydeep Ghosh",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14104"
  },
  {
    "id": "arXiv:2205.14105",
    "title": "Learning to Solve Combinatorial Graph Partitioning Problems via  Efficient Exploration",
    "abstract": "From logistics to the natural sciences, combinatorial optimisation on graphs\nunderpins numerous real-world applications. Reinforcement learning (RL) has\nshown particular promise in this setting as it can adapt to specific problem\nstructures and does not require pre-solved instances for these, often NP-hard,\nproblems. However, state-of-the-art (SOTA) approaches typically suffer from\nsevere scalability issues, primarily due to their reliance on expensive graph\nneural networks (GNNs) at each decision step. We introduce ECORD; a novel RL\nalgorithm that alleviates this expense by restricting the GNN to a single\npre-processing step, before entering a fast-acting exploratory phase directed\nby a recurrent unit. Experimentally, ECORD achieves a new SOTA for RL\nalgorithms on the Maximum Cut problem, whilst also providing orders of\nmagnitude improvement in speed and scalability. Compared to the nearest\ncompetitor, ECORD reduces the optimality gap by up to 73% on 500 vertex graphs\nwith a decreased wall-clock time. Moreover, ECORD retains strong performance\nwhen generalising to larger graphs with up to 10000 vertices.",
    "descriptor": "",
    "authors": [
      "Thomas D. Barrett",
      "Christopher W.F. Parsonson",
      "Alexandre Laterre"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14105"
  },
  {
    "id": "arXiv:2205.14106",
    "title": "Service Composition in Opportunistic Networks: A Load and Mobility Aware  Solution",
    "abstract": "Pervasive networks formed by users' mobile devices have the potential to\nexploit a rich set of distributed service components that can be composed to\nprovide each user with a multitude of application level services. However, in\nmany challenging scenarios, opportunistic networking techniques are required to\nenable communication as devices suffer from intermittent connectivity,\ndisconnections and partitions. This poses novel challenges to service\ncomposition techniques. While several works have discussed middleware and\narchitectures for service composition in well-connected wired networks and in\nstable MANET environments, the underlying mechanism for selecting and\nforwarding service requests in the significantly challenging networking\nenvironment of opportunistic networks has not been entirely addressed. The\nproblem comprises three stages: i) selecting an appropriate service sequence\nset out of available services to obtain the required application level service;\nii) routing results of a previous stage in the composition to the next one\nthrough a multi-hop opportunistic path; and iii) routing final service outcomes\nback to the requester. The proposed algorithm derives efficiency and\neffectiveness by taking into account the estimated load at service providers\nand expected time to opportunistically route information between devices. Based\non this information the algorithm estimates the best composition to obtain a\nrequired service. It is shown that using only local knowledge collected in a\ndistributed manner, performance close to a real-time centralized system can be\nachieved. Applicability and performance guarantee of the service composition\nalgorithm in a range of mobility characteristics are established through\nextensive simulations on real/synthetic traces.",
    "descriptor": "",
    "authors": [
      "Umair Sadiq",
      "Mohan Kumar",
      "Andrea Passarella",
      "Marco Conti"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14106"
  },
  {
    "id": "arXiv:2205.14107",
    "title": "Spartan: Differentiable Sparsity via Regularized Transportation",
    "abstract": "We present Spartan, a method for training sparse neural network models with a\npredetermined level of sparsity. Spartan is based on a combination of two\ntechniques: (1) soft top-k masking of low-magnitude parameters via a\nregularized optimal transportation problem and (2) dual averaging-based\nparameter updates with hard sparsification in the forward pass. This scheme\nrealizes an exploration-exploitation tradeoff: early in training, the learner\nis able to explore various sparsity patterns, and as the soft top-k\napproximation is gradually sharpened over the course of training, the balance\nshifts towards parameter optimization with respect to a fixed sparsity mask.\nSpartan is sufficiently flexible to accommodate a variety of sparsity\nallocation policies, including both unstructured and block structured sparsity,\nas well as general cost-sensitive sparsity allocation mediated by linear models\nof per-parameter costs. On ImageNet-1K classification, Spartan yields 95%\nsparse ResNet-50 models and 90% block sparse ViT-B/16 models while incurring\nabsolute top-1 accuracy losses of less than 1% compared to fully dense\ntraining.",
    "descriptor": "",
    "authors": [
      "Kai Sheng Tai",
      "Taipeng Tian",
      "Ser-Nam Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14107"
  },
  {
    "id": "arXiv:2205.14108",
    "title": "Scalable Interpretability via Polynomials",
    "abstract": "Generalized Additive Models (GAMs) have quickly become the leading choice for\nfully-interpretable machine learning. However, unlike uninterpretable methods\nsuch as DNNs, they lack expressive power and easy scalability, and are hence\nnot a feasible alternative for real-world tasks. We present a new class of GAMs\nthat use tensor rank decompositions of polynomials to learn powerful,\n$\\textit{fully-interpretable}$ models. Our approach, titled Scalable Polynomial\nAdditive Models (SPAM) is effortlessly scalable and models $\\textit{all}$\nhigher-order feature interactions without a combinatorial parameter explosion.\nSPAM outperforms all current interpretable approaches, and matches DNN/XGBoost\nperformance on a series of real-world benchmarks with up to hundreds of\nthousands of features. We demonstrate by human subject evaluations that SPAMs\nare demonstrably more interpretable in practice, and are hence an effortless\nreplacement for DNNs for creating interpretable and high-performance systems\nsuitable for large-scale machine learning.",
    "descriptor": "\nComments: 26 pages including appendix\n",
    "authors": [
      "Abhimanyu Dubey",
      "Filip Radenovic",
      "Dhruv Mahajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14108"
  },
  {
    "id": "arXiv:2205.14109",
    "title": "Bayesian Robust Graph Contrastive Learning",
    "abstract": "Graph Neural Networks (GNNs) have been widely used to learn node\nrepresentations and with outstanding performance on various tasks such as node\nclassification. However, noise, which inevitably exists in real-world graph\ndata, would considerably degrade the performance of GNNs as the noise is easily\npropagated via the graph structure. In this work, we propose a novel and robust\nmethod, Bayesian Robust Graph Contrastive Learning (BRGCL), which trains a GNN\nencoder to learn robust node representations. The BRGCL encoder is a completely\nunsupervised encoder. Two steps are iteratively executed at each epoch of\ntraining the BRGCL encoder: (1) estimating confident nodes and computing robust\ncluster prototypes of node representations through a novel Bayesian\nnonparametric method; (2) prototypical contrastive learning between the node\nrepresentations and the robust cluster prototypes. Experiments on public and\nlarge-scale benchmarks demonstrate the superior performance of BRGCL and the\nrobustness of the learned node representations. The code of BRGCL is available\nat \\url{https://github.com/BRGCL-code/BRGCL-code}.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2106.04714 by other authors\n",
    "authors": [
      "Yancheng Wang",
      "Yingzhen Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14109"
  },
  {
    "id": "arXiv:2205.14110",
    "title": "Service Provisioning in Mobile Environments through Opportunistic  Computing",
    "abstract": "Opportunistic computing is a paradigm for completely self-organised pervasive\nnetworks. Instead of relying only on fixed infrastructures as the cloud, users'\ndevices act as service providers for each other. They use pairwise contacts to\ncollect information about services provided and amount of time to provide them\nby the encountered nodes. At each node, upon generation of a service request,\nthis information is used to choose the most efficient service, or composition\nof services, that satisfy that request, based on local knowledge. Opportunistic\ncomputing can be exploited in several scenarios, including mobile social\nnetworks, IoT and Internet 4.0. In this paper we propose an opportunistic\ncomputing algorithm based on an analytical model, which ranks the available\n(composition of) services, based on their expected completion time. Through the\nmodel, a service requesters picks the one that is expected to be the best.\nExperiments show that the algorithm is accurate in ranking services, thus\nproviding an effective service-selection policy. Such a policy achieves\nsignificantly lower service provisioning times compared to other reference\npolicies. Its performance is tested in a wide range of scenarios varying the\nnodes mobility, the size of input/output parameters, the level of resource\ncongestion, the computational complexity of service executions.",
    "descriptor": "",
    "authors": [
      "Davide Mascitti",
      "Marco Conti",
      "Andrea Passarella",
      "Laura Ricci",
      "Sajal K. Das"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.14110"
  },
  {
    "id": "arXiv:2205.14111",
    "title": "Optimal polynomial meshes exist on any multivariate convex domain",
    "abstract": "We show that optimal polynomial meshes exist for every convex body in\n$\\mathbb{R}^d$, confirming a conjecture by A. Kroo.",
    "descriptor": "",
    "authors": [
      "Feng Dai",
      "Andriy Prymak"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Analysis and ODEs (math.CA)"
    ],
    "url": "https://arxiv.org/abs/2205.14111"
  },
  {
    "id": "arXiv:2205.14112",
    "title": "Improving Road Segmentation in Challenging Domains Using Similar Place  Priors",
    "abstract": "Road segmentation in challenging domains, such as night, snow or rain, is a\ndifficult task. Most current approaches boost performance using fine-tuning,\ndomain adaptation, style transfer, or by referencing previously acquired\nimagery. These approaches share one or more of three significant limitations: a\nreliance on large amounts of annotated training data that can be costly to\nobtain, both anticipation of and training data from the type of environmental\nconditions expected at inference time, and/or imagery captured from a previous\nvisit to the location. In this research, we remove these restrictions by\nimproving road segmentation based on similar places. We use Visual Place\nRecognition (VPR) to find similar but geographically distinct places, and fuse\nsegmentations for query images and these similar place priors using a Bayesian\napproach and novel segmentation quality metric. Ablation studies show the need\nto re-evaluate notions of VPR utility for this task. We demonstrate the system\nachieving state-of-the-art road segmentation performance across multiple\nchallenging condition scenarios including night time and snow, without\nrequiring any prior training or previous access to the same geographical\nlocations. Furthermore, we show that this method is network agnostic, improves\nmultiple baseline techniques and is competitive against methods specialised for\nroad prediction.",
    "descriptor": "\nComments: Accepted into IEEE Robotics and Automation Letters (RA-L) and presented at IEEE International Conference on Robotics and Automation (ICRA 2022)\n",
    "authors": [
      "Connor Malone",
      "Sourav Garg",
      "Ming Xu",
      "Thierry Peynot",
      "Michael Milford"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14112"
  },
  {
    "id": "arXiv:2205.14116",
    "title": "Robust Counterfactual Explanations for Random Forests",
    "abstract": "Counterfactual explanations describe how to modify a feature vector in order\nto flip the outcome of a trained classifier. Several heuristic and optimal\nmethods have been proposed to generate these explanations. However, the\nrobustness of counterfactual explanations when the classifier is re-trained has\nyet to be studied. Our goal is to obtain counterfactual explanations for random\nforests that are robust to algorithmic uncertainty. We study the link between\nthe robustness of ensemble models and the robustness of base learners and frame\nthe generation of robust counterfactual explanations as a chance-constrained\noptimization problem. We develop a practical method with good empirical\nperformance and provide finite-sample and asymptotic guarantees for simple\nrandom forests of stumps. We show that existing methods give surprisingly low\nrobustness: the validity of naive counterfactuals is below $50\\%$ on most data\nsets and can fall to $20\\%$ on large problem instances with many features. Even\nwith high plausibility, counterfactual explanations often exhibit low\nrobustness to algorithmic uncertainty. In contrast, our method achieves high\nrobustness with only a small increase in the distance from counterfactual\nexplanations to their initial observations. Furthermore, we highlight the\nconnection between the robustness of counterfactual explanations and the\npredictive importance of features.",
    "descriptor": "",
    "authors": [
      "Alexandre Forel",
      "Axel Parmentier",
      "Thibaut Vidal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.14116"
  },
  {
    "id": "arXiv:2205.14118",
    "title": "Efficient textual explanations for complex road and traffic scenarios  based on semantic segmentation",
    "abstract": "The complex driving environment brings great challenges to the visual\nperception of autonomous vehicles. The accuracy of visual perception drops off\nsharply under diverse weather conditions and uncertain traffic flow. Black box\nmodel makes it difficult to interpret the mechanisms of visual perception. To\nenhance the user acceptance and reliability of the visual perception system, a\ntextual explanation of the scene evolvement is essential. It analyzes the\ngeometry and topology structure in the complex environment and offers clues to\ndecision and control. However, the existing scene explanation has been\nimplemented as a separate model. It cannot detect comprehensive textual\ninformation and requires a high computational load and time consumption. Thus,\nthis study proposed a comprehensive and efficient textual explanation model for\ncomplex road and traffic scenarios. From 336k video frames of the driving\nenvironment, critical images of complex road and traffic scenarios were\nselected into a dataset. Through transfer learning, this study established an\naccurate and efficient segmentation model to gain semantic information. Based\non the XGBoost algorithm, a comprehensive model was developed. The model\nobtained textual information including road types, the motion of conflict\nobjects, and scenario complexity. The approach was verified on the real-world\nroad. It improved the perception accuracy of critical traffic elements to\n78.8%. The time consumption reached 13 minutes for each epoch, which was 11.5\ntimes more efficient compared with the pre-trained network. The textual\ninformation analyzed from the model was also accordant with reality. The\nfindings explain how autonomous vehicle detects the driving environment, which\nlays a foundation for subsequent decision and control. It can improve the\nperception ability by enriching the prior knowledge and judgments for complex\ntraffic situations.",
    "descriptor": "",
    "authors": [
      "Yiyue Zhao",
      "Xinyu Yun",
      "Chen Chai",
      "Zhiyu Liu",
      "Wenxuan Fan",
      "Xiao Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14118"
  },
  {
    "id": "arXiv:2205.14120",
    "title": "Neural Basis Models for Interpretability",
    "abstract": "Due to the widespread use of complex machine learning models in real-world\napplications, it is becoming critical to explain model predictions. However,\nthese models are typically black-box deep neural networks, explained post-hoc\nvia methods with known faithfulness limitations. Generalized Additive Models\n(GAMs) are an inherently interpretable class of models that address this\nlimitation by learning a non-linear shape function for each feature separately,\nfollowed by a linear model on top. However, these models are typically\ndifficult to train, require numerous parameters, and are difficult to scale.\nWe propose an entirely new subfamily of GAMs that utilizes basis\ndecomposition of shape functions. A small number of basis functions are shared\namong all features, and are learned jointly for a given task, thus making our\nmodel scale much better to large-scale data with high-dimensional features,\nespecially when features are sparse. We propose an architecture denoted as the\nNeural Basis Model (NBM) which uses a single neural network to learn these\nbases. On a variety of tabular and image datasets, we demonstrate that for\ninterpretable machine learning, NBMs are the state-of-the-art in accuracy,\nmodel size, and, throughput and can easily model all higher-order feature\ninteractions.",
    "descriptor": "\nComments: 17 pages including appendix\n",
    "authors": [
      "Filip Radenovic",
      "Abhimanyu Dubey",
      "Dhruv Mahajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14120"
  },
  {
    "id": "arXiv:2205.14122",
    "title": "Writes Hurt: Lessons in Cache Design for Optane NVRAM",
    "abstract": "Intel OptaneTM DC Persistent Memory resides on the memory bus and approaches\nDRAM in access latency. One avenue for its adoption is to employ it in place of\npersistent storage; another is to use it as a cheaper and denser extension of\nDRAM. In pursuit of the latter goal, we present the design of a volatile Optane\nNVRAM cache as a component in a storage engine underlying MongoDB. The primary\ninnovation in our design is a new cache admission policy. We discover that on\nOptane NVRAM, known for its limited write throughput, the presence of writes\ndisproportionately affects the throughput of reads, much more so than on DRAM.\nTherefore, an admission policy that indiscriminately admits new data (and thus\ngenerates writes), severely limits the rate of data retrieval and results in\nexceedingly poor performance for the cache overall. We design an admission\npolicy that balances the rate of admission with the rate of lookups using\ndynamically observed characteristics of the workload. Our implementation\noutperforms OpenCAS (an off-the-shelf Optane-based block cache) in all cases,\nand Intel Memory Mode in cases where the database size exceeds the available\nNVRAM. Our cache is decoupled from the rest of the storage engine and uses\ngeneric metrics to guide its admission policy; therefore our design can be\neasily adopted in other systems.",
    "descriptor": "",
    "authors": [
      "Alexandra Fedorova",
      "Keith Smith",
      "Keith Bostic",
      "Alexander Gorrod",
      "Sue LoVerso",
      "Michael Cahill"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.14122"
  },
  {
    "id": "arXiv:2205.14125",
    "title": "Cycle Mutation: Evolving Permutations via Cycle Induction",
    "abstract": "Evolutionary algorithms solve problems by simulating the evolution of a\npopulation of candidate solutions. We focus on evolving permutations for\nordering problems like the traveling salesperson problem (TSP), as well as\nassignment problems like the quadratic assignment problem (QAP) and largest\ncommon subgraph (LCS). We propose cycle mutation, a new mutation operator whose\ninspiration is the well known cycle crossover operator, and the concept of a\npermutation cycle. We use fitness landscape analysis to explore the problem\ncharacteristics for which cycle mutation works best. As a prerequisite, we\ndevelop new permutation distance measures: cycle distance, $k$-cycle distance,\nand cycle edit distance. The fitness landscape analysis predicts that cycle\nmutation is better suited for assignment and mapping problems than it is for\nordering problems. We experimentally validate these findings showing cycle\nmutation's strengths on problems like QAP and LCS, and its limitations on\nproblems like the TSP, while also showing that it is less prone to local optima\nthan commonly used alternatives. We integrate cycle mutation into the\nopen-source Chips-n-Salsa library, and the new distance metrics into the\nopen-source JavaPermutationTools library.",
    "descriptor": "\nComments: Accepted to: Applied Sciences, special issue on \"Evolutionary Computation: Theories, Techniques, and Applications\"\n",
    "authors": [
      "Vincent A. Cicirello"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14125"
  },
  {
    "id": "arXiv:2205.14127",
    "title": "A family of immersed finite element spaces and applications to three  dimensional $\\mathbf{H}(\\text{curl})$ interface problems",
    "abstract": "Maxwell interface problems are of great importance in many electromagnetic\napplications. Unfitted mesh methods are especially attractive in 3D computation\nas they can circumvent generating complex 3D interface-fitted meshes. However,\nmany unfitted mesh methods rely on non-conforming approximation spaces, which\nmay cause a loss of accuracy for solving Maxwell equations, and the widely-used\npenalty techniques in the literature may not help in recovering the optimal\nconvergence. In this article, we provide a remedy by developing\nN\\'ed\\'elec-type immersed finite element spaces with a Petrov-Galerkin scheme\nthat is able to produce optimal-convergent solutions. To establish a systematic\nframework, we analyze all the $H^1$, $\\mathbf{H}(\\text{curl})$ and\n$\\mathbf{H}(\\text{div})$ IFE spaces and form a discrete de Rham complex. Based\non these fundamental results, we further develop a fast solver using a modified\nHiptmair-Xu preconditioner which works for both the GMRES and CG methods.",
    "descriptor": "",
    "authors": [
      "Long Chen",
      "Ruchi Guo",
      "Jun Zou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14127"
  },
  {
    "id": "arXiv:2205.14128",
    "title": "Meta-Learning Adversarial Bandits",
    "abstract": "We study online learning with bandit feedback across multiple tasks, with the\ngoal of improving average performance across tasks if they are similar\naccording to some natural task-similarity measure. As the first to target the\nadversarial setting, we design a unified meta-algorithm that yields\nsetting-specific guarantees for two important cases: multi-armed bandits (MAB)\nand bandit linear optimization (BLO). For MAB, the meta-algorithm tunes the\ninitialization, step-size, and entropy parameter of the Tsallis-entropy\ngeneralization of the well-known Exp3 method, with the task-averaged regret\nprovably improving if the entropy of the distribution over estimated\noptima-in-hindsight is small. For BLO, we learn the initialization, step-size,\nand boundary-offset of online mirror descent (OMD) with self-concordant barrier\nregularizers, showing that task-averaged regret varies directly with a measure\ninduced by these functions on the interior of the action space. Our adaptive\nguarantees rely on proving that unregularized follow-the-leader combined with\nmultiplicative weights is enough to online learn a non-smooth and non-convex\nsequence of affine functions of Bregman divergences that upper-bound the regret\nof OMD.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Maria-Florina Balcan",
      "Keegan Harris",
      "Mikhail Khodak",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14128"
  },
  {
    "id": "arXiv:2205.14135",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with  IO-Awareness",
    "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and\nmemory complexity of self-attention are quadratic in sequence length.\nApproximate attention methods have attempted to address this problem by trading\noff model quality to reduce the compute complexity, but often do not achieve\nwall-clock speedup. We argue that a missing principle is making attention\nalgorithms IO-aware -- accounting for reads and writes between levels of GPU\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\nuses tiling to reduce the number of memory reads/writes between GPU high\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\nFlashAttention, showing that it requires fewer HBM accesses than standard\nattention, and is optimal for a range of SRAM sizes. We also extend\nFlashAttention to block-sparse attention, yielding an approximate attention\nalgorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15%\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\nMLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K),\nand 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\nlong-document classification) and entirely new capabilities: the first\nTransformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\naccuracy).",
    "descriptor": "",
    "authors": [
      "Tri Dao",
      "Daniel Y. Fu",
      "Stefano Ermon",
      "Atri Rudra",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14135"
  },
  {
    "id": "arXiv:2205.14136",
    "title": "PSL is Dead. Long Live PSL",
    "abstract": "Property Specification Language (PSL) is a form of temporal logic that has\nbeen mainly used in discrete domains (e.g. formal hardware verification). In\nthis paper, we show that by merging machine learning techniques with PSL\nmonitors, we can extend PSL to work on continuous domains. We apply this\ntechnique in machine learning-based anomaly detection to analyze scenarios of\nreal-time streaming events from continuous variables in order to detect\nabnormal behaviors of a system. By using machine learning with formal models,\nwe leverage the strengths of both machine learning methods and formal semantics\nof time. On one hand, machine learning techniques can produce distributions on\ncontinuous variables, where abnormalities can be captured as deviations from\nthe distributions. On the other hand, formal methods can characterize discrete\ntemporal behaviors and relations that cannot be easily learned by machine\nlearning techniques. Interestingly, the anomalies detected by machine learning\nand the underlying time representation used are discrete events. We implemented\na temporal monitoring package (TEF) that operates in conjunction with normal\ndata science packages for anomaly detection machine learning systems, and we\nshow that TEF can be used to perform accurate interpretation of temporal\ncorrelation between events.",
    "descriptor": "\nComments: 7 pages, 16 figures\n",
    "authors": [
      "Kevin Smith",
      "Hai Lin",
      "Praveen Tiwari",
      "Marjorie Sayer",
      "Claudionor Coelho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.14136"
  },
  {
    "id": "arXiv:2205.14137",
    "title": "Digital Sovereignty and Software Engineering for the IoT-laden,  AI/ML-driven Era",
    "abstract": "Today's software engineering already needs to deal with challenges\noriginating from the multidisciplinarity that is required to realize IoT\nproducts: Many variants consist of sensor/actuator-powered systems that already\ntoday use AI/ML systems to better cope with the unstructuredness of their\nintended operational design domain (ODD), while, at the same time, such systems\nneed to be monitored, diagnosed, maintained, and evolved using cloud-powered\ndashboards and data analytics pipelines that process, aggregate, and analyze\ncountless data points preferably in real-time. This position paper discusses\nselected aspects related to Digital Sovereignty from a software engineering's\nperspective for the IoT-laden, AI/ML-driven era: While we can undeniably expect\nmore and more benefits from such solutions, a specific light shall be shed in\nparticular on challenges and responsibilities at design- and operation-time\nthat, at minimum, prepare for and enable or, even better, preserve and extend\ndigital sovereignty from a software engineering's perspective.",
    "descriptor": "",
    "authors": [
      "Christian Berger"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.14137"
  },
  {
    "id": "arXiv:2205.14139",
    "title": "Learning Markovian Homogenized Models in Viscoelasticity",
    "abstract": "Fully resolving dynamics of materials with rapidly-varying features involves\nexpensive fine-scale computations which need to be conducted on macroscopic\nscales. The theory of homogenization provides an approach to derive effective\nmacroscopic equations which eliminates the small scales by exploiting scale\nseparation. An accurate homogenized model avoids the computationally-expensive\ntask of numerically solving the underlying balance laws at a fine scale,\nthereby rendering a numerical solution of the balance laws more computationally\ntractable.\nIn complex settings, homogenization only defines the constitutive model\nimplicitly, and machine learning can be used to learn the constitutive model\nexplicitly from localized fine-scale simulations. In the case of\none-dimensional viscoelasticity, the linearity of the model allows for a\ncomplete analysis. We establish that the homogenized constitutive model may be\napproximated by a recurrent neural network (RNN) that captures the memory. The\nmemory is encapsulated in the evolution of an appropriate finite set of\ninternal variables, discovered through the learning process and dependent on\nthe history of the strain. Simulations are presented which validate the theory.\nGuidance for the learning of more complex models, such as arise in plasticity,\nby similar techniques, is given.",
    "descriptor": "",
    "authors": [
      "Kaushik Bhattacharya",
      "Burigede Liu",
      "Andrew Stuart",
      "Margaret Trautner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.14139"
  },
  {
    "id": "arXiv:2205.14140",
    "title": "CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model  Behavior",
    "abstract": "The increasing size and complexity of modern ML systems has improved their\npredictive capabilities but made their behavior harder to explain. Many\ntechniques for model explanation have been developed in response, but we lack\nclear criteria for assessing these techniques. In this paper, we cast model\nexplanation as the causal inference problem of estimating causal effects of\nreal-world concepts on the output behavior of ML models given actual input\ndata. We introduce CEBaB, a new benchmark dataset for assessing concept-based\nexplanation methods in Natural Language Processing (NLP). CEBaB consists of\nshort restaurant reviews with human-generated counterfactual reviews in which\nan aspect (food, noise, ambiance, service) of the dining experience was\nmodified. Original and counterfactual reviews are annotated with\nmultiply-validated sentiment ratings at the aspect-level and review-level. The\nrich structure of CEBaB allows us to go beyond input features to study the\neffects of abstract, real-world concepts on model behavior. We use CEBaB to\ncompare the quality of a range of concept-based explanation methods covering\ndifferent assumptions and conceptions of the problem, and we seek to establish\nnatural metrics for comparative assessments of these methods.",
    "descriptor": "",
    "authors": [
      "Eldar David Abraham",
      "Karel D'Oosterlinck",
      "Amir Feder",
      "Yair Ori Gat",
      "Atticus Geiger",
      "Christopher Potts",
      "Roi Reichart",
      "Zhengxuan Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14140"
  },
  {
    "id": "arXiv:2205.14141",
    "title": "Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via  Feature Distillation",
    "abstract": "Masked image modeling (MIM) learns representations with remarkably good\nfine-tuning performances, overshadowing previous prevalent pre-training\napproaches such as image classification, instance contrastive learning, and\nimage-text alignment. In this paper, we show that the inferior fine-tuning\nperformance of these pre-training approaches can be significantly improved by a\nsimple post-processing in the form of feature distillation (FD). The feature\ndistillation converts the old representations to new representations that have\na few desirable properties just like those representations produced by MIM.\nThese properties, which we aggregately refer to as optimization friendliness,\nare identified and analyzed by a set of attention- and optimization-related\ndiagnosis tools. With these properties, the new representations show strong\nfine-tuning performance. Specifically, the contrastive self-supervised learning\nmethods are made as competitive in fine-tuning as the state-of-the-art masked\nimage modeling (MIM) algorithms. The CLIP models' fine-tuning performance is\nalso significantly improved, with a CLIP ViT-L model reaching 89.0% top-1\naccuracy on ImageNet-1K classification. More importantly, our work provides a\nway for the future research to focus more effort on the generality and\nscalability of the learnt representations without being pre-occupied with\noptimization friendliness since it can be enhanced rather easily. The code will\nbe available at https://github.com/SwinTransformer/Feature-Distillation.",
    "descriptor": "",
    "authors": [
      "Yixuan Wei",
      "Han Hu",
      "Zhenda Xie",
      "Zheng Zhang",
      "Yue Cao",
      "Jianmin Bao",
      "Dong Chen",
      "Baining Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14141"
  },
  {
    "id": "arXiv:2205.13544",
    "title": "Effective drug combination for Caenorhabditis elegans nematodes  discovered by output-driven feedback system control technique",
    "abstract": "Infections from parasitic nematodes (or roundworms) contribute to a\nsignificant disease burden and productivity losses for humans and livestock.\nThe limited number of anthelmintics (or antinematode drugs) available today to\ntreat these infections are rapidly losing their efficacy as multidrug\nresistance in parasites becomes a global health challenge. We propose an\nengineering approach to discover an anthelmintic drug combination that is more\npotent at killing wild-type Caenorhabditis elegans worms than four individual\ndrugs. In the experiment, freely swimming single worms are enclosed in\nmicrofluidic drug environments to assess the centroid velocity and track\ncurvature of worm movements. After analyzing the behavioral data in every\niteration, the feedback system control (FSC) scheme is used to predict new drug\ncombinations to test. Through a differential evolutionary search, the winning\ndrug combination is reached that produces minimal centroid velocity and high\ntrack curvature, while requiring each drug in less than their EC50\nconcentrations. The FSC approach is model-less and does not need any\ninformation on the drug pharmacology, signaling pathways, or animal biology.\nToward combating multidrug resistance, the method presented here is applicable\nto the discovery of new potent combinations of available anthelmintics on C.\nelegans, parasitic nematodes, and other small model organisms.",
    "descriptor": "",
    "authors": [
      "Xianting Ding",
      "Zach Njus",
      "Taejoon Kong",
      "Wenqiong Su",
      "Chih-Ming Ho",
      "Santosh Pandey"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Image and Video Processing (eess.IV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13544"
  },
  {
    "id": "arXiv:2205.13545",
    "title": "Learning black- and gray-box chemotactic PDEs/closures from agent based  Monte Carlo simulation data",
    "abstract": "We propose a machine learning framework for the data-driven discovery of\nmacroscopic chemotactic Partial Differential Equations (PDEs) -- and the\nclosures that lead to them -- from high-fidelity, individual-based stochastic\nsimulations of E.coli bacterial motility. The fine scale, detailed, hybrid\n(continuum - Monte Carlo) simulation model embodies the underlying biophysics,\nand its parameters are informed from experimental observations of individual\ncells. We exploit Automatic Relevance Determination (ARD) within a Gaussian\nProcess framework for the identification of a parsimonious set of collective\nobservables that parametrize the law of the effective PDEs. Using these\nobservables, in a second step we learn effective, coarse-grained \"Keller-Segel\nclass\" chemotactic PDEs using machine learning regressors: (a) (shallow)\nfeedforward neural networks and (b) Gaussian Processes. The learned laws can be\nblack-box (when no prior knowledge about the PDE law structure is assumed) or\ngray-box when parts of the equation (e.g. the pure diffusion part) is known and\n\"hardwired\" in the regression process. We also discuss data-driven corrections\n(both additive and functional) of analytically known, approximate closures.",
    "descriptor": "\nComments: 33 pages, 5 figures, 1 table\n",
    "authors": [
      "Seungjoon Lee",
      "Yorgos M. Psarellis",
      "Constantinos I. Siettos",
      "Ioannis G. Kevrekidis"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13545"
  },
  {
    "id": "arXiv:2205.13596",
    "title": "A Simplified Treatment of Ramana's Exact Dual for Semidefinite  Programming",
    "abstract": "In semidefinite programming the dual may fail to attain its optimal value and\nthere could be a duality gap, i.e., the primal and dual optimal values may\ndiffer. In a striking paper, Ramana proposed a polynomial size extended dual\nthat does not have these deficiencies and yields a number of fundamental\nresults in complexity theory. In this work we walk the reader through a concise\nand self-contained derivation of Ramana's dual, relying mostly on elementary\nlinear algebra.",
    "descriptor": "\nComments: To appear, Optimization Letters\n",
    "authors": [
      "Bruno F. Louren\u00e7o",
      "G\u00e1bor Pataki"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.13596"
  },
  {
    "id": "arXiv:2205.13601",
    "title": "Exploring General Ap\u00e9ry Limits via the Zudilin-Straub t-transform",
    "abstract": "Inspired by a recent beautiful construction of Armin Straub and Wadim\nZudilin, that 'tweaked' the sum of the $s^{th}$ powers of the $n$-th row of\nPascal's triangle, getting instead of sequences of numbers, sequences of\nrational functions, we do the same for general binomial coefficients sums,\ngetting a practically unlimited supply of Ap\\'ery limits. While getting what we\ncall \"major Ap\\'ery miracles\", proving irrationality of the associated\nconstants (i.e. the so-called Ap\\'ery limits) is very rare, we do get, every\ntime, at least a \"minor Ap\\'ery miracle\" where an explicit constant, defined as\nan (extremely slowly-converging) limit of some explicit sequence, is expressed\nas an Ap\\'ery limit of some recurrence, with some initial conditions, thus\nenabling a very fast computation of that constant, with exponentially decaying\nerror.",
    "descriptor": "\nComments: 10 pgs\n",
    "authors": [
      "Robert Dougherty-Bliss",
      "Doron Zeilberger"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13601"
  },
  {
    "id": "arXiv:2205.13602",
    "title": "Consistent and fast inference in compartmental models of epidemics using  Poisson Approximate Likelihoods",
    "abstract": "Addressing the challenge of scaling-up epidemiological inference to complex\nand heterogeneous models, we introduce Poisson Approximate Likelihood (PAL)\nmethods. In contrast to the popular ODE approach to compartmental modelling, in\nwhich a large population limit is used to motivate a deterministic model, PALs\nare derived from approximate filtering equations for finite-population,\nstochastic compartmental models, and the large population limit drives the\nconsistency of maximum PAL estimators. Our theoretical results appear to be the\nfirst likelihood-based parameter estimation consistency results applicable\nacross a broad class of partially observed stochastic compartmental models.\nCompared to simulation-based methods such as Approximate Bayesian Computation\nand Sequential Monte Carlo, PALs are simple to implement, involving only\nelementary arithmetic operations and no tuning parameters; and fast to\nevaluate, requiring no simulation from the model and having computational cost\nindependent of population size. Through examples, we demonstrate how PALs can\nbe: embedded within Delayed Acceptance Particle Markov Chain Monte Carlo to\nfacilitate Bayesian inference; used to fit an age-structured model of\ninfluenza, taking advantage of automatic differentiation in Stan; and applied\nto calibrate a spatial meta-population model of measles.",
    "descriptor": "",
    "authors": [
      "Michael Whitehouse",
      "Nick Whiteley",
      "Lorenzo Rimella"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13602"
  },
  {
    "id": "arXiv:2205.13614",
    "title": "Emergent organization of receptive fields in networks of excitatory and  inhibitory neurons",
    "abstract": "Local patterns of excitation and inhibition that can generate neural waves\nare studied as a computational mechanism underlying the organization of\nneuronal tunings. Sparse coding algorithms based on networks of excitatory and\ninhibitory neurons are proposed that exhibit topographic maps as the receptive\nfields are adapted to input stimuli. Motivated by a leaky integrate-and-fire\nmodel of neural waves, we propose an activation model that is more typical of\nartificial neural networks. Computational experiments with the activation model\nusing both natural images and natural language text are presented. In the case\nof images, familiar \"pinwheel\" patterns of oriented edge detectors emerge; in\nthe case of text, the resulting topographic maps exhibit a 2-dimensional\nrepresentation of granular word semantics. Experiments with a synthetic model\nof somatosensory input are used to investigate how the network dynamics may\naffect plasticity of neuronal maps under changes to the inputs.",
    "descriptor": "",
    "authors": [
      "Leon Lufkin",
      "Ashish Puri",
      "Ganlin Song",
      "Xinyi Zhong",
      "John Lafferty"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13614"
  },
  {
    "id": "arXiv:2205.13641",
    "title": "Gender differences in research grant allocation -- a mixed picture",
    "abstract": "Gender bias in grant allocation is a deviation from the principle that\nscientific merit should guide grant decisions. However, most studies on gender\nbias in grant allocation focus on gender differences in success rates, without\nincluding variables that measure merit. This study has two main contributions.\nFirstly, it includes several merit variables in the analysis. Secondly, it\nincludes an analysis at the panel level where the selection process takes\nplace, and this enables to study bias more in-depth at the process level. The\nfindings are: (i) After controlling for merit, a consistent pattern of gender\nbias was found in the scores: women receive significant lower grades than men\ndo. (ii) The scores are an input into the two-step decision-making process, and\nthis study shows bias against women in the first selection decision where 75%\nof the applications are rejected, and bias in favor of women in the second\n(final) selection decision. (iii) At the level of individual panels, the\nanalysis shows a mixed pattern of bias: in some panels the odds for women to\nreceive a grant are lower than for men, whereas in other panels we find the\nopposite, next to panels with gender-neutral decision making. (iv) In the case\nunder study, at an aggregated level the allocation of grants seems balanced.\n(v) The mixed pattern at panel level seems to relate characteristics such as\nthe panel composition, and the level of gender stereotyping.",
    "descriptor": "\nComments: 21 pages, 2 figures, 4 tables, preprint\n",
    "authors": [
      "Peter van den Besselaar",
      "Charlie Mom"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2205.13641"
  },
  {
    "id": "arXiv:2205.13662",
    "title": "Explaining Preferences with Shapley Values",
    "abstract": "While preference modelling is becoming one of the pillars of machine\nlearning, the problem of preference explanation remains challenging and\nunderexplored. In this paper, we propose \\textsc{Pref-SHAP}, a Shapley\nvalue-based model explanation framework for pairwise comparison data. We derive\nthe appropriate value functions for preference models and further extend the\nframework to model and explain \\emph{context specific} information, such as the\nsurface type in a tennis game. To demonstrate the utility of\n\\textsc{Pref-SHAP}, we apply our method to a variety of synthetic and\nreal-world datasets and show that richer and more insightful explanations can\nbe obtained over the baseline.",
    "descriptor": "",
    "authors": [
      "Robert Hu",
      "Siu Lun Chau",
      "Jaime Ferrando Huertas",
      "Dino Sejdinovic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.13662"
  },
  {
    "id": "arXiv:2205.13684",
    "title": "Learning with Stochastic Orders",
    "abstract": "Learning high-dimensional distributions is often done with explicit\nlikelihood modeling or implicit modeling via minimizing integral probability\nmetrics (IPMs). In this paper, we expand this learning paradigm to stochastic\norders, namely, the convex or Choquet order between probability measures.\nTowards this end, we introduce the Choquet-Toland distance between probability\nmeasures, that can be used as a drop-in replacement for IPMs. We also introduce\nthe Variational Dominance Criterion (VDC) to learn probability measures with\ndominance constraints, that encode the desired stochastic order between the\nlearned measure and a known baseline. We analyze both quantities and show that\nthey suffer from the curse of dimensionality and propose surrogates via input\nconvex maxout networks (ICMNs), that enjoy parametric rates. Finally, we\nprovide a min-max framework for learning with stochastic orders and validate it\nexperimentally on synthetic and high-dimensional image generation, with\npromising results. The code is available at\nhttps://github.com/yair-schiff/stochastic-orders-ICMN",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Carles Domingo-Enrich",
      "Yair Schiff",
      "Youssef Mroueh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13684"
  },
  {
    "id": "arXiv:2205.13687",
    "title": "Asymptotic Convergence Rate and Statistical Inference for Stochastic  Sequential Quadratic Programming",
    "abstract": "We apply a stochastic sequential quadratic programming (StoSQP) algorithm to\nsolve constrained nonlinear optimization problems, where the objective is\nstochastic and the constraints are deterministic. We study a fully stochastic\nsetup, where only a single sample is available in each iteration for estimating\nthe gradient and Hessian of the objective. We allow StoSQP to select a random\nstepsize $\\bar{\\alpha}_t$ adaptively, such that $\\beta_t\\leq \\bar{\\alpha}_t\n\\leq \\beta_t+\\chi_t$, where $\\beta_t$, $\\chi_t=o(\\beta_t)$ are prespecified\ndeterministic sequences. We also allow StoSQP to solve Newton system inexactly\nvia randomized iterative solvers, e.g., with the sketch-and-project method; and\nwe do not require the approximation error of inexact Newton direction to\nvanish. For this general StoSQP framework, we establish the asymptotic\nconvergence rate for its last iterate, with the worst-case iteration complexity\nas a byproduct; and we perform statistical inference. In particular, with\nproper decaying $\\beta_t,\\chi_t$, we show that: (i) the StoSQP scheme can take\nat most $O(1/\\epsilon^4)$ iterations to achieve $\\epsilon$-stationarity; (ii)\nasymptotically and almost surely, $\\|(x_t -x^\\star, \\lambda_t -\n\\lambda^\\star)\\| = O(\\sqrt{\\beta_t\\log(1/\\beta_t)})+O(\\chi_t/\\beta_t)$, where\n$(x_t,\\lambda_t)$ is the primal-dual StoSQP iterate; (iii) the sequence\n$1/\\sqrt{\\beta_t}\\cdot (x_t -x^\\star, \\lambda_t - \\lambda^\\star)$ converges to\na mean zero Gaussian distribution with a nontrivial covariance matrix.\nMoreover, we establish the Berry-Esseen bound for $(x_t, \\lambda_t)$ to measure\nquantitatively the convergence of its distribution function. We also provide a\npractical estimator for the covariance matrix, from which the confidence\nintervals of $(x^\\star, \\lambda^\\star)$ can be constructed using iterates\n$\\{(x_t,\\lambda_t)\\}_t$. Our theorems are validated using nonlinear problems in\nCUTEst test set.",
    "descriptor": "\nComments: 59 pages, 6 figures\n",
    "authors": [
      "Sen Na",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13687"
  },
  {
    "id": "arXiv:2205.13732",
    "title": "Explicit method to make shortened stabilizer EAQECC from stabilizer QECC",
    "abstract": "In the previous research by Grassl, Huber and Winter, they proved a theorem\nwhich can make entanglement-assisted quantum error-correcting codes (EAQECC)\nfrom general quantum error-correcting codes (QECC). In this paper, we prove\nthat the shortened EAQECC is a stabilizer code if the original EAQECC is a\nstabilizer code.",
    "descriptor": "\nComments: 5 pages, article.cls, no figure\n",
    "authors": [
      "Daiki Ueno",
      "Ryutaroh Matsumoto"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13732"
  },
  {
    "id": "arXiv:2205.13746",
    "title": "Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games",
    "abstract": "We study the problem of finding the Nash equilibrium in a two-player zero-sum\nMarkov game. Due to its formulation as a minimax optimization program, a\nnatural approach to solve the problem is to perform gradient descent/ascent\nwith respect to each player in an alternating fashion. However, due to the\nnon-convexity/non-concavity of the underlying objective function, theoretical\nunderstandings of this method are limited. In our paper, we consider solving an\nentropy-regularized variant of the Markov game. The regularization introduces\nstructure into the optimization landscape that make the solutions more\nidentifiable and allow the problem to be solved more efficiently. Our main\ncontribution is to show that under proper choices of the regularization\nparameter, the gradient descent ascent algorithm converges to the Nash\nequilibrium of the original unregularized problem. We explicitly characterize\nthe finite-time performance of the last iterate of our algorithm, which vastly\nimproves over the existing convergence bound of the gradient descent ascent\nalgorithm without regularization. Finally, we complement the analysis with\nnumerical simulations that illustrate the accelerated convergence of the\nalgorithm.",
    "descriptor": "",
    "authors": [
      "Sihan Zeng",
      "Thinh T. Doan",
      "Justin Romberg"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13746"
  },
  {
    "id": "arXiv:2205.13757",
    "title": "Representing Polymers as Periodic Graphs with Learned Descriptors for  Accurate Polymer Property Predictions",
    "abstract": "One of the grand challenges of utilizing machine learning for the discovery\nof innovative new polymers lies in the difficulty of accurately representing\nthe complex structures of polymeric materials. Although a wide array of\nhand-designed polymer representations have been explored, there has yet to be\nan ideal solution for how to capture the periodicity of polymer structures, and\nhow to develop polymer descriptors without the need for human feature design.\nIn this work, we tackle these problems through the development of our periodic\npolymer graph representation. Our pipeline for polymer property predictions is\ncomprised of our polymer graph representation that naturally accounts for the\nperiodicity of polymers, followed by a message-passing neural network (MPNN)\nthat leverages the power of graph deep learning to automatically learn\nchemically-relevant polymer descriptors. Across a diverse dataset of 10 polymer\nproperties, we find that this polymer graph representation consistently\noutperforms hand-designed representations with a 20% average reduction in\nprediction error. Our results illustrate how the incorporation of chemical\nintuition through directly encoding periodicity into our polymer graph\nrepresentation leads to a considerable improvement in the accuracy and\nreliability of polymer property predictions. We also demonstrate how combining\npolymer graph representations with message-passing neural network architectures\ncan automatically extract meaningful polymer features that are consistent with\nhuman intuition, while outperforming human-derived features. This work\nhighlights the advancement in predictive capability that is possible if using\nchemical descriptors that are specifically optimized for capturing the unique\nchemical structure of polymers.",
    "descriptor": "",
    "authors": [
      "Evan R. Antoniuk",
      "Peggy Li",
      "Bhavya Kailkhura",
      "Anna M. Hiszpanski"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13757"
  },
  {
    "id": "arXiv:2205.13774",
    "title": "Classification of COVID-19 Patients with their Severity Level from Chest  CT Scans using Transfer Learning",
    "abstract": "Background and Objective: During pandemics, the use of artificial\nintelligence (AI) approaches combined with biomedical science play a\nsignificant role in reducing the burden on the healthcare systems and\nphysicians. The rapid increment in cases of COVID-19 has led to an increase in\ndemand for hospital beds and other medical equipment. However, since medical\nfacilities are limited, it is recommended to diagnose patients as per the\nseverity of the infection. Keeping this in mind, we share our research in\ndetecting COVID-19 as well as assessing its severity using chest-CT scans and\nDeep Learning pre-trained models. Dataset: We have collected a total of 1966 CT\nScan images for three different class labels, namely, Non-COVID, Severe COVID,\nand Non-Severe COVID, out of which 714 CT images belong to the Non-COVID\ncategory, 713 CT images are for Non-Severe COVID category and 539 CT images are\nof Severe COVID category. Methods: All of the images are initially\npre-processed using the Contrast Limited Histogram Equalization (CLAHE)\napproach. The pre-processed images are then fed into the VGG-16 network for\nextracting features. Finally, the retrieved characteristics are categorized and\nthe accuracy is evaluated using a support vector machine (SVM) with 10-fold\ncross-validation (CV). Result and Conclusion: In our study, we have combined\nwell-known strategies for pre-processing, feature extraction, and\nclassification which brings us to a remarkable success rate of disease and its\nseverity recognition with an accuracy of 96.05% (97.7% for Non-Severe COVID-19\nimages and 93% for Severe COVID-19 images). Our model can therefore help\nradiologists detect COVID-19 and the extent of its severity.",
    "descriptor": "",
    "authors": [
      "Mansi Gupta",
      "Aman Swaraj",
      "Karan Verma"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13774"
  },
  {
    "id": "arXiv:2205.13779",
    "title": "Forecasting countries' gross domestic product from patent data",
    "abstract": "Recent strides in economic complexity have shown that the future economic\ndevelopment of nations can be predicted with a single \"economic fitness\"\nvariable, which captures countries' competitiveness in international trade. The\npredictions by this low-dimensional approach could match or even outperform\npredictions based on much more sophisticated methods, such as those by the\nInternational Monetary Fund (IMF). However, all prior works in economic\ncomplexity aimed to quantify countries' fitness from World Trade export data,\nwithout considering the possibility to infer countries' potential for growth\nfrom alternative sources of data. Here, motivated by the long-standing\nrelationship between technological development and economic growth, we aim to\nforecast countries' growth from patent data. Specifically, we construct a\ncitation network between countries from the European Patent Office (EPO)\ndataset. Initial results suggest that the H-index centrality in this network is\na potential candidate to gauge national economic performance. To validate this\nconjecture, we construct a two-dimensional plane defined by the H-index and GDP\nper capita, and use a forecasting method based on dynamical systems to test the\npredicting accuracy of the H-index. We find that the predictions based on the\nH-index-GDP plane outperform the predictions by IMF by approximately 35%, and\nthey marginally outperform those by the economic fitness extracted from trade\ndata. Our results could inspire further attempts to identify predictors of\nnational growth from different sources of data related to scientific and\ntechnological innovation.",
    "descriptor": "",
    "authors": [
      "Yucheng Ye",
      "Shuqi Xu",
      "Manuel Sebastian Mariani",
      "Linyuan L\u00fc"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.13779"
  },
  {
    "id": "arXiv:2205.13789",
    "title": "A Sea of Words: An In-Depth Analysis of Anchors for Text Data",
    "abstract": "Anchors [Ribeiro et al. (2018)] is a post-hoc, rule-based interpretability\nmethod. For text data, it proposes to explain a decision by highlighting a\nsmall set of words (an anchor) such that the model to explain has similar\noutputs when they are present in a document. In this paper, we present the\nfirst theoretical analysis of Anchors, considering that the search for the best\nanchor is exhaustive. We leverage this analysis to gain insights on the\nbehavior of Anchors on simple models, including elementary if-then rules and\nlinear classifiers.",
    "descriptor": "\nComments: 10+2 page paper, 15-page appendix\n",
    "authors": [
      "Gianluigi Lopardo",
      "Damien Garreau",
      "Frederic Precioso"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13789"
  },
  {
    "id": "arXiv:2205.13816",
    "title": "Prune and distill: similar reformatting of image information along rat  visual cortex and deep neural networks",
    "abstract": "Visual object recognition has been extensively studied in both neuroscience\nand computer vision. Recently, the most popular class of artificial systems for\nthis task, deep convolutional neural networks (CNNs), has been shown to provide\nexcellent models for its functional analogue in the brain, the ventral stream\nin visual cortex. This has prompted questions on what, if any, are the common\nprinciples underlying the reformatting of visual information as it flows\nthrough a CNN or the ventral stream. Here we consider some prominent\nstatistical patterns that are known to exist in the internal representations of\neither CNNs or the visual cortex and look for them in the other system. We show\nthat intrinsic dimensionality (ID) of object representations along the rat\nhomologue of the ventral stream presents two distinct expansion-contraction\nphases, as previously shown for CNNs. Conversely, in CNNs, we show that\ntraining results in both distillation and active pruning (mirroring the\nincrease in ID) of low- to middle-level image information in single units, as\nrepresentations gain the ability to support invariant discrimination, in\nagreement with previous observations in rat visual cortex. Taken together, our\nfindings suggest that CNNs and visual cortex share a similarly tight\nrelationship between dimensionality expansion/reduction of object\nrepresentations and reformatting of image information.",
    "descriptor": "\nComments: 11 pages, 5 fiures\n",
    "authors": [
      "Paolo Muratore",
      "Sina Tafazoli",
      "Eugenio Piasini",
      "Alessandro Laio",
      "Davide Zoccolan"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13816"
  },
  {
    "id": "arXiv:2205.13827",
    "title": "Error Bound of Empirical $\\ell_2$ Risk Minimization for Noisy Standard  and Generalized Phase Retrieval Problems",
    "abstract": "A noisy generalized phase retrieval (NGPR) problem refers to a problem of\nestimating $x_0 \\in \\mathbb{C}^d$ by noisy quadratic samples\n$\\big\\{x_0^*A_kx_0+\\eta_k\\big\\}_{k=1}^n$ where $A_k$ is a Hermitian matrix and\n$\\eta_k$ is a noise scalar. When $A_k=\\alpha_k\\alpha_k^*$ for some\n$\\alpha_k\\in\\mathbb{C}^d$, it reduces to a standard noisy phase retrieval (NPR)\nproblem. The main aim of this paper is to study the estimation performance of\nempirical $\\ell_2$ risk minimization in both problems when $A_k$ in NGPR, or\n$\\alpha_k$ in NPR, is drawn from sub-Gaussian distribution. Under different\nkinds of noise patterns, we establish error bounds that can imply approximate\nreconstruction and these results are new in the literature. In NGPR, we show\nthe bounds are of $O\\big(\\frac{||\\eta||}{\\sqrt{n}}\\big)$ and\n$O\\big(||\\eta||_\\infty \\sqrt{\\frac{d}{n}}\\big)$ for general noise, and of\n$O\\big(\\sqrt{\\frac{d\\log n}{n}}\\big)$ and $O\\big(\\sqrt{\\frac{d(\\log\nn)^2}{n}}\\big)$ for random noise with sub-Gaussian and sub-exponential tail\nrespectively, where $\\| \\eta \\|$ and $\\| \\eta \\|_{\\infty}$ are the 2-norm and\nsup-norm of the noise vector of $\\eta_k$. Under heavy-tailed noise, by\ntruncating response outliers we propose a robust estimator that possesses an\nerror bound with slower convergence rate. On the other hand, we obtain in NPR\nthe bound is of $O\\big(\\sqrt{\\frac{d\\log n}{n}}\\big)$ and\n$O\\big(\\sqrt{\\frac{d(\\log n)^2}{n}}\\big)$) for sub-Gaussian and sub-exponential\nnoise respectively, which is essentially tighter than the existing bound\n$O\\big(\\frac{||\\eta||_2}{\\sqrt{n}}\\big)$. Although NGPR involving measurement\nmatrix $A_k$ is more computationally demanding than NPR involving measurement\nvector $\\alpha_k$, our results reveal that NGPR exhibits stronger robustness\nthan NPR under biased and deterministic noise. Experimental results are\npresented to confirm and demonstrate our theoretical findings.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Junren Chen",
      "Michael K. Ng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13827"
  },
  {
    "id": "arXiv:2205.13835",
    "title": "Deep Learning Fetal Ultrasound Video Model Match Human Observers in  Biometric Measurements",
    "abstract": "Objective. This work investigates the use of deep convolutional neural\nnetworks (CNN) to automatically perform measurements of fetal body parts,\nincluding head circumference, biparietal diameter, abdominal circumference and\nfemur length, and to estimate gestational age and fetal weight using fetal\nultrasound videos. Approach. We developed a novel multi-task CNN-based\nspatio-temporal fetal US feature extraction and standard plane detection\nalgorithm (called FUVAI) and evaluated the method on 50 freehand fetal US video\nscans. We compared FUVAI fetal biometric measurements with measurements made by\nfive experienced sonographers at two time points separated by at least two\nweeks. Intra- and inter-observer variabilities were estimated. Main results. We\nfound that automated fetal biometric measurements obtained by FUVAI were\ncomparable to the measurements performed by experienced sonographers The\nobserved differences in measurement values were within the range of inter- and\nintra-observer variability. Moreover, analysis has shown that these differences\nwere not statistically significant when comparing any individual medical expert\nto our model. Significance. We argue that FUVAI has the potential to assist\nsonographers who perform fetal biometric measurements in clinical settings by\nproviding them with suggestions regarding the best measuring frames, along with\nautomated measurements. Moreover, FUVAI is able perform these tasks in just a\nfew seconds, which is a huge difference compared to the average of six minutes\ntaken by sonographers. This is significant, given the shortage of medical\nexperts capable of interpreting fetal ultrasound images in numerous countries.",
    "descriptor": "\nComments: Published at Physics in Medicine & Biology\n",
    "authors": [
      "Szymon P\u0142otka",
      "Adam Klasa",
      "Aneta Lisowska",
      "Joanna Seliga-Siwecka",
      "Micha\u0142 Lipa",
      "Tomasz Trzci\u0144ski",
      "Arkadiusz Sitek"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13835"
  },
  {
    "id": "arXiv:2205.13847",
    "title": "Textural-Structural Joint Learning for No-Reference Super-Resolution  Image Quality Assessment",
    "abstract": "Image super-resolution (SR) has been widely investigated in recent years.\nHowever, it is challenging to fairly estimate the performances of various SR\nmethods, as the lack of reliable and accurate criteria for perceptual quality.\nExisting SR image quality assessment (IQA) metrics usually concentrate on the\nspecific kind of degradation without distinguishing the visual sensitive areas,\nwhich have no adaptive ability to describe the diverse SR degeneration\nsituations. In this paper, we focus on the textural and structural degradation\nof image SR which acts as a critical role for visual perception, and design a\ndual stream network to jointly explore the textural and structural information\nfor quality prediction, dubbed TSNet. By mimicking the human vision system\n(HVS) that pays more attention to the significant areas of the image, we\ndevelop the spatial attention mechanism to make the visual-sensitive areas more\ndistinguishable, which improves the prediction accuracy. Feature normalization\n(F-Norm) is also developed to investigate the inherent spatial correlation of\nSR features and boost the network representation capacity. Experimental results\nshow the proposed TSNet predicts the visual quality more accurate than the\nstate-of-the-art IQA methods, and demonstrates better consistency with the\nhuman's perspective. The source code will be made available at\nthis http URL",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Yuqing Liu",
      "Qi Jia",
      "Shanshe Wang",
      "Siwei Ma",
      "Wen Gao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13847"
  },
  {
    "id": "arXiv:2205.13856",
    "title": "Finding Patterns in Visualized Data by Adding Redundant Visual  Information",
    "abstract": "We present \"PATRED\", a technique that uses the addition of redundant\ninformation to facilitate the detection of specific, generally described\npatterns in line-charts during the visual exploration of the charts. We\ncompared different versions of this technique, that differed in the way\nredundancy was added, using nine distance metrics (such as Euclidean, Pearson,\nMutual Information and Jaccard) with judgments from data scientists which\nserved as the \"ground truth\". Results were analyzed with correlations (R2), F1\nscores and Mutual Information with the average ranking by the data scientists.\nSome distance metrics consistently benefit from the addition of redundant\ninformation, while others are only enhanced for specific types of data\nperturbations. The results demonstrate the value of adding redundancy to\nimprove the identification of patterns in time-series data during visual\nexploration.",
    "descriptor": "\nComments: 13 pages, 19 Figures\n",
    "authors": [
      "Salomon Eisler",
      "Joachim Meyer"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13856"
  },
  {
    "id": "arXiv:2205.13903",
    "title": "Subordination Algebras as Semantic Environment of Input/Output Logic",
    "abstract": "We establish a novel connection between two research areas in non-classical\nlogics which have been developed independently of each other so far: on the one\nhand, input/output logic, introduced within a research program developing\nlogical formalizations of normative reasoning in philosophical logic and AI; on\nthe other hand, subordination algebras, investigated in the context of a\nresearch program integrating topological, algebraic, and duality-theoretic\ntechniques in the study of the semantics of modal logic. Specifically, we\npropose that the basic framework of input/output logic, as well as its\nextensions, can be given formal semantics on (slight generalizations of)\nsubordination algebras. The existence of this interpretation brings benefits to\nboth research areas: on the one hand, this connection allows for a novel\nconceptual understanding of subordination algebras as mathematical models of\nthe properties and behaviour of norms; on the other hand, thanks to the well\ndeveloped connection between subordination algebras and modal logic, the output\noperators in input/output logic can be given a new formal representation as\nmodal operators, whose properties can be explicitly axiomatised in a suitable\nlanguage, and be systematically studied by means of mathematically established\nand powerful tools.",
    "descriptor": "",
    "authors": [
      "Andrea De Domenico",
      "Ali Farjami",
      "Krishna Manoorkar",
      "Alessandra Palmigiano",
      "Mattia Panettiere",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.13903"
  },
  {
    "id": "arXiv:2205.13916",
    "title": "Ranking Binary Unlabelled Necklaces in Polynomial Time",
    "abstract": "Unlabelled Necklaces are an equivalence class of cyclic words under both the\nrotation (cyclic shift) and the relabelling operations. The relabelling of a\nword is a bijective mapping from the alphabet to itself. The main result of the\npaper is the first polynomial-time algorithm for ranking unlabelled necklaces\nof a binary alphabet. The time-complexity of the algorithm is $O(n^6 \\log^2\nn)$, where $n$ is the length of the considered necklaces. The key part of the\nalgorithm is to compute the rank of any word with respect to the set of\nunlabelled necklaces by finding three other ranks: the rank over all necklaces,\nthe rank over symmetric unlabelled necklaces, and the rank over necklaces with\nan enclosing labelling. The last two concepts are introduced in this paper.",
    "descriptor": "\nComments: Short version to appear at the 24th International Conference on Descriptional Complexity of Formal systems\n",
    "authors": [
      "Duncan Adamson"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.13916"
  },
  {
    "id": "arXiv:2205.13932",
    "title": "Internal Model-Based Online Optimization",
    "abstract": "In this paper we propose a model-based approach to the design of online\noptimization algorithms, with the goal of improving the tracking of the\nsolution trajectory (trajectories) w.r.t. state-of-the-art methods. We focus\nfirst on quadratic problems with a time-varying linear term, and use digital\ncontrol tools to propose a novel online algorithm that can achieve zero\ntracking error by modeling the cost with a dynamical system. We prove the\nconvergence of the algorithm for both strongly convex and convex problems. We\nfurther discuss the sensitivity of the proposed method to model uncertainties\nand quantify its performance. As part of our discussion, we also proposed a\ncompensated online gradient that achieves our goal for problems with a linear\ndependence on time. We discuss how these algorithms can be applied to general\n(non-quadratic) problems using an approximate model of the cost. We present\nnumerical results that showcase the superior performance of the proposed\nalgorithms over previous methods for both quadratic and non-quadratic problems.",
    "descriptor": "",
    "authors": [
      "Nicola Bastianello",
      "Ruggero Carli",
      "Sandro Zampieri"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13932"
  },
  {
    "id": "arXiv:2205.13935",
    "title": "Combining observational datasets from multiple environments to detect  hidden confounding",
    "abstract": "A common assumption in causal inference from observational data is the\nassumption of no hidden confounding. Yet it is, in general, impossible to\nverify the presence of hidden confounding factors from a single dataset.\nHowever, under the assumption of independent causal mechanisms underlying the\ndata generative process, we demonstrate a way to detect unobserved confounders\nwhen having multiple observational datasets coming from different environments.\nWe present a theory for testable conditional independencies that are only\nviolated during hidden confounding and examine cases where we break its\nassumptions: degenerate & dependent mechanisms, and faithfulness violations.\nAdditionally, we propose a procedure to test these independencies and study its\nempirical finite-sample behavior using simulation studies.",
    "descriptor": "\nComments: 19 pages including references and appendix\n",
    "authors": [
      "Rickard K.A. Karlsson",
      "Jesse H. Krijthe"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13935"
  },
  {
    "id": "arXiv:2205.13976",
    "title": "Hybrid Offline-Online Design for Reconfigurable Intelligent Surface  Aided UAV Communication",
    "abstract": "This letter considers the reconfigurable intelligent surface (RIS)-aided\nunmanned aerial vehicle (UAV) communication systems in urban areas under the\ngeneral Rician fading channel. A hybrid offline-online design is proposed to\nimprove the system performance by leveraging both the statistical channel state\ninformation (S-CSI) and instantaneous channel state information (I-CSI). For\nthe offline phase, we aim to maximize the expected average achievable rate\nbased on the S-CSI by jointly optimizing the RIS's phase-shift and UAV\ntrajectory. The formulated stochastic optimization problem is difficult to\nsolve due to its non-convexity. To tackle this problem, we propose an efficient\nalgorithm by leveraging the stochastic successive convex approximation (SSCA)\ntechniques. For the online phase, the UAV adaptively adjusts the transmit\nbeamforming and user scheduling according to the effective I-CSI. Numerical\nresults verify that the proposed hybrid design performs better than various\nbechmark schemes, and also demonstrate a favorable trade-off between system\nperformance and CSI overhead.",
    "descriptor": "",
    "authors": [
      "Kaiyuan Tian",
      "Bin Duo",
      "Xiaojun Yuan",
      "Wu Luo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13976"
  },
  {
    "id": "arXiv:2205.13983",
    "title": "Quantum Augmented Dual Attack",
    "abstract": "We present a quantum augmented variant of the dual lattice attack on the\nLearning with Errors (LWE) problem, using classical memory with quantum random\naccess (QRACM). Applying our results to lattice parameters from the literature,\nwe find that our algorithm outperforms previous algorithms, assuming unit cost\naccess to a QRACM. On a technical level, we show how to obtain a quantum\nspeedup on the search for Fast Fourier Transform (FFT) coefficients above a\ngiven threshold by leveraging the relative sparseness of the FFT and using\nquantum amplitude estimation. We also discuss the applicability of the Quantum\nFourier Transform in this context. Furthermore, we give a more rigorous\nanalysis of the classical and quantum expected complexity of guessing part of\nthe secret vector where coefficients follow a discrete Gaussian (mod \\(q\\)).",
    "descriptor": "",
    "authors": [
      "Martin R. Albrecht",
      "Yixin Shen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.13983"
  },
  {
    "id": "arXiv:2205.14001",
    "title": "A Single-Adversary-Single-Detector Zero-Sum Game in Networked Control  Systems",
    "abstract": "This paper proposes a game-theoretic approach to address the problem of\noptimal sensor placement for detecting cyber-attacks in networked control\nsystems. The problem is formulated as a zero-sum game with two players, namely\na malicious adversary and a detector. Given a protected target vertex, the\ndetector places a sensor at a single vertex to monitor the system and detect\nthe presence of the adversary. On the other hand, the adversary selects a\nsingle vertex through which to conduct a cyber-attack that maximally disrupts\nthe target vertex while remaining undetected by the detector. As our first\ncontribution, for a given pair of attack and monitor vertices and a known\ntarget vertex, the game payoff function is defined as the output-to-output gain\nof the respective system. Then, the paper characterizes the set of feasible\nactions by the detector that ensures bounded values of the game payoff.\nFinally, an algebraic sufficient condition is proposed to examine whether a\ngiven vertex belongs to the set of feasible monitor vertices. The optimal\nsensor placement is then determined by computing the mixed-strategy Nash\nequilibrium of the zero-sum game through linear programming. The approach is\nillustrated via a numerical example of a 10-vertex networked control system\nwith a given target vertex.",
    "descriptor": "\nComments: 6 pages, 6 figures, 1 table, accepted to the 9th IFAC Conference on Networked Systems, Zurich, July 2022\n",
    "authors": [
      "Anh Tung Nguyen",
      "Andr\u00e9 M. H. Teixeira",
      "Alexander Medvedev"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14001"
  },
  {
    "id": "arXiv:2205.14025",
    "title": "Inference and Sampling for Archimax Copulas",
    "abstract": "Understanding multivariate dependencies in both the bulk and the tails of a\ndistribution is an important problem for many applications, such as ensuring\nalgorithms are robust to observations that are infrequent but have devastating\neffects. Archimax copulas are a family of distributions endowed with a precise\nrepresentation that allows simultaneous modeling of the bulk and the tails of a\ndistribution. Rather than separating the two as is typically done in practice,\nincorporating additional information from the bulk may improve inference of the\ntails, where observations are limited. Building on the stochastic\nrepresentation of Archimax copulas, we develop a non-parametric inference\nmethod and sampling algorithm. Our proposed methods, to the best of our\nknowledge, are the first that allow for highly flexible and scalable inference\nand sampling algorithms, enabling the increased use of Archimax copulas in\npractical settings. We experimentally compare to state-of-the-art density\nmodeling techniques, and the results suggest that the proposed method\neffectively extrapolates to the tails while scaling to higher dimensional data.\nOur findings suggest that the proposed algorithms can be used in a variety of\napplications where understanding the interplay between the bulk and the tails\nof a distribution is necessary, such as healthcare and safety.",
    "descriptor": "",
    "authors": [
      "Yuting Ng",
      "Ali Hasan",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14025"
  },
  {
    "id": "arXiv:2205.14029",
    "title": "Lesion classification by model-based feature extraction: A differential  affine invariant model of soft tissue elasticity",
    "abstract": "The elasticity of soft tissues has been widely considered as a characteristic\nproperty to differentiate between healthy and vicious tissues and, therefore,\nmotivated several elasticity imaging modalities, such as Ultrasound\nElastography, Magnetic Resonance Elastography, and Optical Coherence\nElastography. This paper proposes an alternative approach of modeling the\nelasticity using Computed Tomography (CT) imaging modality for model-based\nfeature extraction machine learning (ML) differentiation of lesions. The model\ndescribes a dynamic non-rigid (or elastic) deformation in differential manifold\nto mimic the soft tissues elasticity under wave fluctuation in vivo. Based on\nthe model, three local deformation invariants are constructed by two tensors\ndefined by the first and second order derivatives from the CT images and used\nto generate elastic feature maps after normalization via a novel signal\nsuppression method. The model-based elastic image features are extracted from\nthe feature maps and fed to machine learning to perform lesion classifications.\nTwo pathologically proven image datasets of colon polyps (44 malignant and 43\nbenign) and lung nodules (46 malignant and 20 benign) were used to evaluate the\nproposed model-based lesion classification. The outcomes of this modeling\napproach reached the score of area under the curve of the receiver operating\ncharacteristics of 94.2 % for the polyps and 87.4 % for the nodules, resulting\nin an average gain of 5 % to 30 % over ten existing state-of-the-art lesion\nclassification methods. The gains by modeling tissue elasticity for ML\ndifferentiation of lesions are striking, indicating the great potential of\nexploring the modeling strategy to other tissue properties for ML\ndifferentiation of lesions.",
    "descriptor": "\nComments: 12 pages, 4 figures, 3 tables\n",
    "authors": [
      "Weiguo Cao",
      "Marc J. Pomeroy",
      "Zhengrong Liang",
      "Yongfeng Gao",
      "Yongyi Shi",
      "Jiaxing Tan",
      "Fangfang Han",
      "Jing Wang",
      "Jianhua Ma",
      "Hongbin Lu",
      "Almas F. Abbasi",
      "Perry J. Pickhardt"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14029"
  },
  {
    "id": "arXiv:2205.14048",
    "title": "Average Adjusted Association: Efficient Estimation with High Dimensional  Confounders",
    "abstract": "The log odds ratio is a common parameter to measure association between\n(binary) outcome and exposure variables. Much attention has been paid to its\nparametric but robust estimation, or its nonparametric estimation as a function\nof confounders. However, discussion on how to use a summary statistic by\naveraging the log odds ratio function is surprisingly difficult to find despite\nthe popularity and importance of averaging in other contexts such as estimating\nthe average treatment effect. We propose a couple of efficient double/debiased\nmachine learning (DML) estimators of the average log odds ratio, where the odds\nratios are adjusted for observed (potentially high dimensional) confounders and\nare averaged over them. The estimators are built from two equivalent forms of\nthe efficient influence function. The first estimator uses a prospective\nprobability of the outcome conditional on the exposure and confounders; the\nsecond one employs a retrospective probability of the exposure conditional on\nthe outcome and confounders. Our framework encompasses random sampling as well\nas outcome-based or exposure-based sampling. Finally, we illustrate how to\napply the proposed estimators using real data.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Sung Jae Jun",
      "Sokbae Lee"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14048"
  },
  {
    "id": "arXiv:2205.14067",
    "title": "Finite mixture of skewed sub-Gaussian stable distributions",
    "abstract": "We propose the finite mixture of skewed sub-Gaussian stable distributions.\nThe maximum likelihood estimator for the parameters of proposed finite mixture\nmodel is computed through the expectation-maximization algorithm. The proposed\nmodel contains the finite mixture of normal and skewed normal distributions.\nSince the tails of proposed model is heavier than even the Student's t\ndistribution, it can be used as a powerful model for robust model-based\nclustering. Performance of the proposed model is demonstrated by clustering\nsimulation data and two sets of real data.",
    "descriptor": "",
    "authors": [
      "Mahdi Teimouri"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14067"
  },
  {
    "id": "arXiv:2205.14081",
    "title": "Towards Quantum Gravity in the Lab on Quantum Processors",
    "abstract": "The holographic principle and its realization in the AdS/CFT correspondence\nled to unexpected connections between general relativity and quantum\ninformation. This set the stage for studying aspects of quantum gravity models,\nwhich are otherwise difficult to access, in table-top quantum-computational\nexperiments. Recent works have designed a special teleportation protocol that\nrealizes a surprising communication phenomenon most naturally explained by the\nphysics of a traversable wormhole. In this work, we have carried out quantum\nexperiments based on this protocol on state-of-the-art quantum computers. The\ntarget quantum processing units (QPUs) included the Quantinuum's trapped-ion\nSystem Model H1-1 and five IBM superconducting QPUs of various architectures,\nwith public and premium user access. We report the observed teleportation\nsignals from these QPUs with the best one reaching 80% of theoretical\npredictions. We outline the experimental challenges we have faced in the course\nof implementation, as well as the new theoretical insights into quantum\ndynamics the work has led to. We also developed QGLab -- an open-source\nend-to-end software solution that facilitates conducting the wormhole-inspired\nteleportation experiments on state-of-the-art and emergent generations of QPUs\nsupported by the Qiskit and tket SDKs. We consider our study and deliverables\nas an early practical step towards the realization of more complex experiments\nfor the indirect probing of quantum gravity in the lab.",
    "descriptor": "\nComments: 21 pages, 6 figures, 2 tables, 1 listing\n",
    "authors": [
      "Illya Shapoval",
      "Vincent Paul Su",
      "Wibe de Jong",
      "Miro Urbanek",
      "Brian Swingle"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "General Relativity and Quantum Cosmology (gr-qc)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2205.14081"
  },
  {
    "id": "arXiv:2205.14085",
    "title": "Vehicle mission guidance by symbolic optimal control",
    "abstract": "Symbolic optimal control is a powerful method to synthesize algorithmically\ncorrect-by-design state-feedback controllers for nonlinear plants. Its\nsolutions are (near-)optimal with respect to a given cost function. In this\nnote, it is demonstrated how symbolic optimal control can be used to calculate\ncontrollers for an optimized routing guidance of vehicle systems in continuous\nstate space. In fact, the capacitated vehicle routing problem and a variant of\ntravelling salesman problem are investigated. The latter problem has a relevant\napplication in case of loss of vehicles during mission. A goods delivery\nscenario and a reconnaissance mission, involving bicycle and aircraft dynamics\nrespectively, are provided as examples.",
    "descriptor": "\nComments: 7 pages, 5 figures. To be published in: Proc. European Control Conference (ECC), 2022\n",
    "authors": [
      "Alexander Weber",
      "Florian Fiege",
      "Alexander Knoll"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14085"
  },
  {
    "id": "arXiv:2205.14090",
    "title": "Surrogate modeling for Bayesian optimization beyond a single Gaussian  process",
    "abstract": "Bayesian optimization (BO) has well-documented merits for optimizing\nblack-box functions with an expensive evaluation cost. Such functions emerge in\napplications as diverse as hyperparameter tuning, drug discovery, and robotics.\nBO hinges on a Bayesian surrogate model to sequentially select query points so\nas to balance exploration with exploitation of the search space. Most existing\nworks rely on a single Gaussian process (GP) based surrogate model, where the\nkernel function form is typically preselected using domain knowledge. To bypass\nsuch a design process, this paper leverages an ensemble (E) of GPs to\nadaptively select the surrogate model fit on-the-fly, yielding a GP mixture\nposterior with enhanced expressiveness for the sought function. Acquisition of\nthe next evaluation input using this EGP-based function posterior is then\nenabled by Thompson sampling (TS) that requires no additional design\nparameters. To endow function sampling with scalability, random feature-based\nkernel approximation is leveraged per GP model. The novel EGP-TS readily\naccommodates parallel operation. To further establish convergence of the\nproposed EGP-TS to the global optimum, analysis is conducted based on the\nnotion of Bayesian regret for both sequential and parallel settings. Tests on\nsynthetic functions and real-world applications showcase the merits of the\nproposed method.",
    "descriptor": "",
    "authors": [
      "Qin Lu",
      "Konstantinos D. Polyzos",
      "Bingcong Li",
      "Georgios B. Giannakis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14090"
  },
  {
    "id": "arXiv:2205.14121",
    "title": "AI-aided multiscale modeling of physiologically-significant blood clots",
    "abstract": "We have developed an AI-aided multiple time stepping (AI-MTS) algorithm and\nmultiscale modeling framework (AI-MSM) and implemented them on the Summit-like\nsupercomputer, AIMOS. AI-MSM is the first of its kind to integrate\nmulti-physics, including intra-platelet, inter-platelet, and fluid-platelet\ninteractions, into one system. It has simulated a record-setting multiscale\nblood clotting model of 102 million particles, of which 70 flowing and 180\naggregating platelets, under dissipative particle dynamics to coarse-grained\nmolecular dynamics. By adaptively adjusting timestep sizes to match the\ncharacteristic time scales of the underlying dynamics, AI-MTS optimally\nbalances speeds and accuracies of the simulations.",
    "descriptor": "",
    "authors": [
      "Yicong Zhu",
      "Changnian Han",
      "Peng Zhang",
      "Guojing Cong",
      "James R.Kozloski",
      "Chih-Chieh Yang",
      "Leili Zhang",
      "Yuefan Deng"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14121"
  },
  {
    "id": "arXiv:1810.00960",
    "title": "On the density of sets of the Euclidean plane avoiding distance 1",
    "abstract": "Comments: 13 pages, 8 figures",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Thomas Bellitto",
      "Arnaud P\u00eacher",
      "Antoine S\u00e9dillot"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1810.00960"
  },
  {
    "id": "arXiv:1812.11571",
    "title": "A Formal Separation Between Strategic and Nonstrategic Behavior",
    "abstract": "A Formal Separation Between Strategic and Nonstrategic Behavior",
    "descriptor": "",
    "authors": [
      "James R. Wright",
      "Kevin Leyton-Brown"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/1812.11571"
  },
  {
    "id": "arXiv:1905.10472",
    "title": "Accelerating Distributed Optimization via Fixed-time Convergent Flows:  Extensions to Non-convex Functions and Consistent Discretization",
    "abstract": "Comments: Under review. 10 pages, 1 figure",
    "descriptor": "\nComments: Under review. 10 pages, 1 figure\n",
    "authors": [
      "Kunal Garg",
      "Mayank Baranwal"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/1905.10472"
  },
  {
    "id": "arXiv:1910.08322",
    "title": "A Multilabel Classification Framework for Approximate Nearest Neighbor  Search",
    "abstract": "A Multilabel Classification Framework for Approximate Nearest Neighbor  Search",
    "descriptor": "",
    "authors": [
      "Ville Hyv\u00f6nen",
      "Elias J\u00e4\u00e4saari",
      "Teemu Roos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.08322"
  },
  {
    "id": "arXiv:1912.08722",
    "title": "Waiting but not Aging: Optimizing Information Freshness Under the Pull  Model",
    "abstract": "Comments: 15 pages. arXiv admin note: substantial text overlap with arXiv:1704.04848",
    "descriptor": "\nComments: 15 pages. arXiv admin note: substantial text overlap with arXiv:1704.04848\n",
    "authors": [
      "Fengjiao Li",
      "Yu Sang",
      "Zhongdong Liu",
      "Bin Li",
      "Huasen Wu",
      "Bo Ji"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1912.08722"
  },
  {
    "id": "arXiv:2004.13472",
    "title": "Linear Dependent Type Theory for Quantum Programming Languages",
    "abstract": "Linear Dependent Type Theory for Quantum Programming Languages",
    "descriptor": "",
    "authors": [
      "Peng Fu",
      "Kohei Kishida",
      "Peter Selinger"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2004.13472"
  },
  {
    "id": "arXiv:2009.03561",
    "title": "Local and Central Differential Privacy for Robustness and Privacy in  Federated Learning",
    "abstract": "Local and Central Differential Privacy for Robustness and Privacy in  Federated Learning",
    "descriptor": "",
    "authors": [
      "Mohammad Naseri",
      "Jamie Hayes",
      "Emiliano De Cristofaro"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2009.03561"
  },
  {
    "id": "arXiv:2011.05268",
    "title": "Towards Interpretable Natural Language Understanding with Explanations  as Latent Variables",
    "abstract": "Comments: NeurIPS 2020. The first three authors contribute equally",
    "descriptor": "\nComments: NeurIPS 2020. The first three authors contribute equally\n",
    "authors": [
      "Wangchunshu Zhou",
      "Jinyi Hu",
      "Hanlin Zhang",
      "Xiaodan Liang",
      "Maosong Sun",
      "Chenyan Xiong",
      "Jian Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.05268"
  },
  {
    "id": "arXiv:2102.06663",
    "title": "Potential singularity formation of incompressible axisymmetric Euler  equations with degenerate viscosity coefficients",
    "abstract": "Comments: 66 pages",
    "descriptor": "\nComments: 66 pages\n",
    "authors": [
      "Thomas Y. Hou",
      "De Huang"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2102.06663"
  },
  {
    "id": "arXiv:2102.11707",
    "title": "Deep ReLU neural networks overcome the curse of dimensionality for  partial integrodifferential equations",
    "abstract": "Deep ReLU neural networks overcome the curse of dimensionality for  partial integrodifferential equations",
    "descriptor": "",
    "authors": [
      "Lukas Gonon",
      "Christoph Schwab"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2102.11707"
  },
  {
    "id": "arXiv:2103.00200",
    "title": "Siamese Labels Auxiliary Learning",
    "abstract": "Siamese Labels Auxiliary Learning",
    "descriptor": "",
    "authors": [
      "Wenrui Gan",
      "Zhulin Liu",
      "C. L. Philip Chen",
      "Tong Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.00200"
  },
  {
    "id": "arXiv:2104.07245",
    "title": "Towards Handling Uncertainty-at-Source in AI -- A Review and Next Steps  for Interval Regression",
    "abstract": "Towards Handling Uncertainty-at-Source in AI -- A Review and Next Steps  for Interval Regression",
    "descriptor": "",
    "authors": [
      "Shaily Kabir",
      "Christian Wagner",
      "Zack Ellerby"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.07245"
  },
  {
    "id": "arXiv:2104.09124",
    "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with  Distilled Contrastive Learning",
    "abstract": "DisCo: Remedy Self-supervised Learning on Lightweight Models with  Distilled Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Yuting Gao",
      "Jia-Xin Zhuang",
      "Shaohui Lin",
      "Hao Cheng",
      "Xing Sun",
      "Ke Li",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.09124"
  },
  {
    "id": "arXiv:2104.10034",
    "title": "On Generating and Labeling Network Traffic with Realistic,  Self-Propagating Malware",
    "abstract": "Comments: 4+2 pages, 3 figures, 1 table, for AI4CS-SDM21",
    "descriptor": "\nComments: 4+2 pages, 3 figures, 1 table, for AI4CS-SDM21\n",
    "authors": [
      "Molly Buchanan",
      "Jeffrey W. Collyer",
      "Jack W. Davidson",
      "Saikat Dey",
      "Mark Gardner",
      "Jason D. Hiser",
      "Jeffry Lang",
      "Alastair Nottingham",
      "Alina Oprea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.10034"
  },
  {
    "id": "arXiv:2104.12710",
    "title": "Optimal Algorithm Allocation for Robotic Network Cloud Systems",
    "abstract": "Comments: This work is accepted for publication in the Elsevier Journal of Robotics and Autonomous Systems. Personal use of this material is permitted. Permission from Elsevier must be obtained for all other uses",
    "descriptor": "\nComments: This work is accepted for publication in the Elsevier Journal of Robotics and Autonomous Systems. Personal use of this material is permitted. Permission from Elsevier must be obtained for all other uses\n",
    "authors": [
      "Saeid Alirezazadeh",
      "Andr\u00e9 Correia",
      "Lu\u00eds A. Alexandre"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2104.12710"
  },
  {
    "id": "arXiv:2105.03824",
    "title": "FNet: Mixing Tokens with Fourier Transforms",
    "abstract": "Comments: To appear at NAACL 2022",
    "descriptor": "\nComments: To appear at NAACL 2022\n",
    "authors": [
      "James Lee-Thorp",
      "Joshua Ainslie",
      "Ilya Eckstein",
      "Santiago Ontanon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03824"
  },
  {
    "id": "arXiv:2105.05763",
    "title": "Iltis: Learning Logic in the Web",
    "abstract": "Iltis: Learning Logic in the Web",
    "descriptor": "",
    "authors": [
      "Gaetano Geck",
      "Christine Quenkert",
      "Marko Schmellenkamp",
      "Jonas Schmidt",
      "Felix Tschirbs",
      "Fabian Vehlken",
      "Thomas Zeume"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2105.05763"
  },
  {
    "id": "arXiv:2105.06340",
    "title": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video  Sequences using Temporal Oriented Reference Frame",
    "abstract": "3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video  Sequences using Temporal Oriented Reference Frame",
    "descriptor": "",
    "authors": [
      "Chuin Hong Yap",
      "Moi Hoon Yap",
      "Adrian K. Davison",
      "Connah Kendrick",
      "Jingting Li",
      "Sujing Wang",
      "Ryan Cunningham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.06340"
  },
  {
    "id": "arXiv:2105.12204",
    "title": "Safe Value Functions",
    "abstract": "Comments: 16 pages, 6 figures. Third version. Under review",
    "descriptor": "\nComments: 16 pages, 6 figures. Third version. Under review\n",
    "authors": [
      "Pierre-Fran\u00e7ois Massiani",
      "Steve Heim",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2105.12204"
  },
  {
    "id": "arXiv:2105.12537",
    "title": "Networks of climate change: Connecting causes and consequences",
    "abstract": "Networks of climate change: Connecting causes and consequences",
    "descriptor": "",
    "authors": [
      "Petter Holme",
      "Juan C. Rocha"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2105.12537"
  },
  {
    "id": "arXiv:2105.13856",
    "title": "Lightweight Cross-Lingual Sentence Representation Learning",
    "abstract": "Comments: ACL 2021 main conference; modified Eq. (2)",
    "descriptor": "\nComments: ACL 2021 main conference; modified Eq. (2)\n",
    "authors": [
      "Zhuoyuan Mao",
      "Prakhar Gupta",
      "Pei Wang",
      "Chenhui Chu",
      "Martin Jaggi",
      "Sadao Kurohashi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.13856"
  },
  {
    "id": "arXiv:2105.14274",
    "title": "Korean-English Machine Translation with Multiple Tokenization Strategy",
    "abstract": "Korean-English Machine Translation with Multiple Tokenization Strategy",
    "descriptor": "",
    "authors": [
      "Dojun Park",
      "Youngjin Jang",
      "Harksoo Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.14274"
  },
  {
    "id": "arXiv:2105.14277",
    "title": "Grammar Accuracy Evaluation (GAE): Quantifiable Quantitative Evaluation  of Machine Translation Models",
    "abstract": "Comments: accepted in the Journal of KIISE (JOK)",
    "descriptor": "\nComments: accepted in the Journal of KIISE (JOK)\n",
    "authors": [
      "Dojun Park",
      "Youngjin Jang",
      "Harksoo Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.14277"
  },
  {
    "id": "arXiv:2106.00215",
    "title": "Necessary conditions for feedback stabilization and safety",
    "abstract": "Comments: version accepted to Journal of Geometric Mechanics",
    "descriptor": "\nComments: version accepted to Journal of Geometric Mechanics\n",
    "authors": [
      "Matthew D. Kvalheim",
      "Daniel E. Koditschek"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)",
      "Mathematical Physics (math-ph)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.00215"
  },
  {
    "id": "arXiv:2106.07057",
    "title": "FairCanary: Rapid Continuous Explainable Fairness",
    "abstract": "Comments: Accepted as a full paper at the 2022 AAAI/ACM Conference on AI, Ethics, and Society",
    "descriptor": "\nComments: Accepted as a full paper at the 2022 AAAI/ACM Conference on AI, Ethics, and Society\n",
    "authors": [
      "Avijit Ghosh",
      "Aalok Shanbhag",
      "Christo Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.07057"
  },
  {
    "id": "arXiv:2106.09256",
    "title": "Seeing Differently, Acting Similarly: Heterogeneously Observable  Imitation Learning",
    "abstract": "Seeing Differently, Acting Similarly: Heterogeneously Observable  Imitation Learning",
    "descriptor": "",
    "authors": [
      "Xin-Qiang Cai",
      "Yao-Xiang Ding",
      "Zi-Xuan Chen",
      "Yuan Jiang",
      "Masashi Sugiyama",
      "Zhi-Hua Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.09256"
  },
  {
    "id": "arXiv:2106.10933",
    "title": "Semi-uniform Input-to-state Stability of Infinite-dimensional Systems",
    "abstract": "Comments: 28 pages",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Masashi Wakaiki"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.10933"
  },
  {
    "id": "arXiv:2107.00490",
    "title": "Data Deduplication with Random Substitutions",
    "abstract": "Data Deduplication with Random Substitutions",
    "descriptor": "",
    "authors": [
      "Hao Lou",
      "Farzad Farnoud"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2107.00490"
  },
  {
    "id": "arXiv:2107.03863",
    "title": "Benchpress: A Scalable and Versatile Workflow for Benchmarking Structure  Learning Algorithms",
    "abstract": "Comments: 39 pages, 8 figure",
    "descriptor": "\nComments: 39 pages, 8 figure\n",
    "authors": [
      "Felix L. Rios",
      "Giusi Moffa",
      "Jack Kuipers"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2107.03863"
  },
  {
    "id": "arXiv:2107.05870",
    "title": "Potential singularity of the 3D Euler equations in the interior domain",
    "abstract": "Comments: 37 pages. This paper has been accepted by Foundation of Computational Mathematics. arXiv admin note: text overlap with arXiv:2102.06663",
    "descriptor": "\nComments: 37 pages. This paper has been accepted by Foundation of Computational Mathematics. arXiv admin note: text overlap with arXiv:2102.06663\n",
    "authors": [
      "Thomas Y. Hou"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.05870"
  },
  {
    "id": "arXiv:2107.06011",
    "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object  Navigation",
    "abstract": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object  Navigation",
    "descriptor": "",
    "authors": [
      "Pierre Marza",
      "Laetitia Matignon",
      "Olivier Simonin",
      "Christian Wolf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2107.06011"
  },
  {
    "id": "arXiv:2107.06509",
    "title": "The potentially singular behavior of the 3D Navier-Stokes equations",
    "abstract": "Comments: 39 pages. This paper has been accepted by Foundation of Computational Mathematics. arXiv admin note: substantial text overlap with arXiv:2107.05870; text overlap with arXiv:2102.06663",
    "descriptor": "\nComments: 39 pages. This paper has been accepted by Foundation of Computational Mathematics. arXiv admin note: substantial text overlap with arXiv:2107.05870; text overlap with arXiv:2102.06663\n",
    "authors": [
      "Thomas Y. Hou"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.06509"
  },
  {
    "id": "arXiv:2107.13981",
    "title": "Classical Risk-Averse Control for a Finite-Horizon Borel Model",
    "abstract": "Comments: This version of the article makes almost-everywhere notions explicit",
    "descriptor": "\nComments: This version of the article makes almost-everywhere notions explicit\n",
    "authors": [
      "Margaret P. Chapman",
      "Kevin M. Smith"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2107.13981"
  },
  {
    "id": "arXiv:2108.01724",
    "title": "Approximating the Manifold Structure of Attributed Incentive Salience  from Large Scale Behavioural Data. A Representation Learning Approach Based  on Artificial Neural Networks",
    "abstract": "Approximating the Manifold Structure of Attributed Incentive Salience  from Large Scale Behavioural Data. A Representation Learning Approach Based  on Artificial Neural Networks",
    "descriptor": "",
    "authors": [
      "Valerio Bonometti",
      "Mathieu J. Ruiz",
      "Anders Drachen",
      "Alex Wade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.01724"
  },
  {
    "id": "arXiv:2108.11926",
    "title": "Re-using Adversarial Mask Discriminators for Test-time Training under  Distribution Shifts",
    "abstract": "Comments: Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) this https URL",
    "descriptor": "\nComments: Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) this https URL\n",
    "authors": [
      "Gabriele Valvano",
      "Andrea Leo",
      "Sotirios A. Tsaftaris"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2108.11926"
  },
  {
    "id": "arXiv:2109.01190",
    "title": "Assisting Decision Making in Scholarly Peer Review: A Preference  Learning Perspective",
    "abstract": "Assisting Decision Making in Scholarly Peer Review: A Preference  Learning Perspective",
    "descriptor": "",
    "authors": [
      "Nils Dycke",
      "Edwin Simpson",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.01190"
  },
  {
    "id": "arXiv:2109.04530",
    "title": "Notes on Generalizing the Maximum Entropy Principle to Uncertain Data",
    "abstract": "Comments: 11 pages",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Kenneth Bogert"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04530"
  },
  {
    "id": "arXiv:2109.07120",
    "title": "Infusing model predictive control into meta-reinforcement learning for  mobile robots in dynamic environments",
    "abstract": "Infusing model predictive control into meta-reinforcement learning for  mobile robots in dynamic environments",
    "descriptor": "",
    "authors": [
      "Jaeuk Shin",
      "Astghik Hakobyan",
      "Mingyu Park",
      "Yeoneung Kim",
      "Gihun Kim",
      "Insoon Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.07120"
  },
  {
    "id": "arXiv:2109.07934",
    "title": "Fast and Secure Routing Algorithms for Quantum Key Distribution Networks",
    "abstract": "Fast and Secure Routing Algorithms for Quantum Key Distribution Networks",
    "descriptor": "",
    "authors": [
      "Shahbaz Akhtar",
      "Krishnakumar G",
      "Vishnu B",
      "Abhishek Sinha"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2109.07934"
  },
  {
    "id": "arXiv:2109.08185",
    "title": "Optimal Partitioning of Non-Convex Environments for Minimum Turn  Coverage Planning",
    "abstract": "Comments: 8 pages, 9 figures, submitted to RA-L with IROS 2022 option",
    "descriptor": "\nComments: 8 pages, 9 figures, submitted to RA-L with IROS 2022 option\n",
    "authors": [
      "Megnath Ramesh",
      "Frank Imeson",
      "Baris Fidan",
      "Stephen L. Smith"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.08185"
  },
  {
    "id": "arXiv:2109.08844",
    "title": "Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks",
    "abstract": "Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks",
    "descriptor": "",
    "authors": [
      "Rahul Parhi",
      "Robert D. Nowak"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2109.08844"
  },
  {
    "id": "arXiv:2110.01822",
    "title": "Verified eigenvalue and eigenvector computations using complex moments  and the Rayleigh$\\unicode{x2013}$Ritz procedure for generalized Hermitian  eigenvalue problems",
    "abstract": "Comments: 22 pages, 4 figures",
    "descriptor": "\nComments: 22 pages, 4 figures\n",
    "authors": [
      "Akira Imakura",
      "Keiichi Morikuni",
      "Akitoshi Takayasu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.01822"
  },
  {
    "id": "arXiv:2110.03349",
    "title": "Real-time Nonlinear MPC Strategy with Full Vehicle Validation for  Autonomous Driving",
    "abstract": "Real-time Nonlinear MPC Strategy with Full Vehicle Validation for  Autonomous Driving",
    "descriptor": "",
    "authors": [
      "Jean Pierre Allamaa",
      "Petr Listov",
      "Herman Van der Auweraer",
      "Colin Jones",
      "Tong Duy Son"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03349"
  },
  {
    "id": "arXiv:2110.03891",
    "title": "Does Momentum Change the Implicit Regularization on Separable Data?",
    "abstract": "Does Momentum Change the Implicit Regularization on Separable Data?",
    "descriptor": "",
    "authors": [
      "Bohan Wang",
      "Qi Meng",
      "Huishuai Zhang",
      "Ruoyu Sun",
      "Wei Chen",
      "Zhi-Ming Ma",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03891"
  },
  {
    "id": "arXiv:2110.06160",
    "title": "Method to Build Equivalent Models of Microgrids for RMS Dynamic  Simulation of Power Systems",
    "abstract": "Comments: 7 pages with 5 figures, preprint accepted for presentation to 11th Bulk Power Systems Dynamics and Control Symposium (IREP 2022). In this version of the paper, the following authors worked on the design and development of the research: Rodrigo A. Ramos, Ahda P. Grilo-Pavani, Artur B. Piardi and Tatiane C. C. Fernandes",
    "descriptor": "\nComments: 7 pages with 5 figures, preprint accepted for presentation to 11th Bulk Power Systems Dynamics and Control Symposium (IREP 2022). In this version of the paper, the following authors worked on the design and development of the research: Rodrigo A. Ramos, Ahda P. Grilo-Pavani, Artur B. Piardi and Tatiane C. C. Fernandes\n",
    "authors": [
      "Rodrigo A. Ramos",
      "Ahda P. Grilo-Pavani",
      "Artur B. Piardi",
      "Tatiane C. C. Fernandes"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.06160"
  },
  {
    "id": "arXiv:2110.07112",
    "title": "On the Sample Complexity of Decentralized Linear Quadratic Regulator  with Partially Nested Information Structure",
    "abstract": "On the Sample Complexity of Decentralized Linear Quadratic Regulator  with Partially Nested Information Structure",
    "descriptor": "",
    "authors": [
      "Lintao Ye",
      "Hao Zhu",
      "Vijay Gupta"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07112"
  },
  {
    "id": "arXiv:2110.08935",
    "title": "InfAnFace: Bridging the infant-adult domain gap in facial landmark  estimation in the wild",
    "abstract": "InfAnFace: Bridging the infant-adult domain gap in facial landmark  estimation in the wild",
    "descriptor": "",
    "authors": [
      "Michael Wan",
      "Shaotong Zhu",
      "Lingfei Luan",
      "Gulati Prateek",
      "Xiaofei Huang",
      "Rebecca Schwartz-Mette",
      "Marie Hayes",
      "Emily Zimmerman",
      "Sarah Ostadabbas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08935"
  },
  {
    "id": "arXiv:2110.10206",
    "title": "IMPROVE Visiolinguistic Performance with Re-Query",
    "abstract": "Comments: 21 pages, 5 tables, 6 figures",
    "descriptor": "\nComments: 21 pages, 5 tables, 6 figures\n",
    "authors": [
      "Stephan J. Lemmer",
      "Jason J. Corso"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.10206"
  },
  {
    "id": "arXiv:2110.13285",
    "title": "Generative Flows as a General Purpose Solution for Inverse Problems",
    "abstract": "Comments: 11 pages, 6 figures",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Jos\u00e9 A. Ch\u00e1vez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13285"
  },
  {
    "id": "arXiv:2110.13674",
    "title": "C$^2$SP-Net: Joint Compression and Classification Network for Epilepsy  Seizure Prediction",
    "abstract": "C$^2$SP-Net: Joint Compression and Classification Network for Epilepsy  Seizure Prediction",
    "descriptor": "",
    "authors": [
      "Di Wu",
      "Yi Shi",
      "Ziyu Wang",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.13674"
  },
  {
    "id": "arXiv:2111.02358",
    "title": "VLMo: Unified Vision-Language Pre-Training with  Mixture-of-Modality-Experts",
    "abstract": "Comments: Work in progress",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Hangbo Bao",
      "Wenhui Wang",
      "Li Dong",
      "Qiang Liu",
      "Owais Khan Mohammed",
      "Kriti Aggarwal",
      "Subhojit Som",
      "Furu Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.02358"
  },
  {
    "id": "arXiv:2111.02915",
    "title": "Local Compatibility Boundary Conditions for High-Order Accurate  Finite-Difference Approximations of PDEs",
    "abstract": "Local Compatibility Boundary Conditions for High-Order Accurate  Finite-Difference Approximations of PDEs",
    "descriptor": "",
    "authors": [
      "Nour G. Al Hassanieh",
      "Jeffrey W. Banks",
      "William D. Henshaw",
      "Donald W. Schwendeman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.02915"
  },
  {
    "id": "arXiv:2111.04635",
    "title": "CORE: a Complex Event Recognition Engine",
    "abstract": "Comments: 30 pages, 11 figures",
    "descriptor": "\nComments: 30 pages, 11 figures\n",
    "authors": [
      "Marco Bucchi",
      "Alejandro Grez",
      "Andr\u00e9s Quintana",
      "Cristian Riveros",
      "Stijn Vansummeren"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.04635"
  },
  {
    "id": "arXiv:2111.10221",
    "title": "Semi-Supervised Domain Generalization in Real World: New Benchmark and  Strong Baseline",
    "abstract": "Comments: 13 pages, 13 figures",
    "descriptor": "\nComments: 13 pages, 13 figures\n",
    "authors": [
      "Luojun Lin",
      "Han Xie",
      "Zhifeng Yang",
      "Zhishu Sun",
      "Wenxi Liu",
      "Yuanlong Yu",
      "Weijie Chen",
      "Shicai Yang",
      "Di Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.10221"
  },
  {
    "id": "arXiv:2111.10281",
    "title": "A new class of MDS symbol-pair codes",
    "abstract": "Comments: The paper has some mistakes, which can not be revised",
    "descriptor": "\nComments: The paper has some mistakes, which can not be revised\n",
    "authors": [
      "Canze Zhu",
      "Qunying Liao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2111.10281"
  },
  {
    "id": "arXiv:2111.11798",
    "title": "Composing Partial Differential Equations with Physics-Aware Neural  Networks",
    "abstract": "Comments: Accepted at ICML2022. Code available at this https URL",
    "descriptor": "\nComments: Accepted at ICML2022. Code available at this https URL\n",
    "authors": [
      "Matthias Karlbauer",
      "Timothy Praditia",
      "Sebastian Otte",
      "Sergey Oladyshkin",
      "Wolfgang Nowak",
      "Martin V. Butz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.11798"
  },
  {
    "id": "arXiv:2111.12157",
    "title": "A Bayesian Model for Online Activity Sample Sizes",
    "abstract": "Comments: 10 pages, 7 figures",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Thomas Richardson",
      "Yu Liu",
      "James McQueen",
      "Doug Hains"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.12157"
  },
  {
    "id": "arXiv:2111.12965",
    "title": "Towards Practical Deployment-Stage Backdoor Attack on Deep Neural  Networks",
    "abstract": "Towards Practical Deployment-Stage Backdoor Attack on Deep Neural  Networks",
    "descriptor": "",
    "authors": [
      "Xiangyu Qi",
      "Tinghao Xie",
      "Ruizhe Pan",
      "Jifeng Zhu",
      "Yong Yang",
      "Kai Bu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.12965"
  },
  {
    "id": "arXiv:2111.15129",
    "title": "Anonymization for Skeleton Action Recognition",
    "abstract": "Anonymization for Skeleton Action Recognition",
    "descriptor": "",
    "authors": [
      "Saemi Moon",
      "Myeonghyeon Kim",
      "Zhenyue Qin",
      "Yang Liu",
      "Dongwoo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.15129"
  },
  {
    "id": "arXiv:2112.04415",
    "title": "On the Ergodic Mutual Information of Keyhole MIMO Channels With  Finite-Alphabet Inputs",
    "abstract": "Comments: 5 pages",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Chongjun Ouyang",
      "Ali Bereyhi",
      "Saba Asaad",
      "Ralf R. M\u00fcller",
      "Julian Cheng",
      "Hongwen Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.04415"
  },
  {
    "id": "arXiv:2112.07632",
    "title": "Homological approximations in persistence theory",
    "abstract": "Comments: v2 (sizable update): added numerous references, reorganized paper, added new section on motivation and related work (Section 3), expanded upon the relationship between homological invariants and dimensions of hom-spaces (Theorem 1.1), extended main Theorem 1.2 (formerly Theorem 1.1), corrected errors in comparisons to other invariants (Section 7). 23 pages, comments welcome!",
    "descriptor": "\nComments: v2 (sizable update): added numerous references, reorganized paper, added new section on motivation and related work (Section 3), expanded upon the relationship between homological invariants and dimensions of hom-spaces (Theorem 1.1), extended main Theorem 1.2 (formerly Theorem 1.1), corrected errors in comparisons to other invariants (Section 7). 23 pages, comments welcome!\n",
    "authors": [
      "Benjamin Blanchette",
      "Thomas Br\u00fcstle",
      "Eric J. Hanson"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)",
      "Representation Theory (math.RT)"
    ],
    "url": "https://arxiv.org/abs/2112.07632"
  },
  {
    "id": "arXiv:2112.08217",
    "title": "Probabilistic Forecasting with Generative Networks via Scoring Rule  Minimization",
    "abstract": "Probabilistic Forecasting with Generative Networks via Scoring Rule  Minimization",
    "descriptor": "",
    "authors": [
      "Lorenzo Pacchiardi",
      "Rilwan Adewoyin",
      "Peter Dueben",
      "Ritabrata Dutta"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.08217"
  },
  {
    "id": "arXiv:2112.11324",
    "title": "Satellite-Based Communications Security: A Survey of Threats, Solutions,  and Research Challenges",
    "abstract": "Comments: 67 pages",
    "descriptor": "\nComments: 67 pages\n",
    "authors": [
      "Pietro Tedeschi",
      "Savio Sciancalepore",
      "Roberto Di Pietro"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2112.11324"
  },
  {
    "id": "arXiv:2112.11937",
    "title": "Adversarial Deep Reinforcement Learning for Improving the Robustness of  Multi-agent Autonomous Driving Policies",
    "abstract": "Adversarial Deep Reinforcement Learning for Improving the Robustness of  Multi-agent Autonomous Driving Policies",
    "descriptor": "",
    "authors": [
      "Aizaz Sharif",
      "Dusica Marijan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.11937"
  },
  {
    "id": "arXiv:2112.11947",
    "title": "Evaluating the Robustness of Deep Reinforcement Learning for Autonomous  and Adversarial Policies in a Multi-agent Urban Driving Environment",
    "abstract": "Evaluating the Robustness of Deep Reinforcement Learning for Autonomous  and Adversarial Policies in a Multi-agent Urban Driving Environment",
    "descriptor": "",
    "authors": [
      "Aizaz Sharif",
      "Dusica Marijan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.11947"
  },
  {
    "id": "arXiv:2112.13236",
    "title": "An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass  Malware Classification",
    "abstract": "Comments: 34 pages, 7 Figures, 11 Tables",
    "descriptor": "\nComments: 34 pages, 7 Figures, 11 Tables\n",
    "authors": [
      "Ferhat Demirk\u0131ran",
      "Aykut \u00c7ay\u0131r",
      "U\u011fur \u00dcnal",
      "Hasan Da\u011f"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.13236"
  },
  {
    "id": "arXiv:2112.15139",
    "title": "Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural  Networks",
    "abstract": "Comments: Accepted at ICML 2022",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Runpei Dong",
      "Zhanhong Tan",
      "Mengdi Wu",
      "Linfeng Zhang",
      "Kaisheng Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.15139"
  },
  {
    "id": "arXiv:2112.15400",
    "title": "A Theoretical Understanding of Gradient Bias in Meta-Reinforcement  Learning",
    "abstract": "A Theoretical Understanding of Gradient Bias in Meta-Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Bo Liu",
      "Xidong Feng",
      "Haifeng Zhang",
      "Jun Wang",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.15400"
  },
  {
    "id": "arXiv:2201.00680",
    "title": "A Comprehensive Survey on Radio Frequency (RF) Fingerprinting:  Traditional Approaches, Deep Learning, and Open Challenges",
    "abstract": "Comments: Preprint under review",
    "descriptor": "\nComments: Preprint under review\n",
    "authors": [
      "Anu Jagannath",
      "Jithin Jagannath",
      "Prem Sagar Pattanshetty Vasanth Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.00680"
  },
  {
    "id": "arXiv:2201.01422",
    "title": "On the Performance of Uplink ISAC Systems",
    "abstract": "Comments: 5 pages",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Chongjun Ouyang",
      "Yuanwei Liu",
      "Hongwen Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.01422"
  },
  {
    "id": "arXiv:2201.02143",
    "title": "Classification of Long Sequential Data using Circular Dilated  Convolutional Neural Networks",
    "abstract": "Classification of Long Sequential Data using Circular Dilated  Convolutional Neural Networks",
    "descriptor": "",
    "authors": [
      "Lei Cheng",
      "Ruslan Khalitov",
      "Tong Yu",
      "Zhirong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.02143"
  },
  {
    "id": "arXiv:2201.03812",
    "title": "Bootstrapping Informative Graph Augmentation via A Meta Learning  Approach",
    "abstract": "Comments: Accepted by International Joint Conference on Artificial Intelligence (IJCAI) 2022",
    "descriptor": "\nComments: Accepted by International Joint Conference on Artificial Intelligence (IJCAI) 2022\n",
    "authors": [
      "Hang Gao",
      "Jiangmeng Li",
      "Wenwen Qiang",
      "Lingyu Si",
      "Fuchun Sun",
      "Changwen Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.03812"
  },
  {
    "id": "arXiv:2201.11860",
    "title": "On the Anonymity of Peer-To-Peer Network Anonymity Schemes Used by  Cryptocurrencies",
    "abstract": "On the Anonymity of Peer-To-Peer Network Anonymity Schemes Used by  Cryptocurrencies",
    "descriptor": "",
    "authors": [
      "Piyush Kumar Sharma",
      "Devashish Gosain",
      "Claudia Diaz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.11860"
  },
  {
    "id": "arXiv:2202.01336",
    "title": "Exploring Transformer Backbones for Heterogeneous Treatment Effect  Estimation",
    "abstract": "Exploring Transformer Backbones for Heterogeneous Treatment Effect  Estimation",
    "descriptor": "",
    "authors": [
      "Yi-Fan Zhang",
      "Hanlin Zhang",
      "Zachary C. Lipton",
      "Li Erran Li",
      "Eric P. Xing"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.01336"
  },
  {
    "id": "arXiv:2202.01402",
    "title": "GALAXY: Graph-based Active Learning at the Extreme",
    "abstract": "GALAXY: Graph-based Active Learning at the Extreme",
    "descriptor": "",
    "authors": [
      "Jifan Zhang",
      "Julian Katz-Samuels",
      "Robert Nowak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.01402"
  },
  {
    "id": "arXiv:2202.02444",
    "title": "Spelunking the Deep: Guaranteed Queries on General Neural Implicit  Surfaces via Range Analysis",
    "abstract": "Comments: appearing in ACM Transactions on Graphics / SIGGRAPH 2022 Journal Papers",
    "descriptor": "\nComments: appearing in ACM Transactions on Graphics / SIGGRAPH 2022 Journal Papers\n",
    "authors": [
      "Nicholas Sharp",
      "Alec Jacobson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.02444"
  },
  {
    "id": "arXiv:2202.02468",
    "title": "Rethinking ValueDice: Does It Really Improve Performance?",
    "abstract": "Comments: This paper appeared at the blog track of the 10th international conference on learning representations (ICLR), 2022. Link: this https URL",
    "descriptor": "\nComments: This paper appeared at the blog track of the 10th international conference on learning representations (ICLR), 2022. Link: this https URL\n",
    "authors": [
      "Ziniu Li",
      "Tian Xu",
      "Yang Yu",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.02468"
  },
  {
    "id": "arXiv:2202.02684",
    "title": "On the Multi-View Information Bottleneck Representation",
    "abstract": "On the Multi-View Information Bottleneck Representation",
    "descriptor": "",
    "authors": [
      "Teng-Hui Huang",
      "Aly El Gamal",
      "Hesham El Gamal"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2202.02684"
  },
  {
    "id": "arXiv:2202.07453",
    "title": "Random Walks for Adversarial Meshes",
    "abstract": "Random Walks for Adversarial Meshes",
    "descriptor": "",
    "authors": [
      "Amir Belder",
      "Gal Yefet",
      "Ran Ben Izhak",
      "Ayellet Tal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.07453"
  },
  {
    "id": "arXiv:2202.07880",
    "title": "$\\textsc{CIS}^2}: A Simplified Commonsense Inference Evaluation for  Story Prose",
    "abstract": "Comments: Published at the Workshop on Commonsense Representation and Reasoning (CSRR) @ ACL 2022",
    "descriptor": "\nComments: Published at the Workshop on Commonsense Representation and Reasoning (CSRR) @ ACL 2022\n",
    "authors": [
      "Bryan Li",
      "Lara J. Martin",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.07880"
  },
  {
    "id": "arXiv:2202.07952",
    "title": "TimeREISE: Time-series Randomized Evolving Input Sample Explanation",
    "abstract": "Comments: 8 pages, 6 figures, 6 tables",
    "descriptor": "\nComments: 8 pages, 6 figures, 6 tables\n",
    "authors": [
      "Dominique Mercier",
      "Andreas Dengel",
      "Sheraz Ahmed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.07952"
  },
  {
    "id": "arXiv:2202.12917",
    "title": "Learning English with Peppa Pig",
    "abstract": "Comments: Accepted to TACL",
    "descriptor": "\nComments: Accepted to TACL\n",
    "authors": [
      "Mitja Nikolaus",
      "Afra Alishahi",
      "Grzegorz Chrupa\u0142a"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2202.12917"
  },
  {
    "id": "arXiv:2203.01157",
    "title": "Artificial Concepts of Artificial Intelligence: Institutional Compliance  and Resistance in AI Startups",
    "abstract": "Artificial Concepts of Artificial Intelligence: Institutional Compliance  and Resistance in AI Startups",
    "descriptor": "",
    "authors": [
      "Amy A. Winecoff",
      "Elizabeth Anne Watkins"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2203.01157"
  },
  {
    "id": "arXiv:2203.01216",
    "title": "A Simple and Universal Rotation Equivariant Point-cloud Network",
    "abstract": "A Simple and Universal Rotation Equivariant Point-cloud Network",
    "descriptor": "",
    "authors": [
      "Ben Finkelshtein",
      "Chaim Baskin",
      "Haggai Maron",
      "Nadav Dym"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.01216"
  },
  {
    "id": "arXiv:2203.01414",
    "title": "ICARUS: A Specialized Architecture for Neural Radiance Field Rendering",
    "abstract": "ICARUS: A Specialized Architecture for Neural Radiance Field Rendering",
    "descriptor": "",
    "authors": [
      "Chaolin Rao",
      "Huangjie Yu",
      "Haochuan Wan",
      "Jindong Zhou",
      "Yueyang Zheng",
      "Yu Ma",
      "Anpei Chen",
      "Minye Wu",
      "Binzhe Yuan",
      "Pingqiang Zhou",
      "Xin Lou",
      "Jingyi Yu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2203.01414"
  },
  {
    "id": "arXiv:2203.02128",
    "title": "Distributionally Robust Bayesian Optimization with $\u03c6$-divergences",
    "abstract": "Comments: 19 pages",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Hisham Husain",
      "Vu Nguyen",
      "Anton van den Hengel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.02128"
  },
  {
    "id": "arXiv:2203.03397",
    "title": "OverlapTransformer: An Efficient and Rotation-Invariant Transformer  Network for LiDAR-Based Place Recognition",
    "abstract": "Comments: Accepted by RAL/IROS 2022",
    "descriptor": "\nComments: Accepted by RAL/IROS 2022\n",
    "authors": [
      "Junyi Ma",
      "Jun Zhang",
      "Jintao Xu",
      "Rui Ai",
      "Weihao Gu",
      "Xieyuanli Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.03397"
  },
  {
    "id": "arXiv:2203.03937",
    "title": "Dynamic Group Transformer: A General Vision Transformer Backbone with  Dynamic Group Attention",
    "abstract": "Comments: 11 pages, 6 figures. Accepted by IJCAI 2022",
    "descriptor": "\nComments: 11 pages, 6 figures. Accepted by IJCAI 2022\n",
    "authors": [
      "Kai Liu",
      "Tianyi Wu",
      "Cong Liu",
      "Guodong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.03937"
  },
  {
    "id": "arXiv:2203.04510",
    "title": "ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling",
    "abstract": "Comments: Accepted for the $38^{\\text {th }}$ Conference on Uncertainty in Artificial Intelligence (UAI 2022)",
    "descriptor": "\nComments: Accepted for the $38^{\\text {th }}$ Conference on Uncertainty in Artificial Intelligence (UAI 2022)\n",
    "authors": [
      "Subhojyoti Mukherjee",
      "Josiah P. Hanna",
      "Robert Nowak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.04510"
  },
  {
    "id": "arXiv:2203.06649",
    "title": "Joint rotational invariance and adversarial training of a dual-stream  Transformer yields state of the art Brain-Score for Area V4",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "William Berrios",
      "Arturo Deza"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2203.06649"
  },
  {
    "id": "arXiv:2203.07889",
    "title": "Comparing two samples through stochastic dominance: a graphical approach",
    "abstract": "Comparing two samples through stochastic dominance: a graphical approach",
    "descriptor": "",
    "authors": [
      "Etor Arza",
      "Josu Ceberio",
      "Ekhi\u00f1e Irurozki",
      "Aritz P\u00e9rez"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2203.07889"
  },
  {
    "id": "arXiv:2203.11207",
    "title": "Hybrid training of optical neural networks",
    "abstract": "Hybrid training of optical neural networks",
    "descriptor": "",
    "authors": [
      "James Spall",
      "Xianxin Guo",
      "A. I. Lvovsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Image and Video Processing (eess.IV)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2203.11207"
  },
  {
    "id": "arXiv:2203.12577",
    "title": "Minimax Regret for Cascading Bandits",
    "abstract": "Comments: This version includes new results for the linear case, simpler proofs in the tabular case, and new simulations",
    "descriptor": "\nComments: This version includes new results for the linear case, simpler proofs in the tabular case, and new simulations\n",
    "authors": [
      "Daniel Vial",
      "Sujay Sanghavi",
      "Sanjay Shakkottai",
      "R. Srikant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.12577"
  },
  {
    "id": "arXiv:2203.12803",
    "title": "A Two-Stage Federated Transfer Learning Framework in Medical Images  Classification on Limited Data: A COVID-19 Case Study",
    "abstract": "Comments: 10 pages, 11 figures",
    "descriptor": "\nComments: 10 pages, 11 figures\n",
    "authors": [
      "Alexandros Shikun Zhang",
      "Naomi Fengqi Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.12803"
  },
  {
    "id": "arXiv:2203.13071",
    "title": "Inner and Outer Approximations of Star-Convex Semialgebraic Sets",
    "abstract": "Comments: Original: 6 pages, 3 figures. Rev1: 8 pages, 4 figures. Minor correction to Lemma 5 (previously Lemma 4). New Lemma 3 and supporting example (D). New discussion regarding computational complexity",
    "descriptor": "\nComments: Original: 6 pages, 3 figures. Rev1: 8 pages, 4 figures. Minor correction to Lemma 5 (previously Lemma 4). New Lemma 3 and supporting example (D). New discussion regarding computational complexity\n",
    "authors": [
      "James Guthrie"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.13071"
  },
  {
    "id": "arXiv:2203.13207",
    "title": "Supervised Training of Siamese Spiking Neural Networks with Earth  Mover's Distance",
    "abstract": "Comments: Revised paper accepted for presentation at 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "descriptor": "\nComments: Revised paper accepted for presentation at 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n",
    "authors": [
      "Mateusz Pabian",
      "Dominik Rzepka",
      "Miros\u0142aw Pawlak"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.13207"
  },
  {
    "id": "arXiv:2203.13457",
    "title": "Chaos is a Ladder: A New Theoretical Understanding of Contrastive  Learning via Augmentation Overlap",
    "abstract": "Comments: Accepeted by ICLR 2022",
    "descriptor": "\nComments: Accepeted by ICLR 2022\n",
    "authors": [
      "Yifei Wang",
      "Qi Zhang",
      "Yisen Wang",
      "Jiansheng Yang",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.13457"
  },
  {
    "id": "arXiv:2203.14057",
    "title": "FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable  Model from a Hybrid Dataset",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Lizhen Wang",
      "Zhiyuan Chen",
      "Tao Yu",
      "Chenguang Ma",
      "Liang Li",
      "Yebin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.14057"
  },
  {
    "id": "arXiv:2203.14402",
    "title": "UV Volumes for Real-time Rendering of Editable Free-view Human  Performance",
    "abstract": "UV Volumes for Real-time Rendering of Editable Free-view Human  Performance",
    "descriptor": "",
    "authors": [
      "Yue Chen",
      "Xuan Wang",
      "Xingyu Chen",
      "Qi Zhang",
      "Xiaoyu Li",
      "Yu Guo",
      "Jue Wang",
      "Fei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.14402"
  },
  {
    "id": "arXiv:2204.00505",
    "title": "Multi Jet Fusion of Nylon-12: A Viable Method to 3D-print Concentric  Tube Robots?",
    "abstract": "Comments: in press; K. Picho and B. Persons contributed equally to this paper",
    "descriptor": "\nComments: in press; K. Picho and B. Persons contributed equally to this paper\n",
    "authors": [
      "Kalani Picho",
      "Brandon Persons",
      "Jesse F. d'Almeida",
      "Nicholas E. Pacheco",
      "Colin Reynolds",
      "Loris Fichera"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.00505"
  },
  {
    "id": "arXiv:2204.00598",
    "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Andy Zeng",
      "Maria Attarian",
      "Brian Ichter",
      "Krzysztof Choromanski",
      "Adrian Wong",
      "Stefan Welker",
      "Federico Tombari",
      "Aveek Purohit",
      "Michael Ryoo",
      "Vikas Sindhwani",
      "Johnny Lee",
      "Vincent Vanhoucke",
      "Pete Florence"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.00598"
  },
  {
    "id": "arXiv:2204.01033",
    "title": "Two iterative formulas of largest and smallest singular value of  nonsingular matrices",
    "abstract": "Two iterative formulas of largest and smallest singular value of  nonsingular matrices",
    "descriptor": "",
    "authors": [
      "Shun Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.01033"
  },
  {
    "id": "arXiv:2204.01102",
    "title": "Formal Privacy for Partially Private Data",
    "abstract": "Comments: 24 pages, 5 figures; submitted to NeurIPS 2022",
    "descriptor": "\nComments: 24 pages, 5 figures; submitted to NeurIPS 2022\n",
    "authors": [
      "Jeremy Seeman",
      "Matthew Reimherr",
      "Aleksandra Slavkovic"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2204.01102"
  },
  {
    "id": "arXiv:2204.01643",
    "title": "Characterizing Parametric and Convergence Stability in Nonconvex and  Nonsmooth Optimizations: A Geometric Approach",
    "abstract": "Comments: submitted (fix some bugs)",
    "descriptor": "\nComments: submitted (fix some bugs)\n",
    "authors": [
      "Xiaotie Deng",
      "Hanyu Li",
      "Ningyuan Li"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2204.01643"
  },
  {
    "id": "arXiv:2204.02128",
    "title": "Computing in Anonymous Dynamic Networks Is Linear",
    "abstract": "Comments: 31 pages, 8 figures",
    "descriptor": "\nComments: 31 pages, 8 figures\n",
    "authors": [
      "Giuseppe A. Di Luna",
      "Giovanni Viglietta"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2204.02128"
  },
  {
    "id": "arXiv:2204.04416",
    "title": "E^2TAD: An Energy-Efficient Tracking-based Action Detector",
    "abstract": "E^2TAD: An Energy-Efficient Tracking-based Action Detector",
    "descriptor": "",
    "authors": [
      "Xin Hu",
      "Zhenyu Wu",
      "Hao-Yu Miao",
      "Siqi Fan",
      "Taiyu Long",
      "Zhenyu Hu",
      "Pengcheng Pi",
      "Yi Wu",
      "Zhou Ren",
      "Zhangyang Wang",
      "Gang Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.04416"
  },
  {
    "id": "arXiv:2204.05080",
    "title": "Semantic Exploration from Language Abstractions and Pretrained  Representations",
    "abstract": "Semantic Exploration from Language Abstractions and Pretrained  Representations",
    "descriptor": "",
    "authors": [
      "Allison C. Tam",
      "Neil C. Rabinowitz",
      "Andrew K. Lampinen",
      "Nicholas A. Roy",
      "Stephanie C. Y. Chan",
      "DJ Strouse",
      "Jane X. Wang",
      "Andrea Banino",
      "Felix Hill"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.05080"
  },
  {
    "id": "arXiv:2204.07293",
    "title": "Towards a Unified Framework for Uncertainty-aware Nonlinear Variable  Selection with Theoretical Guarantees",
    "abstract": "Comments: 50 pages, 16 figures, 11 tables",
    "descriptor": "\nComments: 50 pages, 16 figures, 11 tables\n",
    "authors": [
      "Wenying Deng",
      "Beau Coker",
      "Rajarshi Mukherjee",
      "Jeremiah Zhe Liu",
      "Brent A. Coull"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.07293"
  },
  {
    "id": "arXiv:2204.07485",
    "title": "Big-means: Less is More for K-means Clustering",
    "abstract": "Big-means: Less is More for K-means Clustering",
    "descriptor": "",
    "authors": [
      "Rustam Mussabayev",
      "Nenad Mladenovic",
      "Ravil Mussabayev",
      "Bassem Jarboui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.07485"
  },
  {
    "id": "arXiv:2204.07615",
    "title": "TabNAS: Rejection Sampling for Neural Architecture Search on Tabular  Datasets",
    "abstract": "Comments: 29 pages, 15 figures, 4 tables",
    "descriptor": "\nComments: 29 pages, 15 figures, 4 tables\n",
    "authors": [
      "Chengrun Yang",
      "Gabriel Bender",
      "Hanxiao Liu",
      "Pieter-Jan Kindermans",
      "Madeleine Udell",
      "Yifeng Lu",
      "Quoc Le",
      "Da Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.07615"
  },
  {
    "id": "arXiv:2204.07921",
    "title": "Fast Multi-grid Methods for Minimizing Curvature Energy",
    "abstract": "Fast Multi-grid Methods for Minimizing Curvature Energy",
    "descriptor": "",
    "authors": [
      "Zhenwei Zhang",
      "Ke Chen",
      "Yuping Duan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.07921"
  },
  {
    "id": "arXiv:2204.10586",
    "title": "Efficient Training of Neural Transducer for Speech Recognition",
    "abstract": "Comments: submitted to Interspeech 2022",
    "descriptor": "\nComments: submitted to Interspeech 2022\n",
    "authors": [
      "Wei Zhou",
      "Wilfried Michel",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.10586"
  },
  {
    "id": "arXiv:2204.11127",
    "title": "U-NO: U-shaped Neural Operators",
    "abstract": "U-NO: U-shaped Neural Operators",
    "descriptor": "",
    "authors": [
      "Md Ashiqur Rahman",
      "Zachary E. Ross",
      "Kamyar Azizzadenesheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.11127"
  },
  {
    "id": "arXiv:2204.14057",
    "title": "Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype  Contrast",
    "abstract": "Comments: 8 pages, 4 figures. Accepted by IJCAI-2022",
    "descriptor": "\nComments: 8 pages, 4 figures. Accepted by IJCAI-2022\n",
    "authors": [
      "Boqing Zhu",
      "Kele Xu",
      "Changjian Wang",
      "Zheng Qin",
      "Tao Sun",
      "Huaimin Wang",
      "Yuxing Peng"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.14057"
  },
  {
    "id": "arXiv:2204.14217",
    "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical  Transformers",
    "abstract": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical  Transformers",
    "descriptor": "",
    "authors": [
      "Ming Ding",
      "Wendi Zheng",
      "Wenyi Hong",
      "Jie Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.14217"
  },
  {
    "id": "arXiv:2205.02388",
    "title": "Interactive Grounded Language Understanding in a Collaborative  Environment: IGLU 2021",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2110.06536",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2110.06536\n",
    "authors": [
      "Julia Kiseleva",
      "Ziming Li",
      "Mohammad Aliannejadi",
      "Shrestha Mohanty",
      "Maartje ter Hoeve",
      "Mikhail Burtsev",
      "Alexey Skrynnik",
      "Artem Zholus",
      "Aleksandr Panov",
      "Kavya Srinet",
      "Arthur Szlam",
      "Yuxuan Sun",
      "Marc-Alexandre C\u00f4t\u00e9",
      "Katja Hofmann",
      "Ahmed Awadallah",
      "Linar Abdrazakov",
      "Igor Churin",
      "Putra Manggala",
      "Kata Naszadi",
      "Michiel van der Meer",
      "Taewoon Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.02388"
  },
  {
    "id": "arXiv:2205.02754",
    "title": "Accelerating Path Planning for Autonomous Driving with Hardware-Assisted  Memoization",
    "abstract": "Accelerating Path Planning for Autonomous Driving with Hardware-Assisted  Memoization",
    "descriptor": "",
    "authors": [
      "Mulong Luo",
      "G. Edward Suh"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.02754"
  },
  {
    "id": "arXiv:2205.05167",
    "title": "Robustness of Humans and Machines on Object Recognition with Extreme  Image Transformations",
    "abstract": "Comments: Accepted at CVPR NeuroVision Workshop",
    "descriptor": "\nComments: Accepted at CVPR NeuroVision Workshop\n",
    "authors": [
      "Dakarai Crowder",
      "Girik Malik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.05167"
  },
  {
    "id": "arXiv:2205.05980",
    "title": "\"Teaching Independent Parts Separately\" (TIPSy-GAN) : Improving Accuracy  and Stability in Unsupervised Adversarial 2D to 3D Pose Estimation",
    "abstract": "\"Teaching Independent Parts Separately\" (TIPSy-GAN) : Improving Accuracy  and Stability in Unsupervised Adversarial 2D to 3D Pose Estimation",
    "descriptor": "",
    "authors": [
      "Peter Hardy",
      "Srinandan Dasmahapatra",
      "Hansung Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.05980"
  },
  {
    "id": "arXiv:2205.06440",
    "title": "Exploiting Variational Domain-Invariant User Embedding for Partially  Overlapped Cross Domain Recommendation",
    "abstract": "Exploiting Variational Domain-Invariant User Embedding for Partially  Overlapped Cross Domain Recommendation",
    "descriptor": "",
    "authors": [
      "Weiming Liu",
      "Xiaolin Zheng",
      "Mengling Hu",
      "Chaochao Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.06440"
  },
  {
    "id": "arXiv:2205.06872",
    "title": "LASSO reloaded: a variational analysis perspective with applications to  compressed sensing",
    "abstract": "LASSO reloaded: a variational analysis perspective with applications to  compressed sensing",
    "descriptor": "",
    "authors": [
      "Aaron Berk",
      "Simone Brugiapaglia",
      "Tim Hoheisel"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.06872"
  },
  {
    "id": "arXiv:2205.09511",
    "title": "The Impact of COVID-19 Pandemic on LGBTQ Online Communities",
    "abstract": "Comments: 11 pages, 9 figures",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Yunhao Yuan",
      "Gaurav Verma",
      "Barbara Keller",
      "Talayeh Aledavood"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09511"
  },
  {
    "id": "arXiv:2205.09702",
    "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency  Analysis",
    "abstract": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency  Analysis",
    "descriptor": "",
    "authors": [
      "Maciej Besta",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.09702"
  },
  {
    "id": "arXiv:2205.09898",
    "title": "Let the Model Decide its Curriculum for Multitask Learning",
    "abstract": "Comments: NAACL 2022 Deep Learning for Low-Resource NLP Workshop",
    "descriptor": "\nComments: NAACL 2022 Deep Learning for Low-Resource NLP Workshop\n",
    "authors": [
      "Neeraj Varshney",
      "Swaroop Mishra",
      "Chitta Baral"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09898"
  },
  {
    "id": "arXiv:2205.09909",
    "title": "Sparse Infinite Random Feature Latent Variable Modeling",
    "abstract": "Sparse Infinite Random Feature Latent Variable Modeling",
    "descriptor": "",
    "authors": [
      "Michael Minyi Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09909"
  },
  {
    "id": "arXiv:2205.09956",
    "title": "Structured Attention Composition for Temporal Action Localization",
    "abstract": "Comments: Accepted by T-IP",
    "descriptor": "\nComments: Accepted by T-IP\n",
    "authors": [
      "Le Yang",
      "Junwei Han",
      "Tao Zhao",
      "Nian Liu",
      "Dingwen Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09956"
  },
  {
    "id": "arXiv:2205.10337",
    "title": "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes",
    "abstract": "Comments: Alexander and Andr\\'e share the first authorship, all authors made significant technical contributions to this work",
    "descriptor": "\nComments: Alexander and Andr\\'e share the first authorship, all authors made significant technical contributions to this work\n",
    "authors": [
      "Alexander Kolesnikov",
      "Andr\u00e9 Susano Pinto",
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Jeremiah Harmsen",
      "Neil Houlsby"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10337"
  },
  {
    "id": "arXiv:2205.10431",
    "title": "Learning Dense Reward with Temporal Variant Self-Supervision",
    "abstract": "Comments: 4 pages, 6 figures, accepted to ICRA 2022 RL for Contact-Rich Manipulation Workshop",
    "descriptor": "\nComments: 4 pages, 6 figures, accepted to ICRA 2022 RL for Contact-Rich Manipulation Workshop\n",
    "authors": [
      "Yuning Wu",
      "Jieliang Luo",
      "Hui Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10431"
  },
  {
    "id": "arXiv:2205.11211",
    "title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation",
    "abstract": "Comments: work in progress",
    "descriptor": "\nComments: work in progress\n",
    "authors": [
      "Yichao Du",
      "Weizhi Wang",
      "Zhirui Zhang",
      "Boxing Chen",
      "Tong Xu",
      "Jun Xie",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11211"
  },
  {
    "id": "arXiv:2205.12168",
    "title": "Competitive Prediction-Aware Online Algorithms for Energy Generation  Scheduling in Microgrids",
    "abstract": "Comments: This paper has been accepted into ACM e-Energy 2022 and will appear in the conference proceedings",
    "descriptor": "\nComments: This paper has been accepted into ACM e-Energy 2022 and will appear in the conference proceedings\n",
    "authors": [
      "Ali Menati",
      "Sid Chi-Kin Chau",
      "Minghua Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12168"
  },
  {
    "id": "arXiv:2205.12667",
    "title": "Trilateration-Based Device-Free Sensing: Two Base Stations and One  Passive IRS Are Sufficient",
    "abstract": "Comments: submitted for possible publication",
    "descriptor": "\nComments: submitted for possible publication\n",
    "authors": [
      "Qipeng Wang",
      "Liang Liu",
      "Shuowen Zhang",
      "Francis C. M. Lau"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12667"
  },
  {
    "id": "arXiv:2205.13326",
    "title": "SHREC 2022: pothole and crack detection in the road pavement using  images and RGB-D data",
    "abstract": "SHREC 2022: pothole and crack detection in the road pavement using  images and RGB-D data",
    "descriptor": "",
    "authors": [
      "Elia Moscoso Thompson",
      "Andrea Ranieri",
      "Silvia Biasotti",
      "Miguel Chicchon",
      "Ivan Sipiran",
      "Minh-Khoi Pham",
      "Thang-Long Nguyen-Ho",
      "Hai-Dang Nguyen",
      "Minh-Triet Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.13326"
  },
  {
    "id": "arXiv:2205.13331",
    "title": "TransBoost: Improving the Best ImageNet Performance using Deep  Transduction",
    "abstract": "TransBoost: Improving the Best ImageNet Performance using Deep  Transduction",
    "descriptor": "",
    "authors": [
      "Omer Belhasin",
      "Guy Bar-Shalom",
      "Ran El-Yaniv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13331"
  },
  {
    "id": "arXiv:2205.13419",
    "title": "The way we cite: common metadata used across disciplines for defining  bibliographic references",
    "abstract": "The way we cite: common metadata used across disciplines for defining  bibliographic references",
    "descriptor": "",
    "authors": [
      "Erika Alves dos Santos",
      "Silvio Peroni",
      "Marcos Luiz Mucheroni"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2205.13419"
  },
  {
    "id": "arXiv:2205.13462",
    "title": "FedAug: Reducing the Local Learning Bias Improves Federated Learning on  Heterogeneous Data",
    "abstract": "FedAug: Reducing the Local Learning Bias Improves Federated Learning on  Heterogeneous Data",
    "descriptor": "",
    "authors": [
      "Yongxin Guo",
      "Tao Lin",
      "Xiaoying Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13462"
  },
  {
    "id": "arXiv:2205.13543",
    "title": "Revealing the Dark Secrets of Masked Image Modeling",
    "abstract": "Revealing the Dark Secrets of Masked Image Modeling",
    "descriptor": "",
    "authors": [
      "Zhenda Xie",
      "Zigang Geng",
      "Jingcheng Hu",
      "Zheng Zhang",
      "Han Hu",
      "Yue Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13543"
  }
]