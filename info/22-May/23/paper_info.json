[
  {
    "id": "arXiv:2205.09751",
    "title": "Taylor Genetic Programming for Symbolic Regression",
    "abstract": "Genetic programming (GP) is a commonly used approach to solve symbolic\nregression (SR) problems. Compared with the machine learning or deep learning\nmethods that depend on the pre-defined model and the training dataset for\nsolving SR problems, GP is more focused on finding the solution in a search\nspace. Although GP has good performance on large-scale benchmarks, it randomly\ntransforms individuals to search results without taking advantage of the\ncharacteristics of the dataset. So, the search process of GP is usually slow,\nand the final results could be unstable.To guide GP by these characteristics,\nwe propose a new method for SR, called Taylor genetic programming (TaylorGP)\n(Code and appendix at https://kgae-cup.github.io/TaylorGP/). TaylorGP leverages\na Taylor polynomial to approximate the symbolic equation that fits the dataset.\nIt also utilizes the Taylor polynomial to extract the features of the symbolic\nequation: low order polynomial discrimination, variable separability, boundary,\nmonotonic, and parity. GP is enhanced by these Taylor polynomial techniques.\nExperiments are conducted on three kinds of benchmarks: classical SR, machine\nlearning, and physics. The experimental results show that TaylorGP not only has\nhigher accuracy than the nine baseline methods, but also is faster in finding\nstable results.",
    "descriptor": "\nComments: 9 pages, 6 figures, conference\n",
    "authors": [
      "Baihe He",
      "Qiang Lu",
      "Qingyun Yang",
      "Jake Luo",
      "Zhiguang Wang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09751"
  },
  {
    "id": "arXiv:2205.09752",
    "title": "Local dynamic mode of Cognitive Behavioral Therapy",
    "abstract": "In order to increase mental health equity among the most vulnerable and\nmarginalized communities, it is important to increase access to high-quality\ntherapists. One facet of addressing these needs, is to provide timely feedback\nto clinicians as they interact with their clients, in a way that is also\ncontextualized to specific clients and interactions they have had. Dynamical\nsystems provide a framework through which to analyze interactions. The present\nwork applies these methods to the domain of automated psychotherapist\nevaluation for Cognitive Behavioral Therapy (CBT). Our methods extract local\ndynamic modes from short windows of conversation and learns to correlate the\nobserved dynamics to CBT competence. The results demonstrate the value of this\nparadigm and outlines the way in which these methods can be used to study and\nimprove therapeutic strategies.",
    "descriptor": "",
    "authors": [
      "Victor Ardulov",
      "Torrey A. Creed",
      "David C. Atkins",
      "Shrikanth Narayanan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09752"
  },
  {
    "id": "arXiv:2205.09753",
    "title": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory  Prediction via Scene Encoding",
    "abstract": "One essential task for autonomous driving is to encode the information of a\ndriving scene into vector representations so that the downstream task such as\ntrajectory prediction could perform well. The driving scene is complicated, and\nthere exists heterogeneity within elements, where they own diverse types of\ninformation i.e., agent dynamics, map routing, road lines, etc. Meanwhile,\nthere also exist relativity across elements - meaning they have spatial\nrelations with each other; such relations should be canonically represented\nregarding the relative measurements since the absolute value of the coordinate\nis meaningless. Taking these two observations into consideration, we propose a\nnovel backbone, namely Heterogeneous Driving Graph Transformer (HDGT), which\nmodels the driving scene as a heterogeneous graph with different types of nodes\nand edges. For graph construction, each node represents either an agent or a\nroad element and each edge represents their semantics relations such as\nPedestrian-To-Crosswalk, Lane-To-Left-Lane. As for spatial relation encoding,\ninstead of setting a fixed global reference, the coordinate information of the\nnode as well as its in-edges is transformed to the local node-centric\ncoordinate system. For the aggregation module in the graph neural network\n(GNN), we adopt the transformer structure in a hierarchical way to fit the\nheterogeneous nature of inputs. Experimental results show that the proposed\nmethod achieves new state-of-the-art on INTERACTION Prediction Challenge and\nWaymo Open Motion Challenge, in which we rank 1st and 2nd respectively\nregarding the minADE/minFDE metric.",
    "descriptor": "",
    "authors": [
      "Xiaosong Jia",
      "Penghao Wu",
      "Li Chen",
      "Hongyang Li",
      "Yu Liu",
      "Junchi Yan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.09753"
  },
  {
    "id": "arXiv:2205.09760",
    "title": "Identifying outliers in astronomical images with unsupervised machine  learning",
    "abstract": "Astronomical outliers, such as unusual, rare or unknown types of astronomical\nobjects or phenomena, constantly lead to the discovery of genuinely unforeseen\nknowledge in astronomy. More unpredictable outliers will be uncovered in\nprinciple with the increment of the coverage and quality of upcoming survey\ndata. However, it is a severe challenge to mine rare and unexpected targets\nfrom enormous data with human inspection due to a significant workload.\nSupervised learning is also unsuitable for this purpose since designing proper\ntraining sets for unanticipated signals is unworkable. Motivated by these\nchallenges, we adopt unsupervised machine learning approaches to identify\noutliers in the data of galaxy images to explore the paths for detecting\nastronomical outliers. For comparison, we construct three methods, which are\nbuilt upon the k-nearest neighbors (KNN), Convolutional Auto-Encoder (CAE)+\nKNN, and CAE + KNN + Attention Mechanism (attCAE KNN) separately. Testing sets\nare created based on the Galaxy Zoo image data published online to evaluate the\nperformance of the above methods. Results show that attCAE KNN achieves the\nbest recall (78%), which is 53% higher than the classical KNN method and 22%\nhigher than CAE+KNN. The efficiency of attCAE KNN (10 minutes) is also superior\nto KNN (4 hours) and equal to CAE+KNN(10 minutes) for accomplishing the same\ntask. Thus, we believe it is feasible to detect astronomical outliers in the\ndata of galaxy images in an unsupervised manner. Next, we will apply attCAE KNN\nto available survey datasets to assess its applicability and reliability.",
    "descriptor": "",
    "authors": [
      "Yang Han",
      "Zhiqiang Zou",
      "Nan Li",
      "Yanli Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)"
    ],
    "url": "https://arxiv.org/abs/2205.09760"
  },
  {
    "id": "arXiv:2205.09778",
    "title": "FogROS 2: An Adaptive and Extensible Platform for Cloud and Fog Robotics  Using ROS 2",
    "abstract": "Mobility, power, and price points often dictate that robots do not have\nsufficient computing power on board to run modern robot algorithms at desired\nrates. Cloud computing providers such as AWS, GCP, and Azure offer immense\ncomputing power on demand, but tapping into that power from a robot is\nnon-trivial. In this paper, we present FogROS 2, an easy-to-use, open-source\nplatform to facilitate cloud and fog robotics compatible with the emerging ROS\n2 standard, extending the open-source Robot Operating System (ROS). FogROS 2\nprovisions a cloud computer, deploys and launches ROS 2 nodes to the cloud\ncomputer, sets up secure networking between the robot and cloud, and starts the\napplication running. FogROS 2 is completely redesigned and distinct from its\npredecessor to support ROS 2 applications, transparent video compression and\ncommunication, improved performance and security, support for multiple\ncloud-computing providers, and remote monitoring and visualization. We\ndemonstrate in example applications that the performance gained by using cloud\ncomputers can overcome the network latency to significantly speed up robot\nperformance. In examples, FogROS 2 reduces SLAM latency by 50%, reduces grasp\nplanning time from 14s to 1.2s, and speeds up motion planning 28x. When\ncompared to alternatives, FogROS 2 reduces network utilization by up to 3.8x.\nFogROS 2, source, examples, and documentation is available at\nhttps://github.com/BerkeleyAutomation/FogROS2 .",
    "descriptor": "",
    "authors": [
      "Jeffrey Ichnowski",
      "Kaiyuan Chen",
      "Karthik Dharmarajan",
      "Simeon Adebola",
      "Michael Danielczuk",
      "V\u0131ctor Mayoral-Vilches",
      "Hugo Zhan",
      "Derek Xu",
      "Ramtin Ghassemi",
      "John Kubiatowicz",
      "Ion Stoica",
      "Joseph Gonzalez",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.09778"
  },
  {
    "id": "arXiv:2205.09786",
    "title": "Subset Node Anomaly Tracking over Large Dynamic Graphs",
    "abstract": "Tracking a targeted subset of nodes in an evolving graph is important for\nmany real-world applications. Existing methods typically focus on identifying\nanomalous edges or finding anomaly graph snapshots in a stream way. However,\nedge-oriented methods cannot quantify how individual nodes change over time\nwhile others need to maintain representations of the whole graph all time, thus\ncomputationally inefficient.\nThis paper proposes \\textsc{DynAnom}, an efficient framework to quantify the\nchanges and localize per-node anomalies over large dynamic weighted-graphs.\nThanks to recent advances in dynamic representation learning based on\nPersonalized PageRank, \\textsc{DynAnom} is 1) \\textit{efficient}: the time\ncomplexity is linear to the number of edge events and independent on node size\nof the input graph; 2) \\textit{effective}: \\textsc{DynAnom} can successfully\ntrack topological changes reflecting real-world anomaly; 3) \\textit{flexible}:\ndifferent type of anomaly score functions can be defined for various\napplications. Experiments demonstrate these properties on both benchmark graph\ndatasets and a new large real-world dynamic graph. Specifically, an\ninstantiation method based on \\textsc{DynAnom} achieves the accuracy of 0.5425\ncompared with 0.2790, the best baseline, on the task of node-level anomaly\nlocalization while running 2.3 times faster than the baseline. We present a\nreal-world case study and further demonstrate the usability of \\textsc{DynAnom}\nfor anomaly discovery over large-scale graphs.",
    "descriptor": "\nComments: 9 pages + 2 pages supplement, accepted to 2022 ACM SIGKDD Research Track\n",
    "authors": [
      "Xingzhi Guo",
      "Baojian Zhou",
      "Steven Skiena"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.09786"
  },
  {
    "id": "arXiv:2205.09787",
    "title": "Causal Discovery and Injection for Feed-Forward Neural Networks",
    "abstract": "Neural networks have proven to be effective at solving a wide range of\nproblems but it is often unclear whether they learn any meaningful causal\nrelationship: this poses a problem for the robustness of neural network models\nand their use for high-stakes decisions. We propose a novel method overcoming\nthis issue by injecting knowledge in the form of (possibly partial) causal\ngraphs into feed-forward neural networks, so that the learnt model is\nguaranteed to conform to the graph, hence adhering to expert knowledge. This\nknowledge may be given up-front or during the learning process, to improve the\nmodel through human-AI collaboration. We apply our method to synthetic and real\n(tabular) data showing that it is robust against noise and can improve causal\ndiscovery and prediction performance in low data regimes.",
    "descriptor": "",
    "authors": [
      "Fabrizio Russo",
      "Francesca Toni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09787"
  },
  {
    "id": "arXiv:2205.09788",
    "title": "Probabilistic genotyping code review and testing",
    "abstract": "We discuss a range of miscodes found in probabilistic genotyping (PG)\nsoftware and from other industries that have been reported in the literature\nand have been used to inform PG admissibility hearings. Every instance of the\ndiscovery of a miscode in PG software with which we have been associated has\noccurred either because of testing, use, or repeat calculation of results\neither by us or other users. In all cases found during testing or use something\nhas drawn attention to an anomalous result. Intelligent investigation has led\nto the examination of a small section of the code and detection of the miscode.\nPreviously, three instances from other industries quoted by the Electronic\nFrontier Foundation Amicus brief as part of a PG admissibility hearing\n(atmospheric ozone, NIMIS, and VW) and two previous examples raised in relation\nto PG admissibility (Kerberos and Therac-25) were presented as examples of\nmiscodes and how an extensive code review could have resolved these situations.\nHowever, we discuss how these miscodes might not have been discovered through\ncode review alone. These miscodes could only have been detected through use of\nthe software or through testing. Once the symptoms of the miscode(s) have been\ndetected, a code review serves as a beneficial approach to try and diagnose to\nthe issue.",
    "descriptor": "",
    "authors": [
      "John Buckleton",
      "Jo-Anne Bright",
      "Kevin Cheng",
      "Duncan Taylor"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.09788"
  },
  {
    "id": "arXiv:2205.09791",
    "title": "A Peek at Peak Emotion Recognition",
    "abstract": "Despite much progress in the field of facial expression recognition, little\nattention has been paid to the recognition of peak emotion. Aviezer et al. [1]\nshowed that humans have trouble discerning between positive and negative peak\nemotions. In this work we analyze how deep learning fares on this challenge. We\nfind that (i) despite using very small datasets, features extracted from deep\nlearning models can achieve results significantly better than humans. (ii) We\nfind that deep learning models, even when trained only on datasets tagged by\nhumans, still outperform humans in this task.",
    "descriptor": "\nComments: Submitted to HBU Workshop at ICPR, 6 pages, 5 figures\n",
    "authors": [
      "Tzvi Michelson",
      "Hillel Aviezer",
      "Shmuel Peleg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.09791"
  },
  {
    "id": "arXiv:2205.09795",
    "title": "Learning to Share Autonomy from Repeated Human-Robot Interaction",
    "abstract": "Assistive robot arms try to help their users perform everyday tasks. One way\nrobots can provide this assistance is shared autonomy. Within shared autonomy,\nboth the human and robot maintain control over the robot's motion: as the robot\nbecomes confident it understands what the human wants, it intervenes to\nautomate the task. But how does the robot know these tasks in the first place?\nState-of-the-art approaches to shared autonomy often rely on prior knowledge.\nFor instance, the robot may need to know the human's potential goals\nbeforehand. During long-term interaction these methods will inevitably break\ndown -- sooner or later the human will attempt to perform a task that the robot\ndoes not expect. Accordingly, in this paper we formulate an alternate approach\nto shared autonomy that learns assistance from scratch. Our insight is that\noperators repeat important tasks on a daily basis (e.g., opening the fridge,\nmaking coffee). Instead of relying prior knowledge, we therefore take advantage\nof these repeated interactions to learn assistive policies. We formalize an\nalgorithm that recognizes the human's task, replicates similar demonstrations,\nand returns control when unsure. We then combine learning with control to\ndemonstrate that the error of our approach is uniformly ultimately bounded. We\nperform simulations to support this error bound, compare our approach to\nimitation learning baselines, and explore its capacity to assist for an\nincreasing number of tasks. Finally, we conduct a user study with\nindustry-standard methods and shared autonomy baselines. Our results indicate\nthat learning shared autonomy across repeated interactions (SARI) matches\nexisting approaches for known goals, and outperforms the baselines on tasks\nthat were never specified beforehand.",
    "descriptor": "\nComments: 16 pages, 17 figures. arXiv admin note: substantial text overlap with arXiv:2107.09650\n",
    "authors": [
      "Ananth Jonnavittula",
      "Shaunak A. Mehta",
      "Dylan P. Losey"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.09795"
  },
  {
    "id": "arXiv:2205.09797",
    "title": "Improving Multi-Task Generalization via Regularizing Spurious  Correlation",
    "abstract": "Multi-Task Learning (MTL) is a powerful learning paradigm to improve\ngeneralization performance via knowledge sharing. However, existing studies\nfind that MTL could sometimes hurt generalization, especially when two tasks\nare less correlated. One possible reason that hurts generalization is spurious\ncorrelation, i.e., some knowledge is spurious and not causally related to task\nlabels, but the model could mistakenly utilize them and thus fail when such\ncorrelation changes. In MTL setup, there exist several unique challenges of\nspurious correlation. First, the risk of having non-causal knowledge is higher,\nas the shared MTL model needs to encode all knowledge from different tasks, and\ncausal knowledge for one task could be potentially spurious to the other.\nSecond, the confounder between task labels brings in a different type of\nspurious correlation to MTL. We theoretically prove that MTL is more prone to\ntaking non-causal knowledge from other tasks than single-task learning, and\nthus generalize worse. To solve this problem, we propose Multi-Task Causal\nRepresentation Learning framework, aiming to represent multi-task knowledge via\ndisentangled neural modules, and learn which module is causally related to each\ntask via MTL-specific invariant regularization. Experiments show that it could\nenhance MTL model's performance by 5.5% on average over Multi-MNIST, MovieLens,\nTaskonomy, CityScape, and NYUv2, via alleviating spurious correlation problem.",
    "descriptor": "",
    "authors": [
      "Ziniu Hu",
      "Zhe Zhao",
      "Xinyang Yi",
      "Tiansheng Yao",
      "Lichan Hong",
      "Yizhou Sun",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09797"
  },
  {
    "id": "arXiv:2205.09799",
    "title": "Digital Reconfigurable Intelligent Surfaces: On the Impact of Realistic  Reradiation Models",
    "abstract": "Reconfigurable intelligent surface (RIS) is an emerging technology that is\nunder investigation for different applications in wireless communications. RISs\nare often analyzed and optimized by considering simplified electromagnetic\nreradiation models. In this chapter, we aim to study the impact of realistic\nreradiation models for RISs as a function of the sub-wavelength inter-distance\nbetween nearby elements of the RIS, the quantization levels of the reflection\ncoefficients, the interplay between the amplitude and phase of the reflection\ncoefficients, and the presence of electromagnetic interference. We consider\nboth case studies in which the users may be located in the far-field and\nnear-field regions of an RIS. Our study shows that, due to design constraints,\nsuch as the need of using quantized reflection coefficients or the inherent\ninterplay between the phase and the amplitude of the reflection coefficients,\nan RIS may reradiate power towards unwanted directions that depend on the\nintended and interfering electromagnetic waves. Therefore, it is in general\nimportant to optimize an RIS by considering the entire reradiation pattern by\ndesign to maximize the reradiated power towards the desired directions of\nreradiation while keeping the power reradiated towards other unwanted\ndirections at a low level. Our study shows that a 2-bit digitally controllable\nRIS with an almost constant reflection amplitude as a function of the applied\nphase shift, and whose scattering elements have a size and an inter-distance\nbetween (1/8)th and (1/4)th of the signal wavelength may be a good tradeoff\nbetween performance, implementation complexity and cost. However, the presented\nresults are preliminary and pave the way for further research into the\nperformance of RISs based on accurate and realistic electromagnetic reradiation\nmodels.",
    "descriptor": "\nComments: Invited book chapter - Preprint\n",
    "authors": [
      "Marco Di Renzo",
      "Abdelhamed Ahmed",
      "Alessio Zappone",
      "Vincenzo Galdi",
      "Gabriele Gradoni",
      "Massimo Moccia",
      "Giuseppe Castaldi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Emerging Technologies (cs.ET)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.09799"
  },
  {
    "id": "arXiv:2205.09801",
    "title": "Graph Neural Networks Are More Powerful Than we Think",
    "abstract": "Graph Neural Networks (GNNs) are powerful convolutional architectures that\nhave shown remarkable performance in various node-level and graph-level tasks.\nDespite their success, the common belief is that the expressive power of GNNs\nis limited and that they are at most as discriminative as the Weisfeiler-Lehman\n(WL) algorithm. In this paper we argue the opposite and show that the WL\nalgorithm is the upper bound only when the input to the GNN is the vector of\nall ones. In this direction, we derive an alternative analysis that employs\nlinear algebraic tools and characterize the representational power of GNNs with\nrespect to the eigenvalue decomposition of the graph operators. We show that\nGNNs can distinguish between any graphs that differ in at least one eigenvalue\nand design simple GNN architectures that are provably more expressive than the\nWL algorithm. Thorough experimental analysis on graph isomorphism and graph\nclassification datasets corroborates our theoretical results and demonstrates\nthe effectiveness of the proposed architectures.",
    "descriptor": "",
    "authors": [
      "Charilaos I. Kanatsoulis",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09801"
  },
  {
    "id": "arXiv:2205.09802",
    "title": "Label-invariant Augmentation for Semi-Supervised Graph Classification",
    "abstract": "Recently, contrastiveness-based augmentation surges a new climax in the\ncomputer vision domain, where some operations, including rotation, crop, and\nflip, combined with dedicated algorithms, dramatically increase the model\ngeneralization and robustness. Following this trend, some pioneering attempts\nemploy the similar idea to graph data. Nevertheless, unlike images, it is much\nmore difficult to design reasonable augmentations without changing the nature\nof graphs. Although exciting, the current graph contrastive learning does not\nachieve as promising performance as visual contrastive learning. We conjecture\nthe current performance of graph contrastive learning might be limited by the\nviolation of the label-invariant augmentation assumption. In light of this, we\npropose a label-invariant augmentation for graph-structured data to address\nthis challenge. Different from the node/edge modification and subgraph\nextraction, we conduct the augmentation in the representation space and\ngenerate the augmented samples in the most difficult direction while keeping\nthe label of augmented data the same as the original samples. In the\nsemi-supervised scenario, we demonstrate our proposed method outperforms the\nclassical graph neural network based methods and recent graph contrastive\nlearning on eight benchmark graph-structured data, followed by several in-depth\nexperiments to further explore the label-invariant augmentation in several\naspects.",
    "descriptor": "",
    "authors": [
      "Han Yue",
      "Chunhui Zhang",
      "Chuxu Zhang",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09802"
  },
  {
    "id": "arXiv:2205.09803",
    "title": "Towards a Holistic View on Argument Quality Prediction",
    "abstract": "Argumentation is one of society's foundational pillars, and, sparked by\nadvances in NLP and the vast availability of text data, automated mining of\narguments receives increasing attention. A decisive property of arguments is\ntheir strength or quality. While there are works on the automated estimation of\nargument strength, their scope is narrow: they focus on isolated datasets and\nneglect the interactions with related argument mining tasks, such as argument\nidentification, evidence detection, or emotional appeal. In this work, we close\nthis gap by approaching argument quality estimation from multiple different\nangles: Grounded on rich results from thorough empirical evaluations, we assess\nthe generalization capabilities of argument quality estimation across diverse\ndomains, the interplay with related argument mining tasks, and the impact of\nemotions on perceived argument strength. We find that generalization depends on\na sufficient representation of different domains in the training part. In\nzero-shot transfer and multi-task experiments, we reveal that argument quality\nis among the more challenging tasks but can improve others. Finally, we show\nthat emotions play a minor role in argument quality than is often assumed.",
    "descriptor": "",
    "authors": [
      "Michael Fromm",
      "Max Berrendorf",
      "Johanna Reiml",
      "Isabelle Mayerhofer",
      "Siddharth Bhargava",
      "Evgeniy Faerman",
      "Thomas Seidl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09803"
  },
  {
    "id": "arXiv:2205.09804",
    "title": "Estimation of Entropy in Constant Space with Improved Sample Complexity",
    "abstract": "Recent work of Acharya et al. (NeurIPS 2019) showed how to estimate the\nentropy of a distribution $\\mathcal D$ over an alphabet of size $k$ up to\n$\\pm\\epsilon$ additive error by streaming over $(k/\\epsilon^3) \\cdot\n\\text{polylog}(1/\\epsilon)$ i.i.d. samples and using only $O(1)$ words of\nmemory. In this work, we give a new constant memory scheme that reduces the\nsample complexity to $(k/\\epsilon^2)\\cdot \\text{polylog}(1/\\epsilon)$. We\nconjecture that this is optimal up to $\\text{polylog}(1/\\epsilon)$ factors.",
    "descriptor": "",
    "authors": [
      "Maryam Aliakbarpour",
      "Andrew McGregor",
      "Jelani Nelson",
      "Erik Waingarten"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09804"
  },
  {
    "id": "arXiv:2205.09807",
    "title": "B-spline velocity field level set topology optimization method for  stress and buckling constraints based on discrete adjoint method",
    "abstract": "This paper proposes a new sensitivity computational scheme for velocity field\nlevel set method with discrete adjoint method. The velocity field of level set\nmethod is described in B-spline space. The adjoint equations are constructed\nbased on discretized governing equations. This paper demonstrates that the\nvelocity field of level set method can be fully derived from the discrete\nadjoint method. This enables the circumvention of shape sensitivity analysis\nfor standard level set method. We demonstrate the effectiveness of proposed\nmethod in the context of stress and linearized buckling topology optimization\nproblems.",
    "descriptor": "",
    "authors": [
      "Hao Deng",
      "Kazuhiro Saitou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.09807"
  },
  {
    "id": "arXiv:2205.09809",
    "title": "Calibration Matters: Tackling Maximization Bias in Large-scale  Advertising Recommendation Systems",
    "abstract": "Calibration is defined as the ratio of the average predicted click rate to\nthe true click rate. The optimization of calibration is essential to many\nonline advertising recommendation systems because it directly affects the\ndownstream bids in ads auctions and the amount of money charged to advertisers.\nDespite its importance, calibration optimization often suffers from a problem\ncalled \"maximization bias\". Maximization bias refers to the phenomenon that the\nmaximum of predicted values overestimates the true maximum. The problem is\nintroduced because the calibration is computed on the set selected by the\nprediction model itself. It persists even if unbiased predictions can be\nachieved on every datapoint and worsens when covariate shifts exist between the\ntraining and test sets. To mitigate this problem, we theorize the\nquantification of maximization bias and propose a variance-adjusting debiasing\n(VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and\npractical as it is able to mitigate maximization bias problems under covariate\nshifts, neither incurring additional online serving costs nor compromising the\nranking performance. We demonstrate the effectiveness of the proposed algorithm\nusing a state-of-the-art recommendation neural network model on a large-scale\nreal-world dataset.",
    "descriptor": "",
    "authors": [
      "Yewen Fan",
      "Nian Si",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.09809"
  },
  {
    "id": "arXiv:2205.09813",
    "title": "A Novel Weighted Ensemble Learning Based Agent for the Werewolf Game",
    "abstract": "Werewolf is a popular party game throughout the world, and research on its\nsignificance has progressed in recent years. The Werewolf game is based on\nconversation, and in order to win, participants must use all of their cognitive\nabilities. This communication game requires the playing agents to be very\nsophisticated to win. In this research, we generated a sophisticated agent to\nplay the Werewolf game using a complex weighted ensemble learning approach.\nThis research work aimed to estimate what other agents/players think of us in\nthe game. The agent was developed by aggregating strategies of different\nparticipants in the AI Wolf competition and thereby learning from them using\nmachine learning. Moreover, the agent created was able to perform much better\nthan other competitors using very basic strategies to show the approach's\neffectiveness in the Werewolf game. The machine learning technique used here is\nnot restricted to the Werewolf game but may be extended to any game that\nrequires communication and action depending on other participants.",
    "descriptor": "",
    "authors": [
      "Mohiuddeen Khan",
      "Claus Aranha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.09813"
  },
  {
    "id": "arXiv:2205.09817",
    "title": "MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News  Detection",
    "abstract": "COVID-19 related misinformation and fake news, coined an 'infodemic', has\ndramatically increased over the past few years. This misinformation exhibits\nconcept drift, where the distribution of fake news changes over time, reducing\neffectiveness of previously trained models for fake news detection. Given a set\nof fake news models trained on multiple domains, we propose an adaptive\ndecision module to select the best-fit model for a new sample. We propose\nMiDAS, a multi-domain adaptative approach for fake news detection that ranks\nrelevancy of existing models to new samples. MiDAS contains 2 components: a\ndoman-invariant encoder, and an adaptive model selector. MiDAS integrates\nmultiple pre-trained and fine-tuned models with their training data to create a\ndomain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of\nthe invariant embedding space to estimate each model's relevance to a new\nsample. Higher ranked models provide predictions, and lower ranked models\nabstain. We evaluate MiDAS on generalization to drifted data with 9 fake news\ndatasets, each obtained from different domains and modalities. MiDAS achieves\nnew state-of-the-art performance on multi-domain adaptation for\nout-of-distribution fake news classification.",
    "descriptor": "\nComments: We use Lipschitz smoothness and probabilistic Lipschitzness to build a theoretical foundation for effective multi-domain adaptation using randomized perturbations on unseen data\n",
    "authors": [
      "Abhijit Suprem",
      "Calton Pu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09817"
  },
  {
    "id": "arXiv:2205.09818",
    "title": "A Learning-Based Approach to Approximate Coded Computation",
    "abstract": "Lagrange coded computation (LCC) is essential to solving problems about\nmatrix polynomials in a coded distributed fashion; nevertheless, it can only\nsolve the problems that are representable as matrix polynomials. In this paper,\nwe propose AICC, an AI-aided learning approach that is inspired by LCC but also\nuses deep neural networks (DNNs). It is appropriate for coded computation of\nmore general functions. Numerical simulations demonstrate the suitability of\nthe proposed approach for the coded computation of different matrix functions\nthat are often utilized in digital signal processing.",
    "descriptor": "\nComments: Submitted to IEEE Information Theory Workshop (ITW) 2022\n",
    "authors": [
      "Navneet Agrawal",
      "Yuqin Qiu",
      "Matthias Frey",
      "Igor Bjelakovic",
      "Setareh Maghsudi",
      "Slawomir Stanczak",
      "Jingge Zhu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09818"
  },
  {
    "id": "arXiv:2205.09821",
    "title": "Unsupervised Learning of Depth, Camera Pose and Optical Flow from  Monocular Video",
    "abstract": "We propose DFPNet -- an unsupervised, joint learning system for monocular\nDepth, Optical Flow and egomotion (Camera Pose) estimation from monocular image\nsequences. Due to the nature of 3D scene geometry these three components are\ncoupled. We leverage this fact to jointly train all the three components in an\nend-to-end manner. A single composite loss function -- which involves image\nreconstruction-based loss for depth & optical flow, bidirectional consistency\nchecks and smoothness loss components -- is used to train the network. Using\nhyperparameter tuning, we are able to reduce the model size to less than 5%\n(8.4M parameters) of state-of-the-art DFP models. Evaluation on KITTI and\nCityscapes driving datasets reveals that our model achieves results comparable\nto state-of-the-art in all of the three tasks, even with the significantly\nsmaller model size.",
    "descriptor": "\nComments: 8 pages, 2 figures. arXiv admin note: text overlap with arXiv:1803.02276 by other authors\n",
    "authors": [
      "Dipan Mandal",
      "Abhilash Jain",
      "Sreenivas Subramoney"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09821"
  },
  {
    "id": "arXiv:2205.09823",
    "title": "Public Signals in Network Congestion Games",
    "abstract": "We consider a largely untapped potential for the improvement of traffic\nnetworks that is rooted in the inherent uncertainty of travel times. Travel\ntimes are subject to stochastic uncertainty resulting from various parameters\nsuch as weather condition, occurrences of road works, or traffic accidents.\nLarge mobility services have an informational advantage over single network\nusers as they are able to learn traffic conditions from data. A benevolent\nmobility service may use this informational advantage in order to steer the\ntraffic equilibrium into a favorable direction. The resulting optimization\nproblem is a task commonly referred to as signaling or Bayesian persuasion.\nPrevious work has shown that the underlying signaling problem can be NP-hard to\napproximate within any non-trivial bounds, even for affine cost functions with\nstochastic offsets. In contrast, we show that in this case, the signaling\nproblem is easy for many networks. We tightly characterize the class of\nsingle-commodity networks, in which full information revelation is always an\noptimal signaling strategy. Moreover, we construct a reduction from optimal\nsignaling to computing an optimal collection of support vectors for the Wardrop\nequilibrium. For two states, this insight can be used to compute an optimal\nsignaling scheme. The algorithm runs in polynomial time whenever the number of\ndifferent supports resulting from any signal distribution is bounded to a\npolynomial in the input size. Using a cell decomposition technique, we extend\nthe approach to a polynomial-time algorithm for multi-commodity parallel link\nnetworks with a constant number of commodities, even when we have a constant\nnumber of different states of nature.",
    "descriptor": "",
    "authors": [
      "Svenja M. Griesbach",
      "Martin Hoefer",
      "Max Klimm",
      "Tim Koglin"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.09823"
  },
  {
    "id": "arXiv:2205.09826",
    "title": "DPER: Dynamic Programming for Exist-Random Stochastic SAT",
    "abstract": "In Bayesian inference, the maximum a posteriori (MAP) problem combines the\nmost probable explanation (MPE) and marginalization (MAR) problems. The\ncounterpart in propositional logic is the exist-random stochastic\nsatisfiability (ER-SSAT) problem, which combines the satisfiability (SAT) and\nweighted model counting (WMC) problems. Both MAP and ER-SSAT have the form\n$\\operatorname{argmax}_X \\sum_Y f(X, Y)$, where $f$ is a real-valued function\nover disjoint sets $X$ and $Y$ of variables. These two optimization problems\nrequest a value assignment for the $X$ variables that maximizes the weighted\nsum of $f(X, Y)$ over all value assignments for the $Y$ variables. ER-SSAT has\nbeen shown to be a promising approach to formally verify fairness in supervised\nlearning. Recently, dynamic programming on graded project-join trees has been\nproposed to solve weighted projected model counting (WPMC), a related problem\nthat has the form $\\sum_X \\max_Y f(X, Y)$. We extend this WPMC framework to\nexactly solve ER-SSAT and implement a dynamic-programming solver named DPER.\nOur empirical evaluation indicates that DPER contributes to the portfolio of\nstate-of-the-art ER-SSAT solvers (DC-SSAT and erSSAT) through competitive\nperformance on low-width problem instances.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.08632\n",
    "authors": [
      "Vu H. N. Phan",
      "Moshe Y. Vardi"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.09826"
  },
  {
    "id": "arXiv:2205.09830",
    "title": "Towards Understanding Gender-Seniority Compound Bias in Natural Language  Generation",
    "abstract": "Women are often perceived as junior to their male counterparts, even within\nthe same job titles. While there has been significant progress in the\nevaluation of gender bias in natural language processing (NLP), existing\nstudies seldom investigate how biases toward gender groups change when\ncompounded with other societal biases. In this work, we investigate how\nseniority impacts the degree of gender bias exhibited in pretrained neural\ngeneration models by introducing a novel framework for probing compound bias.\nWe contribute a benchmark robustness-testing dataset spanning two domains, U.S.\nsenatorship and professorship, created using a distant-supervision method. Our\ndataset includes human-written text with underlying ground truth and paired\ncounterfactuals. We then examine GPT-2 perplexity and the frequency of gendered\nlanguage in generated text. Our results show that GPT-2 amplifies bias by\nconsidering women as junior and men as senior more often than the ground truth\nin both domains. These results suggest that NLP applications built using GPT-2\nmay harm women in professional capacities.",
    "descriptor": "\nComments: 6 pages, LREC 2022\n",
    "authors": [
      "Samhita Honnavalli",
      "Aesha Parekh",
      "Lily Ou",
      "Sophie Groenwold",
      "Sharon Levy",
      "Vicente Ordonez",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09830"
  },
  {
    "id": "arXiv:2205.09831",
    "title": "A numerical comparison of some heuristic stopping rules for nonlinear  Landweber iteration",
    "abstract": "The choice of a suitable regularization parameter is an important part of\nmost regularization methods for inverse problems. In the absence of reliable\nestimates of the noise level, heuristic parameter choice rules can be used to\naccomplish this task. While they are already fairly well-understood and tested\nin the case of linear problems, not much is known about their behaviour for\nnonlinear problems and even less in the respective case of iterative\nregularization. Hence, in this paper, we numerically study the performance of\nsome of these rules when used to determine a stopping index for Landweber\niteration for various nonlinear inverse problems. These are chosen from\ndifferent practically relevant fields such as integral equations, parameter\nestimation, and tomography.",
    "descriptor": "\nComments: 28 pages, 12 figures\n",
    "authors": [
      "Simon Hubmer",
      "Ekaterina Sherina",
      "Stefan Kindermann",
      "Kemal Raik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.09831"
  },
  {
    "id": "arXiv:2205.09833",
    "title": "Learning Interface Conditions in Domain Decomposition Solvers",
    "abstract": "Domain decomposition methods are widely used and effective in the\napproximation of solutions to partial differential equations. Yet the optimal\nconstruction of these methods requires tedious analysis and is often available\nonly in simplified, structured-grid settings, limiting their use for more\ncomplex problems. In this work, we generalize optimized Schwarz domain\ndecomposition methods to unstructured-grid problems, using Graph Convolutional\nNeural Networks (GCNNs) and unsupervised learning to learn optimal\nmodifications at subdomain interfaces. A key ingredient in our approach is an\nimproved loss function, enabling effective training on relatively small\nproblems, but robust performance on arbitrarily large problems, with\ncomputational cost linear in problem size. The performance of the learned\nlinear solvers is compared with both classical and optimized domain\ndecomposition algorithms, for both structured- and unstructured-grid problems.",
    "descriptor": "",
    "authors": [
      "Ali Taghibakhshi",
      "Nicolas Nytko",
      "Tareq Zaman",
      "Scott MacLachlan",
      "Luke Olson",
      "Matthew West"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Discrete Mathematics (cs.DM)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.09833"
  },
  {
    "id": "arXiv:2205.09834",
    "title": "Classification of Intra-Pulse Modulation of Radar Signals by Feature  Fusion Based Convolutional Neural Networks",
    "abstract": "Detection and classification of radars based on pulses they transmit is an\nimportant application in electronic warfare systems. In this work, we propose a\nnovel deep-learning based technique that automatically recognizes intra-pulse\nmodulation types of radar signals. Re-assigned spectrogram of measured radar\nsignal and detected outliers of its instantaneous phases filtered by a special\nfunction are used for training multiple convolutional neural networks.\nAutomatically extracted features from the networks are fused to distinguish\nfrequency and phase modulated signals. Simulation results show that the\nproposed FF-CNN (Feature Fusion based Convolutional Neural Network) technique\noutperforms the current state-of-the-art alternatives and is easily scalable\namong broad range of modulation types.",
    "descriptor": "\nComments: Published at EUSIPCO2018\n",
    "authors": [
      "Fatih Cagatay Akyon",
      "Yasar Kemal Alp",
      "Gokhan Gok",
      "Orhan Arikan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.09834"
  },
  {
    "id": "arXiv:2205.09836",
    "title": "Concurrent Policy Blending and System Identification for Generalized  Assistive Control",
    "abstract": "In this work, we address the problem of solving complex collaborative robotic\ntasks subject to multiple varying parameters. Our approach combines\nsimultaneous policy blending with system identification to create generalized\npolicies that are robust to changes in system parameters. We employ a blending\nnetwork whose state space relies solely on parameter estimates from a system\nidentification technique. As a result, this blending network learns how to\nhandle parameter changes instead of trying to learn how to solve the task for a\ngeneralized parameter set simultaneously. We demonstrate our scheme's ability\non a collaborative robot and human itching task in which the human has motor\nimpairments. We then showcase our approach's efficiency with a variety of\nsystem identification techniques when compared to standard domain\nrandomization.",
    "descriptor": "\nComments: Accepted to ICRA 2022\n",
    "authors": [
      "Luke Bhan",
      "Marcos Quinones-Grueiro",
      "Gautam Biswas"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09836"
  },
  {
    "id": "arXiv:2205.09837",
    "title": "Summarization as Indirect Supervision for Relation Extraction",
    "abstract": "Relation extraction (RE) models have been challenged by their reliance on\ntraining data with expensive annotations. Considering that summarization tasks\naim at acquiring concise expressions of synoptical information from the longer\ncontext, these tasks naturally align with the objective of RE, i.e., extracting\na kind of synoptical information that describes the relation of entity\nmentions. We present SuRE, which converts RE into a summarization formulation.\nSuRE leads to more precise and resource-efficient RE based on indirect\nsupervision from summarization tasks. To achieve this goal, we develop sentence\nand relation conversion techniques that essentially bridge the formulation of\nsummarization and RE tasks. We also incorporate constraint decoding techniques\nwith Trie scoring to further enhance summarization-based RE with robust\ninference. Experiments on three RE datasets demonstrate the effectiveness of\nSuRE in both full-dataset and low-resource settings, showing that summarization\nis a promising source of indirect supervision to improve RE models.",
    "descriptor": "",
    "authors": [
      "Keming Lu",
      "I-Hung Hsu",
      "Wenxuan Zhou",
      "Mingyu Derek Ma",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09837"
  },
  {
    "id": "arXiv:2205.09838",
    "title": "Why GANs are overkill for NLP",
    "abstract": "This work offers a novel theoretical perspective on why, despite numerous\nattempts, adversarial approaches to generative modeling (e.g., GANs) have not\nbeen as popular for certain generation tasks, particularly sequential tasks\nsuch as Natural Language Generation, as they have in others, such as Computer\nVision. In particular, on sequential data such as text, maximum-likelihood\napproaches are significantly more utilized than GANs. We show that, while it\nmay seem that maximizing likelihood is inherently different than minimizing\ndistinguishability, this distinction is largely artificial and only holds for\nlimited models. We argue that minimizing KL-divergence (i.e., maximizing\nlikelihood) is a more efficient approach to effectively minimizing the same\ndistinguishability criteria that adversarial models seek to optimize.\nReductions show that minimizing distinguishability can be seen as simply\nboosting likelihood for certain families of models including n-gram models and\nneural networks with a softmax output layer. To achieve a full polynomial-time\nreduction, a novel next-token distinguishability model is considered.",
    "descriptor": "",
    "authors": [
      "David Alvarez-Melis",
      "Vikas Garg",
      "Adam Tauman Kalai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09838"
  },
  {
    "id": "arXiv:2205.09839",
    "title": "HyBNN and FedHyBNN: (Federated) Hybrid Binary Neural Networks",
    "abstract": "Binary Neural Networks (BNNs), neural networks with weights and activations\nconstrained to -1(0) and +1, are an alternative to deep neural networks which\noffer faster training, lower memory consumption and lightweight models, ideal\nfor use in resource constrained devices while being able to utilize the\narchitecture of their deep neural network counterpart. However, the input\nbinarization step used in BNNs causes a severe accuracy loss. In this paper, we\nintroduce a novel hybrid neural network architecture, Hybrid Binary Neural\nNetwork (HyBNN), consisting of a task-independent, general, full-precision\nvariational autoencoder with a binary latent space and a task specific binary\nneural network that is able to greatly limit the accuracy loss due to input\nbinarization by using the full precision variational autoencoder as a feature\nextractor. We use it to combine the state-of-the-art accuracy of deep neural\nnetworks with the much faster training time, quicker test-time inference and\npower efficiency of binary neural networks. We show that our proposed system is\nable to very significantly outperform a vanilla binary neural network with\ninput binarization. We also introduce FedHyBNN, a highly communication\nefficient federated counterpart to HyBNN and demonstrate that it is able to\nreach the same accuracy as its non-federated equivalent. We make our source\ncode, experimental parameters and models available at:\nhttps://anonymous.4open.science/r/HyBNN.",
    "descriptor": "\nComments: 11 pages, 7 figures, submitted to neurips\n",
    "authors": [
      "Kinshuk Dua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.09839"
  },
  {
    "id": "arXiv:2205.09840",
    "title": "A toolbox for idea generation and evaluation: Machine learning,  data-driven, and contest-driven approaches to support idea generation",
    "abstract": "The significance and abundance of data are increasing due to the growing\ndigital data generated from social media, sensors, scholarly literature,\npatents, different forms of documents published online, databases, product\nmanuals, etc. Various data sources can be used to generate ideas, yet, in\naddition to bias, the size of the available digital data is a major challenge\nwhen it comes to manual analysis. Hence, human-machine interaction is essential\nfor generating valuable ideas where machine learning and data-driven techniques\ngenerate patterns from data and serve human sense-making. However, the use of\nmachine learning and data-driven approaches to generate ideas is a relatively\nnew area. Moreover, it is also possible to stimulate innovation using\ncontest-driven idea generation and evaluation. The results and contributions of\nthis thesis can be viewed as a toolbox of idea-generation techniques, including\na list of data-driven and machine learning techniques with corresponding data\nsources and models to support idea generation. In addition, the results include\ntwo models, one method and one framework, to better support data-driven and\ncontest- driven idea generation. The beneficiaries of these artefacts are\npractitioners in data and knowledge engineering, data mining project managers,\nand innovation agents. Innovation agents include incubators, contest\norganizers, consultants, innovation accelerators, and industries. Since the\nproposed artefacts consist of process models augmented with AI techniques,\nhuman-centred AI is a promising area of research that can contribute to the\nartefacts' further development and promote creativity.",
    "descriptor": "\nComments: ISBN 978-91-7911-790-0 ISBN 978-91-7911-791-7 ISSN 1101-8526\n",
    "authors": [
      "Workneh Yilma Ayele"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09840"
  },
  {
    "id": "arXiv:2205.09841",
    "title": "Subcellular Protein Localisation in the Human Protein Atlas using  Ensembles of Diverse Deep Architectures",
    "abstract": "Automated visual localisation of subcellular proteins can accelerate our\nunderstanding of cell function in health and disease. Despite recent advances\nin machine learning (ML), humans still attain superior accuracy by using\ndiverse clues. We show how this gap can be narrowed by addressing three key\naspects: (i) automated improvement of cell annotation quality, (ii) new\nConvolutional Neural Network (CNN) architectures supporting unbalanced and\nnoisy data, and (iii) informed selection and fusion of multiple & diverse\nmachine learning models. We introduce a new \"AI-trains-AI\" method for improving\nthe quality of weak labels and propose novel CNN architectures exploiting\nwavelet filters and Weibull activations. We also explore key factors in the\nmulti-CNN ensembling process by analysing correlations between image-level and\ncell-level predictions. Finally, in the context of the Human Protein Atlas, we\ndemonstrate that our system achieves state-of-the-art performance in the\nmulti-label single-cell classification of protein localisation patterns. It\nalso significantly improves generalisation ability.",
    "descriptor": "",
    "authors": [
      "Syed Sameed Husain",
      "Eng-Jon Ong",
      "Dmitry Minskiy",
      "Mikel Bober-Irizar",
      "Amaia Irizar",
      "Miroslaw Bober"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09841"
  },
  {
    "id": "arXiv:2205.09843",
    "title": "Table Retrieval May Not Necessitate Table-specific Model Design",
    "abstract": "Tables are an important form of structured data for both human and machine\nreaders alike, providing answers to questions that cannot, or cannot easily, be\nfound in texts. Recent work has designed special models and training paradigms\nfor table-related tasks such as table-based question answering and table\nretrieval. Though effective, they add complexity in both modeling and data\nacquisition compared to generic text solutions and obscure which elements are\ntruly beneficial. In this work, we focus on the task of table retrieval, and\nask: \"is table-specific model design necessary for table retrieval, or can a\nsimpler text-based model be effectively used to achieve a similar result?\"\nFirst, we perform an analysis on a table-based portion of the Natural Questions\ndataset (NQ-table), and find that structure plays a negligible role in more\nthan 70% of the cases. Based on this, we experiment with a general Dense\nPassage Retriever (DPR) based on text and a specialized Dense Table Retriever\n(DTR) that uses table-specific model designs. We find that DPR performs well\nwithout any table-specific design and training, and even achieves superior\nresults compared to DTR when fine-tuned on properly linearized tables. We then\nexperiment with three modules to explicitly encode table structures, namely\nauxiliary row/column embeddings, hard attention masks, and soft relation-based\nattention biases. However, none of these yielded significant improvements,\nsuggesting that table-specific model design may not be necessary for table\nretrieval.",
    "descriptor": "\nComments: 11 pages total, 4 figures\n",
    "authors": [
      "Zhiruo Wang",
      "Zhengbao Jiang",
      "Eric Nyberg",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09843"
  },
  {
    "id": "arXiv:2205.09845",
    "title": "Spikemax: Spike-based Loss Methods for Classification",
    "abstract": "Spiking Neural Networks~(SNNs) are a promising research paradigm for low\npower edge-based computing. Recent works in SNN backpropagation has enabled\ntraining of SNNs for practical tasks. However, since spikes are binary events\nin time, standard loss formulations are not directly compatible with spike\noutput. As a result, current works are limited to using mean-squared loss of\nspike count. In this paper, we formulate the output probability interpretation\nfrom the spike count measure and introduce spike-based negative log-likelihood\nmeasure which are more suited for classification tasks especially in terms of\nthe energy efficiency and inference latency. We compare our loss measures with\nother existing alternatives and evaluate using classification performances on\nthree neuromorphic benchmark datasets: NMNIST, DVS Gesture and N-TIDIGITS18. In\naddition, we demonstrate state of the art performances on these datasets,\nachieving faster inference speed and less energy consumption.",
    "descriptor": "\nComments: Accepted by IJCNN 2022\n",
    "authors": [
      "Sumit Bam Shrestha",
      "Longwei Zhu",
      "Pengfei Sun"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.09845"
  },
  {
    "id": "arXiv:2205.09849",
    "title": "Confident Clustering via PCA Compression Ratio and Its Application to  Single-cell RNA-seq Analysis",
    "abstract": "Unsupervised clustering algorithms for vectors has been widely used in the\narea of machine learning. Many applications, including the biological data we\nstudied in this paper, contain some boundary datapoints which show combination\nproperties of two underlying clusters and could lower the performance of the\ntraditional clustering algorithms. We develop a confident clustering method\naiming to diminish the influence of these datapoints and improve the clustering\nresults. Concretely, for a list of datapoints, we give two clustering results.\nThe first-round clustering attempts to classify only pure vectors with high\nconfidence. Based on it, we classify more vectors with less confidence in the\nsecond round. We validate our algorithm on single-cell RNA-seq data, which is a\npowerful and widely used tool in biology area. Our confident clustering shows a\nhigh accuracy on our tested datasets. In addition, unlike traditional\nclustering methods in single-cell analysis, the confident clustering shows high\nstability under different choices of parameters.",
    "descriptor": "",
    "authors": [
      "Yingcong Li",
      "Chandra Sekhar Mukherjee",
      "Jiapeng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.09849"
  },
  {
    "id": "arXiv:2205.09852",
    "title": "Deconfounding Actor-Critic Network with Policy Adaptation for Dynamic  Treatment Regimes",
    "abstract": "Despite intense efforts in basic and clinical research, an individualized\nventilation strategy for critically ill patients remains a major challenge.\nRecently, dynamic treatment regime (DTR) with reinforcement learning (RL) on\nelectronic health records (EHR) has attracted interest from both the healthcare\nindustry and machine learning research community. However, most learned DTR\npolicies might be biased due to the existence of confounders. Although some\ntreatment actions non-survivors received may be helpful, if confounders cause\nthe mortality, the training of RL models guided by long-term outcomes (e.g.,\n90-day mortality) would punish those treatment actions causing the learned DTR\npolicies to be suboptimal. In this study, we develop a new deconfounding\nactor-critic network (DAC) to learn optimal DTR policies for patients. To\nalleviate confounding issues, we incorporate a patient resampling module and a\nconfounding balance module into our actor-critic framework. To avoid punishing\nthe effective treatment actions non-survivors received, we design a short-term\nreward to capture patients' immediate health state changes. Combining\nshort-term with long-term rewards could further improve the model performance.\nMoreover, we introduce a policy adaptation method to successfully transfer the\nlearned model to new-source small-scale datasets. The experimental results on\none semi-synthetic and two different real-world datasets show the proposed\nmodel outperforms the state-of-the-art models. The proposed model provides\nindividualized treatment decisions for mechanical ventilation that could\nimprove patient outcomes.",
    "descriptor": "",
    "authors": [
      "Changchang Yin",
      "Ruoqi Liu",
      "Jeffrey Caterino",
      "Ping Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09852"
  },
  {
    "id": "arXiv:2205.09853",
    "title": "Masked Conditional Video Diffusion for Prediction, Generation, and  Interpolation",
    "abstract": "Video prediction is a challenging task. The quality of video frames from\ncurrent state-of-the-art (SOTA) generative models tends to be poor and\ngeneralization beyond the training data is difficult. Furthermore, existing\nprediction frameworks are typically not capable of simultaneously handling\nother video-related tasks such as unconditional generation or interpolation. In\nthis work, we devise a general-purpose framework called Masked Conditional\nVideo Diffusion (MCVD) for all of these video synthesis tasks using a\nprobabilistic conditional score-based denoising diffusion model, conditioned on\npast and/or future frames. We train the model in a manner where we randomly and\nindependently mask all the past frames or all the future frames. This novel but\nstraightforward setup allows us to train a single model that is capable of\nexecuting a broad range of video tasks, specifically: future/past prediction --\nwhen only future/past frames are masked; unconditional generation -- when both\npast and future frames are masked; and interpolation -- when neither past nor\nfuture frames are masked. Our experiments show that this approach can generate\nhigh-quality frames for diverse types of videos. Our MCVD models are built from\nsimple non-recurrent 2D-convolutional architectures, conditioning on blocks of\nframes and generating blocks of frames. We generate videos of arbitrary lengths\nautoregressively in a block-wise manner. Our approach yields SOTA results\nacross standard video prediction and interpolation benchmarks, with computation\ntimes for training models measured in 1-12 days using $\\le$ 4 GPUs.\nhttps://mask-cond-video-diffusion.github.io",
    "descriptor": "\nComments: 9 pages, 4 figures, 7 tables\n",
    "authors": [
      "Vikram Voleti",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09853"
  },
  {
    "id": "arXiv:2205.09858",
    "title": "Fidyll: A Compiler for Cross-Format Data Stories & Explorable  Explanations",
    "abstract": "Narrative visualization is a powerful communicative tool that can take on\nvarious formats such as interactive articles, slideshows, and data videos.\nThese formats each have their strengths and weaknesses, but existing authoring\ntools only support one output target. We conducted a series of formative\ninterviews with seven domain experts to understand needs and practices around\ncross-format data stories, and developed Fidyll, a cross-format compiler for\nauthoring interactive data stories and explorable explanations. Our open-source\ntool can be used to rapidly create formats including static articles,\nlow-motion articles, interactive articles, slideshows, and videos. We evaluate\nour system through a series of real-world usage scenarios, showing how it\nbenefits authors in the domains of data journalism, scientific publishing, and\nnonprofit advocacy. We show how Fidyll, provides expressive leverage by\nreducing the amount of non-narrative markup that authors need to write by\n80-90% compared to Idyll, an existing markup language for authoring interactive\narticles.",
    "descriptor": "\nComments: 10 pages, 6 figures, for associated examples see this https URL\n",
    "authors": [
      "Matthew Conlen",
      "Jeffrey Heer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.09858"
  },
  {
    "id": "arXiv:2205.09860",
    "title": "Mean-Field Analysis of Two-Layer Neural Networks: Global Optimality with  Linear Convergence Rates",
    "abstract": "We consider optimizing two-layer neural networks in the mean-field regime\nwhere the learning dynamics of network weights can be approximated by the\nevolution in the space of probability measures over the weight parameters\nassociated with the neurons. The mean-field regime is a theoretically\nattractive alternative to the NTK (lazy training) regime which is only\nrestricted locally in the so-called neural tangent kernel space around\nspecialized initializations. Several prior works (\\cite{mei2018mean,\nchizat2018global}) establish the asymptotic global optimality of the mean-field\nregime, but it is still challenging to obtain a quantitative convergence rate\ndue to the complicated nonlinearity of the training dynamics. This work\nestablishes a new linear convergence result for two-layer neural networks\ntrained by continuous-time noisy gradient descent in the mean-field regime. Our\nresult relies on a novelty logarithmic Sobolev inequality for two-layer neural\nnetworks, and uniform upper bounds on the logarithmic Sobolev constants for a\nfamily of measures determined by the evolving distribution of hidden neurons.",
    "descriptor": "",
    "authors": [
      "Jingwei Zhang",
      "Xunpeng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09860"
  },
  {
    "id": "arXiv:2205.09862",
    "title": "Recurrent segmentation meets block models in temporal networks",
    "abstract": "A popular approach to model interactions is to represent them as a network\nwith nodes being the agents and the interactions being the edges. Interactions\nare often timestamped, which leads to having timestamped edges. Many real-world\ntemporal networks have a recurrent or possibly cyclic behaviour. For example,\nsocial network activity may be heightened during certain hours of day. In this\npaper, our main interest is to model recurrent activity in such temporal\nnetworks. As a starting point we use stochastic block model, a popular choice\nfor modelling static networks, where nodes are split into $R$ groups. We extend\nthis model to temporal networks by modelling the edges with a Poisson process.\nWe make the parameters of the process dependent on time by segmenting the time\nline into $K$ segments. To enforce the recurring activity we require that only\n$H < K$ different set of parameters can be used, that is, several, not\nnecessarily consecutive, segments must share their parameters. We prove that\nthe searching for optimal blocks and segmentation is an NP-hard problem.\nConsequently, we split the problem into 3 subproblems where we optimize blocks,\nmodel parameters, and segmentation in turn while keeping the remaining\nstructures fixed. We propose an iterative algorithm that requires $O(KHm + Rn +\nR^2H)$ time per iteration, where $n$ and $m$ are the number of nodes and edges\nin the network. We demonstrate experimentally that the number of required\niterations is typically low, the algorithm is able to discover the ground truth\nfrom synthetic datasets, and show that certain real-world networks exhibit\nrecurrent behaviour as the likelihood does not deteriorate when $H$ is lowered.",
    "descriptor": "",
    "authors": [
      "{Chamalee Wickrama Arachchi",
      "Nikolaj Tatti"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09862"
  },
  {
    "id": "arXiv:2205.09864",
    "title": "Automated Scoring for Reading Comprehension via In-context BERT Tuning",
    "abstract": "Automated scoring of open-ended student responses has the potential to\nsignificantly reduce human grader effort. Recent advances in automated scoring\noften leverage textual representations based on pre-trained language models\nsuch as BERT and GPT as input to scoring models. Most existing approaches train\na separate model for each item/question, which is suitable for scenarios such\nas essay scoring where items can be quite different from one another. However,\nthese approaches have two limitations: 1) they fail to leverage item linkage\nfor scenarios such as reading comprehension where multiple items may share a\nreading passage; 2) they are not scalable since storing one model per item\nbecomes difficult when models have a large number of parameters. In this paper,\nwe report our (grand prize-winning) solution to the National Assessment of\nEducation Progress (NAEP) automated scoring challenge for reading\ncomprehension. Our approach, in-context BERT fine-tuning, produces a single\nshared scoring model for all items with a carefully-designed input structure to\nprovide contextual information on each item. We demonstrate the effectiveness\nof our approach via local evaluations using the training dataset provided by\nthe challenge. We also discuss the biases, common error types, and limitations\nof our approach.",
    "descriptor": "\nComments: Published as a conference paper at AIED 2022. A grand prize-winner for the NAEP AS Challenge. Code available at: this https URL\n",
    "authors": [
      "Nigel Fernandez",
      "Aritra Ghosh",
      "Naiming Liu",
      "Zichao Wang",
      "Beno\u00eet Choffin",
      "Richard Baraniuk",
      "Andrew Lan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.09864"
  },
  {
    "id": "arXiv:2205.09867",
    "title": "Gender Bias in Meta-Embeddings",
    "abstract": "Combining multiple source embeddings to create meta-embeddings is considered\neffective to obtain more accurate embeddings. Different methods have been\nproposed to develop meta-embeddings from a given set of source embeddings.\nHowever, the source embeddings can contain unfair gender bias, and the bias in\nthe combination of multiple embeddings and debiasing it effectively have not\nbeen studied yet. In this paper, we investigate the bias in three types of\nmeta-embeddings: (1) Multi-Source No-Debiasing: meta-embedding from multiple\nsource embeddings without any debiasing. The experimental results show that\nmeta-embedding amplifies the gender bias compared to those of input source\nembeddings; (2) Multi-Source Single-Debiasing: meta-embedding from multiple\nsource embeddings debiased by a single method and it can be created in three\nways: debiasing each source embedding, debiasing the learned meta-embeddings,\nand debiasing both source embeddings and meta-embeddings. The results show that\ndebiasing both is the best in two out of three bias evaluation benchmarks; (3)\nSingle-Source Multi-Debiasing: meta-embedding from the same source embedding\ndebiased by different methods. It performed more effectively than its source\nembeddings debiased with a single method in all three bias evaluation\nbenchmarks.",
    "descriptor": "",
    "authors": [
      "Masahiro Kaneko",
      "Danushka Bollegala",
      "Naoaki Okazaki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09867"
  },
  {
    "id": "arXiv:2205.09868",
    "title": "Service Delay Minimization for Federated Learning over Mobile Devices",
    "abstract": "Federated learning (FL) over mobile devices has fostered numerous intriguing\napplications/services, many of which are delay-sensitive. In this paper, we\npropose a service delay efficient FL (SDEFL) scheme over mobile devices. Unlike\ntraditional communication efficient FL, which regards wireless communications\nas the bottleneck, we find that under many situations, the local computing\ndelay is comparable to the communication delay during the FL training process,\ngiven the development of high-speed wireless transmission techniques. Thus, the\nservice delay in FL should be computing delay + communication delay over\ntraining rounds. To minimize the service delay of FL, simply reducing local\ncomputing/communication delay independently is not enough. The delay trade-off\nbetween local computing and wireless communications must be considered.\nBesides, we empirically study the impacts of local computing control and\ncompression strategies (i.e., the number of local updates, weight quantization,\nand gradient quantization) on computing, communication and service delays.\nBased on those trade-off observation and empirical studies, we develop an\noptimization scheme to minimize the service delay of FL over heterogeneous\ndevices. We establish testbeds and conduct extensive emulations/experiments to\nverify our theoretical analysis. The results show that SDEFL reduces notable\nservice delay with a small accuracy drop compared to peer designs.",
    "descriptor": "\nComments: 15 pages, 9 figures\n",
    "authors": [
      "Rui Chen",
      "Dian Shi",
      "Xiaoqi Qin",
      "Dongjie Liu",
      "Miao Pan",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.09868"
  },
  {
    "id": "arXiv:2205.09869",
    "title": "Transformer with Memory Replay",
    "abstract": "Transformers achieve state-of-the-art performance for natural language\nprocessing tasks by pre-training on large-scale text corpora. They are\nextremely compute-intensive and have very high sample complexity. Memory replay\nis a mechanism that remembers and reuses past examples by saving to and\nreplaying from a memory buffer. It has been successfully used in reinforcement\nlearning and GANs due to better sample efficiency. In this paper, we propose\n\\emph{Transformer with Memory Replay} (TMR), which integrates memory replay\nwith transformer, making transformer more sample-efficient. Experiments on GLUE\nand SQuAD benchmark datasets show that Transformer with Memory Replay achieves\nat least $1\\%$ point increase compared to the baseline transformer model when\npretrained with the same number of examples. Further, by adopting a careful\ndesign that reduces the wall-clock time overhead of memory replay, we also\nempirically achieve a better runtime efficiency.",
    "descriptor": "\nComments: Accepted to AAAI 2022\n",
    "authors": [
      "Rui Liu",
      "Barzan Mozafari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09869"
  },
  {
    "id": "arXiv:2205.09873",
    "title": "Differentially Private Linear Sketches: Efficient Implementations and  Applications",
    "abstract": "Linear sketches have been widely adopted to process fast data streams, and\nthey can be used to accurately answer frequency estimation, approximate top K\nitems, and summarize data distributions. When data are sensitive, it is\ndesirable to provide privacy guarantees for linear sketches to preserve private\ninformation while delivering useful results with theoretical bounds. We show\nthat linear sketches can ensure privacy and maintain their unique properties\nwith a small amount of noise added at initialization. From the differentially\nprivate linear sketches, we showcase that the state-of-the-art quantile sketch\nin the turnstile model can also be private and maintain high performance.\nExperiments further demonstrate that our proposed differentially private\nsketches are quantitatively and qualitatively similar to noise-free sketches\nwith high utilization on synthetic and real datasets.",
    "descriptor": "",
    "authors": [
      "Fuheng Zhao",
      "Dan Qiao",
      "Rachel Redberg",
      "Divyakant Agrawal",
      "Amr El Abbadi",
      "Yu-Xiang Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.09873"
  },
  {
    "id": "arXiv:2205.09875",
    "title": "Incremental Learning with Differentiable Architecture and Forgetting  Search",
    "abstract": "As progress is made on training machine learning models on incrementally\nexpanding classification tasks (i.e., incremental learning), a next step is to\ntranslate this progress to industry expectations. One technique missing from\nincremental learning is automatic architecture design via Neural Architecture\nSearch (NAS). In this paper, we show that leveraging NAS for incremental\nlearning results in strong performance gains for classification tasks.\nSpecifically, we contribute the following: first, we create a strong baseline\napproach for incremental learning based on Differentiable Architecture Search\n(DARTS) and state-of-the-art incremental learning strategies, outperforming\nmany existing strategies trained with similar-sized popular architectures;\nsecond, we extend the idea of architecture search to regularize architecture\nforgetting, boosting performance past our proposed baseline. We evaluate our\nmethod on both RF signal and image classification tasks, and demonstrate we can\nachieve up to a 10% performance increase over state-of-the-art methods. Most\nimportantly, our contribution enables learning from continuous distributions on\nreal-world application data for which the complexity of the data distribution\nis unknown, or the modality less explored (such as RF signal classification).",
    "descriptor": "\nComments: Accepted by the 2022 International Joint Conference on Neural Networks (IJCNN 2022)\n",
    "authors": [
      "James Seale Smith",
      "Zachary Seymour",
      "Han-Pang Chiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09875"
  },
  {
    "id": "arXiv:2205.09877",
    "title": "Probabilistic Quality of Service aware Service Selection",
    "abstract": "In software-as-a-service paradigms software systems are no longer monolithic\npieces of code executing within the boundaries of an organisation, on the\ncontrary, they are conceived as a dynamically changing collection of services,\ncollectively executing, in pursuit of a common business goal. An essential\naspect of service selection is determining whether the Quality of Service (QoS)\nprofile of a service satisfies the QoS requirements of a client.\nIn realistic execution environments, such QoS values might be influenced by\nexternal, non-controllable events, making it impossible for the service\nprovider to guarantee that the values characterised by a QoS profile will be\nmet, naturally leading to the need of a probabilistic interpretation of QoS\nprofile.\nIn this work we propose: 1) a model for describing probabilistic QoS profiles\nbased on multivariate continuous probability distributions, 2) a language for\ndescribing probabilistic QoS requirements, and 3) an automatic procedure for\nassessing whether a probabilistic QoS profile satisfies a probabilistic QoS\nrequirement.",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Agust\u00edn E. Martinez Su\u00f1\u00e9",
      "Carlos G. Lopez Pombo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.09877"
  },
  {
    "id": "arXiv:2205.09878",
    "title": "Real Time Multi-Object Detection for Helmet Safety",
    "abstract": "The National Football League and Amazon Web Services teamed up to develop the\nbest sports injury surveillance and mitigation program via the Kaggle\ncompetition. Through which the NFL wants to assign specific players to each\nhelmet, which would help accurately identify each player's \"exposures\"\nthroughout a football play. We are trying to implement a computer vision based\nML algorithms capable of assigning detected helmet impacts to correct players\nvia tracking information. Our paper will explain the approach to automatically\ntrack player helmets and their collisions. This will also allow them to review\nprevious plays and explore the trends in exposure over time.",
    "descriptor": "",
    "authors": [
      "Mrinal Mathur",
      "Archana Benkkallpalli Chandrashekhar",
      "Venkata Krishna Chaithanya Nuthalapati"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09878"
  },
  {
    "id": "arXiv:2205.09880",
    "title": "Beyond Labels: Visual Representations for Bone Marrow Cell Morphology  Recognition",
    "abstract": "Analyzing and inspecting bone marrow cell cytomorphology is a critical but\nhighly complex and time-consuming component of hematopathology diagnosis.\nRecent advancements in artificial intelligence have paved the way for the\napplication of deep learning algorithms to complex medical tasks. Nevertheless,\nthere are many challenges in applying effective learning algorithms to medical\nimage analysis, such as the lack of sufficient and reliably annotated training\ndatasets and the highly class-imbalanced nature of most medical data. Here, we\nimprove on the state-of-the-art methodologies of bone marrow cell recognition\nby deviating from sole reliance on labeled data and leveraging self-supervision\nin training our learning models. We investigate our approach's effectiveness in\nidentifying bone marrow cell types. Our experiments demonstrate significant\nperformance improvements in conducting different bone marrow cell recognition\ntasks compared to the current state-of-the-art methodologies.",
    "descriptor": "",
    "authors": [
      "Shayan Fazeli",
      "Alireza Samiei",
      "Thomas D. Lee",
      "Majid Sarrafzadeh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09880"
  },
  {
    "id": "arXiv:2205.09883",
    "title": "A Rule Search Framework for the Early Identification of Chronic  Emergency Homeless Shelter Clients",
    "abstract": "This paper uses rule search techniques for the early identification of\nemergency homeless shelter clients who are at risk of becoming long term or\nchronic shelter users. Using a data set from a major North American shelter\ncontaining 12 years of service interactions with over 40,000 individuals, the\noptimized pruning for unordered search (OPUS) algorithm is used to develop\nrules that are both intuitive and effective. The rules are evaluated within a\nframework compatible with the real-time delivery of a housing program meant to\ntransition high risk clients to supportive housing. Results demonstrate that\nthe median time to identification of clients at risk of chronic shelter use\ndrops from 297 days to 162 days when the methods in this paper are applied.",
    "descriptor": "",
    "authors": [
      "Caleb John",
      "Geoffrey G. Messier"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09883"
  },
  {
    "id": "arXiv:2205.09884",
    "title": "Time Series Anomaly Detection via Reinforcement Learning-Based Model  Selection",
    "abstract": "Time series anomaly detection is of critical importance for the reliable and\nefficient operation of real-world systems. Many anomaly detection models have\nbeen developed throughout the years based on various assumptions regarding\nanomaly characteristics. However, due to the complex nature of real-world data,\ndifferent anomalies within a time series usually have diverse profiles\nsupporting different anomaly assumptions, making it difficult to find a single\nanomaly detector that can consistently beat all other models. In this work, to\nharness the benefits of different base models, we assume that a pool of anomaly\ndetection models is accessible and propose to utilize reinforcement learning to\ndynamically select a candidate model from these base models. Experiments on\nreal-world data have been implemented. It is demonstrated that the proposed\nstrategy can outperforms all baseline models in terms of overall performance.",
    "descriptor": "\nComments: 6 pages, 3 figures, submitted to IEEE Canadian Conference on Electrical and Computer Engineering (CCECE) 2022\n",
    "authors": [
      "Jiuqi Elise Zhang",
      "Di Wu",
      "Benoit Boulet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09884"
  },
  {
    "id": "arXiv:2205.09887",
    "title": "Location-Aided Beamforming in Mobile Millimeter-Wave Networks",
    "abstract": "Due to the large bandwidth available, millimeter-Wave (mmWave) bands are\nconsidered a viable opportunity to significantly increase the data rate in\ncellular and wireless networks. Nevertheless, the need for beamforming and\ndirectional communication between the transmitter and the receiver increases\nthe complexity of the channel estimation and link establishment phase.\nLocation-aided beamforming approaches have the potential to enable fast link\nestablishment in mmWave networks. However, these are often very sensitive to\nlocation errors. In this work, we propose a beamforming algorithm based on\ntracking spatial correlation of the available strong paths between the\ntransmitter and the receiver. We show that our method is robust to uncertainty\nin location information, i.e., location error and can provide a reliable\nconnection to a moving user along a trajectory. The numerical results show that\nour approach outperforms benchmarks on various levels of error in the location\ninformation accuracy. The gain is more prominent in high location error\nscenarios.",
    "descriptor": "",
    "authors": [
      "Sara Khosravi",
      "Hossein S.Ghadikolaeiy",
      "Jens Zander",
      "Marina Petrova"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09887"
  },
  {
    "id": "arXiv:2205.09888",
    "title": "Solving sparse polynomial systems using Groebner bases and resultants",
    "abstract": "Solving systems of polynomial equations is a central problem in nonlinear and\ncomputational algebra. Since Buchberger's algorithm for computing Gr\\\"obner\nbases in the 60s, there has been a lot of progress in this domain. Moreover,\nthese equations have been employed to model and solve problems from diverse\ndisciplines such as biology, cryptography, and robotics. Currently, we have a\ngood understanding of how to solve generic systems from a theoretical and\nalgorithmic point of view. However, polynomial equations encountered in\npractice are usually structured, and so many properties and results about\ngeneric systems do not apply to them. For this reason, a common trend in the\nlast decades has been to develop mathematical and algorithmic frameworks to\nexploit specific structures of systems of polynomials.\nArguably, the most common structure is sparsity; that is, the polynomials of\nthe systems only involve a few monomials. Since Bernstein, Khovanskii, and\nKushnirenko's work on the expected number of solutions of sparse systems, toric\ngeometry has been the default mathematical framework to employ sparsity. In\nparticular, it is the crux of the matter behind the extension of classical\ntools to systems, such as resultant computations, homotopy continuation\nmethods, and most recently, Gr\\\"obner bases. In this work, we will review these\nclassical tools, their extensions, and recent progress in exploiting sparsity\nfor solving polynomial systems.\nThis manuscript complements its homonymous tutorial presented at the\nconference ISSAC 2022.",
    "descriptor": "",
    "authors": [
      "Mat\u00edas R. Bender"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2205.09888"
  },
  {
    "id": "arXiv:2205.09891",
    "title": "Interpolating Compressed Parameter Subspaces",
    "abstract": "Inspired by recent work on neural subspaces and mode connectivity, we revisit\nparameter subspace sampling for shifted and/or interpolatable input\ndistributions (instead of a single, unshifted distribution). We enforce a\ncompressed geometric structure upon a set of trained parameters mapped to a set\nof train-time distributions, denoting the resulting subspaces as Compressed\nParameter Subspaces (CPS). We show the success and failure modes of the types\nof shifted distributions whose optimal parameters reside in the CPS. We find\nthat ensembling point-estimates within a CPS can yield a high average accuracy\nacross a range of test-time distributions, including backdoor, adversarial,\npermutation, stylization and rotation perturbations. We also find that the CPS\ncan contain low-loss point-estimates for various task shifts (albeit\ninterpolated, perturbed, unseen or non-identical coarse labels). We further\ndemonstrate this property in a continual learning setting with CIFAR100.",
    "descriptor": "",
    "authors": [
      "Siddhartha Datta",
      "Nigel Shadbolt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09891"
  },
  {
    "id": "arXiv:2205.09892",
    "title": "Obfuscating the Hierarchy of a Digital IP",
    "abstract": "Numerous security threats are emerging from untrusted players in the\nintegrated circuit (IC) ecosystem. Among them, reverse engineering practices\nwith the intent to counterfeit, overproduce, or modify an IC are worrying. In\nrecent years, various techniques have been proposed to mitigate the\naforementioned threats but no technique seems to be adequate to hide the\nhierarchy of a design. Such ability to obfuscate the hierarchy is particularly\nimportant for designs that contain repeated modules. In this paper, we propose\na novel way to obfuscate such designs by leveraging conventional logic\nsynthesis. We exploit multiple optimizations that are available in the\nsynthesis tool to create design diversity. Our security analysis, performed by\nusing the DANA reverse engineering tool, confirms the significant impact of\nthese optimizations on obfuscation. Among the many considered obfuscated design\ninstances, users can find options that incur very small overheads while still\nconfusing the work of a reverse engineer.",
    "descriptor": "",
    "authors": [
      "Giorgi Basiashvili",
      "Zail Ul Abideen",
      "Samuel Pagliarini"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.09892"
  },
  {
    "id": "arXiv:2205.09895",
    "title": "A BCS-GDE Multi-objective Optimization Algorithm for Combined Cooling,  Heating and Power Model with Decision Strategies",
    "abstract": "District energy systems can not only reduce energy consumption but also set\nenergy supply dispatching schemes according to demand. In addition to economic\ncost, energy consumption and pollutant are more worthy of attention when\nevaluating combined cooling, heating and power (CCHP) models. In this paper,\nthe CCHP model is established with the objective of economic cost, primary\nenergy consumption, and pollutant emissions. The mathematical expression of the\nCCHP system is proposed, and a multi-objective optimization model with\nconstraints is established. According to different usage requirements, two\ndecision-making strategies are designed, which can adapt to different\nscenarios. Besides, a generalized differential evolution with the best\ncompromise solution processing mechanism (BCS-GDE) algorithm is proposed to\noptimize the CCHP model for the first time. The algorithm provides the optimal\nenergy scheduling scheme by optimizing the production capacity of different\ncapacity equipment. The simulation is conducted in three application scenarios:\nhotels, offices, and residential buildings. The simulation results show that\nthe model established in this paper can reduce economic cost by 72%, primary\nenergy consumption by 73%, and pollutant emission by 88%. Concurrently, the\nWilcoxon signed-rank test shows that BCSGDE is significantly better than\nOMOPSO, NSGA-II, and SPEA2 with greater than 95% confidence.",
    "descriptor": "\nComments: Accpeted by Applied Thermal Engineering. arXiv admin note: substantial text overlap with arXiv:2108.07394\n",
    "authors": [
      "Jiaze Sun",
      "Jiahui Deng",
      "Yang Li",
      "Nan Han"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09895"
  },
  {
    "id": "arXiv:2205.09897",
    "title": "An Empirical Evaluation of the Implementation of the California Consumer  Privacy Act (CCPA)",
    "abstract": "On January 1, 2020, California passed the California Consumer Privacy Act\n(CCPA) by more than 56% of voters intended to enhance privacy rights and\nconsumer protection for residents of California, United States. Since then,\nmore conditions have been added to the Act to support consumers' privacy. In\naddition, two years after the first effective day of CCPA, consumers have seen\nCalifornia organizations apply approaches to adapt to CCPA. Many organizations\nquickly upgrade their policy to comply with the legislation and create\neffective platforms such as data portals that allow consumers to exercise their\nprivacy rights. However, on the other hand, we still noticed aspects of CCPA\nbeing absent on some websites. Additionally, we found no prior evaluation of\nthe CCPA implementation in organizations. Therefore, the convergence of the\nregulatory landscape and the organization's privacy policy needs to be studied.\nThis paper was about an empirical evaluation of the implementation of the\nCalifornia Consumer Privacy Act. The report includes the evaluations of the\nfollowing industries: social media, financial institutions, mortgages,\nhealthcare providers, and academic institutions. Our approach was to set up a\ncriteria table constructed from the CCPA Act and then use that table as a\nchecklist while reviewing a company's privacy notice. Finally, we concluded\nthis paper with an online tool application design that verifies the CCPA\nimplementation. Upon completion, the application would be free to use so\nconsumers can quickly inspect a website for CCPA compliance. Additionally, it\nis an advising tool that a website admin can utilize to enhance CCPA compliance\nfor their website. The conjunction of this empirical report and a practical\napplication function as a stimulus to promote CCPA implementation in\norganizations and deliver awareness to consumers about privacy rights they can\ndemand.",
    "descriptor": "\nComments: 75 pages, 30 figures, 9 tables\n",
    "authors": [
      "Trong Nguyen"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.09897"
  },
  {
    "id": "arXiv:2205.09898",
    "title": "Let the Model Decide its Curriculum for Multitask Learning",
    "abstract": "Curriculum learning strategies in prior multi-task learning approaches\narrange datasets in a difficulty hierarchy either based on human perception or\nby exhaustively searching the optimal arrangement. However, human perception of\ndifficulty may not always correlate well with machine interpretation leading to\npoor performance and exhaustive search is computationally expensive. Addressing\nthese concerns, we propose two classes of techniques to arrange training\ninstances into a learning curriculum based on difficulty scores computed via\nmodel-based approaches. The two classes i.e Dataset-level and Instance-level\ndiffer in granularity of arrangement. Through comprehensive experiments with 12\ndatasets, we show that instance-level and dataset-level techniques result in\nstrong representations as they lead to an average performance improvement of\n4.17% and 3.15% over their respective baselines. Furthermore, we find that most\nof this improvement comes from correctly answering the difficult instances,\nimplying a greater efficacy of our techniques on difficult tasks.",
    "descriptor": "\nComments: NAACL 2022 Deep Learning for Low-Resource NLP Workshop\n",
    "authors": [
      "Neeraj Varshney",
      "Swaroop Mishra",
      "Chitta Baral"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09898"
  },
  {
    "id": "arXiv:2205.09901",
    "title": "Minimal Explanations for Neural Network Predictions",
    "abstract": "Explaining neural network predictions is known to be a challenging problem.\nIn this paper, we propose a novel approach which can be effectively exploited,\neither in isolation or in combination with other methods, to enhance the\ninterpretability of neural model predictions. For a given input to a trained\nneural model, our aim is to compute a smallest set of input features so that\nthe model prediction changes when these features are disregarded by setting\nthem to an uninformative baseline value. While computing such minimal\nexplanations is computationally intractable in general for fully-connected\nneural networks, we show that the problem becomes solvable in polynomial time\nby a greedy algorithm under mild assumptions on the network's activation\nfunctions. We then show that our tractability result extends seamlessly to more\nadvanced neural architectures such as convolutional and graph neural networks.\nWe conduct experiments to showcase the capability of our method for identifying\nthe input features that are essential to the model's prediction.",
    "descriptor": "",
    "authors": [
      "Ouns El Harzli",
      "Bernardo Cuenca Grau",
      "Ian Horrocks"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09901"
  },
  {
    "id": "arXiv:2205.09904",
    "title": "Deep transfer learning for image classification: a survey",
    "abstract": "Deep neural networks such as convolutional neural networks (CNNs) and\ntransformers have achieved many successes in image classification in recent\nyears. It has been consistently demonstrated that best practice for image\nclassification is when large deep models can be trained on abundant labelled\ndata. However there are many real world scenarios where the requirement for\nlarge amounts of training data to get the best performance cannot be met. In\nthese scenarios transfer learning can help improve performance. To date there\nhave been no surveys that comprehensively review deep transfer learning as it\nrelates to image classification overall. However, several recent general\nsurveys of deep transfer learning and ones that relate to particular\nspecialised target image classification tasks have been published. We believe\nit is important for the future progress in the field that all current knowledge\nis collated and the overarching patterns analysed and discussed. In this survey\nwe formally define deep transfer learning and the problem it attempts to solve\nin relation to image classification. We survey the current state of the field\nand identify where recent progress has been made. We show where the gaps in\ncurrent knowledge are and make suggestions for how to progress the field to\nfill in these knowledge gaps. We present a new taxonomy of the applications of\ntransfer learning for image classification. This taxonomy makes it easier to\nsee overarching patterns of where transfer learning has been effective and,\nwhere it has failed to fulfill its potential. This also allows us to suggest\nwhere the problems lie and how it could be used more effectively. We show that\nunder this new taxonomy, many of the applications where transfer learning has\nbeen shown to be ineffective or even hinder performance are to be expected when\ntaking into account the source and target datasets and the techniques used.",
    "descriptor": "",
    "authors": [
      "Jo Plested",
      "Tom Gedeon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09904"
  },
  {
    "id": "arXiv:2205.09905",
    "title": "On the Impact of Player Capability on Congestion Games",
    "abstract": "We study the impact of player capability on social welfare in congestion\ngames. We introduce a new game, the Distance-bounded Network Congestion game\n(DNC), as the basis of our study. DNC is a symmetric network congestion game\nwith a bound on the number of edges each player can use. We show that DNC is\nPLS-complete in contrast to standard symmetric network congestion games which\nare in P. To model different player capabilities, we propose using programs in\na Domain-Specific Language (DSL) to compactly represent player strategies. We\ndefine a player's capability as the maximum size of the programs they can use.\nWe introduce two variants of DNC with accompanying DSLs representing the\nstrategy spaces. We propose four capability preference properties to\ncharacterize the impact of player capability on social welfare at equilibrium.\nWe then establish necessary and sufficient conditions for the four properties\nin the context of our DNC variants. Finally, we study a specific game where we\nderive exact expressions of the social welfare in terms of the capability\nbound. This provides examples where the social welfare at equilibrium\nincreases, stays the same, or decreases as players become more capable.",
    "descriptor": "",
    "authors": [
      "Yichen Yang",
      "Kai Jia",
      "Martin Rinard"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.09905"
  },
  {
    "id": "arXiv:2205.09911",
    "title": "Can Foundation Models Wrangle Your Data?",
    "abstract": "Foundation Models (FMs) are models trained on large corpora of data that, at\nvery large scale, can generalize to new tasks without any task-specific\nfinetuning. As these models continue to grow in size, innovations continue to\npush the boundaries of what these models can do on language and image tasks.\nThis paper aims to understand an underexplored area of FMs: classical data\ntasks like cleaning and integration. As a proof-of-concept, we cast three data\ncleaning and integration tasks as prompting tasks and evaluate the performance\nof FMs on these tasks. We find that large FMs generalize and achieve SoTA\nperformance on data cleaning and integration tasks, even though they are not\ntrained for these data tasks. We identify specific research challenges and\nopportunities that these models present, including challenges with private and\ntemporal data, and opportunities to make data driven systems more accessible to\nnon-experts. We make our code and experiments publicly available at:\nhttps://github.com/HazyResearch/fm_data_tasks.",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Avanika Narayan",
      "Ines Chami",
      "Laurel Orr",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.09911"
  },
  {
    "id": "arXiv:2205.09921",
    "title": "KERPLE: Kernelized Relative Positional Embedding for Length  Extrapolation",
    "abstract": "Relative positional embeddings (RPE) have received considerable attention\nsince RPEs effectively model the relative distance among tokens and enable\nlength extrapolation. We propose KERPLE, a framework that generalizes relative\nposition embedding for extrapolation by kernelizing positional differences. We\nachieve this goal using conditionally positive definite (CPD) kernels, a class\nof functions known for generalizing distance metrics. To maintain the inner\nproduct interpretation of self-attention, we show that a CPD kernel can be\ntransformed into a PD kernel by adding a constant offset. This offset is\nimplicitly absorbed in the Softmax normalization during self-attention. The\ndiversity of CPD kernels allows us to derive various RPEs that enable length\nextrapolation in a principled way. Experiments demonstrate that the logarithmic\nvariant achieves excellent extrapolation performance on three large language\nmodeling datasets.",
    "descriptor": "\nComments: The first two authors contributed equally to this work\n",
    "authors": [
      "Ta-Chung Chi",
      "Ting-Han Fan",
      "Peter J. Ramadge",
      "Alexander I. Rudnicky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09921"
  },
  {
    "id": "arXiv:2205.09923",
    "title": "Stability Enforced Bandit Algorithms for Channel Selection in Remote  State Estimation",
    "abstract": "In this paper we consider a remote state estimation problem where a sensor\ncan, at each discrete time instant, transmit on one out of M different\ncommunication channels. A key difficulty of the situation at hand is that the\nchannel statistics are unknown. We study the case where both learning of the\nchannel reception probabilities and state estimation is carried out\nsimultaneously. Methods for choosing the channels based on techniques for\nmulti-armed bandits are presented, and shown to provide stability of the remote\nestimator. Furthermore, we define the performance notion of estimation regret,\nand derive bounds on how it scales with time for the considered algorithms.",
    "descriptor": "",
    "authors": [
      "Alex S. Leong",
      "Daniel E. Quevedo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.09923"
  },
  {
    "id": "arXiv:2205.09924",
    "title": "Anomaly Detection for Multivariate Time Series on Large-scale Fluid  Handling Plant Using Two-stage Autoencoder",
    "abstract": "This paper focuses on anomaly detection for multivariate time series data in\nlarge-scale fluid handling plants with dynamic components, such as power\ngeneration, water treatment, and chemical plants, where signals from various\nphysical phenomena are observed simultaneously. In these plants, the need for\nanomaly detection techniques is increasing in order to reduce the cost of\noperation and maintenance, in view of a decline in the number of skilled\nengineers and a shortage of manpower. However, considering the complex behavior\nof high-dimensional signals and the demand for interpretability, the techniques\nconstitute a major challenge. We introduce a Two-Stage AutoEncoder (TSAE) as an\nanomaly detection method suitable for such plants. This is a simple autoencoder\narchitecture that makes anomaly detection more interpretable and more accurate,\nin which based on the premise that plant signals can be separated into two\nbehaviors that have almost no correlation with each other, the signals are\nseparated into long-term and short-term components in a stepwise manner, and\nthe two components are trained independently to improve the inference\ncapability for normal signals. Through experiments on two publicly available\ndatasets of water treatment systems, we have confirmed the high detection\nperformance, the validity of the premise, and that the model behavior was as\nintended, i.e., the technical effectiveness of TSAE.",
    "descriptor": "\nComments: The 2nd Workshop on Large-scale Industrial Time Series Analysis at the 21st IEEE International Conference on Data Mining (ICDM), 2021\n",
    "authors": [
      "Susumu Naito",
      "Yasunori Taguchi",
      "Kouta Nakata",
      "Yuichi Kato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09924"
  },
  {
    "id": "arXiv:2205.09925",
    "title": "On Jointly Optimizing Partial Offloading and SFC Mapping: A Cooperative  Dual-agent Deep Reinforcement Learning Approach",
    "abstract": "Multi-access edge computing (MEC) and network function virtualization (NFV)\nare promising technologies to support emerging IoT applications, especially\nthose computation-intensive. In NFV-enabled MEC environment, service function\nchain (SFC), i.e., a set of ordered virtual network functions (VNFs), can be\nmapped on MEC servers. Mobile devices (MDs) can offload computation-intensive\napplications, which can be represented by SFCs, fully or partially to MEC\nservers for remote execution. This paper studies the partial offloading and SFC\nmapping joint optimization (POSMJO) problem in an NFV-enabled MEC system, where\nan incoming task can be partitioned into two parts, one for local execution and\nthe other for remote execution. The objective is to minimize the average cost\nin the long term which is a combination of execution delay, MD's energy\nconsumption, and usage charge for edge computing. This problem consists of two\nclosely related decision-making steps, namely task partition and VNF placement,\nwhich is highly complex and quite challenging. To address this, we propose a\ncooperative dual-agent deep reinforcement learning (CDADRL) algorithm, where we\ndesign a framework enabling interaction between two agents. Simulation results\nshow that the proposed algorithm outperforms three combinations of deep\nreinforcement learning algorithms in terms of cumulative and average episodic\nrewards and it overweighs a number of baseline algorithms with respect to\nexecution delay, energy consumption, and usage charge.",
    "descriptor": "",
    "authors": [
      "Xinhan Wang",
      "Huanlai Xing",
      "Fuhong Song",
      "Shouxi Luo",
      "Penglin Dai",
      "Bowen Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.09925"
  },
  {
    "id": "arXiv:2205.09927",
    "title": "CertiFair: A Framework for Certified Global Fairness of Neural Networks",
    "abstract": "We consider the problem of whether a Neural Network (NN) model satisfies\nglobal individual fairness. Individual Fairness suggests that similar\nindividuals with respect to a certain task are to be treated similarly by the\ndecision model. In this work, we have two main objectives. The first is to\nconstruct a verifier which checks whether the fairness property holds for a\ngiven NN in a classification task or provide a counterexample if it is\nviolated, i.e., the model is fair if all similar individuals are classified the\nsame, and unfair if a pair of similar individuals are classified differently.\nTo that end, We construct a sound and complete verifier that verifies global\nindividual fairness properties of ReLU NN classifiers using distance-based\nsimilarity metrics. The second objective of this paper is to provide a method\nfor training provably fair NN classifiers from unfair (biased) data. We propose\na fairness loss that can be used during training to enforce fair outcomes for\nsimilar individuals. We then provide provable bounds on the fairness of the\nresulting NN. We run experiments on commonly used fairness datasets that are\npublicly available and we show that global individual fairness can be improved\nby 96 % without significant drop in test accuracy.",
    "descriptor": "",
    "authors": [
      "Haitham Khedr",
      "Yasser Shoukry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09927"
  },
  {
    "id": "arXiv:2205.09928",
    "title": "Cross Reconstruction Transformer for Self-Supervised Time Series  Representation Learning",
    "abstract": "Unsupervised/self-supervised representation learning in time series is\ncritical since labeled samples are usually scarce in real-world scenarios.\nExisting approaches mainly leverage the contrastive learning framework, which\nautomatically learns to understand the similar and dissimilar data pairs.\nNevertheless, they are restricted to the prior knowledge of constructing pairs,\ncumbersome sampling policy, and unstable performances when encountering\nsampling bias. Also, few works have focused on effectively modeling across\ntemporal-spectral relations to extend the capacity of representations. In this\npaper, we aim at learning representations for time series from a new\nperspective and propose Cross Reconstruction Transformer (CRT) to solve the\naforementioned problems in a unified way. CRT achieves time series\nrepresentation learning through a cross-domain dropping-reconstruction task.\nSpecifically, we transform time series into the frequency domain and randomly\ndrop certain parts in both time and frequency domains. Dropping can maximally\npreserve the global context compared to cropping and masking. Then a\ntransformer architecture is utilized to adequately capture the cross-domain\ncorrelations between temporal and spectral information through reconstructing\ndata in both domains, which is called Dropped Temporal-Spectral Modeling. To\ndiscriminate the representations in global latent space, we propose Instance\nDiscrimination Constraint to reduce the mutual information between different\ntime series and sharpen the decision boundaries. Additionally, we propose a\nspecified curriculum learning strategy to optimize the CRT, which progressively\nincreases the dropping ratio in the training process.",
    "descriptor": "",
    "authors": [
      "Wenrui Zhang",
      "Ling Yang",
      "Shijia Geng",
      "Shenda Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09928"
  },
  {
    "id": "arXiv:2205.09930",
    "title": "BayesPCN: A Continually Learnable Predictive Coding Associative Memory",
    "abstract": "Associative memory plays an important role in human intelligence and its\nmechanisms have been linked to attention in machine learning. While the machine\nlearning community's interest in associative memories has recently been\nrekindled, most work has focused on memory recall ($read$) over memory learning\n($write$). In this paper, we present BayesPCN, a hierarchical associative\nmemory capable of performing continual one-shot memory writes without\nmeta-learning. Moreover, BayesPCN is able to gradually forget past observations\n($forget$) to free its memory. Experiments show that BayesPCN can recall\ncorrupted i.i.d. high-dimensional data observed hundreds of \"timesteps\" ago\nwithout a significant drop in recall ability compared to the state-of-the-art\noffline-learned associative memory models.",
    "descriptor": "",
    "authors": [
      "Jason Yoo",
      "Frank Wood"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09930"
  },
  {
    "id": "arXiv:2205.09931",
    "title": "Measure the Diversity of Open Source Software Projects' Forks with Fork  Entropy",
    "abstract": "Forks play a central role in modern pull-based OSS development. Although rich\nempirical results on the participants, challenges, and features of forks have\nbeen announced, there is little discussion on quantitatively measuring the\npopulation of forks around OSS projects. In this paper, we take a step toward\nenriching the set of metrics about forks by proposing the fork entropy to\nmeasure the diversity of fork populations around OSS projects. We\noperationalize the proposed fork entropy based on Rao's quadratic entropy with\na distance function defined on the forks' modifications to project files. After\nverifying the construct validity of fork entropy, we show the usefulness of\nfork entropy in understanding and predicting OSS development in terms of\nexternal productivity, the acceptance rate of external pull-requests, and code\nquality using a dataset consisting of fifty popular OSS projects hosted on\nGitHub. By conducting regression analyses, we find that fork entropy\nsignificantly and positively affects external productivity, the acceptance rate\nof external pull-requests, and code quality, even though sometimes with a small\neffect. However, as expected, fork entropy at a high level sometimes plays a\nnegative role in OSS development. We also observe fork entropy can magically\nmoderate other factors' effect on some project outcomes. We believe our new\nmetric of fork entropy is helpful to guide practices of OSS development.",
    "descriptor": "",
    "authors": [
      "Zhiwen Zheng",
      "Liang Wang",
      "Jierui Zhang",
      "Baihui Sang",
      "Xianping Tao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.09931"
  },
  {
    "id": "arXiv:2205.09933",
    "title": "Hyperspectral Unmixing Based on Nonnegative Matrix Factorization: A  Comprehensive Review",
    "abstract": "Hyperspectral unmixing has been an important technique that estimates a set\nof endmembers and their corresponding abundances from a hyperspectral image\n(HSI). Nonnegative matrix factorization (NMF) plays an increasingly significant\nrole in solving this problem. In this article, we present a comprehensive\nsurvey of the NMF-based methods proposed for hyperspectral unmixing. Taking the\nNMF model as a baseline, we show how to improve NMF by utilizing the main\nproperties of HSIs (e.g., spectral, spatial, and structural information). We\ncategorize three important development directions including constrained NMF,\nstructured NMF, and generalized NMF. Furthermore, several experiments are\nconducted to illustrate the effectiveness of associated algorithms. Finally, we\nconclude the article with possible future directions with the purposes of\nproviding guidelines and inspiration to promote the development of\nhyperspectral unmixing.",
    "descriptor": "",
    "authors": [
      "Xin-Ru Feng",
      "Heng-Chao Li",
      "Rui Wang",
      "Qian Du",
      "Xiuping Jia",
      "Antonio Plaza"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.09933"
  },
  {
    "id": "arXiv:2205.09934",
    "title": "Towards Explanation for Unsupervised Graph-Level Representation Learning",
    "abstract": "Due to the superior performance of Graph Neural Networks (GNNs) in various\ndomains, there is an increasing interest in the GNN explanation problem\n\"\\emph{which fraction of the input graph is the most crucial to decide the\nmodel's decision?}\" Existing explanation methods focus on the supervised\nsettings, \\eg, node classification and graph classification, while the\nexplanation for unsupervised graph-level representation learning is still\nunexplored. The opaqueness of the graph representations may lead to unexpected\nrisks when deployed for high-stake decision-making scenarios. In this paper, we\nadvance the Information Bottleneck principle (IB) to tackle the proposed\nexplanation problem for unsupervised graph representations, which leads to a\nnovel principle, \\textit{Unsupervised Subgraph Information Bottleneck} (USIB).\nWe also theoretically analyze the connection between graph representations and\nexplanatory subgraphs on the label space, which reveals that the expressiveness\nand robustness of representations benefit the fidelity of explanatory\nsubgraphs. Experimental results on both synthetic and real-world datasets\ndemonstrate the superiority of our developed explainer and the validity of our\ntheoretical analysis.",
    "descriptor": "",
    "authors": [
      "Qinghua Zheng",
      "Jihong Wang",
      "Minnan Luo",
      "Yaoliang Yu",
      "Jundong Li",
      "Lina Yao",
      "Xiaojun Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09934"
  },
  {
    "id": "arXiv:2205.09935",
    "title": "Quantitative Analysis of Community Evolution in Developer Social  Networks Around Open Source Software Projects",
    "abstract": "Understanding the evolution of communities in developer social networks\n(DSNs) around open source software (OSS) projects can provide valuable insights\nabout the socio-technical process of OSS development. Existing studies show the\nevolutionary behaviors of social communities can effectively be described using\npatterns including split, shrink, merge, expand, emerge, and extinct. However,\nexisting pattern-based approaches are limited in supporting quantitative\nanalysis, and are potentially problematic for using the patterns in a mutually\nexclusive manner when describing community evolution. In this work, we propose\nthat different patterns can occur simultaneously between every pair of\ncommunities during the evolution, just in different degrees. Four entropy-based\nindices are devised to measure the degree of community split, shrink, merge,\nand expand, respectively, which can provide a comprehensive and quantitative\nmeasure of community evolution in DSNs. The indices have properties desirable\nto quantify community evolution including monotonicity, and bounded maximum and\nminimum values that correspond to meaningful cases. They can also be combined\nto describe more patterns such as community emerge and extinct. We conduct\nexperiments with real-world OSS projects to evaluate the validity of the\nproposed indices. The results suggest the proposed indices can effectively\ncapture community evolution, and are consistent with existing approaches in\ndetecting evolution patterns in DSNs with an accuracy of 94.1\\%. The results\nalso show that the indices are useful in predicting OSS team productivity with\nan accuracy of 0.718. In summary, the proposed approach is among the first to\nquantify the degree of community evolution with respect to different patterns,\nwhich is promising in supporting future research and applications about DSNs\nand OSS development.",
    "descriptor": "",
    "authors": [
      "Liang Wang",
      "Ying Li",
      "Jierui Zhang",
      "Xianping Tao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.09935"
  },
  {
    "id": "arXiv:2205.09943",
    "title": "Explainable Supervised Domain Adaptation",
    "abstract": "Domain adaptation techniques have contributed to the success of deep\nlearning. Leveraging knowledge from an auxiliary source domain for learning in\nlabeled data-scarce target domain is fundamental to domain adaptation. While\nthese techniques result in increasing accuracy, the adaptation process,\nparticularly the knowledge leveraged from the source domain, remains unclear.\nThis paper proposes an explainable by design supervised domain adaptation\nframework - XSDA-Net. We integrate a case-based reasoning mechanism into the\nXSDA-Net to explain the prediction of a test instance in terms of\nsimilar-looking regions in the source and target train images. We empirically\ndemonstrate the utility of the proposed framework by curating the domain\nadaptation settings on datasets popularly known to exhibit part-based\nexplainability.",
    "descriptor": "\nComments: Accepted as a Poster presentation at IJCNN at IEEE-WCCI 2022\n",
    "authors": [
      "Vidhya Kamakshi",
      "Narayanan C Krishnan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09943"
  },
  {
    "id": "arXiv:2205.09944",
    "title": "6G Network AI Architecture for Everyone-Centric Customized Services",
    "abstract": "Mobile communication standards were developed for enhancing transmission and\nnetwork performance by utilizing more radio resources and improving spectrum\nand energy efficiency. How to effectively address diverse user requirements and\nguarantee everyone's Quality of Experience (QoE) remains an open problem. The\nfuture Sixth Generation (6G) system can solve this problem by using pervasive\nintelligence and ubiquitous computing resources to support everyone-centric\ncustomized services anywhere, anytime. In this article, we first introduce the\nconcept of Service Requirement Zone (SRZ) on the user side to characterize the\nrequirements and preferences of specific tasks of individual users. On the\nsystem side, we further introduce the concept of User Satisfaction Ratio (USR)\nto evaluate the system's overall service ability of satisfying diverse tasks\nwith different SRZs. Then, we propose a network Artificial Intelligence (AI)\narchitecture to exploit the pervasive AI and network resources for guaranteeing\nindividualized QoEs. Finally, extensive simulations show that the network AI\narchitecture can consistently offer a higher USR performance than the cloud AI\nand edge AI architectures with respect to different task scheduling algorithms\nunder dynamic network conditions.",
    "descriptor": "",
    "authors": [
      "Yang Yang",
      "Mulei Ma",
      "Hequan Wu",
      "Quan Yu",
      "Ping Zhang",
      "Xiaohu You",
      "Jianjun Wu",
      "Chenghui Peng",
      "Tak-Shing Peter Yum",
      "Sherman Shen",
      "Hamid Aghvami",
      "Geoffrey Y Li",
      "Jiangzhou Wang",
      "Guangyi Liu",
      "Peng Gao",
      "Xiongyan Tang",
      "Chang Cao",
      "John Thompson",
      "Kat-Kit Wong",
      "Shanzhi Chen",
      "Zhiqin Wang",
      "Merouane Debbah",
      "Schahram Dustdar",
      "Frank Eliassen",
      "Tao Chen",
      "Xiangyang Duan",
      "Shaohui Sun",
      "Xiaofeng Tao",
      "Qinyu Zhang",
      "Jianwei Huang",
      "Shuguang Cui",
      "Wenjun Zhang",
      "Jie Li",
      "Yue Gao",
      "Honggang Zhang",
      "Xu Chen",
      "Xiaohu Ge",
      "Yong Xiao",
      "Cheng-Xiang Wang",
      "Zaichen Zhang",
      "Song Ci",
      "Guoqiang Mao",
      "Changle Li",
      "Ziyu Shao",
      "Yong Zhou",
      "Junrui Liang",
      "Kai Li",
      "Liantao Wu",
      "Fanglei Sun",
      "Kunlun Wang",
      "Zening Liu",
      "Kun Yang",
      "Jun Wang",
      "Teng Gao",
      "Hongfeng Shu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.09944"
  },
  {
    "id": "arXiv:2205.09946",
    "title": "Long Run Incremental Cost (LRIC) Distribution Network Pricing in UK,  advising China's Distribution Network",
    "abstract": "Electricity distribution network system is considered one of the key\ncomponent of the modern electrical power system. Due to increase in the energy\ndemand, penetration of renewable energy resources into the power system has\nbeen extensively increasing in recent years. More and more distributed\ngenerations (DGs) are joining the distribution network to create balance in the\npower system and meet the supply and demand of consumers. Today, large amount\nof DGs inclusion in the distribution network system has completely modernized\npower system resulting in a decentralize electricity market. Hence, Government\nof UK is pressurizing 14 distribution network operators (DNOs) to include more\nDGs into their distribution network system. DGs inclusion in the network system\nmight be helpful due to many factors, but it creates many challenges for\ndistribution network system in the long term. The network security is realized\nto be one of the challenge that impact the efficiency of accurate calculation\nand distribution of network pricing among consumers. To address the\naforementioned issue, this research analysed the network security on the basis\nof Long run incremental cost (LRIC) pricing to balance and reduce the network\npricing for the DNOs in UK. However, this study presented an approach of Deep\nreinforcement learning (DRL) also called deep reinforcement learning algorithm\n(DQN) to optimize the reactive power values in the network to balance and\nreduce the network pricing while keeping the network security. The method\nconsiders IEEE14 bus as its mathematical model and practically simulates the\nmethod in MATLAB using DQN algorithm pseudo codes. The network security has\nbeen analysed with and without security factor before and after the nodal\ninjection into the network.",
    "descriptor": "",
    "authors": [
      "Asad Mujeeb",
      "Wang Peng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09946"
  },
  {
    "id": "arXiv:2205.09947",
    "title": "PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems",
    "abstract": "Diagram parsing is an important foundation for geometry problem solving,\nattracting increasing attention in the field of intelligent education and\ndocument image understanding. Due to the complex layout and between-primitive\nrelationship, plane geometry diagram parsing (PGDP) is still a challenging task\ndeserving further research and exploration. An appropriate dataset is critical\nfor the research of PGDP. Although some datasets with rough annotations have\nbeen proposed to solve geometric problems, they are either small in scale or\nnot publicly available. The rough annotations also make them not very useful.\nThus, we propose a new large-scale geometry diagram dataset named PGDP5K and a\nnovel annotation method. Our dataset consists of 5000 diagram samples composed\nof 16 shapes, covering 5 positional relations, 22 symbol types and 6 text\ntypes. Different from previous datasets, our PGDP5K dataset is labeled with\nmore fine-grained annotations at primitive level, including primitive classes,\nlocations and relationships. What is more, combined with above annotations and\ngeometric prior knowledge, it can generate intelligible geometric propositions\nautomatically and uniquely. We performed experiments on PGDP5K and\nIMP-Geometry3K datasets reveal that the state-of-the-art (SOTA) method achieves\nonly 66.07% F1 value. This shows that PGDP5K presents a challenge for future\nresearch. Our dataset is available at\nthis http URL",
    "descriptor": "",
    "authors": [
      "Yihan Hao",
      "Mingliang Zhang",
      "Fei Yin",
      "Linlin Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09947"
  },
  {
    "id": "arXiv:2205.09948",
    "title": "GDSRec: Graph-Based Decentralized Collaborative Filtering for Social  Recommendation",
    "abstract": "Generating recommendations based on user-item interactions and user-user\nsocial relations is a common use case in web-based systems. These connections\ncan be naturally represented as graph-structured data and thus utilizing graph\nneural networks (GNNs) for social recommendation has become a promising\nresearch direction. However, existing graph-based methods fails to consider the\nbias offsets of users (items). For example, a low rating from a fastidious user\nmay not imply a negative attitude toward this item because the user tends to\nassign low ratings in common cases. Such statistics should be considered into\nthe graph modeling procedure. While some past work considers the biases, we\nargue that these proposed methods only treat them as scalars and can not\ncapture the complete bias information hidden in data. Besides, social\nconnections between users should also be differentiable so that users with\nsimilar item preference would have more influence on each other. To this end,\nwe propose Graph-Based Decentralized Collaborative Filtering for Social\nRecommendation (GDSRec). GDSRec treats the biases as vectors and fuses them\ninto the process of learning user and item representations. The statistical\nbias offsets are captured by decentralized neighborhood aggregation while the\nsocial connection strength is defined according to the preference similarity\nand then incorporated into the model design. We conduct extensive experiments\non two benchmark datasets to verify the effectiveness of the proposed model.\nExperimental results show that the proposed GDSRec achieves superior\nperformance compared with state-of-the-art related baselines. Our\nimplementations are available in \\url{https://github.com/MEICRS/GDSRec}.",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Knowledge and Data Engineering\n",
    "authors": [
      "Jiajia Chen",
      "Xin Xin",
      "Xianfeng Liang",
      "Xiangnan He",
      "Jun Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.09948"
  },
  {
    "id": "arXiv:2205.09949",
    "title": "Clustering as Attention: Unified Image Segmentation with Hierarchical  Clustering",
    "abstract": "We propose a hierarchical clustering-based image segmentation scheme for deep\nneural networks, called HCFormer. We interpret image segmentation, including\nsemantic, instance, and panoptic segmentation, as a pixel clustering problem,\nand accomplish it by bottom-up, hierarchical clustering with deep neural\nnetworks. Our hierarchical clustering removes the pixel decoder from\nconventional segmentation models and simplifies the segmentation pipeline,\nresulting in improved segmentation accuracies and interpretability. HCFormer\ncan address semantic, instance, and panoptic segmentation with the same\narchitecture because the pixel clustering is a common approach for various\nimage segmentation. In experiments, HCFormer achieves comparable or superior\nsegmentation accuracies compared to baseline methods on semantic segmentation\n(55.5 mIoU on ADE20K), instance segmentation (47.1 AP on COCO), and panoptic\nsegmentation (55.7 PQ on COCO).",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Teppei Suzuki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09949"
  },
  {
    "id": "arXiv:2205.09956",
    "title": "Structured Attention Composition for Temporal Action Localization",
    "abstract": "Temporal action localization aims at localizing action instances from\nuntrimmed videos. Existing works have designed various effective modules to\nprecisely localize action instances based on appearance and motion features.\nHowever, by treating these two kinds of features with equal importance,\nprevious works cannot take full advantage of each modality feature, making the\nlearned model still sub-optimal. To tackle this issue, we make an early effort\nto study temporal action localization from the perspective of multi-modality\nfeature learning, based on the observation that different actions exhibit\nspecific preferences to appearance or motion modality. Specifically, we build a\nnovel structured attention composition module. Unlike conventional attention,\nthe proposed module would not infer frame attention and modality attention\nindependently. Instead, by casting the relationship between the modality\nattention and the frame attention as an attention assignment process, the\nstructured attention composition module learns to encode the frame-modality\nstructure and uses it to regularize the inferred frame attention and modality\nattention, respectively, upon the optimal transport theory. The final\nframe-modality attention is obtained by the composition of the two individual\nattentions. The proposed structured attention composition module can be\ndeployed as a plug-and-play module into existing action localization\nframeworks. Extensive experiments on two widely used benchmarks show that the\nproposed structured attention composition consistently improves four\nstate-of-the-art temporal action localization methods and builds new\nstate-of-the-art performance on THUMOS14. Code is availabel at\nhttps://github.com/VividLe/Online-Action-Detection.",
    "descriptor": "\nComments: Accepted by T-IP\n",
    "authors": [
      "Le Yang",
      "Junwei Han",
      "Tao Zhao",
      "Nian Liu",
      "Dingwen Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09956"
  },
  {
    "id": "arXiv:2205.09959",
    "title": "A Subspace Method for Time Series Anomaly Detection in Cyber-Physical  Systems",
    "abstract": "Time series anomaly detection is an important process for system monitoring\nand model switching, among other applications in cyber-physical systems. In\nthis document, we present a fast subspace method for time series anomaly\ndetection, with a relatively low computational cost, that has been designed for\nanomaly detection in real sensor signals corresponding to dynamical systems. We\nalso present some general results corresponding to the theoretical foundations\nof our method, together with a prototypical algorithm to for time series\nanomaly detection. Some numerical examples corresponding to applications of the\nprototypical algorithm are presented, and some computational tools based on the\ntheory and algorithms presented in this paper, are provided.",
    "descriptor": "",
    "authors": [
      "Fredy Vides",
      "Esteban Segura",
      "Carlos Vargas-Ag\u00fcero"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.09959"
  },
  {
    "id": "arXiv:2205.09961",
    "title": "Discrete-Convex-Analysis-Based Framework for Warm-Starting Algorithms  with Predictions",
    "abstract": "Augmenting algorithms with learned predictions is a promising approach for\ngoing beyond worst-case bounds. Dinitz, Im, Lavastida, Moseley, and\nVassilvitskii~(2021) have demonstrated that a warm start with learned dual\nsolutions can improve the time complexity of the Hungarian method for weighted\nperfect bipartite matching. We extend and improve their framework in a\nprincipled manner via \\textit{discrete convex analysis} (DCA), a discrete\nanalog of convex analysis. We show the usefulness of our DCA-based framework by\napplying it to weighted perfect bipartite matching, weighted matroid\nintersection, and discrete energy minimization for computer vision. Our\nDCA-based framework yields time complexity bounds that depend on the\n$\\ell_\\infty$-distance from a predicted solution to an optimal solution, which\nhas two advantages relative to the previous $\\ell_1$-distance-dependent bounds:\ntime complexity bounds are smaller, and learning of predictions is more sample\nefficient. We also discuss whether to learn primal or dual solutions from the\nDCA perspective.",
    "descriptor": "",
    "authors": [
      "Shinsaku Sakaue",
      "Taihei Oki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.09961"
  },
  {
    "id": "arXiv:2205.09962",
    "title": "Advanced Feature Learning on Point Clouds using Multi-resolution  Features and Learnable Pooling",
    "abstract": "Existing point cloud feature learning networks often incorporate sequences of\nsampling, neighborhood grouping, neighborhood-wise feature learning, and\nfeature aggregation to learn high-semantic point features that represent the\nglobal context of a point cloud. Unfortunately, the compounded loss of\ninformation concerning granularity and non-maximum point features due to\nsampling and max pooling could adversely affect the high-semantic point\nfeatures from existing networks such that they are insufficient to represent\nthe local context of a point cloud, which in turn may hinder the network in\ndistinguishing fine shapes. To cope with this problem, we propose a novel point\ncloud feature learning network, PointStack, using multi-resolution feature\nlearning and learnable pooling (LP). The multi-resolution feature learning is\nrealized by aggregating point features of various resolutions in the multiple\nlayers, so that the final point features contain both high-semantic and\nhigh-resolution information. On the other hand, the LP is used as a generalized\npooling function that calculates the weighted sum of multi-resolution point\nfeatures through the attention mechanism with learnable queries, in order to\nextract all possible information from all available point features.\nConsequently, PointStack is capable of extracting high-semantic point features\nwith minimal loss of information concerning granularity and non-maximum point\nfeatures. Therefore, the final aggregated point features can effectively\nrepresent both global and local contexts of a point cloud. In addition, both\nthe global structure and the local shape details of a point cloud can be well\ncomprehended by the network head, which enables PointStack to advance the\nstate-of-the-art of feature learning on point clouds. The codes are available\nat https://github.com/kaist-avelab/PointStack.",
    "descriptor": "",
    "authors": [
      "Kevin Tirta Wijaya",
      "Dong-Hee Paek",
      "Seung-Hyun Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09962"
  },
  {
    "id": "arXiv:2205.09963",
    "title": "Sample Complexity of Learning Heuristic Functions for Greedy-Best-First  and A* Search",
    "abstract": "Greedy best-first search (GBFS) and A* search (A*) are popular algorithms for\npath-finding on large graphs. Both use so-called heuristic functions, which\nestimate how close a vertex is to the goal. While heuristic functions have been\nhandcrafted using domain knowledge, recent studies demonstrate that learning\nheuristic functions from data is effective in many applications. Motivated by\nthis emerging approach, we study the sample complexity of learning heuristic\nfunctions for GBFS and A*. We build on a recent framework called\n\\textit{data-driven algorithm design} and evaluate the\n\\textit{pseudo-dimension} of a class of utility functions that measure the\nperformance of parameterized algorithms. Assuming that a vertex set of size $n$\nis fixed, we present $\\mathrm{O}(n\\lg n)$ and $\\mathrm{O}(n^2\\lg n)$ upper\nbounds on the pseudo-dimensions for GBFS and A*, respectively, parameterized by\nheuristic function values. The upper bound for A* can be improved to\n$\\mathrm{O}(n^2\\lg d)$ if every vertex has a degree of at most $d$ and to\n$\\mathrm{O}(n \\lg n)$ if edge weights are integers bounded by\n$\\mathrm{poly}(n)$. We also give $\\Omega(n)$ lower bounds for GBFS and A*,\nwhich imply that our bounds for GBFS and A* under the integer-weight condition\nare tight up to a $\\lg n$ factor. Finally, we discuss a case where the\nperformance of A* is measured by the suboptimality and show that we can\nsometimes obtain a better guarantee by combining a parameter-dependent\nworst-case bound with a sample complexity bound.",
    "descriptor": "",
    "authors": [
      "Shinsaku Sakaue",
      "Taihei Oki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.09963"
  },
  {
    "id": "arXiv:2205.09965",
    "title": "Few-Shot Font Generation by Learning Fine-Grained Local Styles",
    "abstract": "Few-shot font generation (FFG), which aims to generate a new font with a few\nexamples, is gaining increasing attention due to the significant reduction in\nlabor cost. A typical FFG pipeline considers characters in a standard font\nlibrary as content glyphs and transfers them to a new target font by extracting\nstyle information from the reference glyphs. Most existing solutions explicitly\ndisentangle content and style of reference glyphs globally or component-wisely.\nHowever, the style of glyphs mainly lies in the local details, i.e. the styles\nof radicals, components, and strokes together depict the style of a glyph.\nTherefore, even a single character can contain different styles distributed\nover spatial locations. In this paper, we propose a new font generation\napproach by learning 1) the fine-grained local styles from references, and 2)\nthe spatial correspondence between the content and reference glyphs. Therefore,\neach spatial location in the content glyph can be assigned with the right\nfine-grained style. To this end, we adopt cross-attention over the\nrepresentation of the content glyphs as the queries and the representations of\nthe reference glyphs as the keys and values. Instead of explicitly\ndisentangling global or component-wise modeling, the cross-attention mechanism\ncan attend to the right local styles in the reference glyphs and aggregate the\nreference styles into a fine-grained style representation for the given content\nglyphs. The experiments show that the proposed method outperforms the\nstate-of-the-art methods in FFG. In particular, the user studies also\ndemonstrate the style consistency of our approach significantly outperforms\nprevious methods.",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Licheng Tang",
      "Yiyang Cai",
      "Jiaming Liu",
      "Zhibin Hong",
      "Mingming Gong",
      "Minhu Fan",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding",
      "Jingdong Wan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.09965"
  },
  {
    "id": "arXiv:2205.09967",
    "title": "A Fully Controllable Agent in the Path Planning using Goal-Conditioned  Reinforcement Learning",
    "abstract": "The aim of path planning is to reach the goal from starting point by\nsearching for the route of an agent. In the path planning, the routes may vary\ndepending on the number of variables such that it is important for the agent to\nreach various goals. Numerous studies, however, have dealt with a single goal\nthat is predefined by the user. In the present study, I propose a novel\nreinforcement learning framework for a fully controllable agent in the path\nplanning. To do this, I propose a bi-directional memory editing to obtain\nvarious bi-directional trajectories of the agent, in which the behavior of the\nagent and sub-goals are trained on the goal-conditioned RL. As for moving the\nagent in various directions, I utilize the sub-goals dedicated network,\nseparated from a policy network. Lastly, I present the reward shaping to\nshorten the number of steps for the agent to reach the goal. In the\nexperimental result, the agent was able to reach the various goals that have\nnever been visited by the agent in the training. We confirmed that the agent\ncould perform difficult missions such as a round trip and the agent used the\nshorter route with the reward shaping.",
    "descriptor": "",
    "authors": [
      "GyeongTaek Lee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09967"
  },
  {
    "id": "arXiv:2205.09968",
    "title": "A General Framework for quantifying Aleatoric and Epistemic uncertainty  in Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNN) provide a powerful framework that elegantly\nintegrates Graph theory with Machine learning for modeling and analysis of\nnetworked data. We consider the problem of quantifying the uncertainty in\npredictions of GNN stemming from modeling errors and measurement uncertainty.\nWe consider aleatoric uncertainty in the form of probabilistic links and noise\nin feature vector of nodes, while epistemic uncertainty is incorporated via a\nprobability distribution over the model parameters. We propose a unified\napproach to treat both sources of uncertainty in a Bayesian framework, where\nAssumed Density Filtering is used to quantify aleatoric uncertainty and Monte\nCarlo dropout captures uncertainty in model parameters. Finally, the two\nsources of uncertainty are aggregated to estimate the total uncertainty in\npredictions of a GNN. Results in the real-world datasets demonstrate that the\nBayesian model performs at par with a frequentist model and provides additional\ninformation about predictions uncertainty that are sensitive to uncertainties\nin the data and model.",
    "descriptor": "\nComments: 10 pages, 1 figure, 6 Tables\n",
    "authors": [
      "Sai Munikoti",
      "Deepesh Agarwal",
      "Laya Das",
      "Balasubramaniam Natarajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09968"
  },
  {
    "id": "arXiv:2205.09971",
    "title": "On Tackling Explanation Redundancy in Decision Trees",
    "abstract": "Decision trees (DTs) epitomize the ideal of interpretability of machine\nlearning (ML) models. The interpretability of decision trees motivates\nexplainability approaches by so-called intrinsic interpretability, and it is at\nthe core of recent proposals for applying interpretable ML models in high-risk\napplications. The belief in DT interpretability is justified by the fact that\nexplanations for DT predictions are generally expected to be succinct. Indeed,\nin the case of DTs, explanations correspond to DT paths. Since decision trees\nare ideally shallow, and so paths contain far fewer features than the total\nnumber of features, explanations in DTs are expected to be succinct, and hence\ninterpretable. This paper offers both theoretical and experimental arguments\ndemonstrating that, as long as interpretability of decision trees equates with\nsuccinctness of explanations, then decision trees ought not be deemed\ninterpretable. The paper introduces logically rigorous path explanations and\npath explanation redundancy, and proves that there exist functions for which\ndecision trees must exhibit paths with arbitrarily large explanation\nredundancy. The paper also proves that only a very restricted class of\nfunctions can be represented with DTs that exhibit no explanation redundancy.\nIn addition, the paper includes experimental results substantiating that path\nexplanation redundancy is observed ubiquitously in decision trees, including\nthose obtained using different tree learning algorithms, but also in a wide\nrange of publicly available decision trees. The paper also proposes\npolynomial-time algorithms for eliminating path explanation redundancy, which\nin practice require negligible time to compute. Thus, these algorithms serve to\nindirectly attain irreducible, and so succinct, explanations for decision\ntrees.",
    "descriptor": "",
    "authors": [
      "Yacine Izza",
      "Alexey Ignatiev",
      "Joao Marques-Silva"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09971"
  },
  {
    "id": "arXiv:2205.09973",
    "title": "An In-Pipe Inspection Robot With Sensorless Underactuated Magnets and  Omnidirectional Tracks: Design and Implementation",
    "abstract": "This paper presents the plan of an in-pipe climbing robot that works\nutilizing an astute transmission part to investigate complex relationship of\nlines. Standard wheeled/proceeded in-pipe climbing robots are inclined to slip\nand take while investigating in pipe turns. The instrument helps in\naccomplishing the main inevitable result of getting out slip and drag in the\nrobot tracks during advancement. The proposed transmission appreciates the\npractical furthest reaches of the standard two-yield transmission, which is\ndeveloped the basic time for a transmission with three results. The instrument\nconclusively changes the track speeds of the robot considering the powers\napplied on each track inside the line relationship, by getting out the\nessential for any remarkable control. The amusement of the robot crossing in\nthe line network in various orientation and in pipe-turns without slip shows\nthe proposed game plan's adequacy.",
    "descriptor": "\nComments: 6 pages, 2 figures. arXiv admin note: text overlap with arXiv:2201.07865\n",
    "authors": [
      "Gaurav Saha",
      "K. M. Santosh",
      "Anugu Reddy",
      "Ravi Kant",
      "Arjun Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.09973"
  },
  {
    "id": "arXiv:2205.09974",
    "title": "A New Feature Selection Method for LogNNet and its Application for  Diagnosis and Prognosis of COVID-19 Disease Using Routine Blood Values",
    "abstract": "Since February-2020, the world has embarked on an intense struggle with the\nCOVID-19 disease, and health systems have come under a tragic pressure as the\ndisease turned into a pandemic. The aim of this study is to determine the most\neffective routine-blood-values (RBV) in the diagnosis/prognosis of COVID-19\nusing new feature selection method for LogNNet reservoir neural network. First\ndataset in this study consists of a total of 5296-patients with a same number\nof negative and positive covid test. Second dataset consists of a total of\n3899-patients with a diagnosis of COVID-19, who were treated in hospital with\nsevere-infected (203) and mildly-infected (3696). The most important RBVs that\naffect the diagnosis of the disease from the first dataset were\nmean-corpuscular-hemoglobin-concentration (MCHC), mean-corpuscular-hemoglobin\n(MCH) and activated-partial-prothrombin-time (aPTT). The most effective\nfeatures in the prognosis of the disease were erythrocyte-sedimentation-rate\n(ESR), neutrophil-count (NEU), C-reactive-protein (CRP). LogNNet-model achieved\nan accuracy rate of A46 = 99.5% in the diagnosis of the disease with 46\nfeatures and A3 = 99.17% with only MCHC, MCH, and aPTT features. Model reached\nan accuracy rate of A48 = 94.4% in determining the prognosis of the disease\nwith 48 features and A3 = 82.7% with only ESR, NEU, and CRP features. LogNNet\nmodel demonstrated a very high disease diagnosis/prognosis of COVID-19\nperformance without knowing about the symptoms or history of the patients. The\nmodel is suitable for devices with low resources (3-14 kB of RAM used on the\nArduino microcontroller), and is promising to create mobile health monitoring\nsystems in the Internet of Things. Our method will reduce the negative\npressures on the health sector and help doctors understand pathogenesis of\nCOVID-19 through key futures and contribute positively to the treatment\nprocesses.",
    "descriptor": "\nComments: 22 pages, 6 figures, 11 Tables\n",
    "authors": [
      "Mehmet Tahir Huyut",
      "Andrei Velichko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Medical Physics (physics.med-ph)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.09974"
  },
  {
    "id": "arXiv:2205.09977",
    "title": "FairNorm: Fair and Fast Graph Neural Network Training",
    "abstract": "Graph neural networks (GNNs) have been demonstrated to achieve\nstate-of-the-art for a number of graph-based learning tasks, which leads to a\nrise in their employment in various domains. However, it has been shown that\nGNNs may inherit and even amplify bias within training data, which leads to\nunfair results towards certain sensitive groups. Meanwhile, training of GNNs\nintroduces additional challenges, such as slow convergence and possible\ninstability. Faced with these limitations, this work proposes FairNorm, a\nunified normalization framework that reduces the bias in GNN-based learning\nwhile also providing provably faster convergence. Specifically, FairNorm\nemploys fairness-aware normalization operators over different sensitive groups\nwith learnable parameters to reduce the bias in GNNs. The design of FairNorm is\nbuilt upon analyses that illuminate the sources of bias in graph-based\nlearning. Experiments on node classification over real-world networks\ndemonstrate the efficiency of the proposed scheme in improving fairness in\nterms of statistical parity and equal opportunity compared to fairness-aware\nbaselines. In addition, it is empirically shown that the proposed framework\nleads to faster convergence compared to the naive baseline where no\nnormalization is employed.",
    "descriptor": "\nComments: 23 pages, 1 figures, 6 tables\n",
    "authors": [
      "O. Deniz Kose",
      "Yanning Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09977"
  },
  {
    "id": "arXiv:2205.09978",
    "title": "HeadText: Exploring Hands-free Text Entry using Head Gestures by Motion  Sensing on a Smart Earpiece",
    "abstract": "We present HeadText, a hands-free technique on a smart earpiece for text\nentry by motion sensing. Users input text utilizing only 7 head gestures for\nkey selection, word selection, word commitment and word cancelling tasks. Head\ngesture recognition is supported by motion sensing on a smart earpiece to\ncapture head moving signals and machine learning algorithms (K-Nearest-Neighbor\n(KNN) with a Dynamic Time Warping (DTW) distance measurement). A 10-participant\nuser study proved that HeadText could recognize 7 head gestures at an accuracy\nof 94.29%. After that, the second user study presented that HeadText could\nachieve a maximum accuracy of 10.65 WPM and an average accuracy of 9.84 WPM for\ntext entry. Finally, we demonstrate potential applications of HeadText in\nhands-free scenarios for (a). text entry of people with motor impairments, (b).\nprivate text entry, and (c). socially acceptable text entry.",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Songlin Xu",
      "Guanjie Wang",
      "Ziyuan Fang",
      "Guangwei Zhang",
      "Guangzhu Shang",
      "Rongde Lu",
      "Liqun He"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09978"
  },
  {
    "id": "arXiv:2205.09981",
    "title": "Distributed Optimization in Distribution Systems with Grid-Forming and  Grid-Supporting Inverters",
    "abstract": "With massive penetrations of active grid-edge technologies, distributed\ncomputing and optimization paradigm has gained significant attention to solve\ndistribution-level optimal power flow (OPF) problems. However, the application\nof generic distributed optimization techniques to OPF problems leads to a very\nlarge number of macro-iterations or communication rounds among the distributed\ncomputing agents delaying the decision-making process or resulting in\nsuboptimal solutions. Moreover, the existing distribution-level OPF problems\ntypically model inverter-interfaced distributed energy resources (DERs) as\ngrid-following inverters; grid-supporting and grid-forming functionalities have\nnot been explicitly considered. The added complexities introduced by different\ninverter models require further attention to developing an appropriate model\nfor new types of inverter-based DERs and computationally-tractable OPF\nalgorithms. In this paper, we expand the distribution-level OPF model to\ninclude a combination of the grid-forming, grid-supporting, grid-following\ninverter-based DERs and also present the application of a domain-specific\nproblem decomposition and distributed algorithm for the topologically radial\npower distribution systems to efficiently solve distribution-level OPF problem.",
    "descriptor": "",
    "authors": [
      "Rabayet Sadnan",
      "Anamika Dubey"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09981"
  },
  {
    "id": "arXiv:2205.09986",
    "title": "SafeNet: Mitigating Data Poisoning Attacks on Private Machine Learning",
    "abstract": "Secure multiparty computation (MPC) has been proposed to allow multiple\nmutually distrustful data owners to jointly train machine learning (ML) models\non their combined data. However, the datasets used for training ML models might\nbe under the control of an adversary mounting a data poisoning attack, and MPC\nprevents inspecting training sets to detect poisoning. We show that multiple\nMPC frameworks for private ML training are susceptible to backdoor and targeted\npoisoning attacks. To mitigate this, we propose SafeNet, a framework for\nbuilding ensemble models in MPC with formal guarantees of robustness to data\npoisoning attacks. We extend the security definition of private ML training to\naccount for poisoning and prove that our SafeNet design satisfies the\ndefinition. We demonstrate SafeNet's efficiency, accuracy, and resilience to\npoisoning on several machine learning datasets and models. For instance,\nSafeNet reduces backdoor attack success from 100% to 0% for a neural network\nmodel, while achieving 39x faster training and 36x less communication than the\nfour-party MPC framework of Dalskov et al.",
    "descriptor": "",
    "authors": [
      "Harsh Chaudhari",
      "Matthew Jagielski",
      "Alina Oprea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09986"
  },
  {
    "id": "arXiv:2205.09987",
    "title": "Model Predictive Manipulation of Compliant Objects with Multi-Objective  Optimizer and Adversarial Network for Occlusion Compensation",
    "abstract": "The robotic manipulation of compliant objects is currently one of the most\nactive problems in robotics due to its potential to automate many important\napplications. Despite the progress achieved by the robotics community in recent\nyears, the 3D shaping of these types of materials remains an open research\nproblem. In this paper, we propose a new vision-based controller to\nautomatically regulate the shape of compliant objects with robotic arms. Our\nmethod uses an efficient online surface/curve fitting algorithm that quantifies\nthe object's geometry with a compact vector of features; This feedback-like\nvector enables to establish an explicit shape servo-loop. To coordinate the\nmotion of the robot with the computed shape features, we propose a\nreceding-time estimator that approximates the system's sensorimotor model while\nsatisfying various performance criteria. A deep adversarial network is\ndeveloped to robustly compensate for visual occlusions in the camera's field of\nview, which enables to guide the shaping task even with partial observations of\nthe object. Model predictive control is utilized to compute the robot's shaping\nmotions subject to workspace and saturation constraints. A detailed\nexperimental study is presented to validate the effectiveness of the proposed\ncontrol framework.",
    "descriptor": "",
    "authors": [
      "Jiaming Qi",
      "Dongyu Li",
      "Yufeng Gao",
      "Peng Zhou",
      "David Navarro-Alarcon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09987"
  },
  {
    "id": "arXiv:2205.09988",
    "title": "SALTED: A Framework for SAlient Long-Tail Translation Error Detection",
    "abstract": "Traditional machine translation (MT) metrics provide an average measure of\ntranslation quality that is insensitive to the long tail of behavioral problems\nin MT. Examples include translation of numbers, physical units, dropped content\nand hallucinations. These errors, which occur rarely and unpredictably in\nNeural Machine Translation (NMT), greatly undermine the reliability of\nstate-of-the-art MT systems. Consequently, it is important to have visibility\ninto these problems during model development. Towards this direction, we\nintroduce SALTED, a specifications-based framework for behavioral testing of MT\nmodels that provides fine-grained views of salient long-tail errors, permitting\ntrustworthy visibility into previously invisible problems. At the core of our\napproach is the development of high-precision detectors that flag errors (or\nalternatively, verify output correctness) between a source sentence and a\nsystem output. We demonstrate that such detectors could be used not just to\nidentify salient long-tail errors in MT systems, but also for higher-recall\nfiltering of the training data, fixing targeted errors with model fine-tuning\nin NMT and generating novel data for metamorphic testing to elicit further bugs\nin models.",
    "descriptor": "",
    "authors": [
      "Vikas Raunak",
      "Matt Post",
      "Arul Menezes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09988"
  },
  {
    "id": "arXiv:2205.09990",
    "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
    "abstract": "Meta-learning approaches enable machine learning systems to adapt to new\ntasks given few examples by leveraging knowledge from related tasks. However, a\nlarge number of meta-training tasks are still required for generalization to\nunseen tasks during meta-testing, which introduces a critical bottleneck for\nreal-world problems that come with only few tasks, due to various reasons\nincluding the difficulty and cost of constructing tasks. Recently, several task\naugmentation methods have been proposed to tackle this issue using\ndomain-specific knowledge to design augmentation techniques to densify the\nmeta-training task distribution. However, such reliance on domain-specific\nknowledge renders these methods inapplicable to other domains. While Manifold\nMixup based task augmentation methods are domain-agnostic, we empirically find\nthem ineffective on non-image domains. To tackle these limitations, we propose\na novel domain-agnostic task augmentation method, Meta-Interpolation, which\nutilizes expressive neural set functions to densify the meta-training task\ndistribution using bilevel optimization. We empirically validate the efficacy\nof Meta-Interpolation on eight datasets spanning across various domains such as\nimage classification, molecule property prediction, text classification and\nspeech recognition. Experimentally, we show that Meta-Interpolation\nconsistently outperforms all the relevant baselines. Theoretically, we prove\nthat task interpolation with the set function regularizes the meta-learner to\nimprove generalization.",
    "descriptor": "\nComments: First two authors contributed equally. Name order decided by a coin toss\n",
    "authors": [
      "Seanie Lee",
      "Bruno Andreis",
      "Kenji Kawaguchi",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09990"
  },
  {
    "id": "arXiv:2205.09991",
    "title": "Planning with Diffusion for Flexible Behavior Synthesis",
    "abstract": "Model-based reinforcement learning methods often use learning only for the\npurpose of estimating an approximate dynamics model, offloading the rest of the\ndecision-making work to classical trajectory optimizers. While conceptually\nsimple, this combination has a number of empirical shortcomings, suggesting\nthat learned models may not be well-suited to standard trajectory optimization.\nIn this paper, we consider what it would look like to fold as much of the\ntrajectory optimization pipeline as possible into the modeling problem, such\nthat sampling from the model and planning with it become nearly identical. The\ncore of our technical approach lies in a diffusion probabilistic model that\nplans by iteratively denoising trajectories. We show how classifier-guided\nsampling and image inpainting can be reinterpreted as coherent planning\nstrategies, explore the unusual and useful properties of diffusion-based\nplanning methods, and demonstrate the effectiveness of our framework in control\nsettings that emphasize long-horizon decision-making and test-time flexibility.",
    "descriptor": "\nComments: ICML 2022 (long talk). Project page and code at this https URL\n",
    "authors": [
      "Michael Janner",
      "Yilun Du",
      "Joshua B. Tenenbaum",
      "Sergey Levine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09991"
  },
  {
    "id": "arXiv:2205.09992",
    "title": "Asynchronous Byzantine Reliable Broadcast With a Message Adversary",
    "abstract": "This paper considers the problem of reliable broadcast in asynchronous\nauthenticated systems, in which n processes communicate using signed messages\nand up to t processes may behave arbitrarily (Byzantine processes). In\naddition, for each message m broadcast by a correct (i.e., non-Byzantine)\nprocess, a message adversary may prevent up to d correct processes from\nreceiving m. (This message adversary captures network failures such as\ntransient disconnections, silent churn, or message losses.) Considering such a\n\"double\" adversarial context and assuming n > 3t + 2d, a reliable broadcast\nalgorithm is presented. Interestingly, when there is no message adversary\n(i.e., d = 0), the algorithm terminates in two communication steps (so, in this\ncase, this algorithm is optimal in terms of both Byzantine tolerance and time\nefficiency). It is then shown that the condition n > 3t + 2d is necessary for\nimplementing reliable broadcast in the presence of both Byzantine processes and\na message adversary (whether the underlying system is enriched with signatures\nor not).",
    "descriptor": "",
    "authors": [
      "Timoth\u00e9 Albouy",
      "Davide Frey",
      "Michel Raynal",
      "Fran\u00e7ois Ta\u00efani"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.09992"
  },
  {
    "id": "arXiv:2205.09993",
    "title": "Second-order uniformly asymptotic-preserving space-time-ImEx schemes for  hyperbolic balance laws with stiff relaxation",
    "abstract": "We consider hyperbolic systems of conservation laws with relaxation source\nterms leading to a diffusive asymptotic limit under a parabolic scaling. We\nintroduce a new class of secondorder in time and space numerical schemes, which\nare uniformly asymptotic preserving schemes. The proposed Implicit-Explicit\n(ImEx) approach, does not follow the usual path relying on the method of lines,\neither with multi-step methods or Runge-Kutta methods, or semi-discretized in\ntime equations, but is inspired from the Lax-Wendroff approach with the proper\nlevel of implicit treatment of the source term. As a result, it yields a very\ncompact stencil in space and time and we are able to rigorously show that both\nthe second-order accuracy and the stability conditions are independent of the\nfast scales in the asymptotic regime, including the study of boundary\nconditions. We provide an original derivation of l 2 and l $\\infty$ stability\nconditions of the scheme that do not deteriorate the second order accuracy\nwithout relying on a limiter of any type in the linear case, in particular for\nshock solutions, and extend such results to the nonlinear case, showing the\nnovelty of the method. The prototype system for the linear case is the\nhyperbolic heat equation, whereas barotropic Euler equations of gas dynamics\nwith friction are the one for the nonlinear case. The method is also able to\nyield very accurate steady solutions in the nonlinear case when the relaxation\ncoefficient in the source term depends on space. A thorough numerical\nassessment of the proposed strategy is provided by investigating smooth\nsolutions, solutions with shocks and solutions leading to a steady state with\nspace dependent relaxation coefficient.",
    "descriptor": "",
    "authors": [
      "Louis Reboul",
      "Teddy Pichard",
      "Marc Massot"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.09993"
  },
  {
    "id": "arXiv:2205.09995",
    "title": "Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning",
    "abstract": "Learning with little data is challenging but often inevitable in various\napplication scenarios where the labeled data is limited and costly. Recently,\nfew-shot learning (FSL) gained increasing attention because of its\ngeneralizability of prior knowledge to new tasks that contain only a few\nsamples. However, for data-intensive models such as vision transformer (ViT),\ncurrent fine-tuning based FSL approaches are inefficient in knowledge\ngeneralization and thus degenerate the downstream task performances. In this\npaper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an\neffective and efficient FSL on ViT model. The key idea is to apply a mask on\nimage patches to screen out the task-irrelevant ones and to guide the ViT to\nfocus on task-relevant and discriminative patches during FSL. Particularly,\nMG-ViT only introduces an additional mask operation and a residual connection,\nenabling the inheritance of parameters from pre-trained ViT without any other\ncost. To optimally select representative few-shot samples, we also include an\nactive learning based sample selection method to further improve the\ngeneralizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both\nAgri-ImageNet classification task and ACFR apple detection task with\ngradient-weighted class activation mapping (Grad-CAM) as the mask. The\nexperimental results show that the MG-ViT model significantly improves the\nperformance when compared with general fine-tuning based ViT models, providing\nnovel insights and a concrete approach towards generalizing data-intensive and\nlarge-scale deep learning models for FSL.",
    "descriptor": "\nComments: 11 pages,4 figures, submitted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Yuzhong Chen",
      "Zhenxiang Xiao",
      "Lin Zhao",
      "Lu Zhang",
      "Haixing Dai",
      "David Weizhong Liu",
      "Zihao Wu",
      "Changhe Li",
      "Tuo Zhang",
      "Changying Li",
      "Dajiang Zhu",
      "Tianming Liu",
      "Xi Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09995"
  },
  {
    "id": "arXiv:2205.10000",
    "title": "A Linear Algebraic Framework for Quantum Internet Dynamic Scheduling",
    "abstract": "Future quantum internet aims to enable quantum communication between\narbitrary pairs of distant nodes through the sharing of end-to-end\nentanglement, a universal resource for many quantum applications. As in\nclassical networks, quantum networks also have to resolve problems related to\nrouting and satisfaction of service at a sufficient rate. We deal here with the\nproblem of scheduling when multiple commodities must be served through a\nquantum network based on first generation quantum repeaters, or quantum\nswitches. To this end, we introduce a novel discrete-time algebraic model for\narbitrary network topology, including transmission and memory losses, and\nadapted to dynamic scheduling decisions. Our algebraic model allows the\nscheduler to use the storage of temporary intermediate links to optimize the\nperformance, depending on the information availability, ranging from full\nglobal information for a centralized scheduler to partial local information for\na distributed one. As an illustrative example, we compare a simple greedy\nscheduling policy with several Max-Weight inspired scheduling policies and\nillustrate the resulting achievable rate regions for two competing pairs of\nclients through a network.",
    "descriptor": "",
    "authors": [
      "Paolo Fittipaldi",
      "Anastasios Giovanidis",
      "Fr\u00e9d\u00e9ric Grosshans"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.10000"
  },
  {
    "id": "arXiv:2205.10003",
    "title": "InDistill: Transferring Knowledge From Pruned Intermediate Layers",
    "abstract": "Deploying deep neural networks on hardware with limited resources, such as\nsmartphones and drones, constitutes a great challenge due to their\ncomputational complexity. Knowledge distillation approaches aim at transferring\nknowledge from a large model to a lightweight one, also known as teacher and\nstudent respectively, while distilling the knowledge from intermediate layers\nprovides an additional supervision to that task. The capacity gap between the\nmodels, the information encoding that collapses its architectural alignment,\nand the absence of appropriate learning schemes for transferring multiple\nlayers restrict the performance of existing methods. In this paper, we propose\na novel method, termed InDistill, that can drastically improve the performance\nof existing single-layer knowledge distillation methods by leveraging the\nproperties of channel pruning to both reduce the capacity gap between the\nmodels and retain the architectural alignment. Furthermore, we propose a\ncurriculum learning based scheme for enhancing the effectiveness of\ntransferring knowledge from multiple intermediate layers. The proposed method\nsurpasses state-of-the-art performance on three benchmark image datasets.",
    "descriptor": "",
    "authors": [
      "Ioannis Sarridis",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Ioannis Kompatsiaris"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10003"
  },
  {
    "id": "arXiv:2205.10004",
    "title": "RiskLoc: Localization of Multi-dimensional Root Causes by Weighted Risk",
    "abstract": "Failures and anomalies in large-scale software systems are unavoidable\nincidents. When an issue is detected, operators need to quickly and correctly\nidentify its location to facilitate a swift repair. In this work, we consider\nthe problem of identifying the root cause set that best explains an anomaly in\nmulti-dimensional time series with categorical attributes. The huge search\nspace is the main challenge, even for a small number of attributes and small\nvalue sets, the number of theoretical combinations is too large to brute force.\nPrevious approaches have thus focused on reducing the search space, but they\nall suffer from various issues, requiring extensive manual parameter tuning,\nbeing too slow and thus impractical, or being incapable of finding more complex\nroot causes. We propose RiskLoc to solve the problem of multidimensional root\ncause localization. RiskLoc applies a 2-way partitioning scheme and assigns\nelement weights that linearly increase with the distance from the partitioning\npoint. A risk score is assigned to each element that integrates two factors, 1)\nits weighted proportion within the abnormal partition, and 2) the relative\nchange in the deviation score adjusted for the ripple effect property.\nExtensive experiments on multiple datasets verify the effectiveness and\nefficiency of RiskLoc, and for a comprehensive evaluation, we introduce three\nsynthetically generated datasets that complement existing datasets. We\ndemonstrate that RiskLoc consistently outperforms state-of-the-art baselines,\nespecially in more challenging root cause scenarios, with gains in F1-score up\nto 57% over the second-best approach with comparable running times.",
    "descriptor": "",
    "authors": [
      "Marcus Kalander"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10004"
  },
  {
    "id": "arXiv:2205.10006",
    "title": "Self-Supervised Depth Estimation with Isometric-Self-Sample-Based  Learning",
    "abstract": "Managing the dynamic regions in the photometric loss formulation has been a\nmain issue for handling the self-supervised depth estimation problem. Most\nprevious methods have alleviated this issue by removing the dynamic regions in\nthe photometric loss formulation based on the masks estimated from another\nmodule, making it difficult to fully utilize the training images. In this\npaper, to handle this problem, we propose an isometric self-sample-based\nlearning (ISSL) method to fully utilize the training images in a simple yet\neffective way. The proposed method provides additional supervision during\ntraining using self-generated images that comply with pure static scene\nassumption. Specifically, the isometric self-sample generator synthesizes\nself-samples for each training image by applying random rigid transformations\non the estimated depth. Thus both the generated self-samples and the\ncorresponding training image always follow the static scene assumption. We show\nthat plugging our ISSL module into several existing models consistently\nimproves the performance by a large margin. In addition, it also boosts the\ndepth accuracy over different types of scene, i.e., outdoor scenes (KITTI and\nMake3D) and indoor scene (NYUv2), validating its high effectiveness.",
    "descriptor": "",
    "authors": [
      "Geonho Cha",
      "Ho-Deok Jang",
      "Dongyoon Wee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10006"
  },
  {
    "id": "arXiv:2205.10008",
    "title": "Action parsing using context features",
    "abstract": "We propose an action parsing algorithm to parse a video sequence containing\nan unknown number of actions into its action segments. We argue that context\ninformation, particularly the temporal information about other actions in the\nvideo sequence, is valuable for action segmentation. The proposed parsing\nalgorithm temporally segments the video sequence into action segments. The\noptimal temporal segmentation is found using a dynamic programming search\nalgorithm that optimizes the overall classification confidence score. The\nclassification score of each segment is determined using local features\ncalculated from that segment as well as context features calculated from other\ncandidate action segments of the sequence. Experimental results on the\nBreakfast activity data-set showed improved segmentation accuracy compared to\nexisting state-of-the-art parsing techniques.",
    "descriptor": "",
    "authors": [
      "Nagita Mehrseresht"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10008"
  },
  {
    "id": "arXiv:2205.10009",
    "title": "The price of ignorance: how much does it cost to forget noise structure  in low-rank matrix estimation?",
    "abstract": "We consider the problem of estimating a rank-1 signal corrupted by structured\nrotationally invariant noise, and address the following question: how well do\ninference algorithms perform when the noise statistics is unknown and hence\nGaussian noise is assumed? While the matched Bayes-optimal setting with\nunstructured noise is well understood, the analysis of this mismatched problem\nis only at its premises. In this paper, we make a step towards understanding\nthe effect of the strong source of mismatch which is the noise statistics. Our\nmain technical contribution is the rigorous analysis of a Bayes estimator and\nof an approximate message passing (AMP) algorithm, both of which incorrectly\nassume a Gaussian setup. The first result exploits the theory of spherical\nintegrals and of low-rank matrix perturbations; the idea behind the second one\nis to design and analyze an artificial AMP which, by taking advantage of the\nflexibility in the denoisers, is able to \"correct\" the mismatch. Armed with\nthese sharp asymptotic characterizations, we unveil a rich and often unexpected\nphenomenology. For example, despite AMP is in principle designed to efficiently\ncompute the Bayes estimator, the former is outperformed by the latter in terms\nof mean-square error. We show that this performance gap is due to an incorrect\nestimation of the signal norm. In fact, when the SNR is large enough, the\noverlaps of the AMP and the Bayes estimator coincide, and they even match those\nof optimal estimators taking into account the structure of the noise.",
    "descriptor": "",
    "authors": [
      "Jean Barbier",
      "TianQi Hou",
      "Marco Mondelli",
      "Manuel S\u00e1enz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.10009"
  },
  {
    "id": "arXiv:2205.10011",
    "title": "Constructive Interpretability with CoLabel: Corroborative Integration,  Complementary Features, and Collaborative Learning",
    "abstract": "Machine learning models with explainable predictions are increasingly sought\nafter, especially for real-world, mission-critical applications that require\nbias detection and risk mitigation. Inherent interpretability, where a model is\ndesigned from the ground-up for interpretability, provides intuitive insights\nand transparent explanations on model prediction and performance. In this\npaper, we present CoLabel, an approach to build interpretable models with\nexplanations rooted in the ground truth. We demonstrate CoLabel in a vehicle\nfeature extraction application in the context of vehicle make-model recognition\n(VMMR). CoLabel performs VMMR with a composite of interpretable features such\nas vehicle color, type, and make, all based on interpretable annotations of the\nground truth labels. First, CoLabel performs corroborative integration to join\nmultiple datasets that each have a subset of desired annotations of color,\ntype, and make. Then, CoLabel uses decomposable branches to extract\ncomplementary features corresponding to desired annotations. Finally, CoLabel\nfuses them together for final predictions. During feature fusion, CoLabel\nharmonizes complementary branches so that VMMR features are compatible with\neach other and can be projected to the same semantic space for classification.\nWith inherent interpretability, CoLabel achieves superior performance to the\nstate-of-the-art black-box models, with accuracy of 0.98, 0.95, and 0.94 on\nCompCars, Cars196, and BoxCars116K, respectively. CoLabel provides intuitive\nexplanations due to constructive interpretability, and subsequently achieves\nhigh accuracy and usability in mission-critical situations.",
    "descriptor": "",
    "authors": [
      "Abhijit Suprem",
      "Sanjyot Vaidya",
      "Suma Cherkadi",
      "Purva Singh",
      "Joao Eduardo Ferreira",
      "Calton Pu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10011"
  },
  {
    "id": "arXiv:2205.10012",
    "title": "Descartes: Generating Short Descriptions of Wikipedia Articles",
    "abstract": "We introduce and tackle the problem of automatically generating short\ndescriptions of Wikipedia articles (e.g., Belgium has a short description\nCountry in Western Europe). We introduce Descartes, a model that can generate\ndescriptions performing on par with human editors. Our human evaluation results\nindicate that Descartes is preferred over editor-written descriptions about 50%\nof time. Further manual analysis show that Descartes generates descriptions\nconsidered as \"valid\" for 91.3% of articles, this is the as same editor-written\ndescriptions. Such performances are made possible by integrating other signals\nnaturally existing in Wikipedia: (i) articles about the same entity in\ndifferent languages, (ii) existing short descriptions in other languages, and\n(iii) structural information from Wikidata. Our work has direct practical\napplications in helping Wikipedia editors to provide short descriptions for the\nmore than 9 million articles still missing one. Finally, our proposed\narchitecture can easily be re-purposed to address other information gaps in\nWikipedia.",
    "descriptor": "",
    "authors": [
      "Marija Sakota",
      "Maxime Peyrard",
      "Robert West"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10012"
  },
  {
    "id": "arXiv:2205.10014",
    "title": "A Survey of Trustworthy Graph Learning: Reliability, Explainability, and  Privacy Protection",
    "abstract": "Deep graph learning has achieved remarkable progresses in both business and\nscientific areas ranging from finance and e-commerce, to drug and advanced\nmaterial discovery. Despite these progresses, how to ensure various deep graph\nlearning algorithms behave in a socially responsible manner and meet regulatory\ncompliance requirements becomes an emerging problem, especially in\nrisk-sensitive domains. Trustworthy graph learning (TwGL) aims to solve the\nabove problems from a technical viewpoint. In contrast to conventional graph\nlearning research which mainly cares about model performance, TwGL considers\nvarious reliability and safety aspects of the graph learning framework\nincluding but not limited to robustness, explainability, and privacy. In this\nsurvey, we provide a comprehensive review of recent leading approaches in the\nTwGL field from three dimensions, namely, reliability, explainability, and\nprivacy protection. We give a general categorization for existing work and\nreview typical work for each category. To give further insights for TwGL\nresearch, we provide a unified view to inspect previous works and build the\nconnection between them. We also point out some important open problems\nremaining to be solved in the future developments of TwGL.",
    "descriptor": "\nComments: Preprint; 54 pages. arXiv admin note: substantial text overlap with arXiv:2202.07114\n",
    "authors": [
      "Bingzhe Wu",
      "Jintang Li",
      "Junchi Yu",
      "Yatao Bian",
      "Hengtong Zhang",
      "CHaochao Chen",
      "Chengbin Hou",
      "Guoji Fu",
      "Liang Chen",
      "Tingyang Xu",
      "Yu Rong",
      "Xiaolin Zheng",
      "Junzhou Huang",
      "Ran He",
      "Baoyuan Wu",
      "GUangyu Sun",
      "Peng Cui",
      "Zibin Zheng",
      "Zhe Liu",
      "Peilin Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10014"
  },
  {
    "id": "arXiv:2205.10016",
    "title": "Self-Paced Multi-Agent Reinforcement Learning",
    "abstract": "Curriculum reinforcement learning (CRL) aims to speed up learning of a task\nby changing gradually the difficulty of the task from easy to hard through\ncontrol of factors such as initial state or environment dynamics. While\nautomating CRL is well studied in the single-agent setting, in multi-agent\nreinforcement learning (MARL) an open question is whether control of the number\nof agents with other factors in a principled manner is beneficial, prior\napproaches typically relying on hand-crafted heuristics. In addition, how the\ntasks evolve as the number of agents changes remains understudied, which is\ncritical for scaling to more challenging tasks. We introduce self-paced MARL\n(SPMARL) that enables optimizing the number of agents with other environment\nfactors in a principled way, and, show that usual assumptions such as that\nfewer agents make the task always easier are not generally valid. The\ncurriculum induced by SPMARL reveals the evolution of tasks w.r.t. number of\nagents and experiments show that SPMARL improves the performance when the\nnumber of agents sufficiently influences task difficulty.",
    "descriptor": "\nComments: 13 pages, 9 figures\n",
    "authors": [
      "Wenshuai Zhao",
      "Joni Pajarinen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.10016"
  },
  {
    "id": "arXiv:2205.10017",
    "title": "A stochastic game framework for patrolling a border",
    "abstract": "In this paper we consider a stochastic game for modelling the interactions\nbetween smugglers and a patroller along a border. The problem we examine\ninvolves a group of cooperating smugglers making regular attempts to bring\nsmall amounts of illicit goods across a border. A single patroller has the goal\nof preventing the smugglers from doing so, but must pay a cost to travel from\none location to another. We model the problem as a two-player stochastic game\nand look to find the Nash equilibrium to gain insight to real world problems.\nOur framework extends the literature by assuming that the smugglers choose a\ncontinuous quantity of contraband, complicating the analysis of the game. We\ndiscuss a number of properties of Nash equilibria, including the aggregation of\nsmugglers, the discount factors of the players, and the equivalence to a\nzero-sum game. Additionally, we present algorithms to find Nash equilibria that\nare more computationally efficient than existing methods. We also consider\ncertain assumptions on the parameters of the model that give interesting\nequilibrium strategies for the players.",
    "descriptor": "",
    "authors": [
      "Matthew Darlington",
      "Kevin D. Glazebrook",
      "David S. Leslie",
      "Rob Shone",
      "Roberto Szechtman"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.10017"
  },
  {
    "id": "arXiv:2205.10018",
    "title": "NMA: Neural Multi-slot Auctions with Externalities for Online  Advertising",
    "abstract": "Online advertising driven by auctions brings billions of dollars in revenue\nfor social networking services and e-commerce platforms. GSP auction, which is\nsimple and easy to understand for advertisers, has almost become the benchmark\nfor ad auction mechanisms in the industry. However, the allocation stability of\nGSP depends on the separable CTR assumption, which means that GSP considers\nneither position-dependent externalities nor ad-dependent externalities in\nmulti-slot scenario, leading to suboptimal performance. Some GSP-based deep\nauctions (e.g., DeepGSP, DNA) have attempted to upgrade GSP with deep neural\nnetworks, while only modeling local externalities and thus still suboptimal. On\nthe other hand, although VCG-based multi-slot auctions (e.g., VCG, WVCG) take\nexternalities into consideration, they lack an efficient balance of both\nrevenue and social welfare. In this paper, we propose a novel auction named\nNeural Multi-slot Auction (NMA) to tackle the above-mentioned challenges.\nSpecifically, we model the global externalities effectively with a\ncontext-aware list-wise prediction module to achieve better performance. We\ndesign a list-wise deep rank module to guarantee incentive compatibility in\nend-to-end learning. Furthermore, we propose an auxiliary loss for social\nwelfare to effectively reduce the decline of social welfare while maximizing\nrevenue. Experiment results on both offline large-scale datasets and online A/B\ntests demonstrate that NMA obtains higher revenue with balanced social welfare\nthan other existing auction mechanisms (i.e., GSP, DNA, WVCG) in industrial\npractice, and we have successfully deployed NMA on Meituan food delivery\nplatform.",
    "descriptor": "\nComments: 10 pages, 3figures\n",
    "authors": [
      "Guogang Liao",
      "Xuejian Li",
      "Ze Wang",
      "Fan Yang",
      "Muzhi Guan",
      "Bingqi Zhu",
      "Yongkang Wang",
      "Xingxing Wang",
      "Dong Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10018"
  },
  {
    "id": "arXiv:2205.10019",
    "title": "Translating Hanja historical documents to understandable Korean and  English",
    "abstract": "The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of\nJoseon, the 500-year kingdom preceding the modern nation of Korea. The Annals\nwere originally written in an archaic Korean writing system, `Hanja', and\ntranslated into Korean from 1968 to 1993. However, this translation was literal\nand contained many archaic Korean words; thus, a new expert translation effort\nbegan in 2012, completing the records of only one king in a decade. Also,\nexpert translators are working on an English translation, of which only one\nking's records are available because of the high cost and slow progress. Thus,\nwe propose H2KE, the neural machine translation model that translates Hanja\nhistorical documents to understandable Korean and English. Based on the\nmultilingual neural machine translation approach, it translates the historical\ndocument written in Hanja, using both the full dataset of outdated Korean\ntranslation and a small dataset of recently translated Korean and English. We\ncompare our method with two baselines: one is a recent model that\nsimultaneously learns to restore and translate Hanja historical document and\nthe other is the transformer that trained on newly translated corpora only. The\nresults show that our method significantly outperforms the baselines in terms\nof BLEU score in both modern Korean and English translations. We also conduct a\nhuman evaluation that shows that our translation is preferred over the original\nexpert translation.",
    "descriptor": "",
    "authors": [
      "Juhee Son",
      "Jiho Jin",
      "Haneul Yoo",
      "JinYeong Bak",
      "Kyunghyun Cho",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10019"
  },
  {
    "id": "arXiv:2205.10020",
    "title": "Neural Additive Models for Nowcasting",
    "abstract": "Deep neural networks (DNNs) are one of the most highlighted methods in\nmachine learning. However, as DNNs are black-box models, they lack explanatory\npower for their predictions. Recently, neural additive models (NAMs) have been\nproposed to provide this power while maintaining high prediction performance.\nIn this paper, we propose a novel NAM approach for multivariate nowcasting (NC)\nproblems, which comprise an important focus area of machine learning. For the\nmultivariate time-series data used in NC problems, explanations should be\nconsidered for every input value to the variables at distinguishable time\nsteps. By employing generalized additive models, the proposed NAM-NC\nsuccessfully explains each input value's importance for multiple variables and\ntime steps. Experimental results involving a toy example and two real-world\ndatasets show that the NAM-NC predicts multivariate time-series data as\naccurately as state-of-the-art neural networks, while also providing the\nexplanatory importance of each input value. We also examine parameter-sharing\nnetworks using NAM-NC to decrease their complexity, and NAM-MC's hard-tied\nfeature net extracted explanations with good performance.",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Wonkeun Jo",
      "Dongil Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10020"
  },
  {
    "id": "arXiv:2205.10021",
    "title": "Predicting electrode array impedance after one month from cochlear  implantation surgery",
    "abstract": "Sensorineural hearing loss can be treated using Cochlear implantation. After\nthis surgery using the electrode array impedance measurements, we can check the\nstability of the impedance value and the dynamic range. Deterioration of speech\nrecognition scores could happen because of increased impedance values.\nMedicines used to do these measures many times during a year after the surgery.\nPredicting the electrode impedance could help in taking decisions to help the\npatient get better hearing. In this research we used a dataset of 80 patients\nof children who did cochlear implantation using MED-EL FLEX28 electrode array\nof 12 channels. We predicted the electrode impedance on each channel after 1\nmonth from the surgery date. We used different machine learning algorithms like\nneural networks and decision trees. Our results indicates that the electrode\nimpedance can be predicted, and the best algorithm is different based on the\nelectrode channel. Also, the accuracy level varies between 66% and 100% based\non the electrode channel when accepting an error range between 0 and 3 KO.\nFurther research is required to predict the electrode impedance after three\nmonths, six months and one year.",
    "descriptor": "",
    "authors": [
      "Yousef A. Alohali",
      "Yassin Abdelsamad",
      "Tamer Mesallam",
      "Fida Almuhawas",
      "Abdulrahman Hagr",
      "Mahmoud S. Fayed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.10021"
  },
  {
    "id": "arXiv:2205.10022",
    "title": "Towards Consistency in Adversarial Classification",
    "abstract": "In this paper, we study the problem of consistency in the context of\nadversarial examples. Specifically, we tackle the following question: can\nsurrogate losses still be used as a proxy for minimizing the $0/1$ loss in the\npresence of an adversary that alters the inputs at test-time? Different from\nthe standard classification task, this question cannot be reduced to a\npoint-wise minimization problem, and calibration needs not to be sufficient to\nensure consistency. In this paper, we expose some pathological behaviors\nspecific to the adversarial problem, and show that no convex surrogate loss can\nbe consistent or calibrated in this context. It is therefore necessary to\ndesign another class of surrogate functions that can be used to solve the\nadversarial consistency issue. As a first step towards designing such a class,\nwe identify sufficient and necessary conditions for a surrogate loss to be\ncalibrated in both the adversarial and standard settings. Finally, we give some\ndirections for building a class of losses that could be consistent in the\nadversarial framework.",
    "descriptor": "",
    "authors": [
      "Laurent Meunier",
      "Rapha\u00ebl Ettedgui",
      "Rafael Pinot",
      "Yann Chevaleyre",
      "Jamal Atif"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10022"
  },
  {
    "id": "arXiv:2205.10023",
    "title": "Transition-based Semantic Role Labeling with Pointer Networks",
    "abstract": "Semantic role labeling (SRL) focuses on recognizing the predicate-argument\nstructure of a sentence and plays a critical role in many natural language\nprocessing tasks such as machine translation and question answering.\nPractically all available methods do not perform full SRL, since they rely on\npre-identified predicates, and most of them follow a pipeline strategy, using\nspecific models for undertaking one or several SRL subtasks. In addition,\nprevious approaches have a strong dependence on syntactic information to\nachieve state-of-the-art performance, despite being syntactic trees equally\nhard to produce. These simplifications and requirements make the majority of\nSRL systems impractical for real-world applications. In this article, we\npropose the first transition-based SRL approach that is capable of completely\nprocessing an input sentence in a single left-to-right pass, with neither\nleveraging syntactic information nor resorting to additional modules. Thanks to\nour implementation based on Pointer Networks, full SRL can be accurately and\nefficiently done in $O(n^2)$, achieving the best performance to date on the\nmajority of languages from the CoNLL-2009 shared task.",
    "descriptor": "",
    "authors": [
      "Daniel Fern\u00e1ndez-Gonz\u00e1lez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10023"
  },
  {
    "id": "arXiv:2205.10027",
    "title": "pISTA: preconditioned Iterative Soft Thresholding Algorithm for  Graphical Lasso",
    "abstract": "We propose a novel quasi-Newton method for solving the sparse inverse\ncovariance estimation problem also known as the graphical least absolute\nshrinkage and selection operator (GLASSO). This problem is often solved using a\nsecond-order quadratic approximation. However, in such algorithms the Hessian\nterm is complex and computationally expensive to handle. Therefore, our method\nuses the inverse of the Hessian as a preconditioner to simplify and approximate\nthe quadratic element at the cost of a more complex \\(\\ell_1\\) element. The\nvariables of the resulting preconditioned problem are coupled only by the\n\\(\\ell_1\\) sub-derivative of each other, which can be guessed with minimal cost\nusing the gradient itself, allowing the algorithm to be parallelized and\nimplemented efficiently on GPU hardware accelerators. Numerical results on\nsynthetic and real data demonstrate that our method is competitive with other\nstate-of-the-art approaches.",
    "descriptor": "",
    "authors": [
      "Gal Shalom",
      "Eran Treister",
      "Irad Yavneh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.10027"
  },
  {
    "id": "arXiv:2205.10031",
    "title": "Deep Learning-based Inertial Odometry for Pedestrian Tracking using  Attention Mechanism and Res2Net Module",
    "abstract": "Pedestrian dead reckoning is a challenging task due to the low-cost inertial\nsensor error accumulation. Recent research has shown that deep learning methods\ncan achieve impressive performance in handling this issue. In this letter, we\npropose inertial odometry using a deep learning-based velocity estimation\nmethod. The deep neural network based on Res2Net modules and two convolutional\nblock attention modules is leveraged to restore the potential connection\nbetween the horizontal velocity vector and raw inertial data from a smartphone.\nOur network is trained using only fifty percent of the public inertial odometry\ndataset (RoNIN) data. Then, it is validated on the RoNIN testing dataset and\nanother public inertial odometry dataset (OXIOD). Compared with the traditional\nstep-length and heading system-based algorithm, our approach decreases the\nabsolute translation error (ATE) by 76%-86%. In addition, compared with the\nstate-of-the-art deep learning method (RoNIN), our method improves its ATE by\n6%-31.4%.",
    "descriptor": "",
    "authors": [
      "Boxuan Chen",
      "Ruifeng Zhang",
      "Shaochu Wang",
      "Liqiang Zhang",
      "Yu Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10031"
  },
  {
    "id": "arXiv:2205.10032",
    "title": "Survey on Fair Reinforcement Learning: Theory and Practice",
    "abstract": "Fairness-aware learning aims at satisfying various fairness constraints in\naddition to the usual performance criteria via data-driven machine learning\ntechniques. Most of the research in fairness-aware learning employs the setting\nof fair-supervised learning. However, many dynamic real-world applications can\nbe better modeled using sequential decision-making problems and fair\nreinforcement learning provides a more suitable alternative for addressing\nthese problems. In this article, we provide an extensive overview of fairness\napproaches that have been implemented via a reinforcement learning (RL)\nframework. We discuss various practical applications in which RL methods have\nbeen applied to achieve a fair solution with high accuracy. We further include\nvarious facets of the theory of fair reinforcement learning, organizing them\ninto single-agent RL, multi-agent RL, long-term fairness via RL, and offline\nlearning. Moreover, we highlight a few major issues to explore in order to\nadvance the field of fair-RL, namely - i) correcting societal biases, ii)\nfeasibility of group fairness or individual fairness, and iii) explainability\nin RL. Our work is beneficial for both researchers and practitioners as we\ndiscuss articles providing mathematical guarantees as well as articles with\nempirical studies on real-world problems.",
    "descriptor": "",
    "authors": [
      "Pratik Gajane",
      "Akrati Saxena",
      "Maryam Tavakol",
      "George Fletcher",
      "Mykola Pechenizkiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10032"
  },
  {
    "id": "arXiv:2205.10034",
    "title": "SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training  and Inference System",
    "abstract": "With the increasing diversity of ML infrastructures nowadays, distributed\ntraining over heterogeneous computing systems is desired to facilitate the\nproduction of big models. Mixture-of-Experts (MoE) models have been proposed to\nlower the cost of training subject to the overall size of models/data through\ngating and parallelism in a divide-and-conquer fashion. While DeepSpeed has\nmade efforts in carrying out large-scale MoE training over heterogeneous\ninfrastructures, the efficiency of training and inference could be further\nimproved from several system aspects, including load balancing,\ncommunication/computation efficiency, and memory footprint limits. In this\nwork, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and\nFusion communication over Hierarchical storage, so as to enjoy efficient\nparallelisms in various types. For scalable inference in a single node,\nespecially when the model size is larger than GPU memory, SE-MoE forms the\nCPU-GPU memory jointly into a ring of sections to load the model, and executes\nthe computation tasks across the memory sections in a round-robin manner for\nefficient inference. We carried out extensive experiments to evaluate SE-MoE,\nwhere SE-MoE successfully trains a Unified Feature Optimization (UFO) model\nwith a Sparsely-Gated Mixture-of-Experts model of 12B parameters in 8 days on\n48 A100 GPU cards. The comparison against the state-of-the-art shows that\nSE-MoE outperformed DeepSpeed with 33% higher throughput (tokens per second) in\ntraining and 13% higher throughput in inference in general. Particularly, under\nunbalanced MoE Tasks, e.g., UFO, SE-MoE achieved 64% higher throughput with 18%\nlower memory footprints. The code of the framework will be released on:\nhttps://github.com/PaddlePaddle/Paddle.",
    "descriptor": "",
    "authors": [
      "Liang Shen",
      "Zhihua Wu",
      "WeiBao Gong",
      "Hongxiang Hao",
      "Yangfan Bai",
      "HuaChao Wu",
      "Xinxuan Wu",
      "Haoyi Xiong",
      "Dianhai Yu",
      "Yanjun Ma"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10034"
  },
  {
    "id": "arXiv:2205.10036",
    "title": "Exploring Extreme Parameter Compression for Pre-trained Language Models",
    "abstract": "Recent work explored the potential of large-scale Transformer-based\npre-trained models, especially Pre-trained Language Models (PLMs) in natural\nlanguage processing. This raises many concerns from various perspectives, e.g.,\nfinancial costs and carbon emissions. Compressing PLMs like BERT with\nnegligible performance loss for faster inference and cheaper deployment has\nattracted much attention. In this work, we aim to explore larger compression\nratios for PLMs, among which tensor decomposition is a potential but\nunder-investigated one. Two decomposition and reconstruction protocols are\nfurther proposed to improve the effectiveness and efficiency during\ncompression. Our compressed BERT with ${1}/{7}$ parameters in Transformer\nlayers performs on-par with, sometimes slightly better than the original BERT\nin GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base\nwith $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding\nthe embedding layer) and $2.7 \\times$ faster on inference. To show that the\nproposed method is orthogonal to existing compression methods like knowledge\ndistillation, we also explore the benefit of the proposed method on a distilled\nBERT.",
    "descriptor": "\nComments: Accepted at ICLR2022. Code available at this https URL\n",
    "authors": [
      "Yuxin Ren",
      "Benyou Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10036"
  },
  {
    "id": "arXiv:2205.10037",
    "title": "Correct by Design Coordination of Autonomous Driving Systems",
    "abstract": "The paper proposes a method for the correct by design coordination of\nautonomous driving systems (ADS). It builds on previous results on collision\navoidance policies and the modeling of ADS by combining descriptions of their\nstatic environment in the form of maps, and the dynamic behavior of their\nvehicles. An ADS is modeled as a dynamic system involving a set of vehicles\ncoordinated by a Runtime that based on vehicle positions on a map and their\nkinetic attributes, computes free spaces for each vehicle. Vehicles are bounded\nto move within the corresponding allocated free spaces. We provide a correct by\ndesign safe control policy for an ADS if its vehicles and the Runtime respect\ncorresponding assume-guarantee contracts. The result is established by showing\nthat the composition of assume-guarantee contracts is an inductive invariant\nthat entails ADS safety. We show that it is practically possible to define\nspeed control policies for vehicles that comply with their contracts.\nFurthermore, we show that traffic rules can be specified in a linear-time\ntemporal logic, as a class of formulas that constrain vehicle speeds. The main\nresult is that, given a set of traffic rules, it is possible to derive free\nspace policies of the Runtime such that the resulting system behavior is safe\nby design with respect to the rules.",
    "descriptor": "",
    "authors": [
      "Marius Bozga",
      "Joseph Sifakis"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10037"
  },
  {
    "id": "arXiv:2205.10041",
    "title": "Posterior Refinement Improves Sample Efficiency in Bayesian Neural  Networks",
    "abstract": "Monte Carlo (MC) integration is the de facto method for approximating the\npredictive distribution of Bayesian neural networks (BNNs). But, even with many\nMC samples, Gaussian-based BNNs could still yield bad predictive performance\ndue to the posterior approximation's error. Meanwhile, alternatives to MC\nintegration tend to be more expensive or biased. In this work, we\nexperimentally show that the key to good MC-approximated predictive\ndistributions is the quality of the approximate posterior itself. However,\nprevious methods for obtaining accurate posterior approximations are expensive\nand non-trivial to implement. We, therefore, propose to refine Gaussian\napproximate posteriors with normalizing flows. When applied to last-layer BNNs,\nit yields a simple \\emph{post hoc} method for improving pre-existing parametric\napproximations. We show that the resulting posterior approximation is\ncompetitive with even the gold-standard full-batch Hamiltonian Monte Carlo.",
    "descriptor": "",
    "authors": [
      "Agustinus Kristiadi",
      "Runa Eschenhagen",
      "Philipp Hennig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10041"
  },
  {
    "id": "arXiv:2205.10042",
    "title": "ALPINE: Analog In-Memory Acceleration with Tight Processor Integration  for Deep Learning",
    "abstract": "Analog in-memory computing (AIMC) cores offers significant performance and\nenergy benefits for neural network inference with respect to digital logic\n(e.g., CPUs). AIMCs accelerate matrix-vector multiplications, which dominate\nthese applications' run-time. However, AIMC-centric platforms lack the\nflexibility of general-purpose systems, as they often have hard-coded data\nflows and can only support a limited set of processing functions. With the goal\nof bridging this gap in flexibility, we present a novel system architecture\nthat tightly integrates analog in-memory computing accelerators into multi-core\nCPUs in general-purpose systems. We developed a powerful gem5-based full\nsystem-level simulation framework into the gem5-X simulator, ALPINE, which\nenables an in-depth characterization of the proposed architecture. ALPINE\nallows the simulation of the entire computer architecture stack from major\nhardware components to their interactions with the Linux OS. Within ALPINE, we\nhave defined a custom ISA extension and a software library to facilitate the\ndeployment of inference models. We showcase and analyze a variety of mappings\nof different neural network types, and demonstrate up to 20.5x/20.8x\nperformance/energy gains with respect to a SIMD-enabled ARM CPU implementation\nfor convolutional neural networks, multi-layer perceptrons, and recurrent\nneural networks.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Computers (TC)\n",
    "authors": [
      "Joshua Klein",
      "Irem Boybat",
      "Yasir Qureshi",
      "Martino Dazzi",
      "Alexandre Levisse",
      "Giovanni Ansaloni",
      "Marina Zapater",
      "Abu Sebastian",
      "David Atienza"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.10042"
  },
  {
    "id": "arXiv:2205.10044",
    "title": "Towards biologically plausible Dreaming and Planning",
    "abstract": "Humans and animals can learn new skills after practicing for a few hours,\nwhile current reinforcement learning algorithms require a large amount of data\nto achieve good performances. Recent model-based approaches show promising\nresults by reducing the number of necessary interactions with the environment\nto learn a desirable policy. However, these methods require biological\nimplausible ingredients, such as the detailed storage of older experiences, and\nlong periods of offline learning. The optimal way to learn and exploit\nword-models is still an open question. Taking inspiration from biology, we\nsuggest that dreaming might be an efficient expedient to use an inner model. We\npropose a two-module (agent and model) neural network in which \"dreaming\"\n(living new experiences in a model-based simulated environment) significantly\nboosts learning. We also explore \"planning\", an online alternative to dreaming,\nthat shows comparable performances. Importantly, our model does not require the\ndetailed storage of experiences, and learns online the world-model. This is a\nkey ingredient for biological plausibility and implementability (e.g., in\nneuromorphic hardware). Our network is composed of spiking neurons, further\nincreasing the energetic efficiency and the plausibility of the model. To our\nknowledge, there are no previous works proposing biologically plausible\nmodel-based reinforcement learning in recurrent spiking networks. Our work is a\nstep toward building efficient neuromorphic systems for autonomous robots,\ncapable of learning new skills in real-world environments. Even when the\nenvironment is no longer accessible, the robot optimizes learning by\n\"reasoning\" in its own \"mind\". These approaches are of great relevance when the\nacquisition from the environment is slow, expensive (robotics) or unsafe\n(autonomous driving).",
    "descriptor": "",
    "authors": [
      "Cristiano Capone",
      "Pier Stanislao Paolucci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.10044"
  },
  {
    "id": "arXiv:2205.10045",
    "title": "ExMo: Explainable AI Model using Inverse Frequency Decision Rules",
    "abstract": "In this paper, we present a novel method to compute decision rules to build a\nmore accurate interpretable machine learning model, denoted as ExMo. The ExMo\ninterpretable machine learning model consists of a list of IF...THEN...\nstatements with a decision rule in the condition. This way, ExMo naturally\nprovides an explanation for a prediction using the decision rule that was\ntriggered. ExMo uses a new approach to extract decision rules from the training\ndata using term frequency-inverse document frequency (TF-IDF) features. With\nTF-IDF, decision rules with feature values that are more relevant to each class\nare extracted. Hence, the decision rules obtained by ExMo can distinguish the\npositive and negative classes better than the decision rules used in the\nexisting Bayesian Rule List (BRL) algorithm, obtained using the frequent\npattern mining approach. The paper also shows that ExMo learns a qualitatively\nbetter model than BRL. Furthermore, ExMo demonstrates that the textual\nexplanation can be provided in a human-friendly way so that the explanation can\nbe easily understood by non-expert users. We validate ExMo on several datasets\nwith different sizes to evaluate its efficacy. Experimental validation on a\nreal-world fraud detection application shows that ExMo is 20% more accurate\nthan BRL and that it achieves accuracy similar to those of deep learning\nmodels.",
    "descriptor": "\nComments: 18 pages, 7 figues, HCI International 2022 Conference\n",
    "authors": [
      "Pradip Mainali",
      "Ismini Psychoula",
      "Fabien A. P. Petitcolas"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10045"
  },
  {
    "id": "arXiv:2205.10047",
    "title": "Sigmoidally Preconditioned Off-policy Learning:a new exploration method  for reinforcement learning",
    "abstract": "One of the major difficulties of reinforcement learning is learning from {\\em\noff-policy} samples, which are collected by a different policy (behavior\npolicy) from what the algorithm evaluates (the target policy). Off-policy\nlearning needs to correct the distribution of the samples from the behavior\npolicy towards that of the target policy. Unfortunately, important sampling has\nan inherent high variance issue which leads to poor gradient estimation in\npolicy gradient methods. We focus on an off-policy Actor-Critic architecture,\nand propose a novel method, called Preconditioned Proximal Policy Optimization\n(P3O), which can control the high variance of importance sampling by applying a\npreconditioner to the Conservative Policy Iteration (CPI) objective. {\\em This\npreconditioning uses the sigmoid function in a special way that when there is\nno policy change, the gradient is maximal and hence policy gradient will drive\na big parameter update for an efficient exploration of the parameter space}.\nThis is a novel exploration method that has not been studied before given that\nexisting exploration methods are based on the novelty of states and actions. We\ncompare with several best-performing algorithms on both discrete and continuous\ntasks and the results confirmed that {\\em P3O is more off-policy than PPO}\naccording to the \"off-policyness\" measured by the DEON metric, and P3O explores\nin a larger policy space than PPO. Results also show that our P3O maximizes the\nCPI objective better than PPO during the training process.",
    "descriptor": "",
    "authors": [
      "Xing Chen",
      "Dongcui Diao",
      "Hechang Chen",
      "Hengshuai Yao",
      "Jielong Yang",
      "Haiyin Piao",
      "Zhixiao Sun",
      "Bei Jiang",
      "Yi Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10047"
  },
  {
    "id": "arXiv:2205.10049",
    "title": "Assessing Demographic Bias Transfer from Dataset to Model: A Case Study  in Facial Expression Recognition",
    "abstract": "The increasing amount of applications of Artificial Intelligence (AI) has led\nresearchers to study the social impact of these technologies and evaluate their\nfairness. Unfortunately, current fairness metrics are hard to apply in\nmulti-class multi-demographic classification problems, such as Facial\nExpression Recognition (FER). We propose a new set of metrics to approach these\nproblems. Of the three metrics proposed, two focus on the representational and\nstereotypical bias of the dataset, and the third one on the residual bias of\nthe trained model. These metrics combined can potentially be used to study and\ncompare diverse bias mitigation methods. We demonstrate the usefulness of the\nmetrics by applying them to a FER problem based on the popular Affectnet\ndataset. Like many other datasets for FER, Affectnet is a large\nInternet-sourced dataset with 291,651 labeled images. Obtaining images from the\nInternet raises some concerns over the fairness of any system trained on this\ndata and its ability to generalize properly to diverse populations. We first\nanalyze the dataset and some variants, finding substantial racial bias and\ngender stereotypes. We then extract several subsets with different demographic\nproperties and train a model on each one, observing the amount of residual bias\nin the different setups. We also provide a second analysis on a different\ndataset, FER+.",
    "descriptor": "\nComments: 8 pages excluding appendices, 8 figures\n",
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.10049"
  },
  {
    "id": "arXiv:2205.10052",
    "title": "Balancing Exploration and Exploitation for Solving Large-scale  Multiobjective Optimization via Attention Mechanism",
    "abstract": "Large-scale multiobjective optimization problems (LSMOPs) refer to\noptimization problems with multiple conflicting optimization objectives and\nhundreds or even thousands of decision variables. A key point in solving LSMOPs\nis how to balance exploration and exploitation so that the algorithm can search\nin a huge decision space efficiently. Large-scale multiobjective evolutionary\nalgorithms consider the balance between exploration and exploitation from the\nindividual's perspective. However, these algorithms ignore the significance of\ntackling this issue from the perspective of decision variables, which makes the\nalgorithm lack the ability to search from different dimensions and limits the\nperformance of the algorithm. In this paper, we propose a large-scale\nmultiobjective optimization algorithm based on the attention mechanism, called\n(LMOAM). The attention mechanism will assign a unique weight to each decision\nvariable, and LMOAM will use this weight to strike a balance between\nexploration and exploitation from the decision variable level. Nine different\nsets of LSMOP benchmarks are conducted to verify the algorithm proposed in this\npaper, and the experimental results validate the effectiveness of our design.",
    "descriptor": "\nComments: 8 pages, 9 figures, published to CEC 2022\n",
    "authors": [
      "Haokai Hong",
      "Min Jiang",
      "Liang Feng",
      "Qiuzhen Lin",
      "Kay Chen Tan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10052"
  },
  {
    "id": "arXiv:2205.10053",
    "title": "MaskGAE: Masked Graph Modeling Meets Graph Autoencoders",
    "abstract": "We present masked graph autoencoder (MaskGAE), a self-supervised learning\nframework for graph-structured data. Different from previous graph autoencoders\n(GAEs), MaskGAE adopts masked graph modeling (MGM) as a principled pretext\ntask: masking a portion of edges and attempting to reconstruct the missing part\nwith partially visible, unmasked graph structure. To understand whether MGM can\nhelp GAEs learn better representations, we provide both theoretical and\nempirical evidence to justify the benefits of this pretext task. Theoretically,\nwe establish the connections between GAEs and contrastive learning, showing\nthat MGM significantly improves the self-supervised learning scheme of GAEs.\nEmpirically, we conduct extensive experiments on a number of benchmark\ndatasets, demonstrating the superiority of MaskGAE over several\nstate-of-the-arts on both link prediction and node classification tasks. Our\ncode is publicly available at \\url{https://github.com/EdisonLeeeee/MaskGAE}.",
    "descriptor": "\nComments: Preprint. Code available at this https URL\n",
    "authors": [
      "Jintang Li",
      "Ruofan Wu",
      "Wangbin Sun",
      "Liang Chen",
      "Sheng Tian",
      "Liang Zhu",
      "Changhua Meng",
      "Zibin Zheng",
      "Weiqiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10053"
  },
  {
    "id": "arXiv:2205.10056",
    "title": "Leveraging Relational Information for Learning Weakly Disentangled  Representations",
    "abstract": "Disentanglement is a difficult property to enforce in neural representations.\nThis might be due, in part, to a formalization of the disentanglement problem\nthat focuses too heavily on separating relevant factors of variation of the\ndata in single isolated dimensions of the neural representation. We argue that\nsuch a definition might be too restrictive and not necessarily beneficial in\nterms of downstream tasks. In this work, we present an alternative view over\nlearning (weakly) disentangled representations, which leverages concepts from\nrelational learning. We identify the regions of the latent space that\ncorrespond to specific instances of generative factors, and we learn the\nrelationships among these regions in order to perform controlled changes to the\nlatent codes. We also introduce a compound generative model that implements\nsuch a weak disentanglement approach. Our experiments shows that the learned\nrepresentations can separate the relevant factors of variation in the data,\nwhile preserving the information needed for effectively generating high quality\ndata samples.",
    "descriptor": "\nComments: Accepted at WCCI2022\n",
    "authors": [
      "Andrea Valenti",
      "Davide Bacciu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10056"
  },
  {
    "id": "arXiv:2205.10059",
    "title": "Beyond the Granularity: Multi-Perspective Dialogue Collaborative  Selection for Dialogue State Tracking",
    "abstract": "In dialogue state tracking, dialogue history is a crucial material, and its\nutilization varies between different models. However, no matter how the\ndialogue history is used, each existing model uses its own consistent dialogue\nhistory during the entire state tracking process, regardless of which slot is\nupdated. Apparently, it requires different dialogue history to update different\nslots in different turns. Therefore, using consistent dialogue contents may\nlead to insufficient or redundant information for different slots, which\naffects the overall performance. To address this problem, we devise DiCoS-DST\nto dynamically select the relevant dialogue contents corresponding to each slot\nfor state updating. Specifically, it first retrieves turn-level utterances of\ndialogue history and evaluates their relevance to the slot from a combination\nof three perspectives: (1) its explicit connection to the slot name; (2) its\nrelevance to the current turn dialogue; (3) Implicit Mention Oriented\nReasoning. Then these perspectives are combined to yield a decision, and only\nthe selected dialogue contents are fed into State Generator, which explicitly\nminimizes the distracting information passed to the downstream state\nprediction. Experimental results show that our approach achieves new\nstate-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves\nsuperior performance on multiple mainstream benchmark datasets (including\nSim-M, Sim-R, and DSTC2).",
    "descriptor": "\nComments: Accepted by ACL 2022 main conference (long paper)\n",
    "authors": [
      "Jinyu Guo",
      "Kai Shuang",
      "Jijie Li",
      "Zihan Wang",
      "Yixuan Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10059"
  },
  {
    "id": "arXiv:2205.10060",
    "title": "The Unreasonable Effectiveness of Deep Evidential Regression",
    "abstract": "There is a significant need for principled uncertainty reasoning in machine\nlearning systems as they are increasingly deployed in safety-critical domains.\nA new approach with uncertainty-aware regression-based neural networks (NNs),\nbased on learning evidential distributions for aleatoric and epistemic\nuncertainties, shows promise over traditional deterministic methods and typical\nBayesian NNs, notably with the capabilities to disentangle aleatoric and\nepistemic uncertainties. Despite some empirical success of Deep Evidential\nRegression (DER), there are important gaps in the mathematical foundation that\nraise the question of why the proposed technique seemingly works. We detail the\ntheoretical shortcomings and analyze the performance on synthetic and\nreal-world data sets, showing that Deep Evidential Regression is a heuristic\nrather than an exact uncertainty quantification. We go on to propose\ncorrections and redefinitions of how aleatoric and epistemic uncertainties\nshould be extracted from NNs.",
    "descriptor": "\nComments: 14 pages, 25 figures\n",
    "authors": [
      "Nis Meinert",
      "Jakob Gawlikowski",
      "Alexander Lavin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10060"
  },
  {
    "id": "arXiv:2205.10063",
    "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision  Transformers with Locality",
    "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual\nself-supervision area by an elegant asymmetric encoder-decoder design, which\nsignificantly optimizes both the pre-training efficiency and fine-tuning\naccuracy. Notably, the success of the asymmetric structure relies on the\n\"global\" property of Vanilla Vision Transformer (ViT), whose self-attention\nmechanism reasons over arbitrary subset of discrete image patches. However, it\nis still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be\nadopted in MAE pre-training as they commonly introduce operators within \"local\"\nwindows, making it difficult to handle the random sequence of partial vision\ntokens. In this paper, we propose Uniform Masking (UM), successfully enabling\nMAE pre-training for Pyramid-based ViTs with locality (termed \"UM-MAE\" for\nshort). Specifically, UM includes a Uniform Sampling (US) that strictly samples\n$1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM)\nwhich randomly masks a portion of (usually $25\\%$) the already sampled regions\nas learnable tokens. US preserves equivalent elements across multiple\nnon-overlapped local windows, resulting in the smooth support for popular\nPyramid-based ViTs; whilst SM is designed for better transferable visual\nrepresentations since US reduces the difficulty of pixel recovery pre-task that\nhinders the semantic learning. We demonstrate that UM-MAE significantly\nimproves the pre-training efficiency (e.g., it speeds up and reduces the GPU\nmemory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive\nfine-tuning performance across downstream tasks. For example using HTC++\ndetector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only\nin ImageNet-1K can even outperform the one supervised in ImageNet-22K. The\ncodes are available at https://github.com/implus/UM-MAE.",
    "descriptor": "\nComments: An efficient and effective technique that supports MAE-style MIM Pre-training for popular Pyramid-based Vision Transformers (e.g., PVT, Swin)\n",
    "authors": [
      "Xiang Li",
      "Wenhai Wang",
      "Lingfeng Yang",
      "Jian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10063"
  },
  {
    "id": "arXiv:2205.10065",
    "title": "Approximate Dynamic Programming for Constrained Linear Systems: A  Piecewise Quadratic Approximation Approach",
    "abstract": "Approximate dynamic programming (ADP) faces challenges in dealing with\nconstraints in control problems. Model predictive control (MPC) is, in\ncomparison, well-known for its accommodation of constraints and stability\nguarantees, although its computation is sometimes prohibitive. This paper\nintroduces an approach combining the two methodologies to overcome their\nindividual limitations. The predictive control law for constrained linear\nquadratic regulation (CLQR) problems has been proven to be piecewise affine\n(PWA) while the value function is piecewise quadratic. We exploit these formal\nresults from MPC to design an ADP method for CLQR problems. A novel convex and\npiecewise quadratic neural network with a local-global architecture is proposed\nto provide an accurate approximation of the value function, which is used as\nthe cost-to-go function in the online dynamic programming problem. An efficient\ndecomposition algorithm is developed to speed up the online computation.\nRigorous stability analysis of the closed-loop system is conducted for the\nproposed control scheme under the condition that a good approximation of the\nvalue function is achieved. Comparative simulations are carried out to\ndemonstrate the potential of the proposed method in terms of online computation\nand optimality.",
    "descriptor": "",
    "authors": [
      "Kanghui He",
      "Ton van den Boom",
      "Bart De Schutter"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10065"
  },
  {
    "id": "arXiv:2205.10068",
    "title": "Understanding and Mitigating the Uncertainty in Zero-Shot Translation",
    "abstract": "Zero-shot translation is a promising direction for building a comprehensive\nmultilingual neural machine translation (MNMT) system. However, its quality is\nstill not satisfactory due to off-target issues. In this paper, we aim to\nunderstand and alleviate the off-target issues from the perspective of\nuncertainty in zero-shot translation. By carefully examining the translation\noutput and model confidence, we identify two uncertainties that are responsible\nfor the off-target issues, namely, extrinsic data uncertainty and intrinsic\nmodel uncertainty. Based on the observations, we propose two light-weight and\ncomplementary approaches to denoise the training data for model training, and\nmask out the vocabulary of the off-target languages in inference. Extensive\nexperiments on both balanced and unbalanced datasets show that our approaches\nsignificantly improve the performance of zero-shot translation over strong MNMT\nbaselines. Qualitative analyses provide insights into where our approaches\nreduce off-target translations",
    "descriptor": "\nComments: work in progress\n",
    "authors": [
      "Wenxuan Wang",
      "Wenxiang Jiao",
      "Shuo Wang",
      "Zhaopeng Tu",
      "Michael R. Lyu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10068"
  },
  {
    "id": "arXiv:2205.10070",
    "title": "On the Prediction Instability of Graph Neural Networks",
    "abstract": "Instability of trained models, i.e., the dependence of individual node\npredictions on random factors, can affect reproducibility, reliability, and\ntrust in machine learning systems. In this paper, we systematically assess the\nprediction instability of node classification with state-of-the-art Graph\nNeural Networks (GNNs). With our experiments, we establish that multiple\ninstantiations of popular GNN models trained on the same data with the same\nmodel hyperparameters result in almost identical aggregated performance but\ndisplay substantial disagreement in the predictions for individual nodes. We\nfind that up to one third of the incorrectly classified nodes differ across\nalgorithm runs. We identify correlations between hyperparameters, node\nproperties, and the size of the training set with the stability of predictions.\nIn general, maximizing model performance implicitly also reduces model\ninstability.",
    "descriptor": "\nComments: 17 pages, 11 figures\n",
    "authors": [
      "Max Klabunde",
      "Florian Lemmerich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10070"
  },
  {
    "id": "arXiv:2205.10071",
    "title": "Contrastive Learning with Cross-Modal Knowledge Mining for Multimodal  Human Activity Recognition",
    "abstract": "Human Activity Recognition is a field of research where input data can take\nmany forms. Each of the possible input modalities describes human behaviour in\na different way, and each has its own strengths and weaknesses. We explore the\nhypothesis that leveraging multiple modalities can lead to better recognition.\nSince manual annotation of input data is expensive and time-consuming, the\nemphasis is made on self-supervised methods which can learn useful feature\nrepresentations without any ground truth labels. We extend a number of recent\ncontrastive self-supervised approaches for the task of Human Activity\nRecognition, leveraging inertial and skeleton data. Furthermore, we propose a\nflexible, general-purpose framework for performing multimodal self-supervised\nlearning, named Contrastive Multiview Coding with Cross-Modal Knowledge Mining\n(CMC-CMKM). This framework exploits modality-specific knowledge in order to\nmitigate the limitations of typical self-supervised frameworks. The extensive\nexperiments on two widely-used datasets demonstrate that the suggested\nframework significantly outperforms contrastive unimodal and multimodal\nbaselines on different scenarios, including fully-supervised fine-tuning,\nactivity retrieval and semi-supervised learning. Furthermore, it shows\nperformance competitive even compared to supervised methods.",
    "descriptor": "\nComments: to be published in IEEE WCCI 2022 (IJCNN 2022 track)\n",
    "authors": [
      "Razvan Brinzea",
      "Bulat Khaertdinov",
      "Stylianos Asteriadis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10071"
  },
  {
    "id": "arXiv:2205.10072",
    "title": "(Poly)Logarithmic Time Construction of Round-optimal $n$-Block Broadcast  Schedules for Broadcast and irregular Allgather in MPI",
    "abstract": "We give a fast(er), communication-free, parallel construction of optimal\ncommunication schedules that allow broadcasting of $n$ distinct blocks of data\nfrom a root processor to all other processors in $1$-ported, $p$-processor\nnetworks with fully bidirectional communication. For any $p$ and $n$,\nbroadcasting in this model requires $n-1+\\lceil\\log_2 p\\rceil$ communication\nrounds. In contrast to other constructions, all processors follow the same,\ncirculant graph communication pattern, which makes it possible to use the\nschedules for the allgather (all-to-all-broadcast) operation as well. The new\nconstruction takes $O(\\log^3 p)$ time steps per processor, each of which can\ncompute its part of the schedule independently of the other processors in\n$O(\\log p)$ space. The result is a significant improvement over the sequential\n$O(p \\log^2 p)$ time and $O(p\\log p)$ space construction of Tr\\\"aff and Ripke\n(2009) with considerable practical import. The round-optimal schedule\nconstruction is then used to implement communication optimal algorithms the\nbroadcast and (irregular) allgather collective operations as found in MPI (the\n\\emph{Message-Passing Interface}), and significantly and practically improve\nover the implementations in standard MPI libraries (\\texttt{mpich}, OpenMPI,\nIntel MPI) for certain problem ranges. The application to the irregular\nallgather operation is entirely new.",
    "descriptor": "",
    "authors": [
      "Jesper Larsson Tr\u00e4ff"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10072"
  },
  {
    "id": "arXiv:2205.10075",
    "title": "Decentralized Autonomous Organizations for Tax Credit's Tracking",
    "abstract": "Tax credit stimulus and fiscal bonuses had a very important impact on Italian\neconomy in the last decade. Along with a huge expansion in constructions a\nrelevant increase in scams and frauds has come too. The aim of this article is\nto design a possible system to track and control the whole tax credit process\nfrom its generation to its redeem through a Decentralized Autonomous\nOrganization architecture enriched with a Multi Agent Systems to implement\ncontrollers.",
    "descriptor": "",
    "authors": [
      "Giovanni De Gasperis",
      "Sante Dino Facchini",
      "Alessio Susco"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.10075"
  },
  {
    "id": "arXiv:2205.10077",
    "title": "Error Probability Bounds for Coded-Index DNA Storage",
    "abstract": "The DNA storage channel is considered, in which a codeword is comprised of\n$M$ unordered DNA molecules. At reading time, $N$ molecules are sampled with\nreplacement, and then each molecule is sequenced. A coded-index\nconcatenated-coding scheme is considered, in which the $m$th molecule of the\ncodeword is restricted to a subset of all possible molecules (an inner code),\nwhich is unique for each $m$. The decoder has low-complexity, and is based on\nfirst decoding each molecule separately (the inner code), and then decoding the\nsequence of molecules (an outer code). Only mild assumptions are made on the\nsequencing channel, in the form of the existence of an inner code and decoder\nwith vanishing error. The error probability of a random code as well as an\nexpurgated code is analyzed and shown to decay exponentially with $N$. This\nestablishes the importance of increasing the coverage depth $N/M$ in order to\nobtain low error probability.",
    "descriptor": "",
    "authors": [
      "Nir Weinberger"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10077"
  },
  {
    "id": "arXiv:2205.10078",
    "title": "Uzbek affix finite state machine for stemming",
    "abstract": "This work presents a morphological analyzer for the Uzbek language using a\nfinite state machine. The proposed methodology is a morphologic analysis of\nUzbek words by using an affix striping to find a root and without including any\nlexicon. This method helps to perform morphological analysis of words from a\nlarge amount of text at high speed as well as it is not required using of\nmemory for keeping vocabulary. According to Uzbek, an agglutinative language\ncan be designed with finite state machines (FSMs). In contrast to the previous\nworks, this study modeled the completed FSMs for all word classes by using the\nUzbek language's morphotactic rules in right to left order. This paper shows\nthe stages of this methodology including the classification of the affixes, the\ngeneration of the FSMs for each affix class, and the combination into a head\nmachine to make analysis a word.",
    "descriptor": "\nComments: Accepted for publication in the IX International Conference on Computer Processing of Turkic Languages \"TurkLang 2021\", 15 pages, 12 figures\n",
    "authors": [
      "Maksud Sharipov",
      "Ulugbek Salaev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10078"
  },
  {
    "id": "arXiv:2205.10079",
    "title": "Unintended memorisation of unique features in neural networks",
    "abstract": "Neural networks pose a privacy risk due to their propensity to memorise and\nleak training data. We show that unique features occurring only once in\ntraining data are memorised by discriminative multi-layer perceptrons and\nconvolutional neural networks trained on benchmark imaging datasets. We design\nour method for settings where sensitive training data is not available, for\nexample medical imaging. Our setting knows the unique feature, but not the\ntraining data, model weights or the unique feature's label. We develop a score\nestimating a model's sensitivity to a unique feature by comparing the KL\ndivergences of the model's output distributions given modified\nout-of-distribution images. We find that typical strategies to prevent\noverfitting do not prevent unique feature memorisation. And that images\ncontaining a unique feature are highly influential, regardless of the influence\nthe images's other features. We also find a significant variation in\nmemorisation with training seed. These results imply that neural networks pose\na privacy risk to rarely occurring private information. This risk is more\npronounced in healthcare applications since sensitive patient information can\nbe memorised when it remains in training data due to an imperfect data\nsanitisation process.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2202.08099\n",
    "authors": [
      "John Hartley",
      "Sotirios A. Tsaftaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10079"
  },
  {
    "id": "arXiv:2205.10080",
    "title": "An efficient explicit jump HOC immersed interface approach for transient  incompressible viscous flows",
    "abstract": "In the present work, we propose a novel hybrid explicit jump immersed\ninterface approach in conjunction with a higher order compact (HOC) scheme for\nsimulating transient complex flows governed by the streamfunction-vorticity\n($\\psi$-$\\zeta$) formulation of the Navier-Stokes (N-S) equations for\nincompressible viscous flows. A new strategy has been adopted for the jump\nconditions at the irregular points across the interface using Lagrangian\ninterpolation on a Cartesian grid. This approach, which starts with the\ndiscretization of parabolic equations with discontinuities in the solutions,\nsource terms and the coefficients across the interface, can easily be\naccommodated into simulating flow past bluff bodies immersed in the flow. The\nsuperiority of the approach is reflected by the reduced magnitude and faster\ndecay of the errors in comparison to other existing methods. It is seen to\nhandle several fluid flow problems having practical implications in the real\nworld very efficiently, which involves flows involving multiple and moving\nbodies. This includes the flow past a stationary circular and a twenty-four\nedge cactus cylinder, flows past two tandem cylinders, where in one situation\nboth are fixed and in another, one of them is oscillating transversely with\nvariable amplitude in time. To the best of our knowledge, the last two examples\nhave been tackled for the first time by such an approach employing the\n$\\psi$-$\\zeta$ formulation in finite difference set-up. The extreme closeness\nof our computed solutions with the existing numerical and experimental results\nexemplifies the accuracy and the robustness of the proposed approach.",
    "descriptor": "",
    "authors": [
      "Raghav Singhal",
      "Jiten C Kalita"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10080"
  },
  {
    "id": "arXiv:2205.10081",
    "title": "Emergence of Double-slit Interference by Representing Visual Space in  Artificial Neural Networks",
    "abstract": "Artificial neural networks have realized incredible successes at image\nrecognition, but the underlying mechanism of visual space representation\nremains a huge mystery. Grid cells (2014 Nobel Prize) in the entorhinal cortex\nsupport a periodic representation as a metric for coding space. Here, we\ndevelop a self-supervised convolutional neural network to perform visual space\nlocation, leading to the emergence of single-slit diffraction and double-slit\ninterference patterns of waves. Our discoveries reveal the nature of CNN\nencoding visual space to a certain extent. CNN is no longer a black box in\nterms of visual spatial encoding, it is interpretable. Our findings indicate\nthat the periodicity property of waves provides a space metric, suggesting a\ngeneral role of spatial coordinate frame in artificial neural networks.",
    "descriptor": "",
    "authors": [
      "Xiuxiu Bai",
      "Zhe Liu",
      "Yao Gao",
      "Bin Liu",
      "Yongqiang Hao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10081"
  },
  {
    "id": "arXiv:2205.10083",
    "title": "A Unified Experiment Design Approach for Cyclic and Acyclic Causal  Models",
    "abstract": "We study experiment design for the unique identification of the causal graph\nof a system where the graph may contain cycles. The presence of cycles in the\nstructure introduces major challenges for experiment design. Unlike the case of\nacyclic graphs, learning the skeleton of the causal graph from observational\ndistribution may not be possible. Furthermore, intervening on a variable does\nnot necessarily lead to orienting all the edges incident to it. In this paper,\nwe propose an experiment design approach that can learn both cyclic and acyclic\ngraphs and hence, unifies the task of experiment design for both types of\ngraphs. We provide a lower bound on the number of experiments required to\nguarantee the unique identification of the causal graph in the worst case,\nshowing that the proposed approach is order-optimal in terms of the number of\nexperiments up to an additive logarithmic term. Moreover, we extend our result\nto the setting where the size of each experiment is bounded by a constant. For\nthis case, we show that our approach is optimal in terms of the size of the\nlargest experiment required for the unique identification of the causal graph\nin the worst case.",
    "descriptor": "\nComments: 16 pages, 3 figures\n",
    "authors": [
      "Ehsan Mokhtarian",
      "Saber Salehkaleybar",
      "AmirEmad Ghassami",
      "Negar Kiyavash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10083"
  },
  {
    "id": "arXiv:2205.10086",
    "title": "People Tracking and Re-Identifying in Distributed Contexts: Extension of  PoseTReID",
    "abstract": "In our previous paper, we introduced PoseTReID which is a generic framework\nfor real-time 2D multi-person tracking in distributed interaction spaces where\nlong-term people's identities are important for other studies such as behavior\nanalysis, etc. In this paper, we introduce a further study of PoseTReID\nframework in order to give a more complete comprehension of the framework. We\nuse a well-known bounding box detector YOLO (v4) for the detection to compare\nto OpenPose which was used in our last paper, and we use SORT and DeepSORT to\ncompare to centroid which was also used previously, and most importantly for\nthe re-identification, we use a bunch of deep leaning methods such as MLFN,\nOSNet, and OSNet-AIN with our custom classification layer to compare to FaceNet\nwhich was also used earlier in our last paper. By evaluating on our PoseTReID\ndatasets, even though those deep learning re-identification methods are\ndesigned for only short-term re-identification across multiple cameras or\nvideos, it is worth showing that they give impressive results which boost the\noverall tracking performance of PoseTReID framework regardless the type of\ntracking method. At the same time, we also introduce our research-friendly and\nopen source Python toolbox pyppbox, which is pure written in Python and\ncontains all sub-modules which are used this study along with real-time online\nand offline evaluations for our PoseTReID datasets. This pyppbox is available\non GitHub https://github.com/rathaumons/pyppbox .",
    "descriptor": "\nComments: 5 pages, 5 figures, 3 tables, To be submitted to MDMA2022\n",
    "authors": [
      "Ratha Siv",
      "Matei Mancas",
      "Bernard Gosselin",
      "Dona Valy",
      "Sokchenda Sreng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10086"
  },
  {
    "id": "arXiv:2205.10088",
    "title": "Semi-self-supervised Automated ICD Coding",
    "abstract": "Clinical Text Notes (CTNs) contain physicians' reasoning process, written in\nan unstructured free text format, as they examine and interview patients. In\nrecent years, several studies have been published that provide evidence for the\nutility of machine learning for predicting doctors' diagnoses from CTNs, a task\nknown as ICD coding. Data annotation is time consuming, particularly when a\ndegree of specialization is needed, as is the case for medical data. This paper\npresents a method of augmenting a sparsely annotated dataset of Icelandic CTNs\nwith a machine-learned imputation in a semi-self-supervised manner. We train a\nneural network on a small set of annotated CTNs and use it to extract clinical\nfeatures from a set of un-annotated CTNs. These clinical features consist of\nanswers to about a thousand potential questions that a physician might find the\nanswers to during a consultation of a patient. The features are then used to\ntrain a classifier for the diagnosis of certain types of diseases. We report\nthe results of an evaluation of this data augmentation method over three tiers\nof data availability to the physician. Our data augmentation method shows a\nsignificant positive effect which is diminished when clinical features from the\nexamination of the patient and diagnostics are made available. We recommend our\nmethod for augmenting scarce datasets for systems that take decisions based on\nclinical features that do not include examinations or tests.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Hlynur D. Hlynsson",
      "Steind\u00f3r Ellertsson",
      "J\u00f3n F. Da\u00f0ason",
      "Emil L. Sigurdsson",
      "Hrafn Loftsson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10088"
  },
  {
    "id": "arXiv:2205.10089",
    "title": "Kernel Normalized Convolutional Networks",
    "abstract": "Existing deep convolutional neural network (CNN) architectures frequently\nrely upon batch normalization (BatchNorm) to effectively train the model.\nBatchNorm significantly improves model performance, but performs poorly with\nsmaller batch sizes. To address this limitation, we propose kernel\nnormalization and kernel normalized convolutional layers, and incorporate them\ninto kernel normalized convolutional networks (KNConvNets) as the main building\nblocks. We implement KNConvNets corresponding to the state-of-the-art CNNs such\nas ResNet and DenseNet while forgoing BatchNorm layers. Through extensive\nexperiments, we illustrate that KNConvNets consistently outperform their batch,\ngroup, and layer normalized counterparts in terms of both accuracy and\nconvergence rate while maintaining competitive computational efficiency.",
    "descriptor": "",
    "authors": [
      "Reza Nasirigerdeh",
      "Reihaneh Torkzadehmahani",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10089"
  },
  {
    "id": "arXiv:2205.10092",
    "title": "An efficient Deep Spatio-Temporal Context Aware decision Network  (DST-CAN) for Predictive Manoeuvre Planning",
    "abstract": "To ensure the safety and efficiency of its maneuvers, an Autonomous Vehicle\n(AV) should anticipate the future intentions of surrounding vehicles using its\nsensor information. If an AV can predict its surrounding vehicles' future\ntrajectories, it can make safe and efficient manoeuvre decisions. In this\npaper, we present such a Deep Spatio-Temporal Context-Aware decision Network\n(DST-CAN) model for predictive manoeuvre planning of AVs. A memory neuron\nnetwork is used to predict future trajectories of its surrounding vehicles. The\ndriving environment's spatio-temporal information (past, present, and predicted\nfuture trajectories) are embedded into a context-aware grid. The proposed\nDST-CAN model employs these context-aware grids as inputs to a convolutional\nneural network to understand the spatial relationships between the vehicles and\ndetermine a safe and efficient manoeuvre decision. The DST-CAN model also uses\ninformation of human driving behavior on a highway. Performance evaluation of\nDST-CAN has been carried out using two publicly available NGSIM US-101 and I-80\ndatasets. Also, rule-based ground truth decisions have been compared with those\ngenerated by DST-CAN. The results clearly show that DST-CAN can make much\nbetter decisions with 3-sec of predicted trajectories of neighboring vehicles\ncompared to currently existing methods that do not use this prediction.",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Jayabrata Chowdhury",
      "Suresh Sundaram",
      "Nishant Rao",
      "Narasimhan Sundararajan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10092"
  },
  {
    "id": "arXiv:2205.10093",
    "title": "Visual Concepts Tokenization",
    "abstract": "Obtaining the human-like perception ability of abstracting visual concepts\nfrom concrete pixels has always been a fundamental and important target in\nmachine learning research fields such as disentangled representation learning\nand scene decomposition. Towards this goal, we propose an unsupervised\ntransformer-based Visual Concepts Tokenization framework, dubbed VCT, to\nperceive an image into a set of disentangled visual concept tokens, with each\nconcept token responding to one type of independent visual concept.\nParticularly, to obtain these concept tokens, we only use cross-attention to\nextract visual information from the image tokens layer by layer without\nself-attention between concept tokens, preventing information leakage across\nconcept tokens. We further propose a Concept Disentangling Loss to facilitate\nthat different concept tokens represent independent visual concepts. The\ncross-attention and disentangling loss play the role of induction and mutual\nexclusion for the concept tokens, respectively. Extensive experiments on\nseveral popular datasets verify the effectiveness of VCT on the tasks of\ndisentangled representation learning and scene decomposition. VCT achieves the\nstate of the art results by a large margin.",
    "descriptor": "\nComments: Preprint, under review\n",
    "authors": [
      "Tao Yang",
      "Yuwang Wang",
      "Yan Lu",
      "Nanning Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10093"
  },
  {
    "id": "arXiv:2205.10095",
    "title": "How to keep text private? A systematic review of deep learning methods  for privacy-preserving natural language processing",
    "abstract": "Deep learning (DL) models for natural language processing (NLP) tasks often\nhandle private data, demanding protection against breaches and disclosures.\nData protection laws, such as the European Union's General Data Protection\nRegulation (GDPR), thereby enforce the need for privacy. Although many\nprivacy-preserving NLP methods have been proposed in recent years, no\ncategories to organize them have been introduced yet, making it hard to follow\nthe progress of the literature. To close this gap, this article systematically\nreviews over sixty DL methods for privacy-preserving NLP published between 2016\nand 2020, covering theoretical foundations, privacy-enhancing technologies, and\nanalysis of their suitability for real-world scenarios. First, we introduce a\nnovel taxonomy for classifying the existing methods into three categories: data\nsafeguarding methods, trusted methods, and verification methods. Second, we\npresent an extensive summary of privacy threats, datasets for applications, and\nmetrics for privacy evaluation. Third, throughout the review, we describe\nprivacy issues in the NLP pipeline in a holistic view. Further, we discuss open\nchallenges in privacy-preserving NLP regarding data traceability, computation\noverhead, dataset size, the prevalence of human biases in embeddings, and the\nprivacy-utility tradeoff. Finally, this review presents future research\ndirections to guide successive research and development of privacy-preserving\nNLP models.",
    "descriptor": "\nComments: 59 pages, 15 figures\n",
    "authors": [
      "Samuel Sousa",
      "Roman Kern"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10095"
  },
  {
    "id": "arXiv:2205.10098",
    "title": "Adversarial joint attacks on legged robots",
    "abstract": "We address adversarial attacks on the actuators at the joints of legged\nrobots trained by deep reinforcement learning. The vulnerability to the joint\nattacks can significantly impact the safety and robustness of legged robots. In\nthis study, we demonstrate that the adversarial perturbations to the torque\ncontrol signals of the actuators can significantly reduce the rewards and cause\nwalking instability in robots. To find the adversarial torque perturbations, we\ndevelop black-box adversarial attacks, where, the adversary cannot access the\nneural networks trained by deep reinforcement learning. The black box attack\ncan be applied to legged robots regardless of the architecture and algorithms\nof deep reinforcement learning. We employ three search methods for the\nblack-box adversarial attacks: random search, differential evolution, and\nnumerical gradient descent methods. In experiments with the quadruped robot\nAnt-v2 and the bipedal robot Humanoid-v2, in OpenAI Gym environments, we find\nthat differential evolution can efficiently find the strongest torque\nperturbations among the three methods. In addition, we realize that the\nquadruped robot Ant-v2 is vulnerable to the adversarial perturbations, whereas\nthe bipedal robot Humanoid-v2 is robust to the perturbations. Consequently, the\njoint attacks can be used for proactive diagnosis of robot walking instability.",
    "descriptor": "\nComments: 6 pages, 8 figures\n",
    "authors": [
      "Takuto Otomo",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10098"
  },
  {
    "id": "arXiv:2205.10101",
    "title": "MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer  with Multi-Stage Fusion",
    "abstract": "Measuring the perceptual quality of images automatically is an essential task\nin the area of computer vision, as degradations on image quality can exist in\nmany processes from image acquisition, transmission to enhancing. Many Image\nQuality Assessment(IQA) algorithms have been designed to tackle this problem.\nHowever, it still remains un settled due to the various types of image\ndistortions and the lack of large-scale human-rated datasets. In this paper, we\npropose a novel algorithm based on the Swin Transformer [31] with fused\nfeatures from multiple stages, which aggregates information from both local and\nglobal features to better predict the quality. To address the issues of\nsmall-scale datasets, relative rankings of images have been taken into account\ntogether with regression loss to simultaneously optimize the model.\nFurthermore, effective data augmentation strategies are also used to improve\nthe performance. In comparisons with previous works, experiments are carried\nout on two standard IQA datasets and a challenge dataset. The results\ndemonstrate the effectiveness of our work. The proposed method outperforms\nother methods on standard datasets and ranks 2nd in the no-reference track of\nNTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that\nour method is promising in solving diverse IQA problems and thus can be used to\nreal-word applications.",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Jing Wang",
      "Haotian Fa",
      "Xiaoxia Hou",
      "Yitian Xu",
      "Tao Li",
      "Xuechao Lu",
      "Lean Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10101"
  },
  {
    "id": "arXiv:2205.10102",
    "title": "Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral  Compressive Imaging",
    "abstract": "In coded aperture snapshot spectral compressive imaging (CASSI) systems,\nhyperspectral image (HSI) reconstruction methods are employed to recover the\nspatial-spectral signal from a compressed measurement. Among these algorithms,\ndeep unfolding methods demonstrate promising performance but suffer from two\nissues. Firstly, they do not estimate the degradation patterns and\nill-posedness degree from the highly related CASSI to guide the iterative\nlearning. Secondly, they are mainly CNN-based, showing limitations in capturing\nlong-range dependencies. In this paper, we propose a principled\nDegradation-Aware Unfolding Framework (DAUF) that estimates parameters from the\ncompressed image and physical mask, and then uses these parameters to control\neach iteration. Moreover, we customize a novel Half-Shuffle Transformer (HST)\nthat simultaneously captures local contents and non-local dependencies. By\nplugging HST into DAUF, we establish the first Transformer-based deep unfolding\nmethod, Degradation-Aware Unfolding Half-Shuffle Transformer (DAUHST), for HSI\nreconstruction. Experiments show that DAUHST significantly surpasses\nstate-of-the-art methods while requiring cheaper computational and memory\ncosts. Code and models will be released to the public.",
    "descriptor": "\nComments: The first Transformer-based deep unfolding method for spectral compressive imaging\n",
    "authors": [
      "Yuanhao Cai",
      "Jing Lin",
      "Haoqian Wang",
      "Xin Yuan",
      "Henghui Ding",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10102"
  },
  {
    "id": "arXiv:2205.10105",
    "title": "Parameterized Complexity of Weighted Multicut in Trees",
    "abstract": "The Edge Multicut problem is a classical cut problem where given an\nundirected graph $G$, a set of pairs of vertices $\\mathcal{P}$, and a budget\n$k$, the goal is to determine if there is a set $S$ of at most $k$ edges such\nthat for each $(s,t) \\in \\mathcal{P}$, $G-S$ has no path from $s$ to $t$. Edge\nMulticut has been relatively recently shown to be fixed-parameter tractable\n(FPT), parameterized by $k$, by Marx and Razgon [SICOMP 2014], and\nindependently by Bousquet et al. [SICOMP 2018]. In the weighted version of the\nproblem, called Weighted Edge Multicut one is additionally given a weight\nfunction $\\mathtt{wt} : E(G) \\to \\mathbb{N}$ and a weight bound $w$, and the\ngoal is to determine if there is a solution of size at most $k$ and weight at\nmost $w$. Both the FPT algorithms for Edge Multicut by Marx et al. and Bousquet\net al. fail to generalize to the weighted setting. In fact, the weighted\nproblem is non-trivial even on trees and determining whether Weighted Edge\nMulticut on trees is FPT was explicitly posed as an open problem by Bousquet et\nal. [STACS 2009]. In this article, we answer this question positively by\ndesigning an algorithm which uses a very recent result by Kim et al. [STOC\n2022] about directed flow augmentation as subroutine.\nWe also study a variant of this problem where there is no bound on the size\nof the solution, but the parameter is a structural property of the input, for\nexample, the number of leaves of the tree. We strengthen our results by stating\nthem for the more general vertex deletion version.",
    "descriptor": "\nComments: Full version of the paper accepted for WG 2022\n",
    "authors": [
      "Esther Galby",
      "D\u00e1niel Marx",
      "Philipp Schepper",
      "Roohani Sharma",
      "Prafullkumar Tale"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.10105"
  },
  {
    "id": "arXiv:2205.10106",
    "title": "LeNSE: Learning To Navigate Subgraph Embeddings for Large-Scale  Combinatorial Optimisation",
    "abstract": "Combinatorial Optimisation problems arise in several application domains and\nare often formulated in terms of graphs. Many of these problems are NP-hard,\nbut exact solutions are not always needed. Several heuristics have been\ndeveloped to provide near-optimal solutions; however, they do not typically\nscale well with the size of the graph. We propose a low-complexity approach for\nidentifying a (possibly much smaller) subgraph of the original graph where the\nheuristics can be run in reasonable time and with a high likelihood of finding\na global near-optimal solution. The core component of our approach is LeNSE, a\nreinforcement learning algorithm that learns how to navigate the space of\npossible subgraphs using an Euclidean subgraph embedding as its map. To solve\nCO problems, LeNSE is provided with a discriminative embedding trained using\nany existing heuristics using only on a small portion of the original graph.\nWhen tested on three problems (vertex cover, max-cut and influence\nmaximisation) using real graphs with up to $10$ million edges, LeNSE identifies\nsmall subgraphs yielding solutions comparable to those found by running the\nheuristics on the entire graph, but at a fraction of the total run time.",
    "descriptor": "\nComments: To appear in ICML 2022\n",
    "authors": [
      "David Ireland",
      "Giovanni Montana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10106"
  },
  {
    "id": "arXiv:2205.10110",
    "title": "FedNoiL: A Simple Two-Level Sampling Method for Federated Learning with  Noisy Labels",
    "abstract": "Federated learning (FL) aims at training a global model on the server side\nwhile the training data are collected and located at the local devices. Hence,\nthe labels in practice are usually annotated by clients of varying expertise or\ncriteria and thus contain different amounts of noises. Local training on noisy\nlabels can easily result in overfitting to noisy labels, which is devastating\nto the global model through aggregation. Although recent robust FL methods take\nmalicious clients into account, they have not addressed local noisy labels on\neach device and the impact to the global model. In this paper, we develop a\nsimple two-level sampling method \"FedNoiL\" that (1) selects clients for more\nrobust global aggregation on the server; and (2) selects clean labels and\ncorrect pseudo-labels at the client end for more robust local training. The\nsampling probabilities are built upon clean label detection by the global\nmodel. Moreover, we investigate different schedules changing the local epochs\nbetween aggregations over the course of FL, which notably improves the\ncommunication and computation efficiency in noisy label setting. In experiments\nwith homogeneous/heterogeneous data distributions and noise ratios, we observed\nthat direct combinations of SOTA FL methods with SOTA noisy-label learning\nmethods can easily fail but our method consistently achieves better and robust\nperformance.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Zhuowei Wang",
      "Tianyi Zhou",
      "Guodong Long",
      "Bo Han",
      "Jing Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10110"
  },
  {
    "id": "arXiv:2205.10113",
    "title": "Evolutionary Multi-Armed Bandits with Genetic Thompson Sampling",
    "abstract": "As two popular schools of machine learning, online learning and evolutionary\ncomputations have become two important driving forces behind real-world\ndecision making engines for applications in biomedicine, economics, and\nengineering fields. Although there are prior work that utilizes bandits to\nimprove evolutionary algorithms' optimization process, it remains a field of\nblank on how evolutionary approach can help improve the sequential decision\nmaking tasks of online learning agents such as the multi-armed bandits. In this\nwork, we propose the Genetic Thompson Sampling, a bandit algorithm that keeps a\npopulation of agents and update them with genetic principles such as elite\nselection, crossover and mutations. Empirical results in multi-armed bandit\nsimulation environments and a practical epidemic control problem suggest that\nby incorporating the genetic algorithm into the bandit algorithm, our method\nsignificantly outperforms the baselines in nonstationary settings. Lastly, we\nintroduce EvoBandit, a web-based interactive visualization to guide the readers\nthrough the entire learning process and perform lightweight evaluations on the\nfly. We hope to engage researchers into this growing field of research with\nthis investigation.",
    "descriptor": "\nComments: Proceeding of IEEE CEC 2022. This work is one of the first works to solve the online learning problems with distributed evolutionary optimizations, and extends our prior work on contextual bandits (e.g. arXiv:2106.15808) by testing against similar simulated and real-world scenarios. Codes at this https URL\n",
    "authors": [
      "Baihan Lin"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.10113"
  },
  {
    "id": "arXiv:2205.10115",
    "title": "Testing predictive automated driving systems: lessons learned and future  recommendations",
    "abstract": "Conventional vehicles are certified through classical approaches, where\ndifferent physical certification tests are set up on test tracks to assess\nrequired safety levels. These approaches are well suited for vehicles with\nlimited complexity and limited interactions with other entities as last-second\nresources. However, these approaches do not allow to evaluate safety with real\nbehaviors for critical and edge cases, nor to evaluate the ability to\nanticipate them in the mid or long term. This is particularly relevant for\nautomated and autonomous driving functions that make use of advanced predictive\nsystems to anticipate future actions and motions to be considered in the path\nplanning layer. In this paper, we present and analyze the results of physical\ntests on proving grounds of several predictive systems in automated driving\nfunctions developed within the framework of the BRAVE project. Based on our\nexperience in testing predictive automated driving functions, we identify the\nmain limitations of current physical testing approaches when dealing with\npredictive systems, analyze the main challenges ahead, and provide a set of\npractical actions and recommendations to consider in future physical testing\nprocedures for automated and autonomous driving functions.",
    "descriptor": "\nComments: This work has been accepted to the IEEE Intelligent Transportation Systems Magazine for publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Rub\u00e9n Izquierdo Gonzalo",
      "Carlota Salinas Maldonado",
      "Javier Alonso Ruiz",
      "Ignacio Parra Alonso",
      "David Fern\u00e1ndez Llorca",
      "Miguel \u00c1. Sotelo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10115"
  },
  {
    "id": "arXiv:2205.10116",
    "title": "Evolving SimGANs to Improve Abnormal Electrocardiogram Classification",
    "abstract": "Machine Learning models are used in a wide variety of domains. However,\nmachine learning methods often require a large amount of data in order to be\nsuccessful. This is especially troublesome in domains where collecting\nreal-world data is difficult and/or expensive. Data simulators do exist for\nmany of these domains, but they do not sufficiently reflect the real world data\ndue to factors such as a lack of real-world noise. Recently generative\nadversarial networks (GANs) have been modified to refine simulated image data\ninto data that better fits the real world distribution, using the SimGAN\nmethod. While evolutionary computing has been used for GAN evolution, there are\ncurrently no frameworks that can evolve a SimGAN. In this paper we (1) extend\nthe SimGAN method to refine one-dimensional data, (2) modify Easy Cartesian\nGenetic Programming (ezCGP), an evolutionary computing framework, to create\nSimGANs that more accurately refine simulated data, and (3) create new\nfeature-based quantitative metrics to evaluate refined data. We also use our\nframework to augment an electrocardiogram (ECG) dataset, a domain that suffers\nfrom the issues previously mentioned. In particular, while healthy ECGs can be\nsimulated there are no current simulators of abnormal ECGs. We show that by\nusing an evolved SimGAN to refine simulated healthy ECG data to mimic\nreal-world abnormal ECGs, we can improve the accuracy of abnormal ECG\nclassifiers.",
    "descriptor": "\nComments: 8 pages, presented at The Genetic and Evolutionary Computation Conference 2022\n",
    "authors": [
      "Gabriel Wang",
      "Anish Thite",
      "Rodd Talebi",
      "Anthony D'Achille",
      "Alex Mussa",
      "Jason Zutty"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10116"
  },
  {
    "id": "arXiv:2205.10117",
    "title": "DDDM: a Brain-Inspired Framework for Robust Classification",
    "abstract": "Despite their outstanding performance in a broad spectrum of real-world\ntasks, deep artificial neural networks are sensitive to input noises,\nparticularly adversarial perturbations. On the contrary, human and animal\nbrains are much less vulnerable. In contrast to the one-shot inference\nperformed by most deep neural networks, the brain often solves decision-making\nwith an evidence accumulation mechanism that may trade time for accuracy when\nfacing noisy inputs. The mechanism is well described by the Drift-Diffusion\nModel (DDM). In the DDM, decision-making is modeled as a process in which noisy\nevidence is accumulated toward a threshold. Drawing inspiration from the DDM,\nwe propose the Dropout-based Drift-Diffusion Model (DDDM) that combines\ntest-phase dropout and the DDM for improving the robustness for arbitrary\nneural networks. The dropouts create temporally uncorrelated noises in the\nnetwork that counter perturbations, while the evidence accumulation mechanism\nguarantees a reasonable decision accuracy. Neural networks enhanced with the\nDDDM tested in image, speech, and text classification tasks all significantly\noutperform their native counterparts, demonstrating the DDDM as a task-agnostic\ndefense against adversarial attacks.",
    "descriptor": "\nComments: submitted to IJCAI2022\n",
    "authors": [
      "Xiyuan Chen",
      "Xingyu Li",
      "Yi Zhou",
      "Tianming Yang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10117"
  },
  {
    "id": "arXiv:2205.10118",
    "title": "An Artificial Neural Network Functionalized by Evolution",
    "abstract": "The topology of artificial neural networks has a significant effect on their\nperformance. Characterizing efficient topology is a field of promising research\nin Artificial Intelligence. However, it is not a trivial task and it is mainly\nexperimented on through convolutional neural networks. We propose a hybrid\nmodel which combines the tensor calculus of feed-forward neural networks with\nPseudo-Darwinian mechanisms. This allows for finding topologies that are well\nadapted for elaboration of strategies, control problems or pattern recognition\ntasks. In particular, the model can provide adapted topologies at early\nevolutionary stages, and 'structural convergence', which can found applications\nin robotics, big-data and artificial life.",
    "descriptor": "",
    "authors": [
      "Fabien Furfaro",
      "Avner Bar-Hen",
      "Geoffroy Berthelot"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10118"
  },
  {
    "id": "arXiv:2205.10119",
    "title": "Is explainable AI a race against model complexity?",
    "abstract": "Explaining the behaviour of intelligent systems will get increasingly and\nperhaps intractably challenging as models grow in size and complexity. We may\nnot be able to expect an explanation for every prediction made by a brain-scale\nmodel, nor can we expect explanations to remain objective or apolitical. Our\nfunctionalist understanding of these models is of less advantage than we might\nassume. Models precede explanations, and can be useful even when both model and\nexplanation are incorrect. Explainability may never win the race against\ncomplexity, but this is less problematic than it seems.",
    "descriptor": "\nComments: Workshop on Transparency and Explanations in Smart Systems (TExSS 2022), at the 27th International Conference on Intelligent User Interfaces (IUI 2022)\n",
    "authors": [
      "Advait Sarkar"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10119"
  },
  {
    "id": "arXiv:2205.10120",
    "title": "Privacy Preserving Image Registration",
    "abstract": "Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\nliterature on image registration is generally based on the assumption that\nimages are usually accessible to the researcher, from which the spatial\ntransformation is subsequently estimated. This common assumption may not be met\nin current practical applications, since the sensitive nature of medical images\nmay ultimately require their analysis under privacy constraints, preventing to\nshare the image content in clear form. In this work, we formulate the problem\nof image registration under a privacy preserving regime, where images are\nassumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we first propose to optimize the underlying image registration\noperations using gradient approximations. We further revisit the use of\nhomomorphic encryption and use a packing method to allow the encryption and\nmultiplication of large matrices more efficiently. We demonstrate our privacy\npreserving framework in linear and non-linear registration problems, evaluating\nits accuracy and scalability with respect to standard image registration. Our\nresults show that privacy preserving image registration is feasible and can be\nadopted in sensitive medical imaging applications.",
    "descriptor": "",
    "authors": [
      "Riccardo Taiello",
      "Melek \u00d6nen",
      "Olivier Humbert",
      "Marco Lorenzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10120"
  },
  {
    "id": "arXiv:2205.10121",
    "title": "Converting Artificial Neural Networks to Spiking Neural Networks via  Parameter Calibration",
    "abstract": "Spiking Neural Network (SNN), originating from the neural behavior in\nbiology, has been recognized as one of the next-generation neural networks.\nConventionally, SNNs can be obtained by converting from pre-trained Artificial\nNeural Networks (ANNs) by replacing the non-linear activation with spiking\nneurons without changing the parameters. In this work, we argue that simply\ncopying and pasting the weights of ANN to SNN inevitably results in activation\nmismatch, especially for ANNs that are trained with batch normalization (BN)\nlayers. To tackle the activation mismatch issue, we first provide a theoretical\nanalysis by decomposing local conversion error to clipping error and flooring\nerror, and then quantitatively measure how this error propagates throughout the\nlayers using the second-order analysis. Motivated by the theoretical results,\nwe propose a set of layer-wise parameter calibration algorithms, which adjusts\nthe parameters to minimize the activation mismatch. Extensive experiments for\nthe proposed algorithms are performed on modern architectures and large-scale\ntasks including ImageNet classification and MS COCO detection. We demonstrate\nthat our method can handle the SNN conversion with batch normalization layers\nand effectively preserve the high accuracy even in 32 time steps. For example,\nour calibration algorithms can increase up to 65% accuracy when converting\nVGG-16 with BN layers.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2106.06984\n",
    "authors": [
      "Yuhang Li",
      "Shikuang Deng",
      "Xin Dong",
      "Shi Gu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10121"
  },
  {
    "id": "arXiv:2205.10122",
    "title": "Stochastic resonance neurons in artificial neural networks",
    "abstract": "Many modern applications of the artificial neural networks ensue large number\nof layers making traditional digital implementations increasingly complex.\nOptical neural networks offer parallel processing at high bandwidth, but have\nthe challenge of noise accumulation. We propose here a new type of neural\nnetworks using stochastic resonances as an inherent part of the architecture\nand demonstrate a possibility of significant reduction of the required number\nof neurons for a given performance accuracy. We also show that such a neural\nnetwork is more robust against the impact of noise.",
    "descriptor": "",
    "authors": [
      "Egor Manuylovich",
      "Diego Arg\u00fcello Ron",
      "Morteza Kamalian-Kopae",
      "Sergei Turitsyn"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10122"
  },
  {
    "id": "arXiv:2205.10123",
    "title": "Lifelong Personal Context Recognition",
    "abstract": "We focus on the development of AIs which live in lifelong symbiosis with a\nhuman. The key prerequisite for this task is that the AI understands - at any\nmoment in time - the personal situational context that the human is in. We\noutline the key challenges that this task brings forth, namely (i) handling the\nhuman-like and ego-centric nature of the the user's context, necessary for\nunderstanding and providing useful suggestions, (ii) performing lifelong\ncontext recognition using machine learning in a way that is robust to change,\nand (iii) maintaining alignment between the AI's and human's representations of\nthe world through continual bidirectional interaction. In this short paper, we\nsummarize our recent attempts at tackling these challenges, discuss the lessons\nlearned, and highlight directions of future research. The main take-away\nmessage is that pursuing this project requires research which lies at the\nintersection of knowledge representation and machine learning. Neither\ntechnology can achieve this goal without the other.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Andrea Bontempelli",
      "Marcelo Rodas Britez",
      "Xiaoyue Li",
      "Haonan Zhao",
      "Luca Erculiani",
      "Stefano Teso",
      "Andrea Passerini",
      "Fausto Giunchiglia"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10123"
  },
  {
    "id": "arXiv:2205.10124",
    "title": "The Fellowship of the Dyson Ring: ACT\\&Friends' Results and Methods for  GTOC 11",
    "abstract": "Dyson spheres are hypothetical megastructures encircling stars in order to\nharvest most of their energy output. During the 11th edition of the GTOC\nchallenge, participants were tasked with a complex trajectory planning related\nto the construction of a precursor Dyson structure, a heliocentric ring made of\ntwelve stations. To this purpose, we developed several new approaches that\nsynthesize techniques from machine learning, combinatorial optimization,\nplanning and scheduling, and evolutionary optimization effectively integrated\ninto a fully automated pipeline. These include a machine learned transfer time\nestimator, improving the established Edelbaum approximation and thus better\ninforming a Lazy Race Tree Search to identify and collect asteroids with high\narrival mass for the stations; a series of optimally-phased low-thrust\ntransfers to all stations computed by indirect optimization techniques,\nexploiting the synodic periodicity of the system; and a modified Hungarian\nscheduling algorithm, which utilizes evolutionary techniques to arrange a\nmass-balanced arrival schedule out of all transfer possibilities. We describe\nthe steps of our pipeline in detail with a special focus on how our approaches\nmutually benefit from each other. Lastly, we outline and analyze the final\nsolution of our team, ACT&Friends, which ranked second at the GTOC 11\nchallenge.",
    "descriptor": "",
    "authors": [
      "Marcus M\u00e4rtens",
      "Dario Izzo",
      "Emmanuel Blazquez",
      "Moritz von Looz",
      "Pablo G\u00f3mez",
      "Anne Mergy",
      "Giacomo Acciarini",
      "Chit Hong Yam",
      "Javier Hernando Ayuso",
      "Yuri Shimane"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10124"
  },
  {
    "id": "arXiv:2205.10126",
    "title": "On Evaluating Power Loss with HATSGA Algorithm for Power Network  Reconfiguration in the Smart Grid",
    "abstract": "This paper presents the power network reconfiguration algorithm HATSGA with a\n\"R\" modeling approach and evaluates its behavior in computing new\nreconfiguration topologies for the power network in the Smart Grid context. The\nmodeling of the power distribution network with the language \"R\" is used to\nrepresent the network and support the computation of distinct algorithm\nconfigurations towards the evaluation of new reconfiguration topologies. The\nHATSGA algorithm adopts a hybrid Tabu Search and Genetic Algorithm strategy and\ncan be configured in different ways to compute network reconfiguration\nsolutions. The evaluation of power loss with HATSGA uses the IEEE 14-Bus\ntopology as the power test scenario. The evaluation of reconfiguration\ntopologies with minimum power loss with HATSGA indicates that an efficient\nsolution can be reached with a feasible computational time. This suggests that\nHATSGA can be potentially used for computing reconfiguration network topologies\nand, beyond that, it can be used for autonomic self-healing management\napproaches where a feasible computational time is required.",
    "descriptor": "\nComments: 7 pp\n",
    "authors": [
      "Flavio Galvao Calhau",
      "Alysson Pezzutti",
      "Joberto S. B. Martins"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10126"
  },
  {
    "id": "arXiv:2205.10127",
    "title": "Construction of Rough graph to handle uncertain pattern from an  Information System",
    "abstract": "Rough membership function defines the measurement of relationship between\nconditional and decision attribute from an Information system. In this paper we\npropose a new method to construct rough graph through rough membership function\n$\\omega_{G}^F(f)$. Rough graph identifies the pattern between the objects with\nimprecise and uncertain information. We explore the operations and properties\nof rough graph in various stages of its structure.",
    "descriptor": "\nComments: 13 pages, 11 figures\n",
    "authors": [
      "R. Aruna Devi",
      "K. Anitha"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10127"
  },
  {
    "id": "arXiv:2205.10128",
    "title": "Neural-Symbolic Models for Logical Queries on Knowledge Graphs",
    "abstract": "Answering complex first-order logic (FOL) queries on knowledge graphs is a\nfundamental task for multi-hop reasoning. Traditional symbolic methods traverse\na complete knowledge graph to extract the answers, which provides good\ninterpretation for each step. Recent neural methods learn geometric embeddings\nfor complex queries. These methods can generalize to incomplete knowledge\ngraphs, but their reasoning process is hard to interpret. In this paper, we\npropose Graph Neural Network Query Executor (GNN-QE), a neural-symbolic model\nthat enjoys the advantages of both worlds. GNN-QE decomposes a complex FOL\nquery into relation projections and logical operations over fuzzy sets, which\nprovides interpretability for intermediate variables. To reason about the\nmissing links, GNN-QE adapts a graph neural network from knowledge graph\ncompletion to execute the relation projections, and models the logical\noperations with product fuzzy logic. Extensive experiments on 3 datasets show\nthat GNN-QE significantly improves over previous state-of-the-art models in\nanswering FOL queries. Meanwhile, GNN-QE can predict the number of answers\nwithout explicit supervision, and provide visualizations for intermediate\nvariables.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Zhaocheng Zhu",
      "Mikhail Galkin",
      "Zuobai Zhang",
      "Jian Tang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10128"
  },
  {
    "id": "arXiv:2205.10129",
    "title": "Topology-aware Graph Neural Networks for Learning Feasible and Adaptive  ac-OPF Solutions",
    "abstract": "Solving the optimal power flow (OPF) problem is a fundamental task to ensure\nthe system efficiency and reliability in real-time electricity grid operations.\nWe develop a new topology-informed graph neural network (GNN) approach for\npredicting the optimal solutions of real-time ac-OPF problem. To incorporate\ngrid topology to the NN model, the proposed GNN-for-OPF framework innovatively\nexploits the locality property of locational marginal prices and voltage\nmagnitude. Furthermore, we develop a physics-aware (ac-)flow feasibility\nregularization approach for general OPF learning. The advantages of our\nproposed designs include reduced model complexity, improved generalizability\nand feasibility guarantees. By providing the analytical understanding on the\ngraph subspace stability under grid topology contingency, we show the proposed\nGNN can quickly adapt to varying grid topology by an efficient re-training\nstrategy. Numerical tests on various test systems of different sizes have\nvalidated the prediction accuracy, improved flow feasibility, and topology\nadaptivity capability of our proposed GNN-based learning framework.",
    "descriptor": "",
    "authors": [
      "Shaohui Liu",
      "Chengyang Wu",
      "Hao Zhu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.10129"
  },
  {
    "id": "arXiv:2205.10130",
    "title": "Function Regression using Spiking DeepONet",
    "abstract": "One of the main broad applications of deep learning is function regression.\nHowever, despite their demonstrated accuracy and robustness, modern neural\nnetwork architectures require heavy computational resources to train. One\nmethod to mitigate or even resolve this inefficiency has been to draw further\ninspiration from the brain and reformulate the learning process in a more\nbiologically-plausible way, developing what are known as Spiking Neural\nNetworks (SNNs), which have been gaining traction in recent years. In this\npaper we present an SNN-based method to perform regression, which has been a\nchallenge due to the inherent difficulty in representing a function's input\ndomain and continuous output values as spikes. We use a DeepONet - neural\nnetwork designed to learn operators - to learn the behavior of spikes. Then, we\nuse this approach to do function regression. We propose several methods to use\na DeepONet in the spiking framework, and present accuracy and training time for\ndifferent benchmarks.",
    "descriptor": "\nComments: 15 pages, 5 figures and 4 tables\n",
    "authors": [
      "Adar Kahana",
      "Qian Zhang",
      "Leonard Gleyzer",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10130"
  },
  {
    "id": "arXiv:2205.10131",
    "title": "Agent-Based modeling in Medical Research. Example in Health Economics",
    "abstract": "This chapter presents the main lines of agent based modeling in the field of\nmedical research. The general diagram consists of a cohort of patients (virtual\nor real) whose evolution is observed by means of so-called evolution models.\nScenarios can then be explored by varying the parameters of the different\nmodels. This chapter presents techniques for virtual patient generation and\nexamples of execution models. The advantages and disadvantages of these models\nare discussed as well as the pitfalls to be avoided. Finally, an application to\nthe medico-economic study of the impact of the penetration rate of generic\nversions of treatments on the costs associated with HIV treatment is presented.",
    "descriptor": "\nComments: 30 pages, 16 figures\n",
    "authors": [
      "Philippe Saint-Pierre",
      "Romain Demeulemeester",
      "Nad\u00e8ge Costa",
      "Nicolas Savy"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.10131"
  },
  {
    "id": "arXiv:2205.10133",
    "title": "Survey on Tools and Techniques Detecting Microservice API Patterns",
    "abstract": "It is well recognized that design patterns improve system development and\nmaintenance in many aspects. While we commonly recognize these patterns in\nmonolithic systems, many patterns emerged for cloud computing, specifically\nmicroservices. Unfortunately, while various patterns have been proposed,\navailable quality assessment tools often do not recognize many. This article\nperforms a grey literature review to find and catalog available tools to detect\nmicroservice API patterns (MAP). It reasons about mechanisms that can be used\nto detect these patterns. Furthermore, the results indicate gaps and\nopportunities for improvements for quality assessment tools. Finally, the\nreader is provided with a route map to detection techniques that can be used to\nmine MAPs.",
    "descriptor": "",
    "authors": [
      "Alexander Bakhtin",
      "Abdullah Al Maruf",
      "Tomas Cerny",
      "Davide Taibi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10133"
  },
  {
    "id": "arXiv:2205.10137",
    "title": "Practical Strategies of Active Learning to Rank for Web Search",
    "abstract": "While China has become the biggest online market in the world with around 1\nbillion internet users, Baidu runs the world largest Chinese search engine\nserving more than hundreds of millions of daily active users and responding\nbillions queries per day. To handle the diverse query requests from users at\nweb-scale, Baidu has done tremendous efforts in understanding users' queries,\nretrieve relevant contents from a pool of trillions of webpages, and rank the\nmost relevant webpages on the top of results. Among these components used in\nBaidu search, learning to rank (LTR) plays a critical role and we need to\ntimely label an extremely large number of queries together with relevant\nwebpages to train and update the online LTR models. To reduce the costs and\ntime consumption of queries/webpages labeling, we study the problem of Activ\nLearning to Rank (active LTR) that selects unlabeled queries for annotation and\ntraining in this work. Specifically, we first investigate the criterion --\nRanking Entropy (RE) characterizing the entropy of relevant webpages under a\nquery produced by a sequence of online LTR models updated by different\ncheckpoints, using a Query-By-Committee (QBC) method. Then, we explore a new\ncriterion namely Prediction Variances (PV) that measures the variance of\nprediction results for all relevant webpages under a query. Our empirical\nstudies find that RE may favor low-frequency queries from the pool for labeling\nwhile PV prioritizing high-frequency queries more. Finally, we combine these\ntwo complementary criteria as the sample selection strategies for active\nlearning. Extensive experiments with comparisons to baseline algorithms show\nthat the proposed approach could train LTR models achieving higher Discounted\nCumulative Gain (i.e., the relative improvement {\\Delta}DCG4=1.38%) with the\nsame budgeted labeling efforts.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Qingzhong Wang",
      "Haifang Li",
      "Haoyi Xiong",
      "Wen Wang",
      "Jiang Bian",
      "Yu Lu",
      "Shuaiqiang Wang",
      "Zhicong Cheng",
      "Jingbo Zhou",
      "Dawei Yin",
      "Dejing Dou"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.10137"
  },
  {
    "id": "arXiv:2205.10138",
    "title": "Reliability-based Mesh-to-Grid Image Reconstruction",
    "abstract": "This paper presents a novel method for the reconstruction of images from\nsamples located at non-integer positions, called mesh. This is a common\nscenario for many image processing applications, such as super-resolution,\nwarping or virtual view generation in multi-camera systems. The proposed method\nrelies on a set of initial estimates that are later refined by a new\nreliability-based content-adaptive framework that employs denoising in order to\nreduce the reconstruction error. The reliability of the initial estimate is\ncomputed so stronger denoising is applied to less reliable estimates. The\nproposed technique can improve the reconstruction quality by more than 2 dB (in\nterms of PSNR) with respect to the initial estimate and it outperforms the\nstate-of-the-art denoising-based refinement by up to 0.7 dB.",
    "descriptor": "",
    "authors": [
      "J\u00e1n Koloda",
      "J\u00fcrgen Seiler",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10138"
  },
  {
    "id": "arXiv:2205.10139",
    "title": "Towards efficient feature sharing in MIMO architectures",
    "abstract": "Multi-input multi-output architectures propose to train multiple subnetworks\nwithin one base network and then average the subnetwork predictions to benefit\nfrom ensembling for free. Despite some relative success, these architectures\nare wasteful in their use of parameters. Indeed, we highlight in this paper\nthat the learned subnetwork fail to share even generic features which limits\ntheir applicability on smaller mobile and AR/VR devices. We posit this behavior\nstems from an ill-posed part of the multi-input multi-output framework. To\nsolve this issue, we propose a novel unmixing step in MIMO architectures that\nallows subnetworks to properly share features. Preliminary experiments on\nCIFAR-100 show our adjustments allow feature sharing and improve model\nperformance for small architectures.",
    "descriptor": "\nComments: 7 pages, 6 figures, 1 table\n",
    "authors": [
      "R\u00e9my Sun",
      "Alexandre Ram\u00e9",
      "Cl\u00e9ment Masson",
      "Nicolas Thome",
      "Matthieu Cord"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10139"
  },
  {
    "id": "arXiv:2205.10144",
    "title": "The developmental trajectory of object recognition robustness: children  are like small adults but unlike big deep neural networks",
    "abstract": "In laboratory object recognition tasks based on undistorted photographs, both\nadult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike\nadults', whose object recognition performance is robust against a wide range of\nimage distortions, DNNs trained on standard ImageNet (1.3M images) perform\npoorly on distorted images. However, the last two years have seen impressive\ngains in DNN distortion robustness, predominantly achieved through\never-increasing large-scale datasets$\\unicode{x2014}$orders of magnitude larger\nthan ImageNet. While this simple brute-force approach is very effective in\nachieving human-level robustness in DNNs, it raises the question of whether\nhuman robustness, too, is simply due to extensive experience with (distorted)\nvisual input during childhood and beyond. Here we investigate this question by\ncomparing the core object recognition performance of 146 children (aged\n4$\\unicode{x2013}$15) against adults and against DNNs. We find, first, that\nalready 4$\\unicode{x2013}$6 year-olds showed remarkable robustness to image\ndistortions and outperform DNNs trained on ImageNet. Second, we estimated the\nnumber of $\\unicode{x201C}$images$\\unicode{x201D}$ children have been exposed\nto during their lifetime. Compared to various DNNs, children's high robustness\nrequires relatively little data. Third, when recognizing objects\nchildren$\\unicode{x2014}$like adults but unlike DNNs$\\unicode{x2014}$rely\nheavily on shape but not on texture cues. Together our results suggest that the\nremarkable robustness to distortions emerges early in the developmental\ntrajectory of human object recognition and is unlikely the result of a mere\naccumulation of experience with distorted visual input. Even though current\nDNNs match human performance regarding robustness they seem to rely on\ndifferent and more data-hungry strategies to do so.",
    "descriptor": "\nComments: Manuscript under review at Journal of Vision\n",
    "authors": [
      "Lukas S. Huber",
      "Robert Geirhos",
      "Felix A. Wichmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.10144"
  },
  {
    "id": "arXiv:2205.10146",
    "title": "Revisiting GANs by Best-Response Constraint: Perspective, Methodology,  and Application",
    "abstract": "In past years, the minimax type single-level optimization formulation and its\nvariations have been widely utilized to address Generative Adversarial Networks\n(GANs). Unfortunately, it has been proved that these alternating learning\nstrategies cannot exactly reveal the intrinsic relationship between the\ngenerator and discriminator, thus easily result in a series of issues,\nincluding mode collapse, vanishing gradients and oscillations in the training\nphase, etc. In this work, by investigating the fundamental mechanism of GANs\nfrom the perspective of hierarchical optimization, we propose Best-Response\nConstraint (BRC), a general learning framework, that can explicitly formulate\nthe potential dependency of the generator on the discriminator. Rather than\nadopting these existing time-consuming bilevel iterations, we design an\nimplicit gradient scheme with outer-product Hessian approximation as our fast\nsolution strategy. \\emph{Noteworthy, we demonstrate that even with different\nmotivations and formulations, a variety of existing GANs ALL can be uniformly\nimproved by our flexible BRC methodology.} Extensive quantitative and\nqualitative experimental results verify the effectiveness, flexibility and\nstability of our proposed framework.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Risheng Liu",
      "Jiaxin Gao",
      "Xuan Liu",
      "Xin Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10146"
  },
  {
    "id": "arXiv:2205.10152",
    "title": "Investigating the impact of BTI, HCI and time-zero variability on  neuromorphic spike event generation circuits",
    "abstract": "Neuromorphic computing refers to brain-inspired computers, that differentiate\nit from von Neumann architecture. Analog VLSI based neuromorphic circuits is a\ncurrent research interest. Two simpler spiking integrate and fire neuron model\nnamely axon-Hillock (AH) and voltage integrate, and fire (VIF) circuits are\ncommonly used for generating spike events. This paper discusses the impact of\nreliability issues like Bias Temperature instability (BTI) and Hot Carrier\nInjection (HCI), and timezero variability on these CMOS based neuromorphic\ncircuits. AH and VIF circuits are implemented using HKMG based 45nm technology.\nFor reliability analysis, industry standard Cadence RelXpert tool is used. For\ntime-zero variability analysis, 1000 Monte-Carlo simulations are performed.",
    "descriptor": "\nComments: 4 pages, 4 figures, IWPSD 2019\n",
    "authors": [
      "Shaik Jani Babu",
      "Rohit Singh",
      "Siona Menezes Picardo",
      "Nilesh Goel",
      "Sonal Singhal"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.10152"
  },
  {
    "id": "arXiv:2205.10153",
    "title": "Mapping Complex Technologies via Science-Technology Linkages; The Case  of Neuroscience -- A transformer based keyword extraction approach",
    "abstract": "In this paper, we present an efficient deep learning based approach to\nextract technology-related topics and keywords within scientific literature,\nand identify corresponding technologies within patent applications.\nSpecifically, we utilize transformer based language models, tailored for use\nwith scientific text, to detect coherent topics over time and describe these by\nrelevant keywords that are automatically extracted from a large text corpus. We\nidentify these keywords using Named Entity Recognition, distinguishing between\nthose describing methods, applications and other scientific terminology. We\ncreate a large amount of search queries based on combinations of method- and\napplication-keywords, which we use to conduct semantic search and identify\nrelated patents. By doing so, we aim at contributing to the growing body of\nresearch on text-based technology mapping and forecasting that leverages latest\nadvances in natural language processing and deep learning. We are able to map\ntechnologies identified in scientific literature to patent applications,\nthereby providing an empirical foundation for the study of science-technology\nlinkages. We illustrate the workflow as well as results obtained by mapping\npublications within the field of neuroscience to related patent applications.",
    "descriptor": "",
    "authors": [
      "Daniel Hain",
      "Roman Jurowetzki",
      "Mariagrazia Squicciarini"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.10153"
  },
  {
    "id": "arXiv:2205.10154",
    "title": "A Look Inside 5G Standards to Support Time Synchronization for Smart  Manufacturing",
    "abstract": "Connectivity has a major role in the current transformation of smart\nmanufacturing. 5G is foreseen as an integral part of an end-to-end networking\ninfrastructure supporting smart manufacturing operation. The integration of 5G\nwith time-sensitive networking (TSN)is seen as a holistic communication\nsolution for smart factories. The next generation of industrialization consists\nof real-time processes, automation, fixed and cellular networks, and high\ndiversity of devices. The integration of fixed and cellular devices in a single\ninfrastructure to enable end-to-end deterministic communications requires high\naccuracy of time synchronization. The transport technologies in fixed networks\nhave been evolving toward TSN and deterministic networking to fulfill the\nrequirements for industrial communications. The latest cellular specifications\nfor 5G are targeting ultra-reliable and low-latency communications that enable\nthe Industrial Internet of Things. This article describes the state of the art\nfor integrating TSN with 5G networks based on the support given in 3GPP TS\n23.501 Release 16. The requirements for time synchronization in factory\nautomation and integration with 5G networks are presented, together with the\nmost recent advancements in the standardization process.",
    "descriptor": "",
    "authors": [
      "I. G\u00f3dor",
      "M. Luvisotto",
      "S. Ruffini",
      "K. Wang",
      "D. Patel",
      "J. Sachs",
      "O. Dobrijevic",
      "D. P. Venmani",
      "O. Le Moult",
      "J. Costa-Requena",
      "A. Poutanen",
      "C. Marshall",
      "J. Farkas"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10154"
  },
  {
    "id": "arXiv:2205.10155",
    "title": "Large-Signal Stability Analysis of Current-Mode DC-DC Converters",
    "abstract": "Stability guarantees are critical for current-mode dc-dc converters in\nconsumer electronics and energy storage systems. Traditional stability analysis\non current-mode dc-dc converters is incomplete because the inductor current\nramps are considered fixed; but instead, inductor ramps are not fixed because\nthey are dependent on the output voltage in large-signal transients. We\ndemonstrate a new large-signal stability theory which treats current-mode dc-dc\nconverters as a particular type of feedback interconnection system. An\nanalytical and practical stability criterion is provided based on this system.\nThe criterion indicates that the L/R and RC time constants are the design\nparameters which determine the amount of coupling between the current ramp and\nthe output voltage.",
    "descriptor": "",
    "authors": [
      "Xiaofan Cui",
      "Al-Thaddeus Avestruz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.10155"
  },
  {
    "id": "arXiv:2205.10157",
    "title": "Machine Learning for Combinatorial Optimisation of Partially-Specified  Problems: Regret Minimisation as a Unifying Lens",
    "abstract": "It is increasingly common to solve combinatorial optimisation problems that\nare partially-specified. We survey the case where the objective function or the\nrelations between variables are not known or are only partially specified. The\nchallenge is to learn them from available data, while taking into account a set\nof hard constraints that a solution must satisfy, and that solving the\noptimisation problem (esp. during learning) is computationally very demanding.\nThis paper overviews four seemingly unrelated approaches, that can each be\nviewed as learning the objective function of a hard combinatorial optimisation\nproblem: 1) surrogate-based optimisation, 2) empirical model learning, 3)\ndecision-focused learning (`predict + optimise'), and 4) structured-output\nprediction. We formalise each learning paradigm, at first in the ways commonly\nfound in the literature, and then bring the formalisations together in a\ncompatible way using regret. We discuss the differences and interactions\nbetween these frameworks, highlight the opportunities for cross-fertilization\nand survey open directions.",
    "descriptor": "",
    "authors": [
      "Stefano Teso",
      "Laurens Bliek",
      "Andrea Borghesi",
      "Michele Lombardi",
      "Neil Yorke-Smith",
      "Tias Guns",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10157"
  },
  {
    "id": "arXiv:2205.10158",
    "title": "Swapping Semantic Contents for Mixing Images",
    "abstract": "Deep architecture have proven capable of solving many tasks provided a\nsufficient amount of labeled data. In fact, the amount of available labeled\ndata has become the principal bottleneck in low label settings such as\nSemi-Supervised Learning. Mixing Data Augmentations do not typically yield new\nlabeled samples, as indiscriminately mixing contents creates between-class\nsamples. In this work, we introduce the SciMix framework that can learn to\ngenerator to embed a semantic style code into image backgrounds, we obtain new\nmixing scheme for data augmentation. We then demonstrate that SciMix yields\nnovel mixed samples that inherit many characteristics from their non-semantic\nparents. Afterwards, we verify those samples can be used to improve the\nperformance semi-supervised frameworks like Mean Teacher or Fixmatch, and even\nfully supervised learning on a small labeled dataset.",
    "descriptor": "\nComments: Accepted at ICPR 2022, 7 pages, 4 figures, 6 tables\n",
    "authors": [
      "R\u00e9my Sun",
      "Cl\u00e9ment Masson",
      "Gilles H\u00e9naff",
      "Nicolas Thome",
      "Matthieu Cord"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10158"
  },
  {
    "id": "arXiv:2205.10159",
    "title": "Getting a-Round Guarantees: Floating-Point Attacks on Certified  Robustness",
    "abstract": "Adversarial examples pose a security risk as they can alter a classifier's\ndecision through slight perturbations to a benign input. Certified robustness\nhas been proposed as a mitigation strategy where given an input $x$, a\nclassifier returns a prediction and a radius with a provable guarantee that any\nperturbation to $x$ within this radius (e.g., under the $L_2$ norm) will not\nalter the classifier's prediction. In this work, we show that these guarantees\ncan be invalidated due to limitations of floating-point representation that\ncause rounding errors. We design a rounding search method that can efficiently\nexploit this vulnerability to find adversarial examples within the certified\nradius. We show that the attack can be carried out against several linear\nclassifiers that have exact certifiable guarantees and against neural network\nverifiers that return a certified lower bound on a robust radius. Our\nexperiments demonstrate over 50% attack success rate on random linear\nclassifiers, up to 35% on a breast cancer dataset for logistic regression, and\na 9% attack success rate on the MNIST dataset for a neural network whose\ncertified radius was verified by a prominent bound propagation method. We also\nshow that state-of-the-art random smoothed classifiers for neural networks are\nalso susceptible to adversarial examples (e.g., up to 2% attack rate on\nCIFAR10)-validating the importance of accounting for the error rate of\nrobustness guarantees of such classifiers in practice. Finally, as a\nmitigation, we advocate the use of rounded interval arithmetic to account for\nrounding errors.",
    "descriptor": "",
    "authors": [
      "Jiankai Jin",
      "Olga Ohrimenko",
      "Benjamin I. P. Rubinstein"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10159"
  },
  {
    "id": "arXiv:2205.10161",
    "title": "The role of the Big Geographic Sort in the circulation of misinformation  among U.S. Reddit users",
    "abstract": "Past research has attributed the online circulation of misinformation to two\nmain factors - individual characteristics (e.g., a person's information\nliteracy) and social media effects (e.g., algorithm-mediated information\ndiffusion) - and has overlooked a third one: the critical mass created by the\noffline self-segregation of Americans into like-minded geographical regions\nsuch as states (a phenomenon called \"The Big Sort\"). We hypothesized that this\nlatter factor matters for the online spreading of misinformation not least\nbecause online interactions, despite having the potential of being global, end\nup being localized: interaction probability is known to rapidly decay with\ndistance. Upon analysis of more than 8M Reddit comments containing news links\nspanning four years, from January 2016 to December 2019, we found that Reddit\ndid not work as an \"hype machine\" for misinformation (as opposed to what\nprevious work reported for other platforms, circulation was not mainly caused\nby platform-facilitated network effects) but worked as a supply-and-demand\nsystem: misinformation news items scaled linearly with the number of users in\neach state (with a scaling exponent beta=1, and a goodness of fit R2 = 0.95).\nFurthermore, deviations from such a universal pattern were best explained by\nstate-level personality and cultural factors (R2 = {0.12, 0.39}), rather than\nsocioeconomic conditions (R2 = {0.15, 0.29}) or, as one would expect, political\ncharacteristics (R2 ={0.06, 0.21}). Higher-than-expected circulation of any\ntype of news (including reputable news) was found in states characterised by\nresidents who tend to be less diligent in terms of their personality (low in\nconscientiousness) and by loose cultures understating the importance of\nadherence to norms (low in cultural tightness).",
    "descriptor": "",
    "authors": [
      "Lia Bozarth",
      "Daniele Quercia",
      "Licia Capra",
      "Sanja Scepanovic"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10161"
  },
  {
    "id": "arXiv:2205.10162",
    "title": "AutoFedNLP: An efficient FedNLP framework",
    "abstract": "Transformer-based pre-trained models have revolutionized NLP for superior\nperformance and generality. Fine-tuning pre-trained models for downstream tasks\noften require private data, for which federated learning is the de-facto\napproach (i.e., FedNLP). However, our measurements show that FedNLP is\nprohibitively slow due to the large model sizes and the resultant high\nnetwork/computation cost. Towards practical FedNLP, we identify as the key\nbuilding blocks adapters, small bottleneck modules inserted at a variety of\nmodel layers. A key challenge is to properly configure the depth and width of\nadapters, to which the training speed and efficiency is highly sensitive. No\nsilver-bullet configuration exists: the optimal choice varies across downstream\nNLP tasks, desired model accuracy, and client resources. A silver-bullet\nconfiguration does not exist and a non-optimal configuration could\nsignificantly slow down the training. To automate adapter configuration, we\npropose AutoFedNLP, a framework that enhances the existing FedNLP with two\nnovel designs. First, AutoFedNLP progressively upgrades the adapter\nconfiguration throughout a training session. Second, AutoFedNLP continuously\nprofiles future adapter configurations by allocating participant devices to\ntrial groups. To minimize client-side computations, AutoFedNLP exploits the\nfact that a FedNLP client trains on the same samples repeatedly between\nconsecutive changes of adapter configurations, and caches computed activations\non clients. Extensive experiments show that AutoFedNLP can reduce FedNLP's\nmodel convergence delay to no more than several hours, which is up to\n155.5$\\times$ faster compared to vanilla FedNLP and 48$\\times$ faster compared\nto strong baselines.",
    "descriptor": "",
    "authors": [
      "Dongqi Cai",
      "Yaozong Wu",
      "Shangguang Wang",
      "Felix Xiaozhu Lin",
      "Mengwei Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10162"
  },
  {
    "id": "arXiv:2205.10170",
    "title": "An optimal control-based numerical method for scalar transmission  problems with sign-changing coefficients",
    "abstract": "In this work, we present a new numerical method for solving the scalar\ntransmission problem with sign-changing coefficients. In electromagnetism, such\na transmission problem can occur if the domain of interest is made of a\nclassical dielectric material and a metal or a metamaterial, with for instance\nan electric permittivity that is strictly negative in the metal or\nmetamaterial. The method is based on an optimal control reformulation of the\nproblem. Contrary to other existing approaches, the convergence of this method\nis proved without any restrictive condition. In particular, no condition is\nimposed on the a priori regularity of the solution to the problem, and no\ncondition is imposed on the meshes, other than that they fit with the interface\nbetween the two media. Our results are illustrated by some (2D) numerical\nexperiments.",
    "descriptor": "",
    "authors": [
      "Patrick Ciarlet JR",
      "David Lassounon",
      "Mahran Rihani"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10170"
  },
  {
    "id": "arXiv:2205.10174",
    "title": "Pre-hijacked accounts: An Empirical Study of Security Failures in User  Account Creation on the Web",
    "abstract": "The ubiquity of user accounts in websites and online services makes account\nhijacking a serious security concern. Although previous research has studied\nvarious techniques through which an attacker can gain access to a victim's\naccount, relatively little attention has been directed towards the process of\naccount creation. The current trend towards federated authentication (e.g.,\nSingle Sign-On) adds an additional layer of complexity because many services\nnow support both the classic approach in which the user directly sets a\npassword, and the federated approach in which the user authenticates via an\nidentity provider.\nInspired by previous work on preemptive account hijacking [Ghasemisharif et\nal., USENIX SEC 2018], we show that there exists a whole class of account\npre-hijacking attacks. The distinctive feature of these attacks is that the\nattacker performs some action before the victim creates an account, which makes\nit trivial for the attacker to gain access after the victim has\ncreated/recovered the account. Assuming a realistic attacker who knows only the\nvictim's email address, we identify and discuss five different types of account\npre-hijacking attacks.\nTo ascertain the prevalence of such vulnerabilities in the wild, we analyzed\n75 popular services and found that at least 35 of these were vulnerable to one\nor more account pre-hijacking attacks. Whilst some of these may be noticed by\nattentive users, others were completely undetectable from the victim's\nperspective. Finally, we investigated the root cause of these vulnerabilities\nand present a set of security requirements to prevent such vulnerabilities\narising in future.",
    "descriptor": "\nComments: Accepted at USENIX Security 2022\n",
    "authors": [
      "Avinash Sudhodanan",
      "Andrew Paverd"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10174"
  },
  {
    "id": "arXiv:2205.10175",
    "title": "Task Relabelling for Multi-task Transfer using Successor Features",
    "abstract": "Deep Reinforcement Learning has been very successful recently with various\nworks on complex domains. Most works are concerned with learning a single\npolicy that solves the target task, but is fixed in the sense that if the\nenvironment changes the agent is unable to adapt to it. Successor Features\n(SFs) proposes a mechanism that allows learning policies that are not tied to\nany particular reward function. In this work we investigate how SFs may be\npre-trained without observing any reward in a custom environment that features\nresource collection, traps and crafting. After pre-training we expose the SF\nagents to various target tasks and see how well they can transfer to new tasks.\nTransferring is done without any further training on the SF agents, instead\njust by providing a task vector. For training the SFs we propose a task\nrelabelling method which greatly improves the agent's performance.",
    "descriptor": "\nComments: accepted for publication in IEEE Conference on Games (CoG) 2022\n",
    "authors": [
      "Martin Balla",
      "Diego Perez-Liebana"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10175"
  },
  {
    "id": "arXiv:2205.10176",
    "title": "Topology-aware Serverless Function-Execution Scheduling",
    "abstract": "State-of-the-art serverless platforms use hardcoded scheduling policies that\nare unaware of the possible topological constraints of functions. Considering\nthese constraints when scheduling functions leads to sensible performance\nimprovements, e.g., minimising loading times or data-access latencies. This\nissue becomes more pressing when considered in the emerging multi-cloud and\nedge-cloud-continuum systems, where only specific nodes can access specialised,\nlocal resources. To address this problem, we present a declarative language for\ndefining serverless scheduling policies to express constraints on the\ntopologies of schedulers and execution nodes. We implement our approach as an\nextension of the OpenWhisk platform and show relevant scenarios where our\nextension is on par with or outperforms vanilla OpenWhisk",
    "descriptor": "",
    "authors": [
      "Giuseppe De Palma",
      "Saverio Giallorenzo",
      "Jacopo Mauro",
      "Matteo Trentin",
      "Gianluigi Zavattaro"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10176"
  },
  {
    "id": "arXiv:2205.10178",
    "title": "Visually-Augmented Language Modeling",
    "abstract": "Human language is grounded on multimodal knowledge including visual knowledge\nlike colors, sizes, and shapes. However, current large-scale pre-trained\nlanguage models rely on the text-only self-supervised training with massive\ntext data, which precludes them from utilizing relevant visual information when\nnecessary. To address this, we propose a novel pre-training framework, named\nVaLM, to Visually-augment text tokens with retrieved relevant images for\nLanguage Modeling. Specifically, VaLM builds on a novel text-vision alignment\nmethod via an image retrieval module to fetch corresponding images given a\ntextual context. With the visually-augmented context, VaLM uses a visual\nknowledge fusion layer to enable multimodal grounded language modeling by\nattending on both text context and visual knowledge in images. We evaluate the\nproposed model on various multimodal commonsense reasoning tasks, which require\nvisual information to excel. VaLM outperforms the text-only baseline with\nsubstantial gains of +8.66% and +37.81% accuracy on object color and size\nreasoning, respectively.",
    "descriptor": "",
    "authors": [
      "Weizhi Wang",
      "Li Dong",
      "Hao Cheng",
      "Haoyu Song",
      "Xiaodong Liu",
      "Xifeng Yan",
      "Jianfeng Gao",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10178"
  },
  {
    "id": "arXiv:2205.10179",
    "title": "Towards the Generation of Synthetic Images of Palm Vein Patterns: A  Review",
    "abstract": "With the recent success of computer vision and deep learning, remarkable\nprogress has been achieved on automatic personal recognition using vein\nbiometrics. However, collecting large-scale real-world training data for palm\nvein recognition has turned out to be challenging, mainly due to the noise and\nirregular variations included at the time of acquisition. Meanwhile, existing\npalm vein recognition datasets are usually collected under near-infrared light,\nlacking detailed annotations on attributes (e.g., pose), so the influences of\ndifferent attributes on vein recognition have been poorly investigated.\nTherefore, this paper examines the suitability of synthetic vein images\ngenerated to compensate for the urgent lack of publicly available large-scale\ndatasets. Firstly, we present an overview of recent research progress on palm\nvein recognition, from the basic background knowledge to vein anatomical\nstructure, data acquisition, public database, and quality assessment\nprocedures. Then, we focus on the state-of-the-art methods that have allowed\nthe generation of vascular structures for biometric purposes and the modeling\nof biological networks with their respective application domains. In addition,\nwe review the existing research on the generation of style transfer and\nbiological nature-based synthetic palm vein image algorithms. Afterward, we\nformalize a general flowchart for the creation of a synthetic database\ncomparing real palm vein images and generated synthetic samples to obtain some\nunderstanding into the development of the realistic vein imaging system.\nUltimately, we conclude by discussing the challenges, insights, and future\nperspectives in generating synthetic palm vein images for further works.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Edwin H. Salazar-Jurado",
      "Ruber Hern\u00e1ndez-Garc\u00eda",
      "Karina Vilches-Ponce",
      "Ricardo J. Barrientos",
      "Marco Mora",
      "Gaurav Jaswal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10179"
  },
  {
    "id": "arXiv:2205.10183",
    "title": "Prototypical Calibration for Few-shot Learning of Language Models",
    "abstract": "In-context learning of GPT-like models has been recognized as fragile across\ndifferent hand-crafted templates, and demonstration permutations. In this work,\nwe propose prototypical calibration to adaptively learn a more robust decision\nboundary for zero- and few-shot classification, instead of greedy decoding.\nConcretely, our method first adopts Gaussian mixture distribution to estimate\nthe prototypical clusters for all categories. Then we assign each cluster to\nthe corresponding label by solving a weighted bipartite matching problem. Given\nan example, its prediction is calibrated by the likelihood of prototypical\nclusters. Experimental results show that prototypical calibration yields a 15%\nabsolute improvement on a diverse set of tasks. Extensive analysis across\ndifferent scales also indicates that our method calibrates the decision\nboundary as expected, greatly improving the robustness of GPT to templates,\npermutations, and class imbalance.",
    "descriptor": "",
    "authors": [
      "Zhixiong Han",
      "Yaru Hao",
      "Li Dong",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10183"
  },
  {
    "id": "arXiv:2205.10184",
    "title": "E-Scooter Rider Detection and Classification in Dense Urban Environments",
    "abstract": "Accurate detection and classification of vulnerable road users is a safety\ncritical requirement for the deployment of autonomous vehicles in heterogeneous\ntraffic. Although similar in physical appearance to pedestrians, e-scooter\nriders follow distinctly different characteristics of movement and can reach\nspeeds of up to 45kmph. The challenge of detecting e-scooter riders is\nexacerbated in urban environments where the frequency of partial occlusion is\nincreased as riders navigate between vehicles, traffic infrastructure and other\nroad users. This can lead to the non-detection or mis-classification of\ne-scooter riders as pedestrians, providing inaccurate information for accident\nmitigation and path planning in autonomous vehicle applications. This research\nintroduces a novel benchmark for partially occluded e-scooter rider detection\nto facilitate the objective characterization of detection models. A novel,\nocclusion-aware method of e-scooter rider detection is presented that achieves\na 15.93% improvement in detection performance over the current state of the\nart.",
    "descriptor": "",
    "authors": [
      "Shane Gilroy",
      "Darragh Mullins",
      "Edward Jones",
      "Ashkan Parsi",
      "Martin Glavin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10184"
  },
  {
    "id": "arXiv:2205.10186",
    "title": "Bayesian Active Learning with Fully Bayesian Gaussian Processes",
    "abstract": "The bias-variance trade-off is a well-known problem in machine learning that\nonly gets more pronounced the less available data there is. In active learning,\nwhere labeled data is scarce or difficult to obtain, neglecting this trade-off\ncan cause inefficient and non-optimal querying, leading to unnecessary data\nlabeling. In this paper, we focus on active learning with Gaussian Processes\n(GPs). For the GP, the bias-variance trade-off is made by optimization of the\ntwo hyperparameters: the length scale and noise-term. Considering that the\noptimal mode of the joint posterior of the hyperparameters is equivalent to the\noptimal bias-variance trade-off, we approximate this joint posterior and\nutilize it to design two new acquisition functions. The first one is a Bayesian\nvariant of Query-by-Committee (B-QBC), and the second is an extension that\nexplicitly minimizes the predictive variance through a Query by Mixture of\nGaussian Processes (QB-MGP) formulation. Across six common simulators, we\nempirically show that B-QBC, on average, achieves the best marginal likelihood,\nwhereas QB-MGP achieves the best predictive performance. We show that\nincorporating the bias-variance trade-off in the acquisition functions\nmitigates unnecessary and expensive data labeling.",
    "descriptor": "",
    "authors": [
      "Christoffer Riis",
      "Francisco N. Antunes",
      "Frederik Boe H\u00fcttel",
      "Carlos Lima Azevedo",
      "Francisco Camara Pereira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10186"
  },
  {
    "id": "arXiv:2205.10187",
    "title": "Adversarial Body Shape Search for Legged Robots",
    "abstract": "We propose an evolutionary computation method for an adversarial attack on\nthe length and thickness of parts of legged robots by deep reinforcement\nlearning. This attack changes the robot body shape and interferes with\nwalking-we call the attacked body as adversarial body shape. The evolutionary\ncomputation method searches adversarial body shape by minimizing the expected\ncumulative reward earned through walking simulation. To evaluate the\neffectiveness of the proposed method, we perform experiments with three-legged\nrobots, Walker2d, Ant-v2, and Humanoid-v2 in OpenAI Gym. The experimental\nresults reveal that Walker2d and Ant-v2 are more vulnerable to the attack on\nthe length than the thickness of the body parts, whereas Humanoid-v2 is\nvulnerable to the attack on both of the length and thickness. We further\nidentify that the adversarial body shapes break left-right symmetry or shift\nthe center of gravity of the legged robots. Finding adversarial body shape can\nbe used to proactively diagnose the vulnerability of legged robot walking.",
    "descriptor": "\nComments: 6 pages, 7 figures\n",
    "authors": [
      "Takaaki Azakami",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10187"
  },
  {
    "id": "arXiv:2205.10188",
    "title": "A Proximal Algorithm for Sampling from Non-convex Potentials",
    "abstract": "We study sampling problems associated with non-convex potentials that\nmeanwhile lack smoothness. In particular, we consider target distributions that\nsatisfy either logarithmic-Sobolev inequality or Poincar\\'e inequality. Rather\nthan smooth, the potentials are assumed to be semi-smooth or the summation of\nmultiple semi-smooth functions. We develop a sampling algorithm that resembles\nproximal algorithms in optimization for this challenging sampling task. Our\nalgorithm is based on a special case of Gibbs sampling known as the alternating\nsampling framework (ASF). The key contribution of this work is a practical\nrealization of the ASF based on rejection sampling in the non-convex and\nsemi-smooth setting. This work extends the recent algorithm in\n\\cite{LiaChe21,LiaChe22} for non-smooth/semi-smooth log-concave distribution to\nthe setting with non-convex potentials. In almost all the cases of sampling\nconsidered in this work, our proximal sampling algorithm achieves better\ncomplexity than all existing methods.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Jiaming Liang",
      "Yongxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10188"
  },
  {
    "id": "arXiv:2205.10189",
    "title": "Progressive Class Semantic Matching for Semi-supervised Text  Classification",
    "abstract": "Semi-supervised learning is a promising way to reduce the annotation cost for\ntext-classification. Combining with pre-trained language models (PLMs), e.g.,\nBERT, recent semi-supervised learning methods achieved impressive performance.\nIn this work, we further investigate the marriage between semi-supervised\nlearning and a pre-trained language model. Unlike existing approaches that\nutilize PLMs only for model parameter initialization, we explore the inherent\ntopic matching capability inside PLMs for building a more powerful\nsemi-supervised learning approach. Specifically, we propose a joint\nsemi-supervised learning process that can progressively build a standard\n$K$-way classifier and a matching network for the input text and the Class\nSemantic Representation (CSR). The CSR will be initialized from the given\nlabeled sentences and progressively updated through the training process. By\nmeans of extensive experiments, we show that our method can not only bring\nremarkable improvement to baselines, but also overall be more stable, and\nachieves state-of-the-art performance in semi-supervised text classification.",
    "descriptor": "\nComments: NAACL2022 (oral)\n",
    "authors": [
      "Hai-Ming Xu",
      "Lingqiao Liu",
      "Ehsan Abbasnejad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10189"
  },
  {
    "id": "arXiv:2205.10192",
    "title": "On the Trade-off between Redundancy and Local Coherence in Summarization",
    "abstract": "Extractive summarization systems are known to produce poorly coherent and, if\nnot accounted for, highly redundant text. In this work, we tackle the problem\nof summary redundancy in unsupervised extractive summarization of long,\nhighly-redundant documents. For this, we leverage a psycholinguistic theory of\nhuman reading comprehension which directly models local coherence and\nredundancy. Implementing this theory, our system operates at the proposition\nlevel and exploits properties of human memory representations to rank similarly\ncontent units that are coherent and non-redundant, hence encouraging the\nextraction of less redundant final summaries. Because of the impact of the\nsummary length on automatic measures, we control for it by formulating content\nselection as an optimization problem with soft constraints in the budget of\ninformation retrieved. Using summarization of scientific articles as a case\nstudy, extensive experiments demonstrate that the proposed systems extract\nconsistently less redundant summaries across increasing levels of document\nredundancy, whilst maintaining comparable performance (in terms of relevancy\nand local coherence) against strong unsupervised baselines according to\nautomated evaluations.",
    "descriptor": "\nComments: Under revision\n",
    "authors": [
      "Ronald Cardenas",
      "Matthias Galle",
      "Shay B. Cohen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10192"
  },
  {
    "id": "arXiv:2205.10194",
    "title": "New compressed cover tree for k-nearest neighbor search",
    "abstract": "This thesis consists of two topics related to computational geometry and one\ntopic related to topological data analysis (TDA), which combines fields of\ncomputational geometry and algebraic topology for analyzing data. The first\npart studies the classical problem of finding k nearest neighbors to m query\npoints in a larger set of n reference points in any metric space. The second\npart is about the construction of a Minimum Spanning Tree (MST) on any finite\nmetric space. The third part extends the key concept of persistence within\nTopological Data Analysis in a new direction.",
    "descriptor": "\nComments: PhD Thesis\n",
    "authors": [
      "Yury Elkin"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.10194"
  },
  {
    "id": "arXiv:2205.10195",
    "title": "Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video  Restoration",
    "abstract": "How to properly model the inter-frame relation within the video sequence is\nan important but unsolved challenge for video restoration (VR). In this work,\nwe propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to\naddress this problem. On the one hand, the sequence-to-sequence model, which\nhas proven capable of sequence modeling in the field of natural language\nprocessing, is explored for the first time in VR. Optimized serialization\nmodeling shows potential in capturing long-range dependencies among frames. On\nthe other hand, we equip the sequence-to-sequence model with an unsupervised\noptical flow estimator to maximize its potential. The flow estimator is trained\nwith our proposed unsupervised distillation loss, which can alleviate the data\ndiscrepancy and inaccurate degraded optical flow issues of previous flow-based\nmethods. With reliable optical flow, we can establish accurate correspondence\namong multiple frames, narrowing the domain difference between 1D language and\n2D misaligned frames and improving the potential of the sequence-to-sequence\nmodel. S2SVR shows superior performance in multiple VR tasks, including video\ndeblurring, video super-resolution, and compressed video quality enhancement.\nCode and models are publicly available at\nhttps://github.com/linjing7/VR-Baseline",
    "descriptor": "\nComments: ICML 2022; The first sequence-to-sequence model for video restoration\n",
    "authors": [
      "Jing Lin",
      "Xiaowan Hu",
      "Yuanhao Cai",
      "Haoqian Wang",
      "Youliang Yan",
      "Xueyi Zou",
      "Yulun Zhang",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10195"
  },
  {
    "id": "arXiv:2205.10199",
    "title": "A Novel Underwater Image Enhancement and Improved Underwater Biological  Detection Pipeline",
    "abstract": "For aquaculture resource evaluation and ecological environment monitoring,\nautomatic detection and identification of marine organisms is critical.\nHowever, due to the low quality of underwater images and the characteristics of\nunderwater biological, a lack of abundant features may impede traditional\nhand-designed feature extraction approaches or CNN-based object detection\nalgorithms, particularly in complex underwater environment. Therefore, the goal\nof this paper is to perform object detection in the underwater environment.\nThis paper proposed a novel method for capturing feature information, which\nadds the convolutional block attention module (CBAM) to the YOLOv5 backbone.\nThe interference of underwater creature characteristics on object\ncharacteristics is decreased, and the output of the backbone network to object\ninformation is enhanced. In addition, the self-adaptive global histogram\nstretching algorithm (SAGHS) is designed to eliminate the degradation problems\nsuch as low contrast and color loss caused by underwater environmental\ninformation to better restore image quality. Extensive experiments and\ncomprehensive evaluation on the URPC2021 benchmark dataset demonstrate the\neffectiveness and adaptivity of our methods. Beyond that, this paper conducts\nan exhaustive analysis of the role of training data on performance.",
    "descriptor": "\nComments: 14 pages,14 figures\n",
    "authors": [
      "Zheng Liu",
      "Yaoming Zhuang",
      "Pengrun Jia",
      "Chengdong Wu",
      "Hongli Xu ang Zhanlin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10199"
  },
  {
    "id": "arXiv:2205.10201",
    "title": "On the Decentralization of Blockchain-enabled Asynchronous Federated  Learning",
    "abstract": "Federated learning (FL), thanks in part to the emergence of the edge\ncomputing paradigm, is expected to enable true real-time applications in\nproduction environments. However, its original dependence on a central server\nfor orchestration raises several concerns in terms of security, privacy, and\nscalability. To solve some of these worries, blockchain technology is expected\nto bring decentralization, robustness, and enhanced trust to FL. The\nempowerment of FL through blockchain (also referred to as FLchain), however,\nhas some implications in terms of ledger inconsistencies and age of information\n(AoI), which are naturally inherited from the blockchain's fully decentralized\noperation. Such issues stem from the fact that, given the temporary ledger\nversions in the blockchain, FL devices may use different models for training,\nand that, given the asynchronicity of the FL operation, stale local updates\n(computed using outdated models) may be generated. In this paper, we shed light\non the implications of the FLchain setting and study the effect that both the\nAoI and ledger inconsistencies have on the FL performance. To that end, we\nprovide a faithful simulation tool that allows capturing the decentralized and\nasynchronous nature of the FLchain operation.",
    "descriptor": "",
    "authors": [
      "Francesc Wilhelmi",
      "Elia Guerra",
      "Paolo Dini"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.10201"
  },
  {
    "id": "arXiv:2205.10202",
    "title": "How to Guide Adaptive Depth Sampling?",
    "abstract": "Recent advances in depth sensing technologies allow fast electronic\nmaneuvering of the laser beam, as opposed to fixed mechanical rotations. This\nwill enable future sensors, in principle, to vary in real-time the sampling\npattern. We examine here the abstract problem of whether adapting the sampling\npattern for a given frame can reduce the reconstruction error or allow a\nsparser pattern. We propose a constructive generic method to guide adaptive\ndepth sampling algorithms.\nGiven a sampling budget B, a depth predictor P and a desired quality measure\nM, we propose an Importance Map that highlights important sampling locations.\nThis map is defined for a given frame as the per-pixel expected value of M\nproduced by the predictor P, given a pattern of B random samples. This map can\nbe well estimated in a training phase. We show that a neural network can learn\nto produce a highly faithful Importance Map, given an RGB image. We then\nsuggest an algorithm to produce a sampling pattern for the scene, which is\ndenser in regions that are harder to reconstruct. The sampling strategy of our\nmodular framework can be adjusted according to hardware limitations, type of\ndepth predictor, and any custom reconstruction error measure that should be\nminimized. We validate through simulations that our approach outperforms grid\nand random sampling patterns as well as recent state-of-the-art adaptive\nalgorithms.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Ilya Tcenov",
      "Guy Gilboa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10202"
  },
  {
    "id": "arXiv:2205.10203",
    "title": "Learning to Count Anything: Reference-less Class-agnostic Counting with  Weak Supervision",
    "abstract": "Object counting is a seemingly simple task with diverse real-world\napplications. Most counting methods focus on counting instances of specific,\nknown classes. While there are class-agnostic counting methods that can\ngeneralise to unseen classes, these methods require reference images to define\nthe type of object to be counted, as well as instance annotations during\ntraining. We identify that counting is, at its core, a repetition-recognition\ntask and show that a general feature space, with global context, is sufficient\nto enumerate instances in an image without a prior on the object type present.\nSpecifically, we demonstrate that self-supervised vision transformer features\ncombined with a lightweight count regression head achieve competitive results\nwhen compared to other class-agnostic counting tasks without the need for\npoint-level supervision or reference images. Our method thus facilitates\ncounting on a constantly changing set composition. To the best of our\nknowledge, we are both the first reference-less class-agnostic counting method\nas well as the first weakly-supervised class-agnostic counting method.",
    "descriptor": "",
    "authors": [
      "Michael Hobley",
      "Victor Prisacariu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10203"
  },
  {
    "id": "arXiv:2205.10205",
    "title": "Estimation of binary time-frequency masks from ambient noise",
    "abstract": "We investigate the retrieval of a binary time-frequency mask from a few\nobservations of filtered white ambient noise. Confirming household wisdom in\nacoustic modeling, we show that this is possible by inspecting the average\nspectrogram of ambient noise. Specifically, we show that the lower quantile of\nthe average of $\\mathcal{O}(\\log(|\\Omega|/\\varepsilon))$ masked spectrograms is\nenough to identify a rather general mask $\\Omega$ with confidence at least\n$\\varepsilon$, up to shape details concentrated near the boundary of $\\Omega$.\nAs an application, the expected measure of the estimation error is dominated by\nthe perimeter of the time-frequency mask. The estimator requires no knowledge\nof the noise variance, and only a very qualitative profile of the filtering\nwindow, but no exact knowledge of it.",
    "descriptor": "",
    "authors": [
      "Jos\u00e9 Luis Romero",
      "Michael Speckbacher"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Functional Analysis (math.FA)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.10205"
  },
  {
    "id": "arXiv:2205.10207",
    "title": "Measuring algorithmic interpretability: A human-learning-based framework  and the corresponding cognitive complexity score",
    "abstract": "Algorithmic interpretability is necessary to build trust, ensure fairness,\nand track accountability. However, there is no existing formal measurement\nmethod for algorithmic interpretability. In this work, we build upon\nprogramming language theory and cognitive load theory to develop a framework\nfor measuring algorithmic interpretability. The proposed measurement framework\nreflects the process of a human learning an algorithm. We show that the\nmeasurement framework and the resulting cognitive complexity score have the\nfollowing desirable properties - universality, computability, uniqueness, and\nmonotonicity. We illustrate the measurement framework through a toy example,\ndescribe the framework and its conceptual underpinnings, and demonstrate the\nbenefits of the framework, in particular for managers considering tradeoffs\nwhen selecting algorithms.",
    "descriptor": "",
    "authors": [
      "John P. Lalor",
      "Hong Guo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10207"
  },
  {
    "id": "arXiv:2205.10210",
    "title": "Test-time Batch Normalization",
    "abstract": "Deep neural networks often suffer the data distribution shift between\ntraining and testing, and the batch statistics are observed to reflect the\nshift. In this paper, targeting of alleviating distribution shift in test time,\nwe revisit the batch normalization (BN) in the training process and reveals two\nkey insights benefiting test-time optimization: $(i)$ preserving the same\ngradient backpropagation form as training, and $(ii)$ using dataset-level\nstatistics for robust optimization and inference. Based on the two insights, we\npropose a novel test-time BN layer design, GpreBN, which is optimized during\ntesting by minimizing Entropy loss. We verify the effectiveness of our method\non two typical settings with distribution shift, i.e., domain generalization\nand robustness tasks. Our GpreBN significantly improves the test-time\nperformance and achieves the state of the art results.",
    "descriptor": "",
    "authors": [
      "Tao Yang",
      "Shenglong Zhou",
      "Yuwang Wang",
      "Yan Lu",
      "Nanning Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10210"
  },
  {
    "id": "arXiv:2205.10218",
    "title": "Learning Task-relevant Representations for Generalization via  Characteristic Functions of Reward Sequence Distributions",
    "abstract": "Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.",
    "descriptor": "\nComments: Accepted to KDD'22\n",
    "authors": [
      "Rui Yang",
      "Jie Wang",
      "Zijie Geng",
      "Mingxuan Ye",
      "Shuiwang Ji",
      "Bin Li",
      "Feng Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10218"
  },
  {
    "id": "arXiv:2205.10222",
    "title": "An affective and adaptive educational robot",
    "abstract": "In this paper we present an educational robot called Wolly, designed to\nengage children in an affective and social interaction. Indeed, we are now\nfocusing on its role as an educational and affective robot capable of being\ncontrolled by coding instructions and at the same time interacting verbally and\naffectively with children by recognizing their emotions and remembering their\ninterests, and adapting its behavior accordingly.",
    "descriptor": "\nComments: extened version of the paper Wolly: An affective and adaptive educational robot, submitted to CAESAR 2022. arXiv admin note: text overlap with arXiv:2203.06439\n",
    "authors": [
      "Cristina Gena",
      "Alberto Lillo",
      "Claudio Mattutino",
      "Enrico Mosca"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.10222"
  },
  {
    "id": "arXiv:2205.10223",
    "title": "Mosaic Zonotope Shadow Matching for Risk-Aware Autonomous Localization  in Harsh Urban Environments",
    "abstract": "Risk-aware urban localization with the Global Navigation Satellite System\n(GNSS) remains an unsolved problem with frequent misdetection of the user's\nstreet or side of the street. Significant advances in 3D map-aided GNSS use\ngrid-based GNSS shadow matching alongside AI-driven line-of-sight (LOS)\nclassifiers and server-based processing to improve localization accuracy,\nespecially in the cross-street direction. Our prior work introduces a new\nparadigm for shadow matching that proposes set-valued localization with\ncomputationally efficient zonotope set representations. While existing\nliterature improved accuracy and efficiency, the current state of shadow\nmatching theory does not address the needs of risk-aware autonomous systems. We\nextend our prior work to propose Mosaic Zonotope Shadow Matching (MZSM) that\nemploys a classifier-agnostic polytope mosaic architecture to provide\nrisk-awareness and certifiable guarantees on urban positioning. We formulate a\nrecursively expanding binary tree that refines an initial location estimate\nwith set operations into smaller polytopes. Together, the smaller polytopes\nform a mosaic. We weight the tree branches with the probability that the user\nis in line of sight of the satellite and expand the tree with each new\nsatellite observation. Our method yields an exact shadow matching distribution\nfrom which we guarantee uncertainty bounds on the user localization. We perform\nhigh-fidelity simulations using a 3D building map of San Francisco to validate\nour algorithm's risk-aware improvements. We demonstrate that MZSM provides\ncertifiable guarantees across varied data-driven LOS classifier accuracies and\nyields a more precise understanding of the uncertainty over existing methods.\nWe validate that our tree-based construction is efficient and tractable,\ncomputing a mosaic from 14 satellites in 0.63 seconds and growing quadratically\nin the satellite number.",
    "descriptor": "\nComments: Submitted to AIJ Special Issue on Risk-Aware Autonomous Systems: Theory and Practice\n",
    "authors": [
      "Daniel Neamati",
      "Sriramya Bhamidipati",
      "Grace Gao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10223"
  },
  {
    "id": "arXiv:2205.10224",
    "title": "Schedulability Analysis of WSAN Applications: Outperformance of A Model  Checking Approach",
    "abstract": "Wireless sensor and actuator networks (WSAN) are real-time systems which\ndemand high degrees of reliability requirements. To ensure this level of\nreliability, different analysis approaches have been proposed for WSAN\napplications. Among different alternatives, analytical analysis and model\nchecking are two common approaches which are widely used for the formal\nanalysis of WSAN applications. Analytical approaches apply constraint\nsatisfaction methods, whereas model checking generates explicit states of\nmodels and analyze them. In this paper, we compare the two approaches in\nschedulability analysis of WSAN applications using an application for\nmonitoring and control of civil infrastructures, which is implemented on the\nImote2 wireless sensor platform. We show how the highest possible data\nacquisition frequency for this application is computed while meeting the\ndeadlines, and compare the results of the two approaches as well as their\nscalability, extensibility, and flexibility.",
    "descriptor": "",
    "authors": [
      "Ehsan Khamespanah",
      "Morteza Mohaqeqi",
      "Mohammad Ashjaei",
      "Marjan Sirjani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.10224"
  },
  {
    "id": "arXiv:2205.10226",
    "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific  Human Gaze?",
    "abstract": "Learned self-attention functions in state-of-the-art NLP models often\ncorrelate with human attention. We investigate whether self-attention in\nlarge-scale pre-trained language models is as predictive of human eye fixation\npatterns during task-reading as classical cognitive models of human attention.\nWe compare attention functions across two task-specific reading datasets for\nsentiment analysis and relation extraction. We find the predictiveness of\nlarge-scale pre-trained self-attention for human attention depends on `what is\nin the tail', e.g., the syntactic nature of rare contexts. Further, we observe\nthat task-specific fine-tuning does not increase the correlation with human\ntask-specific reading. Through an input reduction experiment we give\ncomplementary insights on the sparsity and fidelity trade-off, showing that\nlower-entropy attention vectors are more faithful.",
    "descriptor": "\nComments: Accepted to ACL 2022\n",
    "authors": [
      "Stephanie Brandl",
      "Oliver Eberle",
      "Jonas Pilot",
      "Anders S\u00f8gaard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10226"
  },
  {
    "id": "arXiv:2205.10227",
    "title": "Label Anchored Contrastive Learning for Language Understanding",
    "abstract": "Contrastive learning (CL) has achieved astonishing progress in computer\nvision, speech, and natural language processing fields recently with\nself-supervised learning. However, CL approach to the supervised setting is not\nfully explored, especially for the natural language understanding\nclassification task. Intuitively, the class label itself has the intrinsic\nability to perform hard positive/negative mining, which is crucial for CL.\nMotivated by this, we propose a novel label anchored contrastive learning\napproach (denoted as LaCon) for language understanding. Specifically, three\ncontrastive objectives are devised, including a multi-head instance-centered\ncontrastive loss (ICL), a label-centered contrastive loss (LCL), and a label\nembedding regularizer (LER). Our approach does not require any specialized\nnetwork architecture or any extra data augmentation, thus it can be easily\nplugged into existing powerful pre-trained language models. Compared to the\nstate-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular\ndatasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates\nsignificant advantages under the few-shot and data imbalance settings, which\nobtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.",
    "descriptor": "",
    "authors": [
      "Zhenyu Zhang",
      "Yuming Zhao",
      "Meng Chen",
      "Xiaodong He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10227"
  },
  {
    "id": "arXiv:2205.10228",
    "title": "You Don't Know My Favorite Color: Preventing Dialogue Representations  from Revealing Speakers' Private Personas",
    "abstract": "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large\npretrained language models. Despite the huge progress, privacy concerns have\narisen recently: training data of large language models can be extracted via\nmodel inversion attacks. On the other hand, the datasets used for training\nchatbots contain many private conversations between two individuals. In this\nwork, we further investigate the privacy leakage of the hidden states of\nchatbots trained by language modeling which has not been well studied yet. We\nshow that speakers' personas can be inferred through a simple neural network\nwith high accuracy. To this end, we propose effective defense objectives to\nprotect persona leakage from hidden states. We conduct extensive experiments to\ndemonstrate that our proposed defense objectives can greatly reduce the attack\naccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve\nlanguage models' powerful generation ability.",
    "descriptor": "\nComments: Conference paper accepted by NAACL 2022\n",
    "authors": [
      "Haoran Li",
      "Yangqiu Song",
      "Lixin Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10228"
  },
  {
    "id": "arXiv:2205.10230",
    "title": "RAR-PINN algorithm for the data-driven vector-soliton solutions and  parameter discovery of coupled nonlinear equations",
    "abstract": "This work aims to provide an effective deep learning framework to predict the\nvector-soliton solutions of the coupled nonlinear equations and their\ninteractions. The method we propose here is a physics-informed neural network\n(PINN) combining with the residual-based adaptive refinement (RAR-PINN)\nalgorithm. Different from the traditional PINN algorithm which takes points\nrandomly, the RAR-PINN algorithm uses an adaptive point-fetching approach to\nimprove the training efficiency for the solutions with steep gradients. A\nseries of experiment comparisons between the RAR-PINN and traditional PINN\nalgorithms are implemented to a coupled generalized nonlinear Schr\\\"{o}dinger\n(CGNLS) equation as an example. The results indicate that the RAR-PINN\nalgorithm has faster convergence rate and better approximation ability,\nespecially in modeling the shape-changing vector-soliton interactions in the\ncoupled systems. Finally, the RAR-PINN method is applied to perform the\ndata-driven discovery of the CGNLS equation, which shows the dispersion and\nnonlinear coefficients can be well approximated.",
    "descriptor": "",
    "authors": [
      "Shu-Mei Qin",
      "Min Li",
      "Tao Xu",
      "Shao-Qun Dong"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10230"
  },
  {
    "id": "arXiv:2205.10232",
    "title": "Exploring the Trade-off between Plausibility, Change Intensity and  Adversarial Power in Counterfactual Explanations using Multi-objective  Optimization",
    "abstract": "There is a broad consensus on the importance of deep learning models in tasks\ninvolving complex data. Often, an adequate understanding of these models is\nrequired when focusing on the transparency of decisions in human-critical\napplications. Besides other explainability techniques, trustworthiness can be\nachieved by using counterfactuals, like the way a human becomes familiar with\nan unknown process: by understanding the hypothetical circumstances under which\nthe output changes. In this work we argue that automated counterfactual\ngeneration should regard several aspects of the produced adversarial instances,\nnot only their adversarial capability. To this end, we present a novel\nframework for the generation of counterfactual examples which formulates its\ngoal as a multi-objective optimization problem balancing three different\nobjectives: 1) plausibility, i.e., the likeliness of the counterfactual of\nbeing possible as per the distribution of the input data; 2) intensity of the\nchanges to the original input; and 3) adversarial power, namely, the\nvariability of the model's output induced by the counterfactual. The framework\ndeparts from a target model to be audited and uses a Generative Adversarial\nNetwork to model the distribution of input data, together with a\nmulti-objective solver for the discovery of counterfactuals balancing among\nthese objectives. The utility of the framework is showcased over six\nclassification tasks comprising image and three-dimensional data. The\nexperiments verify that the framework unveils counterfactuals that comply with\nintuition, increasing the trustworthiness of the user, and leading to further\ninsights, such as the detection of bias and data misrepresentation.",
    "descriptor": "\nComments: 52 pages, 14 figures, under review\n",
    "authors": [
      "Javier Del Ser",
      "Alejandro Barredo-Arrieta",
      "Natalia D\u00edaz-Rodr\u00edguez",
      "Francisco Herrera",
      "Andreas Holzinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10232"
  },
  {
    "id": "arXiv:2205.10233",
    "title": "RigoBERTa: A State-of-the-Art Language Model For Spanish",
    "abstract": "This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.\nRigoBERTa is trained over RigoCorpus, a well-curated corpus formed up from\ndifferent subcorpora with key features. It follows the DeBERTa architecture,\nwhich has several advantages over other architectures of similar size as BERT\nor RoBERTa.\nRigoBERTa performance is assessed over 13 NLU tasks in comparison with other\navailable Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa\noutperformed the three models in 10 out of the 13 tasks, achieving new\n\"State-of-the-Art\" results.",
    "descriptor": "",
    "authors": [
      "Alejandro Vaca Serrano",
      "Guillem Garcia Subies",
      "Helena Montoro Zamorano",
      "Nuria Aldama Garcia",
      "Doaa Samy",
      "David Betancur Sanchez",
      "Antonio Moreno Sandoval",
      "Marta Guerrero Nieto",
      "Alvaro Barbero Jimenez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10233"
  },
  {
    "id": "arXiv:2205.10234",
    "title": "Federated learning for violence incident prediction in a simulated  cross-institutional psychiatric setting",
    "abstract": "Inpatient violence is a common and severe problem within psychiatry. Knowing\nwho might become violent can influence staffing levels and mitigate severity.\nPredictive machine learning models can assess each patient's likelihood of\nbecoming violent based on clinical notes. Yet, while machine learning models\nbenefit from having more data, data availability is limited as hospitals\ntypically do not share their data for privacy preservation. Federated Learning\n(FL) can overcome the problem of data limitation by training models in a\ndecentralised manner, without disclosing data between collaborators. However,\nalthough several FL approaches exist, none of these train Natural Language\nProcessing models on clinical notes. In this work, we investigate the\napplication of Federated Learning to clinical Natural Language Processing,\napplied to the task of Violence Risk Assessment by simulating a\ncross-institutional psychiatric setting. We train and compare four models: two\nlocal models, a federated model and a data-centralised model. Our results\nindicate that the federated model outperforms the local models and has similar\nperformance as the data-centralised model. These findings suggest that\nFederated Learning can be used successfully in a cross-institutional setting\nand is a step towards new applications of Federated Learning based on clinical\nnotes",
    "descriptor": "",
    "authors": [
      "Thomas Borger",
      "Pablo Mosteiro",
      "Heysem Kaya",
      "Emil Rijcken",
      "Albert Ali Salah",
      "Floortje Scheepers",
      "Marco Spruit"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10234"
  },
  {
    "id": "arXiv:2205.10235",
    "title": "An Efficient Methodology to Identify Missing Tags in Large-Scale RFID  Systems",
    "abstract": "Radio frequency identification (RFID) has been widely has broad applications.\nOne such application is to use RFID to track inventory in warehouses and retail\nstores. In this application, timely identifying the missing items is an ongoing\nengineering problem. A feasible solution to this problem is to map each tag to\na time slot and verify the presence of a tag by comparing the status of the\npredicted time slot and the actual time slot. However, existing works are time\ninefficient because they only verify tags one by one in singleton slots but\nignore the collision slots mapped by multiple tags. To accelerate the\nidentification process, we use bit tracking to verify tags in collision slots\nand design two protocols accordingly. We first propose the Sequential String\nbased Missing Tag Identification (SSMTI) protocol, which converts all time\nslots to collision slots and enables tags in each slot to reply to a designed\nstring simultaneously. By using bit tracking to decode the combined string, the\nreader can verify multiple tags together. To improve the performance of SSMTI\nwhen most tags are missing, we further propose the Interactive String based\nMissing Tag Identification (ISMTI) protocol. ISMTI improves the strategies of\ndesigning strings for each collided tag so that the reader can verify more tags\nusing shorter strings than SSMTI.Besides, ISMTI can dynamically adjust the\nverification mechanism according to the proportion of missing tags to maintain\ntime efficiency. We also provide theoretical analysis for proposed protocols to\nminimize execution time and evaluate their performance through extensive\nsimulations. Compared with state-of-the-art solutions, the proposed SSMTI and\nISMTI can reduce the time cost by as much as 39.74% and 68.87%.",
    "descriptor": "",
    "authors": [
      "Chu Chu",
      "Rui Xu",
      "Gang Li",
      "Zhenbing Li",
      "Guangjun Wen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.10235"
  },
  {
    "id": "arXiv:2205.10236",
    "title": "Invariant Extended Kalman Filtering for Human Motion Estimation with  Imperfect Sensor Placement",
    "abstract": "This paper introduces a new invariant extended Kalman filter design that\nproduces real-time state estimates and rapid error convergence for the\nestimation of the human body movement even in the presence of sensor\nmisalignment and initial state estimation errors. The filter fuses the data\nreturned by an inertial measurement unit (IMU) attached to the body (e.g.,\npelvis or chest) and a virtual measurement of zero stance-foot velocity (i.e.,\nleg odometry). The key novelty of the proposed filter lies in that its process\nmodel meets the group affine property while the filter explicitly addresses the\nIMU placement error by formulating its stochastic process model as Brownian\nmotions and incorporating the error in the leg odometry. Although the\nmeasurement model is imperfect (i.e., it does not possess an invariant\nobservation form) and thus its linearization relies on the state estimate,\nexperimental results demonstrate fast convergence of the proposed filter\n(within 0.2 seconds) during squatting motions even under significant IMU\nplacement inaccuracy and initial estimation errors.",
    "descriptor": "\nComments: 7 pages, 6 figures, submitted to American Control Conference (ACC)\n",
    "authors": [
      "Zenan Zhu",
      "Seyed Mostafa Rezayat Sorkhabadi",
      "Yan Gu",
      "Wenlong Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10236"
  },
  {
    "id": "arXiv:2205.10237",
    "title": "M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database",
    "abstract": "The emotional state of a speaker can be influenced by many different factors\nin dialogues, such as dialogue scene, dialogue topic, and interlocutor\nstimulus. The currently available data resources to support such multimodal\naffective analysis in dialogues are however limited in scale and diversity. In\nthis work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue\ndataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different\nTV series, a total of 9,082 turns and 24,449 utterances. M3 ED is annotated\nwith 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and\nneutral) at utterance level, and encompasses acoustic, visual, and textual\nmodalities. To the best of our knowledge, M3ED is the first multimodal\nemotional dialogue dataset in Chinese. It is valuable for cross-culture emotion\nanalysis and recognition. We apply several state-of-the-art methods on the M3ED\ndataset to verify the validity and quality of the dataset. We also propose a\ngeneral Multimodal Dialogue-aware Interaction framework, MDI, to model the\ndialogue context for emotion recognition, which achieves comparable performance\nto the state-of-the-art methods on the M3ED. The full dataset and codes are\navailable.",
    "descriptor": "",
    "authors": [
      "Jinming Zhao",
      "Tenggan Zhang",
      "Jingwen Hu",
      "Yuchen Liu",
      "Qin Jin",
      "Xinchao Wang",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10237"
  },
  {
    "id": "arXiv:2205.10238",
    "title": "Visualizing and Explaining Language Models",
    "abstract": "During the last decade, Natural Language Processing has become, after\nComputer Vision, the second field of Artificial Intelligence that was massively\nchanged by the advent of Deep Learning. Regardless of the architecture, the\nlanguage models of the day need to be able to process or generate text, as well\nas predict missing words, sentences or relations depending on the task. Due to\ntheir black-box nature, such models are difficult to interpret and explain to\nthird parties. Visualization is often the bridge that language model designers\nuse to explain their work, as the coloring of the salient words and phrases,\nclustering or neuron activations can be used to quickly understand the\nunderlying models. This paper showcases the techniques used in some of the most\npopular Deep Learning for NLP visualizations, with a special focus on\ninterpretability and explainability.",
    "descriptor": "",
    "authors": [
      "Adrian M.P. Bra\u015foveanu",
      "R\u0103zvan Andonie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10238"
  },
  {
    "id": "arXiv:2205.10239",
    "title": "AGA: An Accelerated Greedy Additional Algorithm for Test Case  Prioritization",
    "abstract": "In recent years, many test case prioritization (TCP) techniques have been\nproposed to speed up the process of fault detection. However, little work has\ntaken the efficiency problem of these techniques into account. In this paper,\nwe target the Greedy Additional (GA) algorithm, which has been widely\nrecognized to be effective but less efficient, and try to improve its\nefficiency while preserving effectiveness. In our Accelerated GA (AGA)\nalgorithm, we use some extra data structures to reduce redundant data accesses\nin the GA algorithm and thus the time complexity is reduced from\n$\\mathcal{O}(m^2n)$ to $\\mathcal{O}(kmn)$ when $n > m$, where $m$ is the number\nof test cases, $n$ is the number of program elements, and $k$ is the iteration\nnumber. Moreover, we observe the impact of iteration numbers on prioritization\nefficiency on our dataset and propose to use a specific iteration number in the\nAGA algorithm to further improve the efficiency. We conducted experiments on 55\nopen-source subjects. In particular, we implemented each TCP algorithm with two\nkinds of widely-used input formats, adjacency matrix and adjacency list. Since\na TCP algorithm with adjacency matrix is less efficient than the algorithm with\nadjacency list, the result analysis is mainly conducted based on TCP algorithms\nwith adjacency list. The results show that AGA achieves 5.95X speedup ratio\nover GA on average, while it achieves the same average effectiveness as GA in\nterms of Average Percentage of Fault Detected (APFD). Moreover, we conducted an\nindustrial case study on 22 subjects, collected from Baidu, and find that the\naverage speedup ratio of AGA over GA is 44.27X, which indicates the practical\nusage of AGA in real-world scenarios.",
    "descriptor": "\nComments: IEEE Transactions on Software Engineering, 2021\n",
    "authors": [
      "Feng Li",
      "Jianyi Zhou",
      "Yinzhu Li",
      "Dan Hao",
      "Lu Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10239"
  },
  {
    "id": "arXiv:2205.10241",
    "title": "Arbitrary high-order structure-preserving schemes for the generalized  Rosenau-type equation",
    "abstract": "In this paper, we are concerned with arbitrarily high-order\nmomentum-preserving and energy-preserving schemes for solving the generalized\nRosenau-type equation, respectively. The derivation of the momentum-preserving\nschemes is made within the symplectic Runge-Kutta method, coupled with the\nstandard Fourier pseudo-spectral method in space. Unlike the\nmomentum-preserving scheme, the energy-preserving one relies on the use of the\nquadratic auxiliary variable approach and the symplectic Runge-Kutta method, as\nwell as the standard Fourier pseudo-spectral method. Extensive numerical tests\nand comparisons are also addressed to illustrate the performance of the\nproposed schemes.",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Chaolong Jiang",
      "Xu Qian",
      "Songhe Song",
      "Chenxuan Zheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10241"
  },
  {
    "id": "arXiv:2205.10242",
    "title": "EXODUS: Stable and Efficient Training of Spiking Neural Networks",
    "abstract": "Spiking Neural Networks (SNNs) are gaining significant traction in machine\nlearning tasks where energy-efficiency is of utmost importance. Training such\nnetworks using the state-of-the-art back-propagation through time (BPTT) is,\nhowever, very time-consuming. Previous work by Shrestha and Orchard [2018]\nemploys an efficient GPU-accelerated back-propagation algorithm called SLAYER,\nwhich speeds up training considerably. SLAYER, however, does not take into\naccount the neuron reset mechanism while computing the gradients, which we\nargue to be the source of numerical instability. To counteract this, SLAYER\nintroduces a gradient scale hyperparameter across layers, which needs manual\ntuning. In this paper, (i) we modify SLAYER and design an algorithm called\nEXODUS, that accounts for the neuron reset mechanism and applies the Implicit\nFunction Theorem (IFT) to calculate the correct gradients (equivalent to those\ncomputed by BPTT), (ii) we eliminate the need for ad-hoc scaling of gradients,\nthus, reducing the training complexity tremendously, (iii) we demonstrate, via\ncomputer simulations, that EXODUS is numerically stable and achieves a\ncomparable or better performance than SLAYER especially in various tasks with\nSNNs that rely on temporal features. Our code is available at\nhttps://github.com/synsense/sinabs-exodus.",
    "descriptor": "",
    "authors": [
      "Felix Christian Bauer",
      "Gregor Lenz",
      "Saeid Haghighatshoar",
      "Sadique Sheik"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10242"
  },
  {
    "id": "arXiv:2205.10247",
    "title": "SADAM: Stochastic Adam, A Stochastic Operator for First-Order  Gradient-based Optimizer",
    "abstract": "In this work, to efficiently help escape the stationary and saddle points, we\npropose, analyze, and generalize a stochastic strategy performed as an operator\nfor a first-order gradient descent algorithm in order to increase the target\naccuracy and reduce time consumption. Unlike existing algorithms, the proposed\nstochastic the strategy does not require any batches and sampling techniques,\nenabling efficient implementation and maintaining the initial first-order\noptimizer's convergence rate, but provides an incomparable improvement of\ntarget accuracy when optimizing the target functions. In short, the proposed\nstrategy is generalized, applied to Adam, and validated via the decomposition\nof biomedical signals using Deep Matrix Fitting and another four peer\noptimizers. The validation results show that the proposed random strategy can\nbe easily generalized for first-order optimizers and efficiently improve the\ntarget accuracy.",
    "descriptor": "\nComments: 9 pages, 4 figures, an advanced first-order optimizer\n",
    "authors": [
      "Wei Zhang",
      "Yu Bao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10247"
  },
  {
    "id": "arXiv:2205.10249",
    "title": "Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR  Prediction",
    "abstract": "Rich user behavior data has been proven to be of great value for\nClick-Through Rate (CTR) prediction applications, especially in industrial\nrecommender, search, or advertising systems. However, it's non-trivial for\nreal-world systems to make full use of long-term user behaviors due to the\nstrict requirements of online serving time. Most previous works adopt the\nretrieval-based strategy, where a small number of user behaviors are retrieved\nfirst for subsequent attention. However, the retrieval-based methods are\nsub-optimal and would cause more or less information losses, and it's difficult\nto balance the effectiveness and efficiency of the retrieval algorithm.\nIn this paper, we propose \\textbf{SDIM} (\\textbf{S}ampling-based\n\\textbf{D}eep \\textbf{I}nterest \\textbf{M}odeling), a simple yet effective\nsampling-based end-to-end approach for modeling long-term user behaviors. We\nsample from multiple hash functions to generate hash signatures of the\ncandidate item and each item in the user behavior sequence, and obtain the user\ninterest by directly gathering behavior items associated with the candidate\nitem with the same hash signature. We show theoretically and experimentally\nthat the proposed method performs on par with standard attention-based models\non modeling long-term user behaviors, while being sizable times faster. We also\nintroduce the deployment of SDIM in our system. Specifically, we decouple the\nbehavior sequence hashing, which is the most time-consuming part, from the CTR\nmodel by designing a separate module named BSE (behavior Sequence Encoding).\nBSE is latency-free for the CTR server, enabling us to model extremely long\nuser behaviors. Both offline and online experiments are conducted to\ndemonstrate the effectiveness of SDIM. SDIM now has been deployed online in the\nsearch system of Meituan APP.",
    "descriptor": "\nComments: Under review, 11 pages\n",
    "authors": [
      "Yue Cao",
      "XiaoJiang Zhou",
      "Jiaqi Feng",
      "Peihao Huang",
      "Yao Xiao",
      "Dayao Chen",
      "Sheng Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10249"
  },
  {
    "id": "arXiv:2205.10250",
    "title": "Explanatory machine learning for sequential human teaching",
    "abstract": "The topic of comprehensibility of machine-learned theories has recently drawn\nincreasing attention. Inductive Logic Programming (ILP) uses logic programming\nto derive logic theories from small data based on abduction and induction\ntechniques. Learned theories are represented in the form of rules as\ndeclarative descriptions of obtained knowledge. In earlier work, the authors\nprovided the first evidence of a measurable increase in human comprehension\nbased on machine-learned logic rules for simple classification tasks. In a\nlater study, it was found that the presentation of machine-learned explanations\nto humans can produce both beneficial and harmful effects in the context of\ngame learning. We continue our investigation of comprehensibility by examining\nthe effects of the ordering of concept presentations on human comprehension. In\nthis work, we examine the explanatory effects of curriculum order and the\npresence of machine-learned explanations for sequential problem-solving. We\nshow that 1) there exist tasks A and B such that learning A before B has a\nbetter human comprehension with respect to learning B before A and 2) there\nexist tasks A and B such that the presence of explanations when learning A\ncontributes to improved human comprehension when subsequently learning B. We\npropose a framework for the effects of sequential teaching on comprehension\nbased on an existing definition of comprehensibility and provide evidence for\nsupport from data collected in human trials. Empirical results show that\nsequential teaching of concepts with increasing complexity a) has a beneficial\neffect on human comprehension and b) leads to human re-discovery of\ndivide-and-conquer problem-solving strategies, and c) studying machine-learned\nexplanations allows adaptations of human problem-solving strategy with better\nperformance.",
    "descriptor": "\nComments: Submitted to the International Joint Conference on Learning & Reasoning (IJCLR) 2022\n",
    "authors": [
      "Lun Ai",
      "Johannes Langer",
      "Stephen H. Muggleton",
      "Ute Schmid"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10250"
  },
  {
    "id": "arXiv:2205.10254",
    "title": "A Demographic Attribute Guided Approach to Age Estimation",
    "abstract": "Face-based age estimation has attracted enormous attention due to wide\napplications to public security surveillance, human-computer interaction, etc.\nWith vigorous development of deep learning, age estimation based on deep neural\nnetwork has become the mainstream practice. However, seeking a more suitable\nproblem paradigm for age change characteristics, designing the corresponding\nloss function and designing a more effective feature extraction module still\nneeds to be studied. What is more, change of face age is also related to\ndemographic attributes such as ethnicity and gender, and the dynamics of\ndifferent age groups is also quite different. This problem has so far not been\npaid enough attention to. How to use demographic attribute information to\nimprove the performance of age estimation remains to be further explored. In\nlight of these issues, this research makes full use of auxiliary information of\nface attributes and proposes a new age estimation approach with an attribute\nguidance module. We first design a multi-scale attention residual convolution\nunit (MARCU) to extract robust facial features other than simply using other\nstandard feature modules such as VGG and ResNet. Then, after being especially\ntreated through full connection (FC) layers, the facial demographic attributes\nare weight-summed by 1*1 convolutional layer and eventually merged with the age\nfeatures by a global FC layer. Lastly, we propose a new error compression\nranking (ECR) loss to better converge the age regression value. Experimental\nresults on three public datasets of UTKFace, LAP2016 and Morph show that our\nproposed approach achieves superior performance compared to other\nstate-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Zhicheng Cao",
      "Kaituo Zhang",
      "Liaojun Pang",
      "Heng Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.10254"
  },
  {
    "id": "arXiv:2205.10255",
    "title": "Tower: Data Structures in Quantum Superposition",
    "abstract": "Emerging quantum algorithms for problems such as element distinctness, subset\nsum, and closest pair demonstrate computational advantages by relying on\nabstract data structures. Practically realizing such an algorithm as a program\nfor a quantum computer requires an efficient implementation of the data\nstructure whose operations correspond to unitary operators that manipulate\nquantum superpositions of data.\nTo correctly operate in superposition, an implementation must satisfy three\nproperties -- reversibility, history independence, and bounded-time execution.\nStandard implementations, such as representing an abstract set as a hash table,\nfail these properties, calling for tools to develop specialized\nimplementations.\nIn this work, we present Core Tower, the first language for quantum\nprogramming with random-access memory. Core Tower enables the developer to\nimplement data structures as pointer-based, linked data. It features a\nreversible semantics enabling every valid program to be translated to a unitary\nquantum circuit.\nWe present Boson, the first memory allocator that supports reversible,\nhistory-independent, and constant-time dynamic memory allocation in quantum\nsuperposition.\nWe also present Tower, a language for quantum programming with inductive data\nstructures. Tower features a type system that bounds all recursion using\nclassical parameters.\nUsing Tower, we implement Ground, the first quantum library of data\nstructures, including lists, stacks, queues, strings, and sets. We provide the\nfirst executable implementation of sets that satisfies all three mandated\nproperties of reversibility, history independence, and bounded-time execution.",
    "descriptor": "\nComments: 24 pages, 15 figures\n",
    "authors": [
      "Charles Yuan",
      "Michael Carbin"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10255"
  },
  {
    "id": "arXiv:2205.10257",
    "title": "Frontrunning Block Attack in PoA Clique: A Case Study",
    "abstract": "As a fundamental technology of decentralized finance (DeFi), blockchain's\nability to maintain a distributed fair ledger is threatened by manipulation of\nblock/transaction order. In this paper, we propose a frontrunning block attack\nagainst the Clique-based Proof of Authority (PoA) algorithms. Our attack can\nfrontrun blocks from honest in-turn sealers by breaking the proper order of\nleader selection. By falsifying the priority parameters (both\n\\textit{difficulty} and \\textit{delay time}), a malicious out-of-turn sealer\ncan always successfully occupy the leader position and produce advantageous\nblocks that may contain profitable transactions. As a typical instance, we\napply our attack to a mature Clique-engined project, HPB (\\$3,058,901, as of\nApril 2022). Experimental results demonstrate the effectiveness and\nfeasibility. Then, we further recommend fixes that make identity checks\neffective. Our investigation and suggestion have been submitted to its official\nteam and got their approval. We believe this work can act as, at least, a\nwarning case for Clique variants to avoid repeating these design mistakes.",
    "descriptor": "\nComments: This work was in part presented at IEEE ICBC 2022\n",
    "authors": [
      "Xinrui Zhang",
      "Qin Wang",
      "Rujia Li",
      "Qi Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10257"
  },
  {
    "id": "arXiv:2205.10259",
    "title": "Random Coordinate Descent for Resource Allocation in Open Multi-Agent  Systems",
    "abstract": "We analyze the distributed random coordinate descent algorithm for solving\nseparable resource allocation problems in the context of an open multi-agent\nsystem, where agents can be replaced during the process. First, we establish\nthe linear convergence of the algorithm in closed systems, in terms of the\nestimate towards the minimizer, for general graphs and appropriate step-size.\nNext, we estimate the change of the optimal solution after a replacement to\nevaluate its effect on the distance between the estimate and the minimizer.\nFrom these two elements, we derive stability conditions in open systems and\nestablish the linear convergence of the algorithm towards a steady state\nexpected error. Our results allow characterizing the trade-off between speed of\nconvergence and robustness to agent replacements, under the assumptions that\nlocal functions are smooth strongly convex and have their minimizers located in\na given ball.",
    "descriptor": "\nComments: 13 pages, 9 figures, submitted to IEEE Transactions on Automatic Control\n",
    "authors": [
      "Charles Monnoyer de Galland",
      "Renato Vizuete",
      "Julien M. Hendrickx",
      "Elena Panteley",
      "Paolo Frasca"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.10259"
  },
  {
    "id": "arXiv:2205.10264",
    "title": "DEMAND: Deep Matrix Approximately NonlinearDecomposition to Identify  Meta, Canonical, and Sub-Spatial Pattern of functional Magnetic Resonance  Imaging in the Human Brain",
    "abstract": "Deep Neural Networks (DNNs) have already become a crucial computational\napproach to revealing the spatial patterns in the human brain; however, there\nare three major shortcomings in utilizing DNNs to detect the spatial patterns\nin functional Magnetic Resonance Signals: 1). It is a fully connected\narchitecture that increases the complexity of network structures that is\ndifficult to optimize and vulnerable to overfitting; 2). The requirement of\nlarge training samples results in erasing the individual/minor patterns in\nfeature extraction; 3). The hyperparameters are required to be tuned manually,\nwhich is time-consuming. Therefore, we propose a novel deep nonlinear matrix\nfactorization named Deep Matrix Approximately Nonlinear Decomposition (DEMAND)\nin this work to take advantage of the shallow linear model, e.g., Sparse\nDictionary Learning (SDL) and DNNs. At first, the proposed DEMAND employs a\nnon-fully connected and multilayer-stacked architecture that is easier to be\noptimized compared with canonical DNNs; furthermore, due to the efficient\narchitecture, training DEMAND can avoid overfitting and enables the recognition\nof individual/minor features based on a small dataset such as an individual\ndata; finally, a novel rank estimator technique is introduced to tune all\nhyperparameters of DEMAND automatically. Moreover, the proposed DEMAND is\nvalidated by four other peer methodologies via real functional Magnetic\nResonance Imaging data in the human brain. In short, the validation results\ndemonstrate that DEMAND can reveal the reproducible meta, canonical, and\nsub-spatial features of the human brain more efficiently than other peer\nmethodologies.",
    "descriptor": "\nComments: 10 pages, 6 figures, an advanced deep nonlinear matrix factorization technique\n",
    "authors": [
      "Wei Zhang",
      "Yu Bao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10264"
  },
  {
    "id": "arXiv:2205.10266",
    "title": "Analysis of Co-Laughter Gesture Relationship on RGB videos in Dyadic  Conversation Contex",
    "abstract": "The development of virtual agents has enabled human-avatar interactions to\nbecome increasingly rich and varied. Moreover, an expressive virtual agent i.e.\nthat mimics the natural expression of emotions, enhances social interaction\nbetween a user (human) and an agent (intelligent machine). The set of\nnon-verbal behaviors of a virtual character is, therefore, an important\ncomponent in the context of human-machine interaction. Laughter is not just an\naudio signal, but an intrinsic relationship of multimodal non-verbal\ncommunication, in addition to audio, it includes facial expressions and body\nmovements. Motion analysis often relies on a relevant motion capture dataset,\nbut the main issue is that the acquisition of such a dataset is expensive and\ntime-consuming. This work studies the relationship between laughter and body\nmovements in dyadic conversations. The body movements were extracted from\nvideos using deep learning based pose estimator model. We found that, in the\nexplored NDC-ME dataset, a single statistical feature (i.e, the maximum value,\nor the maximum of Fourier transform) of a joint movement weakly correlates with\nlaughter intensity by 30%. However, we did not find a direct correlation\nbetween audio features and body movements. We discuss about the challenges to\nuse such dataset for the audio-driven co-laughter motion synthesis task.",
    "descriptor": "\nComments: 5 pages, 2 figures, 2 tables\n",
    "authors": [
      "Hugo Bohy",
      "Ahmad Hammoudeh",
      "Antoine Maiorca",
      "St\u00e9phane Dupont",
      "Thierry Dutoit"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10266"
  },
  {
    "id": "arXiv:2205.10268",
    "title": "B-cos Networks: Alignment is All We Need for Interpretability",
    "abstract": "We present a new direction for increasing the interpretability of deep neural\nnetworks (DNNs) by promoting weight-input alignment during training. For this,\nwe propose to replace the linear transforms in DNNs by our B-cos transform. As\nwe show, a sequence (network) of such transforms induces a single linear\ntransform that faithfully summarises the full model computations. Moreover, the\nB-cos transform introduces alignment pressure on the weights during\noptimisation. As a result, those induced linear transforms become highly\ninterpretable and align with task-relevant features. Importantly, the B-cos\ntransform is designed to be compatible with existing architectures and we show\nthat it can easily be integrated into common models such as VGGs, ResNets,\nInceptionNets, and DenseNets, whilst maintaining similar performance on\nImageNet. The resulting explanations are of high visual quality and perform\nwell under quantitative metrics for interpretability. Code available at\nhttps://www.github.com/moboehle/B-cos.",
    "descriptor": "",
    "authors": [
      "Moritz B\u00f6hle",
      "Mario Fritz",
      "Bernt Schiele"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10268"
  },
  {
    "id": "arXiv:2205.10271",
    "title": "Compression ensembles quantify aesthetic complexity and the evolution of  visual art",
    "abstract": "The quantification of visual aesthetics and complexity have a long history,\nthe latter previously operationalized via the application of compression\nalgorithms. Here we generalize and extend the compression approach beyond\nsimple complexity measures to quantify algorithmic distance in historical and\ncontemporary visual media. The proposed \"ensemble\" approach works by\ncompressing a large number of transformed versions of a given input image,\nresulting in a vector of associated compression ratios. This approach is more\nefficient than other compression-based algorithmic distances, and is\nparticularly suited for the quantitative analysis of visual artifacts, because\nhuman creative processes can be understood as algorithms in the broadest sense.\nUnlike comparable image embedding methods using machine learning, our approach\nis fully explainable through the transformations. We demonstrate that the\nmethod is cognitively plausible and fit for purpose by evaluating it against\nhuman complexity judgments, and on automated detection tasks of authorship and\nstyle. We show how the approach can be used to reveal and quantify trends in\nart historical data, both on the scale of centuries and in rapidly evolving\ncontemporary NFT art markets. We further quantify temporal resemblance to\ndisambiguate artists outside the documented mainstream from those who are\ndeeply embedded in Zeitgeist. Finally, we note that compression ensembles\nconstitute a quantitative representation of the concept of visual family\nresemblance, as distinct sets of dimensions correspond to shared visual\ncharacteristics otherwise hard to pin down. Our approach provides a new\nperspective for the study of visual art, algorithmic image analysis, and\nquantitative aesthetics more generally.",
    "descriptor": "",
    "authors": [
      "Andres Karjus",
      "Mar Canet Sol\u00e0",
      "Tillmann Ohm",
      "Sebastian E. Ahnert",
      "Maximilian Schich"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10271"
  },
  {
    "id": "arXiv:2205.10272",
    "title": "Salient Skin Lesion Segmentation via Dilated Scale-Wise Feature Fusion  Network",
    "abstract": "Skin lesion detection in dermoscopic images is essential in the accurate and\nearly diagnosis of skin cancer by a computerized apparatus. Current skin lesion\nsegmentation approaches show poor performance in challenging circumstances such\nas indistinct lesion boundaries, low contrast between the lesion and the\nsurrounding area, or heterogeneous background that causes over/under\nsegmentation of the skin lesion. To accurately recognize the lesion from the\nneighboring regions, we propose a dilated scale-wise feature fusion network\nbased on convolution factorization. Our network is designed to simultaneously\nextract features at different scales which are systematically fused for better\ndetection. The proposed model has satisfactory accuracy and efficiency. Various\nexperiments for lesion segmentation are performed along with comparisons with\nthe state-of-the-art models. Our proposed model consistently showcases\nstate-of-the-art results.",
    "descriptor": "",
    "authors": [
      "Pourya Shamsolmoali",
      "Masoumeh Zareapoor",
      "Eric Granger",
      "Huiyu Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10272"
  },
  {
    "id": "arXiv:2205.10275",
    "title": "Stochastic MPC with robustness to bounded parametric uncertainty",
    "abstract": "The performance of model-based control techniques strongly depends on the\nquality of the employed dynamics model. If strong guarantees are desired, it is\ntherefore common to robustly treat all possible sources of uncertainty, such as\nmodel inaccuracies or external disturbances. This, however, can result in\noverly conservative control strategies. In this paper, we present a stochastic\nmodel predictive control approach for discrete-time LTI systems subject to\nbounded parametric uncertainty and potentially unbounded stochastic additive\nnoise. The proposed scheme makes use of homothetic tubes along the prediction\nhorizon for a robust treatment of parametric uncertainty. Stochastic noise is\nhandled by non-conservatively tightening constraints using the concept of\nprobabilistic reachable sets (PRS). In order to accommodate all possible\nparametric uncertainties, we provide a strategy for generating \"robustified\"\nPRS based only on first and second moments of the noise sequence. In the case\nof quadratic cost functions, and under a further i.i.d. assumption on the noise\ndistribution, we also provide an average asymptotic performance bound for the\nl2-norm of the closed-loop state. Finally, we demonstrate our scheme on both an\nillustrative example, and in a building temperature control problem.",
    "descriptor": "",
    "authors": [
      "Elena Arcari",
      "Andrea Iannelli",
      "Andrea Carron",
      "Melanie N. Zeilinger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10275"
  },
  {
    "id": "arXiv:2205.10277",
    "title": "Loco-Manipulation Planning for Legged Robots: Offline and Online  Strategies",
    "abstract": "The deployment of robots within realistic environments requires the\ncapability to plan and refine the loco-manipulation trajectories on the fly to\navoid unexpected interactions with a dynamic environment. This extended\nabstract provides a pipeline to offline plan a configuration space global\ntrajectory based on a randomized strategy, and to online locally refine it\ndepending on any change of the dynamic environment and the robot state. The\noffline planner directly plans in the contact space, and additionally seeks for\nwhole-body feasible configurations compliant with the sampled contact states.\nThe planned trajectory, made by a discrete set of contacts and configurations,\ncan be seen as a graph and it can be online refined during the execution of the\nglobal trajectory. The online refinement is carried out by a graph optimization\nplanner exploiting visual information. It locally acts on the global initial\nplan to account for possible changes in the environment. While the offline\nplanner is a concluded work, tested on the humanoid COMAN+, the online local\nplanner is still a work-in-progress which has been tested on a reduced model of\nthe CENTAURO robot to avoid dynamic and static obstacles interfering with a\nwheeled motion task. Both the COMAN+ and the CENTAURO robots have been designed\nat the Italian Institute of Technology (IIT).",
    "descriptor": "",
    "authors": [
      "Luca Rossini",
      "Paolo Ferrari",
      "Francesco Ruscelli",
      "Arturo Laurenzi",
      "Nikos G. Tsagarakis",
      "Enrico Mingo Hoffman"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10277"
  },
  {
    "id": "arXiv:2205.10279",
    "title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative  Priors",
    "abstract": "Deep learning is increasingly moving towards a transfer learning paradigm\nwhereby large foundation models are fine-tuned on downstream tasks, starting\nfrom an initialization learned on the source task. But an initialization\ncontains relatively little information about the source task. Instead, we show\nthat we can learn highly informative posteriors from the source task, through\nsupervised or self-supervised approaches, which then serve as the basis for\npriors that modify the whole loss surface on the downstream task. This simple\nmodular approach enables significant performance gains and more data-efficient\nlearning on a variety of downstream classification and segmentation tasks,\nserving as a drop-in replacement for standard pre-training strategies. These\nhighly informative priors also can be saved for future use, similar to\npre-trained weights, and stand in contrast to the zero-mean isotropic\nuninformative priors that are typically used in Bayesian deep learning.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Ravid Shwartz-Ziv",
      "Micah Goldblum",
      "Hossein Souri",
      "Sanyam Kapoor",
      "Chen Zhu",
      "Yann LeCun",
      "Andrew Gordon Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10279"
  },
  {
    "id": "arXiv:2205.10282",
    "title": "Heterformer: A Transformer Architecture for Node Representation Learning  on Heterogeneous Text-Rich Networks",
    "abstract": "We study node representation learning on heterogeneous text-rich networks,\nwhere nodes and edges are multi-typed and some types of nodes are associated\nwith text information. Although recent studies on graph neural networks (GNNs)\nand pretrained language models (PLMs) have demonstrated their power in encoding\nnetwork and text signals, respectively, less focus has been given to delicately\ncoupling these two types of models on heterogeneous text-rich networks.\nSpecifically, existing GNNs rarely model text in each node in a contextualized\nway; existing PLMs can hardly be applied to characterize graph structures due\nto their sequence architecture. In this paper, we propose Heterformer, a\nHeterogeneous GNN-nested transformer that blends GNNs and PLMs into a unified\nmodel. Different from previous \"cascaded architectures\" that directly add GNN\nlayers upon a PLM, our Heterformer alternately stacks two modules - a\ngraph-attention-based neighbor aggregation module and a transformer-based text\nand neighbor joint encoding module - to facilitate thorough mutual enhancement\nbetween network and text signals. Meanwhile, Heterformer is capable of\ncharacterizing network heterogeneity and nodes without text information.\nComprehensive experiments on three large-scale datasets from different domains\ndemonstrate the superiority of Heterformer over state-of-the-art baselines in\nlink prediction, transductive/inductive node classification, node clustering,\nand semantics-based retrieval.",
    "descriptor": "",
    "authors": [
      "Bowen Jin",
      "Yu Zhang",
      "Qi Zhu",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10282"
  },
  {
    "id": "arXiv:2205.10287",
    "title": "On the SDEs and Scaling Rules for Adaptive Gradient Algorithms",
    "abstract": "Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential\nEquation (SDE) has allowed researchers to enjoy the benefits of studying a\ncontinuous optimization trajectory while carefully preserving the stochasticity\nof SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam,\nhas been challenging because there were no rigorously proven SDE approximations\nfor these methods. This paper derives the SDE approximations for RMSprop and\nAdam, giving theoretical guarantees of their correctness as well as\nexperimental validation of their applicability to common large-scaling vision\nand language settings. A key practical result is the derivation of a\n$\\textit{square root scaling rule}$ to adjust the optimization hyperparameters\nof RMSprop and Adam when changing batch size, and its empirical validation in\ndeep learning settings.",
    "descriptor": "",
    "authors": [
      "Sadhika Malladi",
      "Kaifeng Lyu",
      "Abhishek Panigrahi",
      "Sanjeev Arora"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10287"
  },
  {
    "id": "arXiv:2205.10290",
    "title": "Semi-Blind Joint Channel and Symbol Estimation for IRS-Assisted MIMO  Systems",
    "abstract": "Intelligent reflecting surface (IRS) is a promising technology for the 6th\ngeneration of wireless systems, realizing the smart radio environment concept.\nIn this paper, we present a novel tensor-based receiver for IRS-assisted\nmultiple-input multiple-output communications capable of jointly estimating the\nchannels and the transmitted data streams in a semi-blind fashion. Assuming a\nfully passive IRS architecture and introducing a simple space-time coding\nscheme at the transmitter, the received signal model can be advantageously\nbuilt using the PARATUCK tensor model, which can be seen as a hybrid of\nparallel factor analysis and Tucker models. Exploiting the algebraic structure\nof the PARATUCK tensor model, a semi-blind receiver is derived. The proposed\nreceiver is based on a trilinear alternating least squares method that\niteratively estimates the two involved - IRS- base station and user\nterminal-IRS-communication channels and the transmitted symbol matrix. We\ndiscuss identifiability conditions that ensure the joint semi-blind recovery of\nthe involved channel and symbol matrices, and propose a joint design of the\ncoding and IRS reflection matrices to optimize the receiver performance. For\nthe proposed semi-blind receiver, the derivation of the expected Cram\\'er-Rao\nlower bound is also provided. A numerical performance evaluation of the\nproposed receiver design corroborates its superior performance in terms of the\nnormalized mean squared error of the estimated channels and the achieved symbol\nerror rate.",
    "descriptor": "",
    "authors": [
      "Gilderlan Tavares de Ara\u00fajo",
      "Andr\u00e9 Lima F\u00e9rrer de Almeida",
      "R\u00e9my Boyer",
      "G\u00e1bor Fodor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10290"
  },
  {
    "id": "arXiv:2205.10292",
    "title": "Vulnerability Analysis and Performance Enhancement of Authentication  Protocol in Dynamic Wireless Power Transfer Systems",
    "abstract": "Recent advancements in wireless charging technology, as well as the\npossibility of utilizing it in the Electric Vehicle (EV) domain for dynamic\ncharging solutions, have fueled the demand for a secure and usable protocol in\nthe Dynamic Wireless Power Transfer (DWPT) technology. The DWPT must operate in\nthe presence of malicious adversaries that can undermine the charging process\nand harm the customer service quality, while preserving the privacy of the\nusers. Recently, it was shown that the DWPT system is susceptible to\nadversarial attacks, including replay, denial-of-service and free-riding\nattacks, which can lead to the adversary blocking the authorized user from\ncharging, enabling free charging for free riders and exploiting the location\nprivacy of the customers. In this paper, we study the current State-Of-The-Art\n(SOTA) authentication protocols and make the following two contributions: a) we\nshow that the SOTA is vulnerable to the tracking of the user activity and b) we\npropose an enhanced authentication protocol that eliminates the vulnerability\nwhile providing improved efficiency compared to the SOTA authentication\nprotocols. By adopting authentication messages based only on exclusive OR\noperations, hashing, and hash chains, we optimize the protocol to achieve a\ncomplexity that varies linearly with the number of charging pads, providing\nimproved scalability. Compared to SOTA, the proposed scheme has a performance\ngain in the computational cost of around 90% on average for each pad.",
    "descriptor": "\nComments: 16 pages, conference\n",
    "authors": [
      "Tommaso Bianchi",
      "Surudhi Asokraj",
      "Alessandro Brighente",
      "Mauro Conti",
      "Radha Poovendran"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10292"
  },
  {
    "id": "arXiv:2205.10293",
    "title": "Delator: Automatic Detection of Money Laundering Evidence on Transaction  Graphs via Neural Networks",
    "abstract": "Money laundering is one of the most relevant criminal activities today, due\nto its potential to cause massive financial losses to governments, banks, etc.\nWe propose DELATOR, a new CAAT (computer-assisted audit technology) to detect\nmoney laundering activities based on neural network models that encode bank\ntransfers as a large-scale temporal graph. In collaboration with a Brazilian\nbank, we design and apply an evaluation strategy to quantify DELATOR's\nperformance on historic data comprising millions of clients. DELATOR\noutperforms an off-the-shelf solution from Amazon AWS by 18.9% with respect to\nAUC. We conducted real experiments that led to discovery of 8 new suspicious\namong 100 analyzed cases, which would have been reported to the authorities\nunder the current criteria.",
    "descriptor": "\nComments: in Portuguese language. Accepted for publication in the 11th Brazilian Workshop on Social Network Analysis and Mining (BraSNAM)\n",
    "authors": [
      "Henrique S. Assump\u00e7\u00e3o",
      "Fabr\u00edcio Souza",
      "Leandro Lacerda Campos",
      "Vin\u00edcius T. de Castro Pires",
      "Paulo M. Laurentys de Almeida",
      "Fabricio Murai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10293"
  },
  {
    "id": "arXiv:2205.10295",
    "title": "Linking sanctions to norms in practice",
    "abstract": "Within social simulation, we often want agents to interact both with larger\nsystems of norms, as well as respond to their own and other agents norm\nviolations. However, there are currently no norm specifications that allow us\nto interact with all of these components. To address this issue, this paper\nintroduces the concept of violation modalities in CTL. These modalities do not\nonly allow us to keep track of violations, but also allow us to define the\nusual deontic operators. On top of this, they give us a convenient way of\nlinking together various different norms, and allow us to reason about norms\nwith repeated violations and obligations. We will discuss various properties of\nthe modalities and the deontic operators, and will also discuss some ways in\nwhich this formalization can guide an implementation of normative systems.",
    "descriptor": "",
    "authors": [
      "Ren\u00e9 Mellema",
      "Frank Dignum"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.10295"
  },
  {
    "id": "arXiv:2205.10298",
    "title": "Low-cost Relevance Generation and Evaluation Metrics for Entity  Resolution in AI",
    "abstract": "Entity Resolution (ER) in voice assistants is a prime component during run\ntime that resolves entities in users request to real world entities. ER\ninvolves two major functionalities 1. Relevance generation and 2. Ranking. In\nthis paper we propose a low cost relevance generation framework by generating\nfeatures using customer implicit and explicit feedback signals. The generated\nrelevance datasets can serve as test sets to measure ER performance. We also\nintroduce a set of metrics that accurately measures the performance of ER\nsystems in various dimensions. They provide great interpretability to deep dive\nand identifying root cause of ER issues, whether the problem is in relevance\ngeneration or ranking.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Venkat Varada",
      "Mina Ghashami",
      "Jitesh Mehta",
      "Haotian Jiang",
      "Kurtis Voris"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10298"
  },
  {
    "id": "arXiv:2205.10301",
    "title": "Expander Decomposition with Fewer Inter-Cluster Edges Using a Spectral  Cut Player",
    "abstract": "A $(\\phi,\\epsilon)$-Expander-decomposition of a graph $G$ is a partition of\n$V$ into clusters $V_1,\\ldots,V_k$ with conductance $\\Phi(G[V_i]) \\ge \\phi$,\nsuch that there are at most $\\epsilon \\phi m$ inter-cluster edges. We consider\nthe problem of computing such a decomposition for a given $G$ and expansion\nparameter $\\phi$, while minimizing $\\epsilon$. Saranurak and Wang [SW19] gave a\nrandomized $O(m \\log^4m/\\phi)$ algorithm for computing a $(\\phi, \\log^3\nn)$-expander decomposition. As a main building block, [SW19] use an adaptation\nof the algorithm of R\\\"{a}cke et al. [RST14] for computing an approximate\nbalanced sparse cut. Both algorithms rely on the cut-matching game of Khandekar\net al. [KRV09]\nOrecchia et al. [OSVV08], using spectral analysis, improved upon [KRV09] by\ngiving a fast algorithm that computes a sparse cut with better approximation\nguarantee. Using the technique of [OSVV08] for computing expander\ndecompositions or balanced cuts [RST14, SW19], encounters many hurdles since\nthe graph structure constantly changes, making it difficult to perform spectral\nanalysis.\nIn this paper, we manage to exploit the technique of [OSVV08] to compute an\nexpander decomposition, hence improving the result by Saranurak and Wang\n[SW19]. Specifically, we give a randomized algorithm for computing a $(\\phi,\n\\log^2 {n})$-expander decomposition of a graph, in $O(m\\log^7 m + \\frac{m\n\\log^4m}{\\phi})$ time. Our new result is achieved by using a novel combination\nof a symmetric version of the potential functions of [OSVV08, RST14, SW19] with\na new variation of Cheeger's inequality for the notion of near-expansion.",
    "descriptor": "\nComments: 46 pages\n",
    "authors": [
      "Daniel Agassy",
      "Dani Dorfman",
      "Haim Kaplan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.10301"
  },
  {
    "id": "arXiv:2205.10302",
    "title": "Individual Fairness in Prophet Inequalities",
    "abstract": "Prophet inequalities are performance guarantees for online algorithms (a.k.a.\nstopping rules) solving the following \"hiring problem\": a decision maker\nsequentially inspects candidates whose values are independent random numbers\nand is asked to hire at most one candidate by selecting it before inspecting\nthe values of future candidates in the sequence. A classic result in optimal\nstopping theory asserts that there exist stopping rules guaranteeing that the\ndecision maker will hire a candidate whose expected value is at least half as\ngood as the expected value of the candidate hired by a \"prophet\", i.e. one who\nhas simultaneous access to the realizations of all candidates' values.\nSuch stopping rules have provably good performance but might treat individual\ncandidates unfairly in a number of different ways. In this work we identify two\ntypes of individual fairness that might be desirable in optimal stopping\nproblems. We call them identity-independent fairness (IIF) and time-independent\nfairness (TIF) and give precise definitions in the context of the hiring\nproblem. We give polynomial-time algorithms for finding the optimal IIF/TIF\nstopping rules for a given instance with discrete support and we manage to\nrecover a prophet inequality with factor $1/2$ when the decision maker's\nstopping rule is required to satisfy both fairness properties while the prophet\nis unconstrained. We also explore worst-case ratios between optimal selection\nrules in the presence vs. absence of individual fairness constraints, in both\nthe online and offline settings. Finally, we consider a framework in which the\ndecision maker doesn't know the distributions of candidates' values but has\naccess to independent samples from each distribution. We provide\nconstant-competitive IIF/TIF algorithms using one sample per distribution in\nthe offline setting and two samples per distribution in the online setting.",
    "descriptor": "\nComments: 25 pages, 1 figure, to appear as extended abstract in the proceedings of Economics and Computation (EC) '22\n",
    "authors": [
      "Makis Arsenis",
      "Robert Kleinberg"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.10302"
  },
  {
    "id": "arXiv:2205.10309",
    "title": "A Fully Implicit Method for Robust Frictional Contact Handling in  Elastic Rods",
    "abstract": "Accurate frictional contact is critical in simulating the assembly of\nrod-like structures in the practical world, such as knots, hairs, flagella, and\nmore. Due to their high geometric nonlinearity and elasticity, rod-on-rod\ncontact remains a challenging problem tackled by researchers in both\ncomputational mechanics and computer graphics. Typically, frictional contact is\nregarded as constraints for the equations of motions of a system. Such\nconstraints are often computed independently at every time step in a dynamic\nsimulation, thus slowing down the simulation and possibly introducing numerical\nconvergence issues. This paper proposes a fully implicit penalty-based\nfrictional contact method, Implicit Contact Model (IMC), that efficiently and\nrobustly captures accurate frictional contact responses. We showcase our\nalgorithm's performance for the challenging and novel contact scenario of\nflagella bundling in fluid medium, a significant phenomenon in biology that\nmotivates novel engineering applications in soft robotics. In addition to this,\nwe offer a side-by-side comparison with Incremental Potential Contact (IPC), a\nstate-of-the-art contact handling algorithm. We show that IMC possesses\ncomparable accuracy to IPC while converging at a faster rate for the flagella\nbundling case.",
    "descriptor": "\nComments: A video summarizing this work is available on YouTube: this https URL\n",
    "authors": [
      "Dezhong Tong",
      "Andrew Choi",
      "Jungseock Joo",
      "M. Khalid Jawed"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.10309"
  },
  {
    "id": "arXiv:2205.10312",
    "title": "ClusterEA: Scalable Entity Alignment with Stochastic Training and  Normalized Mini-batch Similarities",
    "abstract": "Entity alignment (EA) aims at finding equivalent entities in different\nknowledge graphs (KGs). Embedding-based approaches have dominated the EA task\nin recent years. Those methods face problems that come from the geometric\nproperties of embedding vectors, including hubness and isolation. To solve\nthese geometric problems, many normalization approaches have been adopted to\nEA. However, the increasing scale of KGs renders it is hard for EA models to\nadopt the normalization processes, thus limiting their usage in real-world\napplications. To tackle this challenge, we present ClusterEA, a general\nframework that is capable of scaling up EA models and enhancing their results\nby leveraging normalization methods on mini-batches with a high entity\nequivalent rate. ClusterEA contains three components to align entities between\nlarge-scale KGs, including stochastic training, ClusterSampler, and\nSparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic\nfashion to produce entity embeddings. Based on the embeddings, a novel\nClusterSampler strategy is proposed for sampling highly overlapped\nmini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes\nlocal and global similarity and then fuses all similarity matrices to obtain\nthe final similarity matrix. Extensive experiments with real-life datasets on\nEA benchmarks offer insight into the proposed framework, and suggest that it is\ncapable of outperforming the state-of-the-art scalable EA framework by up to 8\ntimes in terms of Hits@1.",
    "descriptor": "\nComments: Accepted by ACM SIGKDD 2022 Research Track\n",
    "authors": [
      "Yunjun Gao",
      "Xiaoze Liu",
      "Junyang Wu",
      "Tianyi Li",
      "Pengfei Wang",
      "Lu Chen"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10312"
  },
  {
    "id": "arXiv:2205.10316",
    "title": "Seeking entropy: complex behavior from intrinsic motivation to occupy  action-state path space",
    "abstract": "Intrinsic motivation generates behaviors that do not necessarily lead to\nimmediate reward, but help exploration and learning. Here we show that agents\nhaving the sole goal of maximizing occupancy of future actions and states, that\nis, moving and exploring on the long term, are capable of complex behavior\nwithout any reference to external rewards. We find that action-state path\nentropy is the only measure consistent with additivity and other intuitive\nproperties of expected future action-state path occupancy. We provide\nanalytical expressions that relate the optimal policy with the optimal\nstate-value function, from where we prove uniqueness of the solution of the\nassociated Bellman equation and convergence of our algorithm to the optimal\nstate-value function. Using discrete and continuous state tasks, we show that\n`dancing', hide-and-seek and a basic form of altruistic behavior naturally\nresult from entropy seeking without external rewards. Intrinsically motivated\nagents can objectively determine what states constitute rewards, exploiting\nthem to ultimately maximize action-state path entropy.",
    "descriptor": "",
    "authors": [
      "Jorge Ram\u00edrez-Ruiz",
      "Dmytro Grytskyy",
      "Rub\u00e9n Moreno-Bote"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.10316"
  },
  {
    "id": "arXiv:2205.10320",
    "title": "Nothing makes sense in deep learning, except in the light of evolution",
    "abstract": "Deep Learning (DL) is a surprisingly successful branch of machine learning.\nThe success of DL is usually explained by focusing analysis on a particular\nrecent algorithm and its traits. Instead, we propose that an explanation of the\nsuccess of DL must look at the population of all algorithms in the field and\nhow they have evolved over time. We argue that cultural evolution is a useful\nframework to explain the success of DL. In analogy to biology, we use\n`development' to mean the process converting the pseudocode or text description\nof an algorithm into a fully trained model. This includes writing the\nprogramming code, compiling and running the program, and training the model. If\nall parts of the process don't align well then the resultant model will be\nuseless (if the code runs at all!). This is a constraint. A core component of\nevolutionary developmental biology is the concept of deconstraints -- these are\nmodification to the developmental process that avoid complete failure by\nautomatically accommodating changes in other components. We suggest that many\nimportant innovations in DL, from neural networks themselves to hyperparameter\noptimization and AutoGrad, can be seen as developmental deconstraints. These\ndeconstraints can be very helpful to both the particular algorithm in how it\nhandles challenges in implementation and the overall field of DL in how easy it\nis for new ideas to be generated. We highlight how our perspective can both\nadvance DL and lead to new insights for evolutionary biology.",
    "descriptor": "\nComments: 11 pages, 2 figures, 1 table\n",
    "authors": [
      "Artem Kaznatcheev",
      "Konrad Paul Kording"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2205.10320"
  },
  {
    "id": "arXiv:2205.10325",
    "title": "Classifying Human Activities using Machine Learning and Deep Learning  Techniques",
    "abstract": "Human Activity Recognition (HAR) describes the machines ability to recognize\nhuman actions. Nowadays, most people on earth are health conscious, so people\nare more interested in tracking their daily activities using Smartphones or\nSmart Watches, which can help them manage their daily routines in a healthy\nway. With this objective, Kaggle has conducted a competition to classify 6\ndifferent human activities distinctly based on the inertial signals obtained\nfrom 30 volunteers smartphones. The main challenge in HAR is to overcome the\ndifficulties of separating human activities based on the given data such that\nno two activities overlap. In this experimentation, first, Data visualization\nis done on expert generated features with the help of t distributed Stochastic\nNeighborhood Embedding followed by applying various Machine Learning techniques\nlike Logistic Regression, Linear SVC, Kernel SVM, Decision trees to better\nclassify the 6 distinct human activities. Moreover, Deep Learning techniques\nlike Long Short-Term Memory (LSTM), Bi-Directional LSTM, Recurrent Neural\nNetwork (RNN), and Gated Recurrent Unit (GRU) are trained using raw time series\ndata. Finally, metrics like Accuracy, Confusion matrix, precision and recall\nare used to evaluate the performance of the Machine Learning and Deep Learning\nmodels. Experiment results proved that the Linear Support Vector Classifier in\nmachine learning and Gated Recurrent Unit in Deep Learning provided better\naccuracy for human activity recognition compared to other classifiers.",
    "descriptor": "",
    "authors": [
      "Sanku Satya Uday",
      "Satti Thanuja Pavani",
      "T.Jaya Lakshmi",
      "Rohit Chivukula"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10325"
  },
  {
    "id": "arXiv:2205.10328",
    "title": "Data-driven Distributed Control to Scale EV Integration into Power Grid",
    "abstract": "Electric vehicles (EVs) are finally making their way onto the roads, but the\nchallenges concerning long charging times and impact on congestion of the power\ndistribution grid are still not resolved. Proposed solutions depend on heavy\ncommunication and rigorous computation and mostly need real-time connectivity\nfor optimal operation; thereby, they are not scalable. With the availability of\nhistorical measurement data, EV chargers can take better-informed actions while\nstaying mostly off-line. This study develops a distributed and data-driven\ncongestion detection methodology together with the Additive Increase\nMultiplicative Decrease (AIMD) algorithm to control mass EV charging in a\ndistribution grid. The proposed distributed AIMD algorithm performs very\nclosely to the ideal AIMD in terms of fairness and congestion handling, and its\ncommunication need is significantly low. The results can provide crucial\ninsights on how data can be used to reveal the inner dynamics and structure of\nthe power grid and help develop more advanced data-driven algorithms for grid\nintegrated power electronics control.",
    "descriptor": "\nComments: This paper is accepted for presentation at 2022 IEEE Power & Energy Society General Meeting (PES-GM)\n",
    "authors": [
      "Emin Ucer",
      "Mithat Kisacikoglu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10328"
  },
  {
    "id": "arXiv:2205.10330",
    "title": "A Review of Safe Reinforcement Learning: Methods, Theory and  Applications",
    "abstract": "Reinforcement learning has achieved tremendous success in many complex\ndecision making tasks. When it comes to deploying RL in the real world, safety\nconcerns are usually raised, leading to a growing demand for safe reinforcement\nlearning algorithms, such as in autonomous driving and robotics scenarios.\nWhile safety control has a long history, the study of safe RL algorithms is\nstill in the early stages. To establish a good foundation for future research\nin this thread, in this paper, we provide a review for safe RL from the\nperspectives of methods, theory and applications. Firstly, we review the\nprogress of safe RL from five dimensions and come up with five problems that\nare crucial for safe RL being deployed in real-world applications, coined as\n\"2H3W\". Secondly, we analyze the theory and algorithm progress from the\nperspectives of answering the \"2H3W\" problems. Then, the sample complexity of\nsafe RL methods is reviewed and discussed, followed by an introduction of the\napplications and benchmarks of safe RL algorithms. Finally, we open the\ndiscussion of the challenging problems in safe RL, hoping to inspire more\nfuture research on this thread.\nTo advance the study of safe RL algorithms, we release a benchmark suite, an\nopen-sourced repository containing the implementations of major safe RL\nalgorithms, along with tutorials at the link:\nhttps://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git.",
    "descriptor": "",
    "authors": [
      "Shangding Gu",
      "Long Yang",
      "Yali Du",
      "Guang Chen",
      "Florian Walter",
      "Jun Wang",
      "Yaodong Yang",
      "Alois Knoll"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10330"
  },
  {
    "id": "arXiv:2205.10334",
    "title": "UCC: Uncertainty guided Cross-head Co-training for Semi-Supervised  Semantic Segmentation",
    "abstract": "Deep neural networks (DNNs) have witnessed great successes in semantic\nsegmentation, which requires a large number of labeled data for training. We\npresent a novel learning framework called Uncertainty guided Cross-head\nCo-training (UCC) for semi-supervised semantic segmentation. Our framework\nintroduces weak and strong augmentations within a shared encoder to achieve\nco-training, which naturally combines the benefits of consistency and\nself-training. Every segmentation head interacts with its peers and, the weak\naugmentation result is used for supervising the strong. The consistency\ntraining samples' diversity can be boosted by Dynamic Cross-Set Copy-Paste\n(DCSCP), which also alleviates the distribution mismatch and class imbalance\nproblems. Moreover, our proposed Uncertainty Guided Re-weight Module (UGRM)\nenhances the self-training pseudo labels by suppressing the effect of the\nlow-quality pseudo labels from its peer via modeling uncertainty. Extensive\nexperiments on Cityscapes and PASCAL VOC 2012 demonstrate the effectiveness of\nour UCC. Our approach significantly outperforms other state-of-the-art\nsemi-supervised semantic segmentation methods. It achieves 77.17$\\%$, 76.49$\\%$\nmIoU on Cityscapes and PASCAL VOC 2012 datasets respectively under 1/16\nprotocols, which are +10.1$\\%$, +7.91$\\%$ better than the supervised baseline.",
    "descriptor": "\nComments: 10 pages, CVPR2022\n",
    "authors": [
      "Jiashuo Fan",
      "Bin Gao",
      "Huan Jin",
      "Lihui Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10334"
  },
  {
    "id": "arXiv:2205.10337",
    "title": "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes",
    "abstract": "We introduce UViM, a unified approach capable of modeling a wide range of\ncomputer vision tasks. In contrast to previous models, UViM has the same\nfunctional form for all tasks; it requires no task-specific modifications which\nrequire extensive human expertise. The approach involves two components: (I) a\nbase model (feed-forward) which is trained to directly predict raw vision\noutputs, guided by a learned discrete code and (II) a language model\n(autoregressive) that is trained to generate the guiding code. These components\ncomplement each other: the language model is well-suited to modeling structured\ninterdependent data, while the base model is efficient at dealing with\nhigh-dimensional outputs. We demonstrate the effectiveness of UViM on three\ndiverse and challenging vision tasks: panoptic segmentation, depth prediction\nand image colorization, where we achieve competitive and near state-of-the-art\nresults. Our experimental results suggest that UViM is a promising candidate\nfor a unified modeling approach in computer vision.",
    "descriptor": "\nComments: Alexander and Andr\\'e share the first authorship, all authors made significant technical contributions to this work\n",
    "authors": [
      "Alexander Kolesnikov",
      "Andr\u00e9 Susano Pinto",
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Jeremiah Harmsen",
      "Neil Houlsby"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10337"
  },
  {
    "id": "arXiv:2205.10338",
    "title": "Efficient visual object representation using a biologically plausible  spike-latency code and winner-take-all inhibition",
    "abstract": "Deep neural networks have surpassed human performance in key visual\nchallenges such as object recognition, but require a large amount of energy,\ncomputation, and memory. In contrast, spiking neural networks (SNNs) have the\npotential to improve both the efficiency and biological plausibility of object\nrecognition systems. Here we present a SNN model that uses spike-latency coding\nand winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli\nfrom the Fashion MNIST dataset. Stimuli were preprocessed with center-surround\nreceptive fields and then fed to a layer of spiking neurons whose synaptic\nweights were updated using spike-timing-dependent-plasticity (STDP). We\ninvestigate how the quality of the represented objects changes under different\nWTA-I schemes and demonstrate that a network of 150 spiking neurons can\nefficiently represent objects with as little as 40 spikes. Studying how core\nobject recognition may be implemented using biologically plausible learning\nrules in SNNs may not only further our understanding of the brain, but also\nlead to novel and efficient artificial vision systems.",
    "descriptor": "",
    "authors": [
      "Melani Sanchez-Garcia",
      "Michael Beyeler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10338"
  },
  {
    "id": "arXiv:2205.10343",
    "title": "Towards Understanding Grokking: An Effective Theory of Representation  Learning",
    "abstract": "We aim to understand grokking, a phenomenon where models generalize long\nafter overfitting their training set. We present both a microscopic analysis\nanchored by an effective theory and a macroscopic analysis of phase diagrams\ndescribing learning performance across hyperparameters. We find that\ngeneralization originates from structured representations whose training\ndynamics and dependence on training set size can be predicted by our effective\ntheory in a toy setting. We observe empirically the presence of four learning\nphases: comprehension, grokking, memorization, and confusion. We find\nrepresentation learning to occur only in a \"Goldilocks zone\" (including\ncomprehension and grokking) between memorization and confusion. Compared to the\ncomprehension phase, the grokking phase stays closer to the memorization phase,\nleading to delayed generalization. The Goldilocks phase is reminiscent of\n\"intelligence from starvation\" in Darwinian evolution, where resource\nlimitations drive discovery of more efficient solutions. This study not only\nprovides intuitive explanations of the origin of grokking, but also highlights\nthe usefulness of physics-inspired tools, e.g., effective theories and phase\ndiagrams, for understanding deep learning.",
    "descriptor": "\nComments: 20 pages, 16 figures\n",
    "authors": [
      "Ziming Liu",
      "Ouail Kitouni",
      "Niklas Nolte",
      "Eric J. Michaud",
      "Max Tegmark",
      "Mike Williams"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Artificial Intelligence (cs.AI)",
      "Classical Physics (physics.class-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10343"
  },
  {
    "id": "arXiv:2205.10347",
    "title": "Diverse super-resolution with pretrained deep hiererarchical VAEs",
    "abstract": "Image super-resolution is a one-to-many problem, but most deep-learning based\nmethods only provide one single solution to this problem. In this work, we\ntackle the problem of diverse super-resolution by reusing VD-VAE, a\nstate-of-the art variational autoencoder (VAE). We find that the hierarchical\nlatent representation learned by VD-VAE naturally separates the image\nlow-frequency information, encoded in the latent groups at the top of the\nhierarchy, from the image high-frequency details, determined by the latent\ngroups at the bottom of the latent hierarchy. Starting from this observation,\nwe design a super-resolution model exploiting the specific structure of VD-VAE\nlatent space. Specifically, we train an encoder to encode low-resolution images\nin the subset of VD-VAE latent space encoding the low-frequency information,\nand we combine this encoder with VD-VAE generative model to sample diverse\nsuper-resolved version of a low-resolution input. We demonstrate the ability of\nour method to generate diverse solutions to the super-resolution problem on\nface super-resolution with upsampling factors x4, x8, and x16.",
    "descriptor": "\nComments: 21 pages , 5 figures\n",
    "authors": [
      "Jean Prost",
      "Antoine Houdard",
      "Nicolas Papadakis",
      "Andr\u00e9s Almansa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10347"
  },
  {
    "id": "arXiv:2205.10348",
    "title": "General Ramified Recurrence and Polynomial-time Completeness",
    "abstract": "We exhibit a sound and complete implicit-complexity formalism for functions\nfeasibly computable by structural recursions over inductively defined data\nstructures. Feasibly computable here means that the structural-recursive\ndefinition runs in time polynomial in the size of the representation of the\ninputs where these representations may make use of data sharing. Inductively\ndefined data structures here includes lists and trees. Soundness here means\nthat the programs within the implicit-complexity formalism have feasible run\ntimes. Completeness here means that each function computed by a feasible\nstructural recursion has a program in the implicit-complexity formalism. This\npaper is a follow up on the work of Avanzini, Dal Lago, Martini, and Zorzi who\nfocused on the soundness of such formalisms but did not consider the question\nof completeness.",
    "descriptor": "\nComments: PDFLaTeX, 41 pages with 2 figures\n",
    "authors": [
      "Norman Danner",
      "James S. Royer"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.10348"
  },
  {
    "id": "arXiv:2205.10350",
    "title": "Lossless Acceleration for Seq2seq Generation with Aggressive Decoding",
    "abstract": "We study lossless acceleration for seq2seq generation with a novel decoding\nalgorithm -- Aggressive Decoding. Unlike the previous efforts (e.g.,\nnon-autoregressive decoding) speeding up seq2seq generation at the cost of\nquality loss, our approach aims to yield the identical (or better) generation\ncompared with autoregressive decoding but in a significant speedup, achieved by\ninnovative cooperation of aggressive decoding and verification that are both\nefficient due to parallel computing.\nWe propose two Aggressive Decoding paradigms for 2 kinds of seq2seq tasks: 1)\nFor the seq2seq tasks whose inputs and outputs are highly similar (e.g.,\nGrammatical Error Correction), we propose Input-guided Aggressive Decoding\n(IAD) that aggressively copies from the input sentence as drafted decoded\ntokens to verify in parallel; 2) For other general seq2seq tasks (e.g., Machine\nTranslation), we propose Generalized Aggressive Decoding (GAD) that first\nemploys an additional non-autoregressive decoding model for aggressive decoding\nand then verifies in parallel in the autoregressive manner.\nWe test Aggressive Decoding on the most popular 6-layer Transformer model on\nGPU in multiple seq2seq tasks: 1) For IAD, we show that it can introduce a\n7x-9x speedup for the Transformer in Grammatical Error Correction and Text\nSimplification tasks with the identical results as greedy decoding; 2) For GAD,\nwe observe a 3x-5x speedup with the identical or even better quality in two\nimportant seq2seq tasks: Machine Translation and Abstractive Summarization.\nMoreover, Aggressive Decoding can benefit even more from stronger computing\ndevices that are better at parallel computing. Given the lossless quality as\nwell as significant and promising speedup, we believe Aggressive Decoding may\npotentially evolve into a de facto standard for efficient and lossless seq2seq\ngeneration in the near future.",
    "descriptor": "\nComments: 24-page Microsoft Research Technical Report. Content overlap with arXiv:2106.04970 and arXiv:2203.16487\n",
    "authors": [
      "Tao Ge",
      "Heming Xia",
      "Xin Sun",
      "Si-Qing Chen",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10350"
  },
  {
    "id": "arXiv:2205.10351",
    "title": "Enriching StyleGAN with Illumination Physics",
    "abstract": "StyleGAN generates novel images of a scene from latent codes which are\nimpressively disentangled. But StyleGAN generates images that are \"like\" its\ntraining set. This paper shows how to use simple physical properties of images\nto enrich StyleGAN's generation capacity. We use an intrinsic image method to\ndecompose an image, then search the latent space of a pretrained StyleGAN to\nfind novel directions that fix one component (say, albedo) and vary another\n(say, shading). Therefore, we can change the lighting of a complex scene\nwithout changing the scene layout, object colors, and shapes. Or we can change\nthe colors of objects without changing shading intensity or their scene layout.\nOur experiments suggest the proposed method, StyLitGAN, can add and remove\nluminaires in the scene and generate images with realistic lighting effects --\ncast shadows, soft shadows, inter-reflections, glossy effects -- requiring no\nlabeled paired relighting data or any other geometric supervision. Qualitative\nevaluation confirms that our generated images are realistic and that we can\nchange or fix components at will. Quantitative evaluation shows that\npre-trained StyleGAN could not produce the images StyLitGAN produces; we can\nautomatically generate realistic out-of-distribution images, and so can\nsignificantly enrich the range of images StyleGAN can produce.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Anand Bhattad",
      "D.A. Forsyth"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10351"
  },
  {
    "id": "arXiv:2204.11689",
    "title": "Deep electric field predictions by drift-reduced Braginskii theory with  plasma-neutral interactions based upon experimental images of boundary  turbulence",
    "abstract": "We present 2-dimensional turbulent electric field calculations via\nphysics-informed deep learning consistent with (i) drift-reduced Braginskii\ntheory under the framework of an axisymmetric fusion plasma with purely\ntoroidal field and (ii) experimental estimates of the fluctuating electron\ndensity and temperature obtained from analysis of gas puff imaging of a\ndischarge on the Alcator C-Mod tokamak. The inclusion of effects from the\nlocally puffed atomic helium on particle and energy sources within the reduced\nplasma turbulence model are found to strengthen correlations between the\nelectric field and electron pressure. The neutrals are also directly associated\nwith an observed broadening in the distribution of turbulent field amplitudes\nand increased ${\\bf E \\times B}$ shearing rates.",
    "descriptor": "\nComments: 7 pages, 3 figures, 2 tables\n",
    "authors": [
      "Abhilash Mathews",
      "Jerry Hughes",
      "James Terry",
      "Seung-Gyou Baek"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.11689"
  },
  {
    "id": "arXiv:2205.09520",
    "title": "Dynamic SAFFRON: Disease Control Over Time Via Group Testing",
    "abstract": "We consider the dynamic infection spread model that is based on the discrete\nSIR model which assumes infections to be spread over time via infected and\nnon-isolated individuals. In our system, the main objective is not to minimize\nthe number of required tests to identify every infection, but instead, to\nutilize the available, given testing capacity $T$ at each time instance to\nefficiently control the infection spread. We introduce and study a novel\nperformance metric, which we coin as $\\epsilon$-disease control time. This\nmetric can be used to measure how fast a given algorithm can control the spread\nof a disease. We characterize the performance of dynamic individual testing\nalgorithm and introduce a novel dynamic SAFFRON based group testing algorithm.\nWe present theoretical results and implement the proposed algorithms to compare\ntheir performances.",
    "descriptor": "",
    "authors": [
      "Batuhan Arasli",
      "Sennur Ulukus"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2205.09520"
  },
  {
    "id": "arXiv:2205.09776",
    "title": "mat2qubit: A lightweight pythonic package for qubit encodings of  vibrational, bosonic, graph coloring, routing, scheduling, and general matrix  problems",
    "abstract": "Preparing problems for execution on quantum computers can require many\ncompilation steps. Automated compilation software is useful not only for easier\nand faster problem execution, but also for facilitating the comparison between\ndifferent algorithmic choices. Here we describe mat2qubit, a Python package for\nencoding several classes of classical and quantum problems into qubit\nrepresentations. It is intended for use especially on Hamiltonians and\nfunctions defined over variables (e.g. particles) with cardinality larger than\n2. More specifically, mat2qubit may be used to compile bosonic,\nphononic/vibrational, and spin-$s$ problems, as well as classical problems such\nas graph coloring, routing, scheduling, and classical linear algebra more\ngenerally. In order to facilitate numerical analyses and ease of\nprogrammability, a built-in computer algebra system (CAS) allows for fully\nsymbolic preparation and manipulation of problems (with symbolic operators,\nsymbolic coefficients, and symbolic particle labels) before the final\ncompilation into qubits is performed. We expect this code to be useful in the\npreparation and analysis of various classes of physics, chemistry, materials,\nand optimization problems for execution on digital quantum computers.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Nicolas PD Sawaya"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Discrete Mathematics (cs.DM)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.09776"
  },
  {
    "id": "arXiv:2205.09779",
    "title": "Residual Dynamic Mode Decomposition: Robust and verified Koopmanism",
    "abstract": "Dynamic Mode Decomposition (DMD) describes complex dynamic processes through\na hierarchy of simpler coherent features. DMD is regularly used to understand\nthe fundamental characteristics of turbulence and is closely related to Koopman\noperators. However, verifying the decomposition, equivalently the computed\nspectral features of Koopman operators, remains a major challenge due to the\ninfinite-dimensional nature of Koopman operators. Challenges include spurious\n(unphysical) modes, and dealing with continuous spectra, both of which occur\nregularly in turbulent flows. Residual Dynamic Mode Decomposition (ResDMD),\nintroduced by (Colbrook & Townsend 2021), overcomes some of these challenges\nthrough the data-driven computation of residuals associated with the full\ninfinite-dimensional Koopman operator. ResDMD computes spectra and\npseudospectra of general Koopman operators with error control, and computes\nsmoothed approximations of spectral measures (including continuous spectra)\nwith explicit high-order convergence theorems. ResDMD thus provides robust and\nverified Koopmanism. We implement ResDMD and demonstrate its application in a\nvariety of fluid dynamic situations, at varying Reynolds numbers, arising from\nboth numerical and experimental data. Examples include: vortex shedding behind\na cylinder; hot-wire data acquired in a turbulent boundary layer; particle\nimage velocimetry data focusing on a wall-jet flow; and acoustic pressure\nsignals of laser-induced plasma. We present some advantages of ResDMD, namely,\nthe ability to verifiably resolve non-linear, transient modes, and spectral\ncalculation with reduced broadening effects. We also discuss how a new modal\nordering based on residuals enables greater accuracy with a smaller dictionary\nthan the traditional modulus ordering. This paves the way for greater dynamic\ncompression of large datasets without sacrificing accuracy.",
    "descriptor": "",
    "authors": [
      "Matthew J. Colbrook",
      "Lorna J. Ayton",
      "M\u00e1t\u00e9 Sz\u0151ke"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Spectral Theory (math.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.09779"
  },
  {
    "id": "arXiv:2205.09784",
    "title": "End-to-End Zero-Shot Voice Style Transfer with Location-Variable  Convolutions",
    "abstract": "Zero-shot voice conversion is becoming an increasingly popular research\ndirection, as it promises the ability to transform speech to match the voice\nstyle of any speaker. However, little work has been done on end-to-end methods\nfor this task, which are appealing because they remove the need for a separate\nvocoder to generate audio from intermediate features. In this work, we propose\nLocation-Variable Convolution-based Voice Conversion (LVC-VC), a model for\nperforming end-to-end zero-shot voice conversion that is based on a neural\nvocoder. LVC-VC utilizes carefully designed input features that have\ndisentangled content and speaker style information, and the vocoder-like\narchitecture learns to combine them to simultaneously perform voice conversion\nwhile synthesizing audio. To the best of our knowledge, LVC-VC is one of the\nfirst models to be proposed that can perform zero-shot voice conversion in an\nend-to-end manner, and it is the first to do so using a vocoder-like neural\nframework. Experiments show that our model achieves competitive or better voice\nstyle transfer performance compared to several baselines while maintaining the\nintelligibility of transformed speech much better.",
    "descriptor": "",
    "authors": [
      "Wonjune Kang",
      "Deb Roy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.09784"
  },
  {
    "id": "arXiv:2205.09812",
    "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking  Events",
    "abstract": "The modeling of turn-taking in dialog can be viewed as the modeling of the\ndynamics of voice activity of the interlocutors. We extend prior work and\ndefine the predictive task of Voice Activity Projection, a general,\nself-supervised objective, as a way to train turn-taking models without the\nneed of labeled data. We highlight a theoretical weakness with prior\napproaches, arguing for the need of modeling the dependency of voice activity\nevents in the projection window. We propose four zero-shot tasks, related to\nthe prediction of upcoming turn-shifts and backchannels, and show that the\nproposed model outperforms prior work.",
    "descriptor": "\nComments: Submitted to INTERSPEECH 2022, 5 pages, 4 figures\n",
    "authors": [
      "Erik Ekstedt",
      "Gabriel Skantze"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.09812"
  },
  {
    "id": "arXiv:2205.09824",
    "title": "Deep Learning Methods for Proximal Inference via Maximum Moment  Restriction",
    "abstract": "The No Unmeasured Confounding Assumption is widely used to identify causal\neffects in observational studies. Recent work on proximal inference has\nprovided alternative identification results that succeed even in the presence\nof unobserved confounders, provided that one has measured a sufficiently rich\nset of proxy variables, satisfying specific structural conditions. However,\nproximal inference requires solving an ill-posed integral equation. Previous\napproaches have used a variety of machine learning techniques to estimate a\nsolution to this integral equation, commonly referred to as the bridge\nfunction. However, prior work has often been limited by relying on\npre-specified kernel functions, which are not data adaptive and struggle to\nscale to large datasets. In this work, we introduce a flexible and scalable\nmethod based on a deep neural network to estimate causal effects in the\npresence of unmeasured confounding using proximal inference. Our method\nachieves state of the art performance on two well-established proximal\ninference benchmarks. Finally, we provide theoretical consistency guarantees\nfor our method.",
    "descriptor": "\nComments: 44 pages, 20 figures\n",
    "authors": [
      "Benjamin Kompa",
      "David R. Bellamy",
      "Thomas Kolokotrones",
      "James M. Robins",
      "Andrew L. Beam"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09824"
  },
  {
    "id": "arXiv:2205.09825",
    "title": "Algorithms for Weak Optimal Transport with an Application to Economics",
    "abstract": "The theory of weak optimal transport (WOT), introduced by [Gozlan et al.,\n2017], generalizes the classic Monge-Kantorovich framework by allowing the\ntransport cost between one point and the points it is matched with to be\nnonlinear. In the so-called barycentric version of WOT, the cost for\ntransporting a point $x$ only depends on $x$ and on the barycenter of the\npoints it is matched with. This aggregation property of WOT is appealing in\nmachine learning, economics and finance. Yet algorithms to compute WOT have\nonly been developed for the special case of quadratic barycentric WOT, or\ndepend on neural networks with no guarantee on the computed value and matching.\nThe main difficulty lies in the transportation constraints which are costly to\nproject onto. In this paper, we propose to use mirror descent algorithms to\nsolve the primal and dual versions of the WOT problem. We also apply our\nalgorithms to the variant of WOT introduced by [Chon\\'e et al., 2022] where\nmass is distributed from one space to another through unnormalized kernels\n(WOTUK). We empirically compare the solutions of WOT and WOTUK with classical\nOT. We illustrate our numerical methods to the economic framework of [Chon\\'e\nand Kramarz, 2021], namely the matching between workers and firms on labor\nmarkets.",
    "descriptor": "",
    "authors": [
      "Fran\u00e7ois-Pierre Paty",
      "Philippe Chon\u00e9",
      "Francis Kramarz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.09825"
  },
  {
    "id": "arXiv:2205.09829",
    "title": "Capturing cross-session neural population variability through  self-supervised identification of consistent neuron ensembles",
    "abstract": "Decoding stimuli or behaviour from recorded neural activity is a common\napproach to interrogate brain function in research, and an essential part of\nbrain-computer and brain-machine interfaces. Reliable decoding even from small\nneural populations is possible because high dimensional neural population\nactivity typically occupies low dimensional manifolds that are discoverable\nwith suitable latent variable models. Over time however, drifts in activity of\nindividual neurons and instabilities in neural recording devices can be\nsubstantial, making stable decoding over days and weeks impractical. While this\ndrift cannot be predicted on an individual neuron level, population level\nvariations over consecutive recording sessions such as differing sets of\nneurons and varying permutations of consistent neurons in recorded data may be\nlearnable when the underlying manifold is stable over time. Classification of\nconsistent versus unfamiliar neurons across sessions and accounting for\ndeviations in the order of consistent recording neurons in recording datasets\nover sessions of recordings may then maintain decoding performance. In this\nwork we show that self-supervised training of a deep neural network can be used\nto compensate for this inter-session variability. As a result, a sequential\nautoencoding model can maintain state-of-the-art behaviour decoding performance\nfor completely unseen recording sessions several days into the future. Our\napproach only requires a single recording session for training the model, and\nis a step towards reliable, recalibration-free brain computer interfaces.",
    "descriptor": "",
    "authors": [
      "Justin Jude",
      "Matthew G. Perich",
      "Lee E. Miller",
      "Matthias H. Hennig"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09829"
  },
  {
    "id": "arXiv:2205.09842",
    "title": "Generation of Artificial CT Images using Patch-based Conditional  Generative Adversarial Networks",
    "abstract": "Deep learning has a great potential to alleviate diagnosis and prognosis for\nvarious clinical procedures. However, the lack of a sufficient number of\nmedical images is the most common obstacle in conducting image-based analysis\nusing deep learning. Due to the annotations scarcity, semi-supervised\ntechniques in the automatic medical analysis are getting high attention.\nArtificial data augmentation and generation techniques such as generative\nadversarial networks (GANs) may help overcome this obstacle. In this work, we\npresent an image generation approach that uses generative adversarial networks\nwith a conditional discriminator where segmentation masks are used as\nconditions for image generation. We validate the feasibility of GAN-enhanced\nmedical image generation on whole heart computed tomography (CT) images and its\nseven substructures, namely: left ventricle, right ventricle, left atrium,\nright atrium, myocardium, pulmonary arteries, and aorta. Obtained results\ndemonstrate the suitability of the proposed adversarial approach for the\naccurate generation of high-quality CT images. The presented method shows great\npotential to facilitate further research in the domain of artificial medical\nimage generation.",
    "descriptor": "\nComments: Proceedings of the 7th International Conference on Smart and Sustainable Technologies (SpliTech 2022)\n",
    "authors": [
      "Marija Habijan",
      "Irena Galic"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09842"
  },
  {
    "id": "arXiv:2205.09850",
    "title": "Human Gender Prediction Based on Deep Transfer Learning from Panoramic  Radiograph Images",
    "abstract": "Panoramic Dental Radiography (PDR) image processing is one of the most\nextensively used manual methods for gender determination in forensic medicine.\nManual approaches require a wide range of mandibular parameter measurements in\nmetric units. Besides being time-consuming, these methods also necessitate the\nemployment of experienced professionals. In this context, deep learning models\nare widely utilized in the auto-analysis of radiological images nowadays, owing\nto their high processing speed, accuracy, and stability. In our study, a data\nset consisting of 24,000 dental panoramic images was prepared for binary\nclassification, and the transfer learning method was used to accelerate the\ntraining and increase the performance of our proposed DenseNet121 deep learning\nmodel. With the transfer learning method, instead of starting the learning\nprocess from scratch, the existing patterns learned beforehand were used.\nExtensive comparisons were made using deep transfer learning (DTL) models\nVGG16, ResNet50, and EfficientNetB6 to assess the classification performance of\nthe proposed model in PDR images. According to the findings of the comparative\nanalysis, the proposed model outperformed the other approaches by achieving a\nsuccess rate of 97.25% in gender classification.",
    "descriptor": "\nComments: 8 pages, 11 figures, 10 tables\n",
    "authors": [
      "I. Atas"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09850"
  },
  {
    "id": "arXiv:2205.09872",
    "title": "Content-Context Factorized Representations for Automated Speech  Recognition",
    "abstract": "Deep neural networks have largely demonstrated their ability to perform\nautomated speech recognition (ASR) by extracting meaningful features from input\naudio frames. Such features, however, may consist not only of information about\nthe spoken language content, but also may contain information about unnecessary\ncontexts such as background noise and sounds or speaker identity, accent, or\nprotected attributes. Such information can directly harm generalization\nperformance, by introducing spurious correlations between the spoken words and\nthe context in which such words were spoken. In this work, we introduce an\nunsupervised, encoder-agnostic method for factoring speech-encoder\nrepresentations into explicit content-encoding representations and spurious\ncontext-encoding representations. By doing so, we demonstrate improved\nperformance on standard ASR benchmarks, as well as improved performance in both\nreal-world and artificially noisy ASR scenarios.",
    "descriptor": "",
    "authors": [
      "David M. Chan",
      "Shalini Ghosh"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.09872"
  },
  {
    "id": "arXiv:2205.09874",
    "title": "Explainable Graph Theory-Based Identification of Meter-Transformer  Mapping",
    "abstract": "Distributed energy resources are better for the environment but may cause\ntransformer overload in distribution grids, calling for recovering\nmeter-transformer mapping to provide situational awareness, i.e., the\ntransformer loading. The challenge lies in recovering meter-transformer (M.T.)\nmapping for two common scenarios, e.g., large distances between a meter and its\nparent transformer or high similarity of a meter's consumption pattern to a\nnon-parent transformer's meters. Past methods either assume a variety of data\nas in the transmission grid or ignore the two common scenarios mentioned above.\nTherefore, we propose to utilize the above observation via spectral embedding\nby using the property that inter-transformer meter consumptions are not the\nsame and that the noise in data is limited so that all the k smallest\neigenvalues of the voltage-based Laplacian matrix are smaller than the next\nsmallest eigenvalue of the ideal Laplacian matrix. We also provide a guarantee\nbased on this understanding. Furthermore, we partially relax the assumption by\nutilizing location information to aid voltage information for areas\ngeographically far away but with similar voltages. Numerical simulations on the\nIEEE test systems and real feeders from our partner utility show that the\nproposed method correctly identifies M.T. mapping.",
    "descriptor": "",
    "authors": [
      "Bilal Saleem",
      "Yang Weng"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09874"
  },
  {
    "id": "arXiv:2205.09899",
    "title": "Breaking the $\\sqrt{T}$ Barrier: Instance-Independent Logarithmic Regret  in Stochastic Contextual Linear Bandits",
    "abstract": "We prove an instance independent (poly) logarithmic regret for stochastic\ncontextual bandits with linear payoff. Previously, in \\cite{chu2011contextual},\na lower bound of $\\mathcal{O}(\\sqrt{T})$ is shown for the contextual linear\nbandit problem with arbitrary (adversarily chosen) contexts. In this paper, we\nshow that stochastic contexts indeed help to reduce the regret from $\\sqrt{T}$\nto $\\polylog(T)$. We propose Low Regret Stochastic Contextual Bandits\n(\\texttt{LR-SCB}), which takes advantage of the stochastic contexts and\nperforms parameter estimation (in $\\ell_2$ norm) and regret minimization\nsimultaneously. \\texttt{LR-SCB} works in epochs, where the parameter estimation\nof the previous epoch is used to reduce the regret of the current epoch. The\n(poly) logarithmic regret of \\texttt{LR-SCB} stems from two crucial facts: (a)\nthe application of a norm adaptive algorithm to exploit the parameter\nestimation and (b) an analysis of the shifted linear contextual bandit\nalgorithm, showing that shifting results in increasing regret. We have also\nshown experimentally that stochastic contexts indeed incurs a regret that\nscales with $\\polylog(T)$.",
    "descriptor": "\nComments: To appear in ICML 2022\n",
    "authors": [
      "Avishek Ghosh",
      "Abishek Sankararaman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09899"
  },
  {
    "id": "arXiv:2205.09900",
    "title": "Estimating the frame potential of large-scale quantum circuit sampling  using tensor networks up to 50 qubits",
    "abstract": "We develop numerical protocols for estimating the frame potential, the 2-norm\ndistance between a given ensemble and the exact Haar randomness, using the\n\\texttt{QTensor} platform. Our tensor-network-based algorithm has polynomial\ncomplexity for shallow circuits and is high performing using CPU and GPU\nparallelism. We apply the above methods to two problems: the Brown-Susskind\nconjecture, with local and parallel random circuits in terms of the Haar\ndistance and the approximate $k$-design properties of the hardware efficient\nans{\\\"a}tze in quantum machine learning, which induce the barren plateau\nproblem. We estimate frame potentials with these ensembles up to 50 qubits and\n$k=5$, examine the Haar distance of the hardware-efficient ans{\\\"a}tze, and\nverify the Brown-Susskind conjecture numerically. Our work shows that\nlarge-scale tensor network simulations could provide important hints toward\nopen problems in quantum information science.",
    "descriptor": "\nComments: 11 pages, many figures\n",
    "authors": [
      "Minzhao Liu",
      "Junyu Liu",
      "Yuri Alexeev",
      "Liang Jiang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2205.09900"
  },
  {
    "id": "arXiv:2205.09906",
    "title": "Data Augmentation for Compositional Data: Advancing Predictive Models of  the Microbiome",
    "abstract": "Data augmentation plays a key role in modern machine learning pipelines.\nWhile numerous augmentation strategies have been studied in the context of\ncomputer vision and natural language processing, less is known for other data\nmodalities. Our work extends the success of data augmentation to compositional\ndata, i.e., simplex-valued data, which is of particular interest in the context\nof the human microbiome. Drawing on key principles from compositional data\nanalysis, such as the Aitchison geometry of the simplex and subcompositions, we\ndefine novel augmentation strategies for this data modality. Incorporating our\ndata augmentations into standard supervised learning pipelines results in\nconsistent performance gains across a wide range of standard benchmark\ndatasets. In particular, we set a new state-of-the-art for key disease\nprediction tasks including colorectal cancer, type 2 diabetes, and Crohn's\ndisease. In addition, our data augmentations enable us to define a novel\ncontrastive learning model, which improves on previous representation learning\napproaches for microbiome compositional data. Our code is available at\nhttps://github.com/cunningham-lab/AugCoDa.",
    "descriptor": "",
    "authors": [
      "Elliott Gordon-Rodriguez",
      "Thomas P. Quinn",
      "John P. Cunningham"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09906"
  },
  {
    "id": "arXiv:2205.09909",
    "title": "Sparse Infinite Random Feature Latent Variable Modeling",
    "abstract": "We propose a non-linear, Bayesian non-parametric latent variable model where\nthe latent space is assumed to be sparse and infinite dimensional a priori\nusing an Indian buffet process prior. A posteriori, the number of instantiated\ndimensions in the latent space is guaranteed to be finite. The purpose of\nplacing the Indian buffet process on the latent variables is to: 1.)\nAutomatically and probabilistically select the number of latent dimensions. 2.)\nImpose sparsity in the latent space, where the Indian buffet process will\nselect which elements are exactly zero. Our proposed model allows for sparse,\nnon-linear latent variable modeling where the number of latent dimensions is\nselected automatically. Inference is made tractable using the random Fourier\napproximation and we can easily implement posterior inference through Markov\nchain Monte Carlo sampling. This approach is amenable to many observation\nmodels beyond the Gaussian setting. We demonstrate the utility of our method on\na variety of synthetic, biological and text datasets and show that we can\nobtain superior test set performance compared to previous latent variable\nmodels.",
    "descriptor": "",
    "authors": [
      "Michael Minyi Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09909"
  },
  {
    "id": "arXiv:2205.09914",
    "title": "Robust Expected Information Gain for Optimal Bayesian Experimental  Design Using Ambiguity Sets",
    "abstract": "The ranking of experiments by expected information gain (EIG) in Bayesian\nexperimental design is sensitive to changes in the model's prior distribution,\nand the approximation of EIG yielded by sampling will have errors similar to\nthe use of a perturbed prior. We define and analyze \\emph{robust expected\ninformation gain} (REIG), a modification of the objective in EIG maximization\nby minimizing an affine relaxation of EIG over an ambiguity set of\ndistributions that are close to the original prior in KL-divergence. We show\nthat, when combined with a sampling-based approach to estimating EIG, REIG\ncorresponds to a `log-sum-exp' stabilization of the samples used to estimate\nEIG, meaning that it can be efficiently implemented in practice. Numerical\ntests combining REIG with variational nested Monte Carlo (VNMC), adaptive\ncontrastive estimation (ACE) and mutual information neural estimation (MINE)\nsuggest that in practice REIG also compensates for the variability of\nunder-sampled estimators.",
    "descriptor": "\nComments: The 38th Conference on Uncertainty in Artificial Intelligence, 2022\n",
    "authors": [
      "Jinwoo Go",
      "Tobin Isaac"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.09914"
  },
  {
    "id": "arXiv:2205.09940",
    "title": "Conformal Prediction with Temporal Quantile Adjustments",
    "abstract": "We develop Temporal Quantile Adjustment (TQA), a general method to construct\nefficient and valid prediction intervals (PIs) for regression on\ncross-sectional time series data. Such data is common in many domains,\nincluding econometrics and healthcare. A canonical example in healthcare is\npredicting patient outcomes using physiological time-series data, where a\npopulation of patients composes a cross-section. Reliable PI estimators in this\nsetting must address two distinct notions of coverage: cross-sectional coverage\nacross a cross-sectional slice, and longitudinal coverage along the temporal\ndimension for each time series. Recent works have explored adapting Conformal\nPrediction (CP) to obtain PIs in the time series context. However, none handles\nboth notions of coverage simultaneously. CP methods typically query a\npre-specified quantile from the distribution of nonconformity scores on a\ncalibration set. TQA adjusts the quantile to query in CP at each time $t$,\naccounting for both cross-sectional and longitudinal coverage in a\ntheoretically-grounded manner. The post-hoc nature of TQA facilitates its use\nas a general wrapper around any time series regression model. We validate TQA's\nperformance through extensive experimentation: TQA generally obtains efficient\nPIs and improves longitudinal coverage while preserving cross-sectional\ncoverage.",
    "descriptor": "",
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.09940"
  },
  {
    "id": "arXiv:2205.10024",
    "title": "Trend analysis and forecasting air pollution in Rwanda",
    "abstract": "Air pollution is a major public health problem worldwide although the lack of\ndata is a global issue for most low and middle income countries. Ambient air\npollution in the form of fine particulate matter (PM2.5) exceeds the World\nHealth Organization guidelines in Rwanda with a daily average of around 42.6\nmicrogram per meter cube. Monitoring and mitigation strategies require an\nexpensive investment in equipment to collect pollution data. Low-cost sensor\ntechnology and machine learning methods have appeared as an alternative\nsolution to get reliable information for decision making. This paper analyzes\nthe trend of air pollution in Rwanda and proposes forecasting models suitable\nto data collected by a network of low-cost sensors deployed in Rwanda.",
    "descriptor": "",
    "authors": [
      "Paterne Gahungu",
      "Jean Remy Kubwimana"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10024"
  },
  {
    "id": "arXiv:2205.10054",
    "title": "Towards Extremely Fast Bilevel Optimization with Self-governed  Convergence Guarantees",
    "abstract": "Gradient methods have become mainstream techniques for Bi-Level Optimization\n(BLO) in learning and vision fields. The validity of existing works heavily\nrelies on solving a series of approximation subproblems with extraordinarily\nhigh accuracy. Unfortunately, to achieve the approximation accuracy requires\nexecuting a large quantity of time-consuming iterations and computational\nburden is naturally caused. This paper is thus devoted to address this critical\ncomputational issue. In particular, we propose a single-level formulation to\nuniformly understand existing explicit and implicit Gradient-based BLOs\n(GBLOs). This together with our designed counter-example can clearly illustrate\nthe fundamental numerical and theoretical issues of GBLOs and their naive\naccelerations. By introducing the dual multipliers as a new variable, we then\nestablish Bilevel Alternating Gradient with Dual Correction (BAGDC), a general\nframework, which significantly accelerates different categories of existing\nmethods by taking specific settings. A striking feature of our convergence\nresult is that, compared to those original unaccelerated GBLO versions, the\nfast BAGDC admits a unified non-asymptotic convergence theory towards\nstationarity. A variety of numerical experiments have also been conducted to\ndemonstrate the superiority of the proposed algorithmic framework.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Risheng Liu",
      "Xuan Liu",
      "Wei Yao",
      "Shangzhi Zeng",
      "Jin Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10054"
  },
  {
    "id": "arXiv:2205.10055",
    "title": "A Case of Exponential Convergence Rates for SVM",
    "abstract": "Classification is often the first problem described in introductory machine\nlearning classes. Generalization guarantees of classification have historically\nbeen offered by Vapnik-Chervonenkis theory. Yet those guarantees are based on\nintractable algorithms, which has led to the theory of surrogate methods in\nclassification. Guarantees offered by surrogate methods are based on\ncalibration inequalities, which have been shown to be highly sub-optimal under\nsome margin conditions, failing short to capture exponential convergence\nphenomena. Those \"super\" fast rates are becoming to be well understood for\nsmooth surrogates, but the picture remains blurry for non-smooth losses such as\nthe hinge loss, associated with the renowned support vector machines. In this\npaper, we present a simple mechanism to obtain fast convergence rates and we\ninvestigate its usage for SVM. In particular, we show that SVM can exhibit\nexponential convergence rates even without assuming the hard Tsybakov margin\ncondition.",
    "descriptor": "\nComments: 16 pages, 6 figures\n",
    "authors": [
      "Vivien Cabannes",
      "Stefano Vigogna"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10055"
  },
  {
    "id": "arXiv:2205.10082",
    "title": "On Calibration of Ensemble-Based Credal Predictors",
    "abstract": "In recent years, several classification methods that intend to quantify\nepistemic uncertainty have been proposed, either by producing predictions in\nthe form of second-order distributions or sets of probability distributions. In\nthis work, we focus on the latter, also called credal predictors, and address\nthe question of how to evaluate them: What does it mean that a credal predictor\nrepresents epistemic uncertainty in a faithful manner? To answer this question,\nwe refer to the notion of calibration of probabilistic predictors and extend it\nto credal predictors. Broadly speaking, we call a credal predictor calibrated\nif it returns sets that cover the true conditional probability distribution. To\nverify this property for the important case of ensemble-based credal\npredictors, we propose a novel nonparametric calibration test that generalizes\nan existing test for probabilistic predictors to the case of credal predictors.\nMaking use of this test, we empirically show that credal predictors based on\ndeep neural networks are often not well calibrated.",
    "descriptor": "",
    "authors": [
      "Thomas Mortier",
      "Viktor Bengs",
      "Eyke H\u00fcllermeier",
      "Stijn Luca",
      "Willem Waegeman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10082"
  },
  {
    "id": "arXiv:2205.10125",
    "title": "Some neighborhood-related fuzzy covering-based rough set models and  their applications for decision making",
    "abstract": "Fuzzy rough set (FRS) has a great effect on data mining processes and the\nfuzzy logical operators play a key role in the development of FRS theory. In\norder to further generalize the FRS theory to more complicated data\nenvironments, we firstly propose four types of fuzzy neighborhood operators\nbased on fuzzy covering by overlap functions and their implicators in this\npaper. Meanwhile, the derived fuzzy coverings from an original fuzzy covering\nare defined and the equalities among overlap function-based fuzzy neighborhood\noperators based on a finite fuzzy covering are also investigated. Secondly, we\nprove that new operators can be divided into seventeen groups according to\nequivalence relations, and the partial order relations among these seventeen\nclasses of operators are discussed, as well. Go further, the comparisons with $\nt$-norm-based fuzzy neighborhood operators given by D'eer et al. are also made\nand two types of neighborhood-related fuzzy covering-based rough set models,\nwhich are defined via different fuzzy neighborhood operators that are on the\nbasis of diverse kinds of fuzzy logical operators proposed. Furthermore, the\ngroupings and partially order relations are also discussed. Finally, a novel\nfuzzy TOPSIS methodology is put forward to solve a biosynthetic nanomaterials\nselect issue, and the rationality and enforceability of our new approach is\nverified by comparing its results with nine different methods.",
    "descriptor": "",
    "authors": [
      "Gongao Qi",
      "Bin Yang",
      "Wei Li"
    ],
    "subjectives": [
      "General Mathematics (math.GM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10125"
  },
  {
    "id": "arXiv:2205.10145",
    "title": "Kernel Estimates as General Concept for the Measuring of Pedestrian  Density",
    "abstract": "The standard density definition produces scattered values. Hence approaches\nimproving features of the density estimates has been invented for many use\ncases. Presented general framework evaluating density using various kernels\nbrings desired properties of density estimates and incorporates the most of\nordinarily used methods. Extensive parametric study is performed on\nexperimental data to illustrate effects of kernel selection (e.g. Gauss, cone)\nand its parametrization (blur). Quantitative evaluation of introduced quality\ncriteria illustrates that kernel densities satisfy user requirements, e.g.\nconic kernel with radius in $[0.7, 1.2]$ m. These parametric values are also\ninterpretable from proxemic theory indicating correctness of the whole concept.\nBesides, the kernel approach is directly compared to Voronoi approximation and\ncustomized distance to the nearest pedestrian - the comparison indicates a\nrelevant correspondence. Furthermore, the kernel approach is supposed to be\nvalid from mathematical perspective, since introduced Borsalino kernel has\npromising mathematical properties enabling future analytical research.",
    "descriptor": "\nComments: Submitted to Transportmetrica A: Transport Science on May 20, 2022\n",
    "authors": [
      "Jana Vackov\u00e1",
      "Marek Buk\u00e1\u010dek"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Multiagent Systems (cs.MA)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.10145"
  },
  {
    "id": "arXiv:2205.10200",
    "title": "The Fairness of Credit Scoring Models",
    "abstract": "In credit markets, screening algorithms aim to discriminate between good-type\nand bad-type borrowers. However, when doing so, they also often discriminate\nbetween individuals sharing a protected attribute (e.g. gender, age, racial\norigin) and the rest of the population. In this paper, we show how (1) to test\nwhether there exists a statistically significant difference between protected\nand unprotected groups, which we call lack of fairness and (2) to identify the\nvariables that cause the lack of fairness. We then use these variables to\noptimize the fairness-performance trade-off. Our framework provides guidance on\nhow algorithmic fairness can be monitored by lenders, controlled by their\nregulators, and improved for the benefit of protected groups.",
    "descriptor": "",
    "authors": [
      "Christophe Hurlin",
      "Christophe P\u00e9rignon",
      "S\u00e9bastien Saurin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Risk Management (q-fin.RM)"
    ],
    "url": "https://arxiv.org/abs/2205.10200"
  },
  {
    "id": "arXiv:2205.10215",
    "title": "Analysis Social Sparsity Audio Declipper",
    "abstract": "We develop the analysis (cosparse) variant of the popular audio declipping\nalgorithm of Siedenburg et al. Furthermore, we extend it by the possibility of\nweighting the time-frequency coefficients. We examine the audio reconstruction\nperformance of several combinations of weights and shrinkage operators. We show\nthat weights improve the reconstruction quality in some cases; however, the\noverall scores achieved by the non-weighted are not surpassed. Yet, the\nanalysis Empirical Wiener (EW) shrinkage was able to reach the quality of a\ncomputationally more expensive competitor, the Persistent Empirical Wiener\n(PEW). Moreover, the proposed analysis variant using PEW slightly outperforms\nthe synthesis counterpart in terms of an auditory-motivated metric.",
    "descriptor": "",
    "authors": [
      "Pavel Z\u00e1vi\u0161ka",
      "Pavel Rajmic"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.10215"
  },
  {
    "id": "arXiv:2205.10217",
    "title": "Memorization and Optimization in Deep Neural Networks with Minimum  Over-parameterization",
    "abstract": "The Neural Tangent Kernel (NTK) has emerged as a powerful tool to provide\nmemorization, optimization and generalization guarantees in deep neural\nnetworks. A line of work has studied the NTK spectrum for two-layer and deep\nnetworks with at least a layer with $\\Omega(N)$ neurons, $N$ being the number\nof training samples. Furthermore, there is increasing evidence suggesting that\ndeep networks with sub-linear layer widths are powerful memorizers and\noptimizers, as long as the number of parameters exceeds the number of samples.\nThus, a natural open question is whether the NTK is well conditioned in such a\nchallenging sub-linear setup. In this paper, we answer this question in the\naffirmative. Our key technical contribution is a lower bound on the smallest\nNTK eigenvalue for deep networks with the minimum possible\nover-parameterization: the number of parameters is roughly $\\Omega(N)$ and,\nhence, the number of neurons is as little as $\\Omega(\\sqrt{N})$. To showcase\nthe applicability of our NTK bounds, we provide two results concerning\nmemorization capacity and optimization guarantees for gradient descent\ntraining.",
    "descriptor": "",
    "authors": [
      "Simone Bombari",
      "Mohammad Hossein Amani",
      "Marco Mondelli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10217"
  },
  {
    "id": "arXiv:2205.10243",
    "title": "Near-field focusing using phased arrays with dynamic polarization  control",
    "abstract": "Phased arrays in near-field communication allow the transmitter to focus\nwireless signals in a small region around the receiver. Proper focusing is\nachieved by carefully tuning the phase shifts and the polarization of the\nsignals transmitted from the phased array. In this paper, we study the impact\nof polarization on near-field focusing and investigate the use of dynamic\npolarization control (DPC) phased arrays in this context. Our studies indicate\nthat the optimal polarization configuration for near-field focusing varies\nspatially across the antenna array. Such a spatial variation motivates the need\nfor DPC phased arrays which allow independent polarization control across\ndifferent antennas. We show using simulations that DPC phased arrays in the\nnear-field achieve a higher received signal-to-noise ratio than conventional\nswitched- or dual-polarization phased arrays.",
    "descriptor": "\nComments: to appear in Proc. of IEEE EUSIPCO 2022\n",
    "authors": [
      "Nitin Jonathan Myers",
      "Yanki Aslan",
      "Geethu Joseph"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10243"
  },
  {
    "id": "arXiv:2205.10248",
    "title": "The Self-Similarity of Free Semigroups and Groups",
    "abstract": "We give a survey on results regarding self-similar and automaton\npresentations of free groups and semigroups and related products. Furthermore,\nwe discuss open problems and results with respect to algebraic decision\nproblems in this area.",
    "descriptor": "",
    "authors": [
      "Emanuele Rodaro",
      "Jan Philipp W\u00e4chter"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.10248"
  },
  {
    "id": "arXiv:2205.10267",
    "title": "Reproducibility of the First Image of a Black Hole in the Galaxy M87  from the Event Horizon Telescope (EHT) Collaboration",
    "abstract": "This paper presents an interdisciplinary effort aiming to develop and share\nsustainable knowledge necessary to analyze, understand, and use published\nscientific results to advance reproducibility in multi-messenger astrophysics.\nSpecifically, we target the breakthrough work associated with the generation of\nthe first image of a black hole, called M87. The image was computed by the\nEvent Horizon Telescope Collaboration. Based on the artifacts made available by\nEHT, we deliver documentation, code, and a computational environment to\nreproduce the first image of a black hole. Our deliverables support new\ndiscovery in multi-messenger astrophysics by providing all the necessary tools\nfor generalizing methods and findings from the EHT use case. Challenges\nencountered during the reproducibility of EHT results are reported. The result\nof our effort is an open-source, containerized software package that enables\nthe public to reproduce the first image of a black hole in the galaxy M87.",
    "descriptor": "",
    "authors": [
      "Ria Patel",
      "Brandan Roachell",
      "Silvina Caino-Lores",
      "Ross Ketron",
      "Jacob Leonard",
      "Nigel Tan",
      "Duncan Brown",
      "Ewa Deelman",
      "Michela Taufer"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "High Energy Astrophysical Phenomena (astro-ph.HE)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "General Relativity and Quantum Cosmology (gr-qc)"
    ],
    "url": "https://arxiv.org/abs/2205.10267"
  },
  {
    "id": "arXiv:2205.10278",
    "title": "Self-supervised deep learning MRI reconstruction with Noisier2Noise",
    "abstract": "In recent years, there has been attention on leveraging the statistical\nmodeling capabilities of neural networks for reconstructing sub-sampled\nMagnetic Resonance Imaging (MRI) data. Most proposed methods assume the\nexistence of a representative fully-sampled dataset and use fully-supervised\ntraining. However, for many applications, fully sampled training data is not\navailable, and may be highly impractical to acquire. The development of\nself-supervised methods, which use only sub-sampled data for training, are\ntherefore highly desirable. This work extends the Noisier2Noise framework,\nwhich was originally constructed for self-supervised denoising tasks, to\nvariable density sub-sampled MRI data. Further, we use the Noisier2Noise\nframework to analytically explain the performance of Self-Supervised Learning\nvia Data Undersampling (SSDU), a recently proposed method that performs well in\npractice but until now lacked theoretical justification. We also use\nNoisier2Noise to propose a modification of SSDU that we find substantially\nimproves its reconstruction quality and robustness, offering a test set\nmean-squared-error within 1% of fully supervised training on the fastMRI brain\ndataset.",
    "descriptor": "\nComments: Submitted to IEEE TMI on 20th May 2022\n",
    "authors": [
      "Charles Millard",
      "Mark Chiew"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10278"
  },
  {
    "id": "arXiv:2205.10291",
    "title": "Hyper-diffusion on multiplex networks",
    "abstract": "Multiplex networks describe systems whose interactions can be of different\nnature, and are fundamental to understand complexity of networks beyond the\nframework of simple graphs. Recently it has been pointed out that restricting\nthe attention to pairwise interactions is also a limitation, as the vast\nmajority of complex systems include higher-order interactions and these\nstrongly affect their dynamics. Here, we propose hyper-diffusion on multiplex\nnetwork, a dynamical process in which the diffusion on each single layer is\ncoupled with the diffusion in other layers thanks to the presence of\nhigher-order interactions occurring when there exists link overlap in different\nlayers. We show that hyper-diffusion on a duplex (a network with two layers) is\ndriven by Hyper-Laplacians in which the relevance of higher-order interactions\ncan be tuned by a continuous parameter $\\delta_{11}$. By combining tools of\nspectral graph theory, applied topology and network science we provide a\ngeneral understanding of hyper-diffusion on duplex networks, including\ntheoretical bounds on the Fiedler and the largest eigenvalue of\nHyper-Laplacians and the asymptotic expansion of their spectrum for\n$\\delta_{11}\\ll1$ and $\\delta_{11}\\gg1$. Although hyper-diffusion on multiplex\nnetworks does not imply a direct \"transfer of mass\" among the layers (i.e. the\naverage state of replica nodes in each layer is a conserved quantity of the\ndynamics), we find that the dynamics of the two layers is coupled as the\nrelaxation to the steady state becomes synchronous when higher-order\ninteractions are taken into account and the Fiedler eigenvalue of the\nHyper-Laplacian is not localized on a single layer of the duplex network.",
    "descriptor": "\nComments: 27 pages, 5 figures, 8 appendices\n",
    "authors": [
      "Reza Ghorbanchian",
      "Vito Latora",
      "Ginestra Bianconi"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10291"
  },
  {
    "id": "arXiv:2205.10321",
    "title": "User Localization using RF Sensing: A Performance comparison between LIS  and mmWave Radars",
    "abstract": "Since electromagnetic signals are omnipresent, Radio Frequency (RF)-sensing\nhas the potential to become a universal sensing mechanism with applications in\nlocalization, smart-home, retail, gesture recognition, intrusion detection,\netc. Two emerging technologies in RF-sensing, namely sensing through Large\nIntelligent Surfaces (LISs) and mmWave Frequency-Modulated Continuous-Wave\n(FMCW) radars, have been successfully applied to a wide range of applications.\nIn this work, we compare LIS and mmWave radars for localization in real-world\nand simulated environments. In our experiments, the mmWave radar achieves 0.71\nIntersection Over Union (IOU) and 3cm error for bounding boxes, while LIS has\n0.56 IOU and 10cm distance error. Although the radar outperforms the LIS in\nterms of accuracy, LIS features additional applications in communication in\naddition to sensing scenarios.",
    "descriptor": "",
    "authors": [
      "Cristian J. Vaca-Rubio",
      "Dariush Salami",
      "Petar Popovski",
      "Elisabeth de Carvalho",
      "Zheng-Hua Tan",
      "Stephan Sigg"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10321"
  },
  {
    "id": "arXiv:2205.10323",
    "title": "Low power communication signal enhancement method of Internet of things  based on nonlocal mean denoising",
    "abstract": "In order to improve the transmission effect of low-power communication signal\nof Internet of things and compress the enhancement time of low-power\ncommunication signal, this paper designs a low-power communication signal\nenhancement method of Internet of things based on nonlocal mean denoising.\nFirstly, the residual of one-dimensional communication layer is pre processed\nby convolution core to obtain the residual of one-dimensional communication\nlayer; Then, according to the two classification recognition method, the noise\nreduction signal feature recognition of the low-power communication signal of\nthe Internet of things is realized, the non local mean noise reduction\nalgorithm is used to remove the low-power communication signal of the Internet\nof things, and the weight value between similar blocks is calculated according\nto the European distance method. Finally, the low-power communication signal\nenhancement of the Internet of things is realized by the non local mean value\ndenoising method. The experimental results show that the communication signal\nenhancement time overhead of this method is low, which is always less than\n2.6s. The lowest bit error rate after signal enhancement is about 1%, and the\nsignal-to-noise ratio is up to 18 dB, which shows that this method can achieve\nsignal enhancement.",
    "descriptor": "",
    "authors": [
      "Mingchuan Tian",
      "Jizheng Liu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10323"
  },
  {
    "id": "arXiv:2205.10324",
    "title": "Estimating electricity saving-potential in small offices using adaptive  thermal comfort",
    "abstract": "The smart control informed by IoT sensors and enabled by remotely controlled\ndevices can optimize the building operation to minimize unnecessary energy\nconsumption and improve indoor thermal comfort. This paper quantifies the\npotential for electricity savings in small office buildings from smart\nthermostat control and occupancy-informed smart plug control. This is done by\nsimulating the effect of adaptive setpoint temperature, occupancy-based HVAC\ncontrol, and night-purge free cooling on small office buildings across all\nmajor climate zones in the United States. Adopting these smart control measures\ncan achieve 8.9% to 20.4% of savings in total electricity consumption of small\noffice buildings, or equivalent to annual reductions between 12.2 kWh/m2 and\n30.4 kWh/m2 in electricity usage intensity. Among all climate zones, the hot\nand dry climates benefit the most from proposed smart controls and achieve the\nhighest percentages of electricity savings",
    "descriptor": "",
    "authors": [
      "Yujiao Chen",
      "Rongxin Yin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10324"
  },
  {
    "id": "arXiv:2205.10326",
    "title": "The Tensor Track VII: From Quantum Gravity to Artificial Intelligence",
    "abstract": "Assuming some familiarity with quantum field theory and with the tensor track\napproach that one of us presented in the previous series Tensor Track I to VI,\nwe provide, as usual, the developments in quantum gravity of the last two\nyears. Next we present in some detail two algorithms inspired by Random Tensor\nTheory which has been developed in the quantum gravity context. One is devoted\nto the detection and recovery of a signal in a random tensor, that can be\nassociated to the noise, with new theoretical guarantees for more general cases\nsuch as tensors with different dimensions. The other, SMPI, is more ambitious\nbut maybe less rigorous. It is devoted to significantly and fundamentally\nimprove the performance of algorithms for Tensor principal component analysis\nbut without complete theoretical guarantees yet. Then we sketch all sorts of\napplication relevant to information theory and artificial intelligence and\nprovide their corresponding bibliography.",
    "descriptor": "\nComments: 23 pages, 7 figures, 1 table\n",
    "authors": [
      "Mohamed Ouerfelli",
      "Vincent Rivasseau",
      "Mohamed Tamaazousti"
    ],
    "subjectives": [
      "High Energy Physics - Theory (hep-th)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10326"
  },
  {
    "id": "arXiv:2205.10327",
    "title": "What's the Harm? Sharp Bounds on the Fraction Negatively Affected by  Treatment",
    "abstract": "The fundamental problem of causal inference -- that we never observe\ncounterfactuals -- prevents us from identifying how many might be negatively\naffected by a proposed intervention. If, in an A/B test, half of users click\n(or buy, or watch, or renew, etc.), whether exposed to the standard experience\nA or a new one B, hypothetically it could be because the change affects no one,\nbecause the change positively affects half the user population to go from\nno-click to click while negatively affecting the other half, or something in\nbetween. While unknowable, this impact is clearly of material importance to the\ndecision to implement a change or not, whether due to fairness, long-term,\nsystemic, or operational considerations. We therefore derive the\ntightest-possible (i.e., sharp) bounds on the fraction negatively affected (and\nother related estimands) given data with only factual observations, whether\nexperimental or observational. Naturally, the more we can stratify individuals\nby observable covariates, the tighter the sharp bounds. Since these bounds\ninvolve unknown functions that must be learned from data, we develop a robust\ninference algorithm that is efficient almost regardless of how and how fast\nthese functions are learned, remains consistent when some are mislearned, and\nstill gives valid conservative bounds when most are mislearned. Our methodology\naltogether therefore strongly supports credible conclusions: it avoids\nspuriously point-identifying this unknowable impact, focusing on the best\nbounds instead, and it permits exceedingly robust inference on these. We\ndemonstrate our method in simulation studies and in a case study of career\ncounseling for the unemployed.",
    "descriptor": "",
    "authors": [
      "Nathan Kallus"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10327"
  },
  {
    "id": "arXiv:2205.10342",
    "title": "Self-supervised 3D anatomy segmentation using self-distilled masked  image transformer (SMIT)",
    "abstract": "Vision transformers, with their ability to more efficiently model long-range\ncontext, have demonstrated impressive accuracy gains in several computer vision\nand medical image analysis tasks including segmentation. However, such methods\nneed large labeled datasets for training, which is hard to obtain for medical\nimage analysis. Self-supervised learning (SSL) has demonstrated success in\nmedical image segmentation using convolutional networks. In this work, we\ndeveloped a \\underline{s}elf-distillation learning with \\underline{m}asked\n\\underline{i}mage modeling method to perform SSL for vision\n\\underline{t}ransformers (SMIT) applied to 3D multi-organ segmentation from CT\nand MRI. Our contribution is a dense pixel-wise regression within masked\npatches called masked image prediction, which we combined with masked patch\ntoken distillation as pretext task to pre-train vision transformers. We show\nour approach is more accurate and requires fewer fine tuning datasets than\nother pretext tasks. Unlike prior medical image methods, which typically used\nimage sets arising from disease sites and imaging modalities corresponding to\nthe target tasks, we used 3,643 CT scans (602,708 images) arising from head and\nneck, lung, and kidney cancers as well as COVID-19 for pre-training and applied\nit to abdominal organs segmentation from MRI pancreatic cancer patients as well\nas publicly available 13 different abdominal organs segmentation from CT. Our\nmethod showed clear accuracy improvement (average DSC of 0.875 from MRI and\n0.878 from CT) with reduced requirement for fine-tuning datasets over commonly\nused pretext tasks. Extensive comparisons against multiple current SSL methods\nwere done. Code will be made available upon acceptance for publication.",
    "descriptor": "\nComments: This paper has been early accepted by MICCAI 2022\n",
    "authors": [
      "Jue Jiang",
      "Neelam Tyagi",
      "Kathryn Tringale",
      "Christopher Crane",
      "Harini Veeraraghavan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10342"
  },
  {
    "id": "arXiv:1808.09406",
    "title": "Almost Envy-Free Allocations with Connected Bundles",
    "abstract": "Comments: Accepted journal version",
    "descriptor": "\nComments: Accepted journal version\n",
    "authors": [
      "Vittorio Bil\u00f2",
      "Ioannis Caragiannis",
      "Michele Flammini",
      "Ayumi Igarashi",
      "Gianpiero Monaco",
      "Dominik Peters",
      "Cosimo Vinci",
      "William S. Zwicker"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/1808.09406"
  },
  {
    "id": "arXiv:1905.10696",
    "title": "Lifelong Neural Predictive Coding: Learning Cumulatively Online without  Forgetting",
    "abstract": "Comments: Additional updates/edits/restructuring of dynamics/mechanics",
    "descriptor": "\nComments: Additional updates/edits/restructuring of dynamics/mechanics\n",
    "authors": [
      "Alexander Ororbia",
      "Ankur Mali",
      "Daniel Kifer",
      "C. Lee Giles"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1905.10696"
  },
  {
    "id": "arXiv:1906.04376",
    "title": "Recognizing License Plates in Real-Time",
    "abstract": "Comments: License Plate Detection and Recognition, Computer Vision, Supervised Learning",
    "descriptor": "\nComments: License Plate Detection and Recognition, Computer Vision, Supervised Learning\n",
    "authors": [
      "Xuewen Yang",
      "Xin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1906.04376"
  },
  {
    "id": "arXiv:1910.03090",
    "title": "Instagram Fake and Automated Account Detection",
    "abstract": "Instagram Fake and Automated Account Detection",
    "descriptor": "",
    "authors": [
      "Fatih Cagatay Akyon",
      "Esat Kalfaoglu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.03090"
  },
  {
    "id": "arXiv:2001.02297",
    "title": "Generating Semantic Adversarial Examples via Feature Manipulation",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:1705.09064 by other authors",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1705.09064 by other authors\n",
    "authors": [
      "Shuo Wang",
      "Surya Nepal",
      "Carsten Rudolph",
      "Marthie Grobler",
      "Shangyu Chen",
      "Tianle Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2001.02297"
  },
  {
    "id": "arXiv:2002.07262",
    "title": "Denotational semantics as a foundation for cost recurrence extraction  for functional languages",
    "abstract": "Comments: Revisions, mostly minor, some more discussion of related work. To appear in Journal of Functional Programming",
    "descriptor": "\nComments: Revisions, mostly minor, some more discussion of related work. To appear in Journal of Functional Programming\n",
    "authors": [
      "Norman Danner",
      "Daniel R. Licata"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2002.07262"
  },
  {
    "id": "arXiv:2003.07545",
    "title": "Interpretable Personalization via Policy Learning with Linear Decision  Boundaries",
    "abstract": "Interpretable Personalization via Policy Learning with Linear Decision  Boundaries",
    "descriptor": "",
    "authors": [
      "Zhaonan Qu",
      "Isabella Qian",
      "Zhengyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.07545"
  },
  {
    "id": "arXiv:2005.00175",
    "title": "Selecting Informative Contexts Improves Language Model Finetuning",
    "abstract": "Comments: Accepted submission at the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
    "descriptor": "\nComments: Accepted submission at the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n",
    "authors": [
      "Richard Antonello",
      "Nicole Beckage",
      "Javier Turek",
      "Alexander Huth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2005.00175"
  },
  {
    "id": "arXiv:2007.14717",
    "title": "Almost exact recovery in noisy semi-supervised learning",
    "abstract": "Almost exact recovery in noisy semi-supervised learning",
    "descriptor": "",
    "authors": [
      "Konstantin Avrachenkov",
      "Maximilien Dreveton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.14717"
  },
  {
    "id": "arXiv:2008.08612",
    "title": "A Survey on Text Simplification",
    "abstract": "Comments: plagiarism check failed",
    "descriptor": "\nComments: plagiarism check failed\n",
    "authors": [
      "Punardeep Sikka",
      "Vijay Mago"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2008.08612"
  },
  {
    "id": "arXiv:2009.05794",
    "title": "Open Benchmarking for Click-Through Rate Prediction",
    "abstract": "Comments: Accepted in CIKM 2021. See BARS-CTR at this https URL",
    "descriptor": "\nComments: Accepted in CIKM 2021. See BARS-CTR at this https URL\n",
    "authors": [
      "Jieming Zhu",
      "Jinyang Liu",
      "Shuai Yang",
      "Qi Zhang",
      "Xiuqiang He"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2009.05794"
  },
  {
    "id": "arXiv:2009.07514",
    "title": "A Unified Approach to Synchronization Problems over Subgroups of the  Orthogonal Group",
    "abstract": "A Unified Approach to Synchronization Problems over Subgroups of the  Orthogonal Group",
    "descriptor": "",
    "authors": [
      "Huikang Liu",
      "Man-Chung Yue",
      "Anthony Man-Cho So"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2009.07514"
  },
  {
    "id": "arXiv:2010.00990",
    "title": "An alternative proof of the vulnerability of retrieval in high intrinsic  dimensionality neighborhood",
    "abstract": "An alternative proof of the vulnerability of retrieval in high intrinsic  dimensionality neighborhood",
    "descriptor": "",
    "authors": [
      "Teddy Furon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.00990"
  },
  {
    "id": "arXiv:2011.02747",
    "title": "Incremental Refinements and Multiple Descriptions with Feedback",
    "abstract": "Comments: 62 pages. Accepted in the IEEE Transactions on Information Theory",
    "descriptor": "\nComments: 62 pages. Accepted in the IEEE Transactions on Information Theory\n",
    "authors": [
      "Jan \u00d8stergaard",
      "Uri Erez",
      "Ram Zamir"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2011.02747"
  },
  {
    "id": "arXiv:2011.09058",
    "title": "Layer-Wise Data-Free CNN Compression",
    "abstract": "Layer-Wise Data-Free CNN Compression",
    "descriptor": "",
    "authors": [
      "Maxwell Horton",
      "Yanzi Jin",
      "Ali Farhadi",
      "Mohammad Rastegari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.09058"
  },
  {
    "id": "arXiv:2012.00933",
    "title": "Global and Individualized Community Detection in Inhomogeneous  Multilayer Networks",
    "abstract": "Comments: Accepted to Annals of Statistics",
    "descriptor": "\nComments: Accepted to Annals of Statistics\n",
    "authors": [
      "Shuxiao Chen",
      "Sifan Liu",
      "Zongming Ma"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.00933"
  },
  {
    "id": "arXiv:2101.05921",
    "title": "An Improved Approximation Algorithm for the Minimum $k$-Edge Connected  Multi-Subgraph Problem",
    "abstract": "An Improved Approximation Algorithm for the Minimum $k$-Edge Connected  Multi-Subgraph Problem",
    "descriptor": "",
    "authors": [
      "Anna R. Karlin",
      "Nathan Klein",
      "Shayan Oveis Gharan",
      "Xinzhi Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2101.05921"
  },
  {
    "id": "arXiv:2101.06131",
    "title": "Flow stability for dynamic community detection",
    "abstract": "Comments: 54 pages, 14 figures. Accepted version by Science Advances",
    "descriptor": "\nComments: 54 pages, 14 figures. Accepted version by Science Advances\n",
    "authors": [
      "Alexandre Bovet",
      "Jean-Charles Delvenne",
      "Renaud Lambiotte"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2101.06131"
  },
  {
    "id": "arXiv:2104.04391",
    "title": "Flow-based Spatio-Temporal Structured Prediction of Dynamics",
    "abstract": "Comments: 11 pages, LaTeX; typos corrected, updated",
    "descriptor": "\nComments: 11 pages, LaTeX; typos corrected, updated\n",
    "authors": [
      "Mohsen Zand",
      "Ali Etemad",
      "Michael Greenspan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.04391"
  },
  {
    "id": "arXiv:2104.08223",
    "title": "MeshTalk: 3D Face Animation from Speech using Cross-Modality  Disentanglement",
    "abstract": "Comments: updated link to github repository and supplemental video",
    "descriptor": "\nComments: updated link to github repository and supplemental video\n",
    "authors": [
      "Alexander Richard",
      "Michael Zollhoefer",
      "Yandong Wen",
      "Fernando de la Torre",
      "Yaser Sheikh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.08223"
  },
  {
    "id": "arXiv:2104.08666",
    "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language  Models",
    "abstract": "Comments: Accepted to 4th Workshop on Gender Bias in Natural Language Processing, NAACL 2022",
    "descriptor": "\nComments: Accepted to 4th Workshop on Gender Bias in Natural Language Processing, NAACL 2022\n",
    "authors": [
      "Tejas Srinivasan",
      "Yonatan Bisk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.08666"
  },
  {
    "id": "arXiv:2105.02127",
    "title": "A class of new high-order finite-volume TENO schemes for hyperbolic  conservation laws with unstructured meshes",
    "abstract": "Comments: 53 pages, 23 figures",
    "descriptor": "\nComments: 53 pages, 23 figures\n",
    "authors": [
      "Zhe Ji",
      "Tian Liang",
      "Lin Fu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2105.02127"
  },
  {
    "id": "arXiv:2105.07480",
    "title": "In Congestion Games, Taxes Achieve Optimal Approximation",
    "abstract": "Comments: A preliminary version of this work appeared at the 22nd ACM Conference on Economics & Computation (EC'21)",
    "descriptor": "\nComments: A preliminary version of this work appeared at the 22nd ACM Conference on Economics & Computation (EC'21)\n",
    "authors": [
      "Dario Paccagnan",
      "Martin Gairing"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2105.07480"
  },
  {
    "id": "arXiv:2105.09978",
    "title": "A Generic Solution to Register-bounded Synthesis with an Application to  Discrete Orders",
    "abstract": "Comments: Previously this version appeared as arXiv:2205.01952 which was submitted as a new work by accident. This is a full version of same-name paper accepted to ICALP'22",
    "descriptor": "\nComments: Previously this version appeared as arXiv:2205.01952 which was submitted as a new work by accident. This is a full version of same-name paper accepted to ICALP'22\n",
    "authors": [
      "L\u00e9o Exibard",
      "Emmanuel Filiot",
      "Ayrat Khalimov"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2105.09978"
  },
  {
    "id": "arXiv:2105.10433",
    "title": "When is Assortment Optimization Optimal?",
    "abstract": "Comments: Full version of EC2022 paper. Forthcoming in Management Science",
    "descriptor": "\nComments: Full version of EC2022 paper. Forthcoming in Management Science\n",
    "authors": [
      "Will Ma"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2105.10433"
  },
  {
    "id": "arXiv:2105.15119",
    "title": "Policies for the Dynamic Traveling Maintainer Problem with Alerts",
    "abstract": "Policies for the Dynamic Traveling Maintainer Problem with Alerts",
    "descriptor": "",
    "authors": [
      "Paulo da Costa",
      "Peter Verleijsdonk",
      "Simon Voorberg",
      "Alp Akcay",
      "Stella Kapodistria",
      "Willem van Jaarsveld",
      "Yingqian Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.15119"
  },
  {
    "id": "arXiv:2106.00948",
    "title": "Unsupervised Out-of-Domain Detection via Pre-trained Transformers",
    "abstract": "Comments: Accepted by ACL 2021. Code is available at this https URL",
    "descriptor": "\nComments: Accepted by ACL 2021. Code is available at this https URL\n",
    "authors": [
      "Keyang Xu",
      "Tongzheng Ren",
      "Shikun Zhang",
      "Yihao Feng",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.00948"
  },
  {
    "id": "arXiv:2106.05238",
    "title": "On Algorithmic Stability in Unsupervised Representation Learning",
    "abstract": "Comments: 10 pages plus appendix",
    "descriptor": "\nComments: 10 pages plus appendix\n",
    "authors": [
      "Matthew Willetts",
      "Brooks Paige"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05238"
  },
  {
    "id": "arXiv:2106.07785",
    "title": "Multivariate Public Key Cryptosystem from Sidon Spaces",
    "abstract": "Comments: Appeared in Public-Key Cryptography - PKC 2021, 24th IACR International Conference on Practice and Theory of Public Key Cryptography",
    "descriptor": "\nComments: Appeared in Public-Key Cryptography - PKC 2021, 24th IACR International Conference on Practice and Theory of Public Key Cryptography\n",
    "authors": [
      "Netanel Raviv",
      "Ben Langton",
      "Itzhak Tamo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.07785"
  },
  {
    "id": "arXiv:2107.00309",
    "title": "Adversarial Sample Detection for Speaker Verification by Neural Vocoders",
    "abstract": "Comments: Accepted by ICASSP 2022",
    "descriptor": "\nComments: Accepted by ICASSP 2022\n",
    "authors": [
      "Haibin Wu",
      "Po-chun Hsu",
      "Ji Gao",
      "Shanshan Zhang",
      "Shen Huang",
      "Jian Kang",
      "Zhiyong Wu",
      "Helen Meng",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2107.00309"
  },
  {
    "id": "arXiv:2107.04184",
    "title": "Greedy structure learning from data that contain systematic missing  values",
    "abstract": "Greedy structure learning from data that contain systematic missing  values",
    "descriptor": "",
    "authors": [
      "Yang Liu",
      "Anthony C. Constantinou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.04184"
  },
  {
    "id": "arXiv:2107.06777",
    "title": "Synthesis in Style: Semantic Segmentation of Historical Documents using  Synthetic Data",
    "abstract": "Comments: Code available at: this https URL",
    "descriptor": "\nComments: Code available at: this https URL\n",
    "authors": [
      "Christian Bartz",
      "Hendrik Raetz",
      "Jona Otholt",
      "Christoph Meinel",
      "Haojin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.06777"
  },
  {
    "id": "arXiv:2107.10201",
    "title": "Learning a Large Neighborhood Search Algorithm for Mixed Integer  Programs",
    "abstract": "Learning a Large Neighborhood Search Algorithm for Mixed Integer  Programs",
    "descriptor": "",
    "authors": [
      "Nicolas Sonnerat",
      "Pengming Wang",
      "Ira Ktena",
      "Sergey Bartunov",
      "Vinod Nair"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.10201"
  },
  {
    "id": "arXiv:2107.11032",
    "title": "On shared and multiple information",
    "abstract": "On shared and multiple information",
    "descriptor": "",
    "authors": [
      "Cesare Magri"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2107.11032"
  },
  {
    "id": "arXiv:2107.13226",
    "title": "Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for  Short-Term Forecasting of Transit Passenger Flow",
    "abstract": "Comments: 21 pages,15 figures",
    "descriptor": "\nComments: 21 pages,15 figures\n",
    "authors": [
      "Yuxin He",
      "Lishuai Li",
      "Xinting Zhu",
      "Kwok Leung Tsui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.13226"
  },
  {
    "id": "arXiv:2107.14210",
    "title": "uiCA: Accurate Throughput Prediction of Basic Blocks on Recent Intel  Microarchitectures",
    "abstract": "uiCA: Accurate Throughput Prediction of Basic Blocks on Recent Intel  Microarchitectures",
    "descriptor": "",
    "authors": [
      "Andreas Abel",
      "Jan Reineke"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2107.14210"
  },
  {
    "id": "arXiv:2108.05055",
    "title": "Statistical Dependency Guided Contrastive Learning for Multiple Labeling  in Prenatal Ultrasound",
    "abstract": "Comments: Accepted by MICCAI-MLMI 2021",
    "descriptor": "\nComments: Accepted by MICCAI-MLMI 2021\n",
    "authors": [
      "Shuangchi He",
      "Zehui Lin",
      "Xin Yang",
      "Chaoyu Chen",
      "Jian Wang",
      "Xue Shuang",
      "Ziwei Deng",
      "Qin Liu",
      "Yan Cao",
      "Xiduo Lu",
      "Ruobing Huang",
      "Nishant Ravikumar",
      "Alejandro Frangi",
      "Yuanji Zhang",
      "Yi Xiong",
      "Dong Ni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.05055"
  },
  {
    "id": "arXiv:2108.10934",
    "title": "Mitigating Statistical Bias within Differentially Private Synthetic Data",
    "abstract": "Mitigating Statistical Bias within Differentially Private Synthetic Data",
    "descriptor": "",
    "authors": [
      "Sahra Ghalebikesabi",
      "Harrison Wilde",
      "Jack Jewson",
      "Arnaud Doucet",
      "Sebastian Vollmer",
      "Chris Holmes"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.10934"
  },
  {
    "id": "arXiv:2108.12603",
    "title": "WALNUT: A Benchmark on Weakly Supervised Learning for Natural Language  Understanding",
    "abstract": "Comments: Accepted to NAACL 2022 (Long Paper)",
    "descriptor": "\nComments: Accepted to NAACL 2022 (Long Paper)\n",
    "authors": [
      "Guoqing Zheng",
      "Giannis Karamanolakis",
      "Kai Shu",
      "Ahmed Hassan Awadallah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.12603"
  },
  {
    "id": "arXiv:2109.02282",
    "title": "The asymptotic distribution of the condition number for random circulant  matrices",
    "abstract": "Comments: 23 pages. References were added",
    "descriptor": "\nComments: 23 pages. References were added\n",
    "authors": [
      "Gerardo Barrera",
      "Paulo Manrique-Mir\u00f3n"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.02282"
  },
  {
    "id": "arXiv:2109.03709",
    "title": "Speeding up PCA with priming",
    "abstract": "Speeding up PCA with priming",
    "descriptor": "",
    "authors": [
      "B\u00e1lint M\u00e1t\u00e9",
      "Fran\u00e7ois Fleuret"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.03709"
  },
  {
    "id": "arXiv:2109.04353",
    "title": "Cross DQN: Cross Deep Q Network for Ads Allocation in Feed",
    "abstract": "Comments: Accepted by WWW-22",
    "descriptor": "\nComments: Accepted by WWW-22\n",
    "authors": [
      "Guogang Liao",
      "Ze Wang",
      "Xiaoxu Wu",
      "Xiaowen Shi",
      "Chuheng Zhang",
      "Yongkang Wang",
      "Xingxing Wang",
      "Dong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04353"
  },
  {
    "id": "arXiv:2109.11797",
    "title": "CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models",
    "abstract": "Comments: Work in progress",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Yuan Yao",
      "Ao Zhang",
      "Zhengyan Zhang",
      "Zhiyuan Liu",
      "Tat-Seng Chua",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.11797"
  },
  {
    "id": "arXiv:2109.12218",
    "title": "Long-Range Transformers for Dynamic Spatiotemporal Forecasting",
    "abstract": "Long-Range Transformers for Dynamic Spatiotemporal Forecasting",
    "descriptor": "",
    "authors": [
      "Jake Grigsby",
      "Zhe Wang",
      "Yanjun Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.12218"
  },
  {
    "id": "arXiv:2109.14078",
    "title": "Learning Periodic Tasks from Human Demonstrations",
    "abstract": "Comments: Accepted to ICRA 2022. Project page: this https URL",
    "descriptor": "\nComments: Accepted to ICRA 2022. Project page: this https URL\n",
    "authors": [
      "Jingyun Yang",
      "Junwu Zhang",
      "Connor Settle",
      "Akshara Rai",
      "Rika Antonova",
      "Jeannette Bohg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.14078"
  },
  {
    "id": "arXiv:2110.06975",
    "title": "Guided Policy Search using Sequential Convex Programming for  Initialization of Trajectory Optimization Algorithms",
    "abstract": "Comments: Presented in American Control Conference (ACC) 2022",
    "descriptor": "\nComments: Presented in American Control Conference (ACC) 2022\n",
    "authors": [
      "Taewan Kim",
      "Purnanand Elango",
      "Danylo Malyuta",
      "Behcet Acikmese"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.06975"
  },
  {
    "id": "arXiv:2110.07166",
    "title": "CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in  Abstractive Summarization",
    "abstract": "CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in  Abstractive Summarization",
    "descriptor": "",
    "authors": [
      "Prafulla Kumar Choubey",
      "Alexander R. Fabbri",
      "Jesse Vig",
      "Chien-Sheng Wu",
      "Wenhao Liu",
      "Nazneen Fatema Rajani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07166"
  },
  {
    "id": "arXiv:2110.08412",
    "title": "Evaluating the Faithfulness of Importance Measures in NLP by Recursively  Masking Allegedly Important Tokens and Retraining",
    "abstract": "Evaluating the Faithfulness of Importance Measures in NLP by Recursively  Masking Allegedly Important Tokens and Retraining",
    "descriptor": "",
    "authors": [
      "Andreas Madsen",
      "Nicholas Meade",
      "Vaibhav Adlakha",
      "Siva Reddy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08412"
  },
  {
    "id": "arXiv:2110.09035",
    "title": "Edge Rewiring Goes Neural: Boosting Network Resilience without Rich  Features",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Shanchao Yang",
      "Kaili Ma",
      "Baoxiang Wang",
      "Tianshu Yu",
      "Hongyuan Zha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.09035"
  },
  {
    "id": "arXiv:2110.11628",
    "title": "CI-Based One-Bit Precoding for Multiuser Downlink Massive MIMO Systems  with PSK Modulation: A Negative $\\ell_1$ Penalty Approach",
    "abstract": "Comments: 13 pages, 8 figures, submitted for possible publication. arXiv admin note: text overlap with arXiv:2110.04768",
    "descriptor": "\nComments: 13 pages, 8 figures, submitted for possible publication. arXiv admin note: text overlap with arXiv:2110.04768\n",
    "authors": [
      "Zheyu Wu",
      "Bo Jiang",
      "Ya-Feng Liu",
      "Yu-Hong Dai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.11628"
  },
  {
    "id": "arXiv:2110.15447",
    "title": "On Wasted Contributions: Understanding the Dynamics of  Contributor-Abandoned Pull Requests",
    "abstract": "Comments: Manuscript accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM)",
    "descriptor": "\nComments: Manuscript accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM)\n",
    "authors": [
      "SayedHassan Khatoonabadi",
      "Diego Elias Costa",
      "Rabe Abdalkareem",
      "Emad Shihab"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.15447"
  },
  {
    "id": "arXiv:2111.03904",
    "title": "On pseudo-absence generation and machine learning for locust breeding  ground prediction in Africa",
    "abstract": "Comments: AI for Humanitarian Assistance and Disaster Response (AI+HADR) workshop, NeurIPS 2021",
    "descriptor": "\nComments: AI for Humanitarian Assistance and Disaster Response (AI+HADR) workshop, NeurIPS 2021\n",
    "authors": [
      "Ibrahim Salihu Yusuf",
      "Kale-ab Tessera",
      "Thomas Tumiel",
      "Zohra Slim",
      "Amine Kerkeni",
      "Sella Nevo",
      "Arnu Pretorius"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2111.03904"
  },
  {
    "id": "arXiv:2111.06336",
    "title": "Character-level HyperNetworks for Hate Speech Detection",
    "abstract": "Character-level HyperNetworks for Hate Speech Detection",
    "descriptor": "",
    "authors": [
      "Tomer Wullach",
      "Amir Adler",
      "Einat Minkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2111.06336"
  },
  {
    "id": "arXiv:2111.07603",
    "title": "Counterfactual Temporal Point Processes",
    "abstract": "Counterfactual Temporal Point Processes",
    "descriptor": "",
    "authors": [
      "Kimia Noorbakhsh",
      "Manuel Gomez Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.07603"
  },
  {
    "id": "arXiv:2111.09311",
    "title": "Shifted Brownian Fluctuation Game",
    "abstract": "Shifted Brownian Fluctuation Game",
    "descriptor": "",
    "authors": [
      "Song-Kyoo Kim"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2111.09311"
  },
  {
    "id": "arXiv:2111.12389",
    "title": "Track Boosting and Synthetic Data Aided Drone Detection",
    "abstract": "Comments: Published at AVSS 2021",
    "descriptor": "\nComments: Published at AVSS 2021\n",
    "authors": [
      "Fatih Cagatay Akyon",
      "Ogulcan Eryuksel",
      "Kamil Anil Ozfuttu",
      "Sinan Onur Altinuc"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.12389"
  },
  {
    "id": "arXiv:2111.13207",
    "title": "Characteristic Neural Ordinary Differential Equations",
    "abstract": "Characteristic Neural Ordinary Differential Equations",
    "descriptor": "",
    "authors": [
      "Xingzi Xu",
      "Ali Hasan",
      "Khalil Elkhalil",
      "Jie Ding",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.13207"
  },
  {
    "id": "arXiv:2112.00734",
    "title": "Personalized Federated Learning with Adaptive Batchnorm for Healthcare",
    "abstract": "Comments: Accepted by IEEE Transactions on Big Data; code: this https URL arXiv admin note: substantial text overlap with arXiv:2106.01009",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Big Data; code: this https URL arXiv admin note: substantial text overlap with arXiv:2106.01009\n",
    "authors": [
      "Wang Lu",
      "Jindong Wang",
      "Yiqiang Chen",
      "Xin Qin",
      "Renjun Xu",
      "Dimitrios Dimitriadis",
      "Tao Qin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00734"
  },
  {
    "id": "arXiv:2112.00900",
    "title": "Empirical Game-Theoretic Analysis for Mean Field Games",
    "abstract": "Comments: 14 papes, 3 figures",
    "descriptor": "\nComments: 14 papes, 3 figures\n",
    "authors": [
      "Yongzhao Wang",
      "Michael P. Wellman"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.00900"
  },
  {
    "id": "arXiv:2112.01317",
    "title": "Monolith to Microservices: Representing Application Software through  Heterogeneous Graph Neural Network",
    "abstract": "Comments: The paper has been accepted for publication at IJCAI-ECAI 2022 (main research track)",
    "descriptor": "\nComments: The paper has been accepted for publication at IJCAI-ECAI 2022 (main research track)\n",
    "authors": [
      "Alex Mathai",
      "Sambaran Bandyopadhyay",
      "Utkarsh Desai",
      "Srikanth Tamilselvam"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.01317"
  },
  {
    "id": "arXiv:2112.05008",
    "title": "Millimeter Wave Localization with Imperfect Training Data using Shallow  Neural Networks",
    "abstract": "Comments: 6 pages, 9 figures. The paper was accepted at IEEE WCNC 2022",
    "descriptor": "\nComments: 6 pages, 9 figures. The paper was accepted at IEEE WCNC 2022\n",
    "authors": [
      "Anish Shastri",
      "Joan Palacios",
      "Paolo Casari"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.05008"
  },
  {
    "id": "arXiv:2112.07030",
    "title": "Clustering with fair-center representation: parameterized approximation  algorithms and heuristics",
    "abstract": "Clustering with fair-center representation: parameterized approximation  algorithms and heuristics",
    "descriptor": "",
    "authors": [
      "Suhas Thejaswi",
      "Ameet Gadekar",
      "Bruno Ordozgoiti",
      "Michal Osadnik"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2112.07030"
  },
  {
    "id": "arXiv:2112.07457",
    "title": "Triangulation candidates for Bayesian optimization",
    "abstract": "Comments: 10 pages, 5 figures",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Robert B. Gramacy",
      "Annie Sauer",
      "Nathan Wycoff"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.07457"
  },
  {
    "id": "arXiv:2112.08770",
    "title": "Proposition-Level Clustering for Multi-Document Summarization",
    "abstract": "Comments: NAACl 2022",
    "descriptor": "\nComments: NAACl 2022\n",
    "authors": [
      "Ori Ernst",
      "Avi Caciularu",
      "Ori Shapira",
      "Ramakanth Pasunuru",
      "Mohit Bansal",
      "Jacob Goldberger",
      "Ido Dagan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.08770"
  },
  {
    "id": "arXiv:2112.09436",
    "title": "Privacy preserving n-party scalar product protocol",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Florian van Daalen",
      "Inigo Bermejo",
      "Lianne Ippel",
      "Andre Dekkers"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.09436"
  },
  {
    "id": "arXiv:2112.09693",
    "title": "Generalisation effects of predictive uncertainty estimation in deep  learning for digital pathology",
    "abstract": "Generalisation effects of predictive uncertainty estimation in deep  learning for digital pathology",
    "descriptor": "",
    "authors": [
      "Milda Pocevi\u010di\u016bt\u0117",
      "Gabriel Eilertsen",
      "Sofia Jarkman",
      "Claes Lundstr\u00f6m"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2112.09693"
  },
  {
    "id": "arXiv:2112.12594",
    "title": "Continual Depth-limited Responses for Computing Counter-strategies in  Extensive-form Games",
    "abstract": "Comments: 12 pages, 16 figures",
    "descriptor": "\nComments: 12 pages, 16 figures\n",
    "authors": [
      "David Milec",
      "Viliam Lis\u00fd"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2112.12594"
  },
  {
    "id": "arXiv:2201.01893",
    "title": "Flow-Guided Sparse Transformer for Video Deblurring",
    "abstract": "Comments: ICML 2022; The First Transformer-based method for Video Deblurring",
    "descriptor": "\nComments: ICML 2022; The First Transformer-based method for Video Deblurring\n",
    "authors": [
      "Jing Lin",
      "Yuanhao Cai",
      "Xiaowan Hu",
      "Haoqian Wang",
      "Youliang Yan",
      "Xueyi Zou",
      "Henghui Ding",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.01893"
  },
  {
    "id": "arXiv:2201.02664",
    "title": "Optimizing the Communication-Accuracy Trade-off in Federated Learning  with Rate-Distortion Theory",
    "abstract": "Optimizing the Communication-Accuracy Trade-off in Federated Learning  with Rate-Distortion Theory",
    "descriptor": "",
    "authors": [
      "Nicole Mitchell",
      "Johannes Ball\u00e9",
      "Zachary Charles",
      "Jakub Kone\u010dn\u00fd"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.02664"
  },
  {
    "id": "arXiv:2201.03326",
    "title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning  Approach",
    "abstract": "Comments: Accepted as Oral at IJCNN 2022. arXiv admin note: substantial text overlap with arXiv:2012.06755",
    "descriptor": "\nComments: Accepted as Oral at IJCNN 2022. arXiv admin note: substantial text overlap with arXiv:2012.06755\n",
    "authors": [
      "Davide Buffelli",
      "Fabio Vandin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.03326"
  },
  {
    "id": "arXiv:2201.03948",
    "title": "Function Computation Under Privacy, Secrecy, Distortion, and  Communication Constraints",
    "abstract": "Comments: A previous version appeared in the Entropy journal",
    "descriptor": "\nComments: A previous version appeared in the Entropy journal\n",
    "authors": [
      "Onur G\u00fcnl\u00fc"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.03948"
  },
  {
    "id": "arXiv:2201.04560",
    "title": "Pseudopotential Lattice Boltzmann Method for boiling heat transfer: a  mesh refinement procedure",
    "abstract": "Comments: 29 pages, 16 figures",
    "descriptor": "\nComments: 29 pages, 16 figures\n",
    "authors": [
      "Alfredo Jaramillo",
      "Vin\u00edcius Pessoa Mapelli",
      "Luben Cabezas-G\u00f3mez"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.04560"
  },
  {
    "id": "arXiv:2201.08504",
    "title": "Deep reinforcement learning under signal temporal logic constraints  using Lagrangian relaxation",
    "abstract": "Comments: 12 pages, 15 figures",
    "descriptor": "\nComments: 12 pages, 15 figures\n",
    "authors": [
      "Junya Ikemoto",
      "Toshimitsu Ushio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.08504"
  },
  {
    "id": "arXiv:2201.11429",
    "title": "GMRES using pseudoinverse for range symmetric singular systems",
    "abstract": "Comments: Sentence at end of Section 1 when rhs contains discretization, measurement errors. Section 2 on motivation. Theorem 4.1: necessary, sufficient conditions for inconsistent, consistent cases. After Theorem 4.1, difference between theory and experiments explained. Modified Definition 2. Eliminated results for plat1919, saylr3. Modified Conclusions. References 1,2,3 on applications",
    "descriptor": "\nComments: Sentence at end of Section 1 when rhs contains discretization, measurement errors. Section 2 on motivation. Theorem 4.1: necessary, sufficient conditions for inconsistent, consistent cases. After Theorem 4.1, difference between theory and experiments explained. Modified Definition 2. Eliminated results for plat1919, saylr3. Modified Conclusions. References 1,2,3 on applications\n",
    "authors": [
      "Kota Sugihara",
      "Ken Hayami",
      "Liao Zeyu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.11429"
  },
  {
    "id": "arXiv:2201.11443",
    "title": "Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond",
    "abstract": "Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond",
    "descriptor": "",
    "authors": [
      "Nils Dycke",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.11443"
  },
  {
    "id": "arXiv:2201.11876",
    "title": "Regionalized Optimization",
    "abstract": "Regionalized Optimization",
    "descriptor": "",
    "authors": [
      "Gr\u00e9goire Sergeant-Perthuis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.11876"
  },
  {
    "id": "arXiv:2201.12293",
    "title": "Understanding Why Generalized Reweighting Does Not Improve Over ERM",
    "abstract": "Comments: 38 pages",
    "descriptor": "\nComments: 38 pages\n",
    "authors": [
      "Runtian Zhai",
      "Chen Dan",
      "Zico Kolter",
      "Pradeep Ravikumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12293"
  },
  {
    "id": "arXiv:2202.00834",
    "title": "Nonlinear Initialization Methods for Low-Rank Neural Networks",
    "abstract": "Comments: 32 pages, 4 figures, in submission. fixed some errors in previous versions and re-structured/re-focused the paper",
    "descriptor": "\nComments: 32 pages, 4 figures, in submission. fixed some errors in previous versions and re-structured/re-focused the paper\n",
    "authors": [
      "Kiran Vodrahalli",
      "Rakesh Shivanna",
      "Maheswaran Sathiamoorthy",
      "Sagar Jain",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.00834"
  },
  {
    "id": "arXiv:2202.01354",
    "title": "Dissecting BFT Consensus: In Trusted Components we Trust!",
    "abstract": "Dissecting BFT Consensus: In Trusted Components we Trust!",
    "descriptor": "",
    "authors": [
      "Suyash Gupta",
      "Sajjad Rahnama",
      "Shubham Pandey",
      "Natacha Crooks",
      "Mohammad Sadoghi"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.01354"
  },
  {
    "id": "arXiv:2202.01748",
    "title": "Sequentially learning the topological ordering of causal directed  acyclic graphs with likelihood ratio scores",
    "abstract": "Sequentially learning the topological ordering of causal directed  acyclic graphs with likelihood ratio scores",
    "descriptor": "",
    "authors": [
      "Gabriel Ruiz",
      "Oscar Hernan Madrid Padilla",
      "Qing Zhou"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.01748"
  },
  {
    "id": "arXiv:2202.04010",
    "title": "Multilevel Binary Polar-Coded Modulation Achieving the Capacity of  Asymmetric Channels",
    "abstract": "Comments: 6 pages, 4 figures; to be presented at ISIT 2022",
    "descriptor": "\nComments: 6 pages, 4 figures; to be presented at ISIT 2022\n",
    "authors": [
      "Constantin Runge",
      "Thomas Wiegart",
      "Diego Lentner",
      "Tobias Prinz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2202.04010"
  },
  {
    "id": "arXiv:2202.04599",
    "title": "Missing Data Imputation and Acquisition with Deep Hierarchical Models  and Hamiltonian Monte Carlo",
    "abstract": "Missing Data Imputation and Acquisition with Deep Hierarchical Models  and Hamiltonian Monte Carlo",
    "descriptor": "",
    "authors": [
      "Ignacio Peis",
      "Chao Ma",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04599"
  },
  {
    "id": "arXiv:2202.05146",
    "title": "EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction",
    "abstract": "Comments: 39th International Conference on Machine Learning (ICML 2022)",
    "descriptor": "\nComments: 39th International Conference on Machine Learning (ICML 2022)\n",
    "authors": [
      "Hannes St\u00e4rk",
      "Octavian-Eugen Ganea",
      "Lagnajit Pattanaik",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05146"
  },
  {
    "id": "arXiv:2202.06679",
    "title": "Liveness and Latency of Byzantine State-Machine Replication",
    "abstract": "Liveness and Latency of Byzantine State-Machine Replication",
    "descriptor": "",
    "authors": [
      "Manuel Bravo",
      "Gregory Chockler",
      "Alexey Gotsman"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.06679"
  },
  {
    "id": "arXiv:2202.06757",
    "title": "Variational quantum solutions to the Shortest Vector Problem",
    "abstract": "Variational quantum solutions to the Shortest Vector Problem",
    "descriptor": "",
    "authors": [
      "Martin R. Albrecht",
      "Milo\u0161 Prokop",
      "Yixin Shen",
      "Petros Wallden"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2202.06757"
  },
  {
    "id": "arXiv:2202.07638",
    "title": "On the design of scalable networks rejecting first order disturbances",
    "abstract": "Comments: Accept to be presented in NecSys22, Zurich",
    "descriptor": "\nComments: Accept to be presented in NecSys22, Zurich\n",
    "authors": [
      "Shihao Xie",
      "Giovanni Russo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2202.07638"
  },
  {
    "id": "arXiv:2202.07761",
    "title": "Further Collapses in TFNP",
    "abstract": "Further Collapses in TFNP",
    "descriptor": "",
    "authors": [
      "Mika G\u00f6\u00f6s",
      "Alexandros Hollender",
      "Siddhartha Jain",
      "Gilbert Maystre",
      "William Pires",
      "Robert Robere",
      "Ran Tao"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2202.07761"
  },
  {
    "id": "arXiv:2202.08539",
    "title": "When, where, and how to add new neurons to ANNs",
    "abstract": "Comments: Accepted at the 1st AutoML conference, 2022",
    "descriptor": "\nComments: Accepted at the 1st AutoML conference, 2022\n",
    "authors": [
      "Kaitlin Maile",
      "Emmanuel Rachelson",
      "Herv\u00e9 Luga",
      "Dennis G. Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08539"
  },
  {
    "id": "arXiv:2202.09048",
    "title": "Task Specific Attention is one more thing you need for object detection",
    "abstract": "Task Specific Attention is one more thing you need for object detection",
    "descriptor": "",
    "authors": [
      "Sang Yon Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.09048"
  },
  {
    "id": "arXiv:2202.10122",
    "title": "HCMD-zero: Learning Value Aligned Mechanisms from Data",
    "abstract": "HCMD-zero: Learning Value Aligned Mechanisms from Data",
    "descriptor": "",
    "authors": [
      "Jan Balaguer",
      "Raphael Koster",
      "Ari Weinstein",
      "Lucy Campbell-Gillingham",
      "Christopher Summerfield",
      "Matthew Botvinick",
      "Andrea Tacchetti"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2202.10122"
  },
  {
    "id": "arXiv:2202.10134",
    "title": "MCMARL: Parameterizing Value Function via Mixture of Categorical  Distributions for Multi-Agent Reinforcement Learning",
    "abstract": "MCMARL: Parameterizing Value Function via Mixture of Categorical  Distributions for Multi-Agent Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Jian Zhao",
      "Mingyu Yang",
      "Youpeng Zhao",
      "Xunhan Hu",
      "Wengang Zhou",
      "Jiangcheng Zhu",
      "Houqiang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10134"
  },
  {
    "id": "arXiv:2202.10199",
    "title": "Permutation Predictions for Non-Clairvoyant Scheduling",
    "abstract": "Comments: SPAA 2022",
    "descriptor": "\nComments: SPAA 2022\n",
    "authors": [
      "Alexander Lindermayr",
      "Nicole Megow"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10199"
  },
  {
    "id": "arXiv:2202.11891",
    "title": "HMD-EgoPose: Head-Mounted Display-Based Egocentric Marker-Less Tool and  Hand Pose Estimation for Augmented Surgical Guidance",
    "abstract": "Comments: Accepted for publication in IJCARS; 17 pages, 3 figures",
    "descriptor": "\nComments: Accepted for publication in IJCARS; 17 pages, 3 figures\n",
    "authors": [
      "Mitchell Doughty",
      "Nilesh R. Ghugre"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.11891"
  },
  {
    "id": "arXiv:2202.12384",
    "title": "TwistSLAM: Constrained SLAM in Dynamic Environment",
    "abstract": "TwistSLAM: Constrained SLAM in Dynamic Environment",
    "descriptor": "",
    "authors": [
      "Mathieu Gonzalez",
      "Eric Marchand",
      "Amine Kacete",
      "J\u00e9r\u00f4me Royan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.12384"
  },
  {
    "id": "arXiv:2203.00316",
    "title": "First do not fall: learning to exploit a wall with a damaged humanoid  robot",
    "abstract": "Comments: Video presenting the results: this https URL",
    "descriptor": "\nComments: Video presenting the results: this https URL\n",
    "authors": [
      "Timoth\u00e9e Anne",
      "Elo\u00efse Dalin",
      "Ivan Bergonzani",
      "Serena Ivaldi",
      "Jean-Baptiste Mouret"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.00316"
  },
  {
    "id": "arXiv:2203.01369",
    "title": "ePA*SE: Edge-based Parallel A* for Slow Evaluations",
    "abstract": "Comments: Proceedings of the International Symposium on Combinatorial Search (SoCS) 2022",
    "descriptor": "\nComments: Proceedings of the International Symposium on Combinatorial Search (SoCS) 2022\n",
    "authors": [
      "Shohin Mukherjee",
      "Sandip Aine",
      "Maxim Likhachev"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.01369"
  },
  {
    "id": "arXiv:2203.02321",
    "title": "Actuator Scheduling for Linear Systems: A Convex Relaxation Approach",
    "abstract": "Comments: 8 pages, 4 figures",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Junjie Jiao",
      "Dipankar Maity",
      "John S. Baras",
      "Sandra Hirche"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.02321"
  },
  {
    "id": "arXiv:2203.03397",
    "title": "OverlapTransformer: An Efficient and Rotation-Invariant Transformer  Network for LiDAR-Based Place Recognition",
    "abstract": "Comments: This paper is a cooperation between two countries and is currently under checking",
    "descriptor": "\nComments: This paper is a cooperation between two countries and is currently under checking\n",
    "authors": [
      "Junyi Ma",
      "Jun Zhang",
      "Jintao Xu",
      "Rui Ai",
      "Weihao Gu",
      "Cyrill Stachniss",
      "Xieyuanli Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.03397"
  },
  {
    "id": "arXiv:2203.03989",
    "title": "Adaptor: Objective-Centric Adaptation Framework for Language Models",
    "abstract": "Comments: 60th Annual Meeting of the ACL (ACL 2022): System Demonstrations paper",
    "descriptor": "\nComments: 60th Annual Meeting of the ACL (ACL 2022): System Demonstrations paper\n",
    "authors": [
      "Michal \u0160tef\u00e1nik",
      "V\u00edt Novotn\u00fd",
      "Nikola Groverov\u00e1",
      "Petr Sojka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.03989"
  },
  {
    "id": "arXiv:2203.04051",
    "title": "Robot Learning of Mobile Manipulation with Reachability Behavior Priors",
    "abstract": "Comments: Submitted to RAL-IROS 2022",
    "descriptor": "\nComments: Submitted to RAL-IROS 2022\n",
    "authors": [
      "Snehal Jauhri",
      "Jan Peters",
      "Georgia Chalvatzaki"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.04051"
  },
  {
    "id": "arXiv:2203.04275",
    "title": "Robust Multi-Task Learning and Online Refinement for Spacecraft Pose  Estimation across Domain Gap",
    "abstract": "Comments: Additional experiment with $\\phi$ = 6, restructuring of paper; to be presented at the 11th International Workshop on Satellite Constellations & Formation Flying, Milan, Italy, 7-10 June 2022",
    "descriptor": "\nComments: Additional experiment with $\\phi$ = 6, restructuring of paper; to be presented at the 11th International Workshop on Satellite Constellations & Formation Flying, Milan, Italy, 7-10 June 2022\n",
    "authors": [
      "Tae Ha Park",
      "Simone D'Amico"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.04275"
  },
  {
    "id": "arXiv:2203.06660",
    "title": "Incomplete List Setting of the Hospitals/Residents Problem with  Maximally Satisfying Lower Quotas",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2105.03093",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2105.03093\n",
    "authors": [
      "Kazuhisa Makino",
      "Shuichi Miyazaki",
      "Yu Yokoi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2203.06660"
  },
  {
    "id": "arXiv:2203.07772",
    "title": "Fast Autofocusing using Tiny Transformer Networks for Digital  Holographic Microscopy",
    "abstract": "Fast Autofocusing using Tiny Transformer Networks for Digital  Holographic Microscopy",
    "descriptor": "",
    "authors": [
      "St\u00e9phane Cuenat",
      "Louis Andr\u00e9oli",
      "Antoine N. Andr\u00e9",
      "Patrick Sandoz",
      "Guillaume J. Laurent",
      "Rapha\u00ebl Couturier",
      "Maxime Jacquot"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2203.07772"
  },
  {
    "id": "arXiv:2203.07774",
    "title": "An Empirical Study of Market Inefficiencies in Uniswap and SushiSwap",
    "abstract": "An Empirical Study of Market Inefficiencies in Uniswap and SushiSwap",
    "descriptor": "",
    "authors": [
      "Jan Arvid Berg",
      "Robin Fritsch",
      "Lioba Heimbach",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2203.07774"
  },
  {
    "id": "arXiv:2203.09046",
    "title": "Memristive deep belief neural network by silicon synapses",
    "abstract": "Memristive deep belief neural network by silicon synapses",
    "descriptor": "",
    "authors": [
      "Wei Wang",
      "Loai Danial",
      "Yang Li",
      "Eric Herbelin",
      "Evgeny Pikhay",
      "Yakov Roizin",
      "Barak Hoffer",
      "Zhongrui Wang",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2203.09046"
  },
  {
    "id": "arXiv:2203.09493",
    "title": "Modellieren mit Heraklit: Prinzipien und Fallstudie",
    "abstract": "Comments: 16 pages, 10 figures, author prepared version of final manuscript accepted at Modellierung 2022, in German",
    "descriptor": "\nComments: 16 pages, 10 figures, author prepared version of final manuscript accepted at Modellierung 2022, in German\n",
    "authors": [
      "Peter Fettke",
      "Wolfgang Reisig"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2203.09493"
  },
  {
    "id": "arXiv:2203.09590",
    "title": "Enhanced Temporal Knowledge Embeddings with Contextualized Language  Representations",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Zhen Han",
      "Ruotong Liao",
      "Beiyan Liu",
      "Yao Zhang",
      "Zifeng Ding",
      "Jindong Gu",
      "Heinz K\u00f6ppl",
      "Hinrich Sch\u00fctze",
      "Volker Tresp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.09590"
  },
  {
    "id": "arXiv:2203.12644",
    "title": "Linearizing Transformer with Key-Value Memory",
    "abstract": "Linearizing Transformer with Key-Value Memory",
    "descriptor": "",
    "authors": [
      "Yizhe Zhang",
      "Deng Cai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.12644"
  },
  {
    "id": "arXiv:2203.13094",
    "title": "Six Insights into 6G: Orientation and Input for Developing Your  Strategic 6G Research Plan",
    "abstract": "Comments: 20 pages, 1 figure",
    "descriptor": "\nComments: 20 pages, 1 figure\n",
    "authors": [
      "Kimberley Parsons Trommler",
      "Matthias Hafner",
      "Wolfgang Kellerer",
      "Peter Merz",
      "Sigurd Schuster",
      "Josef Urban",
      "Uwe Baeder",
      "Bertram Gunzelmann",
      "Andreas Kornbichler"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2203.13094"
  },
  {
    "id": "arXiv:2203.13319",
    "title": "Remember and Forget Experience Replay for Multi-Agent Reinforcement  Learning",
    "abstract": "Remember and Forget Experience Replay for Multi-Agent Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Pascal Weber",
      "Daniel W\u00e4lchli",
      "Mustafa Zeqiri",
      "Petros Koumoutsakos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.13319"
  },
  {
    "id": "arXiv:2203.14098",
    "title": "Uncertainty-aware Contrastive Distillation for Incremental Semantic  Segmentation",
    "abstract": "Comments: TPAMI",
    "descriptor": "\nComments: TPAMI\n",
    "authors": [
      "Guanglei Yang",
      "Enrico Fini",
      "Dan Xu",
      "Paolo Rota",
      "Mingli Ding",
      "Moin Nabi",
      "Xavier Alameda-Pineda",
      "Elisa Ricci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.14098"
  },
  {
    "id": "arXiv:2203.14465",
    "title": "STaR: Bootstrapping Reasoning With Reasoning",
    "abstract": "STaR: Bootstrapping Reasoning With Reasoning",
    "descriptor": "",
    "authors": [
      "Eric Zelikman",
      "Yuhuai Wu",
      "Jesse Mu",
      "Noah D. Goodman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.14465"
  },
  {
    "id": "arXiv:2203.16423",
    "title": "Data-Driven Model Predictive Control for Linear Time-Periodic Systems",
    "abstract": "Comments: 12 pages, 7 figures, extended version of a CDC 2022 submission",
    "descriptor": "\nComments: 12 pages, 7 figures, extended version of a CDC 2022 submission\n",
    "authors": [
      "Ruiqi Li",
      "John W. Simpson-Porco",
      "Stephen L. Smith"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.16423"
  },
  {
    "id": "arXiv:2203.16487",
    "title": "Lossless Speedup of Autoregressive Translation with Generalized  Aggressive Decoding",
    "abstract": "Comments: Work in progress",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Heming Xia",
      "Tao Ge",
      "Furu Wei",
      "Zhifang Sui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.16487"
  },
  {
    "id": "arXiv:2203.17225",
    "title": "A Baseline Readability Model for Cebuano",
    "abstract": "Comments: Accepted to BEA Workshop at NAACL 2022",
    "descriptor": "\nComments: Accepted to BEA Workshop at NAACL 2022\n",
    "authors": [
      "Lloyd Lois Antonie Reyes",
      "Michael Antonio Iba\u00f1ez",
      "Ranz Sapinit",
      "Mohammed Hussien",
      "Joseph Marvin Imperial"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.17225"
  },
  {
    "id": "arXiv:2204.00888",
    "title": "Learning List-wise Representation in Reinforcement Learning for Ads  Allocation with Multiple Auxiliary Tasks",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2109.04353, arXiv:2204.00377",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2109.04353, arXiv:2204.00377\n",
    "authors": [
      "Ze Wang",
      "Guogang Liao",
      "Xiaowen Shi",
      "Xiaoxu Wu",
      "Chuheng Zhang",
      "Yongkang Wang",
      "Xingxing Wang",
      "Dong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.00888"
  },
  {
    "id": "arXiv:2204.02329",
    "title": "Can language models learn from explanations in context?",
    "abstract": "Can language models learn from explanations in context?",
    "descriptor": "",
    "authors": [
      "Andrew K. Lampinen",
      "Ishita Dasgupta",
      "Stephanie C. Y. Chan",
      "Kory Matthewson",
      "Michael Henry Tessler",
      "Antonia Creswell",
      "James L. McClelland",
      "Jane X. Wang",
      "Felix Hill"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.02329"
  },
  {
    "id": "arXiv:2204.07441",
    "title": "COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for  Cross-Modal Retrieval",
    "abstract": "Comments: Accepted by CVPR2022",
    "descriptor": "\nComments: Accepted by CVPR2022\n",
    "authors": [
      "Haoyu Lu",
      "Nanyi Fei",
      "Yuqi Huo",
      "Yizhao Gao",
      "Zhiwu Lu",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.07441"
  },
  {
    "id": "arXiv:2204.09179",
    "title": "On the Representation Collapse of Sparse Mixture of Experts",
    "abstract": "On the Representation Collapse of Sparse Mixture of Experts",
    "descriptor": "",
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Shaohan Huang",
      "Damai Dai",
      "Shuming Ma",
      "Barun Patra",
      "Saksham Singhal",
      "Payal Bajaj",
      "Xia Song",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.09179"
  },
  {
    "id": "arXiv:2204.10615",
    "title": "Generalized Quantifiers as a Source of Error in Multilingual NLU  Benchmarks",
    "abstract": "Comments: To appear at NAACL 2022",
    "descriptor": "\nComments: To appear at NAACL 2022\n",
    "authors": [
      "Ruixiang Cui",
      "Daniel Hershcovich",
      "Anders S\u00f8gaard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2204.10615"
  },
  {
    "id": "arXiv:2204.10851",
    "title": "Exploiting Session Information in BERT-based Session-aware Sequential  Recommendation",
    "abstract": "Comments: 6 pages, accepted in The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) 2022, short paper",
    "descriptor": "\nComments: 6 pages, accepted in The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) 2022, short paper\n",
    "authors": [
      "Jinseok Seol",
      "Youngrok Ko",
      "Sang-goo Lee"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.10851"
  },
  {
    "id": "arXiv:2204.11206",
    "title": "Bounding the Effects of Continuous Treatments for Hidden Confounders",
    "abstract": "Bounding the Effects of Continuous Treatments for Hidden Confounders",
    "descriptor": "",
    "authors": [
      "Myrl G. Marmarelis",
      "Greg Ver Steeg",
      "Neda Jahanshad",
      "Aram Galstyan"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.11206"
  },
  {
    "id": "arXiv:2204.11411",
    "title": "Road Traffic Law Adaptive Decision-making for Self-Driving Vehicles",
    "abstract": "Road Traffic Law Adaptive Decision-making for Self-Driving Vehicles",
    "descriptor": "",
    "authors": [
      "Jiaxin Liu",
      "Wenhui Zhou",
      "Hong Wang",
      "Zhong Cao",
      "Wenhao Yu",
      "Chengxiang Zhao",
      "Ding Zhao",
      "Diange Yang",
      "Jun Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.11411"
  },
  {
    "id": "arXiv:2204.11589",
    "title": "Hybrid Transfer in Deep Reinforcement Learning for Ads Allocation",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2204.00888, arXiv:2204.00377",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2204.00888, arXiv:2204.00377\n",
    "authors": [
      "Ze Wang",
      "Guogang Liao",
      "Xiaowen Shi",
      "Xiaoxu Wu",
      "Chuheng Zhang",
      "Bingqi Zhu",
      "Yongkang Wang",
      "Xingxing Wang",
      "Dong Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.11589"
  },
  {
    "id": "arXiv:2204.12668",
    "title": "Adaptable Text Matching via Meta-Weight Regulator",
    "abstract": "Comments: 10 pages, accepted to SIGIR 2022",
    "descriptor": "\nComments: 10 pages, accepted to SIGIR 2022\n",
    "authors": [
      "Bo Zhang",
      "Chen Zhang",
      "Fang Ma",
      "Dawei Song"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.12668"
  },
  {
    "id": "arXiv:2204.13111",
    "title": "Open challenges for Machine Learning based Early Decision-Making  research",
    "abstract": "Open challenges for Machine Learning based Early Decision-Making  research",
    "descriptor": "",
    "authors": [
      "Alexis Bondu",
      "Youssef Achenchabe",
      "Albert Bifet",
      "Fabrice Cl\u00e9rot",
      "Antoine Cornu\u00e9jols",
      "Joao Gama",
      "Georges H\u00e9brail",
      "Vincent Lemaire",
      "Pierre-Fran\u00e7ois Marteau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13111"
  },
  {
    "id": "arXiv:2204.14146",
    "title": "Training Language Models with Language Feedback",
    "abstract": "Comments: The First Workshop on Learning with Natural Language Supervision at ACL 2022",
    "descriptor": "\nComments: The First Workshop on Learning with Natural Language Supervision at ACL 2022\n",
    "authors": [
      "J\u00e9r\u00e9my Scheurer",
      "Jon Ander Campos",
      "Jun Shern Chan",
      "Angelica Chen",
      "Kyunghyun Cho",
      "Ethan Perez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.14146"
  },
  {
    "id": "arXiv:2205.00111",
    "title": "Privacy Sensitive Speech Analysis Using Federated Learning to Assess  Depression",
    "abstract": "Comments: 5 pages, 4 figures. Published in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)",
    "descriptor": "\nComments: 5 pages, 4 figures. Published in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)\n",
    "authors": [
      "Suhas BN",
      "Saeed Abdullah"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.00111"
  },
  {
    "id": "arXiv:2205.01271",
    "title": "Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation",
    "abstract": "Comments: 11 pages",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Yihan Wang",
      "Muyang Li",
      "Han Cai",
      "Wei-Ming Chen",
      "Song Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.01271"
  },
  {
    "id": "arXiv:2205.01952",
    "title": "A Generic Solution to Register-bounded Synthesis with an Application to  Discrete Orders",
    "abstract": "Comments: This work was intended as a replacement of arXiv:2105.09978 and any subsequent updates will appear there",
    "descriptor": "\nComments: This work was intended as a replacement of arXiv:2105.09978 and any subsequent updates will appear there\n",
    "authors": [
      "L\u00e9o Exibard",
      "Emmanuel Filiot",
      "Ayrat Khalimov"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.01952"
  },
  {
    "id": "arXiv:2205.02070",
    "title": "DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches",
    "abstract": "DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches",
    "descriptor": "",
    "authors": [
      "Xian Wu",
      "Chen Wang",
      "Hongbo Fu",
      "Ariel Shamir",
      "Song-Hai Zhang",
      "Shi-Min Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.02070"
  },
  {
    "id": "arXiv:2205.02152",
    "title": "Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2  segmentation models",
    "abstract": "Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2  segmentation models",
    "descriptor": "",
    "authors": [
      "Constantine Maganaris",
      "Eftychios Protopapadakis",
      "Nikolaos Bakalos",
      "Nikolaos Doulamis",
      "Dimitris Kalogeras",
      "Aikaterini Angeli"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02152"
  },
  {
    "id": "arXiv:2205.02561",
    "title": "LDSA: Learning Dynamic Subtask Assignment in Cooperative Multi-Agent  Reinforcement Learning",
    "abstract": "LDSA: Learning Dynamic Subtask Assignment in Cooperative Multi-Agent  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Mingyu Yang",
      "Jian Zhao",
      "Xunhan Hu",
      "Wengang Zhou",
      "Jiangcheng Zhu",
      "Houqiang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02561"
  },
  {
    "id": "arXiv:2205.02708",
    "title": "Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular  Property Prediction",
    "abstract": "Comments: 19 pages, 6 figures, 4 tables, 1 algorithm",
    "descriptor": "\nComments: 19 pages, 6 figures, 4 tables, 1 algorithm\n",
    "authors": [
      "Wenlin Chen",
      "Austin Tripp",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02708"
  },
  {
    "id": "arXiv:2205.03143",
    "title": "How to Minimize the Weighted Sum AoI in Multi-Source Status Update  Systems: OMA or NOMA?",
    "abstract": "How to Minimize the Weighted Sum AoI in Multi-Source Status Update  Systems: OMA or NOMA?",
    "descriptor": "",
    "authors": [
      "Jixuan Wang",
      "Deli Qiao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.03143"
  },
  {
    "id": "arXiv:2205.03269",
    "title": "A Rapid Power-iterative Root-MUSIC Estimator for Massive/Ultra-massive  MIMO Receiver",
    "abstract": "A Rapid Power-iterative Root-MUSIC Estimator for Massive/Ultra-massive  MIMO Receiver",
    "descriptor": "",
    "authors": [
      "Qijuan Jie",
      "Xuehui Wang",
      "Peng Chen",
      "Yiwen Chen",
      "Feng Shu",
      "Peng Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.03269"
  },
  {
    "id": "arXiv:2205.05051",
    "title": "Matrix pencils with the numerical range equal to the whole complex plane",
    "abstract": "Comments: 9",
    "descriptor": "\nComments: 9\n",
    "authors": [
      "Vadym Koval",
      "Patryk Pagacz"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.05051"
  },
  {
    "id": "arXiv:2205.05862",
    "title": "AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for  Language Modeling",
    "abstract": "AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for  Language Modeling",
    "descriptor": "",
    "authors": [
      "Haoqin Tu",
      "Zhongliang Yang",
      "Jinshuai Yang",
      "Siyu Zhang",
      "Yongfeng Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.05862"
  },
  {
    "id": "arXiv:2205.06846",
    "title": "Optimal Parameter-free Online Learning with Switching Cost",
    "abstract": "Optimal Parameter-free Online Learning with Switching Cost",
    "descriptor": "",
    "authors": [
      "Zhiyu Zhang",
      "Ashok Cutkosky",
      "Ioannis Ch. Paschalidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.06846"
  },
  {
    "id": "arXiv:2205.07292",
    "title": "A Computational Framework of Cortical Microcircuits Approximates  Sign-concordant Random Backpropagation",
    "abstract": "A Computational Framework of Cortical Microcircuits Approximates  Sign-concordant Random Backpropagation",
    "descriptor": "",
    "authors": [
      "Yukun Yang",
      "Peng Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.07292"
  },
  {
    "id": "arXiv:2205.07611",
    "title": "Noise-Tolerant Learning for Audio-Visual Action Recognition",
    "abstract": "Comments: This work is going to be submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work is going to be submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Haochen Han",
      "Qinghua Zheng",
      "Minnan Luo",
      "Kaiyao Miao",
      "Feng Tian",
      "Yan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.07611"
  },
  {
    "id": "arXiv:2205.08013",
    "title": "Continual learning on 3D point clouds with random compressed rehearsal",
    "abstract": "Comments: 10 pages, 3 figures",
    "descriptor": "\nComments: 10 pages, 3 figures\n",
    "authors": [
      "Maciej Zamorski",
      "Micha\u0142 Stypu\u0142kowski",
      "Konrad Karanowski",
      "Tomasz Trzci\u0144ski",
      "Maciej Zi\u0119ba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.08013"
  },
  {
    "id": "arXiv:2205.08078",
    "title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of  Vision Transformers",
    "abstract": "Comments: 38 pages, 2 figures. To appear in ICML 2022",
    "descriptor": "\nComments: 38 pages, 2 figures. To appear in ICML 2022\n",
    "authors": [
      "Arda Sahiner",
      "Tolga Ergen",
      "Batu Ozturkler",
      "John Pauly",
      "Morteza Mardani",
      "Mert Pilanci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.08078"
  },
  {
    "id": "arXiv:2205.08300",
    "title": "Sampling-Based Verification of CTMCs with Uncertain Rates",
    "abstract": "Sampling-Based Verification of CTMCs with Uncertain Rates",
    "descriptor": "",
    "authors": [
      "Thom S. Badings",
      "Nils Jansen",
      "Sebastian Junges",
      "Marielle Stoelinga",
      "Matthias Volk"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.08300"
  },
  {
    "id": "arXiv:2205.08449",
    "title": "Connection-minimal Abduction in EL via Translation to FOL -- Technical  Report",
    "abstract": "Comments: This paper is the technical report version, including appendices, of an IJCAR 2022 paper (to appear)",
    "descriptor": "\nComments: This paper is the technical report version, including appendices, of an IJCAR 2022 paper (to appear)\n",
    "authors": [
      "Fajar Haifani",
      "Patrick Koopmann",
      "Sophie Tourret",
      "Christoph Weidenbach"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.08449"
  },
  {
    "id": "arXiv:2205.08774",
    "title": "Bond Percolation in Small-World Graphs with Power-Law Distribution",
    "abstract": "Bond Percolation in Small-World Graphs with Power-Law Distribution",
    "descriptor": "",
    "authors": [
      "Luca Becchetti",
      "Andrea Clementi",
      "Francesco Pasquale",
      "Luca Trevisan",
      "Isabella Ziccardi"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.08774"
  },
  {
    "id": "arXiv:2205.08972",
    "title": "The Structure of Configurations in One-Dimensional Majority Cellular  Automata: From Cell Stability to Configuration Periodicity",
    "abstract": "The Structure of Configurations in One-Dimensional Majority Cellular  Automata: From Cell Stability to Configuration Periodicity",
    "descriptor": "",
    "authors": [
      "Yonatan Nakar",
      "Dana Ron"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.08972"
  },
  {
    "id": "arXiv:2205.09005",
    "title": "Confidential Machine Learning within Graphcore IPUs",
    "abstract": "Confidential Machine Learning within Graphcore IPUs",
    "descriptor": "",
    "authors": [
      "Kapil Vaswani",
      "Stavros Volos",
      "C\u00e9dric Fournet",
      "Antonio Nino Diaz",
      "Ken Gordon",
      "Balaji Vembu",
      "Sam Webster",
      "David Chisnall",
      "Saurabh Kulkarni",
      "Graham Cunningham",
      "Richard Osborne",
      "Dan Wilkinson"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.09005"
  },
  {
    "id": "arXiv:2205.09054",
    "title": "Position Aided Beam Prediction in the Real World: How Useful GPS  Locations Actually Are?",
    "abstract": "Comments: Submitted to IEEE. Datasets and code files are available on the DeepSense website: this https URL",
    "descriptor": "\nComments: Submitted to IEEE. Datasets and code files are available on the DeepSense website: this https URL\n",
    "authors": [
      "Jo\u00e3o Morais",
      "Arash Behboodi",
      "Hamed Pezeshki",
      "Ahmed Alkhateeb"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09054"
  },
  {
    "id": "arXiv:2205.09067",
    "title": "Automatic Rule Induction for Efficient Semi-Supervised Learning",
    "abstract": "Automatic Rule Induction for Efficient Semi-Supervised Learning",
    "descriptor": "",
    "authors": [
      "Reid Pryzant",
      "Ziyi Yang",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09067"
  },
  {
    "id": "arXiv:2205.09185",
    "title": "AI-assisted Optimization of the ECCE Tracking System at the Electron Ion  Collider",
    "abstract": "Comments: 16 pages, 18 figures, 2 appendices, 3 tables",
    "descriptor": "\nComments: 16 pages, 18 figures, 2 appendices, 3 tables\n",
    "authors": [
      "C. Fanelli",
      "Z. Papandreou",
      "K. Suresh",
      "J. K. Adkins",
      "Y. Akiba",
      "A. Albataineh",
      "M. Amaryan",
      "I. C. Arsene",
      "C. Ayerbe Gayoso",
      "J. Bae",
      "X. Bai",
      "M.D. Baker",
      "M. Bashkanov",
      "R. Bellwied",
      "F. Benmokhtar",
      "V. Berdnikov",
      "J. C. Bernauer",
      "F. Bock",
      "W. Boeglin",
      "M. Borysova",
      "E. Brash",
      "P. Brindza",
      "W. J. Briscoe",
      "M. Brooks",
      "S. Bueltmann",
      "M. H. S. Bukhari",
      "A. Bylinkin",
      "R. Capobianco",
      "W.-C. Chang",
      "Y. Cheon",
      "K. Chen",
      "K.-F. Chen",
      "K.-Y. Cheng",
      "M. Chiu",
      "T. Chujo",
      "Z. Citron",
      "E. Cline",
      "E. Cohen",
      "T. Cormier",
      "Y. Corrales Morales",
      "C. Cotton",
      "J. Crafts",
      "C. Crawford",
      "S. Creekmore",
      "C.Cuevas",
      "J. Cunningham",
      "G. David",
      "C. T. Dean",
      "M. Demarteau",
      "S. Diehl",
      "N. Doshita",
      "R. Dupre",
      "J. M. Durham",
      "R. Dzhygadlo",
      "R. Ehlers",
      "L. El Fassi",
      "A. Emmert",
      "R. Ent",
      "R. Fatemi",
      "S. Fegan",
      "M. Finger",
      "M. Finger Jr.",
      "J. Frantz",
      "M. Friedman",
      "I. Friscic"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Nuclear Experiment (nucl-ex)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.09185"
  },
  {
    "id": "arXiv:2205.09295",
    "title": "Are Prompt-based Models Clueless?",
    "abstract": "Are Prompt-based Models Clueless?",
    "descriptor": "",
    "authors": [
      "Pride Kavumba",
      "Ryo Takahashi",
      "Yusuke Oda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09295"
  },
  {
    "id": "arXiv:2205.09350",
    "title": "Cross-lingual Inflection as a Data Augmentation Method for Parsing",
    "abstract": "Comments: 10 pages, 7 tables, 5 figures. Workshop on Insights from Negative Results in NLP 2022 (co-located with ACL)",
    "descriptor": "\nComments: 10 pages, 7 tables, 5 figures. Workshop on Insights from Negative Results in NLP 2022 (co-located with ACL)\n",
    "authors": [
      "Alberto Mu\u00f1oz-Ortiz",
      "Carlos G\u00f3mez-Rodr\u00edguez",
      "David Vilares"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09350"
  },
  {
    "id": "arXiv:2205.09510",
    "title": "An Introduction to Quantum Machine Learning for Engineers",
    "abstract": "Comments: This is a first draft, and is currently under review. Comments are very welcome, including corrections",
    "descriptor": "\nComments: This is a first draft, and is currently under review. Comments are very welcome, including corrections\n",
    "authors": [
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09510"
  },
  {
    "id": "arXiv:2205.09542",
    "title": "Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning",
    "abstract": "Comments: Accepted by SIGGRAPH 2022",
    "descriptor": "\nComments: Accepted by SIGGRAPH 2022\n",
    "authors": [
      "Yuxin Zhang",
      "Fan Tang",
      "Weiming Dong",
      "Haibin Huang",
      "Chongyang Ma",
      "Tong-Yee Lee",
      "Changsheng Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.09542"
  },
  {
    "id": "arXiv:2205.09548",
    "title": "ODBO: Bayesian Optimization with Search Space Prescreening for Directed  Protein Evolution",
    "abstract": "Comments: 25 pages, 11 figures",
    "descriptor": "\nComments: 25 pages, 11 figures\n",
    "authors": [
      "Lixue Cheng",
      "Ziyi Yang",
      "Benben Liao",
      "Changyu Hsieh",
      "Shengyu Zhang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.09548"
  },
  {
    "id": "arXiv:2205.09612",
    "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence  Network",
    "abstract": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence  Network",
    "descriptor": "",
    "authors": [
      "Yao-Ching Yu",
      "Shi-Jinn Horng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09612"
  },
  {
    "id": "arXiv:2205.09663",
    "title": "Collision Detection Accelerated: An Optimization Perspective",
    "abstract": "Comments: RSS 2022, 12 pages, 9 figures, 2 tables",
    "descriptor": "\nComments: RSS 2022, 12 pages, 9 figures, 2 tables\n",
    "authors": [
      "Louis Montaut",
      "Quentin Le Lidec",
      "Vladimir Petrik",
      "Josef Sivic",
      "Justin Carpentier"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.09663"
  },
  {
    "id": "arXiv:2205.09669",
    "title": "Semi-WTC: A Practical Semi-supervised Framework for Attack  Categorization through Weight-Task Consistency",
    "abstract": "Comments: Tech report",
    "descriptor": "\nComments: Tech report\n",
    "authors": [
      "Zihan Li",
      "Wentao Chen",
      "Zhiqing Wei",
      "Xingqi Luo",
      "Bing Su"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09669"
  },
  {
    "id": "arXiv:2205.09702",
    "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency  Analysis",
    "abstract": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency  Analysis",
    "descriptor": "",
    "authors": [
      "Maciej Besta",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.09702"
  }
]