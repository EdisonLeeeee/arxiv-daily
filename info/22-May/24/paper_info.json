[
  {
    "id": "arXiv:2205.10355",
    "title": "Deep Quality Estimation: Creating Surrogate Models for Human Quality  Ratings",
    "abstract": "Human ratings are abstract representations of segmentation quality. To\napproximate human quality ratings on scarce expert data, we train surrogate\nquality estimation models. We evaluate on a complex multi-class segmentation\nproblem, specifically glioma segmentation following the BraTS annotation\nprotocol. The training data features quality ratings from 15 expert\nneuroradiologists on a scale ranging from 1 to 6 stars for various\ncomputer-generated and manual 3D annotations. Even though the networks operate\non 2D images and with scarce training data, we can approximate segmentation\nquality within a margin of error comparable to human intra-rater reliability.\nSegmentation quality prediction has broad applications. While an understanding\nof segmentation quality is imperative for successful clinical translation of\nautomatic segmentation quality algorithms, it can play an essential role in\ntraining new segmentation models. Due to the split-second inference times, it\ncan be directly applied within a loss function or as a fully-automatic dataset\ncuration mechanism in a federated learning setting.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Florian Kofler",
      "Ivan Ezhov",
      "Lucas Fidon",
      "Izabela Horvath",
      "Ezequiel de la Rosa",
      "John LaMaster",
      "Hongwei Li",
      "Tom Finck",
      "Suprosanna Shit",
      "Johannes Paetzold",
      "Spyridon Bakas",
      "Marie Piraud",
      "Jan Kirschke",
      "Tom Vercauteren",
      "Claus Zimmer",
      "Benedikt Wiestler",
      "Bjoern Menze"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10355"
  },
  {
    "id": "arXiv:2205.10356",
    "title": "EXPANSE: A Deep Continual / Progressive Learning System for Deep  Transfer Learning",
    "abstract": "Deep transfer learning techniques try to tackle the limitations of deep\nlearning, the dependency on extensive training data and the training costs, by\nreusing obtained knowledge. However, the current DTL techniques suffer from\neither catastrophic forgetting dilemma (losing the previously obtained\nknowledge) or overly biased pre-trained models (harder to adapt to target data)\nin finetuning pre-trained models or freezing a part of the pre-trained model,\nrespectively. Progressive learning, a sub-category of DTL, reduces the effect\nof the overly biased model in the case of freezing earlier layers by adding a\nnew layer to the end of a frozen pre-trained model. Even though it has been\nsuccessful in many cases, it cannot yet handle distant source and target data.\nWe propose a new continual/progressive learning approach for deep transfer\nlearning to tackle these limitations. To avoid both catastrophic forgetting and\noverly biased-model problems, we expand the pre-trained model by expanding\npre-trained layers (adding new nodes to each layer) in the model instead of\nonly adding new layers. Hence the method is named EXPANSE. Our experimental\nresults confirm that we can tackle distant source and target data using this\ntechnique. At the same time, the final model is still valid on the source data,\nachieving a promising deep continual learning approach. Moreover, we offer a\nnew way of training deep learning models inspired by the human education\nsystem. We termed this two-step training: learning basics first, then adding\ncomplexities and uncertainties. The evaluation implies that the two-step\ntraining extracts more meaningful features and a finer basin on the error\nsurface since it can achieve better accuracy in comparison to regular training.\nEXPANSE (model expansion and two-step training) is a systematic continual\nlearning approach applicable to different problems and DL models.",
    "descriptor": "\nComments: 12 Pages, 2 figures, 4 tables, submitting to NIPS 2022\n",
    "authors": [
      "Mohammadreza Iman",
      "John A. Miller",
      "Khaled Rasheed",
      "Robert M. Branchinst",
      "Hamid R. Arabnia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10356"
  },
  {
    "id": "arXiv:2205.10357",
    "title": "SOL: Reducing the Maintenance Overhead for Integrating Hardware Support  into AI Frameworks",
    "abstract": "The increased interest in Artificial Intelligence (AI) raised the need for\nhighly optimized and sophisticated AI frameworks. Starting with the Lua-based\nTorch many frameworks have emerged over time, such as Theano, Caffe, Chainer,\nCNTK, MxNet, PyTorch, DL4J, or TensorFlow. All of these provide a high level\nscripting API that allows users to easily design neural networks and run these\non various kinds of hardware. What the user usually does not see is the high\neffort put into these frameworks to provide peak execution performance. While\nmainstream CPUs and GPUs have the \"luxury\" to have a wide spread user base in\nthe open source community, less mainstream CPU, GPU or accelerator vendors need\nto put in a high effort to get their hardware supported by these frameworks.\nThis includes not only the development of highly efficient compute libraries\nsuch as CUDNN, OneDNN or VEDNN but also supporting an ever growing number of\nsimpler compute operations such as summation and multiplications. Each of these\nframeworks, nowadays, supports several hundred of unique operations, with\ntensors of various sizes, shapes and data types, which end up in thousands of\ncompute kernels required for each device type. And the number of operations\nkeeps increasing.\nThat is why NEC Laboratories Europe started developing the SOL AI\nOptimization project already years ago, to deliver optimal performance to users\nwhile keeping the maintenance burden minimal.",
    "descriptor": "",
    "authors": [
      "Nicolas Weber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.10357"
  },
  {
    "id": "arXiv:2205.10358",
    "title": "A Hardware-Aware Framework for Accelerating Neural Architecture Search  Across Modalities",
    "abstract": "Recent advances in Neural Architecture Search (NAS) such as one-shot NAS\noffer the ability to extract specialized hardware-aware sub-network\nconfigurations from a task-specific super-network. While considerable effort\nhas been employed towards improving the first stage, namely, the training of\nthe super-network, the search for derivative high-performing sub-networks is\nstill under-explored. Popular methods decouple the super-network training from\nthe sub-network search and use performance predictors to reduce the\ncomputational burden of searching on different hardware platforms. We propose a\nflexible search framework that automatically and efficiently finds optimal\nsub-networks that are optimized for different performance metrics and hardware\nconfigurations. Specifically, we show how evolutionary algorithms can be paired\nwith lightly trained objective predictors in an iterative cycle to accelerate\narchitecture search in a multi-objective setting for various modalities\nincluding machine translation and image classification.",
    "descriptor": "",
    "authors": [
      "Daniel Cummings",
      "Anthony Sarah",
      "Sharath Nittur Sridhar",
      "Maciej Szankin",
      "Juan Pablo Munoz",
      "Sairam Sundaresan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10358"
  },
  {
    "id": "arXiv:2205.10362",
    "title": "FIND:Explainable Framework for Meta-learning",
    "abstract": "Meta-learning is used to efficiently enable the automatic selection of\nmachine learning models by combining data and prior knowledge. Since the\ntraditional meta-learning technique lacks explainability, as well as\nshortcomings in terms of transparency and fairness, achieving explainability\nfor meta-learning is crucial. This paper proposes FIND, an interpretable\nmeta-learning framework that not only can explain the recommendation results of\nmeta-learning algorithm selection, but also provide a more complete and\naccurate explanation of the recommendation algorithm's performance on specific\ndatasets combined with business scenarios. The validity and correctness of this\nframework have been demonstrated by extensive experiments.",
    "descriptor": "",
    "authors": [
      "Xinyue Shao",
      "Hongzhi Wang",
      "Xiao Zhu",
      "Feng Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10362"
  },
  {
    "id": "arXiv:2205.10363",
    "title": "Robust Task-Oriented Dialogue Generation with Contrastive Pre-training  and Adversarial Filtering",
    "abstract": "Data artifacts incentivize machine learning models to learn non-transferable\ngeneralizations by taking advantage of shortcuts in the data, and there is\ngrowing evidence that data artifacts play a role for the strong results that\ndeep learning models achieve in recent natural language processing benchmarks.\nIn this paper, we focus on task-oriented dialogue and investigate whether\npopular datasets such as MultiWOZ contain such data artifacts. We found that by\nonly keeping frequent phrases in the training examples, state-of-the-art models\nperform similarly compared to the variant trained with full data, suggesting\nthey exploit these spurious correlations to solve the task. Motivated by this,\nwe propose a contrastive learning based framework to encourage the model to\nignore these cues and focus on learning generalisable patterns. We also\nexperiment with adversarial filtering to remove \"easy\" training instances so\nthat the model would focus on learning from the \"harder\" instances. We conduct\na number of generalization experiments -- e.g., cross-domain/dataset and\nadversarial tests -- to assess the robustness of our approach and found that it\nworks exceptionally well.",
    "descriptor": "",
    "authors": [
      "Shiquan Yang",
      "Xinting Huang",
      "Jey Han Lau",
      "Sarah Erfani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10363"
  },
  {
    "id": "arXiv:2205.10364",
    "title": "Learning to Reverse DNNs from AI Programs Automatically",
    "abstract": "With the privatization deployment of DNNs on edge devices, the security of\non-device DNNs has raised significant concern. To quantify the model leakage\nrisk of on-device DNNs automatically, we propose NNReverse, the first\nlearning-based method which can reverse DNNs from AI programs without domain\nknowledge. NNReverse trains a representation model to represent the semantics\nof binary code for DNN layers. By searching the most similar function in our\ndatabase, NNReverse infers the layer type of a given function's binary code. To\nrepresent assembly instructions semantics precisely, NNReverse proposes a more\nfine-grained embedding model to represent the textual and structural-semantic\nof assembly functions.",
    "descriptor": "\nComments: This paper is accepted by IJCAI 2022\n",
    "authors": [
      "Simin Chen",
      "Hamed Khanpour",
      "Cong Liu",
      "Wei Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10364"
  },
  {
    "id": "arXiv:2205.10365",
    "title": "A Correlation Information-based Spatiotemporal Network for Traffic Flow  Forecasting",
    "abstract": "With the growth of transport modes, high traffic forecasting precision is\nrequired in intelligent transportation systems. Most previous works utilize the\ntransformer architecture based on graph neural networks and attention\nmechanisms to discover spatiotemporal dependencies and dynamic relationships.\nThe correlation information among spatiotemporal sequences, however, has not\nbeen thoroughly considered. In this paper, we present two elaborate\nspatiotemporal representations, spatial correlation information (SCorr) and\ntemporal correlation information (TCorr), among spatiotemporal sequences based\non the maximal information coefficient. Using SCorr, we propose a novel\ncorrelation information-based spatiotemporal network (CorrSTN), including a\ndynamic graph neural network component incorporating correlation information\ninto the spatial structure effectively and a multi-head attention component\nutilizing spatial correlation information to extract dynamic temporal\ndependencies accurately. Using TCorr, we further explore the correlation\npattern among different periodic data and then propose a novel data selection\nscheme to identify the most relevant data. The experimental results on the\nhighway traffic flow (PEMS07 and PEMS08) and metro crowd flow (HZME inflow and\noutflow) datasets demonstrate that CorrSTN outperforms the state-of-the-art\nmethods in terms of predictive performance. In particular, on the HZME\n(outflow) dataset, our model makes significant improvements compared with the\nlatest model ASTGNN by 12.7%, 14.4% and 27.4% in the metrics of MAE, RMSE and\nMAPE, respectively.",
    "descriptor": "",
    "authors": [
      "Weiguo Zhu",
      "Yongqi Sun",
      "Xintong Yi",
      "Yan Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10365"
  },
  {
    "id": "arXiv:2205.10366",
    "title": "Actively Tracking the Optimal Arm in Non-Stationary Environments with  Mandatory Probing",
    "abstract": "We study a novel multi-armed bandit (MAB) setting which mandates the agent to\nprobe all the arms periodically in a non-stationary environment. In particular,\nwe develop \\texttt{TS-GE} that balances the regret guarantees of classical\nThompson sampling (TS) with the broadcast probing (BP) of all the arms\nsimultaneously in order to actively detect a change in the reward\ndistributions. Once a system-level change is detected, the changed arm is\nidentified by an optional subroutine called group exploration (GE) which scales\nas $\\log_2(K)$ for a $K-$armed bandit setting. We characterize the probability\nof missed detection and the probability of false-alarm in terms of the\nenvironment parameters. The latency of change-detection is upper bounded by\n$\\sqrt{T}$ while within a period of $\\sqrt{T}$, all the arms are probed at\nleast once. We highlight the conditions in which the regret guarantee of\n\\texttt{TS-GE} outperforms that of the state-of-the-art algorithms, in\nparticular, \\texttt{ADSWITCH} and \\texttt{M-UCB}. Furthermore, unlike the\nexisting bandit algorithms, \\texttt{TS-GE} can be deployed for applications\nsuch as timely status updates, critical control, and wireless energy transfer,\nwhich are essential features of next-generation wireless communication\nnetworks. We demonstrate the efficacy of \\texttt{TS-GE} by employing it in a n\nindustrial internet-of-things (IIoT) network designed for simultaneous wireless\ninformation and power transfer (SWIPT).",
    "descriptor": "",
    "authors": [
      "Gourab Ghatak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10366"
  },
  {
    "id": "arXiv:2205.10369",
    "title": "Deployment of Energy-Efficient Deep Learning Models on Cortex-M based  Microcontrollers using Deep Compression",
    "abstract": "Large Deep Neural Networks (DNNs) are the backbone of today's artificial\nintelligence due to their ability to make accurate predictions when being\ntrained on huge datasets. With advancing technologies, such as the Internet of\nThings, interpreting large quantities of data generated by sensors is becoming\nan increasingly important task. However, in many applications not only the\npredictive performance but also the energy consumption of deep learning models\nis of major interest. This paper investigates the efficient deployment of deep\nlearning models on resource-constrained microcontroller architectures via\nnetwork compression. We present a methodology for the systematic exploration of\ndifferent DNN pruning, quantization, and deployment strategies, targeting\ndifferent ARM Cortex-M based low-power systems. The exploration allows to\nanalyze trade-offs between key metrics such as accuracy, memory consumption,\nexecution time, and power consumption. We discuss experimental results on three\ndifferent DNN architectures and show that we can compress them to below 10\\% of\ntheir original parameter count before their predictive quality decreases. This\nalso allows us to deploy and evaluate them on Cortex-M based microcontrollers.",
    "descriptor": "\nComments: 12 pages, 7 figures\n",
    "authors": [
      "Mark Deutel",
      "Philipp Woller",
      "Christopher Mutschler",
      "J\u00fcrgen Teich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10369"
  },
  {
    "id": "arXiv:2205.10370",
    "title": "Diversity vs. Recognizability: Human-like generalization in one-shot  generative models",
    "abstract": "Robust generalization to new concepts has long remained a distinctive feature\nof human intelligence. However, recent progress in deep generative models has\nnow led to neural architectures capable of synthesizing novel instances of\nunknown visual concepts from a single training example. Yet, a more precise\ncomparison between these models and humans is not possible because existing\nperformance metrics for generative models (i.e., FID, IS, likelihood) are not\nappropriate for the one-shot generation scenario. Here, we propose a new\nframework to evaluate one-shot generative models along two axes: sample\nrecognizability vs. diversity (i.e., intra-class variability). Using this\nframework, we perform a systematic evaluation of representative one-shot\ngenerative models on the Omniglot handwritten dataset. We first show that\nGAN-like and VAE-like models fall on opposite ends of the\ndiversity-recognizability space. Extensive analyses of the effect of key model\nparameters further revealed that spatial attention and context integration have\na linear contribution to the diversity-recognizability trade-off. In contrast,\ndisentanglement transports the model along a parabolic curve that could be used\nto maximize recognizability. Using the diversity-recognizability framework, we\nwere able to identify models and parameters that closely approximate human\ndata.",
    "descriptor": "",
    "authors": [
      "Victor Boutin",
      "Lakshya Singhal",
      "Xavier Thomas",
      "Thomas Serre"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10370"
  },
  {
    "id": "arXiv:2205.10374",
    "title": "DELMAR: Deep Linear Matrix Approximately Reconstruction to Extract  Hierarchical Functional Connectivity in the Human Brain",
    "abstract": "The Matrix Decomposition techniques have been a vital computational approach\nto analyzing the hierarchy of functional connectivity in the human brain.\nHowever, there are still four shortcomings of these methodologies: 1). Large\ntraining samples; 2). Manually tuning hyperparameters; 3). Time-consuming and\nrequire extensive computational source; 4). It cannot guarantee convergence to\na unique fixed point.\nTherefore, we propose a novel deep matrix factorization technique called Deep\nLinear Matrix Approximate Reconstruction (DELMAR) to bridge the abovementioned\ngaps. The advantages of the proposed method are: at first, proposed DELMAR can\nestimate the important hyperparameters automatically; furthermore, DELMAR\nemploys the matrix backpropagation to reduce the potential accumulative errors;\nfinally, an orthogonal projection is introduced to update all variables of\nDELMAR rather than directly calculating the inverse matrices.\nThe validation experiments of three peer methods and DELMAR using real\nfunctional MRI signal of the human brain demonstrates that our proposed method\ncan efficiently identify the spatial feature in fMRI signal even faster and\nmore accurately than other peer methods. Moreover, the theoretical analyses\nindicate that DELMAR can converge to the unique fixed point and even enable the\naccurate approximation of original input as DNNs.",
    "descriptor": "\nComments: 10 pages, 6 figures, a deep linear matrix factorization technique. arXiv admin note: text overlap with arXiv:2205.10264\n",
    "authors": [
      "Wei Zhang",
      "Yu Bao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10374"
  },
  {
    "id": "arXiv:2205.10386",
    "title": "A Dynamic Weighted Tabular Method for Convolutional Neural Networks",
    "abstract": "Traditional Machine Learning (ML) models like Support Vector Machine, Random\nForest, and Logistic Regression are generally preferred for classification\ntasks on tabular datasets. Tabular data consists of rows and columns\ncorresponding to instances and features, respectively. Past studies indicate\nthat traditional classifiers often produce unsatisfactory results in complex\ntabular datasets. Hence, researchers attempt to use the powerful Convolutional\nNeural Networks (CNN) for tabular datasets. Recent studies propose several\ntechniques like SuperTML, Conditional GAN (CTGAN), and Tabular Convolution\n(TAC) for applying Convolutional Neural Networks (CNN) on tabular data. These\nmodels outperform the traditional classifiers and substantially improve the\nperformance on tabular data. This study introduces a novel technique, namely,\nDynamic Weighted Tabular Method (DWTM), that uses feature weights dynamically\nbased on statistical techniques to apply CNNs on tabular datasets. The method\nassigns weights dynamically to each feature based on their strength of\nassociativity to the class labels. Each data point is converted into images and\nfed to a CNN model. The features are allocated image canvas space based on\ntheir weights. The DWTM is an improvement on the previously mentioned methods\nas it dynamically implements the entire experimental setting rather than using\nthe static configuration provided in the previous methods. Furthermore, it uses\nthe novel idea of using feature weights to create image canvas space. In this\npaper, the DWTM is applied to six benchmarked tabular datasets and it achieves\noutstanding performance (i.e., average accuracy = 95%) on all of them.",
    "descriptor": "",
    "authors": [
      "Md Ifraham Iqbal",
      "Md. Saddam Hossain Mukta",
      "Ahmed Rafi Hasan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10386"
  },
  {
    "id": "arXiv:2205.10390",
    "title": "EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex  Structures",
    "abstract": "Protein complexes are macromolecules essential to the functioning and\nwell-being of all living organisms. As the structure of a protein complex, in\nparticular its region of interaction between multiple protein subunits (i.e.,\nchains), has a notable influence on the biological function of the complex,\ncomputational methods that can quickly and effectively be used to refine and\nassess the quality of a protein complex's 3D structure can directly be used\nwithin a drug discovery pipeline to accelerate the development of new\ntherapeutics and improve the efficacy of future vaccines. In this work, we\nintroduce the Equivariant Graph Refiner (EGR), a novel E(3)-equivariant graph\nneural network (GNN) for multi-task structure refinement and assessment of\nprotein complexes. Our experiments on new, diverse protein complex datasets,\nall of which we make publicly available in this work, demonstrate the\nstate-of-the-art effectiveness of EGR for atomistic refinement and assessment\nof protein complexes and outline directions for future work in the field. In\ndoing so, we establish a baseline for future studies in macromolecular\nrefinement and structure analysis.",
    "descriptor": "\nComments: 17 pages, 3 figures, and 6 tables. Under review\n",
    "authors": [
      "Alex Morehead",
      "Xiao Chen",
      "Tianqi Wu",
      "Jian Liu",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10390"
  },
  {
    "id": "arXiv:2205.10395",
    "title": "Assessing visual acuity in visual prostheses through a virtual-reality  system",
    "abstract": "Current visual implants still provide very low resolution and limited field\nof view, thus limiting visual acuity in implanted patients. Developments of new\nstrategies of artificial vision simulation systems by harnessing new\nadvancements in technologies are of upmost priorities for the development of\nnew visual devices. In this work, we take advantage of virtual-reality software\npaired with a portable head-mounted display and evaluated the performance of\nnormally sighted participants under simulated prosthetic vision with variable\nfield of view and number of pixels. Our simulated prosthetic vision system\nallows simple experimentation in order to study the design parameters of future\nvisual prostheses. Ten normally sighted participants volunteered for a visual\nacuity study. Subjects were required to identify computer-generated Landolt-C\ngap orientation and different stimulus based on light perception,\ntime-resolution, light location and motion perception commonly used for visual\nacuity examination in the sighted. Visual acuity scores were recorded across\ndifferent conditions of number of electrodes and size of field of view. Our\nresults showed that of all conditions tested, a field of view of 20{\\deg} and\n1000 phosphenes of resolution proved the best, with a visual acuity of 1.3\nlogMAR. Furthermore, performance appears to be correlated with phosphene\ndensity, but showing a diminishing return when field of view is less than\n20{\\deg}. The development of new artificial vision simulation systems can be\nuseful to guide the development of new visual devices and the optimization of\nfield of view and resolution to provide a helpful and valuable visual aid to\nprofoundly or totally blind patients.",
    "descriptor": "",
    "authors": [
      "Melani Sanchez-Garcia",
      "Roberto Morollon-Ruiz",
      "Ruben Martinez-Cantin",
      "Jose J. Guerrero",
      "Eduardo Fernandez-Jover"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10395"
  },
  {
    "id": "arXiv:2205.10397",
    "title": "Modernizing Open-Set Speech Language Identification",
    "abstract": "While most modern speech Language Identification methods are closed-set, we\nwant to see if they can be modified and adapted for the open-set problem. When\nswitching to the open-set problem, the solution gains the ability to reject an\naudio input when it fails to match any of our known language options. We tackle\nthe open-set task by adapting two modern-day state-of-the-art approaches to\nclosed-set language identification: the first using a CRNN with attention and\nthe second using a TDNN. In addition to enhancing our input feature embeddings\nusing MFCCs, log spectral features, and pitch, we will be attempting two\napproaches to out-of-set language detection: one using thresholds, and the\nother essentially performing a verification task. We will compare both the\nperformance of the TDNN and the CRNN, as well as our detection approaches.",
    "descriptor": "\nComments: 7 pages, 6 figures, 3 tables, Technical Report: Recognition Technologies, Inc\n",
    "authors": [
      "Mustafa Eyceoz",
      "Justin Lee",
      "Homayoon Beigi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.10397"
  },
  {
    "id": "arXiv:2205.10399",
    "title": "Multilingual Normalization of Temporal Expressions with Masked Language  Models",
    "abstract": "The detection and normalization of temporal expressions is an important task\nand a preprocessing step for many applications. However, prior work on\nnormalization is rule-based, which severely limits the applicability in\nreal-world multilingual settings, due to the costly creation of new rules. We\npropose a novel neural method for normalizing temporal expressions based on\nmasked language modeling. Our multilingual method outperforms prior rule-based\nsystems in many languages, and in particular, for low-resource languages with\nperformance improvements of up to 35 F1 on average compared to the state of the\nart.",
    "descriptor": "",
    "authors": [
      "Lukas Lange",
      "Jannik Str\u00f6tgen",
      "Heike Adel",
      "Dietrich Klakow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10399"
  },
  {
    "id": "arXiv:2205.10400",
    "title": "Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining  for Task-Oriented Dialog",
    "abstract": "Research on (multi-domain) task-oriented dialog (TOD) has predominantly\nfocused on the English language, primarily due to the shortage of robust TOD\ndatasets in other languages, preventing the systematic investigation of\ncross-lingual transfer for this crucial NLP application area. In this work, we\nintroduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from\nthe well-established English dataset MultiWOZ, that spans four typologically\ndiverse languages: Chinese, German, Arabic, and Russian. In contrast to\nconcurrent efforts, Multi2WOZ contains gold-standard dialogs in target\nlanguages that are directly comparable with development and test portions of\nthe English dataset, enabling reliable and comparative estimates of\ncross-lingual transfer performance for TOD. We then introduce a new framework\nfor multilingual conversational specialization of pretrained language models\n(PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream\nTOD tasks. Using such conversational PrLMs specialized for concrete target\nlanguages, we systematically benchmark a number of zero-shot and few-shot\ncross-lingual transfer approaches on two standard TOD tasks: Dialog State\nTracking and Response Retrieval. Our experiments show that, in most setups, the\nbest performance entails the combination of (I) conversational specialization\nin the target language and (ii) few-shot transfer for the concrete TOD task.\nMost importantly, we show that our conversational specialization in the target\nlanguage allows for an exceptionally sample-efficient few-shot transfer for\ndownstream TOD tasks.",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Chia-Chien Hung",
      "Anne Lauscher",
      "Ivan Vuli\u0107",
      "Simone Paolo Ponzetto",
      "Goran Glava\u0161"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10400"
  },
  {
    "id": "arXiv:2205.10402",
    "title": "Ethics of Open Data",
    "abstract": "This chapter addresses emergent ethical issues in producing, using, curating,\nand providing services for open data. Our goal is to provide an introduction to\nhow ethical topics in open data manifest in practical dilemmas for scholarly\ncommunications and some approaches to understanding and working through them.\nWe begin with a brief overview of what can be thought of as three basic\ntheories of ethics that intersect with dilemmas in openness, accountability,\ntransparency, and fairness in data: Virtue, Consequential, and\nNon-consequential ethics. We then map these kinds of ethics to the practical\nquestions that arise in provisioning infrastructures, providing services, and\nsupporting sustainable research in science and scholarship that depends upon\nopen access to data. Throughout, we attempt to offer concrete examples of\npotential ethical dilemmas facing scholarly communication with respect to open\ndata, and try to make clear what kinds of ethical positions are helpful to\npractitioners. In doing so, we hope to both clarify the ethical questions\nfacing librarians doing practical work to support open data access, as well as\nsituate current debates in the field with respect to these three kinds of\nethics.",
    "descriptor": "\nComments: Chapter accepted for publication in ACRL's 'Scholarly Communication and Open Culture'\n",
    "authors": [
      "Nic Weber",
      "Brandon Locke"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2205.10402"
  },
  {
    "id": "arXiv:2205.10403",
    "title": "Tackling Provably Hard Representative Selection via Graph Neural  Networks",
    "abstract": "Representative selection (RS) is the problem of finding a small subset of\nexemplars from an unlabeled dataset, and has numerous applications in\nsummarization, active learning, data compression and many other domains. In\nthis paper, we focus on finding representatives that optimize the accuracy of a\nmodel trained on the selected representatives. We study RS for data represented\nas attributed graphs. We develop RS-GNN, a representation learning-based RS\nmodel based on Graph Neural Networks. Empirically, we demonstrate the\neffectiveness of RS-GNN on problems with predefined graph structures as well as\nproblems with graphs induced from node feature similarities, by showing that\nRS-GNN achieves significant improvements over established baselines that\noptimize surrogate functions. Theoretically, we establish a new hardness result\nfor RS by proving that RS is hard to approximate in polynomial time within any\nreasonable factor, which implies a significant gap between the optimum solution\nof widely-used surrogate functions and the actual accuracy of the model, and\nprovides justification for the superiority of representation learning-based\napproaches such as RS-GNN over surrogate functions.",
    "descriptor": "\nComments: 18 pages, 3 figures\n",
    "authors": [
      "Seyed Mehran Kazemi",
      "Anton Tsitsulin",
      "Hossein Esfandiari",
      "MohammadHossein Bateni",
      "Deepak Ramachandran",
      "Bryan Perozzi",
      "Vahab Mirrokni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.10403"
  },
  {
    "id": "arXiv:2205.10405",
    "title": "Demo: A Transparent Antenna System for In-Building Networks",
    "abstract": "For in-building networks, the potential of transparent antennas, which are\nused as windows of a building, is presented in this paper. In this scenario, a\ntransparent window antenna communicates with outdoor devices or base stations,\nand the indoor repeaters act as relay stations of the transparent window\nantenna for indoor devices. At indoor, back lobe waves of the transparent\nwindow antenna are defined as interference to in-building networks. Hence, we\nanalyze different SIR and SINR results according to the location of an indoor\nrepeater through 3D ray tracing system-level simulation. Furthermore, a\nlink-level simulation through a full-duplex software-defined radio platform\nwith the fabricated transparent antenna is presented to examine the feasibility\nof the transparent antenna.",
    "descriptor": "\nComments: 2 pages, 3 figures\n",
    "authors": [
      "Sang-Hyun Park",
      "Soo-Min Kim",
      "Seonghoon Kim",
      "HongIl Yoo",
      "Byoungnam Kim",
      "Chan-Byoung Chae"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.10405"
  },
  {
    "id": "arXiv:2205.10406",
    "title": "Combining Contrastive and Supervised Learning for Video Super-Resolution  Detection",
    "abstract": "Upscaled video detection is a helpful tool in multimedia forensics, but it is\na challenging task that involves various upscaling and compression algorithms.\nThere are many resolution-enhancement methods, including interpolation and\ndeep-learning-based super-resolution, and they leave unique traces. In this\nwork, we propose a new upscaled-resolution-detection method based on learning\nof visual representations using contrastive and cross-entropy losses. To\nexplain how the method detects videos, we systematically review the major\ncomponents of our framework - in particular, we show that most\ndata-augmentation approaches hinder the learning of the method. Through\nextensive experiments on various datasets, we demonstrate that our method\neffectively detects upscaling even in compressed videos and outperforms the\nstate-of-the-art alternatives. The code and models are publicly available at\nhttps://github.com/msu-video-group/SRDM",
    "descriptor": "",
    "authors": [
      "Viacheslav Meshchaninov",
      "Ivan Molodetskikh",
      "Dmitriy Vatolin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10406"
  },
  {
    "id": "arXiv:2205.10407",
    "title": "Prototyping three key properties of specific curiosity in computational  reinforcement learning",
    "abstract": "Curiosity for machine agents has been a focus of intense research. The study\nof human and animal curiosity, particularly specific curiosity, has unearthed\nseveral properties that would offer important benefits for machine learners,\nbut that have not yet been well-explored in machine intelligence. In this work,\nwe introduce three of the most immediate of these properties -- directedness,\ncessation when satisfied, and voluntary exposure -- and show how they may be\nimplemented together in a proof-of-concept reinforcement learning agent;\nfurther, we demonstrate how the properties manifest in the behaviour of this\nagent in a simple non-episodic grid-world environment that includes\ncuriosity-inducing locations and induced targets of curiosity. As we would\nhope, the agent exhibits short-term directed behaviour while updating long-term\npreferences to adaptively seek out curiosity-inducing situations. This work\ntherefore presents a novel view into how specific curiosity operates and in the\nfuture might be integrated into the behaviour of goal-seeking, decision-making\nagents in complex environments.",
    "descriptor": "\nComments: 5 pages, 6 figures, accepted at the 5th Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM2022), June 8-11, 2022\n",
    "authors": [
      "Nadia M. Ady",
      "Roshan Shariff",
      "Johannes G\u00fcnther",
      "Patrick M. Pilarski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10407"
  },
  {
    "id": "arXiv:2205.10408",
    "title": "Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of  Social Media Posts",
    "abstract": "We present a novel approach incorporating transformer-based language models\ninto infectious disease modelling. Text-derived features are quantified by\ntracking high-density clusters of sentence-level representations of Reddit\nposts within specific US states' COVID-19 subreddits. We benchmark these\nclustered embedding features against features extracted from other high-quality\ndatasets. In a threshold-classification task, we show that they outperform all\nother feature types at predicting upward trend signals, a significant result\nfor infectious disease modelling in areas where epidemiological data is\nunreliable. Subsequently, in a time-series forecasting task we fully utilise\nthe predictive power of the caseload and compare the relative strengths of\nusing different supplementary datasets as covariate feature sets in a\ntransformer-based time-series model.",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Felix Drinkall",
      "Stefan Zohren",
      "Janet B. Pierrehumbert"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10408"
  },
  {
    "id": "arXiv:2205.10411",
    "title": "Educational Tools for Mapuzugun",
    "abstract": "Mapuzugun is the language of the Mapuche people. Due to political and\nhistorical reasons, its number of speakers has decreased and the language has\nbeen excluded from the educational system in Chile and Argentina. For this\nreason, it is very important to support the revitalization of the Mapuzugun in\nall spaces and media of society. In this work we present a tool towards\nsupporting educational activities of Mapuzugun, tailored to the characteristics\nof the language. The tool consists of three parts: design and development of an\northography detector and converter; a morphological analyzer; and an informal\ntranslator. We also present a case study with Mapuzugun students showing\npromising results.\nShort Abstract in Mapuzuzgun: T\\\"ufachi k\\\"uzaw pegelfi ki\\~ne zugun\nk\\\"uzawpey\\\"um kelluaetew pu mapuzugun chillkatufe kimal kizu ta\\~ni zugun.",
    "descriptor": "\nComments: To be presented at the 17th Workshop on Innovative Use of NLP for Building Educational Applications\n",
    "authors": [
      "Cristian Ahumada",
      "Claudio Gutierrez",
      "Antonios Anastasopoulos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10411"
  },
  {
    "id": "arXiv:2205.10412",
    "title": "Current Trends and Approaches in Synonyms Extraction: Potential  Adaptation to Arabic",
    "abstract": "Extracting synonyms from dictionaries or corpora is gaining special attention\nas synonyms play an important role in improving NLP application performance.\nThis paper presents a survey of the different approaches and trends used in\nautomatically extracting the synonyms. These approaches can be divided into\nfour main categories. The first approach is to find the Synonyms using a\ntranslation graph. The second approach is to discover new transition pairs such\nas (Arabic-English) (English-France) then (Arabic-France). The third approach\nis to construct new WordNets by exploring synonymy graphs, and the fourth\napproach is to find similar words from corpora using Deep Learning methods,\nsuch as word embeddings and recently BERT models. The paper also presents a\ncomparative analysis between these approaches and highlights potential\nadaptation to generate synonyms automatically in the Arabic language as future\nwork.",
    "descriptor": "",
    "authors": [
      "Eman Naser-Karajah",
      "Nabil Arman",
      "Mustafa Jarrar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.10412"
  },
  {
    "id": "arXiv:2205.10416",
    "title": "ARLO: A Framework for Automated Reinforcement Learning",
    "abstract": "Automated Reinforcement Learning (AutoRL) is a relatively new area of\nresearch that is gaining increasing attention. The objective of AutoRL consists\nin easing the employment of Reinforcement Learning (RL) techniques for the\nbroader public by alleviating some of its main challenges, including data\ncollection, algorithm selection, and hyper-parameter tuning. In this work, we\npropose a general and flexible framework, namely ARLO: Automated Reinforcement\nLearning Optimizer, to construct automated pipelines for AutoRL. Based on this,\nwe propose a pipeline for offline and one for online RL, discussing the\ncomponents, interaction, and highlighting the difference between the two\nsettings. Furthermore, we provide a Python implementation of such pipelines,\nreleased as an open-source library. Our implementation has been tested on an\nillustrative LQG domain and on classic MuJoCo environments, showing the ability\nto reach competitive performances requiring limited human intervention. We also\nshowcase the full pipeline on a realistic dam environment, automatically\nperforming the feature selection and the model generation tasks.",
    "descriptor": "",
    "authors": [
      "Marco Mussi",
      "Davide Lombarda",
      "Alberto Maria Metelli",
      "Francesco Trov\u00f2",
      "Marcello Restelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10416"
  },
  {
    "id": "arXiv:2205.10423",
    "title": "Learning Geometrically Disentangled Representations of Protein Folding  Simulations",
    "abstract": "Massive molecular simulations of drug-target proteins have been used as a\ntool to understand disease mechanism and develop therapeutics. This work\nfocuses on learning a generative neural network on a structural ensemble of a\ndrug-target protein, e.g. SARS-CoV-2 Spike protein, obtained from\ncomputationally expensive molecular simulations. Model tasks involve\ncharacterizing the distinct structural fluctuations of the protein bound to\nvarious drug molecules, as well as efficient generation of protein\nconformations that can serve as an complement of a molecular simulation engine.\nSpecifically, we present a geometric autoencoder framework to learn separate\nlatent space encodings of the intrinsic and extrinsic geometries of the protein\nstructure. For this purpose, the proposed Protein Geometric AutoEncoder\n(ProGAE) model is trained on the protein contact map and the orientation of the\nbackbone bonds of the protein. Using ProGAE latent embeddings, we reconstruct\nand generate the conformational ensemble of a protein at or near the\nexperimental resolution, while gaining better interpretability and\ncontrollability in term of protein structure generation from the learned latent\nspace. Additionally, ProGAE models are transferable to a different state of the\nsame protein or to a new protein of different size, where only the dense layer\ndecoding from the latent representation needs to be retrained. Results show\nthat our geometric learning-based method enjoys both accuracy and efficiency\nfor generating complex structural variations, charting the path toward scalable\nand improved approaches for analyzing and enhancing high-cost simulations of\ndrug-target proteins.",
    "descriptor": "\nComments: 13 pages, appeared at SimDL ICLR Workshop 2021\n",
    "authors": [
      "N. Joseph Tatro",
      "Payel Das",
      "Pin-Yu Chen",
      "Vijil Chenthamarakshan",
      "Rongjie Lai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2205.10423"
  },
  {
    "id": "arXiv:2205.10425",
    "title": "\"Nudes? Shouldn't I charge for these?\" : Exploring What Motivates  Content Creation on OnlyFans",
    "abstract": "Social media platforms are increasingly considering models to incentivize\ncreators to publish high quality content on their platforms. As a result,\nsocial media content creation has transformed into a form of gig work for some\ncreators. In order to better design social media platforms to support this\nlabor, we need to understand professional creators' motivations. In this work,\nwe present a qualitative interview study of the motivations of $22$ U.S.\nOnlyFans creators. OnlyFans is a subscription-based social media platform that\nis unique in that it is primarily associated with sexual content (although it\nis not marketed as such) and thus creators are positioned at the intersection\nof professional content creation and sex work, exposing them to a unique set of\npotential challenges. Beyond the typical motivations for pursuing other forms\nof gig work (e.g., flexibility, autonomy) our findings highlight three key\nfactors explaining the rapid growth of OnlyFans despite the potential stigma of\nsexual content creation: (1) societal visibility and mainstream acceptance of\nOnlyFans, created through a combination of celebrity hype and the design of the\nplatform itself; (2) platform design: affordances for boundary setting with\nclients, privacy from the public, and content archives, which together create a\nlabor environment participants viewed as better than other forms of gig work, a\nnatural avenue for sexual expression, and enabling monetization of existing\ncontent, audiences, and skills; and (3) the pandemic, which led to both high\ndemand for immediate income while waiting for -- or after running out of --\nunemployment benefits, and increased free time, which increased general demand\nfor pornographic content.",
    "descriptor": "",
    "authors": [
      "Vaughn Hamilton",
      "Ananta Soneji",
      "Allison McDonald",
      "Elissa Redmiles"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.10425"
  },
  {
    "id": "arXiv:2205.10428",
    "title": "Multiple-criteria Heuristic Rating Estimation",
    "abstract": "One of the most widespread multi-criteria decision-making methods is the\nAnalytic Hierarchy Process (AHP). AHP successfully combines the pairwise\ncomparisons method and the hierarchical approach. It allows the decision-maker\nto set priorities for all ranked alternatives. But what if, for some of them,\ntheir ranking value is known (e.g., it can be determined differently)? The\nHeuristic Rating Estimation (HRE) method proposed in 2014 tried to bring the\nanswer to this question. However, the considerations were limited to a model\nthat did not consider many criteria. In this work, we go a step further and\nanalyze how HRE can be used as part of the AHP hierarchical framework. The\ntheoretical considerations are accompanied by illustrative examples showing HRE\nas a multiple-criteria decision-making method.",
    "descriptor": "\nComments: 24 pages, 2 figures\n",
    "authors": [
      "Anna K\u0119dzior",
      "Konrad Ku\u0142akowski"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.10428"
  },
  {
    "id": "arXiv:2205.10430",
    "title": "Using machine learning on new feature sets extracted from 3D models of  broken animal bones to classify fragments according to break agent",
    "abstract": "Distinguishing agents of bone modification at paleoanthropological sites is\nat the root of much of the research directed at understanding early hominin\nexploitation of large animal resources and the effects those subsistence\nbehaviors had on early hominin evolution. However, current methods,\nparticularly in the area of fracture pattern analysis as a signal of marrow\nexploitation, have failed to overcome equifinality. Furthermore, researchers\ndebate the replicability and validity of current and emerging methods for\nanalyzing bone modifications. Here we present a new approach to fracture\npattern analysis aimed at distinguishing bone fragments resulting from hominin\nbone breakage and those produced by carnivores. This new method uses 3D models\nof fragmentary bone to extract a much richer dataset that is more transparent\nand replicable than feature sets previously used in fracture pattern analysis.\nSupervised machine learning algorithms are properly used to classify bone\nfragments according to agent of breakage with average mean accuracy of 77%\nacross tests.",
    "descriptor": "",
    "authors": [
      "Katrina Yezzi-Woodley",
      "Alexander Terwilliger",
      "Jiafeng Li",
      "Eric Chen",
      "Martha Tappen",
      "Jeff Calder",
      "Peter J. Olver"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10430"
  },
  {
    "id": "arXiv:2205.10431",
    "title": "Learning Dense Reward with Temporal Variant Self-Supervision",
    "abstract": "Rewards play an essential role in reinforcement learning. In contrast to\nrule-based game environments with well-defined reward functions, complex\nreal-world robotic applications, such as contact-rich manipulation, lack\nexplicit and informative descriptions that can directly be used as a reward.\nPrevious effort has shown that it is possible to algorithmically extract dense\nrewards directly from multimodal observations. In this paper, we aim to extend\nthis effort by proposing a more efficient and robust way of sampling and\nlearning. In particular, our sampling approach utilizes temporal variance to\nsimulate the fluctuating state and action distribution of a manipulation task.\nWe then proposed a network architecture for self-supervised learning to better\nincorporate temporal information in latent representations. We tested our\napproach in two experimental setups, namely joint-assembly and door-opening.\nPreliminary results show that our approach is effective and efficient in\nlearning dense rewards, and the learned rewards lead to faster convergence than\nbaselines.",
    "descriptor": "\nComments: 8 pages, 6 figures, submitted to ICRA 2022 Workshop\n",
    "authors": [
      "Yuning Wu",
      "Jieliang Luo",
      "Hui Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10431"
  },
  {
    "id": "arXiv:2205.10433",
    "title": "Economic model predictive control of integrated energy systems: A  multi-time-scale framework",
    "abstract": "In this work, a composite economic model predictive control (CEMPC) is\nproposed for the optimal operation of a stand-alone integrated energy system\n(IES). Time-scale multiplicity exists in IESs dynamics is taken into account\nand addressed using multi-time-scale decomposition. The entire IES is\ndecomposed into three reduced-order subsystems with slow, medium, and fast\ndynamics. Subsequently, the CEMPC, which includes slow economic model\npredictive control (EMPC), medium EMPC and fast EMPC, is developed. The EMPCs\ncommunicate with each other to ensure consistency in decision-making. In the\nslow EMPC, the global control objectives are optimized, and the manipulated\ninputs explicitly affecting the slow dynamics are applied. The medium EMPC\noptimizes the control objectives correlated with the medium dynamics and\napplies the corresponding optimal medium inputs to the IES, while the fast EMPC\noptimizes the fast dynamics relevant objectives and makes a decision on the\nmanipulated inputs directly associated with the fast dynamics. Meanwhile,\nthermal comfort is integrated into the CEMPC in the form of zone tracking of\nthe building temperature for achieving more control degrees of freedom to\nprioritize satisfying the electric demand and reducing operating costs of the\nIES. Moreover, a long-term EMPC based on a simplified slow subsystem model is\ndeveloped and incorporated into the CEMPC to ensure that the operating state\naccommodates long-term forecasts for external conditions. Finally, the\neffectiveness and superiority of the proposed method are demonstrated via\nsimulations and a comparison with a hierarchical real-time optimization\nmechanism.",
    "descriptor": "",
    "authors": [
      "Long Wu",
      "Xunyuan Yin",
      "Lei Pan",
      "Jinfeng Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10433"
  },
  {
    "id": "arXiv:2205.10435",
    "title": "Towards Better Understanding Attribution Methods",
    "abstract": "Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods. Finally, we propose a post-processing smoothing step that\nsignificantly improves the performance of some attribution methods, and discuss\nits applicability.",
    "descriptor": "\nComments: 30 pages, 31 figures, 2 tables, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022\n",
    "authors": [
      "Sukrut Rao",
      "Moritz B\u00f6hle",
      "Bernt Schiele"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10435"
  },
  {
    "id": "arXiv:2205.10438",
    "title": "Dynamic Ensemble Selection Using Fuzzy Hyperboxes",
    "abstract": "Most dynamic ensemble selection (DES) methods utilize the K-Nearest Neighbors\n(KNN) algorithm to estimate the competence of classifiers in a small region\nsurrounding the query sample. However, KNN is very sensitive to the local\ndistribution of the data. Moreover, it also has a high computational cost as it\nrequires storing the whole data in memory and performing multiple distance\ncalculations during inference. Hence, the dependency on the KNN algorithm ends\nup limiting the use of DES techniques for large-scale problems. This paper\npresents a new DES framework based on fuzzy hyperboxes called FH-DES. Each\nhyperbox can represent a group of samples using only two data points (Min and\nMax corners). Thus, the hyperbox-based system will have less computational\ncomplexity than other dynamic selection methods. In addition, despite the\nKNN-based approaches, the fuzzy hyperbox is not sensitive to the local data\ndistribution. Therefore, the local distribution of the samples does not affect\nthe system's performance. Furthermore, in this research, for the first time,\nmisclassified samples are used to estimate the competence of the classifiers,\nwhich has not been observed in previous fusion approaches. Experimental results\ndemonstrate that the proposed method has high classification accuracy while\nhaving a lower complexity when compared with the state-of-the-art dynamic\nselection methods. The implemented code is available at\nhttps://github.com/redavtalab/FH-DES_IJCNN.git.",
    "descriptor": "",
    "authors": [
      "Reza Davtalab",
      "Rafael M.O. Cruz",
      "Robert Sabourin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10438"
  },
  {
    "id": "arXiv:2205.10439",
    "title": "How Useful are Gradients for OOD Detection Really?",
    "abstract": "One critical challenge in deploying highly performant machine learning models\nin real-life applications is out of distribution (OOD) detection. Given a\npredictive model which is accurate on in distribution (ID) data, an OOD\ndetection system will further equip the model with the option to defer\nprediction when the input is novel and the model has little confidence in\nprediction. There has been some recent interest in utilizing the gradient\ninformation in pre-trained models for OOD detection. While these methods have\nshown competitive performance, there are misconceptions about the true\nmechanism underlying them, which conflate their performance with the necessity\nof gradients. In this work, we provide an in-depth analysis and comparison of\ngradient based methods and elucidate the key components that warrant their OOD\ndetection performance. We further propose a general, non-gradient based method\nof OOD detection which improves over previous baselines in both performance and\ncomputational efficiency.",
    "descriptor": "",
    "authors": [
      "Conor Igoe",
      "Youngseog Chung",
      "Ian Char",
      "Jeff Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10439"
  },
  {
    "id": "arXiv:2205.10441",
    "title": "Predicting Seriousness of Injury in a Traffic Accident: A New Imbalanced  Dataset and Benchmark",
    "abstract": "The paper introduces a new dataset to assess the performance of machine\nlearning algorithms in the prediction of the seriousness of injury in a traffic\naccident. The dataset is created by aggregating publicly available datasets\nfrom the UK Department for Transport, which are drastically imbalanced with\nmissing attributes sometimes approaching 50\\% of the overall data\ndimensionality. The paper presents the data analysis pipeline starting from the\npublicly available data of road traffic accidents and ending with predictors of\npossible injuries and their degree of severity. It addresses the huge\nincompleteness of public data with a MissForest model. The paper also\nintroduces two baseline approaches to create injury predictors: a supervised\nartificial neural network and a reinforcement learning model. The dataset can\npotentially stimulate diverse aspects of machine learning research on\nimbalanced datasets and the two approaches can be used as baseline references\nwhen researchers test more advanced learning algorithms in this area.",
    "descriptor": "",
    "authors": [
      "Paschalis Lagias",
      "George D. Magoulas",
      "Ylli Prifti",
      "Alessandro Provetti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10441"
  },
  {
    "id": "arXiv:2205.10442",
    "title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
    "abstract": "Solving crossword puzzles requires diverse reasoning capabilities, access to\na vast amount of knowledge about language and the world, and the ability to\nsatisfy the constraints imposed by the structure of the puzzle. In this work,\nwe introduce solving crossword puzzles as a new natural language understanding\ntask. We release the specification of a corpus of crossword puzzles collected\nfrom the New York Times daily crossword spanning 25 years and comprised of a\ntotal of around nine thousand puzzles. These puzzles include a diverse set of\nclues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank,\nabbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues\nthat depend on the answers to other clues. We separately release the\nclue-answer pairs from these puzzles as an open-domain question answering\ndataset containing over half a million unique clue-answer pairs. For the\nquestion answering task, our baselines include several sequence-to-sequence and\nretrieval-based generative models. We also introduce a non-parametric\nconstraint satisfaction baseline for solving the entire crossword puzzle.\nFinally, we propose an evaluation framework which consists of several\ncomplementary performance metrics.",
    "descriptor": "\nComments: Accepted as long paper at ACL 2022\n",
    "authors": [
      "Saurabh Kulshreshtha",
      "Olga Kovaleva",
      "Namrata Shivagunde",
      "Anna Rumshisky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10442"
  },
  {
    "id": "arXiv:2205.10449",
    "title": "A Hybrid Model for Forecasting Short-Term Electricity Demand",
    "abstract": "Currently the UK Electric market is guided by load (demand) forecasts\npublished every thirty minutes by the regulator. A key factor in predicting\ndemand is weather conditions, with forecasts published every hour. We present\nHYENA: a hybrid predictive model that combines feature engineering (selection\nof the candidate predictor features), mobile-window predictors and finally LSTM\nencoder-decoders to achieve higher accuracy with respect to mainstream models\nfrom the literature. HYENA decreased MAPE loss by 16\\% and RMSE loss by 10\\%\nover the best available benchmark model, thus establishing a new state of the\nart for the UK electric load (and price) forecasting.",
    "descriptor": "",
    "authors": [
      "Maria Eleni Athanasopoulou",
      "Justina Deveikyte",
      "Alan Mosca",
      "Ilaria Peri",
      "Alessandro Provetti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10449"
  },
  {
    "id": "arXiv:2205.10450",
    "title": "Temporally Precise Action Spotting in Soccer Videos Using Dense  Detection Anchors",
    "abstract": "We present a model for temporally precise action spotting in videos, which\nuses a dense set of detection anchors, predicting a detection confidence and\ncorresponding fine-grained temporal displacement for each anchor. We experiment\nwith two trunk architectures, both of which are able to incorporate large\ntemporal contexts while preserving the smaller-scale features required for\nprecise localization: a one-dimensional version of a u-net, and a Transformer\nencoder (TE). We also suggest best practices for training models of this kind,\nby applying Sharpness-Aware Minimization (SAM) and mixup data augmentation. We\nachieve a new state-of-the-art on SoccerNet-v2, the largest soccer video\ndataset of its kind, with marked improvements in temporal localization.\nAdditionally, our ablations show: the importance of predicting the temporal\ndisplacements; the trade-offs between the u-net and TE trunks; and the benefits\nof training with SAM and mixup.",
    "descriptor": "",
    "authors": [
      "Jo\u00e3o V. B. Soares",
      "Avijit Shah",
      "Topojoy Biswas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10450"
  },
  {
    "id": "arXiv:2205.10451",
    "title": "Searching for PETs: Using Distributional and Sentiment-Based Methods to  Find Potentially Euphemistic Terms",
    "abstract": "This paper presents a linguistically driven proof of concept for finding\npotentially euphemistic terms, or PETs. Acknowledging that PETs tend to be\ncommonly used expressions for a certain range of sensitive topics, we make use\nof distributional similarities to select and filter phrase candidates from a\nsentence and rank them using a set of simple sentiment-based metrics. We\npresent the results of our approach tested on a corpus of sentences containing\neuphemisms, demonstrating its efficacy for detecting single and multi-word PETs\nfrom a broad range of topics. We also discuss future potential for\nsentiment-based methods on this task.",
    "descriptor": "",
    "authors": [
      "Patrick Lee",
      "Martha Gavidia",
      "Anna Feldman",
      "Jing Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10451"
  },
  {
    "id": "arXiv:2205.10454",
    "title": "E2FL: Equal and Equitable Federated Learning",
    "abstract": "Federated Learning (FL) enables data owners to train a shared global model\nwithout sharing their private data. Unfortunately, FL is susceptible to an\nintrinsic fairness issue: due to heterogeneity in clients' data distributions,\nthe final trained model can give disproportionate advantages across the\nparticipating clients. In this work, we present Equal and Equitable Federated\nLearning (E2FL) to produce fair federated learning models by preserving two\nmain fairness properties, equity and equality, concurrently. We validate the\nefficiency and fairness of E2FL in different real-world FL applications, and\nshow that E2FL outperforms existing baselines in terms of the resulting\nefficiency, fairness of different groups, and fairness among all individual\nclients.",
    "descriptor": "",
    "authors": [
      "Hamid Mozaffari",
      "Amir Houmansadr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10454"
  },
  {
    "id": "arXiv:2205.10455",
    "title": "Pre-training Transformer Models with Sentence-Level Objectives for  Answer Sentence Selection",
    "abstract": "An important task for designing QA systems is answer sentence selection\n(AS2): selecting the sentence containing (or constituting) the answer to a\nquestion from a set of retrieved relevant documents. In this paper, we propose\nthree novel sentence-level transformer pre-training objectives that incorporate\nparagraph-level semantics within and across documents, to improve the\nperformance of transformers for AS2, and mitigate the requirement of large\nlabeled datasets. Our experiments on three public and one industrial AS2\ndatasets demonstrate the empirical superiority of our pre-trained transformers\nover baseline models such as RoBERTa and ELECTRA for AS2.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Luca Di Liello",
      "Siddhant Garg",
      "Luca Soldaini",
      "Alessandro Moschitti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10455"
  },
  {
    "id": "arXiv:2205.10456",
    "title": "PSO-Convolutional Neural Networks with Heterogeneous Learning Rate",
    "abstract": "Convolutional Neural Networks (ConvNets) have been candidly deployed in the\nscope of computer vision and related fields. Nevertheless, the dynamics of\ntraining of these neural networks lie still elusive: it is hard and\ncomputationally expensive to train them. A myriad of architectures and training\nstrategies have been proposed to overcome this challenge and address several\nproblems in image processing such as speech, image and action recognition as\nwell as object detection. In this article, we propose a novel Particle Swarm\nOptimization (PSO) based training for ConvNets. In such framework, the vector\nof weights of each ConvNet is typically cast as the position of a particle in\nphase space whereby PSO collaborative dynamics intertwines with Stochastic\nGradient Descent (SGD) in order to boost training performance and\ngeneralization. Our approach goes as follows: i) [warm-up phase] each ConvNet\nis trained independently via SGD; ii) [collaborative phase] ConvNets share\namong themselves their current vector of weights (or particle-position) along\nwith their gradient estimates of the Loss function. Distinct step sizes are\ncoined by distinct ConvNets. By properly blending ConvNets with large (possibly\nrandom) step-sizes along with more conservative ones, we propose an algorithm\nwith competitive performance with respect to other PSO-based approaches on\nCifar-10 (accuracy of 98.31%). These accuracy levels are obtained by resorting\nto only four ConvNets -- such results are expected to scale with the number of\ncollaborative ConvNets accordingly. We make our source codes available for\ndownload https://github.com/leonlha/PSO-ConvNet-Dynamics.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Nguyen Huu Phong",
      "Augusto Santos",
      "Bernardete Ribeiro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10456"
  },
  {
    "id": "arXiv:2205.10457",
    "title": "Robust Sensible Adversarial Learning of Deep Neural Networks for Image  Classification",
    "abstract": "The idea of robustness is central and critical to modern statistical\nanalysis. However, despite the recent advances of deep neural networks (DNNs),\nmany studies have shown that DNNs are vulnerable to adversarial attacks. Making\nimperceptible changes to an image can cause DNN models to make the wrong\nclassification with high confidence, such as classifying a benign mole as a\nmalignant tumor and a stop sign as a speed limit sign. The trade-off between\nrobustness and standard accuracy is common for DNN models. In this paper, we\nintroduce sensible adversarial learning and demonstrate the synergistic effect\nbetween pursuits of standard natural accuracy and robustness. Specifically, we\ndefine a sensible adversary which is useful for learning a robust model while\nkeeping high natural accuracy. We theoretically establish that the Bayes\nclassifier is the most robust multi-class classifier with the 0-1 loss under\nsensible adversarial learning. We propose a novel and efficient algorithm that\ntrains a robust model using implicit loss truncation. We apply sensible\nadversarial learning for large-scale image classification to a handwritten\ndigital image dataset called MNIST and an object recognition colored image\ndataset called CIFAR10. We have performed an extensive comparative study to\ncompare our method with other competitive methods. Our experiments empirically\ndemonstrate that our method is not sensitive to its hyperparameter and does not\ncollapse even with a small model capacity while promoting robustness against\nvarious attacks and keeping high natural accuracy.",
    "descriptor": "",
    "authors": [
      "Jungeum Kim",
      "Xiao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10457"
  },
  {
    "id": "arXiv:2205.10458",
    "title": "Swim: A Runtime for Distributed Event-Driven Applications",
    "abstract": "Swim extends the actor model to support applications composed of linked\ndistributed actors that continuously analyze boundless streams of events from\nmillions of sources, to respond in-sync with the real-world.\nSwim builds a running application from streaming events, creating a\ndistributed dataflow graph of linked, stateful, concurrent streaming actors\nthat is overlaid on a mesh of runtime instances. Streaming actors are vertices\nin the dataflow graph that concurrently analyze new events and modify their\nstates. A link is an edge in the graph and is a URI binding to an actor's\nstreaming API. The Swim runtime streams every actor state change over its links\nto other (possibly remote) actors using op-based CRDTs that asynchronously\nupdate remotely cached actor state replicas. This frees local actors to compute\nat any time, using the latest replicas of remote state. Actors evaluate\nparametric functions, including geospatial, analytical, and predictive, to\ndiscover new relationships and forge or break links, dynamically adapting the\ndataflow graph to model the changing real-world. Swim applications are tiny,\nrobust and resource efficient, and remain effortlessly in-sync with the\nreal-world, analyzing, learning, and predicting on-the-fly.",
    "descriptor": "",
    "authors": [
      "Chris Sachs",
      "Ajay Govindarajan",
      "Simon Crosby"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10458"
  },
  {
    "id": "arXiv:2205.10459",
    "title": "Multi-stage Resilience Management of Smart Power Distribution Systems: A  Stochastic Robust Optimization Model",
    "abstract": "Significant outages from weather and climate extremes have highlighted the\ncritical need for resilience-centered risk management of the grid. This paper\nproposes a multi-stage stochastic robust optimization (SRO) model that advances\nthe existing planning frameworks on two main fronts. First, it captures\ninteractions of operational measures with hardening decisions. Second, it\nproperly treats the multitude of uncertainties in planning. The SRO model\ncoordinates hardening and system operational measures for smart power\ndistribution systems equipped with distributed generation units and switches.\nTo capture the uncertainty in the incurred damage by extreme events, an\nuncertainty set is developed by integrating probabilistic information of\nhurricanes with the performance of overhead structures. A novel probabilistic\nmodel for the repair time of damaged lines is derived to account for the\nuncertainty in the recovery process. A solution strategy based on the\nintegration of a differential evolution algorithm and a mixed-integer solver is\ndesigned to solve the resilience maximization model. The proposed approach is\napplied to a modified IEEE 33-bus system with 485 utility poles and a 118-bus\nsystem with 1841 poles. The systems are mapped on the Harris County, TX, U.S.\nResults reveal that optimal hardening decisions can be significantly influenced\nby resilience operational measures.",
    "descriptor": "",
    "authors": [
      "Nariman L. Dehghani",
      "Abdollah Shafieezadeh"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.10459"
  },
  {
    "id": "arXiv:2205.10464",
    "title": "Synthesis from Satisficing and Temporal Goals",
    "abstract": "Reactive synthesis from high-level specifications that combine hard\nconstraints expressed in Linear Temporal Logic LTL with soft constraints\nexpressed by discounted-sum (DS) rewards has applications in planning and\nreinforcement learning. An existing approach combines techniques from LTL\nsynthesis with optimization for the DS rewards but has failed to yield a sound\nalgorithm. An alternative approach combining LTL synthesis with satisficing DS\nrewards (rewards that achieve a threshold) is sound and complete for integer\ndiscount factors, but, in practice, a fractional discount factor is desired.\nThis work extends the existing satisficing approach, presenting the first sound\nalgorithm for synthesis from LTL and DS rewards with fractional discount\nfactors. The utility of our algorithm is demonstrated on robotic planning\ndomains.",
    "descriptor": "",
    "authors": [
      "Suguman Bansal",
      "Lydia Kavraki",
      "Moshe Y. Vardi",
      "Andrew Wells"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10464"
  },
  {
    "id": "arXiv:2205.10466",
    "title": "A Survey on Physiological Signal Based Emotion Recognition",
    "abstract": "Physiological Signals are the most reliable form of signals for emotion\nrecognition, as they cannot be controlled deliberately by the subject. Existing\nreview papers on emotion recognition based on physiological signals surveyed\nonly the regular steps involved in the workflow of emotion recognition such as\npreprocessing, feature extraction, and classification. While these are\nimportant steps, such steps are required for any signal processing application.\nEmotion recognition poses its own set of challenges that are very important to\naddress for a robust system. Thus, to bridge the gap in the existing\nliterature, in this paper, we review the effect of inter-subject data variance\non emotion recognition, important data annotation techniques for emotion\nrecognition and their comparison, data preprocessing techniques for each\nphysiological signal, data splitting techniques for improving the\ngeneralization of emotion recognition models and different multimodal fusion\ntechniques and their comparison. Finally we discuss key challenges and future\ndirections in this field.",
    "descriptor": "",
    "authors": [
      "Zeeshan Ahmad",
      "Naimul Khan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10466"
  },
  {
    "id": "arXiv:2205.10468",
    "title": "Deep Learning for Omnidirectional Vision: A Survey and New Perspectives",
    "abstract": "Omnidirectional image (ODI) data is captured with a 360x180 field-of-view,\nwhich is much wider than the pinhole cameras and contains richer spatial\ninformation than the conventional planar images. Accordingly, omnidirectional\nvision has attracted booming attention due to its more advantageous performance\nin numerous applications, such as autonomous driving and virtual reality. In\nrecent years, the availability of customer-level 360 cameras has made\nomnidirectional vision more popular, and the advance of deep learning (DL) has\nsignificantly sparked its research and applications. This paper presents a\nsystematic and comprehensive review and analysis of the recent progress in DL\nmethods for omnidirectional vision. Our work covers four main contents: (i) An\nintroduction to the principle of omnidirectional imaging, the convolution\nmethods on the ODI, and datasets to highlight the differences and difficulties\ncompared with the 2D planar image data; (ii) A structural and hierarchical\ntaxonomy of the DL methods for omnidirectional vision; (iii) A summarization of\nthe latest novel learning strategies and applications; (iv) An insightful\ndiscussion of the challenges and open problems by highlighting the potential\nresearch directions to trigger more research in the community.",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Hao Ai",
      "Zidong Cao",
      "Jinjing Zhu",
      "Haotian Bai",
      "Yucheng Chen",
      "Ling Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10468"
  },
  {
    "id": "arXiv:2205.10469",
    "title": "Masterful: A Training Platform for Computer Vision Models",
    "abstract": "Masterful is a software platform to train deep learning computer vision\nmodels. Data and model architecture are inputs to the platform, and the output\nis a trained model. The platform's primary goal is to maximize a trained\nmodel's accuracy, which it achieves through its regularization and\nsemi-supervised learning implementations. The platform's secondary goal is to\nminimize the amount of manual experimentation typically required to tune\ntraining hyperparameters, which it achieves via multiple metalearning\nalgorithms which are custom built to control the platform's regularization and\nsemi-supervised learning implementations. The platform's tertiary goal is to\nminimize the computing resources required to train a model, which it achieves\nvia another set of metalearning algorithms which are purpose built to control\nTensorflow's optimization implementations. The platform builds on top of\nTensorflow's data management, architecture, automatic differentiation, and\noptimization implementations.",
    "descriptor": "",
    "authors": [
      "Samuel Wookey",
      "Yaoshiang Ho",
      "Tom Rikert",
      "Juan David Gil Lopez",
      "Juan Manuel Mu\u00f1oz Beancur",
      "Santiago Cortes",
      "Ray Tawil",
      "Aaron Sabin",
      "Jack Lynch",
      "Travis Harper",
      "Nikhil Gajendrakumar"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10469"
  },
  {
    "id": "arXiv:2205.10471",
    "title": "Retrieval-Augmented Multilingual Keyphrase Generation with  Retriever-Generator Iterative Training",
    "abstract": "Keyphrase generation is the task of automatically predicting keyphrases given\na piece of long text. Despite its recent flourishing, keyphrase generation on\nnon-English languages haven't been vastly investigated. In this paper, we call\nattention to a new setting named multilingual keyphrase generation and we\ncontribute two new datasets, EcommerceMKP and AcademicMKP, covering six\nlanguages. Technically, we propose a retrieval-augmented method for\nmultilingual keyphrase generation to mitigate the data shortage problem in\nnon-English languages. The retrieval-augmented model leverages keyphrase\nannotations in English datasets to facilitate generating keyphrases in\nlow-resource languages. Given a non-English passage, a cross-lingual dense\npassage retrieval module finds relevant English passages. Then the associated\nEnglish keyphrases serve as external knowledge for keyphrase generation in the\ncurrent language. Moreover, we develop a retriever-generator iterative training\nalgorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual\npassage retriever. Comprehensive experiments and ablations show that the\nproposed approach outperforms all baselines.",
    "descriptor": "\nComments: NAACL 2022 (Findings)\n",
    "authors": [
      "Yifan Gao",
      "Qingyu Yin",
      "Zheng Li",
      "Rui Meng",
      "Tong Zhao",
      "Bing Yin",
      "Irwin King",
      "Michael R. Lyu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10471"
  },
  {
    "id": "arXiv:2205.10473",
    "title": "De novo design of protein target specific scaffold-based Inhibitors via  Reinforcement Learning",
    "abstract": "Efficient design and discovery of target-driven molecules is a critical step\nin facilitating lead optimization in drug discovery. Current approaches to\ndevelop molecules for a target protein are intuition-driven, hampered by slow\niterative design-test cycles due to computational challenges in utilizing 3D\nstructural data, and ultimately limited by the expertise of the chemist -\nleading to bottlenecks in molecular design. In this contribution, we propose a\nnovel framework, called 3D-MolGNN$_{RL}$, coupling reinforcement learning (RL)\nto a deep generative model based on 3D-Scaffold to generate target candidates\nspecific to a protein building up atom by atom from the starting core scaffold.\n3D-MolGNN$_{RL}$ provides an efficient way to optimize key features by\nmulti-objective reward function within a protein pocket using parallel graph\nneural network models. The agent learns to build molecules in 3D space while\noptimizing the activity, binding affinity, potency, and synthetic accessibility\nof the candidates generated for infectious disease protein targets. Our\napproach can serve as an interpretable artificial intelligence (AI) tool for\nlead optimization with optimized activity, potency, and biophysical properties.",
    "descriptor": "\nComments: Published at the MLDD workshop, ICLR 2022\n",
    "authors": [
      "Andrew D. McNaughton",
      "Mridula S. Bontha",
      "Carter R. Knutson",
      "Jenna A. Pope",
      "Neeraj Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10473"
  },
  {
    "id": "arXiv:2205.10475",
    "title": "DeepStruct: Pretraining of Language Models for Structure Prediction",
    "abstract": "We introduce a method for improving the structural understanding abilities of\nlanguage models. Unlike previous approaches that finetune the models with\ntask-specific augmentation, we pretrain language models on a collection of\ntask-agnostic corpora to generate structures from text. Our structure\npretraining enables zero-shot transfer of the learned knowledge that models\nhave about the structure tasks. We study the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks including open information\nextraction, joint entity and relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event extraction, coreference\nresolution, factual probe, intent detection, and dialogue state tracking. We\nfurther enhance the pretraining with the task-specific training sets. We show\nthat a 10B parameter language model transfers non-trivially to most tasks and\nobtains state-of-the-art performance on 21 of 28 datasets that we evaluate.",
    "descriptor": "\nComments: ACL 2022\n",
    "authors": [
      "Chenguang Wang",
      "Xiao Liu",
      "Zui Chen",
      "Haoyun Hong",
      "Jie Tang",
      "Dawn Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10475"
  },
  {
    "id": "arXiv:2205.10479",
    "title": "DKG: A Descriptive Knowledge Graph for Explaining Relationships between  Entities",
    "abstract": "In this paper, we propose Descriptive Knowledge Graph (DKG) - an open and\ninterpretable form of modeling relationships between entities. In DKGs,\nrelationships between entities are represented by relation descriptions. For\ninstance, the relationship between entities of machine learning and algorithm\ncan be described as \"Machine learning explores the study and construction of\nalgorithms that can learn from and make predictions on data.\" To construct\nDKGs, we propose a self-supervised learning method to extract relation\ndescriptions with the analysis of dependency patterns and a transformer-based\nrelation description synthesizing model to generate relation descriptions.\nExperiments demonstrate that our system can extract and generate high-quality\nrelation descriptions for explaining entity relationships.",
    "descriptor": "",
    "authors": [
      "Jie Huang",
      "Kerui Zhu",
      "Kevin Chen-Chuan Chang",
      "Jinjun Xiong",
      "Wen-mei Hwu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10479"
  },
  {
    "id": "arXiv:2205.10481",
    "title": "Semi-Supervised Subspace Clustering via Tensor Low-Rank Representation",
    "abstract": "In this letter, we propose a novel semi-supervised subspace clustering\nmethod, which is able to simultaneously augment the initial supervisory\ninformation and construct a discriminative affinity matrix. By representing the\nlimited amount of supervisory information as a pairwise constraint matrix, we\nobserve that the ideal affinity matrix for clustering shares the same low-rank\nstructure as the ideal pairwise constraint matrix. Thus, we stack the two\nmatrices into a 3-D tensor, where a global low-rank constraint is imposed to\npromote the affinity matrix construction and augment the initial pairwise\nconstraints synchronously. Besides, we use the local geometry structure of\ninput samples to complement the global low-rank prior to achieve better\naffinity matrix learning. The proposed model is formulated as a Laplacian graph\nregularized convex low-rank tensor representation problem, which is further\nsolved with an alternative iterative algorithm. In addition, we propose to\nrefine the affinity matrix with the augmented pairwise constraints.\nComprehensive experimental results on six commonly-used benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art methods. The\ncode is publicly available at\nhttps://github.com/GuanxingLu/Subspace-Clustering.",
    "descriptor": "",
    "authors": [
      "Guanxing Lu",
      "Yuheng Jia",
      "Junhui Hou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10481"
  },
  {
    "id": "arXiv:2205.10483",
    "title": "Deep Reinforcement Learning Coordinated Receiver Beamforming for  Millimeter-Wave Train-ground Communications",
    "abstract": "As more and more people choose high-speed rail (HSR) as a means of\ntransportation for short trips, there is ever growing demand of high quality of\nmultimedia services. With its rich spectrum resources, millimeter wave\n(mm-wave) communications can satisfy the high network capacity requirements for\nHSR. Also, it is possible for receivers (RXs) to be equipped with antenna\narrays in mm-wave communication systems due to its short wavelength. However,\nas HSRs run with high speed, the received signal power (RSP) varies rapidly\nover a cell and it is the lowest at the edge of the cell compared to other\nlocations. Consequently, it is necessary to conduct research on RX beamforming\nfor HSR in mm-wave band to improve the quality of the received signal. In this\npaper, we focus on RX beamforming for a mm-wave train-ground communication\nsystem. To improve the RSP, we propose an effective RX beamforming scheme based\non deep reinforcement learning (DRL), and develop a deep Q-network (DQN)\nalgorithm to train and determine the optimal RX beam direction with the purpose\nof maximizing average RSP. Through extensive simulations, we demonstrate that\nthe proposed scheme has better performance than the four baseline schemes in\nterms of average RSP at most positions on the railway.",
    "descriptor": "\nComments: 15 pages, 10 figures, IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "Xutao Zhou",
      "Xiangfei Zhang",
      "Chen Chen",
      "Yong Niu",
      "Zhu Han",
      "He Wang",
      "Chengjun Sun",
      "Bo Ai",
      "Ning Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10483"
  },
  {
    "id": "arXiv:2205.10484",
    "title": "Nuclear Norm Maximization Based Curiosity-Driven Learning",
    "abstract": "To handle the sparsity of the extrinsic rewards in reinforcement learning,\nresearchers have proposed intrinsic reward which enables the agent to learn the\nskills that might come in handy for pursuing the rewards in the future, such as\nencouraging the agent to visit novel states. However, the intrinsic reward can\nbe noisy due to the undesirable environment's stochasticity and directly\napplying the noisy value predictions to supervise the policy is detrimental to\nimprove the learning performance and efficiency. Moreover, many previous\nstudies employ $\\ell^2$ norm or variance to measure the exploration novelty,\nwhich will amplify the noise due to the square operation. In this paper, we\naddress aforementioned challenges by proposing a novel curiosity leveraging the\nnuclear norm maximization (NNM), which can quantify the novelty of exploring\nthe environment more accurately while providing high-tolerance to the noise and\noutliers. We conduct extensive experiments across a variety of benchmark\nenvironments and the results suggest that NNM can provide state-of-the-art\nperformance compared with previous curiosity methods. On 26 Atari games subset,\nNNM achieves a human-normalized score of 1.09, which doubles that of\ncompetitive intrinsic rewards-based approaches. Our code will be released\npublicly to enhance the reproducibility.",
    "descriptor": "",
    "authors": [
      "Chao Chen",
      "Zijian Gao",
      "Kele Xu",
      "Sen Yang",
      "Yiying Li",
      "Bo Ding",
      "Dawei Feng",
      "Huaimin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10484"
  },
  {
    "id": "arXiv:2205.10487",
    "title": "Scaling Laws and Interpretability of Learning from Repeated Data",
    "abstract": "Recent large language models have been trained on vast datasets, but also\noften on repeated data, either intentionally for the purpose of upweighting\nhigher quality data, or unintentionally because data deduplication is not\nperfect and the model is exposed to repeated data at the sentence, paragraph,\nor document level. Some works have reported substantial negative performance\neffects of this repeated data. In this paper we attempt to study repeated data\nsystematically and to understand its effects mechanistically. To do this, we\ntrain a family of models where most of the data is unique but a small fraction\nof it is repeated many times. We find a strong double descent phenomenon, in\nwhich repeated data can lead test loss to increase midway through training. A\npredictable range of repetition frequency leads to surprisingly severe\ndegradation in performance. For instance, performance of an 800M parameter\nmodel can be degraded to that of a 2x smaller model (400M params) by repeating\n0.1% of the data 100 times, despite the other 90% of the training tokens\nremaining unique. We suspect there is a range in the middle where the data can\nbe memorized and doing so consumes a large fraction of the model's capacity,\nand this may be where the peak of degradation occurs. Finally, we connect these\nobservations to recent mechanistic interpretability work - attempting to\nreverse engineer the detailed computations performed by the model - by showing\nthat data repetition disproportionately damages copying and internal structures\nassociated with generalization, such as induction heads, providing a possible\nmechanism for the shift from generalization to memorization. Taken together,\nthese results provide a hypothesis for why repeating a relatively small\nfraction of data in large language models could lead to disproportionately\nlarge harms to performance.",
    "descriptor": "\nComments: 23 pages, 22 figures\n",
    "authors": [
      "Danny Hernandez",
      "Tom Brown",
      "Tom Conerly",
      "Nova DasSarma",
      "Dawn Drain",
      "Sheer El-Showk",
      "Nelson Elhage",
      "Zac Hatfield-Dodds",
      "Tom Henighan",
      "Tristan Hume",
      "Scott Johnston",
      "Ben Mann",
      "Chris Olah",
      "Catherine Olsson",
      "Dario Amodei",
      "Nicholas Joseph",
      "Jared Kaplan",
      "Sam McCandlish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10487"
  },
  {
    "id": "arXiv:2205.10489",
    "title": "Social Fragmentation Transitions in Large-Scale Adaptive Social Network  Simulations",
    "abstract": "Social fragmentation transition is a transition of social states between many\ndisconnected communities with distinct opinions and a well-connected single\nnetwork with homogeneous opinions. This is a timely research topic with high\nrelevance to various current societal issues. We had previously studied this\nproblem using numerical simulations of adaptive social network models and found\nthat two individual behavioral traits, homophily and attention to novelty, had\nthe most statistically significant impact on the outcomes of social network\nevolution. However, our previous study was limited in terms of the range of\nparameter values examined, and possible interactions between multiple\nbehavioral traits were largely ignored. In this study, we conducted a\nsubstantially larger-scale numerical simulation experiment of the same model\nwith an expanded parameter sweep range by an order of magnitude in each\nparameter dimension, resulting in a total of 116,640 simulation runs. To\ncapture nontrivial interactions among behavioral parameters, we modeled and\nvisualized the dependence of outcome measures on the model parameters using\nartificial neural networks. Results show that, while the competition between\nhomophily and attention to novelty is still the primary determinant of social\nfragmentation, another transition plane emerges when individuals have strong\nsocial conformity behavior, which was not previously known. This implies that\nsocial fragmentation transition can also occur in the homophily-social\nconformity trade-off, the two behavioral traits that have very similar\nmicroscopic individual-level effects but produce very different macroscopic\ncollective-level outcomes, illustrating the nontrivial macroscopic dynamics of\ncomplex collective systems.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Hiroki Sayama"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ],
    "url": "https://arxiv.org/abs/2205.10489"
  },
  {
    "id": "arXiv:2205.10490",
    "title": "Mapping Emulation for Knowledge Distillation",
    "abstract": "This paper formalizes the source-blind knowledge distillation problem that is\nessential to federated learning. A new geometric perspective is presented to\nview such a problem as aligning generated distributions between the teacher and\nstudent. With its guidance, a new architecture MEKD is proposed to emulate the\ninverse mapping through generative adversarial training. Unlike mimicking\nlogits and aligning logit distributions, reconstructing the mapping from\nclassifier-logits has a geometric intuition of decreasing empirical distances,\nand theoretical guarantees using the universal function approximation and\noptimal mass transportation theories. A new algorithm is also proposed to train\nthe student model that reaches the teacher's performance source-blindly. On\nvarious benchmarks, MEKD outperforms existing source-blind KD methods,\nexplainable with ablation studies and visualized results.",
    "descriptor": "\nComments: 11 pages, 5 figures, 3 lemmas, 3 corollaries, 1 algorithm\n",
    "authors": [
      "Jing Ma",
      "Xiang Xiang",
      "Zihan Zhang",
      "Yuwen Tan",
      "Yiming Wan",
      "Zhigang Zeng",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10490"
  },
  {
    "id": "arXiv:2205.10491",
    "title": "LSTM-Based Adaptive Vehicle Position Control for Dynamic Wireless  Charging",
    "abstract": "Dynamic wireless charging (DWC) is an emerging technology that allows\nelectric vehicles (EVs) to be wirelessly charged while in motion. It is gaining\nsignificant momentum as it can potentially address the range limitation issue\nfor EVs. However, due to significant power loss caused by wireless power\ntransfer, improving charging efficiency remains as a major challenge for DWC\nsystems. This paper presents the first LSTM-based vehicle motion control system\nfor DWC designed to maximize charging efficiency. The dynamics of the\nelectromagnetic field generated by the transmitter coils of a DWC system are\nmodeled based on a multi-layer LSTM. The LSTM model is used to make a\nprediction of the lateral position where the electromagnetic strength is\nexpected to be maximal and to control the EV motion accordingly to optimize\ncharging efficiency. Simulations were conducted to demonstrate that our\nLSTM-based approach achieves by up to 162.3% higher charging efficiency\ncompared with state-of-the-art vehicle motion control systems focused on\nkeeping an EV in the center of lane.",
    "descriptor": "",
    "authors": [
      "Lokesh Chandra Das",
      "Dipankar Dasgupta",
      "Myounggyu Won"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10491"
  },
  {
    "id": "arXiv:2205.10492",
    "title": "Theoretically Accurate Regularization Technique for Matrix Factorization  based Recommender Systems",
    "abstract": "Regularization is a popular technique to solve the overfitting problem of\nmachine learning algorithms. Most regularization technique relies on parameter\nselection of the regularization coefficient. Plug-in method and\ncross-validation approach are two most common parameter selection approaches\nfor regression methods such as Ridge Regression, Lasso Regression and Kernel\nRegression. Matrix factorization based recommendation system also has heavy\nreliance on the regularization technique. Most people select a single scalar\nvalue to regularize the user feature vector and item feature vector\nindependently or collectively. In this paper, we prove that such approach of\nselecting regularization coefficient is invalid, and we provide a theoretically\naccurate method that outperforms the most widely used approach in both accuracy\nand fairness metrics.",
    "descriptor": "",
    "authors": [
      "Hao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.10492"
  },
  {
    "id": "arXiv:2205.10495",
    "title": "Enriched Robust Multi-View Kernel Subspace Clustering",
    "abstract": "Subspace clustering is to find underlying low-dimensional subspaces and\ncluster the data points correctly. In this paper, we propose a novel multi-view\nsubspace clustering method. Most existing methods suffer from two critical\nissues. First, they usually adopt a two-stage framework and isolate the\nprocesses of affinity learning, multi-view information fusion and clustering.\nSecond, they assume the data lies in a linear subspace which may fail in\npractice as most real-world datasets may have non-linearity structures. To\naddress the above issues, in this paper we propose a novel Enriched Robust\nMulti-View Kernel Subspace Clustering framework where the consensus affinity\nmatrix is learned from both multi-view data and spectral clustering. Due to the\nobjective and constraints which is difficult to optimize, we propose an\niterative optimization method which is easy to implement and can yield closed\nsolution in each step. Extensive experiments have validated the superiority of\nour method over state-of-the-art clustering methods.",
    "descriptor": "",
    "authors": [
      "Mengyuan Zhang",
      "Kai Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10495"
  },
  {
    "id": "arXiv:2205.10497",
    "title": "eBIM-GNN : Fast and Scalable energy analysis through BIMs and Graph  Neural Networks",
    "abstract": "Building Information Modeling has been used to analyze as well as increase\nthe energy efficiency of the buildings. It has shown significant promise in\nexisting buildings by deconstruction and retrofitting. Current cities which\nwere built without the knowledge of energy savings are now demanding better\nways to become smart in energy utilization. However, the existing methods of\ngenerating BIMs work on building basis. Hence they are slow and expensive when\nwe scale to a larger community or even entire towns or cities. In this paper,\nwe propose a method to creation of prototype buildings that enable us to match\nand generate statistics very efficiently. Our method suggests better energy\nefficient prototypes for the existing buildings. The existing buildings are\nidentified and located in the 3D point cloud. We perform experiments on\nsynthetic dataset to demonstrate the working of our approach.",
    "descriptor": "",
    "authors": [
      "Rucha Bhalchandra Joshi",
      "Annada Prasad Behera",
      "Subhankar Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10497"
  },
  {
    "id": "arXiv:2205.10498",
    "title": "Named Entity Linking on Namesakes",
    "abstract": "We propose a simple and practical method of named entity linking (NEL), and\nexplore its features and performance on a dataset of ambiguous named entities -\nNamesakes. We represent knowledge base (KB) entity by a set of embeddings. Our\nobservations suggest that it is reasonable to keep a limited number of such\nembeddings, and that the number of mentions required to create a KB entity is\nimportant. We show that representations of entities in the knowledge base (KB)\ncan be adjusted using only KB data, and the adjustment improves NEL\nperformance.",
    "descriptor": "\nComments: 9 pages, 12 figures\n",
    "authors": [
      "Oleg Vasilyev",
      "Alex Dauenhauer",
      "Vedant Dharnidharka",
      "John Bohannon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10498"
  },
  {
    "id": "arXiv:2205.10504",
    "title": "How to Find Actionable Static Analysis Warnings",
    "abstract": "Automatically generated static code warnings suffer from a large number of\nfalse alarms. Hence, developers only take action on a small percent of those\nwarnings. To better predict which static code warnings should not be ignored,\nwe suggest that analysts need to look deeper into their algorithms to find\nchoices that better improve the particulars of their specific problem.\nSpecifically, we show here that effective predictors of such warnings can be\ncreated by methods that locally adjust the decision boundary (between\nactionable warnings and others). These methods yield a new high water-mark for\nrecognizing actionable static code warnings. For eight open-source Java\nprojects (CASSANDRA, JMETER, COMMONS, LUCENE-SOLR, ANT, TOMCAT, DERBY) we\nachieve perfect test results on 4/8 datasets and, overall, a median AUC (area\nunder the true negatives, true positives curve) of 92\\%.",
    "descriptor": "\nComments: v1\n",
    "authors": [
      "Rahul Yedida",
      "Hong Jin Kang",
      "Huy Tu",
      "Xueqi Yang",
      "David Lo",
      "Tim Menzies"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10504"
  },
  {
    "id": "arXiv:2205.10505",
    "title": "Deeper vs Wider: A Revisit of Transformer Configuration",
    "abstract": "Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.",
    "descriptor": "",
    "authors": [
      "Fuzhao Xue",
      "Jianghai Chen",
      "Aixin Sun",
      "Xiaozhe Ren",
      "Zangwei Zheng",
      "Xiaoxin He",
      "Xin Jiang",
      "Yang You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10505"
  },
  {
    "id": "arXiv:2205.10507",
    "title": "Travel Time, Distance and Costs Optimization for Paratransit Operations  using Graph Convolutional Neural Network",
    "abstract": "The provision of paratransit services is one option to meet the\ntransportation needs of Vulnerable Road Users (VRUs). Like any other means of\ntransportation, paratransit has obstacles such as high operational costs and\nlonger trip times. As a result, customers are dissatisfied, and paratransit\noperators have a low approval rating. Researchers have undertaken various\nstudies over the years to better understand the travel behaviors of paratransit\ncustomers and how they are operated. According to the findings of these\nresearches, paratransit operators confront the challenge of determining the\noptimal route for their trips in order to save travel time. Depending on the\nnature of the challenge, most research used different optimization techniques\nto solve these routing problems. As a result, the goal of this study is to use\nGraph Convolutional Neural Networks (GCNs) to assist paratransit operators in\nresearching various operational scenarios in a strategic setting in order to\noptimize routing, minimize operating costs and minimize their users' travel\ntime. The study was carried out by using a randomized simulated dataset to help\ndetermine the decision to make in terms of fleet composition and capacity under\ndifferent situations. For the various scenarios investigated, the GCN assisted\nin determining the minimum optimal gap.",
    "descriptor": "",
    "authors": [
      "Kelvin Kwakye",
      "Younho Seong",
      "Sun Yi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10507"
  },
  {
    "id": "arXiv:2205.10511",
    "title": "Improving Long Tailed Document-Level Relation Extraction via Easy  Relation Augmentation and Contrastive Learning",
    "abstract": "Towards real-world information extraction scenario, research of relation\nextraction is advancing to document-level relation extraction(DocRE). Existing\napproaches for DocRE aim to extract relation by encoding various information\nsources in the long context by novel model architectures. However, the inherent\nlong-tailed distribution problem of DocRE is overlooked by prior work. We argue\nthat mitigating the long-tailed distribution problem is crucial for DocRE in\nthe real-world scenario. Motivated by the long-tailed distribution problem, we\npropose an Easy Relation Augmentation(ERA) method for improving DocRE by\nenhancing the performance of tailed relations. In addition, we further propose\na novel contrastive learning framework based on our ERA, i.e., ERACL, which can\nfurther improve the model performance on tailed relations and achieve\ncompetitive overall DocRE performance compared to the state-of-arts.",
    "descriptor": "",
    "authors": [
      "Yangkai Du",
      "Tengfei Ma",
      "Lingfei Wu",
      "Yiming Wu",
      "Xuhong Zhang",
      "Bo Long",
      "Shouling Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10511"
  },
  {
    "id": "arXiv:2205.10513",
    "title": "Computable Artificial General Intelligence",
    "abstract": "An artificial general intelligence (AGI), by one definition, is an agent that\nrequires less information than any other to make an accurate prediction. It is\narguable that the general reinforcement learning agent AIXI not only met this\ndefinition, but was the only mathematical formalism to do so. Though a\nsignificant result, AIXI was incomputable and its performance subjective. This\npaper proposes an alternative formalism of AGI which overcomes both problems.\nFormal proof of its performance is given, along with a simple implementation\nand experimental results that support these claims.",
    "descriptor": "\nComments: Experiment code available on TechRxiv: this https URL\n",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10513"
  },
  {
    "id": "arXiv:2205.10517",
    "title": "Pre-training Data Quality and Quantity for a Low-Resource Language: New  Corpus and BERT Models for Maltese",
    "abstract": "Multilingual language models such as mBERT have seen impressive cross-lingual\ntransfer to a variety of languages, but many languages remain excluded from\nthese models. In this paper, we analyse the effect of pre-training with\nmonolingual data for a low-resource language that is not included in mBERT --\nMaltese -- with a range of pre-training set ups. We conduct evaluations with\nthe newly pre-trained models on three morphosyntactic tasks -- dependency\nparsing, part-of-speech tagging, and named-entity recognition -- and one\nsemantic classification task -- sentiment analysis. We also present a newly\ncreated corpus for Maltese, and determine the effect that the pre-training data\nsize and domain have on the downstream performance. Our results show that using\na mixture of pre-training domains is often superior to using Wikipedia text\nonly. We also find that a fraction of this corpus is enough to make significant\nleaps in performance over Wikipedia-trained models. We pre-train and compare\ntwo models on the new corpus: a monolingual BERT model trained from scratch\n(BERTu), and a further pre-trained multilingual BERT (mBERTu). The models\nachieve state-of-the-art performance on these tasks, despite the new corpus\nbeing considerably smaller than typically used corpora for high-resourced\nlanguages. On average, BERTu outperforms or performs competitively with mBERTu,\nand the largest gains are observed for higher-level tasks.",
    "descriptor": "\nComments: DeepLo 2022 camera-ready version\n",
    "authors": [
      "Kurt Micallef",
      "Albert Gatt",
      "Marc Tanti",
      "Lonneke van der Plas",
      "Claudia Borg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10517"
  },
  {
    "id": "arXiv:2205.10519",
    "title": "Impact of Multiple Fully-Absorbing Receivers in Molecular Communications",
    "abstract": "Molecular communication is a promising solution to enable intra-body\ncommunications among nanomachines. However, malicious and non-cooperative\nreceivers can degrade the performance, compromising these systems' security.\nAnalyzing the communication and security performance of these systems requires\naccurate channel models. However, such models are not present in the\nliterature. In this work, we develop an analytical framework to derive the\nhitting probability of a molecule on a fully absorbing receiver (FAR) in the\npresence of other FARs, which can be either be cooperative or malicious. We\nfirst present an approximate hitting probability expression for the 3-FARs\ncase. A simplified expression is obtained for the case when FARs are\nsymmetrically positioned. Using the derived expressions, we study the impact of\nmalicious receivers on the intended receiver and discuss how to minimize this\nimpact to obtain a secure communication channel. We also study the gain that\ncan be obtained by the cooperation of these FARs. We then present an approach\nto extend the analysis for a system with N FARs. The derived expressions can be\nused to analyze and design multiple input/output and secure molecular\ncommunication systems.",
    "descriptor": "",
    "authors": [
      "Nithin V. Sabu",
      "Abhishek K. Gupta",
      "Neeraj Varshney",
      "Anshuman Jindal"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10519"
  },
  {
    "id": "arXiv:2205.10520",
    "title": "Fair Allocation of Indivisible Chores: Beyond Additive Valuations",
    "abstract": "In this work, we study the maximin share (MMS) fair allocation of indivisible\nchores. For additive valuations, Huang and Lu [EC, 2021] designed an algorithm\nto compute a 11/9-approximate MMS fair allocation, and Feige et al. [WINE,\n2021] proved that no algorithm can achieve better than 44/43 approximation.\nBeyond additive valuations, unlike the allocation of goods, very little is\nknown. We first prove that for submodular valuations, in contrast to the\nallocation of goods where constant approximations are proved by Barman and\nKrishnamurthy [TEAC, 2020] and Ghodsi et al [AIJ, 2022], the best possible\napproximation ratio is n. We then focus on two concrete settings where the\nvaluations are combinatorial. In the first setting, agents need to use bins to\npack a set of items where the items may have different sizes to different\nagents and the agents want to use as few bins as possible to pack the items\nassigned to her. In the second setting, each agent has a set of machines that\ncan be used to process a set of items, and the objective is to minimize the\nmakespan of processing the items assigned to her. For both settings, we design\nconstant approximation algorithms, and show that if the fairness notion is\nchanged to proportionality up to one/any item, the best approximation ratio is\nn.",
    "descriptor": "",
    "authors": [
      "Bo Li",
      "Fangxiao Wang",
      "Yu Zhou"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.10520"
  },
  {
    "id": "arXiv:2205.10528",
    "title": "Point is a Vector: A Feature Representation in Point Analysis",
    "abstract": "The irregularity and disorder of point clouds bring many challenges to point\ncloud analysis. PointMLP suggests that geometric information is not the only\ncritical point in point cloud analysis. It achieves promising result based on a\nsimple multi-layer perception (MLP) structure with geometric affine module.\nHowever, these MLP-like structures aggregate features only with fixed weights,\nwhile differences in the semantic information of different point features are\nignored. So we propose a novel Point-Vector Representation of the point feature\nto improve feature aggregation by using inductive bias. The direction of the\nintroduced vector representation can dynamically modulate the aggregation of\ntwo point features according to the semantic relationship. Based on it, we\ndesign a novel Point2Vector MLP architecture. Experiments show that it achieves\nstate-of-the-art performance on the classification task of ScanObjectNN\ndataset, with 1% increase, compared with the previous best method. We hope our\nmethod can help people better understand the role of semantic information in\npoint cloud analysis and lead to explore more and better feature\nrepresentations or other ways.",
    "descriptor": "",
    "authors": [
      "Xin Deng",
      "WengYu Zhang",
      "Qing Ding",
      "XinMing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10528"
  },
  {
    "id": "arXiv:2205.10529",
    "title": "Fine-Grained Visual Classification using Self Assessment Classifier",
    "abstract": "Extracting discriminative features plays a crucial role in the fine-grained\nvisual classification task. Most of the existing methods focus on developing\nattention or augmentation mechanisms to achieve this goal. However, addressing\nthe ambiguity in the top-k prediction classes is not fully investigated. In\nthis paper, we introduce a Self Assessment Classifier, which simultaneously\nleverages the representation of the image and top-k prediction classes to\nreassess the classification results. Our method is inspired by continual\nlearning with coarse-grained and fine-grained classifiers to increase the\ndiscrimination of features in the backbone and produce attention maps of\ninformative areas on the image. In practice, our method works as an auxiliary\nbranch and can be easily integrated into different architectures. We show that\nby effectively addressing the ambiguity in the top-k prediction classes, our\nmethod achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and\nFGVC Aircraft datasets. Furthermore, our method also consistently improves the\naccuracy of different existing fine-grained classifiers with a unified setup.",
    "descriptor": "",
    "authors": [
      "Tuong Do",
      "Huy Tran",
      "Erman Tjiputra",
      "Quang D. Tran",
      "Anh Nguyen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10529"
  },
  {
    "id": "arXiv:2205.10530",
    "title": "Scenario-based Multi-product Advertising Copywriting Generation for  E-Commerce",
    "abstract": "In this paper, we proposed an automatic Scenario-based Multi-product\nAdvertising Copywriting Generation system (SMPACG) for E-Commerce, which has\nbeen deployed on a leading Chinese e-commerce platform. The proposed SMPACG\nconsists of two main components: 1) an automatic multi-product combination\nselection module, which itself is consisted of a topic prediction model, a\npattern and attribute-based selection model and an arbitrator model; and 2) an\nautomatic multi-product advertising copywriting generation module, which\ncombines our proposed domain-specific pretrained language model and\nknowledge-based data enhancement model. The SMPACG is the first system that\nrealizes automatic scenario-based multi-product advertising contents\ngeneration, which achieves significant improvements over other state-of-the-art\nmethods. The SMPACG has been not only developed for directly serving for our\ne-commerce recommendation system, but also used as a real-time writing\nassistant tool for merchants.",
    "descriptor": "",
    "authors": [
      "Xueying Zhang",
      "Kai Shen",
      "Chi Zhang",
      "Xiaochuan Fan",
      "Yun Xiao",
      "Zhen He",
      "Bo Long",
      "Lingfei Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10530"
  },
  {
    "id": "arXiv:2205.10535",
    "title": "Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art  machine learning algorithms for credit scoring",
    "abstract": "Artificial intelligence (AI) and machine learning (ML) have become vital to\nremain competitive for financial services companies around the globe. The two\nmodels currently competing for the pole position in credit risk management are\ndeep learning (DL) and gradient boosting machines (GBM). This paper benchmarked\nthose two algorithms in the context of credit scoring using three distinct\ndatasets with different features to account for the reality that model\nchoice/power is often dependent on the underlying characteristics of the\ndataset. The experiment has shown that GBM tends to be more powerful than DL\nand has also the advantage of speed due to lower computational requirements.\nThis makes GBM the winner and choice for credit scoring. However, it was also\nshown that the outperformance of GBM is not always guaranteed and ultimately\nthe concrete problem scenario or dataset will determine the final model choice.\nOverall, based on this study both algorithms can be considered state-of-the-art\nfor binary classification tasks on structured datasets, while GBM should be the\ngo-to solution for most problem scenarios due to easier use, significantly\nfaster training time, and superior accuracy.",
    "descriptor": "\nComments: Submitted for publication in International Journal of Information Management\n",
    "authors": [
      "Marc Schmitt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Finance (q-fin.CP)",
      "Risk Management (q-fin.RM)"
    ],
    "url": "https://arxiv.org/abs/2205.10535"
  },
  {
    "id": "arXiv:2205.10536",
    "title": "Knowledge Distillation from A Stronger Teacher",
    "abstract": "Unlike existing knowledge distillation methods focus on the baseline\nsettings, where the teacher models and training strategies are not that strong\nand competing as state-of-the-art approaches, this paper presents a method\ndubbed DIST to distill better from a stronger teacher. We empirically find that\nthe discrepancy of predictions between the student and a stronger teacher may\ntend to be fairly severer. As a result, the exact match of predictions in KL\ndivergence would disturb the training and make existing methods perform poorly.\nIn this paper, we show that simply preserving the relations between the\npredictions of teacher and student would suffice, and propose a\ncorrelation-based loss to capture the intrinsic inter-class relations from the\nteacher explicitly. Besides, considering that different instances have\ndifferent semantic similarities to each class, we also extend this relational\nmatch to the intra-class level. Our method is simple yet practical, and\nextensive experiments demonstrate that it adapts well to various architectures,\nmodel sizes and training strategies, and can achieve state-of-the-art\nperformance consistently on image classification, object detection, and\nsemantic segmentation tasks. Code is available at:\nhttps://github.com/hunto/DIST_KD .",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Tao Huang",
      "Shan You",
      "Fei Wang",
      "Chen Qian",
      "Chang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10536"
  },
  {
    "id": "arXiv:2205.10538",
    "title": "Automated machine learning: AI-driven decision making in business  analytics",
    "abstract": "The realization that AI-driven decision-making is indispensable in todays\nfast-paced and ultra-competitive marketplace has raised interest in industrial\nmachine learning (ML) applications significantly. The current demand for\nanalytics experts vastly exceeds the supply. One solution to this problem is to\nincrease the user-friendliness of ML frameworks to make them more accessible\nfor the non-expert. Automated machine learning (AutoML) is an attempt to solve\nthe problem of expertise by providing fully automated off-the-shelf solutions\nfor model choice and hyperparameter tuning. This paper analyzed the potential\nof AutoML for applications within business analytics, which could help to\nincrease the adoption rate of ML across all industries. The H2O AutoML\nframework was benchmarked against a manually tuned stacked ML model on three\nreal-world datasets to test its performance, robustness, and reliability. The\nmanually tuned ML model could reach a performance advantage in all three case\nstudies used in the experiment. Nevertheless, the H2O AutoML package proved to\nbe quite potent. It is fast, easy to use, and delivers reliable results, which\ncome close to a professionally tuned ML model. The H2O AutoML framework in its\ncurrent capacity is a valuable tool to support fast prototyping with the\npotential to shorten development and deployment cycles. It can also bridge the\nexisting gap between supply and demand for ML experts and is a big step towards\nfully automated decisions in business analytics.",
    "descriptor": "\nComments: Submitted for publication to International Journal of Information Management\n",
    "authors": [
      "Marc Schmitt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.10538"
  },
  {
    "id": "arXiv:2205.10539",
    "title": "On the Feasibility and Generality of Patch-based Adversarial Attacks on  Semantic Segmentation Problems",
    "abstract": "Deep neural networks were applied with success in a myriad of applications,\nbut in safety critical use cases adversarial attacks still pose a significant\nthreat. These attacks were demonstrated on various classification and detection\ntasks and are usually considered general in a sense that arbitrary network\noutputs can be generated by them.\nIn this paper we will demonstrate through simple case studies both in\nsimulation and in real-life, that patch based attacks can be utilised to alter\nthe output of segmentation networks. Through a few examples and the\ninvestigation of network complexity, we will also demonstrate that the number\nof possible output maps which can be generated via patch-based attacks of a\ngiven size is typically smaller than the area they effect or areas which should\nbe attacked in case of practical applications.\nWe will prove that based on these results most patch-based attacks cannot be\ngeneral in practice, namely they can not generate arbitrary output maps or if\nthey could, they are spatially limited and this limit is significantly smaller\nthan the receptive field of the patches.",
    "descriptor": "",
    "authors": [
      "Soma Kontar",
      "Andras Horvath"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10539"
  },
  {
    "id": "arXiv:2205.10546",
    "title": "Improvements to Self-Supervised Representation Learning for Masked Image  Modeling",
    "abstract": "This paper explores improvements to the masked image modeling (MIM) paradigm.\nThe MIM paradigm enables the model to learn the main object features of the\nimage by masking the input image and predicting the masked part by the unmasked\npart. We found the following three main directions for MIM to be improved.\nFirst, since both encoders and decoders contribute to representation learning,\nMIM uses only encoders for downstream tasks, which ignores the impact of\ndecoders on representation learning. Although the MIM paradigm already employs\nsmall decoders with asymmetric structures, we believe that continued reduction\nof decoder parameters is beneficial to improve the representational learning\ncapability of the encoder . Second, MIM solves the image prediction task by\ntraining the encoder and decoder together , and does not design a separate task\nfor the encoder . To further enhance the performance of the encoder when\nperforming downstream tasks, we designed the encoder for the tasks of\ncomparative learning and token position prediction. Third, since the input\nimage may contain background and other objects, and the proportion of each\nobject in the image varies, reconstructing the tokens related to the background\nor to other objects is not meaningful for MIM to understand the main object\nrepresentations. Therefore we use ContrastiveCrop to crop the input image so\nthat the input image contains as much as possible only the main objects. Based\non the above three improvements to MIM, we propose a new model, Contrastive\nMasked AutoEncoders (CMAE). We achieved a Top-1 accuracy of 65.84% on\ntinyimagenet using the ViT-B backbone, which is +2.89 outperforming the MAE of\ncompeting methods when all conditions are equal. Code will be made available.",
    "descriptor": "",
    "authors": [
      "Jiawei Mao",
      "Xuesong Yin",
      "Yuanqi Chang",
      "Honggu Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10546"
  },
  {
    "id": "arXiv:2205.10550",
    "title": "KGNN: Harnessing Kernel-based Networks for Semi-supervised Graph  Classification",
    "abstract": "This paper studies semi-supervised graph classification, which is an\nimportant problem with various applications in social network analysis and\nbioinformatics. This problem is typically solved by using graph neural networks\n(GNNs), which yet rely on a large number of labeled graphs for training and are\nunable to leverage unlabeled graphs. We address the limitations by proposing\nthe Kernel-based Graph Neural Network (KGNN). A KGNN consists of a GNN-based\nnetwork as well as a kernel-based network parameterized by a memory network.\nThe GNN-based network performs classification through learning graph\nrepresentations to implicitly capture the similarity between query graphs and\nlabeled graphs, while the kernel-based network uses graph kernels to explicitly\ncompare each query graph with all the labeled graphs stored in a memory for\nprediction. The two networks are motivated from complementary perspectives, and\nthus combing them allows KGNN to use labeled graphs more effectively. We\njointly train the two networks by maximizing their agreement on unlabeled\ngraphs via posterior regularization, so that the unlabeled graphs serve as a\nbridge to let both networks mutually enhance each other. Experiments on a range\nof well-known benchmark datasets demonstrate that KGNN achieves impressive\nperformance over competitive baselines.",
    "descriptor": "\nComments: Published as a full paper at WSDM 2022\n",
    "authors": [
      "Wei Ju",
      "Junwei Yang",
      "Meng Qu",
      "Weiping Song",
      "Jianhao Shen",
      "Ming Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.10550"
  },
  {
    "id": "arXiv:2205.10552",
    "title": "Smoothing Codes and Lattices: Systematic Study and New Bounds",
    "abstract": "In this article we revisit smoothing bounds in parallel between lattices\n$and$ codes. Initially introduced by Micciancio and Regev, these bounds were\ninstantiated with Gaussian distributions and were crucial for arguing the\nsecurity of many lattice-based cryptosystems. Unencumbered by direct\napplication concerns, we provide a systematic study of how these bounds are\nobtained for both lattices $and$ codes, transferring techniques between both\nareas. We also consider various spherically symmetric noise distributions.\nWe found that the best strategy for a worst-case bound combines Parseval's\nIdentity, the Cauchy-Schwarz inequality, and the second linear programming\nbound, and this for both codes and lattices, and for all noise distributions at\nhand. For an average-case analysis, the linear programming bound can be\nreplaced by a tight average count.\nThis alone gives optimal results for spherically uniform noise over random\ncodes and random lattices. This also improves previous Gaussian smoothing bound\nfor worst-case lattices, but surprisingly this provides even better results for\nuniform noise than for Gaussian (or Bernoulli noise for codes).\nThis counter-intuitive situation can be resolved by adequate decomposition\nand truncation of Gaussian and Bernoulli distribution into a superposition of\nuniform noise, giving further improvement for those cases, and putting them on\npar with the uniform cases.",
    "descriptor": "",
    "authors": [
      "Thomas Debris-Alazard",
      "L\u00e9o Ducas",
      "Nicolas Resch",
      "Jean-Pierre Tillich"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10552"
  },
  {
    "id": "arXiv:2205.10553",
    "title": "Robot Person Following in Uniform Crowd Environment",
    "abstract": "Person-tracking robots have many applications, such as in security, elderly\ncare, and socializing robots. Such a task is particularly challenging when the\nperson is moving in a Uniform crowd. Also, despite significant progress of\ntrackers reported in the literature, state-of-the-art trackers have hardly\naddressed person following in such scenarios. In this work, we focus on\nimproving the perceptivity of a robot for a person following task by developing\na robust and real-time applicable object tracker. We present a new robot person\ntracking system with a new RGB-D tracker, Deep Tracking with RGB-D (DTRD) that\nis resilient to tricky challenges introduced by the uniform crowd environment.\nOur tracker utilizes transformer encoder-decoder architecture with RGB and\ndepth information to discriminate the target person from similar distractors. A\nsubstantial amount of comprehensive experiments and results demonstrate that\nour tracker has higher performance in two quantitative evaluation metrics and\nconfirms its superiority over other SOTA trackers.",
    "descriptor": "",
    "authors": [
      "Adarsh Ghimire",
      "Xiaoxiong Zhang",
      "Sajid Javed",
      "Jorge Dias",
      "Naoufel Werghi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10553"
  },
  {
    "id": "arXiv:2205.10556",
    "title": "Cycle-GAN for eye-tracking",
    "abstract": "This manuscript presents a not typical implementation of the cycle generative\nadversarial networks (Cycle-GAN) method for eye-tracking tasks.",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Ildar Rakhmatulin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10556"
  },
  {
    "id": "arXiv:2205.10558",
    "title": "CORAL: Contextual Response Retrievability Loss Function for Training  Dialog Generation Models",
    "abstract": "Natural Language Generation (NLG) represents a large collection of tasks in\nthe field of NLP. While many of these tasks have been tackled well by the\ncross-entropy (CE) loss, the task of dialog generation poses a few unique\nchallenges for this loss function. First, CE loss assumes that for any given\ninput, the only possible output is the one available as the ground truth in the\ntraining dataset. In general, this is not true for any task, as there can be\nmultiple semantically equivalent sentences, each with a different surface form.\nThis problem gets exaggerated further for the dialog generation task, as there\ncan be multiple valid responses (for a given context) that not only have\ndifferent surface forms but are also not semantically equivalent. Second, CE\nloss does not take the context into consideration while processing the response\nand, hence, it treats all ground truths with equal importance irrespective of\nthe context. But, we may want our final agent to avoid certain classes of\nresponses (e.g. bland, non-informative or biased responses) and give relatively\nhigher weightage for more context-specific responses. To circumvent these\nshortcomings of the CE loss, in this paper, we propose a novel loss function,\nCORAL, that directly optimizes recently proposed estimates of human preference\nfor generated responses. Using CORAL, we can train dialog generation models\nwithout assuming non-existence of response other than the ground-truth. Also,\nthe CORAL loss is computed based on both the context and the response.\nExtensive comparisons on two benchmark datasets show that the proposed methods\noutperform strong state-of-the-art baseline models of different sizes.",
    "descriptor": "\nComments: 15 pages, 3 figures\n",
    "authors": [
      "Bishal Santra",
      "Ravi Ghadia",
      "Arpit Dwivedi",
      "Manish Gupta",
      "Pawan Goyal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10558"
  },
  {
    "id": "arXiv:2205.10560",
    "title": "Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation",
    "abstract": "Traditionally, sign language resources have been collected in controlled\nsettings for specific tasks involving supervised sign classification or\nlinguistic studies accompanied by specific annotation type. To date, very few\nwho explored signing videos found online on social media platforms as well as\nthe use of unsupervised methods applied to such resources. Due to the fact that\nthe field is striving to achieve acceptable model performance on the data that\ndiffers from that seen during training calls for more diversity in sign\nlanguage data, stepping away from the data obtained in controlled laboratory\nsettings. Moreover, since the sign language data collection and annotation\ncarries large overheads, it is desirable to accelerate the annotation process.\nConsidering the aforementioned tendencies, this paper takes the side of\nharvesting online data in a pursuit for automatically generating and annotating\nsign language corpora through phoneme clustering.",
    "descriptor": "\nComments: 11 pages, 10 figures, VarDial2020\n",
    "authors": [
      "Boris Mocialov",
      "Graham Turner",
      "Helen Hastie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10560"
  },
  {
    "id": "arXiv:2205.10564",
    "title": "Shared-Control Robotic Manipulation in Virtual Reality",
    "abstract": "In this paper, we present the implementation details of a Virtual Reality\n(VR)-based teleoperation interface for moving a robotic manipulator. We propose\nan iterative human-in-the-loop design where the user sets the next task-space\nwaypoint for the robot's end effector and executes the action on the physical\nrobot before setting the next waypoints. Information from the robot's\nsurroundings is provided to the user in two forms: as a point cloud in 3D space\nand a video stream projected on a virtual wall. The feasibility of the selected\nend effector pose is communicated to the user by the color of the virtual end\neffector. The interface is demonstrated to successfully work for a pick and\nplace scenario, however, our trials showed that the fluency of the interaction\nand the autonomy level of the system can be increased.",
    "descriptor": "\nComments: 6 pages, 8 figures, submitted to HORA 2022\n",
    "authors": [
      "Shiyu Xu",
      "Scott Moore",
      "Akansel Cosgun"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10564"
  },
  {
    "id": "arXiv:2205.10568",
    "title": "Secure and Efficient Decentralized Federated Learning with Data  Representation Protection",
    "abstract": "Federated learning (FL) is a promising technical support to the vision of\nubiquitous artificial intelligence in the sixth generation (6G) wireless\ncommunication network. However, traditional FL heavily relies on a trusted\ncentralized server. Besides, FL is vulnerable to poisoning attacks, and the\nglobal aggregation of model updates makes the private training data under the\nrisk of being reconstructed. What's more, FL suffers from efficiency problem\ndue to heavy communication cost. Although decentralized FL eliminates the\nproblem of the central dependence of traditional FL, it makes other problems\nmore serious. In this paper, we propose BlockDFL, an efficient fully\npeer-to-peer (P2P) framework for decentralized FL. It integrates gradient\ncompression and our designed voting mechanism with blockchain to efficiently\ncoordinate multiple peer participants without mutual trust to carry out\ndecentralized FL, while preventing data from being reconstructed according to\ntransmitted model updates. Extensive experiments conducted on two real-world\ndatasets exhibit that BlockDFL obtains competitive accuracy compared to\ncentralized FL and can defend against poisoning attacks while achieving\nefficiency and scalability. Especially when the proportion of malicious\nparticipants is as high as 40 percent, BlockDFL can still preserve the accuracy\nof FL, which outperforms existing fully decentralized FL frameworks.",
    "descriptor": "",
    "authors": [
      "Zhen Qin",
      "Shuiguang Deng",
      "Xueqiang Yan",
      "Schahram Dustdar",
      "Albert Y. Zomaya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10568"
  },
  {
    "id": "arXiv:2205.10569",
    "title": "HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware  Transformer Reranking",
    "abstract": "Deep pre-trained language models (e,g. BERT) are effective at large-scale\ntext retrieval task. Existing text retrieval systems with state-of-the-art\nperformance usually adopt a retrieve-then-reranking architecture due to the\nhigh computational cost of pre-trained language models and the large corpus\nsize. Under such a multi-stage architecture, previous studies mainly focused on\noptimizing single stage of the framework thus improving the overall retrieval\nperformance. However, how to directly couple multi-stage features for\noptimization has not been well studied. In this paper, we design Hybrid List\nAware Transformer Reranking (HLATR) as a subsequent reranking module to\nincorporate both retrieval and reranking stage features. HLATR is lightweight\nand can be easily parallelized with existing text retrieval systems so that the\nreranking process can be performed in a single yet efficient processing.\nEmpirical experiments on two large-scale text retrieval datasets show that\nHLATR can efficiently improve the ranking performance of existing multi-stage\ntext retrieval methods.",
    "descriptor": "\nComments: Work in progress. HLAR part of the \"AliceMind SLM + HLAR\" method in MS MARCO Passage Ranking Submission\n",
    "authors": [
      "Yanzhao Zhang",
      "Dingkun Long",
      "Guangwei Xu",
      "Pengjun Xie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10569"
  },
  {
    "id": "arXiv:2205.10570",
    "title": "Synchronous Consensus During Incomplete Synchrony",
    "abstract": "We present an algorithm for synchronous deterministic Byzantine consensus,\ntolerant to links failures and links asynchrony. It cares for a class of\nnetworks with specific needs, where both safety and liveness are essential, and\ntimely irrevocable consensus has priority over highest throughput. The\nalgorithm operates with redundant delivery of messages via indirect paths of up\nto 3 hops, aims all correct processes to obtain a coherent view of the system\nstate within a bounded time, and establishes consensus with no need of leader.\nConsensus involves exchange of 2*n*n*n asymmetrically authenticated messages\nand tolerates up to < n/2 faulty processes. We show that in a consensus system\nwith known members: 1) The existing concepts for delivery over a fraction of\nlinks and gossip-based reliable multicast can be extended to also circumvent\nasynchronous links and thereby convert the reliable delivery into a reliable\nbounded delivery. 2) A system of synchronous processes with bounded delivery\ndoes not need a leader - all correct processes from connected majority derive\nand propose the same consensus value from atomically consistent individual\nviews on system state. 3) The required for bounded delivery asymmetric\nauthentication of messages is sufficient for safety of the consensus algorithm.\nKey finding: the impossibility of safety and liveness of consensus in partial\nsynchrony is not valid in the entire space between synchrony and asynchrony. A\nsystem of synchronized synchronous processes, which communicate with\nasymmetrically authenticated messages over a medium susceptible to asynchrony\nand faults, can operate with: 1) defined tolerance to number of asynchronous\nand/or faulty links per number of stop-failed and/or Byzantine processes; 2)\nleaderless algorithm with bounded termination; and 3) conceptually ensured\nsimultaneous safety and bounded liveness.",
    "descriptor": "",
    "authors": [
      "Ivan Klianev"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10570"
  },
  {
    "id": "arXiv:2205.10571",
    "title": "ADT-SSL: Adaptive Dual-Threshold for Semi-Supervised Learning",
    "abstract": "Semi-Supervised Learning (SSL) has advanced classification tasks by inputting\nboth labeled and unlabeled data to train a model jointly. However, existing SSL\nmethods only consider the unlabeled data whose predictions are beyond a fixed\nthreshold (e.g., 0.95), ignoring the valuable information from those less than\n0.95. We argue that these discarded data have a large proportion and are\nusually of hard samples, thereby benefiting the model training. This paper\nproposes an Adaptive Dual-Threshold method for Semi-Supervised Learning\n(ADT-SSL). Except for the fixed threshold, ADT extracts another class-adaptive\nthreshold from the labeled data to take full advantage of the unlabeled data\nwhose predictions are less than 0.95 but more than the extracted one.\nAccordingly, we engage CE and $L_2$ loss functions to learn from these two\ntypes of unlabeled data, respectively. For highly similar unlabeled data, we\nfurther design a novel similar loss to make the prediction of the model\nconsistency. Extensive experiments are conducted on benchmark datasets,\nincluding CIFAR-10, CIFAR-100, and SVHN. Experimental results show that the\nproposed ADT-SSL achieves state-of-the-art classification accuracy.",
    "descriptor": "",
    "authors": [
      "Zechen Liang",
      "Yuan-Gen Wang",
      "Wei Lu",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10571"
  },
  {
    "id": "arXiv:2205.10573",
    "title": "Spectral Neural Operators",
    "abstract": "A plentitude of applications in scientific computing requires the\napproximation of mappings between Banach spaces. Recently introduced Fourier\nNeural Operator (FNO) and Deep Operator Network (DeepONet) can provide this\nfunctionality. For both of these neural operators, the input function is\nsampled on a given grid (uniform for FNO), and the output function is\nparametrized by a neural network. We argue that this parametrization leads to\n1) opaque output that is hard to analyze and 2) systematic bias caused by\naliasing errors in the case of FNO. The alternative, advocated in this article,\nis to use Chebyshev and Fourier series for both domain and codomain. The\nresulting Spectral Neural Operator (SNO) has transparent output, never suffers\nfrom aliasing, and may include many exact (lossless) operations on functions.\nThe functionality is based on well-developed fast, and stable algorithms from\nspectral methods. The implementation requires only standard numerical linear\nalgebra. Our benchmarks show that for many operators, SNO is superior to FNO\nand DeepONet.",
    "descriptor": "",
    "authors": [
      "V. Fanaskov",
      "I. Oseledets"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10573"
  },
  {
    "id": "arXiv:2205.10574",
    "title": "A graphical representation of binary linear codes",
    "abstract": "A binary $[n,k]$-linear code $\\mathcal{C}$ is a $k$-dimensional subspace of\n$\\mathbb{F}_2^n$. For $\\boldsymbol{x}\\in \\mathbb{F}_2^n$, the set\n$\\boldsymbol{x}+\\mathcal{C}$ is a coset of $\\mathcal{C}$. In this work we study\na partial ordering on the set of cosets of a binary linear code $\\mathcal{C}$\nof length $n$ and we construct a graph using the orphan structure of this code.",
    "descriptor": "",
    "authors": [
      "Lisbeth Danyeli Delgado Ordo\u00f1ez",
      "John H. Castillo",
      "Alexander Holgu\u00edn-Villa"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10574"
  },
  {
    "id": "arXiv:2205.10575",
    "title": "UVA Resources for the Biomedical Vocabulary Alignment at Scale in the  UMLS Metathesaurus",
    "abstract": "The construction and maintenance process of the UMLS (Unified Medical\nLanguage System) Metathesaurus is time-consuming, costly, and error-prone as it\nrelies on (1) the lexical and semantic processing for suggesting synonymous\nterms, and (2) the expertise of UMLS editors for curating the suggestions. For\nimproving the UMLS Metathesaurus construction process, our research group has\ndefined a new task called UVA (UMLS Vocabulary Alignment) and generated a\ndataset for evaluating the task. Our group has also developed different\nbaselines for this task using logical rules (RBA), and neural networks (LexLM\nand ConLM).\nIn this paper, we present a set of reusable and reproducible resources\nincluding (1) a dataset generator, (2) three datasets generated by using the\ngenerator, and (3) three baseline approaches. We describe the UVA dataset\ngenerator and its implementation generalized for any given UMLS release. We\ndemonstrate the use of the dataset generator by generating datasets\ncorresponding to three UMLS releases, 2020AA, 2021AA, and 2021AB. We provide\nthree UVA baselines using the three existing approaches (LexLM, ConLM, and\nRBA). The code, the datasets, and the experiments are publicly available,\nreusable, and reproducible with any UMLS release (a no-cost license agreement\nis required for downloading the UMLS).",
    "descriptor": "",
    "authors": [
      "Vinh Nguyen",
      "Olivier Bodenreider"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10575"
  },
  {
    "id": "arXiv:2205.10577",
    "title": "Non-Autoregressive Neural Machine Translation: A Call for Clarity",
    "abstract": "Non-autoregressive approaches aim to improve the inference speed of\ntranslation models by only requiring a single forward pass to generate the\noutput sequence instead of iteratively producing each predicted token.\nConsequently, their translation quality still tends to be inferior to their\nautoregressive counterparts due to several issues involving output token\ninterdependence. In this work, we take a step back and revisit several\ntechniques that have been proposed for improving non-autoregressive translation\nmodels and compare their combined translation quality and speed implications\nunder third-party testing environments. We provide novel insights for\nestablishing strong baselines using length prediction or CTC-based architecture\nvariants and contribute standardized BLEU, chrF++, and TER scores using\nsacreBLEU on four translation tasks, which crucially have been missing as\ninconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7\nBLEU points. Our open-sourced code is integrated into fairseq for\nreproducibility.",
    "descriptor": "",
    "authors": [
      "Robin M. Schmidt",
      "Telmo Pires",
      "Stephan Peitz",
      "Jonas L\u00f6\u00f6f"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10577"
  },
  {
    "id": "arXiv:2205.10578",
    "title": "Multi-feature Co-learning for Image Inpainting",
    "abstract": "Image inpainting has achieved great advances by simultaneously leveraging\nimage structure and texture features. However, due to lack of effective\nmulti-feature fusion techniques, existing image inpainting methods still show\nlimited improvement. In this paper, we design a deep multi-feature co-learning\nnetwork for image inpainting, which includes Soft-gating Dual Feature Fusion\n(SDFF) and Bilateral Propagation Feature Aggregation (BPFA) modules. To be\nspecific, we first use two branches to learn structure features and texture\nfeatures separately. Then the proposed SDFF module integrates structure\nfeatures into texture features, and meanwhile uses texture features as an\nauxiliary in generating structure features. Such a co-learning strategy makes\nthe structure and texture features more consistent. Next, the proposed BPFA\nmodule enhances the connection from local feature to overall consistency by\nco-learning contextual attention, channel-wise information and feature space,\nwhich can further refine the generated structures and textures. Finally,\nextensive experiments are performed on benchmark datasets, including CelebA,\nPlaces2, and Paris StreetView. Experimental results demonstrate the superiority\nof the proposed method over the state-of-the-art. The source codes are\navailable at https://github.com/GZHU-DVL/MFCL-Inpainting.",
    "descriptor": "",
    "authors": [
      "Jiayu Lin",
      "Yuan-Gen Wang",
      "Wenzhi Tang",
      "Aifeng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10578"
  },
  {
    "id": "arXiv:2205.10579",
    "title": "Boosting Camouflaged Object Detection with Dual-Task Interactive  Transformer",
    "abstract": "Camouflaged object detection intends to discover the concealed objects hidden\nin the surroundings. Existing methods follow the bio-inspired framework, which\nfirst locates the object and second refines the boundary. We argue that the\ndiscovery of camouflaged objects depends on the recurrent search for the object\nand the boundary. The recurrent processing makes the human tired and helpless,\nbut it is just the advantage of the transformer with global search ability.\nTherefore, a dual-task interactive transformer is proposed to detect both\naccurate position of the camouflaged object and its detailed boundary. The\nboundary feature is considered as Query to improve the camouflaged object\ndetection, and meanwhile the object feature is considered as Query to improve\nthe boundary detection. The camouflaged object detection and the boundary\ndetection are fully interacted by multi-head self-attention. Besides, to obtain\nthe initial object feature and boundary feature, transformer-based backbones\nare adopted to extract the foreground and background. The foreground is just\nobject, while foreground minus background is considered as boundary. Here, the\nboundary feature can be obtained from blurry boundary region of the foreground\nand background. Supervised by the object, the background and the boundary\nground truth, the proposed model achieves state-of-the-art performance in\npublic datasets. https://github.com/liuzywen/COD",
    "descriptor": "\nComments: Accepted by ICPR2022\n",
    "authors": [
      "Zhengyi Liu",
      "Zhili Zhang",
      "Wei Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10579"
  },
  {
    "id": "arXiv:2205.10580",
    "title": "Towards Secure Virtual Elections: Multiparty Computation of Order Based  Voting Rules",
    "abstract": "Electronic voting systems are essential for holding virtual elections, and\nthe need for such systems increases due to the COVID-19 pandemic and the social\ndistancing that it mandates. One of the main challenges in e-voting systems is\nto secure the voting process: namely, to certify that the computed results are\nconsistent with the cast ballots, and that the privacy of the voters is\npreserved. We propose herein a secure voting protocol for elections that are\ngoverned by order-based voting rules. Our protocol offers perfect ballot\nsecrecy, in the sense that it issues only the required output, while no other\ninformation on the cast ballots is revealed. Such perfect secrecy, which is\nachieved by employing secure multiparty computation tools, may increase the\nvoters' confidence and, consequently, encourage them to vote according to their\ntrue preferences. Evaluation of the protocol's computational costs establishes\nthat it is lightweight and can be readily implemented in real-life electronic\nelections.",
    "descriptor": "",
    "authors": [
      "Tamir Tassa",
      "Lihi Dery"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10580"
  },
  {
    "id": "arXiv:2205.10581",
    "title": "Evaluating Performance of Machine Learning Models for Diabetic  Sensorimotor Polyneuropathy Severity Classification using Biomechanical  Signals during Gait",
    "abstract": "Diabetic sensorimotor polyneuropathy (DSPN) is one of the prevalent forms of\nneuropathy affected by diabetic patients that involves alterations in\nbiomechanical changes in human gait. In literature, for the last 50 years,\nresearchers are trying to observe the biomechanical changes due to DSPN by\nstudying muscle electromyography (EMG), and ground reaction forces (GRF).\nHowever, the literature is contradictory. In such a scenario, we are proposing\nto use Machine learning techniques to identify DSPN patients by using EMG, and\nGRF data. We have collected a dataset consists of three lower limb muscles EMG\n(tibialis anterior (TA), vastus lateralis (VL), gastrocnemius medialis (GM) and\n3-dimensional GRF components (GRFx, GRFy, and GRFz). Raw EMG and GRF signals\nwere preprocessed, and a newly proposed feature extraction technique scheme\nfrom literature was applied to extract the best features from the signals. The\nextracted feature list was ranked using Relief feature ranking techniques, and\nhighly correlated features were removed. We have trained different ML models to\nfind out the best-performing model and optimized that model. We trained the\noptimized ML models for different combinations of muscles and GRF components\nfeatures, and the performance matrix was evaluated. This study has found\nensemble classifier model was performing in identifying DSPN Severity, and we\noptimized it before training. For EMG analysis, we have found the best accuracy\nof 92.89% using the Top 14 features for features from GL, VL and TA muscles\ncombined. In the GRF analysis, the model showed 94.78% accuracy by using the\nTop 15 features for the feature combinations extracted from GRFx, GRFy and GRFz\nsignals. The performance of ML-based DSPN severity classification models,\nimproved significantly, indicating their reliability in DSPN severity\nclassification, for biomechanical data.",
    "descriptor": "\nComments: 17 pages, 15 figures, 8 tables\n",
    "authors": [
      "Fahmida Haque",
      "Mamun Bin Ibne Reaz",
      "Muhammad Enamul Hoque Chowdhury",
      "Serkan Kiranyaz",
      "Mohamed Abdelmoniem",
      "Emadeddin Hussein",
      "Mohammed Shaat",
      "Sawal Hamid Md Ali",
      "Ahmad Ashrif A Bakar",
      "Geetika Srivastava",
      "Mohammad Arif Sobhan Bhuiyan",
      "Mohd Hadri Hafiz Mokhtar",
      "Edi Kurniawan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10581"
  },
  {
    "id": "arXiv:2205.10583",
    "title": "Improving automatically generated code from Codex via Automated Program  Repair",
    "abstract": "Large language models, e.g., Codex and AlphaCode, have shown capability in\nproducing working code for many programming tasks. However, the success rate of\nexisting models remains low, especially for complex programming tasks. One of\nthe reasons is that language models lack awareness of program semantics (e.g.,\ntype information), resulting in incorrect programs (or even programs which do\nnot compile). In this paper, we systematically study whether automated program\nrepair (APR) techniques can fix the incorrect solutions produced by language\nmodels in LeetCode contests. The goal is to study whether APR techniques can\nenhance confidence in the code produced by language models. Our study revealed\nthat: (1) automatically generated codes share some common programming mistakes\nwith human-crafted solutions, indicating existing APR tools have the potential\nto fix auto-generated code; (2) TBar and Recoder, two well-known Java APR tools\nbased on templates and learning respectively, increase the number of solved\ntasks from 37 to 42 on 60 easy level tasks, while increase from 5 to 9 on 53\nmedium-level programming tasks; (3) given bug location information provided by\na statistical fault localization approach, the newly released Codex edit mode,\nwhich supports changing existing code, may outperform existing APR tools in\nfixing incorrect solutions. By analyzing the experimental results generated by\nthese tools, we provide several suggestions on how to improve current APR\ntools.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Zhiyu Fan",
      "Xiang Gao",
      "Abhik Roychoudhury",
      "Shin Hwei Tan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10583"
  },
  {
    "id": "arXiv:2205.10586",
    "title": "Calibration of Natural Language Understanding Models with Venn--ABERS  Predictors",
    "abstract": "Transformers, currently the state-of-the-art in natural language\nunderstanding (NLU) tasks, are prone to generate uncalibrated predictions or\nextreme probabilities, making the process of taking different decisions based\non their output relatively difficult. In this paper we propose to build several\ninductive Venn--ABERS predictors (IVAP), which are guaranteed to be well\ncalibrated under minimal assumptions, based on a selection of pre-trained\ntransformers. We test their performance over a set of diverse NLU tasks and\nshow that they are capable of producing well-calibrated probabilistic\npredictions that are uniformly spread over the [0,1] interval -- all while\nretaining the original model's predictive accuracy.",
    "descriptor": "\nComments: Submitted to the 11th Symposium on Conformal and Probabilistic Prediction with Applications - COPA 2022\n",
    "authors": [
      "Patrizio Giovannotti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10586"
  },
  {
    "id": "arXiv:2205.10587",
    "title": "A comprehensive survey on semantic facial attribute editing using  generative adversarial networks",
    "abstract": "Generating random photo-realistic images has experienced tremendous growth\nduring the past few years due to the advances of the deep convolutional neural\nnetworks and generative models. Among different domains, face photos have\nreceived a great deal of attention and a large number of face generation and\nmanipulation models have been proposed. Semantic facial attribute editing is\nthe process of varying the values of one or more attributes of a face image\nwhile the other attributes of the image are not affected. The requested\nmodifications are provided as an attribute vector or in the form of driving\nface image and the whole process is performed by the corresponding models. In\nthis paper, we survey the recent works and advances in semantic facial\nattribute editing. We cover all related aspects of these models including the\nrelated definitions and concepts, architectures, loss functions, datasets,\nevaluation metrics, and applications. Based on their architectures, the\nstate-of-the-art models are categorized and studied as encoder-decoder,\nimage-to-image, and photo-guided models. The challenges and restrictions of the\ncurrent state-of-the-art methods are discussed as well.",
    "descriptor": "",
    "authors": [
      "Ahmad Nickabadi",
      "Maryam Saeedi Fard",
      "Nastaran Moradzadeh Farid",
      "Najmeh Mohammadbagheri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10587"
  },
  {
    "id": "arXiv:2205.10588",
    "title": "Micro-video recommendation model based on graph neural network and  attention mechanism",
    "abstract": "With the rapid development of Internet technology and the comprehensive\npopularity of Internet applications, online activities have gradually become an\nindispensable part of people's daily life. The original recommendation learning\nalgorithm is mainly based on user-microvideo interaction for learning, modeling\nthe user-micro-video connection relationship, which is difficult to capture the\nmore complex relationships between nodes. To address the above problems, we\npropose a personalized recommendation model based on graph neural network,\nwhich utilizes the feature that graph neural network can tap deep information\nof graph data more effectively, and transforms the input user rating\ninformation and item side information into graph structure, for effective\nfeature extraction, based on the importance sampling strategy. The\nimportance-based sampling strategy measures the importance of neighbor nodes to\nthe central node by calculating the relationship tightness between the neighbor\nnodes and the central node, and selects the neighbor nodes for recommendation\ntasks based on the importance level, which can be more targeted to select the\nsampling neighbors with more influence on the target micro-video nodes. The\npooling aggregation strategy, on the other hand, trains the aggregation weights\nby inputting the neighborhood node features into the fully connected layer\nbefore aggregating the neighborhood features, and then introduces the pooling\nlayer for feature aggregation, and finally aggregates the obtained neighborhood\naggregation features with the target node itself, which directly introduces a\nsymmetric trainable function to fuse the neighborhood weight training into the\nmodel to better capture the different neighborhood nodes' differential features\nin a learnable manner to allow for a more accurate representation of the\ncurrent node features.",
    "descriptor": "",
    "authors": [
      "Chan Ching Ting",
      "Mathew Bowles",
      "Ibrahim Idewu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.10588"
  },
  {
    "id": "arXiv:2205.10591",
    "title": "Multiplierless Design of Very Large Constant Multiplications in  Cryptography",
    "abstract": "This brief addresses the problem of implementing very large constant\nmultiplications by a single variable under the shift-adds architecture using a\nminimum number of adders/subtractors. Due to the intrinsic complexity of the\nproblem, we introduce an approximate algorithm, called T\\~OLL, which partitions\nthe very large constants into smaller ones. To reduce the number of operations,\nT\\~OLL incorporates graph-based and common subexpression elimination methods\nproposed for the shift-adds design of constant multiplications. It can also\nconsider the delay of a multiplierless design defined in terms of the maximum\nnumber of operations in series, i.e., the number of adder-steps, while reducing\nthe number of operations. High-level experimental results show that the\nadder-steps of a shift-adds design can be reduced significantly with a little\noverhead in the number of operations. Gate-level experimental results indicate\nthat while the shift-adds design can lead to a 36.6\\% reduction in gate-level\narea with respect to a design using a multiplier, the delay-aware optimization\ncan yield a 48.3\\% reduction in minimum achievable delay of the shift-adds\ndesign when compared to the area-aware optimization.",
    "descriptor": "",
    "authors": [
      "Levent Aksoy",
      "Debapriya Basu Roy",
      "Malik Imran",
      "Patrick Karl",
      "Samuel Pagliarini"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10591"
  },
  {
    "id": "arXiv:2205.10592",
    "title": "Facing the Void: Overcoming Missing Data in Multi-View Imagery",
    "abstract": "In some scenarios, a single input image may not be enough to allow the object\nclassification. In those cases, it is crucial to explore the complementary\ninformation extracted from images presenting the same object from multiple\nperspectives (or views) in order to enhance the general scene understanding\nand, consequently, increase the performance. However, this task, commonly\ncalled multi-view image classification, has a major challenge: missing data. In\nthis paper, we propose a novel technique for multi-view image classification\nrobust to this problem. The proposed method, based on state-of-the-art deep\nlearning-based approaches and metric learning, can be easily adapted and\nexploited in other applications and domains. A systematic evaluation of the\nproposed algorithm was conducted using two multi-view aerial-ground datasets\nwith very distinct properties. Results show that the proposed algorithm\nprovides improvements in multi-view image classification accuracy when compared\nto state-of-the-art methods. Code available at\n\\url{https://github.com/Gabriellm2003/remote_sensing_missing_data}.",
    "descriptor": "",
    "authors": [
      "Gabriel Machado",
      "Keiller Nogueira",
      "Matheus Barros Pereira",
      "Jefersson Alex dos Santos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10592"
  },
  {
    "id": "arXiv:2205.10593",
    "title": "Few-Shot Natural Language Inference Generation with PDD: Prompt and  Dynamic Demonstration",
    "abstract": "Natural Language Inference Generation task is to generate a text hypothesis\ngiven a text premise and a logical relation between the two. This task can be\nused in data augmentation and controllable text generation in practice. In this\npaper, we propose language models with prompt and dynamic demonstration\n(LM-PDD) to tackle this problem in few-shot settings. Our framework outperforms\nstandard fine-tuned models with low resource, achieving an average 8% absolute\nimprovement on SNLI and MNLI datasets, and the results on 13 natural language\nclassification tasks also show that our dynamic demonstration method has good\ngeneralizability.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Kaijian Li",
      "Shansan Gong",
      "Kenny Q. Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10593"
  },
  {
    "id": "arXiv:2205.10599",
    "title": "Networks of international football: communities, evolution and  globalization of the game",
    "abstract": "As the most popular sport around the globe, the game of football has recently\nintrigued much research interest to explore and distill useful and appealing\ninformation from the sport. Network science and graph-centric methods have been\npreviously applied to study the importance of football players and teams. In\nthis paper, for the first time we study the macroscopic evolution of the\nfootball society from a complex network point of view. Football game records\nwithin a time window of over a century were collected and expressed in a graph\nformat, where participant teams are represented by graph nodes and the games\nbetween them are the graph edges. We carry out community detection and temporal\nanalysis to reveal the dynamic features and the community structures embedded\nwithin the football network, offering the evidence of a continuously expanding\nfootball society. Spatio-temporal analysis is also implemented to unveil the\ntemporal states that represent distinct development stages in the football\nhistory. Our analysis suggests that the evolution of the game receives\nconsiderable impact not only from major sport events, but also from multiple\nsocial and political incidents. The game of football and its evolution reflect\nsignificant historical transitions and turning points, and can provide a novel\nperspective for the study of the worldwide globalization process.",
    "descriptor": "",
    "authors": [
      "Yang Li",
      "Gonzalo Mateos"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10599"
  },
  {
    "id": "arXiv:2205.10606",
    "title": "Unitarity of some barycentric rational approximants",
    "abstract": "The exponential function maps the imaginary axis to the unit circle and, for\nmany applications, this unitarity property is also desirable from its\napproximations. We show that this property is conserved not only by the\n(k,k)-rational barycentric interpolant of the exponential on the imaginary\naxis, but also by (k,k)-rational barycentric approximants that minimize a\nlinearized approximation error. These results are a consequence of certain\nproperties of singular vectors of Loewner-type matrices associated to\nlinearized approximation errors. Prominent representatives of this class are\nrational approximants computed by the adaptive Antoulas--Anderson (AAA) method\nand the AAA--Lawson method. Our results also lead to a modified procedure with\nimproved numerical stability of the unitarity property and reduced\ncomputational cost.",
    "descriptor": "\nComments: 23 pages, 2 figures\n",
    "authors": [
      "Tobias Jawecki",
      "Pranav Singh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10606"
  },
  {
    "id": "arXiv:2205.10607",
    "title": "Coordinating Policies Among Multiple Agents via an Intelligent  Communication Channel",
    "abstract": "In Multi-Agent Reinforcement Learning (MARL), specialized channels are often\nintroduced that allow agents to communicate directly with one another. In this\npaper, we propose an alternative approach whereby agents communicate through an\nintelligent facilitator that learns to sift through and interpret signals\nprovided by all agents to improve the agents' collective performance. To ensure\nthat this facilitator does not become a centralized controller, agents are\nincentivized to reduce their dependence on the messages it conveys, and the\nmessages can only influence the selection of a policy from a fixed set, not\ninstantaneous actions given the policy. We demonstrate the strength of this\narchitecture over existing baselines on several cooperative MARL environments.",
    "descriptor": "",
    "authors": [
      "Dianbo Liu",
      "Vedant Shah",
      "Oussama Boussif",
      "Cristian Meo",
      "Anirudh Goyal",
      "Tianmin Shu",
      "Michael Mozer",
      "Nicolas Heess",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10607"
  },
  {
    "id": "arXiv:2205.10608",
    "title": "SERVFAIL: The Unintended Consequences of Algorithm Agility in DNSSEC",
    "abstract": "Cryptographic algorithm agility is an important property for DNSSEC: it\nallows easy deployment of new algorithms if the existing ones are no longer\nsecure. Significant operational and research efforts are dedicated to pushing\nthe deployment of new algorithms in DNSSEC forward. Recent research shows that\nDNSSEC is gradually achieving algorithm agility: most DNSSEC supporting\nresolvers can validate a number of different algorithms and domains are\nincreasingly signed with cryptographically strong ciphers.\nIn this work we show for the first time that the cryptographic agility in\nDNSSEC, although critical for making DNS secure with strong cryptography, also\nintroduces a severe vulnerability. We find that under certain conditions, when\nnew algorithms are listed in signed DNS responses, the resolvers do not\nvalidate DNSSEC. As a result, domains that deploy new ciphers, risk exposing\nthe validating resolvers to cache poisoning attacks.\nWe use this to develop DNSSEC-downgrade attacks and show that in some\nsituations these attacks can be launched even by off-path adversaries. We\nexperimentally and ethically evaluate our attacks against popular DNS resolver\nimplementations, public DNS providers, and DNS services used by web clients\nworldwide. We validate the success of DNSSEC-downgrade attacks by poisoning the\nresolvers: we inject fake records, in signed domains, into the caches of\nvalidating resolvers. We find that major DNS providers, such as Google Public\nDNS and Cloudflare, as well as 70% of DNS resolvers used by web clients are\nvulnerable to our attacks.\nWe trace the factors that led to this situation and provide recommendations.",
    "descriptor": "",
    "authors": [
      "Elias Heftrig",
      "Jean-Pierre Seifert",
      "Haya Shulman",
      "Peter Thomassen",
      "Michael Waidner",
      "Nils Wisiol"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10608"
  },
  {
    "id": "arXiv:2205.10611",
    "title": "Lightweight Human Pose Estimation Using Heatmap-Weighting Loss",
    "abstract": "Recent research on human pose estimation exploits complex structures to\nimprove performance on benchmark datasets, ignoring the resource overhead and\ninference speed when the model is actually deployed. In this paper, we lighten\nthe computation cost and parameters of the deconvolution head network in\nSimpleBaseline and introduce an attention mechanism that utilizes original,\ninter-level, and intra-level information to intensify the accuracy.\nAdditionally, we propose a novel loss function called heatmap weighting loss,\nwhich generates weights for each pixel on the heatmap that makes the model more\nfocused on keypoints. Experiments demonstrate our method achieves a balance\nbetween performance, resource volume, and inference speed. Specifically, our\nmethod can achieve 65.3 AP score on COCO test-dev, while the inference speed is\n55 FPS and 18 FPS on the mobile GPU and CPU, respectively.",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Shiqi Li",
      "Xiang Xiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.10611"
  },
  {
    "id": "arXiv:2205.10614",
    "title": "A New Metric and Its Scheme Construction for Evolving $2$-Threshold  Secret Sharing Schemes",
    "abstract": "Evolving secret sharing schemes do not require prior knowledge of the number\nof parties $n$ and $n$ may be infinitely countable. It is known that the\nevolving $2$-threshold secret sharing scheme and prefix coding of integers have\na one-to-one correspondence. However, it is not known what prefix coding of\nintegers to use to construct the scheme better. In this paper, we propose a new\nmetric $K_{\\Sigma}$ for evolving $2$-threshold secret sharing schemes $\\Sigma$.\nWe prove that the metric $K_{\\Sigma}\\geq 1.5$ and construct a new prefix coding\nof integers, termed $\\lambda$ code, to achieve the metric\n$K_{\\Lambda}=1.59375$. Thus, it is proved that the range of the metric\n$K_{\\Sigma}$ for the optimal $(2,\\infty)$-threshold secret sharing scheme is\n$1.5\\leq K_{\\Sigma}\\leq1.59375$. In addition, the reachable lower bound of the\nsum of share sizes for $(2,n)$-threshold secret sharing schemes is proved.",
    "descriptor": "",
    "authors": [
      "Wei Yan",
      "Sian-Jheng Lin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10614"
  },
  {
    "id": "arXiv:2205.10617",
    "title": "Gradient Concealment: Free Lunch for Defending Adversarial Attacks",
    "abstract": "Recent studies show that the deep neural networks (DNNs) have achieved great\nsuccess in various tasks. However, even the \\emph{state-of-the-art} deep\nlearning based classifiers are extremely vulnerable to adversarial examples,\nresulting in sharp decay of discrimination accuracy in the presence of enormous\nunknown attacks. Given the fact that neural networks are widely used in the\nopen world scenario which can be safety-critical situations, mitigating the\nadversarial effects of deep learning methods has become an urgent need.\nGenerally, conventional DNNs can be attacked with a dramatically high success\nrate since their gradient is exposed thoroughly in the white-box scenario,\nmaking it effortless to ruin a well trained classifier with only imperceptible\nperturbations in the raw data space. For tackling this problem, we propose a\nplug-and-play layer that is training-free, termed as \\textbf{G}radient\n\\textbf{C}oncealment \\textbf{M}odule (GCM), concealing the vulnerable direction\nof gradient while guaranteeing the classification accuracy during the inference\ntime. GCM reports superior defense results on the ImageNet classification\nbenchmark, improving up to 63.41\\% top-1 attack robustness (AR) when faced with\nadversarial inputs compared to the vanilla DNNs. Moreover, we use GCM in the\nCVPR 2022 Robust Classification Challenge, currently achieving \\textbf{2nd}\nplace in Phase II with only a tiny version of ConvNext. The code will be made\navailable.",
    "descriptor": "",
    "authors": [
      "Sen Pei",
      "Jiaxi Sun",
      "Xiaopeng Zhang",
      "Gaofeng Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10617"
  },
  {
    "id": "arXiv:2205.10620",
    "title": "Graph Neural Network Enhanced Approximate Message Passing for MIMO  Detection",
    "abstract": "Efficient multiple-input multiple-output (MIMO) detection algorithms with\nsatisfactory performance and low complexity are critical for future\nmulti-antenna systems to meet the high throughput and ultra-low latency\nrequirements in 5G and beyond communications. In this paper, we propose a low\ncomplexity graph neural network (GNN) enhanced approximate message passing\n(AMP) algorithm, AMP-GNN, for MIMO detection. The structure of the neural\nnetwork is customized by unfolding the AMP algorithm and introducing the GNN\nmodule to address the inaccuracy of the Gaussian approximation for multiuser\ninterference cancellation. Numerical results will show that the proposed\nAMP-GNN significantly improves the performance of the AMP detector and achieves\ncomparable performance as the state-of-the-art deep learning-based MIMO\ndetectors but with reduced computational complexity.",
    "descriptor": "\nComments: 6 pages, 5 figures, Submitted to IEEE Globecom 2022\n",
    "authors": [
      "Hengtao He",
      "Alva Kosasihy",
      "Xianghao Yu",
      "Jun Zhang",
      "S.H. Song",
      "Wibowo Hardjawanay",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10620"
  },
  {
    "id": "arXiv:2205.10621",
    "title": "Learning Meta Representations of One-shot Relations for Temporal  Knowledge Graph Link Prediction",
    "abstract": "Few-shot relational learning for static knowledge graphs (KGs) has drawn\ngreater interest in recent years, while few-shot learning for temporal\nknowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs contain\nrich temporal information, thus requiring temporal reasoning techniques for\nmodeling. This poses a greater challenge in learning few-shot relations in the\ntemporal context. In this paper, we revisit the previous work related to\nfew-shot relational learning in KGs and extend two existing TKG reasoning\ntasks, i.e., interpolated and extrapolated link prediction, to the one-shot\nsetting. We propose four new large-scale benchmark datasets and develop a TKG\nreasoning model for learning one-shot relations in TKGs. Experimental results\nshow that our model can achieve superior performance on all datasets in both\ninterpolation and extrapolation tasks.",
    "descriptor": "",
    "authors": [
      "Zifeng Ding",
      "Bailan He",
      "Yunpu Ma",
      "Zhen Han",
      "Volker Tresp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10621"
  },
  {
    "id": "arXiv:2205.10624",
    "title": "CEP3: Community Event Prediction with Neural Point Process on Graph",
    "abstract": "Many real world applications can be formulated as event forecasting on\nContinuous Time Dynamic Graphs (CTDGs) where the occurrence of a timed event\nbetween two entities is represented as an edge along with its occurrence\ntimestamp in the graphs.However, most previous works approach the problem in\ncompromised settings, either formulating it as a link prediction task on the\ngraph given the event time or a time prediction problem given which event will\nhappen next. In this paper, we propose a novel model combining Graph Neural\nNetworks and Marked Temporal Point Process (MTPP) that jointly forecasts\nmultiple link events and their timestamps on communities over a CTDG. Moreover,\nto scale our model to large graphs, we factorize the jointly event prediction\nproblem into three easier conditional probability modeling problems.To evaluate\nthe effectiveness of our model and the rationale behind such a decomposition,\nwe establish a set of benchmarks and evaluation metrics for this event\nforecasting task. Our experiments demonstrate the superior performance of our\nmodel in terms of both model accuracy and training efficiency.",
    "descriptor": "",
    "authors": [
      "Xuhong Wang",
      "Sirui Chen",
      "Yixuan He",
      "Minjie Wang",
      "Quan Gan",
      "Yupu Yang",
      "Junchi Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10624"
  },
  {
    "id": "arXiv:2205.10625",
    "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language  Models",
    "abstract": "We propose a novel prompting strategy, least-to-most prompting, that enables\nlarge language models to better perform multi-step reasoning tasks.\nLeast-to-most prompting first reduces a complex problem into a list of\nsubproblems, and then sequentially solves the subproblems, whereby solving a\ngiven subproblem is facilitated by the model's answers to previously solved\nsubproblems. Experiments on symbolic manipulation, compositional generalization\nand numerical reasoning demonstrate that least-to-most prompting can generalize\nto examples that are harder than those seen in the prompt context,\noutperforming other prompting-based approaches by a large margin. A notable\nempirical result is that the GPT-3 code-davinci-002 model with\nleast-to-most-prompting can solve the SCAN benchmark with an accuracy of 99.7%\nusing 14 examples. As a comparison, the neural-symbolic models in the\nliterature specialized for solving SCAN are trained with the full training set\nof more than 15,000 examples.",
    "descriptor": "",
    "authors": [
      "Denny Zhou",
      "Nathanael Sch\u00e4rli",
      "Le Hou",
      "Jason Wei",
      "Nathan Scales",
      "Xuezhi Wang",
      "Dale Schuurmans",
      "Olivier Bousquet",
      "Quoc Le",
      "Ed Chi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10625"
  },
  {
    "id": "arXiv:2205.10627",
    "title": "DProQ: A Gated-Graph Transformer for Protein Complex Structure  Assessment",
    "abstract": "Proteins interact to form complexes to carry out essential biological\nfunctions. Computational methods have been developed to predict the structures\nof protein complexes. However, an important challenge in protein complex\nstructure prediction is to estimate the quality of predicted protein complex\nstructures without any knowledge of the corresponding native structures. Such\nestimations can then be used to select high-quality predicted complex\nstructures to facilitate biomedical research such as protein function analysis\nand drug discovery. We challenge this significant task with DProQ, which\nintroduces a gated neighborhood-modulating Graph Transformer (GGT) designed to\npredict the quality of 3D protein complex structures. Notably, we incorporate\nnode and edge gates within a novel Graph Transformer framework to control\ninformation flow during graph message passing. We train and evaluate DProQ on\nfour newly-developed datasets that we make publicly available in this work. Our\nrigorous experiments demonstrate that DProQ achieves state-of-the-art\nperformance in ranking protein complex structures.",
    "descriptor": "\nComments: 18 pages, 3 figures, 13 tables. Under review\n",
    "authors": [
      "Xiao Chen",
      "Alex Morehead",
      "Jian Liu",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.10627"
  },
  {
    "id": "arXiv:2205.10629",
    "title": "User-Interactive Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning algorithms still lack trust in practice due to\nthe risk that the learned policy performs worse than the original policy that\ngenerated the dataset or behaves in an unexpected way that is unfamiliar to the\nuser. At the same time, offline RL algorithms are not able to tune their most\nimportant hyperparameter - the proximity of the learned policy to the original\npolicy. We propose an algorithm that allows the user to tune this\nhyperparameter at runtime, thereby overcoming both of the above mentioned\nissues simultaneously. This allows users to start with the original behavior\nand grant successively greater deviation, as well as stopping at any time when\nthe policy deteriorates or the behavior is too far from the familiar one.",
    "descriptor": "",
    "authors": [
      "Phillip Swazinna",
      "Steffen Udluft",
      "Thomas Runkler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10629"
  },
  {
    "id": "arXiv:2205.10634",
    "title": "New quality measures for quadrilaterals and new discrete functionals for  grid generation",
    "abstract": "In this paper, we review some grid quality metrics Robinson1987, Lo1989,\nField2000, Knupp2001, Remacle2012 and define some new quality measures for\nquadrilateral elements. Usually, a maximum value of a quality measure\ncorresponds to the minimum value of the energy density over the grid\nIvanenko2000.\nWe also define new discrete functionals which are implemented as objective\nfunctions in an optimization-based method for quadrilateral grid generation and\nimprovement. These functionals are linearly combined with a discrete functional\nwhose domain has an infinite barrier at the boundary of the set of unfolded\ngrids like $S_{\\omega,\\epsilon}(G)$, see Barrera2010, in order to preserve\nconvex grid cells in each step of the optimization process.",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Guilmer Gonz\u00e1lez Flores",
      "Pablo Barrera S\u00e1nchez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10634"
  },
  {
    "id": "arXiv:2205.10635",
    "title": "SplitPlace: AI Augmented Splitting and Placement of Large-Scale Neural  Networks in Mobile Edge Environments",
    "abstract": "In recent years, deep learning models have become ubiquitous in industry and\nacademia alike. Deep neural networks can solve some of the most complex\npattern-recognition problems today, but come with the price of massive compute\nand memory requirements. This makes the problem of deploying such large-scale\nneural networks challenging in resource-constrained mobile edge computing\nplatforms, specifically in mission-critical domains like surveillance and\nhealthcare. To solve this, a promising solution is to split resource-hungry\nneural networks into lightweight disjoint smaller components for pipelined\ndistributed processing. At present, there are two main approaches to do this:\nsemantic and layer-wise splitting. The former partitions a neural network into\nparallel disjoint models that produce a part of the result, whereas the latter\npartitions into sequential models that produce intermediate results. However,\nthere is no intelligent algorithm that decides which splitting strategy to use\nand places such modular splits to edge nodes for optimal performance. To combat\nthis, this work proposes a novel AI-driven online policy, SplitPlace, that uses\nMulti-Armed-Bandits to intelligently decide between layer and semantic\nsplitting strategies based on the input task's service deadline demands.\nSplitPlace places such neural network split fragments on mobile edge devices\nusing decision-aware reinforcement learning for efficient and scalable\ncomputing. Moreover, SplitPlace fine-tunes its placement engine to adapt to\nvolatile environments. Our experiments on physical mobile-edge environments\nwith real-world workloads show that SplitPlace can significantly improve the\nstate-of-the-art in terms of average response time, deadline violation rate,\ninference accuracy, and total reward by up to 46, 69, 3 and 12 percent\nrespectively.",
    "descriptor": "\nComments: Accepted in IEEE Transactions on Mobile Computing\n",
    "authors": [
      "Shreshth Tuli",
      "Giuliano Casale",
      "Nicholas R. Jennings"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.10635"
  },
  {
    "id": "arXiv:2205.10636",
    "title": "AutoLink: Self-supervised Learning of Human Skeletons and Object  Outlines by Linking Keypoints",
    "abstract": "Structured representations such as keypoints are widely used in pose\ntransfer, conditional image generation, animation, and 3D reconstruction.\nHowever, their supervised learning requires expensive annotation for each\ntarget domain. We propose a self-supervised method that learns to disentangle\nobject structure from the appearance with a graph of 2D keypoints linked by\nstraight edges. Both the keypoint location and their pairwise edge weights are\nlearned, given only a collection of images depicting the same object class. The\ngraph is interpretable, for example, AutoLink recovers the human skeleton\ntopology when applied to images showing people. Our key ingredients are i) an\nencoder that predicts keypoint locations in an input image, ii) a shared graph\nas a latent variable that links the same pairs of keypoints in every image,\niii) an intermediate edge map that combines the latent graph edge weights and\nkeypoint locations in a soft, differentiable manner, and iv) an inpainting\nobjective on randomly masked images. Although simpler, AutoLink outperforms\nexisting self-supervised methods on the established keypoint and pose\nestimation benchmarks and paves the way for structure-conditioned generative\nmodels on more diverse datasets.",
    "descriptor": "",
    "authors": [
      "Xingzhe He",
      "Bastian Wandt",
      "Helge Rhodin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10636"
  },
  {
    "id": "arXiv:2205.10637",
    "title": "Symmetry Teleportation for Accelerated Optimization",
    "abstract": "Existing gradient-based optimization methods update the parameters locally,\nin a direction that minimizes the loss function. We study a different approach,\nsymmetry teleportation, that allows the parameters to travel a large distance\non the loss level set, in order to improve the convergence speed in subsequent\nsteps. Teleportation exploits parameter space symmetries of the optimization\nproblem and transforms parameters while keeping the loss invariant. We derive\nthe loss-invariant group actions for test functions and multi-layer neural\nnetworks, and prove a necessary condition of when teleportation improves\nconvergence rate. We also show that our algorithm is closely related to second\norder methods. Experimentally, we show that teleportation improves the\nconvergence speed of gradient descent and AdaGrad for several optimization\nproblems including test functions, multi-layer regressions, and MNIST\nclassification.",
    "descriptor": "",
    "authors": [
      "Bo Zhao",
      "Nima Dehmamy",
      "Robin Walters",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10637"
  },
  {
    "id": "arXiv:2205.10640",
    "title": "Learning to Dynamically Select Cost Optimal Schedulers in Cloud  Computing Environments",
    "abstract": "The operational cost of a cloud computing platform is one of the most\nsignificant Quality of Service (QoS) criteria for schedulers, crucial to keep\nup with the growing computational demands. Several data-driven deep neural\nnetwork (DNN)-based schedulers have been proposed in recent years that\noutperform alternative approaches by providing scalable and effective resource\nmanagement for dynamic workloads. However, state-of-the-art schedulers rely on\nadvanced DNNs with high computational requirements, implying high scheduling\ncosts. In non-stationary contexts, the most sophisticated schedulers may not\nalways be required, and it may be sufficient to rely on low-cost schedulers to\ntemporarily save operational costs. In this work, we propose MetaNet, a\nsurrogate model that predicts the operational costs and scheduling overheads of\na large number of DNN-based schedulers and chooses one on-the-fly to jointly\noptimize job scheduling and execution costs. This facilitates improvements in\nexecution costs, energy usage and service level agreement violations of up to\n11%, 43% and 13% compared to the state-of-the-art methods.",
    "descriptor": "\nComments: Accepted as a poster in SIGMETRICS 2022\n",
    "authors": [
      "Shreshth Tuli",
      "Giuliano Casale",
      "Nicholas R. Jennings"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.10640"
  },
  {
    "id": "arXiv:2205.10642",
    "title": "MetaNet: Automated Dynamic Selection of Scheduling Policies in Cloud  Environments",
    "abstract": "Task scheduling is a well-studied problem in the context of optimizing the\nQuality of Service (QoS) of cloud computing environments. In order to sustain\nthe rapid growth of computational demands, one of the most important QoS\nmetrics for cloud schedulers is the execution cost. In this regard, several\ndata-driven deep neural networks (DNNs) based schedulers have been proposed in\nrecent years to allow scalable and efficient resource management in dynamic\nworkload settings. However, optimal scheduling frequently relies on\nsophisticated DNNs with high computational needs implying higher execution\ncosts. Further, even in non-stationary environments, sophisticated schedulers\nmight not always be required and we could briefly rely on low-cost schedulers\nin the interest of cost-efficiency. Therefore, this work aims to solve the\nnon-trivial meta problem of online dynamic selection of a scheduling policy\nusing a surrogate model called MetaNet. Unlike traditional solutions with a\nfixed scheduling policy, MetaNet on-the-fly chooses a scheduler from a large\nset of DNN based methods to optimize task scheduling and execution costs in\ntandem. Compared to state-of-the-art DNN schedulers, this allows for\nimprovement in execution costs, energy consumption, response time and service\nlevel agreement violations by up to 11, 43, 8 and 13 percent, respectively.",
    "descriptor": "\nComments: Accepted in IEEE CLOUD 2022\n",
    "authors": [
      "Shreshth Tuli",
      "Giuliano Casale",
      "Nicholas R. Jennings"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10642"
  },
  {
    "id": "arXiv:2205.10643",
    "title": "Self-Supervised Speech Representation Learning: A Review",
    "abstract": "Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.",
    "descriptor": "",
    "authors": [
      "Abdelrahman Mohamed",
      "Hung-yi Lee",
      "Lasse Borgholt",
      "Jakob D. Havtorn",
      "Joakim Edin",
      "Christian Igel",
      "Katrin Kirchhoff",
      "Shang-Wen Li",
      "Karen Livescu",
      "Lars Maal\u00f8e",
      "Tara N. Sainath",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.10643"
  },
  {
    "id": "arXiv:2205.10646",
    "title": "Context Matters for Image Descriptions for Accessibility: Challenges for  Referenceless Evaluation Metrics",
    "abstract": "Few images on the Web receive alt-text descriptions that would make them\naccessible to blind and low vision (BLV) users. Image-based NLG systems have\nprogressed to the point where they can begin to address this persistent\nsocietal problem, but these systems will not be fully successful unless we\nevaluate them on metrics that guide their development correctly. Here, we argue\nagainst current referenceless metrics -- those that don't rely on\nhuman-generated ground-truth descriptions -- on the grounds that they do not\nalign with the needs of BLV users. The fundamental shortcoming of these metrics\nis that they cannot take context into account, whereas contextual information\nis highly valued by BLV users. To substantiate these claims, we present a study\nwith BLV participants who rated descriptions along a variety of dimensions. An\nin-depth analysis reveals that the lack of context-awareness makes current\nreferenceless metrics inadequate for advancing image accessibility, requiring a\nrethinking of referenceless evaluation metrics for image-based NLG systems.",
    "descriptor": "",
    "authors": [
      "Elisa Kreiss",
      "Cynthia Bennett",
      "Shayan Hooshmand",
      "Eric Zelikman",
      "Meredith Ringel Morris",
      "Christopher Potts"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10646"
  },
  {
    "id": "arXiv:2205.10647",
    "title": "Identifying and Mitigating Instability in Embeddings of the Degenerate  Core",
    "abstract": "Are the embeddings of a graph's degenerate core stable? What happens to the\nembeddings of nodes in the degenerate core as we systematically remove\nperiphery nodes (by repeated peeling off $k$-cores)? We discover three patterns\nw.r.t. instability in degenerate-core embeddings across a variety of popular\ngraph embedding algorithms and datasets. We use regression to quantify the\nchange point in graph embedding stability. Furthermore, we present the STABLE\nalgorithm, which takes an existing graph embedding algorithm and makes it\nstable. We show the effectiveness of STABLE in terms of making the\ndegenerate-core embedding stable and still producing state-of-the-art link\nprediction performance.",
    "descriptor": "",
    "authors": [
      "David Liu",
      "Tina Eliassi-Rad"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10647"
  },
  {
    "id": "arXiv:2205.10648",
    "title": "Global reconstruction of initial conditions of nonlinear parabolic  equations via the Carleman-contraction method",
    "abstract": "We propose a global convergent numerical method to reconstruct the initial\ncondition of a nonlinear parabolic equation from the measurement of both\nDirichlet and Neumann data on the boundary of a bounded domain. The first step\nin our method is to derive, from the nonlinear governing parabolic equation, a\nnonlinear systems of elliptic partial differential equations (PDEs) whose\nsolution yields directly the solution of the inverse source problem. We then\nestablish a contraction mapping-like iterative scheme to solve this system. The\nconvergence of this iterative scheme is rigorously proved by employing a\nCarleman estimate and the argument in the proof of the traditional contraction\nmapping principle. This convergence is fast in both theoretical and numerical\nsenses. Moreover, our method, unlike the methods based on optimization, does\nnot require a good initial guess of the true solution. Numerical examples are\npresented to verify these results.",
    "descriptor": "",
    "authors": [
      "Thuy T. Le"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10648"
  },
  {
    "id": "arXiv:2205.10649",
    "title": "Towards the Effects of Alignment Edits on the Quality of Experience of  360 Videos",
    "abstract": "The optimization of viewers' quality of experience (QoE) in 360 videos faces\ntwo major roadblocks: inaccurate adaptive streaming and viewers missing the\nplot of a story. Alignment edit emerged as a promising mechanism to avoid both\nissues at once. Alignment edits act on the content, matching the users'\nviewport with a region of interest in the video content. As a consequence,\nviewers' attention is focused, reducing exploratory behavior and enabling the\noptimization of network resources; in addition, it allows for a precise\nselection of events to be shown to viewers, supporting viewers to follow the\nstoryline. In this work, we investigate the effects of alignment edits on QoE\nby conducting two user studies. Specifically, we measured three QoE factors:\npresence, comfort, and overall QoE. We introduce a new alignment edit, named\n\\textit{Fade-rotation}, based on a mechanism to reduce cybersickness in VR\ngames. In the user studies, we tested four versions of fade-rotation and\ncompared them with instant alignment. We observed from the results that gradual\nalignment achieves good levels of comfort for all contents and rotational speed\ntested, showing its validity. We observed a decrease in head motion after both\nalignment edits, with the gradual edit reaching a reduction in head speed of\n8\\% greater than that of instant alignment, confirming the usefulness of these\nedits for streaming video on-demand. Finally, parameters to implement\n\\textit{Fade-rotation} are described.",
    "descriptor": "\nComments: 14 pages, 13 figures, 4 tables\n",
    "authors": [
      "Lucas Althoff",
      "Alessandro Rodrigues",
      "Myl\u00e8ne C. Q. Farias"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.10649"
  },
  {
    "id": "arXiv:2205.10650",
    "title": "Transformer-based out-of-distribution detection for clinically safe  segmentation",
    "abstract": "In a clinical setting it is essential that deployed image processing systems\nare robust to the full range of inputs they might encounter and, in particular,\ndo not make confidently wrong predictions. The most popular approach to safe\nprocessing is to train networks that can provide a measure of their\nuncertainty, but these tend to fail for inputs that are far outside the\ntraining data distribution. Recently, generative modelling approaches have been\nproposed as an alternative; these can quantify the likelihood of a data sample\nexplicitly, filtering out any out-of-distribution (OOD) samples before further\nprocessing is performed. In this work, we focus on image segmentation and\nevaluate several approaches to network uncertainty in the far-OOD and near-OOD\ncases for the task of segmenting haemorrhages in head CTs. We find all of these\napproaches are unsuitable for safe segmentation as they provide confidently\nwrong predictions when operating OOD. We propose performing full 3D OOD\ndetection using a VQ-GAN to provide a compressed latent representation of the\nimage and a transformer to estimate the data likelihood. Our approach\nsuccessfully identifies images in both the far- and near-OOD cases. We find a\nstrong relationship between image likelihood and the quality of a model's\nsegmentation, making this approach viable for filtering images unsuitable for\nsegmentation. To our knowledge, this is the first time transformers have been\napplied to perform OOD detection on 3D image data.",
    "descriptor": "\nComments: Accepted at MIDL 2022 (Oral)\n",
    "authors": [
      "Mark S Graham",
      "Petru-Daniel Tudosiu",
      "Paul Wright",
      "Walter Hugo Lopez Pinaya",
      "U Jean-Marie",
      "Yee Mah",
      "James Teo",
      "Rolf H J\u00e4ger",
      "David Werring",
      "Parashkev Nachev",
      "Sebastien Ourselin",
      "M Jorge Cardoso"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10650"
  },
  {
    "id": "arXiv:2205.10652",
    "title": "Are Graph Neural Networks Really Helpful for Knowledge Graph Completion?",
    "abstract": "Knowledge graphs (KGs) facilitate a wide variety of applications due to their\nability to store relational knowledge applicable to many areas. Despite great\nefforts invested in creation and maintenance, even the largest KGs are far from\ncomplete. Hence, KG completion (KGC) has become one of the most crucial tasks\nfor KG research. Recently, considerable literature in this space has centered\naround the use of Graph Neural Networks (GNNs) to learn powerful embeddings\nwhich leverage topological structures in the KGs. Specifically, dedicated\nefforts have been made to extend GNNs, which are commonly designed for simple\nhomogeneous and uni-relational graphs, to the KG context which has diverse and\nmulti-relational connections between entities, by designing more complex\naggregation schemes over neighboring nodes (crucial to GNN performance) to\nappropriately leverage multi-relational information. The success of these\nmethods is naturally attributed to the use of GNNs over simpler multi-layer\nperceptron (MLP) models, owing to their additional aggregation functionality.\nIn this work, we find that surprisingly, simple MLP models are able to achieve\ncomparable performance to GNNs, suggesting that aggregation may not be as\ncrucial as previously believed. With further exploration, we show careful\nscoring function and loss function design has a much stronger influence on KGC\nmodel performance, and aggregation is not practically required. This suggests a\nconflation of scoring function design, loss function design, and aggregation in\nprior work, with promising insights regarding the scalability of\nstate-of-the-art KGC methods today, as well as careful attention to more\nsuitable aggregation designs for KGC tasks tomorrow.",
    "descriptor": "",
    "authors": [
      "Juanhui Li",
      "Harry Shomer",
      "Jiayuan Ding",
      "Yiqi Wang",
      "Yao Ma",
      "Neil Shah",
      "Jiliang Tang",
      "Dawei Yin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10652"
  },
  {
    "id": "arXiv:2205.10653",
    "title": "Towards real-time and energy efficient Siamese tracking - a  hardware-software approach",
    "abstract": "Siamese trackers have been among the state-of-the-art solutions in each\nVisual Object Tracking (VOT) challenge over the past few years. However, with\ngreat accuracy comes great computational complexity: to achieve real-time\nprocessing, these trackers have to be massively parallelised and are usually\nrun on high-end GPUs. Easy to implement, this approach is energy consuming, and\nthus cannot be used in many low-power applications. To overcome this, one can\nuse energy-efficient embedded devices, such as heterogeneous platforms joining\nthe ARM processor system with programmable logic (FPGA). In this work, we\npropose a hardware-software implementation of the well-known fully connected\nSiamese tracker (SiamFC). We have developed a quantised Siamese network for the\nFINN accelerator, using algorithm-accelerator co-design, and performed design\nspace exploration to achieve the best efficiency-to-energy ratio (determined by\nFPS and used resources). For our network, running in the programmable logic\npart of the Zynq UltraScale+ MPSoC ZCU104, we achieved the processing of almost\n50 frames-per-second with tracker accuracy on par with its floating point\ncounterpart, as well as the original SiamFC network. The complete tracking\nsystem, implemented in ARM with the network accelerated on FPGA, achieves up to\n17 fps. These results bring us towards bridging the gap between the highly\naccurate but energy-demanding algorithms and energy-efficient solutions ready\nto be used in low-power, edge systems.",
    "descriptor": "\nComments: Accepted for DASIP 2022 workshop\n",
    "authors": [
      "Dominika Przewlocka-Rus",
      "Tomasz Kryjak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10653"
  },
  {
    "id": "arXiv:2205.10655",
    "title": "Swept-Angle Synthetic Wavelength Interferometry",
    "abstract": "We present a new imaging technique, swept-angle synthetic wavelength\ninterferometry, for full-field micron-scale 3D sensing. As in conventional\nsynthetic wavelength interferometry, our technique uses light consisting of two\noptical wavelengths, resulting in per-pixel interferometric measurements whose\nphase encodes scene depth. Our technique additionally uses a new type of light\nsource that, by emulating spatially-incoherent illumination, makes\ninterferometric measurements insensitive to global illumination effects that\nconfound depth information. The resulting technique combines the speed of\nfull-field interferometric setups with the robustness to global illumination of\nscanning interferometric setups. Overall, our technique can recover full-frame\ndepth at a spatial and axial resolution of a few micrometers using as few as 16\nmeasurements, resulting in fast acquisition at frame rates of 10 Hz. We build\nan experimental prototype and use it to demonstrate these capabilities, by\nscanning a variety of scenes that contain challenging light transport effects\nsuch as interreflections, subsurface scattering, and specularities. We validate\nthe accuracy of our measurements by showing that they closely match reference\nmeasurements from a full-field optical coherence tomography system, despite\nbeing captured at orders of magnitude faster acquisition times and while\noperating under strong ambient light.",
    "descriptor": "",
    "authors": [
      "Alankar Kotwal",
      "Anat Levin",
      "Ioannis Gkioulekas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2205.10655"
  },
  {
    "id": "arXiv:2205.10656",
    "title": "Experiences with task-based programming using cluster nodes as OpenMP  devices",
    "abstract": "Programming a distributed system, such as a cluster, requires extended use of\nlow-level communication libraries and can often become cumbersome and error\nprone for the average developer. In this work, we consider each node of a\ncluster as a separate OpenMP device, able to run code with OpenMP directives in\nparallel. We make use of the OpenMP device model to provide an easy and\nintuitive way to program available cluster nodes. Based on that, we examine\nmodifications that were necessary to make existing task-based applications able\nto exploit such an infrastructure. Finally, we evaluate the performance of the\nsystem and conclude that one can gain significant speedup, as long as the\napplication tasks do not produce excessive communication overheads.",
    "descriptor": "\nComments: In HPCS 2020, The 18th International Conference on High Performance Computing and Simulation, March 2021\n",
    "authors": [
      "Ilias Keftakis",
      "Vassilios V. Dimakopoulos"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10656"
  },
  {
    "id": "arXiv:2205.10658",
    "title": "BunchBFT: Across-Cluster Consensus Protocol",
    "abstract": "In this paper, we present BunchBFT Byzantine fault-tolerant state-machine\nreplication for high performance and scalability. At the heart of BunchBFT is a\nnovel design called the cluster-based approach that divides the replicas into\nclusters of replicas. By combining this cluster-based approach with\nhierarchical communications across clusters, piggybacking techniques for\nsending messages across clusters, and decentralized leader election for each\ncluster, BunchBFT achieves high performance and scalability.\nWe also prove that BunchBFT satisfies the basic safety and liveness\nproperties of Byzantine consensus. We implemented a prototype of BunchBFT in\nour PaxiBFT framework to show that the BunchBFT can improve the MirBFT's\nthroughput by 10x, depending on the available bandwidth on wide-area links.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Salem Alqahtani",
      "Murat Demirbas"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10658"
  },
  {
    "id": "arXiv:2205.10660",
    "title": "Vision Transformers in 2022: An Update on Tiny ImageNet",
    "abstract": "The recent advances in image transformers have shown impressive results and\nhave largely closed the gap between traditional CNN architectures. The standard\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\nImageNet-1k. After finetuning, researches will often consider the transfer\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\nTiny ImageNet. This paper offers an update on vision transformers' performance\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\nresult with a validation accuracy of 91.35%. Code is available here:\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers",
    "descriptor": "",
    "authors": [
      "Ethan Huynh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10660"
  },
  {
    "id": "arXiv:2205.10661",
    "title": "An Empirical Investigation of Commonsense Self-Supervision with  Knowledge Graphs",
    "abstract": "Self-supervision based on the information extracted from large knowledge\ngraphs has been shown to improve the generalization of language models, in\nzero-shot evaluation on various downstream language reasoning tasks. Since\nthese improvements are reported in aggregate, however, little is known about\n(i) how to select the appropriate knowledge for solid performance across tasks,\n(ii) how to combine this knowledge with neural language models, and (iii) how\nthese pairings affect granular task performance. In this paper, we study the\neffect of knowledge sampling strategies and sizes that can be used to generate\nsynthetic data for adapting language models. We study the effect of different\nsynthetic datasets on language models with various architectures and sizes. The\nresulting models are evaluated against four task properties: domain overlap,\nanswer similarity, vocabulary overlap, and answer length. Our experiments show\nthat encoder-decoder models benefit from more data to learn from, whereas\nsampling strategies that balance across different aspects yield best\nperformance. Most of the improvement occurs on questions with short answers and\ndissimilar answer candidates, which corresponds to the characteristics of the\ndata used for pre-training.",
    "descriptor": "",
    "authors": [
      "Jiarui Zhang",
      "Filip Ilievski",
      "Kaixin Ma",
      "Jonathan Francis",
      "Alessandro Oltramari"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10661"
  },
  {
    "id": "arXiv:2205.10662",
    "title": "Equivariant Mesh Attention Networks",
    "abstract": "Equivariance to symmetries has proven to be a powerful inductive bias in deep\nlearning research. Recent works on mesh processing have concentrated on various\nkinds of natural symmetries, including translations, rotations, scaling, node\npermutations, and gauge transformations. To date, no existing architecture is\nequivariant to all of these transformations. Moreover, previous implementations\nhave not always applied these symmetry transformations to the test dataset.\nThis inhibits the ability to determine whether the model attains the claimed\nequivariance properties. In this paper, we present an attention-based\narchitecture for mesh data that is provably equivariant to all transformations\nmentioned above. We carry out experiments on the FAUST and TOSCA datasets, and\napply the mentioned symmetries to the test set only. Our results confirm that\nour proposed architecture is equivariant, and therefore robust, to these\nlocal/global transformations.",
    "descriptor": "\nComments: Implementation can be found at: this https URL\n",
    "authors": [
      "Sourya Basu",
      "Jose Gallego-Posada",
      "Francesco Vigan\u00f2",
      "James Rowbottom",
      "Taco Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10662"
  },
  {
    "id": "arXiv:2205.10664",
    "title": "Temporal Domain Generalization with Drift-Aware Dynamic Neural Network",
    "abstract": "Temporal domain generalization is a promising yet extremely challenging area\nwhere the goal is to learn models under temporally changing data distributions\nand generalize to unseen data distributions following the trends of the change.\nThe advancement of this area is challenged by: 1) characterizing data\ndistribution drift and its impacts on models, 2) expressiveness in tracking the\nmodel dynamics, and 3) theoretical guarantee on the performance. To address\nthem, we propose a Temporal Domain Generalization with Drift-Aware Dynamic\nNeural Network (DRAIN) framework. Specifically, we formulate the problem into a\nBayesian framework that jointly models the relation between data and model\ndynamics. We then build a recurrent graph generation scenario to characterize\nthe dynamic graph-structured neural networks learned across different time\npoints. It captures the temporal drift of model parameters and data\ndistributions and can predict models in the future without the presence of\nfuture data. In addition, we explore theoretical guarantees of the model\nperformance under the challenging temporal DG setting and provide theoretical\nanalysis, including uncertainty and generalization error. Finally, extensive\nexperiments on several real-world benchmarks with temporal drift demonstrate\nthe effectiveness and efficiency of the proposed method.",
    "descriptor": "\nComments: Preprint: 16 pages, 5 figures\n",
    "authors": [
      "Guangji Bai",
      "Ling Chen",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10664"
  },
  {
    "id": "arXiv:2205.10666",
    "title": "MultiBiSage: A Web-Scale Recommendation System Using Multiple Bipartite  Graphs at Pinterest",
    "abstract": "Graph Convolutional Networks (GCN) can efficiently integrate graph structure\nand node features to learn high-quality node embeddings. These embeddings can\nthen be used for several tasks such as recommendation and search. At Pinterest,\nwe have developed and deployed PinSage, a data-efficient GCN that learns pin\nembeddings from the Pin-Board graph. The Pin-Board graph contains pin and board\nentities and the graph captures the pin belongs to a board interaction.\nHowever, there exist several entities at Pinterest such as users, idea pins,\ncreators, and there exist heterogeneous interactions among these entities such\nas add-to-cart, follow, long-click.\nIn this work, we show that training deep learning models on graphs that\ncaptures these diverse interactions would result in learning higher-quality pin\nembeddings than training PinSage on only the Pin-Board graph. To that end, we\nmodel the diverse entities and their diverse interactions through multiple\nbipartite graphs and propose a novel data-efficient MultiBiSage model.\nMultiBiSage can capture the graph structure of multiple bipartite graphs to\nlearn high-quality pin embeddings. We take this pragmatic approach as it allows\nus to utilize the existing infrastructure developed at Pinterest -- such as\nPixie system that can perform optimized random-walks on billion node graphs,\nalong with existing training and deployment workflows. We train MultiBiSage on\nsix bipartite graphs including our Pin-Board graph. Our offline metrics show\nthat MultiBiSage significantly outperforms the deployed latest version of\nPinSage on multiple user engagement metrics.",
    "descriptor": "",
    "authors": [
      "Saket Gurukar",
      "Nikil Pancha",
      "Andrew Zhai",
      "Eric Kim",
      "Samson Hu",
      "Srinivasan Parthasarathy",
      "Charles Rosenberg",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10666"
  },
  {
    "id": "arXiv:2205.10667",
    "title": "Individual Topology Structure of Eye Movement Trajectories",
    "abstract": "Traditionally, extracting patterns from eye movement data relies on\nstatistics of different macro-events such as fixations and saccades. This\nrequires an additional preprocessing step to separate the eye movement\nsubtypes, often with a number of parameters on which the classification results\ndepend. Besides that, definitions of such macro events are formulated in\ndifferent ways by different researchers.\nWe propose an application of a new class of features to the quantitative\nanalysis of personal eye movement trajectories structure. This new class of\nfeatures based on algebraic topology allows extracting patterns from different\nmodalities of gaze such as time series of coordinates and amplitudes, heatmaps,\nand point clouds in a unified way at all scales from micro to macro. We\nexperimentally demonstrate the competitiveness of the new class of features\nwith the traditional ones and their significant synergy while being used\ntogether for the person authentication task on the recently published eye\nmovement trajectories dataset.",
    "descriptor": "",
    "authors": [
      "Arsenii Onuchin",
      "Oleg Kachan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10667"
  },
  {
    "id": "arXiv:2205.10668",
    "title": "Supplementary Results of a Comparative Syntactic and Semantic Study of  Terms for Software Testing Glossaries",
    "abstract": "This preprint specifies supplementary material and the results of a\ncomparative, syntactic, and semantic study of terms for three software testing\nglossaries. The three software testing glossaries are: the ISO 29119-1,\nConcepts and definitions standard, the Standard Glossary of Terms used in\nSoftware Testing (version 3.5) by the International Software Testing\nQualifications Board (ISTQB), and the glossary provided by the Test Maturity\nModel integration (TMMi) Foundation. First, we categorized the glossaries terms\nby analyzing their given semantics into 8 categories, namely: test project,\nstrategy, or organization; testing process or activity; test goal, requirement,\nor entity; test work product; testing method or technique; testing agent, role,\nor tool; other terms somewhat related to test; and terms beyond the testing\ndomain. Also, we designed a set of metrics that serve to extract and produce\ndata that support the analysis of syntactic and semantic similarities and\ndiscrepancies, and ultimately the analysis of consistencies and inconsistencies\nbetween glossaries' terms. It is important to remark that in this work the\nwhole analysis is focused on glossaries terms whose names end with the word\n\"testing\".",
    "descriptor": "\nComments: 55 pages, 6 Appendixes with tables\n",
    "authors": [
      "Luis Olsina",
      "Philip Lew",
      "Guido Tebes"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10668"
  },
  {
    "id": "arXiv:2205.10670",
    "title": "Online Coreference Resolution for Dialogue Processing: Improving  Mention-Linking on Real-Time Conversations",
    "abstract": "This paper suggests a direction of coreference resolution for online decoding\non actively generated input such as dialogue, where the model accepts an\nutterance and its past context, then finds mentions in the current utterance as\nwell as their referents, upon each dialogue turn. A baseline and four\nincremental-updated models adapted from the mention-linking paradigm are\nproposed for this new setting, which address different aspects including the\nsingletons, speaker-grounded encoding and cross-turn mention contextualization.\nOur approach is assessed on three datasets: Friends, OntoNotes, and BOLT.\nResults show that each aspect brings out steady improvement, and our best\nmodels outperform the baseline by over 10%, presenting an effective system for\nthis setting. Further analysis highlights the task characteristics, such as the\nsignificance of addressing the mention recall.",
    "descriptor": "\nComments: Accepted by *SEM 2022\n",
    "authors": [
      "Liyan Xu",
      "Jinho D. Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10670"
  },
  {
    "id": "arXiv:2205.10671",
    "title": "Pessimism for Offline Linear Contextual Bandits using $\\ell_p$  Confidence Sets",
    "abstract": "We present a family $\\{\\hat{\\pi}\\}_{p\\ge 1}$ of pessimistic learning rules\nfor offline learning of linear contextual bandits, relying on confidence sets\nwith respect to different $\\ell_p$ norms, where $\\hat{\\pi}_2$ corresponds to\nBellman-consistent pessimism (BCP), while $\\hat{\\pi}_\\infty$ is a novel\ngeneralization of lower confidence bound (LCB) to the linear setting. We show\nthat the novel $\\hat{\\pi}_\\infty$ learning rule is, in a sense, adaptively\noptimal, as it achieves the minimax performance (up to log factors) against all\n$\\ell_q$-constrained problems, and as such it strictly dominates all other\npredictors in the family, including $\\hat{\\pi}_2$.",
    "descriptor": "",
    "authors": [
      "Gene Li",
      "Cong Ma",
      "Nathan Srebro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10671"
  },
  {
    "id": "arXiv:2205.10674",
    "title": "NS3: Neuro-Symbolic Semantic Code Search",
    "abstract": "Semantic code search is the task of retrieving a code snippet given a textual\ndescription of its functionality. Recent work has been focused on using\nsimilarity metrics between neural embeddings of text and code. However, current\nlanguage models are known to struggle with longer, compositional text, and\nmulti-step reasoning. To overcome this limitation, we propose supplementing the\nquery sentence with a layout of its semantic structure. The semantic layout is\nused to break down the final reasoning decision into a series of lower-level\ndecisions. We use a Neural Module Network architecture to implement this idea.\nWe compare our model - NS3 (Neuro-Symbolic Semantic Search) - to a number of\nbaselines, including state-of-the-art semantic code retrieval methods, and\nevaluate on two datasets - CodeSearchNet and Code Search and Question\nAnswering. We demonstrate that our approach results in more precise code\nretrieval, and we study the effectiveness of our modular design when handling\ncompositional queries.",
    "descriptor": "",
    "authors": [
      "Shushan Arakelyan",
      "Anna Hakhverdyan",
      "Miltiadis Allamanis",
      "Christophe Hauser",
      "Luis Garcia",
      "Xiang Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10674"
  },
  {
    "id": "arXiv:2205.10676",
    "title": "Terraform -- Automating Infrastructure as a Service",
    "abstract": "Developing a software service requires a strict software development life\ncycle and process. This process demands controlling all application code\nthrough source control management as well as a rigorous versioning and\nbranching strategy. However, the platform and infrastructure also benefit from\nthis rigor. Software services must be deployed to a target run time environment\nand provisioning that environment through manual user actions is tedious and\nerror-prone. Provisioning manually also becomes prohibitive as the number of\nresources grow and spread globally over multiple regions. The answer is to\napply the same rigor to provisioning the infrastructure as applied to\ndeveloping the application software. Terraform provides a platform allowing\ninfrastructure resources to be defined in code. This code not only allows the\nautomation of the infrastructure provisioning but also allows for a strict\ndevelopment and review life cycle, same as the application software.",
    "descriptor": "",
    "authors": [
      "Michael Howard"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.10676"
  },
  {
    "id": "arXiv:2205.10677",
    "title": "Risk-Driven Design of Perception Systems",
    "abstract": "Modern autonomous systems rely on perception modules to process complex\nsensor measurements into state estimates. These estimates are then passed to a\ncontroller, which uses them to make safety-critical decisions. It is therefore\nimportant that we design perception systems to minimize errors that reduce the\noverall safety of the system. We develop a risk-driven approach to designing\nperception systems that accounts for the effect of perceptual errors on the\nperformance of the fully-integrated, closed-loop system. We formulate a risk\nfunction to quantify the effect of a given perceptual error on overall safety,\nand show how we can use it to design safer perception systems by including a\nrisk-dependent term in the loss function and generating training data in\nrisk-sensitive regions. We evaluate our techniques on a realistic vision-based\naircraft detect and avoid application and show that risk-driven design reduces\ncollision risk by 37% over a baseline system.",
    "descriptor": "\nComments: 17 pages, 10 figures\n",
    "authors": [
      "Anthony L. Corso",
      "Sydney M. Katz",
      "Craig Innes",
      "Xin Du",
      "Subramanian Ramamoorthy",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10677"
  },
  {
    "id": "arXiv:2205.10678",
    "title": "On the problem of entity matching and its application in automated  settlement of receivables",
    "abstract": "This paper covers automated settlement of receivables in non-governmental\norganizations. We tackle the problem with entity matching techniques. We\nconsider setup, where base algorithm is used for preliminary ranking of\nmatches, then we apply several novel methods to increase matching quality of\nbase algorithm: score post processing, cascade model and chain model. The\nmethods presented here contribute to automated settlement of receivables,\nentity matching and multilabel classification in open-world scenario. We\nevaluate our approach on real world operational data which come from company\nproviding settlement of receivables as a service: proposed methods boost recall\nfrom 78% (base model) to >90% at precision 99%.",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Lukasz Czekaj",
      "Tomasz Biegus",
      "Robert Kitlowski",
      "Stanislaw Raczynski",
      "Mateusz Olszewski",
      "Jakub Dziedzic",
      "Pawe\u0142 Tomasik",
      "Ryszard Kozera",
      "Alexander Prokopenya",
      "Robert Olszewski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10678"
  },
  {
    "id": "arXiv:2205.10679",
    "title": "What do you need to consider when designing mobile health intervention?",
    "abstract": "Designing theory-driven digital health interventions is a challenging task\nthat needs support. We created a guide for the incomers in the field on how to\ndesign digital health interventions with case studies from the Cancer Better\nLife Experience (CAPABLE) European project. The guide explains how behaviour\nchange theories can inform customisation and personalisation of the\nintervention. The proposed SATO (ideaS expAnded wiTh bciO) design workflow is\nbased on the IDEAS (Integrate, Design, Assess, and Share) framework and is\naligned with the Behaviour Change Intervention Ontology (BCIO). We provide a\nchecklist of the activities that should be performed during intervention\nplanning as well as app design templates which bundle together relevant\nbehaviour change techniques. In the process of creating this guide, we found\nthe necessity to extend the BCIO to support the scenarios of multiple clinical\ngoals in the same application. The extension utilizes existing classes and\nproperties where possible.",
    "descriptor": "\nComments: 31 pages, 6 figures\n",
    "authors": [
      "Aneta Lisowska",
      "Szymon Wilk",
      "Mor Peleg"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.10679"
  },
  {
    "id": "arXiv:2205.10682",
    "title": "A Novel Markov Model for Near-Term Railway Delay Prediction",
    "abstract": "Predicting the near-future delay with accuracy for trains is momentous for\nrailway operations and passengers' traveling experience. This work aims to\ndesign prediction models for train delays based on Netherlands Railway data. We\nfirst develop a chi-square test to show that the delay evolution over stations\nfollows a first-order Markov chain. We then propose a delay prediction model\nbased on non-homogeneous Markov chains. To deal with the sparsity of the\ntransition matrices of the Markov chains, we propose a novel matrix recovery\napproach that relies on Gaussian kernel density estimation. Our numerical tests\nshow that this recovery approach outperforms other heuristic approaches in\nprediction accuracy. The Markov chain model we propose also shows to be better\nthan other widely-used time series models with respect to both interpretability\nand prediction accuracy. Moreover, our proposed model does not require a\ncomplicated training process, which is capable of handling large-scale\nforecasting problems.",
    "descriptor": "\nComments: 36 pages, 3 figures, 4 tables\n",
    "authors": [
      "Jin Xu",
      "Weiqi Wang",
      "Zheming Gao",
      "Haochen Luo",
      "Qian Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10682"
  },
  {
    "id": "arXiv:2205.10683",
    "title": "Scalable and Efficient Training of Large Convolutional Neural Networks  with Differential Privacy",
    "abstract": "Large convolutional neural networks (CNN) can be difficult to train in the\ndifferentially private (DP) regime, since the optimization algorithms require a\ncomputationally expensive operation, known as the per-sample gradient clipping.\nWe propose an efficient and scalable implementation of this clipping on\nconvolutional layers, termed as the mixed ghost clipping, that significantly\neases the private training in terms of both time and space complexities,\nwithout affecting the accuracy. The improvement in efficiency is rigorously\nstudied through the first complexity analysis for the mixed ghost clipping and\nexisting DP training algorithms.\nExtensive experiments on vision classification tasks, with large ResNet, VGG,\nand Vision Transformers, demonstrate that DP training with mixed ghost clipping\nadds $1\\sim 10\\%$ memory overhead and $<2\\times$ slowdown to the standard\nnon-private training. Specifically, when training VGG19 on CIFAR10, the mixed\nghost clipping is $3\\times$ faster than state-of-the-art Opacus library with\n$18\\times$ larger maximum batch size. To emphasize the significance of\nefficient DP training on convolutional layers, we achieve 96.7\\% accuracy on\nCIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous\nbest results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy\nengine (\\url{https://github.com/JialinMao/private_CNN}) that implements DP\ntraining of CNN with a few lines of code.",
    "descriptor": "",
    "authors": [
      "Zhiqi Bu",
      "Jialin Mao",
      "Shiyun Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Complexity (cs.CC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10683"
  },
  {
    "id": "arXiv:2205.10684",
    "title": "Neural Augmented Min-Sum Decoding of Short Block Codes for Fading  Channels",
    "abstract": "In the decoding of linear block codes, it was shown that noticeable gains in\nterms of bit error rate can be achieved by introducing learnable parameters to\nthe Belief Propagation (BP) decoder. Despite the success of these methods,\nthere are two key open problems. The first is the lack of analysis for channels\nother than AWGN. The second is the interpretation of the weights learned and\ntheir effect on the reliability of the BP decoder. In this work, we aim to\nbridge this gap by looking at non-AWGN channels such as Extended Typical Urban\n(ETU) channel. We study the effect of entangling the weights and how the\nperformance holds across different channel settings for the min-sum version of\nBP decoder. We show that while entanglement has little degradation in the AWGN\nchannel, a significant loss is observed in more complex channels. We also\nprovide insights into the weights learned and their connection to the structure\nof the underlying code. Finally, we evaluate our algorithm on the over-the-air\nchannels using Software Defined Radios.",
    "descriptor": "",
    "authors": [
      "Sravan Kumar Ankireddy",
      "Hyeji Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10684"
  },
  {
    "id": "arXiv:2205.10685",
    "title": "Probabilistic Structured Grammatical Evolution",
    "abstract": "The grammars used in grammar-based Genetic Programming (GP) methods have a\nsignificant impact on the quality of the solutions generated since they define\nthe search space by restricting the solutions to its syntax. In this work, we\npropose Probabilistic Structured Grammatical Evolution (PSGE), a new approach\nthat combines the Structured Grammatical Evolution (SGE) and Probabilistic\nGrammatical Evolution (PGE) representation variants and mapping mechanisms. The\ngenotype is a set of dynamic lists, one for each non-terminal in the grammar,\nwith each element of the list representing a probability used to select the\nnext Probabilistic Context-Free Grammar (PCFG) derivation rule. PSGE\nstatistically outperformed Grammatical Evolution (GE) on all six benchmark\nproblems studied. In comparison to PGE, PSGE outperformed 4 of the 6 problems\nanalyzed.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2204.08985\n",
    "authors": [
      "Jessica M\u00e9gane",
      "Nuno Louren\u00e7o",
      "Penousal Machado"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10685"
  },
  {
    "id": "arXiv:2205.10686",
    "title": "Post-breach Recovery: Protection against White-box Adversarial Examples  for Leaked DNN Models",
    "abstract": "Server breaches are an unfortunate reality on today's Internet. In the\ncontext of deep neural network (DNN) models, they are particularly harmful,\nbecause a leaked model gives an attacker \"white-box\" access to generate\nadversarial examples, a threat model that has no practical robust defenses. For\npractitioners who have invested years and millions into proprietary DNNs, e.g.\nmedical imaging, this seems like an inevitable disaster looming on the horizon.\nIn this paper, we consider the problem of post-breach recovery for DNN\nmodels. We propose Neo, a new system that creates new versions of leaked\nmodels, alongside an inference time filter that detects and removes adversarial\nexamples generated on previously leaked models. The classification surfaces of\ndifferent model versions are slightly offset (by introducing hidden\ndistributions), and Neo detects the overfitting of attacks to the leaked model\nused in its generation. We show that across a variety of tasks and attack\nmethods, Neo is able to filter out attacks from leaked models with very high\naccuracy, and provides strong protection (7--10 recoveries) against attackers\nwho repeatedly breach the server. Neo performs well against a variety of strong\nadaptive attacks, dropping slightly in # of breaches recoverable, and\ndemonstrates potential as a complement to DNN defenses in the wild.",
    "descriptor": "",
    "authors": [
      "Shawn Shan",
      "Wenxin Ding",
      "Emily Wenger",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10686"
  },
  {
    "id": "arXiv:2205.10687",
    "title": "Revisiting Pre-trained Language Models and their Evaluation for Arabic  Natural Language Understanding",
    "abstract": "There is a growing body of work in recent years to develop pre-trained\nlanguage models (PLMs) for the Arabic language. This work concerns addressing\ntwo major problems in existing Arabic PLMs which constraint progress of the\nArabic NLU and NLG fields.First, existing Arabic PLMs are not well-explored and\ntheir pre-trainig can be improved significantly using a more methodical\napproach. Second, there is a lack of systematic and reproducible evaluation of\nthese models in the literature. In this work, we revisit both the pre-training\nand evaluation of Arabic PLMs. In terms of pre-training, we explore improving\nArabic LMs from three perspectives: quality of the pre-training data, size of\nthe model, and incorporating character-level information. As a result, we\nrelease three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and\ntwo T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a\ncomprehensive empirical study to systematically evaluate the performance of\nexisting state-of-the-art models on ALUE that is a leaderboard-powered\nbenchmark for Arabic NLU tasks, and on a subset of the ARGEN benchmark for\nArabic NLG tasks. We show that our models significantly outperform existing\nArabic PLMs and achieve a new state-of-the-art performance on discriminative\nand generative Arabic NLU and NLG tasks. Our models and source code to\nreproduce of results will be made available shortly.",
    "descriptor": "",
    "authors": [
      "Abbas Ghaddar",
      "Yimeng Wu",
      "Sunyam Bagga",
      "Ahmad Rashid",
      "Khalil Bibi",
      "Mehdi Rezagholizadeh",
      "Chao Xing",
      "Yasheng Wang",
      "Duan Xinyu",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Xin Jiang",
      "Qun Liu",
      "Philippe Langlais"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10687"
  },
  {
    "id": "arXiv:2205.10688",
    "title": "Co-design of Embodied Neural Intelligence via Constrained Evolution",
    "abstract": "We introduce a novel co-design method for autonomous moving agents' shape\nattributes and locomotion by combining deep reinforcement learning and\nevolution with user control. Our main inspiration comes from evolution, which\nhas led to wide variability and adaptation in Nature and has the potential to\nsignificantly improve design and behavior simultaneously. Our method takes an\ninput agent with optional simple constraints such as leg parts that should not\nevolve or allowed ranges of changes. It uses physics-based simulation to\ndetermine its locomotion and finds a behavior policy for the input design,\nlater used as a baseline for comparison. The agent is then randomly modified\nwithin the allowed ranges creating a new generation of several hundred agents.\nThe generation is trained by transferring the previous policy, which\nsignificantly speeds up the training. The best-performing agents are selected,\nand a new generation is formed using their crossover and mutations. The next\ngenerations are then trained until satisfactory results are reached. We show a\nwide variety of evolved agents, and our results show that even with only 10% of\nchanges, the overall performance of the evolved agents improves 50%. If more\nsignificant changes to the initial design are allowed, our experiments'\nperformance improves even more to 150%. Contrary to related work, our co-design\nworks on a single GPU and provides satisfactory results by training thousands\nof agents within one hour.",
    "descriptor": "",
    "authors": [
      "Zhiquan Wang",
      "Bedrich Benes",
      "Ahmed H. Qureshi",
      "Christos Mousas"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10688"
  },
  {
    "id": "arXiv:2205.10689",
    "title": "Diversity Preference-Aware Link Recommendation for Online Social  Networks",
    "abstract": "Link recommendation, which recommends links to connect unlinked online social\nnetwork users, is a fundamental social network analytics problem with ample\nbusiness implications. Existing link recommendation methods tend to recommend\nsimilar friends to a user but overlook the user's diversity preference,\nalthough social psychology theories suggest the criticality of diversity\npreference to link recommendation performance. In recommender systems, a field\nrelated to link recommendation, a number of diversification methods have been\nproposed to improve the diversity of recommended items. Nevertheless, diversity\npreference is distinct from diversity studied by diversification methods. To\naddress these research gaps, we define and operationalize the concept of\ndiversity preference for link recommendation and propose a new link\nrecommendation problem: the diversity preference-aware link recommendation\nproblem. We then analyze key properties of the new link recommendation problem\nand develop a novel link recommendation method to solve the problem. Using two\nlarge-scale online social network data sets, we conduct extensive empirical\nevaluations to demonstrate the superior performance of our method over\nrepresentative diversification methods adapted for link recommendation as well\nas state-of-the-art link recommendation methods.",
    "descriptor": "\nComments: 50 pages, 3 figures\n",
    "authors": [
      "Kexin Yin",
      "Xiao Fang",
      "Bintong Chen",
      "Olivia Sheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10689"
  },
  {
    "id": "arXiv:2205.10692",
    "title": "All You Need Is Logs: Improving Code Completion by Learning from  Anonymous IDE Usage Logs",
    "abstract": "Integrated Development Environments (IDE) are designed to make users more\nproductive, as well as to make their work more comfortable. To achieve this, a\nlot of diverse tools are embedded into IDEs, and the developers of IDEs can\nemploy anonymous usage logs to collect the data about how they are being used\nto improve them. A particularly important component that this can be applied to\nis code completion, since improving code completion using statistical learning\ntechniques is a well-established research area.\nIn this work, we propose an approach for collecting completion usage logs\nfrom the users in an IDE and using them to train a machine learning based model\nfor ranking completion candidates. We developed a set of features that describe\ncompletion candidates and their context, and deployed their anonymized\ncollection in the Early Access Program of IntelliJ-based IDEs. We used the logs\nto collect a dataset of code completions from users, and employed it to train a\nranking CatBoost model. Then, we evaluated it in two settings: on a held-out\nset of the collected completions and in a separate A/B test on two different\ngroups of users in the IDE. Our evaluation shows that using a simple ranking\nmodel trained on the past user behavior logs significantly improved code\ncompletion experience. Compared to the default heuristics-based ranking, our\nmodel demonstrated a decrease in the number of typing actions necessary to\nperform the completion in the IDE from 2.073 to 1.832.\nThe approach adheres to privacy requirements and legal constraints, since it\ndoes not require collecting personal information, performing all the necessary\nanonymization on the client's side. Importantly, it can be improved\ncontinuously: implementing new features, collecting new data, and evaluating\nnew models - this way, we have been using it in production since the end of\n2020.",
    "descriptor": "\nComments: 11 pages, 4 figures\n",
    "authors": [
      "Vitaliy Bibaev",
      "Alexey Kalina",
      "Vadim Lomshakov",
      "Yaroslav Golubev",
      "Alexander Bezzubov",
      "Nikita Povarov",
      "Timofey Bryksin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10692"
  },
  {
    "id": "arXiv:2205.10695",
    "title": "Evaluation of User Perception on Biometric Fingerprint System",
    "abstract": "Biometric systems involve security assurance to make our system highly\nsecured and robust. Nowadays, biometric technology has been fixed into new\nsystems with the aim of enforcing strong privacy and security. Several\ninnovative system have been introduced, and most of them have biometrics\ninstalled to protect military bases, banking machines, and other sophisticated\nsystems, such as online tracking systems. Businesses can now focus on their\ncore functions and feel confident about their data security. Despite the\nbenefits and enhancements in security that biometrics offer, there are also\nsome vulnerabilities. This study aimed to investigate the biometric\nvulnerabilities in a healthcare facility and propose possible countermeasures\nfor biometric system vulnerabilities.",
    "descriptor": "",
    "authors": [
      "Jones Yeboah",
      "Victor Adewopo",
      "Sylvia Azumah",
      "Izunna Okpala"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.10695"
  },
  {
    "id": "arXiv:2205.10696",
    "title": "Life after BERT: What do Other Muppets Understand about Language?",
    "abstract": "Existing pre-trained transformer analysis works usually focus only on one or\ntwo model families at a time, overlooking the variability of the architecture\nand pre-training objectives. In our work, we utilize the oLMpics benchmark and\npsycholinguistic probing datasets for a diverse set of 29 models including T5,\nBART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for\nautoregressive models and evaluate GPT networks of different sizes. Our\nfindings show that none of these models can resolve compositional questions in\na zero-shot fashion, suggesting that this skill is not learnable using existing\npre-training objectives. Furthermore, we find that global model decisions such\nas architecture, directionality, size of the dataset, and pre-training\nobjective are not predictive of a model's linguistic capabilities.",
    "descriptor": "",
    "authors": [
      "Vladislav Lialin",
      "Kevin Zhao",
      "Namrata Shivagunde",
      "Anna Rumshisky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10696"
  },
  {
    "id": "arXiv:2205.10702",
    "title": "An Analytical Range-Angle Dependent Beam Focusing Model for Terahertz  Linear Antenna Array",
    "abstract": "This paper considers a scenario in which the Terahertz (THz) transmitter\nequipped with a linear antenna array wishes to focus its beam to a desired\nspatial region in the array near-field. The goal is to compute the achievable\nspatial region and determine how the system parameters such as the carrier\nfrequency, the array dimension and the user's location affect its beam focusing\nperformance. First, based on a theorem from analytic geometry, we show that the\nachievable focusing spatial region constitutes a rotated ellipse, with the x\nand y coordinates denoting the range and angle, respectively. In this way, the\ndetermination of the spatial region is reduced to a problem of deriving the\ncoverage of an ellipse. The achievable coverage is then obtained in closed\nform, and the construction of carrier frequency offsets that can analytically\ncontrol the beam focusing performance is provided. Numerical results validate\nthe theoretical findings and demonstrate the performance of the proposed\nmethod.",
    "descriptor": "\nComments: 14 pages,3 figures\n",
    "authors": [
      "Lingxiang Li",
      "Haoran Li",
      "Zhi Chen",
      "Weixin Chen",
      "Shaoqian Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10702"
  },
  {
    "id": "arXiv:2205.10706",
    "title": "GL-RG: Global-Local Representation Granularity for Video Captioning",
    "abstract": "Video captioning is a challenging task as it needs to accurately transform\nvisual understanding into natural language description. To date,\nstate-of-the-art methods inadequately model global-local representation across\nvideo frames for caption generation, leaving plenty of room for improvement. In\nthis work, we approach the video captioning task from a new perspective and\npropose a GL-RG framework for video captioning, namely a\n\\textbf{G}lobal-\\textbf{L}ocal \\textbf{R}epresentation \\textbf{G}ranularity.\nOur GL-RG demonstrates three advantages over the prior efforts: 1) we\nexplicitly exploit extensive visual representations from different video ranges\nto improve linguistic expression; 2) we devise a novel global-local encoder to\nproduce rich semantic vocabulary to obtain a descriptive granularity of video\ncontents across frames; 3) we develop an incremental training strategy which\norganizes model learning in an incremental fashion to incur an optimal\ncaptioning behavior. Experimental results on the challenging MSR-VTT and MSVD\ndatasets show that our DL-RG outperforms recent state-of-the-art methods by a\nsignificant margin. Code is available at \\url{https://github.com/ylqi/GL-RG}.",
    "descriptor": "\nComments: Accepted to IJCAI 2022\n",
    "authors": [
      "Liqi Yan",
      "Qifan Wang",
      "Yiming Cui",
      "Fuli Feng",
      "Xiaojun Quan",
      "Xiangyu Zhang",
      "Dongfang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10706"
  },
  {
    "id": "arXiv:2205.10710",
    "title": "Phrase-level Textual Adversarial Attack with Label Preservation",
    "abstract": "Generating high-quality textual adversarial examples is critical for\ninvestigating the pitfalls of natural language processing (NLP) models and\nfurther promoting their robustness. Existing attacks are usually realized\nthrough word-level or sentence-level perturbations, which either limit the\nperturbation space or sacrifice fluency and textual quality, both affecting the\nattack effectiveness. In this paper, we propose Phrase-Level Textual\nAdversarial aTtack (PLAT) that generates adversarial samples through\nphrase-level perturbations. PLAT first extracts the vulnerable phrases as\nattack targets by a syntactic parser, and then perturbs them by a pre-trained\nblank-infilling model. Such flexible perturbation design substantially expands\nthe search space for more effective attacks without introducing too many\nmodifications, and meanwhile maintaining the textual fluency and grammaticality\nvia contextualized generation using surrounding texts. Moreover, we develop a\nlabel-preservation filter leveraging the likelihoods of language models\nfine-tuned on each class, rather than textual similarity, to rule out those\nperturbations that potentially alter the original class label for humans.\nExtensive experiments and human evaluation demonstrate that PLAT has a superior\nattack effectiveness as well as a better label consistency than strong\nbaselines.",
    "descriptor": "\nComments: 9 pages + 2 pages references + 8 pages appendix\n",
    "authors": [
      "Yibin Lei",
      "Yu Cao",
      "Dianqi Li",
      "Tianyi Zhou",
      "Meng Fang",
      "Mykola Pechenizkiy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10710"
  },
  {
    "id": "arXiv:2205.10711",
    "title": "Active Source Free Domain Adaptation",
    "abstract": "Source free domain adaptation (SFDA) aims to transfer a trained source model\nto the unlabeled target domain without accessing the source data. However, the\nSFDA setting faces an effect bottleneck due to the absence of source data and\ntarget supervised information, as evidenced by the limited performance gains of\nnewest SFDA methods. In this paper, for the first time, we introduce a more\npractical scenario called active source free domain adaptation (ASFDA) that\npermits actively selecting a few target data to be labeled by experts. To\nachieve that, we first find that those satisfying the properties of\nneighbor-chaotic, individual-different, and target-like are the best points to\nselect, and we define them as the minimum happy (MH) points. We then propose\nminimum happy points learning (MHPL) to actively explore and exploit MH points.\nWe design three unique strategies: neighbor ambient uncertainty, neighbor\ndiversity relaxation, and one-shot querying, to explore the MH points. Further,\nto fully exploit MH points in the learning process, we design a neighbor focal\nloss that assigns the weighted neighbor purity to the cross-entropy loss of MH\npoints to make the model focus more on them. Extensive experiments verify that\nMHPL remarkably exceeds the various types of baselines and achieves significant\nperformance gains at a small cost of labeling.",
    "descriptor": "\nComments: 9 pages (not including references and checklist), 4 figures,\n",
    "authors": [
      "Fan Wang",
      "Zhongyi Han",
      "Zhiyan Zhang",
      "Yilong Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10711"
  },
  {
    "id": "arXiv:2205.10712",
    "title": "Housekeep: Tidying Virtual Households using Commonsense Reasoning",
    "abstract": "We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the\nhome for embodied AI. In Housekeep, an embodied agent must tidy a house by\nrearranging misplaced objects without explicit instructions specifying which\nobjects need to be rearranged. Instead, the agent must learn from and is\nevaluated against human preferences of which objects belong where in a tidy\nhouse. Specifically, we collect a dataset of where humans typically place\nobjects in tidy and untidy houses constituting 1799 objects, 268 object\ncategories, 585 placements, and 105 rooms. Next, we propose a modular baseline\napproach for Housekeep that integrates planning, exploration, and navigation.\nIt leverages a fine-tuned large language model (LLM) trained on an internet\ntext corpus for effective planning. We show that our baseline agent generalizes\nto rearranging unseen objects in unknown environments. See our webpage for more\ndetails: https://yashkant.github.io/housekeep/",
    "descriptor": "",
    "authors": [
      "Yash Kant",
      "Arun Ramachandran",
      "Sriram Yenamandra",
      "Igor Gilitschenski",
      "Dhruv Batra",
      "Andrew Szot",
      "Harsh Agrawal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10712"
  },
  {
    "id": "arXiv:2205.10713",
    "title": "Understanding and Supporting the Design Systems Practice",
    "abstract": "Design systems represent a user interaction design and development approach\nthat is currently of avid interest in the industry. However, little research\nwork has been done to synthesize knowledge related to design systems in order\nto inform the design of tools to support their creation, maintenance, and usage\npractices. This paper represents an important step in which we explored the\nissues that design system projects usually deal with and the perceptions and\nvalues of design system project leaders. Through this exploration, we aim to\ninvestigate the needs for tools that support the design system approach. We\nfound that the open source communities around design systems focused on\ndiscussing issues related to behaviors of user interface components of design\nsystems. At the same time, leaders of design system projects faced considerable\nchallenges when evolving their design systems to make them both capable of\ncapturing stable design knowledge and flexible to the needs of the various\nconcrete products. They valued a bottom-up approach for design system creation\nand maintenance, in which components are elevated and merged from the evolving\nproducts. Our findings synthesize the knowledge and lay foundations for\ndesigning techniques and tools aimed at supporting the design system practice\nand related modern user interaction design and development approaches.",
    "descriptor": "\nComments: 28 pages, 2 figures, to appear in EMSE\n",
    "authors": [
      "Yassine Lamine",
      "Jinghui Cheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.10713"
  },
  {
    "id": "arXiv:2205.10714",
    "title": "Interpretable Proof Generation via Iterative Backward Reasoning",
    "abstract": "We present IBR, an Iterative Backward Reasoning model to solve the proof\ngeneration tasks on rule-based Question Answering (QA), where models are\nrequired to reason over a series of textual rules and facts to find out the\nrelated proof path and derive the final answer. We handle the limitations of\nexisted works in two folds: 1) enhance the interpretability of reasoning\nprocedures with detailed tracking, by predicting nodes and edges in the proof\npath iteratively backward from the question; 2) promote the efficiency and\naccuracy via reasoning on the elaborate representations of nodes and history\npaths, without any intermediate texts that may introduce external noise during\nproof generation. There are three main modules in IBR, QA and proof strategy\nprediction to obtain the answer and offer guidance for the following procedure;\nparent node prediction to determine a node in the existing proof that a new\nchild node will link to; child node prediction to find out which new node will\nbe added to the proof. Experiments on both synthetic and paraphrased datasets\ndemonstrate that IBR has better in-domain performance as well as cross-domain\ntransferability than several strong baselines. Our code and models are\navailable at https://github.com/find-knowledge/IBR .",
    "descriptor": "\nComments: 14 pages (2 page references + 3 page appendix)\n",
    "authors": [
      "Hanhao Qu",
      "Yu Cao",
      "Jun Gao",
      "Liang Ding",
      "Ruifeng Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10714"
  },
  {
    "id": "arXiv:2205.10715",
    "title": "Policy-based Primal-Dual Methods for Convex Constrained Markov Decision  Processes",
    "abstract": "We study convex Constrained Markov Decision Processes (CMDPs) in which the\nobjective is concave and the constraints are convex in the state-action\nvisitation distribution. We propose a policy-based primal-dual algorithm that\nupdates the primal variable via policy gradient ascent and updates the dual\nvariable via projected sub-gradient descent. Despite the loss of additivity\nstructure and the nonconvex nature, we establish the global convergence of the\nproposed algorithm by leveraging a hidden convexity in the problem under the\ngeneral soft-max parameterization, and prove the\n$\\mathcal{O}\\left(T^{-1/3}\\right)$ convergence rate in terms of both optimality\ngap and constraint violation. When the objective is strongly concave in the\nvisitation distribution, we prove an improved convergence rate of\n$\\mathcal{O}\\left(T^{-1/2}\\right)$. By introducing a pessimistic term to the\nconstraint, we further show that a zero constraint violation can be achieved\nwhile preserving the same convergence rate for the optimality gap. This work is\nthe first one in the literature that establishes non-asymptotic convergence\nguarantees for policy-based primal-dual methods for solving infinite-horizon\ndiscounted convex CMDPs.",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Donghao Ying",
      "Mengzi Guo",
      "Yuhao Ding",
      "Javad Lavaei",
      "Zuo-Jun",
      "Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.10715"
  },
  {
    "id": "arXiv:2205.10721",
    "title": "System-Level Evaluation of Beam Hopping in NR-Based LEO Satellite  Communication System",
    "abstract": "Satellite communication by leveraging the use of low earth orbit (LEO)\nsatellites is expected to play an essential role in future communication\nsystems through providing ubiquitous and continuous wireless connectivity. This\nthus has motivated the work in the 3rd generation partnership project (3GPP) to\nensure the operation of fifth generation (5G) New Radio (NR) protocols for\nnon-terrestrial network (NTN). In this paper, we consider a NR-based LEO\nsatellite communication system, where satellites equipped with phased array\nantennas are employed to serve user equipments (UEs) on the ground. To reduce\npayload weight and meet the time-varying traffic demands of UEs, an efficient\nbeam hopping scheme considering both the traffic demands and inter-beam\ninterference is proposed to jointly schedule beams and satellite transmit\npower. Then based on NR protocols, we present the first system-level\nevaluations of beam hopping scheme in LEO satellite system under different\noperating frequency bands and traffic models. Simulation results indicate that\nsignificant performance gains can be achieved by the proposed beam hopping\nscheme, especially under the distance limit constraint that avoids scheduling\nadjacent beams simultaneously, as compared to benchmark schemes.",
    "descriptor": "\nComments: 6 pages, 13 figures\n",
    "authors": [
      "Jingwei Zhang",
      "Dali Qin",
      "Chuili Kong",
      "Feiran Zhao",
      "Rong Li",
      "Jun Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10721"
  },
  {
    "id": "arXiv:2205.10724",
    "title": "Learnable Visual Words for Interpretable Image Recognition",
    "abstract": "To interpret deep models' predictions, attention-based visual cues are widely\nused in addressing \\textit{why} deep models make such predictions. Beyond that,\nthe current research community becomes more interested in reasoning\n\\textit{how} deep models make predictions, where some prototype-based methods\nemploy interpretable representations with their corresponding visual cues to\nreveal the black-box mechanism of deep model behaviors. However, these\npioneering attempts only either learn the category-specific prototypes and\ndeteriorate their generalizing capacities, or demonstrate several illustrative\nexamples without a quantitative evaluation of visual-based interpretability\nwith further limitations on their practical usages. In this paper, we revisit\nthe concept of visual words and propose the Learnable Visual Words (LVW) to\ninterpret the model prediction behaviors with two novel modules: semantic\nvisual words learning and dual fidelity preservation. The semantic visual words\nlearning relaxes the category-specific constraint, enabling the general visual\nwords shared across different categories. Beyond employing the visual words for\nprediction to align visual words with the base model, our dual fidelity\npreservation also includes the attention guided semantic alignment that\nencourages the learned visual words to focus on the same conceptual regions for\nprediction. Experiments on six visual benchmarks demonstrate the superior\neffectiveness of our proposed LVW in both accuracy and model interpretation\nover the state-of-the-art methods. Moreover, we elaborate on various in-depth\nanalyses to further explore the learned visual words and the generalizability\nof our method for unseen categories.",
    "descriptor": "",
    "authors": [
      "Wenxiao Xiao",
      "Zhengming Ding",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10724"
  },
  {
    "id": "arXiv:2205.10726",
    "title": "TWEET-FID: An Annotated Dataset for Multiple Foodborne Illness Detection  Tasks",
    "abstract": "Foodborne illness is a serious but preventable public health problem -- with\ndelays in detecting the associated outbreaks resulting in productivity loss,\nexpensive recalls, public safety hazards, and even loss of life. While social\nmedia is a promising source for identifying unreported foodborne illnesses,\nthere is a dearth of labeled datasets for developing effective outbreak\ndetection models. To accelerate the development of machine learning-based\nmodels for foodborne outbreak detection, we thus present TWEET-FID\n(TWEET-Foodborne Illness Detection), the first publicly available annotated\ndataset for multiple foodborne illness incident detection tasks. TWEET-FID\ncollected from Twitter is annotated with three facets: tweet class, entity\ntype, and slot type, with labels produced by experts as well as by crowdsource\nworkers. We introduce several domain tasks leveraging these three facets: text\nrelevance classification (TRC), entity mention detection (EMD), and slot\nfilling (SF). We describe the end-to-end methodology for dataset design,\ncreation, and labeling for supporting model development for these tasks. A\ncomprehensive set of results for these tasks leveraging state-of-the-art\nsingle- and multi-task deep learning methods on the TWEET-FID dataset are\nprovided. This dataset opens opportunities for future research in foodborne\noutbreak detection.",
    "descriptor": "\nComments: LREC 2022\n",
    "authors": [
      "Ruofan Hu",
      "Dongyu Zhang",
      "Dandan Tao",
      "Thomas Hartvigsen",
      "Hao Feng",
      "Elke Rundensteiner"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10726"
  },
  {
    "id": "arXiv:2205.10728",
    "title": "Neural Lyapunov Differentiable Predictive Control",
    "abstract": "We present a learning-based predictive control methodology using the\ndifferentiable programming framework with probabilistic Lyapunov-based\nstability guarantees. The neural Lyapunov differentiable predictive control\n(NLDPC) learns the policy by constructing a computational graph encompassing\nthe system dynamics, state and input constraints, and the necessary Lyapunov\ncertification constraints, and thereafter using the automatic differentiation\nto update the neural policy parameters. In conjunction, our approach jointly\nlearns a Lyapunov function that certifies the regions of state-space with\nstable dynamics. We also provide a sampling-based statistical guarantee for the\ntraining of NLDPC from the distribution of initial conditions. Our offline\ntraining approach provides a computationally efficient and scalable alternative\nto classical explicit model predictive control solutions. We substantiate the\nadvantages of the proposed approach with simulations to stabilize the double\nintegrator model and on an example of controlling an aircraft model.",
    "descriptor": "\nComments: 8 pages; 9 figures\n",
    "authors": [
      "Sayak Mukherjee",
      "J\u00e1n Drgo\u0148a",
      "Aaron Tuor",
      "Mahantesh Halappanavar",
      "Draguna Vrabie"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10728"
  },
  {
    "id": "arXiv:2205.10729",
    "title": "Near-Optimal Algorithms for Autonomous Exploration and Multi-Goal  Stochastic Shortest Path",
    "abstract": "We revisit the incremental autonomous exploration problem proposed by Lim &\nAuer (2012). In this setting, the agent aims to learn a set of near-optimal\ngoal-conditioned policies to reach the $L$-controllable states: states that are\nincrementally reachable from an initial state $s_0$ within $L$ steps in\nexpectation. We introduce a new algorithm with stronger sample complexity\nbounds than existing ones. Furthermore, we also prove the first lower bound for\nthe autonomous exploration problem. In particular, the lower bound implies that\nour proposed algorithm, Value-Aware Autonomous Exploration, is nearly\nminimax-optimal when the number of $L$-controllable states grows polynomially\nwith respect to $L$. Key in our algorithm design is a connection between\nautonomous exploration and multi-goal stochastic shortest path, a new problem\nthat naturally generalizes the classical stochastic shortest path problem. This\nnew problem and its connection to autonomous exploration can be of independent\ninterest.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Haoyuan Cai",
      "Tengyu Ma",
      "Simon Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10729"
  },
  {
    "id": "arXiv:2205.10733",
    "title": "GraB: Finding Provably Better Data Permutations than Random Reshuffling",
    "abstract": "Random reshuffling, which randomly permutes the dataset each epoch, is widely\nadopted in model training because it yields faster convergence than\nwith-replacement sampling. Recent studies indicate greedily chosen data\norderings can further speed up convergence empirically, at the cost of using\nmore computation and memory. However, greedy ordering lacks theoretical\njustification and has limited utility due to its non-trivial memory and\ncomputation overhead. In this paper, we first formulate an example-ordering\nframework named herding and answer affirmatively that SGD with herding\nconverges at the rate $O(T^{-2/3})$ on smooth, non-convex objectives, faster\nthan the $O(n^{1/3}T^{-2/3})$ obtained by random reshuffling, where $n$ denotes\nthe number of data points and $T$ denotes the total number of iterations. To\nreduce the memory overhead, we leverage discrepancy minimization theory to\npropose an online Gradient Balancing algorithm (GraB) that enjoys the same rate\nas herding, while reducing the memory usage from $O(nd)$ to just $O(d)$ and\ncomputation from $O(n^2)$ to $O(n)$, where $d$ denotes the model dimension. We\nshow empirically on applications including MNIST, CIFAR10, WikiText and GLUE\nthat GraB can outperform random reshuffling in terms of both training and\nvalidation performance, and even outperform state-of-the-art greedy ordering\nwhile reducing memory usage over $100\\times$.",
    "descriptor": "",
    "authors": [
      "Yucheng Lu",
      "Wentao Guo",
      "Christopher De Sa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10733"
  },
  {
    "id": "arXiv:2205.10734",
    "title": "Limit Cycles Analysis and Control of Evolutionary Game Dynamics with  Environmental Feedback",
    "abstract": "Recently, an evolutionary game dynamics model taking into account the\nenvironmental feedback has been proposed to describe the co-evolution of\nstrategic actions of a population of individuals and the state of the\nsurrounding environment; correspondingly a range of interesting dynamic\nbehaviors have been reported. In this paper, we provide new theoretical insight\ninto such behaviors and discuss control options. Instead of the standard\nreplicator dynamics, we use a more realistic and comprehensive model of\nreplicator-mutator dynamics, to describe the strategic evolution of the\npopulation. After integrating the environment feedback, we study the effect of\nmutations on the resulting closed-loop system dynamics. We prove the conditions\nfor two types of bifurcations, Hopf bifurcation and Heteroclinic bifurcation,\nboth of which result in stable limit cycles. These limit cycles have not been\nidentified in existing works, and we further prove that such limit cycles are\nin fact persistent in a large parameter space and are almost globally stable.\nIn the end, an intuitive control policy based on incentives is applied, and the\neffectiveness of this control policy is examined by analysis and simulations.",
    "descriptor": "\nComments: 19 pages, 6 figures\n",
    "authors": [
      "Lulu Gong",
      "Weijia Yao",
      "Jian Gao",
      "Ming Cao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.10734"
  },
  {
    "id": "arXiv:2205.10736",
    "title": "Should Models Be Accurate?",
    "abstract": "Model-based Reinforcement Learning (MBRL) holds promise for data-efficiency\nby planning with model-generated experience in addition to learning with\nexperience from the environment. However, in complex or changing environments,\nmodels in MBRL will inevitably be imperfect, and their detrimental effects on\nlearning can be difficult to mitigate. In this work, we question whether the\nobjective of these models should be the accurate simulation of environment\ndynamics at all. We focus our investigations on Dyna-style planning in a\nprediction setting. First, we highlight and support three motivating points: a\nperfectly accurate model of environment dynamics is not practically achievable,\nis not necessary, and is not always the most useful anyways. Second, we\nintroduce a meta-learning algorithm for training models with a focus on their\nusefulness to the learner instead of their accuracy in modelling the\nenvironment. Our experiments show that in a simple non-stationary environment,\nour algorithm enables faster learning than even using an accurate model built\nwith domain-specific knowledge of the non-stationarity.",
    "descriptor": "\nComments: The 5th Multidisciplinary Conference on Reinforcement Learning and Decision Making ( RLDM 2022 )\n",
    "authors": [
      "Esra'a Saleh",
      "John D. Martin",
      "Anna Koop",
      "Arash Pourzarabi",
      "Michael Bowling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10736"
  },
  {
    "id": "arXiv:2205.10737",
    "title": "ALITA: A Large-scale Incremental Dataset for Long-term Autonomy",
    "abstract": "For long-term autonomy, most place recognition methods are mainly evaluated\non simplified scenarios or simulated datasets, which cannot provide solid\nevidence to evaluate the readiness for current Simultaneous Localization and\nMapping (SLAM). In this paper, we present a long-term place recognition dataset\nfor use in mobile localization under large-scale dynamic environments. This\ndataset includes a campus-scale track and a city-scale track: 1) the\ncampus-track focuses the long-term property, we record LiDAR device and an\nomnidirectional camera on 10 trajectories, and each trajectory are repeatly\nrecorded 8 times under variant illumination conditions. 2) the city-track\nfocuses the large-scale property, we mount the LiDAR device on the vehicle and\ntraversing through a 120km trajectories, which contains open streets,\nresidential areas, natural terrains, etc. They includes 200 hours of raw data\nof all kinds scenarios within urban environments. The ground truth position for\nboth tracks are provided on each trajectory, which is obtained from the Global\nPosition System with an additional General ICP based point cloud refinement. To\nsimplify the evaluation procedure, we also provide the Python-API with a set of\nplace recognition metrics is proposed to quickly load our dataset and evaluate\nthe recognition performance against different methods. This dataset targets at\nfinding methods with high place recognition accuracy and robustness, and\nproviding real robotic system with long-term autonomy. The dataset and the\nprovided tools can be accessed from https://github.com/MetaSLAM/ALITA.",
    "descriptor": "\nComments: 4 pages, 2 figures\n",
    "authors": [
      "Peng Yin",
      "Shiqi Zhao",
      "Ruohai Ge",
      "Ivan Cisneros",
      "Ruijie Fu",
      "Ji Zhang",
      "Howie Choset",
      "Sebastian Scherer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10737"
  },
  {
    "id": "arXiv:2205.10738",
    "title": "OTAdapt: Optimal Transport-based Approach For Unsupervised Domain  Adaptation",
    "abstract": "Unsupervised domain adaptation is one of the challenging problems in computer\nvision. This paper presents a novel approach to unsupervised domain adaptations\nbased on the optimal transport-based distance. Our approach allows aligning\ntarget and source domains without the requirement of meaningful metrics across\ndomains. In addition, the proposal can associate the correct mapping between\nsource and target domains and guarantee a constraint of topology between source\nand target domains. The proposed method is evaluated on different datasets in\nvarious problems, i.e. (i) digit recognition on MNIST, MNIST-M, USPS datasets,\n(ii) Object recognition on Amazon, Webcam, DSLR, and VisDA datasets, (iii)\nInsect Recognition on the IP102 dataset. The experimental results show that our\nproposed method consistently improves performance accuracy. Also, our framework\ncould be incorporated with any other CNN frameworks within an end-to-end deep\nnetwork design for recognition problems to improve their performance.",
    "descriptor": "\nComments: Accepted to ICPR 2022\n",
    "authors": [
      "Thanh-Dat Truong",
      "Naga Venkata Sai Raviteja Chappa",
      "Xuan Bac Nguyen",
      "Ngan Le",
      "Ashley Dowling",
      "Khoa Luu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10738"
  },
  {
    "id": "arXiv:2205.10739",
    "title": "Offline Policy Comparison with Confidence: Benchmarks and Baselines",
    "abstract": "Decision makers often wish to use offline historical data to compare\nsequential-action policies at various world states. Importantly, computational\ntools should produce confidence values for such offline policy comparison (OPC)\nto account for statistical variance and limited data coverage. Nevertheless,\nthere is little work that directly evaluates the quality of confidence values\nfor OPC. In this work, we address this issue by creating benchmarks for OPC\nwith Confidence (OPCC), derived by adding sets of policy comparison queries to\ndatasets from offline reinforcement learning. In addition, we present an\nempirical evaluation of the risk versus coverage trade-off for a class of\nmodel-based baselines. In particular, the baselines learn ensembles of dynamics\nmodels, which are used in various ways to produce simulations for answering\nqueries with confidence values. While our results suggest advantages for\ncertain baseline variations, there appears to be significant room for\nimprovement in future work.",
    "descriptor": "",
    "authors": [
      "Anurag Koul",
      "Mariano Phielipp",
      "Alan Fern"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10739"
  },
  {
    "id": "arXiv:2205.10741",
    "title": "Exploiting Constructive Interference for Backscatter Communication  Systems",
    "abstract": "Backscatter communication (BackCom), one of the core technologies to realize\nzero-power communication, is expected to be a pivotal paradigm for the next\ngeneration of the Internet of Things (IoT). However, the \"strong\" direct link\n(DL) interference (DLI) is traditionally assumed to be harmful, and generally\ndrowns out the \"weak\" backscattered signals accordingly, thus deteriorating the\nperformance of BackCom. In contrast to the previous efforts to eliminate the\nDLI, in this paper, we exploit the constructive interference (CI), in which the\nDLI contributes to the backscattered signal. To be specific, our objective is\nto maximize the received signal power by jointly optimizing the receive\nbeamforming vectors and tag selection factors, which is, however, non-convex\nand difficult to solve due to constraints on the Kullback-Leibler (KL)\ndivergence. In order to solve this problem, we first decompose the original\nproblem, and then propose two algorithms to solve the sub-problem with\nbeamforming design via a change of variables and semi-definite programming\n(SDP) and a greedy algorithm to solve the sub-problem with tag selection. In\norder to gain insight into the CI, we consider a special case with the\nsingle-antenna reader to reveal the channel angle between the backscattering\nlink (BL) and the DL, in which the DLI will become constructive. Simulation\nresults show that significant performance gain can always be achieved in the\nproposed algorithms compared with the traditional algorithms without the DL in\nterms of the strength of the received signal. The derived constructive channel\nangle for the BackCom system with the single-antenna reader is also confirmed\nby simulation results.",
    "descriptor": "",
    "authors": [
      "Gu Bowen",
      "Li Dong",
      "Liu Ye",
      "Xu Yongjun"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10741"
  },
  {
    "id": "arXiv:2205.10743",
    "title": "Do Deep Learning Models and News Headlines Outperform Conventional  Prediction Techniques on Forex Data?",
    "abstract": "Foreign Exchange (FOREX) is a decentralised global market for exchanging\ncurrencies. The Forex market is enormous, and it operates 24 hours a day. Along\nwith country-specific factors, Forex trading is influenced by cross-country\nties and a variety of global events. Recent pandemic scenarios such as COVID19\nand local elections can also have a significant impact on market pricing. We\ntested and compared various predictions with external elements such as news\nitems in this work. Additionally, we compared classical machine learning\nmethods to deep learning algorithms. We also added sentiment features from news\nheadlines using NLP-based word embeddings and compared the performance. Our\nresults indicate that simple regression model like linear, SGD, and Bagged\nperformed better than deep learning models such as LSTM and RNN for single-step\nforecasting like the next two hours, the next day, and seven days.\nSurprisingly, news articles failed to improve the predictions indicating\ndomain-based and relevant information only adds value. Among the text\nvectorization techniques, Word2Vec and SentenceBERT perform better.",
    "descriptor": "\nComments: Accepted at ICADCML 2022\n",
    "authors": [
      "Sucharita Atha",
      "Bharath Kumar Bolla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10743"
  },
  {
    "id": "arXiv:2205.10744",
    "title": "All Birds with One Stone: Multi-task Text Classification for Efficient  Inference with One Forward Pass",
    "abstract": "Multi-Task Learning (MTL) models have shown their robustness, effectiveness,\nand efficiency for transferring learned knowledge across tasks. In real\nindustrial applications such as web content classification, multiple\nclassification tasks are predicted from the same input text such as a web\narticle. However, at the serving time, the existing multitask transformer\nmodels such as prompt or adaptor based approaches need to conduct N forward\npasses for N tasks with O(N) computation cost. To tackle this problem, we\npropose a scalable method that can achieve stronger performance with close to\nO(1) computation cost via only one forward pass. To illustrate real application\nusage, we release a multitask dataset on news topic and style classification.\nOur experiments show that our proposed method outperforms strong baselines on\nboth the GLUE benchmark and our news dataset. Our code and dataset are publicly\navailable at https://bit.ly/mtop-code.",
    "descriptor": "",
    "authors": [
      "Jiaxin Huang",
      "Tianqi Liu",
      "Jialu Liu",
      "Adam D. Lelkes",
      "Cong Yu",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10744"
  },
  {
    "id": "arXiv:2205.10745",
    "title": "Classification of Quasars, Galaxies, and Stars in the Mapping of the  Universe Multi-modal Deep Learning",
    "abstract": "In this paper, the fourth version the Sloan Digital Sky Survey (SDSS-4), Data\nRelease 16 dataset was used to classify the SDSS dataset into galaxies, stars,\nand quasars using machine learning and deep learning architectures. We\nefficiently utilize both image and metadata in tabular format to build a novel\nmulti-modal architecture and achieve state-of-the-art results. In addition, our\nexperiments on transfer learning using Imagenet weights on five different\narchitectures (Resnet-50, DenseNet-121 VGG-16, Xception, and EfficientNet)\nreveal that freezing all layers and adding a final trainable layer may not be\nan optimal solution for transfer learning. It is hypothesized that higher the\nnumber of trainable layers, higher will be the training time and accuracy of\npredictions. It is also hypothesized that any subsequent increase in the number\nof training layers towards the base layers will not increase in accuracy as the\npre trained lower layers only help in low level feature extraction which would\nbe quite similar in all the datasets. Hence the ideal level of trainable layers\nneeds to be identified for each model in respect to the number of parameters.\nFor the tabular data, we compared classical machine learning algorithms\n(Logistic Regression, Random Forest, Decision Trees, Adaboost, LightGBM etc.,)\nwith artificial neural networks. Our works shed new light on transfer learning\nand multi-modal deep learning architectures. The multi-modal architecture not\nonly resulted in higher metrics (accuracy, precision, recall, F1 score) than\nmodels using only image data or tabular data. Furthermore, multi-modal\narchitecture achieved the best metrics in lesser training epochs and improved\nthe metrics on all classes.",
    "descriptor": "\nComments: Presented at Deep Learning Developers Conference, 2021, Bangalore\n",
    "authors": [
      "Sabeesh Ethiraj",
      "Bharath Kumar Bolla"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10745"
  },
  {
    "id": "arXiv:2205.10747",
    "title": "Language Models with Image Descriptors are Strong Few-Shot  Video-Language Learners",
    "abstract": "The goal of this work is to build flexible video-language models that can\ngeneralize to various video-to-text tasks from few examples, such as\ndomain-specific captioning, question answering, and future event prediction.\nExisting few-shot video-language learners focus exclusively on the encoder,\nresulting in the absence of a video-to-text decoder to handle generative tasks.\nVideo captioners have been pretrained on large-scale video-language datasets,\nbut they rely heavily on finetuning and lack the ability to generate text for\nunseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language\nLearner via Image and Language models, which demonstrates strong performance on\nfew-shot video-to-text tasks without the necessity of pretraining or finetuning\non any video datasets. We use the image-language models to translate the video\ncontent into frame captions, object, attribute, and event phrases, and compose\nthem into a temporal structure template. We then instruct a language model,\nwith a prompt containing a few in-context examples, to generate a target output\nfrom the composed content. The flexibility of prompting allows the model to\ncapture any form of text input, such as automatic speech recognition (ASR)\ntranscripts. Our experiments demonstrate the power of language models in\nunderstanding videos on a wide variety of video-language tasks, including video\ncaptioning, video question answering, video caption retrieval, and video future\nevent prediction. Especially, on video future event prediction, our few-shot\nmodel significantly outperforms state-of-the-art supervised models trained on\nlarge-scale video datasets.",
    "descriptor": "",
    "authors": [
      "Zhenhailong Wang",
      "Manling Li",
      "Ruochen Xu",
      "Luowei Zhou",
      "Jie Lei",
      "Xudong Lin",
      "Shuohang Wang",
      "Ziyi Yang",
      "Chenguang Zhu",
      "Derek Hoiem",
      "Shih-Fu Chang",
      "Mohit Bansal",
      "Heng Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10747"
  },
  {
    "id": "arXiv:2205.10748",
    "title": "Preparing data for pathological artificial intelligence with  clinical-grade performance",
    "abstract": "[Purpose] The pathology is decisive for disease diagnosis, but relies heavily\non the experienced pathologists. Recently, pathological artificial intelligence\n(PAI) is thought to improve diagnostic accuracy and efficiency. However, the\nhigh performance of PAI based on deep learning in the laboratory generally\ncannot be reproduced in the clinic. [Methods] Because the data preparation is\nimportant for PAI, the paper has reviewed PAI-related studies in the PubMed\ndatabase published from January 2017 to February 2022, and 118 studies were\nincluded. The in-depth analysis of methods for preparing data is performed,\nincluding obtaining slides of pathological tissue, cleaning, screening, and\nthen digitizing. Expert review, image annotation, dataset division for model\ntraining and validation are also discussed. We further discuss the reasons why\nthe high performance of PAI is not reproducible in the clinical practices and\nshow some effective ways to improve clinical performances of PAI. [Results] The\nrobustness of PAI depend on randomized collection of representative disease\nslides, including rigorous quality control and screening, correction of digital\ndiscrepancies, reasonable annotation, and the amount of data. The digital\npathology is fundamental of clinical-grade PAI, and the techniques of data\nstandardization and weakly supervised learning methods based on whole slide\nimage (WSI) are effective ways to overcome obstacles of performance\nreproduction. [Conclusion] The representative data, the amount of labeling and\nconsistency from multi-centers is the key to performance reproduction. The\ndigital pathology for clinical diagnosis, data standardization and technique of\nWSI-based weakly supervised learning hopefully build clinical-grade PAI.\nKeywords: pathological artificial intelligence; data preparation;\nclinical-grade; deep learning",
    "descriptor": "",
    "authors": [
      "Yuanqing Yang",
      "Kai Sun",
      "Yanhua Gao",
      "Kuangsong Wang",
      "Gang Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10748"
  },
  {
    "id": "arXiv:2205.10749",
    "title": "Vanishing Spaces of Random Sets and Applications to Reed-Muller Codes",
    "abstract": "We study the following natural question on random sets of points in\n$\\mathbb{F}_2^m$: Given a random set of $k$ points $Z=\\{z_1, z_2, \\dots, z_k\\}\n\\subseteq \\mathbb{F}_2^m$, what is the dimension of the space of degree at most\n$r$ multilinear polynomials that vanish on all points in $Z$?\nWe show that, for $r \\leq \\gamma m$ (where $\\gamma > 0$ is a small, absolute\nconstant) and $k = (1-\\epsilon) \\cdot \\binom{m}{\\leq r}$ for any constant\n$\\epsilon > 0$, the space of degree at most $r$ multilinear polynomials\nvanishing on a random set $Z = \\{z_1,\\ldots, z_k\\}$ has dimension exactly\n$\\binom{m}{\\leq r} - k$ with probability $1 - o(1)$. This bound shows that\nrandom sets have a much smaller space of degree at most $r$ multilinear\npolynomials vanishing on them, compared to the worst-case bound (due to Wei\n(IEEE Trans. Inform. Theory, 1991)) of $\\binom{m}{\\leq r} - \\binom{\\log_2\nk}{\\leq r} \\gg \\binom{m}{\\leq r} - k$.\nUsing this bound, we show that high-degree Reed-Muller codes\n($\\text{RM}(m,d)$ with $d > (1-\\gamma) m$) \"achieve capacity\" under the Binary\nErasure Channel in the sense that, for any $\\epsilon > 0$, we can recover from\n$(1 - \\epsilon) \\cdot \\binom{m}{\\leq m-d-1}$ random erasures with probability\n$1 - o(1)$. This also implies that $\\text{RM}(m,d)$ is also efficiently\ndecodable from $\\approx \\binom{m}{\\leq m-(d/2)}$ random errors for the same\nrange of parameters.",
    "descriptor": "\nComments: 17 pages, In CCC'2022\n",
    "authors": [
      "Siddharth Bhandari",
      "Prahladh Harsha",
      "Ramprasad Saptharishi",
      "Srikanth Srinivasan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.10749"
  },
  {
    "id": "arXiv:2205.10750",
    "title": "Multi-Agent Feedback Enabled Neural Networks for Intelligent  Communications",
    "abstract": "In the intelligent communication field, deep learning (DL) has attracted much\nattention due to its strong fitting ability and data-driven learning\ncapability. Compared with the typical DL feedforward network structures, an\nenhancement structure with direct data feedback have been studied and proved to\nhave better performance than the feedfoward networks. However, due to the above\nsimple feedback methods lack sufficient analysis and learning ability on the\nfeedback data, it is inadequate to deal with more complicated nonlinear systems\nand therefore the performance is limited for further improvement. In this\npaper, a novel multi-agent feedback enabled neural network (MAFENN) framework\nis proposed, which make the framework have stronger feedback learning\ncapabilities and more intelligence on feature abstraction, denoising or\ngeneration, etc. Furthermore, the MAFENN framework is theoretically formulated\ninto a three-player Feedback Stackelberg game, and the game is proved to\nconverge to the Feedback Stackelberg equilibrium. The design of MAFENN\nframework and algorithm are dedicated to enhance the learning capability of the\nfeedfoward DL networks or their variations with the simple data feedback. To\nverify the MAFENN framework's feasibility in wireless communications, a\nmulti-agent MAFENN based equalizer (MAFENN-E) is developed for wireless fading\nchannels with inter-symbol interference (ISI). Experimental results show that\nwhen the quadrature phase-shift keying (QPSK) modulation scheme is adopted, the\nSER performance of our proposed method outperforms that of the traditional\nequalizers by about 2 dB in linear channels. When in nonlinear channels, the\nSER performance of our proposed method outperforms that of either traditional\nor DL based equalizers more significantly, which shows the effectiveness and\nrobustness of our proposal in the complex channel environment.",
    "descriptor": "",
    "authors": [
      "Fanglei Sun",
      "Yang Li",
      "Ying Wen",
      "Jingchen Hu",
      "Jun Wang",
      "Yang Yang",
      "Kai Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10750"
  },
  {
    "id": "arXiv:2205.10752",
    "title": "Covariance Matrix Adaptation MAP-Annealing",
    "abstract": "Single-objective optimization algorithms search for the single\nhighest-quality solution with respect to an objective. In contrast, quality\ndiversity (QD) optimization algorithms, such as Covariance Matrix Adaptation\nMAP-Elites (CMA-ME), search for a collection of solutions that are both\nhigh-quality with respect to an objective and diverse with respect to specified\nmeasure functions. We propose a new quality diversity algorithm, Covariance\nMatrix Adaptation MAP-Annealing (CMA-MAE), which bridges the gap between\nsingle-objective optimization and QD optimization. We prove that CMA-MAE\nsmoothly blends between the Covariance Matrix Adaptation Evolution Strategy\n(CMA-ES) single-objective optimizer and CMA-ME by gradually annealing a\ndiscount function with a scalar learning rate. We show that CMA-MAE has better\nperformance than the current state-of-the-art QD algorithms on several\nbenchmark domains and that its performance is empirically invariant to the\narchive resolution and robust to the discount function learning rate.",
    "descriptor": "",
    "authors": [
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10752"
  },
  {
    "id": "arXiv:2205.10756",
    "title": "Real Time Detection Free Tracking of Multiple Objects Via Equilibrium  Optimizer",
    "abstract": "Multiple objects tracking (MOT) is a difficult task, as it usually requires\nspecial hardware and higher computation complexity. In this work, we present a\nnew framework of MOT by using of equilibrium optimizer (EO) algorithm and\nreducing the resolution of the bounding boxes of the objects to solve such\nproblems in the detection free framework. First, in the first frame the target\nobjects are initialized and its size is computed, then its resolution is\nreduced if it is higher than a threshold, and then modeled by their kernel\ncolor histogram to establish a feature model. The Bhattacharya distances\nbetween the histogram of object models and other candidates are used as the\nfitness function to be optimized. Multiple agents are generated by EO,\naccording to the number of the target objects to be tracked. EO algorithm is\nused because of its efficiency and lower computation cost compared to other\nalgorithms in global optimization. Experimental results confirm that EO\nmulti-object tracker achieves satisfying tracking results then other trackers.",
    "descriptor": "",
    "authors": [
      "Djemai Charef-Khodja",
      "Toumi Abida"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10756"
  },
  {
    "id": "arXiv:2205.10759",
    "title": "Sequential/Session-based Recommendations: Challenges, Approaches,  Applications and Opportunities",
    "abstract": "In recent years, sequential recommender systems (SRSs) and session-based\nrecommender systems (SBRSs) have emerged as a new paradigm of RSs to capture\nusers' short-term but dynamic preferences for enabling more timely and accurate\nrecommendations. Although SRSs and SBRSs have been extensively studied, there\nare many inconsistencies in this area caused by the diverse descriptions,\nsettings, assumptions and application domains. There is no work to provide a\nunified framework and problem statement to remove the commonly existing and\nvarious inconsistencies in the area of SR/SBR. There is a lack of work to\nprovide a comprehensive and systematic demonstration of the data\ncharacteristics, key challenges, most representative and state-of-the-art\napproaches, typical real-world applications and important future research\ndirections in the area. This work aims to fill in these gaps so as to\nfacilitate further research in this exciting and vibrant area.",
    "descriptor": "",
    "authors": [
      "Shoujin Wang",
      "Qi Zhang",
      "Liang Hu",
      "Xiuzhen Zhang",
      "Yan Wang",
      "Charu Aggarwal"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10759"
  },
  {
    "id": "arXiv:2205.10760",
    "title": "CNNs are Myopic",
    "abstract": "We claim that Convolutional Neural Networks (CNNs) learn to classify images\nusing only small seemingly unrecognizable tiles. We show experimentally that\nCNNs trained only using such tiles can match or even surpass the performance of\nCNNs trained on full images. Conversely, CNNs trained on full images show\nsimilar predictions on small tiles. We also propose the first a priori\ntheoretical model for convolutional data sets that seems to explain this\nbehavior. This gives additional support to the long standing suspicion that\nCNNs do not need to understand the global structure of images to achieve\nstate-of-the-art accuracies. Surprisingly it also suggests that over-fitting is\nnot needed either.",
    "descriptor": "",
    "authors": [
      "Vamshi C. Madala",
      "Shivkumar Chandrasekaran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10760"
  },
  {
    "id": "arXiv:2205.10762",
    "title": "How sensitive are translation systems to extra contexts? Mitigating  gender bias in Neural Machine Translation models through relevant contexts",
    "abstract": "Neural Machine Translation systems built on top of Transformer-based\narchitectures are routinely improving the state-of-the-art in translation\nquality according to word-overlap metrics. However, a growing number of studies\nalso highlight the inherent gender bias that these models incorporate during\ntraining, which reflects poorly in their translations. In this work, we\ninvestigate whether these models can be instructed to fix their bias during\ninference using targeted, guided instructions as contexts. By translating\nrelevant contextual sentences during inference along with the input, we observe\nlarge improvements in reducing the gender bias in translations, across three\npopular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric\nto assess several large pretrained models (OPUS-MT, M2M-100) on their\nsensitivity towards using contexts during translation to correct their biases.\nOur approach requires no fine-tuning, and thus can be used easily in production\nsystems to de-bias translations from stereotypical gender-occupation bias. We\nhope our method, along with our metric, can be used to build better, bias-free\ntranslation systems.",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Shanya Sharma",
      "Manan Dey",
      "Koustuv Sinha"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10762"
  },
  {
    "id": "arXiv:2205.10763",
    "title": "A Deep Gradient Correction Method for Iteratively Solving Linear Systems",
    "abstract": "We present a novel deep learning approach to approximate the solution of\nlarge, sparse, symmetric, positive-definite linear systems of equations. These\nsystems arise from many problems in applied science, e.g., in numerical methods\nfor partial differential equations. Algorithms for approximating the solution\nto these systems are often the bottleneck in problems that require their\nsolution, particularly for modern applications that require many millions of\nunknowns. Indeed, numerical linear algebra techniques have been investigated\nfor many decades to alleviate this computational burden. Recently, data-driven\ntechniques have also shown promise for these problems. Motivated by the\nconjugate gradients algorithm that iteratively selects search directions for\nminimizing the matrix norm of the approximation error, we design an approach\nthat utilizes a deep neural network to accelerate convergence via data-driven\nimprovement of the search directions. Our method leverages a carefully chosen\nconvolutional network to approximate the action of the inverse of the linear\noperator up to an arbitrary constant. We train the network using unsupervised\nlearning with a loss function equal to the $L^2$ difference between an input\nand the system matrix times the network evaluation, where the unspecified\nconstant in the approximate inverse is accounted for. We demonstrate the\nefficacy of our approach on spatially discretized Poisson equations with\nmillions of degrees of freedom arising in computational fluid dynamics\napplications. Unlike state-of-the-art learning approaches, our algorithm is\ncapable of reducing the linear system residual to a given tolerance in a small\nnumber of iterations, independent of the problem size. Moreover, our method\ngeneralizes effectively to various systems beyond those encountered during\ntraining.",
    "descriptor": "\nComments: 13 pages, 4 figures\n",
    "authors": [
      "Ayano Kaneda",
      "Osman Akar",
      "Jingyu Chen",
      "Victoria Kala",
      "David Hyde",
      "Joseph Teran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10763"
  },
  {
    "id": "arXiv:2205.10764",
    "title": "Evidence for Hypodescent in Visual Semantic AI",
    "abstract": "We examine the state-of-the-art multimodal \"visual semantic\" model CLIP\n(\"Contrastive Language Image Pretraining\") for the rule of hypodescent, or\none-drop rule, whereby multiracial people are more likely to be assigned a\nracial or ethnic label corresponding to a minority or disadvantaged racial or\nethnic group than to the equivalent majority or advantaged group. A face\nmorphing experiment grounded in psychological research demonstrating\nhypodescent indicates that, at the midway point of 1,000 series of morphed\nimages, CLIP associates 69.7% of Black-White female images with a Black text\nlabel over a White text label, and similarly prefers Latina (75.8%) and Asian\n(89.1%) text labels at the midway point for Latina-White female and Asian-White\nfemale morphs, reflecting hypodescent. Additionally, assessment of the\nunderlying cosine similarities in the model reveals that association with White\nis correlated with association with \"person,\" with Pearson's rho as high as\n0.82 over a 21,000-image morph series, indicating that a White person\ncorresponds to the default representation of a person in CLIP. Finally, we show\nthat the stereotype-congruent pleasantness association of an image correlates\nwith association with the Black text label in CLIP, with Pearson's rho = 0.48\nfor 21,000 Black-White multiracial male images, and rho = 0.41 for Black-White\nmultiracial female images. CLIP is trained on English-language text gathered\nusing data collected from an American website (Wikipedia), and our findings\ndemonstrate that CLIP embeds the values of American racial hierarchy,\nreflecting the implicit and explicit beliefs that are present in human minds.\nWe contextualize these findings within the history and psychology of\nhypodescent. Overall, the data suggests that AI supervised using natural\nlanguage will, unless checked, learn biases that reflect racial hierarchies.",
    "descriptor": "\nComments: To be published at ACM FAccT 2022\n",
    "authors": [
      "Robert Wolfe",
      "Mahzarin R. Banaji",
      "Aylin Caliskan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.10764"
  },
  {
    "id": "arXiv:2205.10766",
    "title": "Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey",
    "abstract": "Multi-object tracking (MOT) aims to associate target objects across video\nframes in order to obtain entire moving trajectories. With the advancement of\ndeep neural networks and the increasing demand for intelligent video analysis,\nMOT has gained significantly increased interest in the computer vision\ncommunity. Embedding methods play an essential role in object location\nestimation and temporal identity association in MOT. Unlike other computer\nvision tasks, such as image classification, object detection,\nre-identification, and segmentation, embedding methods in MOT have large\nvariations, and they have never been systematically analyzed and summarized. In\nthis survey, we first conduct a comprehensive overview with in-depth analysis\nfor embedding methods in MOT from seven different perspectives, including\npatch-level embedding, single-frame embedding, cross-frame joint embedding,\ncorrelation embedding, sequential embedding, tracklet embedding, and\ncross-track relational embedding. We further summarize the existing widely used\nMOT datasets and analyze the advantages of existing state-of-the-art methods\naccording to their embedding strategies. Finally, some critical yet\nunder-investigated areas and future research directions are discussed.",
    "descriptor": "",
    "authors": [
      "Gaoang Wang",
      "Mingli Song",
      "Jenq-Neng Hwang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10766"
  },
  {
    "id": "arXiv:2205.10767",
    "title": "Human Instance Matting via Mutual Guidance and Multi-Instance Refinement",
    "abstract": "This paper introduces a new matting task called human instance matting (HIM),\nwhich requires the pertinent model to automatically predict a precise alpha\nmatte for each human instance. Straightforward combination of closely related\ntechniques, namely, instance segmentation, soft segmentation and\nhuman/conventional matting, will easily fail in complex cases requiring\ndisentangling mingled colors belonging to multiple instances along hairy and\nthin boundary structures. To tackle these technical challenges, we propose a\nhuman instance matting framework, called InstMatt, where a novel mutual\nguidance strategy working in tandem with a multi-instance refinement module is\nused, for delineating multi-instance relationship among humans with complex and\noverlapping boundaries if present. A new instance matting metric called\ninstance matting quality (IMQ) is proposed, which addresses the absence of a\nunified and fair means of evaluation emphasizing both instance recognition and\nmatting quality. Finally, we construct a HIM benchmark for evaluation, which\ncomprises of both synthetic and natural benchmark images. In addition to\nthorough experimental results on complex cases with multiple and overlapping\nhuman instances each has intricate boundaries, preliminary results are\npresented on general instance matting. Code and benchmark are available in\nhttps://github.com/nowsyn/InstMatt.",
    "descriptor": "\nComments: 16 pages, 20 figures, CVPR2022 Oral\n",
    "authors": [
      "Yanan Sun",
      "Chi-Keung Tang",
      "Yu-Wing Tai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10767"
  },
  {
    "id": "arXiv:2205.10768",
    "title": "Neuro-Symbolic Artificial Intelligence (AI) for Intent based Semantic  Communication",
    "abstract": "Intent-based networks that integrate sophisticated machine reasoning\ntechnologies will be a cornerstone of future wireless 6G systems. Intent-based\ncommunication requires the network to consider the semantics (meanings) and\neffectiveness (at end-user) of the data transmission. This is essential if 6G\nsystems are to communicate reliably with fewer bits while simultaneously\nproviding connectivity to heterogeneous users. In this paper, contrary to state\nof the art, which lacks explainability of data, the framework of neuro-symbolic\nartificial intelligence (NeSy AI) is proposed as a pillar for learning causal\nstructure behind the observed data. In particular, the emerging concept of\ngenerative flow networks (GFlowNet) is leveraged for the first time in a\nwireless system to learn the probabilistic structure which generates the data.\nFurther, a novel optimization problem for learning the optimal encoding and\ndecoding functions is rigorously formulated with the intent of achieving higher\nsemantic reliability. Novel analytical formulations are developed to define key\nmetrics for semantic message transmission, including semantic distortion,\nsemantic similarity, and semantic reliability. These semantic measure functions\nrely on the proposed definition of semantic content of the knowledge base and\nthis information measure is reflective of the nodes' reasoning capabilities.\nSimulation results validate the ability to communicate efficiently (with less\nbits but same semantics) and significantly better compared to a conventional\nsystem which does not exploit the reasoning capabilities.",
    "descriptor": "",
    "authors": [
      "Christo Kurisummoottil Thomas",
      "Walid Saad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10768"
  },
  {
    "id": "arXiv:2205.10770",
    "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of  Large Language Models",
    "abstract": "Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.",
    "descriptor": "",
    "authors": [
      "Kushal Tirumala",
      "Aram H. Markosyan",
      "Luke Zettlemoyer",
      "Armen Aghajanyan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10770"
  },
  {
    "id": "arXiv:2205.10773",
    "title": "A Domain-adaptive Pre-training Approach for Language Bias Detection in  News",
    "abstract": "Media bias is a multi-faceted construct influencing individual behavior and\ncollective decision-making. Slanted news reporting is the result of one-sided\nand polarized writing which can occur in various forms. In this work, we focus\non an important form of media bias, i.e. bias by word choice. Detecting biased\nword choices is a challenging task due to its linguistic complexity and the\nlack of representative gold-standard corpora. We present DA-RoBERTa, a new\nstate-of-the-art transformer-based model adapted to the media bias domain which\nidentifies sentence-level bias with an F1 score of 0.814. In addition, we also\ntrain, DA-BERT and DA-BART, two more transformer models adapted to the bias\ndomain. Our proposed domain-adapted models outperform prior bias detection\napproaches on the same data.",
    "descriptor": "",
    "authors": [
      "Jan-David Krieger",
      "Timo Spinde",
      "Terry Ruas",
      "Juhi Kulshrestha",
      "Bela Gipp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10773"
  },
  {
    "id": "arXiv:2205.10775",
    "title": "Ada-Ranker: A Data Distribution Adaptive Ranking Paradigm for Sequential  Recommendation",
    "abstract": "A large-scale recommender system usually consists of recall and ranking\nmodules. The goal of ranking modules (aka rankers) is to elaborately\ndiscriminate users' preference on item candidates proposed by recall modules.\nWith the success of deep learning techniques in various domains, we have\nwitnessed the mainstream rankers evolve from traditional models to deep neural\nmodels. However, the way that we design and use rankers remains unchanged:\noffline training the model, freezing the parameters, and deploying it for\nonline serving. Actually, the candidate items are determined by specific user\nrequests, in which underlying distributions (e.g., the proportion of items for\ndifferent categories, the proportion of popular or new items) are highly\ndifferent from one another in a production environment. The classical\nparameter-frozen inference manner cannot adapt to dynamic serving\ncircumstances, making rankers' performance compromised. In this paper, we\npropose a new training and inference paradigm, termed as Ada-Ranker, to address\nthe challenges of dynamic online serving. Instead of using parameter-frozen\nmodels for universal serving, Ada-Ranker can adaptively modulate parameters of\na ranker according to the data distribution of the current group of item\ncandidates. We first extract distribution patterns from the item candidates.\nThen, we modulate the ranker by the patterns to make the ranker adapt to the\ncurrent data distribution. Finally, we use the revised ranker to score the\ncandidate list. In this way, we empower the ranker with the capacity of\nadapting from a global model to a local model which better handles the current\ntask.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Xinyan Fan",
      "Jianxun Lian",
      "Wayne Xin Zhao",
      "Zheng Liu",
      "Chaozhuo Li",
      "Xing Xie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.10775"
  },
  {
    "id": "arXiv:2205.10778",
    "title": "Sleep Posture One-Shot Learning Framework Using Kinematic Data  Augmentation: In-Silico and In-Vivo Case Studies",
    "abstract": "Sleep posture is linked to several health conditions such as nocturnal cramps\nand more serious musculoskeletal issues. However, in-clinic sleep assessments\nare often limited to vital signs (e.g. brain waves). Wearable sensors with\nembedded inertial measurement units have been used for sleep posture\nclassification; nonetheless, previous works consider only few (commonly four)\npostures, which are inadequate for advanced clinical assessments. Moreover,\nposture learning algorithms typically require longitudinal data collection to\nfunction reliably, and often operate on raw inertial sensor readings unfamiliar\nto clinicians. This paper proposes a new framework for sleep posture\nclassification based on a minimal set of joint angle measurements. The proposed\nframework is validated on a rich set of twelve postures in two experimental\npipelines: computer animation to obtain synthetic postural data, and human\nparticipant pilot study using custom-made miniature wearable sensors. Through\nfusing raw geo-inertial sensor measurements to compute a filtered estimate of\nrelative segment orientations across the wrist and ankle joints, the body\nposture can be characterised in a way comprehensible to medical experts. The\nproposed sleep posture learning framework offers plug-and-play posture\nclassification by capitalising on a novel kinematic data augmentation method\nthat requires only one training example per posture. Additionally, a new metric\ntogether with data visualisations are employed to extract meaningful insights\nfrom the postures dataset, demonstrate the added value of the data augmentation\nmethod, and explain the classification performance. The proposed framework\nattained promising overall accuracy as high as 100% on synthetic data and 92.7%\non real data, on par with state of the art data-hungry algorithms available in\nthe literature.",
    "descriptor": "\nComments: 27 pages, 15 figures\n",
    "authors": [
      "Omar Elnaggar",
      "Frans Coenen",
      "Andrew Hopkinson",
      "Lyndon Mason",
      "Paolo Paoletti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10778"
  },
  {
    "id": "arXiv:2205.10780",
    "title": "Data-aided Active User Detection with a User Activity Extraction Network  for Grant-free SCMA Systems",
    "abstract": "In grant-free sparse code multiple access system, joint optimization of\ncontention resources for users and active user detection (AUD) at the receiver\nis a complex combinatorial problem. To this end, we propose a deep\nlearning-based data-aided AUD scheme which extracts a priori user activity\ninformation via a novel user activity extraction network (UAEN). This is\nenabled by an end-to-end training of an autoencoder (AE), which simultaneously\noptimizes the contention resources, i.e., preamble sequences, each associated\nwith one of the codebooks, and extraction of user activity information from\nboth preamble and data transmission. Furthermore, we propose self-supervised\npre-training scheme for the UAEN, which ensures the convergence of offline\nend-to-end training. Simulation results demonstrated that the proposed AUD\nscheme achieved 3 to 5dB gain at a target activity detection error rate of\n${{10}^{-3}}$ compared to the state-of-the-art DL-based AUD schemes.",
    "descriptor": "",
    "authors": [
      "Minsig Han",
      "Ameha T. Abebe",
      "Chung G. Kang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10780"
  },
  {
    "id": "arXiv:2205.10781",
    "title": "A Hierarchical MPC Approach to Car-Following via Linearly Constrained  Quadratic Programming",
    "abstract": "Single-lane car following is a fundamental task in autonomous driving. A\ndesirable car following controller should keep a reasonable range of distances\nto the preceding vehicle and do so as smoothly as possible. To achieve this,\nnumerous control methods have been proposed in the literature: some only rely\non local sensing; others also make use of non-local downstream observations.\nWhile local methods are capable of attenuating high-frequency velocity\noscillation and are economical to compute, non-local methods can dampen a wider\nspectrum of oscillatory traffic but incur a larger cost in computing. In this\narticle, we design a novel non-local controller that is capable of smoothing a\nwide range of oscillatory traffic and is amenable to real-time applications.\nThe controller consists of 1) a planning layer, which solves for an optimal\ntrajectory using non-local observation generated from a real-time estimated\ntime of arrival estimator, and 2) a tracking layer, which realizes the planned\ntrajectory whenever safety permits. At the core of the controller design is an\noptimization procedure reduced to a light-weight quadratic program, designed\nspecifically for extensibility and real-time implementation. Numerical\nexperiments suggest that the proposed controller has fast running time, can\nsimultaneously maintain a variable headway while driving with modest\naccelerations, and is capable of performing under imperfect traffic\npredictions.",
    "descriptor": "\nComments: 6 pages, 6 figures, 1 table\n",
    "authors": [
      "Fangyu Wu",
      "Alexandre Bayen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10781"
  },
  {
    "id": "arXiv:2205.10782",
    "title": "Instruction Induction: From Few Examples to Natural Language Task  Descriptions",
    "abstract": "Large language models are able to perform a task by conditioning on a few\ninput-output demonstrations - a paradigm known as in-context learning. We show\nthat language models can explicitly infer an underlying task from a few\ndemonstrations by prompting them to generate a natural language instruction\nthat fits the examples. To explore this ability, we introduce the instruction\ninduction challenge, compile a dataset consisting of 24 tasks, and define a\nnovel evaluation metric based on executing the generated instruction. We\ndiscover that, to a large extent, the ability to generate instructions does\nindeed emerge when using a model that is both large enough and aligned to\nfollow instructions; InstructGPT achieves 65.7% of human performance in our\nexecution-based metric, while the original GPT-3 model reaches only 9.8% of\nhuman performance. This surprising result suggests that instruction induction\nmight be a viable learning paradigm in and of itself, where instead of fitting\na set of latent continuous parameters to the data, one searches for the best\ndescription in the natural language hypothesis space.",
    "descriptor": "",
    "authors": [
      "Or Honovich",
      "Uri Shaham",
      "Samuel R. Bowman",
      "Omer Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10782"
  },
  {
    "id": "arXiv:2205.10785",
    "title": "Responsible Artificial Intelligence -- from Principles to Practice",
    "abstract": "The impact of Artificial Intelligence does not depend only on fundamental\nresearch and technological developments, but for a large part on how these\nsystems are introduced into society and used in everyday situations. AI is\nchanging the way we work, live and solve challenges but concerns about\nfairness, transparency or privacy are also growing. Ensuring responsible,\nethical AI is more than designing systems whose result can be trusted. It is\nabout the way we design them, why we design them, and who is involved in\ndesigning them. In order to develop and use AI responsibly, we need to work\ntowards technical, societal, institutional and legal methods and tools which\nprovide concrete support to AI practitioners, as well as awareness and training\nto enable participation of all, to ensure the alignment of AI systems with our\nsocieties' principles and values.",
    "descriptor": "\nComments: This paper is a curated version of my keynote at the Web Conference 2022. arXiv admin note: substantial text overlap with arXiv:2202.07446\n",
    "authors": [
      "Virginia Dignum"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10785"
  },
  {
    "id": "arXiv:2205.10787",
    "title": "A Dirichlet Process Mixture of Robust Task Models for Scalable Lifelong  Reinforcement Learning",
    "abstract": "While reinforcement learning (RL) algorithms are achieving state-of-the-art\nperformance in various challenging tasks, they can easily encounter\ncatastrophic forgetting or interference when faced with lifelong streaming\ninformation. In the paper, we propose a scalable lifelong RL method that\ndynamically expands the network capacity to accommodate new knowledge while\npreventing past memories from being perturbed. We use a Dirichlet process\nmixture to model the non-stationary task distribution, which captures task\nrelatedness by estimating the likelihood of task-to-cluster assignments and\nclusters the task models in a latent space. We formulate the prior distribution\nof the mixture as a Chinese restaurant process (CRP) that instantiates new\nmixture components as needed. The update and expansion of the mixture are\ngoverned by the Bayesian non-parametric framework with an expectation\nmaximization (EM) procedure, which dynamically adapts the model complexity\nwithout explicit task boundaries or heuristics. Moreover, we use the domain\nrandomization technique to train robust prior parameters for the initialization\nof each task model in the mixture, thus the resulting model can better\ngeneralize and adapt to unseen tasks. With extensive experiments conducted on\nrobot navigation and locomotion domains, we show that our method successfully\nfacilitates scalable lifelong RL and outperforms relevant existing methods.",
    "descriptor": "\nComments: Manuscript accepted by IEEE Transactions on Cybernetics, 2022, DOI: DOI: 10.1109/TCYB.2022.3170485\n",
    "authors": [
      "Zhi Wang",
      "Chunlin Chen",
      "Daoyi Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10787"
  },
  {
    "id": "arXiv:2205.10788",
    "title": "Learning Muti-expert Distribution Calibration for Long-tailed Video  Classification",
    "abstract": "Most existing state-of-the-art video classification methods assume the\ntraining data obey a uniform distribution. However, video data in the real\nworld typically exhibit long-tail class distribution and imbalance, which\nextensively results in a model bias towards head class and leads to relatively\nlow performance on tail class. While the current long-tail classification\nmethods usually focus on image classification, adapting it to video data is not\na trivial extension. We propose an end-to-end multi-experts distribution\ncalibration method based on two-level distribution information to address these\nchallenges. The method jointly considers the distribution of samples in each\nclass (intra-class distribution) and the diverse distributions of overall data\n(inter-class distribution) to solve the problem of imbalanced data under\nlong-tailed distribution. By modeling this two-level distribution information,\nthe model can consider the head classes and the tail classes and significantly\ntransfer the knowledge from the head classes to improve the performance of the\ntail classes. Extensive experiments verify that our method achieves\nstate-of-the-art performance on the long-tailed video classification task.",
    "descriptor": "",
    "authors": [
      "Yufan Hu",
      "Junyu Gao",
      "Changsheng Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10788"
  },
  {
    "id": "arXiv:2205.10791",
    "title": "Federated Spectrum Learning for Reconfigurable Intelligent  Surfaces-Aided Wireless Edge Networks",
    "abstract": "Increasing concerns on intelligent spectrum sensing call for efficient\ntraining and inference technologies. In this paper, we propose a novel\nfederated learning (FL) framework, dubbed federated spectrum learning (FSL),\nwhich exploits the benefits of reconfigurable intelligent surfaces (RISs) and\novercomes the unfavorable impact of deep fading channels. Distinguishingly, we\nendow conventional RISs with spectrum learning capabilities by leveraging a\nfully-trained convolutional neural network (CNN) model at each RIS controller,\nthereby helping the base station to cooperatively infer the users who request\nto participate in FL at the beginning of each training iteration. To fully\nexploit the potential of FL and RISs, we address three technical challenges:\nRISs phase shifts configuration, user-RIS association, and wireless bandwidth\nallocation. The resulting joint learning, wireless resource allocation, and\nuser-RIS association design is formulated as an optimization problem whose\nobjective is to maximize the system utility while considering the impact of FL\nprediction accuracy. In this context, the accuracy of FL prediction interplays\nwith the performance of resource optimization. In particular, if the accuracy\nof the trained CNN model deteriorates, the performance of resource allocation\nworsens. The proposed FSL framework is tested by using real radio frequency\n(RF) traces and numerical results demonstrate its advantages in terms of\nspectrum prediction accuracy and system utility: a better CNN prediction\naccuracy and FL system utility can be achieved with a larger number of RISs and\nreflecting elements.",
    "descriptor": "",
    "authors": [
      "Bo Yang",
      "Xuelin Cao",
      "Chongwen Huang",
      "Chau Yuen",
      "Marco Di Renzo",
      "Yong Liang Guan",
      "Dusit Niyato",
      "Lijun Qian",
      "Merouane Debbah"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10791"
  },
  {
    "id": "arXiv:2205.10793",
    "title": "Knowledge Distillation via the Target-aware Transformer",
    "abstract": "Knowledge distillation becomes a de facto standard to improve the performance\nof small neural networks. Most of the previous works propose to regress the\nrepresentational features from the teacher to the student in a one-to-one\nspatial matching fashion. However, people tend to overlook the fact that, due\nto the architecture differences, the semantic information on the same spatial\nlocation usually vary. This greatly undermines the underlying assumption of the\none-to-one distillation approach. To this end, we propose a novel one-to-all\nspatial matching knowledge distillation approach. Specifically, we allow each\npixel of the teacher feature to be distilled to all spatial locations of the\nstudent features given its similarity, which is generated from a target-aware\ntransformer. Our approach surpasses the state-of-the-art methods by a\nsignificant margin on various computer vision benchmarks, such as ImageNet,\nPascal VOC and COCOStuff10k. Code will be released soon.",
    "descriptor": "\nComments: CVPR2022(Oral)\n",
    "authors": [
      "Sihao Lin",
      "Hongwei Xie",
      "Bing Wang",
      "Kaicheng Yu",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Gang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10793"
  },
  {
    "id": "arXiv:2205.10798",
    "title": "PAC-Wrap: Semi-Supervised PAC Anomaly Detection",
    "abstract": "Anomaly detection is essential for preventing hazardous outcomes for\nsafety-critical applications like autonomous driving. Given their\nsafety-criticality, these applications benefit from provable bounds on various\nerrors in anomaly detection. To achieve this goal in the semi-supervised\nsetting, we propose to provide Probably Approximately Correct (PAC) guarantees\non the false negative and false positive detection rates for anomaly detection\nalgorithms. Our method (PAC-Wrap) can wrap around virtually any existing\nsemi-supervised and unsupervised anomaly detection method, endowing it with\nrigorous guarantees. Our experiments with various anomaly detectors and\ndatasets indicate that PAC-Wrap is broadly effective.",
    "descriptor": "\nComments: Accepted by SIGKDD 2022\n",
    "authors": [
      "Shuo Li",
      "Xiayan Ji",
      "Edgar Dobriban",
      "Oleg Sokolsky",
      "Insup Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10798"
  },
  {
    "id": "arXiv:2205.10802",
    "title": "Inverse-Inverse Reinforcement Learning. How to Hide Strategy from an  Adversarial Inverse Reinforcement Learner",
    "abstract": "Inverse reinforcement learning (IRL) deals with estimating an agent's utility\nfunction from its actions. In this paper, we consider how an agent can hide its\nstrategy and mitigate an adversarial IRL attack; we call this inverse IRL\n(I-IRL). How should the decision maker choose its response to ensure a poor\nreconstruction of its strategy by an adversary performing IRL to estimate the\nagent's strategy? This paper comprises four results: First, we present an\nadversarial IRL algorithm that estimates the agent's strategy while controlling\nthe agent's utility function. Our second result for I-IRL result spoofs the IRL\nalgorithm used by the adversary. Our I-IRL results are based on revealed\npreference theory in micro-economics. The key idea is for the agent to\ndeliberately choose sub-optimal responses that sufficiently masks its true\nstrategy. Third, we give a sample complexity result for our main I-IRL result\nwhen the agent has noisy estimates of the adversary specified utility function.\nFinally, we illustrate our I-IRL scheme in a radar problem where a\nmeta-cognitive radar is trying to mitigate an adversarial target.",
    "descriptor": "",
    "authors": [
      "Kunal Pattanayak",
      "Vikram Krishnamurthy",
      "Christopher Berry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10802"
  },
  {
    "id": "arXiv:2205.10803",
    "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
    "abstract": "Self-supervised learning (SSL) has been extensively explored in recent years.\nParticularly, generative SSL has seen emerging success in natural language\nprocessing and other fields, such as the wide adoption of BERT and GPT. Despite\nthis, contrastive learning-which heavily relies on structural data augmentation\nand complicated training strategies-has been the dominant approach in graph\nSSL, while the progress of generative SSL on graphs, especially graph\nautoencoders (GAEs), has thus far not reached the potential as promised in\nother fields. In this paper, we identify and examine the issues that negatively\nimpact the development of GAEs, including their reconstruction objective,\ntraining robustness, and error metric. We present a masked graph autoencoder\nGraphMAE that mitigates these issues for generative self-supervised graph\nlearning. Instead of reconstructing structures, we propose to focus on feature\nreconstruction with both a masking strategy and scaled cosine error that\nbenefit the robust training of GraphMAE. We conduct extensive experiments on 21\npublic datasets for three different graph learning tasks. The results manifest\nthat GraphMAE-a simple graph autoencoder with our careful designs-can\nconsistently generate outperformance over both contrastive and generative\nstate-of-the-art baselines. This study provides an understanding of graph\nautoencoders and demonstrates the potential of generative self-supervised\nlearning on graphs.",
    "descriptor": "\nComments: 11 pages; Accepted by KDD'22\n",
    "authors": [
      "Zhenyu Hou",
      "Xiao Liu",
      "Yuxiao Dong",
      "Hongxia yang",
      "Chunjie Wang",
      "Jie Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10803"
  },
  {
    "id": "arXiv:2205.10805",
    "title": "Deep Learning-Based Synchronization for Uplink NB-IoT",
    "abstract": "We propose a neural network (NN)-based algorithm for device detection and\ntime of arrival (ToA) and carrier frequency offset (CFO) estimation for the\nnarrowband physical random-access channel (NPRACH) of narrowband internet of\nthings (NB-IoT). The introduced NN architecture leverages residual\nconvolutional networks as well as knowledge of the preamble structure of the 5G\nNew Radio (5G NR) specifications. Benchmarking on a 3rd Generation Partnership\nProject (3GPP) urban microcell (UMi) channel model with random drops of users\nagainst a state-of-the-art baseline shows that the proposed method enables up\nto 8 dB gains in false negative rate (FNR) as well as significant gains in\nfalse positive rate (FPR) and ToA and CFO estimation accuracy. Moreover, our\nsimulations indicate that the proposed algorithm enables gains over a wide\nrange of channel conditions, CFOs, and transmission probabilities. The\nintroduced synchronization method operates at the base station (BS) and,\ntherefore, introduces no additional complexity on the user devices. It could\nlead to an extension of battery lifetime by reducing the preamble length or the\ntransmit power.",
    "descriptor": "",
    "authors": [
      "Fay\u00e7al A\u00eft Aoudia",
      "Jakob Hoydis",
      "Sebastian Cammerer",
      "Matthijs Van Keirsbilck",
      "Alexander Keller"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10805"
  },
  {
    "id": "arXiv:2205.10815",
    "title": "Recent Advances in Rate Control: From Optimization to Implementation and  Beyond",
    "abstract": "Video coding is a video compression technique that compresses the original\nvideo sequence to achieve a smaller archive file or a lower transmission\nbandwidth under constraints on the visual quality loss. Rate control (RC) plays\na critical role in video coding. It can achieve stable stream output in\npractical applications, especially in real-time video applications such as\nvideo conferencing or game live streaming. Most RC algorithms directly or\nindirectly characterize the relationship between the bit rate (R) and\nquantization (Q) and then allocate bits for every coding unit to guarantee the\nglobal bit rate and video quality level. This paper comprehensively reviews the\nclassic RC technologies used in international video standards of past\ngenerations, analyses the mathematical models and implementation mechanisms of\nvarious schemes, and compares the performance of recent state-of-the-art RC\nalgorithms. Finally, we discuss future directions and new application areas for\nRC methods. We hope that this review can help support the development,\nimplementation, and application of RC in new video coding standards.",
    "descriptor": "",
    "authors": [
      "Xuekai Wei",
      "Mingliang Zhou",
      "Heqiang Wang",
      "Haoyan Yang",
      "Lei Chen",
      "Sam Kwong"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.10815"
  },
  {
    "id": "arXiv:2205.10816",
    "title": "Chain of Thought Imitation with Procedure Cloning",
    "abstract": "Imitation learning aims to extract high-performance policies from logged\ndemonstrations of expert behavior. It is common to frame imitation learning as\na supervised learning problem in which one fits a function approximator to the\ninput-output mapping exhibited by the logged demonstrations (input observations\nto output actions). While the framing of imitation learning as a supervised\ninput-output learning problem allows for applicability in a wide variety of\nsettings, it is also an overly simplistic view of the problem in situations\nwhere the expert demonstrations provide much richer insight into expert\nbehavior. For example, applications such as path navigation, robot\nmanipulation, and strategy games acquire expert demonstrations via planning,\nsearch, or some other multi-step algorithm, revealing not just the output\naction to be imitated but also the procedure for how to determine this action.\nWhile these intermediate computations may use tools not available to the agent\nduring inference (e.g., environment simulators), they are nevertheless\ninformative as a way to explain an expert's mapping of state to actions. To\nproperly leverage expert procedure information without relying on the\nprivileged tools the expert may have used to perform the procedure, we propose\nprocedure cloning, which applies supervised sequence prediction to imitate the\nseries of expert computations. This way, procedure cloning learns not only what\nto do (i.e., the output action), but how and why to do it (i.e., the\nprocedure). Through empirical analysis on navigation, simulated robotic\nmanipulation, and game-playing environments, we show that imitating the\nintermediate computations of an expert's behavior enables procedure cloning to\nlearn policies exhibiting significant generalization to unseen environment\nconfigurations, including those configurations for which running the expert's\nprocedure directly is infeasible.",
    "descriptor": "",
    "authors": [
      "Mengjiao Yang",
      "Dale Schuurmans",
      "Pieter Abbeel",
      "Ofir Nachum"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10816"
  },
  {
    "id": "arXiv:2205.10821",
    "title": "Information Leakage in Index Coding",
    "abstract": "We study the information leakage to a guessing adversary in index coding with\na general message distribution. Under both vanishing-error and zero-error\ndecoding assumptions, we develop lower and upper bounds on the optimal leakage\nrate, which are based on the broadcast rate of the subproblem induced by the\nset of messages the adversary tries to guess. When the messages are independent\nand uniformly distributed, the lower and upper bounds match, establishing an\nequivalence between the two rates.",
    "descriptor": "\nComments: Published in Proceedings of IEEE Information Theory Workshop (ITW) 2021\n",
    "authors": [
      "Yucheng Liu",
      "Lawrence Ong",
      "Phee Lep Yeoh",
      "Parastoo Sadeghi",
      "Joerg Kliewer",
      "Sarah Johnson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10821"
  },
  {
    "id": "arXiv:2205.10822",
    "title": "A Graph Enhanced BERT Model for Event Prediction",
    "abstract": "Predicting the subsequent event for an existing event context is an important\nbut challenging task, as it requires understanding the underlying relationship\nbetween events. Previous methods propose to retrieve relational features from\nevent graph to enhance the modeling of event correlation. However, the sparsity\nof event graph may restrict the acquisition of relevant graph information, and\nhence influence the model performance. To address this issue, we consider\nautomatically building of event graph using a BERT model. To this end, we\nincorporate an additional structured variable into BERT to learn to predict the\nevent connections in the training process. Hence, in the test process, the\nconnection relationship for unseen events can be predicted by the structured\nvariable. Results on two event prediction tasks: script event prediction and\nstory ending prediction, show that our approach can outperform state-of-the-art\nbaseline methods.",
    "descriptor": "",
    "authors": [
      "Li Du",
      "Xiao Ding",
      "Yue Zhang",
      "Kai Xiong",
      "Ting Liu",
      "Bing Qin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10822"
  },
  {
    "id": "arXiv:2205.10824",
    "title": "ReLU Fields: The Little Non-linearity That Could",
    "abstract": "In many recent works, multi-layer perceptions (MLPs) have been shown to be\nsuitable for modeling complex spatially-varying functions including images and\n3D scenes. Although the MLPs are able to represent complex scenes with\nunprecedented quality and memory footprint, this expressive power of the MLPs,\nhowever, comes at the cost of long training and inference times. On the other\nhand, bilinear/trilinear interpolation on regular grid based representations\ncan give fast training and inference times, but cannot match the quality of\nMLPs without requiring significant additional memory. Hence, in this work, we\ninvestigate what is the smallest change to grid-based representations that\nallows for retaining the high fidelity result of MLPs while enabling fast\nreconstruction and rendering times. We introduce a surprisingly simple change\nthat achieves this task -- simply allowing a fixed non-linearity (ReLU) on\ninterpolated grid values. When combined with coarse to-fine optimization, we\nshow that such an approach becomes competitive with the state-of-the-art. We\nreport results on radiance fields, and occupancy fields, and compare against\nmultiple existing alternatives. Code and data for the paper are available at\nhttps://geometry.cs.ucl.ac.uk/projects/2022/relu_fields.",
    "descriptor": "\nComments: Conference accept at SIGGRAPH 2022\n",
    "authors": [
      "Animesh Karnewar",
      "Tobias Ritschel",
      "Oliver Wang",
      "Niloy J. Mitra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.10824"
  },
  {
    "id": "arXiv:2205.10825",
    "title": "A Convolutional Dispersion Relation Preserving Scheme for the Acoustic  Wave Equation",
    "abstract": "We propose an accurate numerical scheme for approximating the solution of the\ntwo dimensional acoustic wave problem. We use machine learning to find a\nstencil suitable even in the presence of high wavenumbers. The proposed scheme\nincorporates physically informed elements from the field of optimized numerical\nschemes into a convolutional optimization machine learning algorithm.",
    "descriptor": "",
    "authors": [
      "Oded Ovadia",
      "Adar Kahana",
      "Eli Turkel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.10825"
  },
  {
    "id": "arXiv:2205.10827",
    "title": "Information Leakage in Index Coding With Sensitive and Non-Sensitive  Messages",
    "abstract": "Information leakage to a guessing adversary in index coding is studied, where\nsome messages in the system are sensitive and others are not. The non-sensitive\nmessages can be used by the server like secret keys to mitigate leakage of the\nsensitive messages to the adversary. We construct a deterministic linear coding\nscheme, developed from the rank minimization method based on fitting matrices\n(Bar-Yossef et al. 2011). The linear scheme leads to a novel upper bound on the\noptimal information leakage rate, which is proved to be tight over all\ndeterministic scalar linear codes. We also derive a converse result from a\ngraph-theoretic perspective, which holds in general over all deterministic and\nstochastic coding schemes.",
    "descriptor": "\nComments: Accepted by IEEE International Symposium on Information Theory (ISIT) 2022\n",
    "authors": [
      "Yucheng Liu",
      "Lawrence Ong",
      "Phee Lep Yeoh",
      "Parastoo Sadeghi",
      "Joerg Kliewer",
      "Sarah Johnson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10827"
  },
  {
    "id": "arXiv:2205.10828",
    "title": "What Do Compressed Multilingual Machine Translation Models Forget?",
    "abstract": "Recently, very large pre-trained models achieve state-of-the-art results in\nvarious natural language processing (NLP) tasks, but their size makes it more\nchallenging to apply them in resource-constrained environments. Compression\ntechniques allow to drastically reduce the size of the model and therefore its\ninference time with negligible impact on top-tier metrics. However, the general\nperformance hides a drastic performance drop on under-represented features,\nwhich could result in the amplification of biases encoded by the model. In this\nwork, we analyze the impacts of compression methods on Multilingual Neural\nMachine Translation models (MNMT) for various language groups and semantic\nfeatures by extensive analysis of compressed models on different NMT\nbenchmarks, e.g. FLORES-101, MT-Gender, and DiBiMT. Our experiments show that\nthe performance of under-represented languages drops significantly, while the\naverage BLEU metric slightly decreases. Interestingly, the removal of noisy\nmemorization with the compression leads to a significant improvement for some\nmedium-resource languages. Finally, we demonstrate that the compression\namplifies intrinsic gender and semantic biases, even in high-resource\nlanguages.",
    "descriptor": "\nComments: 20 pages, 12 Figures, 9 Tables\n",
    "authors": [
      "Alireza Mohammadshahi",
      "Vassilina Nikoulina",
      "Alexandre Berard",
      "Caroline Brun",
      "James Henderson",
      "Laurent Besacier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10828"
  },
  {
    "id": "arXiv:2205.10830",
    "title": "A review on Deep Neural Network for Computer Network Traffic  Classification",
    "abstract": "Focus on Deep Neural Network based malicious and normal computer Network\nTraffic classification. (such as attacks, phishing, any other illegal activity\nand normal traffic identification). In this paper, the main idea is to review,\nexisted Neural Network based network traffic classification. Which indicates\nintrusion activity classification and detection. It is very important to\nclassify network traffic to safeguard any system, connected to computer\nnetwork. There are a variety of NN architecture for it, with different rate of\naccuracy. On this paper we will do relative compression among them. Index\nTerms-Computer Network, Network traffic, Packet, Intrusion, DOS\n(Denial-of-service), unauthorized access, IDS (Intrusion Detection System), IPS\n(Intrusion Prevention Systems), R2L (Remote to Local Attack), Probing, U2R\n(User to Root Attack), DNN (Deep Neural Network), CRNN (Convolutional Recurrent\nNeural Network), RPROP (Resilient propagation).",
    "descriptor": "",
    "authors": [
      "Md. Ariful Haque",
      "Dr. Rajesh Palit"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.10830"
  },
  {
    "id": "arXiv:2205.10834",
    "title": "On the Parameterized Complexity of the $s$-Club Cluster Edge Deletion  Problem",
    "abstract": "We study the parameterized complexity of the $s$-Club Cluster Edge Deletion\nproblem: Given a graph $G$ and two integers $s \\ge 2$ and $k \\ge 1$, is it\npossible to remove at most $k$ edges from $G$ such that each connected\ncomponent of the resulting graph has diameter at most $s$? This problem is\nknown to be NP-hard already when $s = 2$. We prove that it admits a\nfixed-parameter tractable algorithm when parameterized by $s$ and the treewidth\nof the input graph.",
    "descriptor": "",
    "authors": [
      "Fabrizio Montecchiani",
      "Giacomo Ortali",
      "Tommaso Piselli",
      "Alessandra Tappini"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.10834"
  },
  {
    "id": "arXiv:2205.10835",
    "title": "Multilingual Machine Translation with Hyper-Adapters",
    "abstract": "Multilingual machine translation suffers from negative interference across\nlanguages. A common solution is to relax parameter sharing with\nlanguage-specific modules like adapters. However, adapters of related languages\nare unable to transfer information, and their total number of parameters\nbecomes prohibitively expensive as the number of languages grows. In this work,\nwe overcome these drawbacks using hyper-adapters -- hyper-networks that\ngenerate adapters from language and layer embeddings. While past work had poor\nresults when scaling hyper-networks, we propose a rescaling fix that\nsignificantly improves convergence and enables training larger hyper-networks.\nWe find that hyper-adapters are more parameter efficient than regular adapters,\nreaching the same performance with up to 12 times less parameters. When using\nthe same number of parameters and FLOPS, our approach consistently outperforms\nregular adapters. Also, hyper-adapters converge faster than alternative\napproaches and scale better than regular dense networks. Our analysis shows\nthat hyper-adapters learn to encode language relatedness, enabling positive\ntransfer across languages.",
    "descriptor": "",
    "authors": [
      "Christos Baziotis",
      "Mikel Artetxe",
      "James Cross",
      "Shruti Bhosale"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10835"
  },
  {
    "id": "arXiv:2205.10836",
    "title": "On the Price of Fairness of Allocating Contiguous Blocks",
    "abstract": "In this work, we revisit the problem of fairly allocating a number of\nindivisible items that are located on a line to multiple agents. A feasible\nallocation requires that the allocated items to each agent are connected on the\nline. The items can be goods on which agents have non-negative utilities, or\nchores on which the utilities are non-positive. Our objective is to understand\nthe extent to which welfare is inevitably sacrificed by enforcing the\nallocations to be fair, i.e., price of fairness (PoF). We study both\negalitarian and utilitarian welfare. Previous works by Suksompong [Discret.\nAppl. Math., 2019] and H\\\"ohne and van Stee [Inf. Comput., 2021] have studied\nPoF regarding the notions of envy-freeness and proportionality. However, these\nfair allocations barely exist for indivisible items, and thus in this work, we\nfocus on the relaxations of maximin share fairness and proportionality up to\none item, which are guaranteed to be satisfiable. For most settings, we give\n(almost) tight ratios of PoF and all the upper bounds are proved by designing\npolynomial time algorithms.",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Ankang Sun",
      "Bo Li"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.10836"
  },
  {
    "id": "arXiv:2205.10837",
    "title": "Neural Inverse Kinematics",
    "abstract": "Inverse kinematic (IK) methods recover the parameters of the joints, given\nthe desired position of selected elements in the kinematic chain. While the\nproblem is well-defined and low-dimensional, it has to be solved rapidly,\naccounting for multiple possible solutions. In this work, we propose a neural\nIK method that employs the hierarchical structure of the problem to\nsequentially sample valid joint angles conditioned on the desired position and\non the preceding joints along the chain. In our solution, a hypernetwork $f$\nrecovers the parameters of multiple primary networks {$g_1,g_2,\\dots,g_N$,\nwhere $N$ is the number of joints}, such that each $g_i$ outputs a distribution\nof possible joint angles, and is conditioned on the sampled values obtained\nfrom the previous primary networks $g_j, j<i$. The hypernetwork can be trained\non readily available pairs of matching joint angles and positions, without\nobserving multiple solutions. At test time, a high-variance joint distribution\nis presented, by sampling sequentially from the primary networks. We\ndemonstrate the advantage of the proposed method both in comparison to other IK\nmethods for isolated instances of IK and with regard to following the path of\nthe end effector in Cartesian space.",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Raphael Bensadoun",
      "Shir Gur",
      "Nitsan Blau",
      "Tom Shenkar",
      "Lior Wolf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10837"
  },
  {
    "id": "arXiv:2205.10838",
    "title": "Grad-CAM++ is Equivalent to Grad-CAM With Positive Gradients",
    "abstract": "The Grad-CAM algorithm provides a way to identify what parts of an image\ncontribute most to the output of a classifier deep network. The algorithm is\nsimple and widely used for localization of objects in an image, although some\nresearchers have point out its limitations, and proposed various alternatives.\nOne of them is Grad-CAM++, that according to its authors can provide better\nvisual explanations for network predictions, and does a better job at locating\nobjects even for occurrences of multiple object instances in a single image.\nHere we show that Grad-CAM++ is practically equivalent to a very simple\nvariation of Grad-CAM in which gradients are replaced with positive gradients.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Miguel Lerma",
      "Mirtha Lucas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10838"
  },
  {
    "id": "arXiv:2205.10839",
    "title": "Deep Learning for Visual Speech Analysis: A Survey",
    "abstract": "Visual speech, referring to the visual domain of speech, has attracted\nincreasing attention due to its wide applications, such as public security,\nmedical treatment, military defense, and film entertainment. As a powerful AI\nstrategy, deep learning techniques have extensively promoted the development of\nvisual speech learning. Over the past five years, numerous deep learning based\nmethods have been proposed to address various problems in this area, especially\nautomatic visual speech recognition and generation. To push forward future\nresearch on visual speech, this paper aims to present a comprehensive review of\nrecent progress in deep learning methods on visual speech analysis. We cover\ndifferent aspects of visual speech, including fundamental problems, challenges,\nbenchmark datasets, a taxonomy of existing methods, and state-of-the-art\nperformance. Besides, we also identify gaps in current research and discuss\ninspiring future research directions.",
    "descriptor": "\nComments: 20 pages, 8 figures\n",
    "authors": [
      "Changchong Sheng",
      "Gangyao Kuang",
      "Liang Bai",
      "Chenping Hou",
      "Yulan Guo",
      "Xin Xu",
      "Matti Pietik\u00e4inen",
      "Li Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10839"
  },
  {
    "id": "arXiv:2205.10840",
    "title": "Self-supervised U-net for few-shot learning of object segmentation in  microscopy images",
    "abstract": "State-of-the-art segmentation performances are achieved by deep neural\nnetworks. Training these networks from only a few training examples is\nchallenging while producing annotated images that provide supervision is\ntedious. Recently, self-supervision, i.e. designing a neural pipeline providing\nsynthetic or indirect supervision, has proved to significantly increase\ngeneralization performances of models trained on few shots. This paper\nintroduces one such neural pipeline in the context of microscopic image\nsegmentation. By leveraging the rather simple content of these images a trainee\nnetwork can be mentored by a referee network which has been previously trained\non synthetically generated pairs of corrupted/correct region masks.",
    "descriptor": "",
    "authors": [
      "Arnaud Deleruyelle",
      "Cristian Versari",
      "John Klein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10840"
  },
  {
    "id": "arXiv:2205.10841",
    "title": "Robust Modeling and Controls for Racing on the Edge",
    "abstract": "Race cars are routinely driven to the edge of their handling limits in\ndynamic scenarios well above 200mph. Similar challenges are posed in autonomous\nracing, where a software stack, instead of a human driver, interacts within a\nmulti-agent environment. For an Autonomous Racing Vehicle (ARV), operating at\nthe edge of handling limits and acting safely in these dynamic environments is\nstill an unsolved problem. In this paper, we present a baseline controls stack\nfor an ARV capable of operating safely up to 140mph. Additionally, limitations\nin the current approach are discussed to highlight the need for improved\ndynamics modeling and learning.",
    "descriptor": "",
    "authors": [
      "Joshua Spisak",
      "Andrew Saba",
      "Nayana Suvarna",
      "Brian Mao",
      "Chuan Tian Zhang",
      "Chris Chang",
      "Sebastian Scherer",
      "Deva Ramanan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10841"
  },
  {
    "id": "arXiv:2205.10842",
    "title": "Addressing Strategic Manipulation Disparities in Fair Classification",
    "abstract": "In real-world classification settings, individuals respond to classifier\npredictions by updating their features to increase their likelihood of\nreceiving a particular (positive) decision (at a certain cost). Yet, when\ndifferent demographic groups have different feature distributions or different\ncost functions, prior work has shown that individuals from minority groups\noften pay a higher cost to update their features. Fair classification aims to\naddress such classifier performance disparities by constraining the classifiers\nto satisfy statistical fairness properties. However, we show that standard\nfairness constraints do not guarantee that the constrained classifier reduces\nthe disparity in strategic manipulation cost. To address such biases in\nstrategic settings and provide equal opportunities for strategic manipulation,\nwe propose a constrained optimization framework that constructs classifiers\nthat lower the strategic manipulation cost for the minority groups. We develop\nour framework by studying theoretical connections between group-specific\nstrategic cost disparity and standard selection rate fairness metrics (e.g.,\nstatistical rate and true positive rate). Empirically, we show the efficacy of\nthis approach over multiple real-world datasets.",
    "descriptor": "",
    "authors": [
      "Vijay Keswani",
      "L. Elisa Celis"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10842"
  },
  {
    "id": "arXiv:2205.10843",
    "title": "Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in  E-commerce",
    "abstract": "In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for\nwidespread applications such as product search and recommendation. For example,\nwhen users search for \"running\" in e-commerce, they would like to find items\nhighly related to running, such as \"running shoes\" rather than \"shoes\".\nHowever, many existing CSK collections rank statements solely by confidence\nscores, and there is no information about which ones are salient from a human\nperspective. In this work, we define the task of supervised salience\nevaluation, where given a CSK triple, the model is required to learn whether\nthe triple is salient or not. In addition to formulating the new task, we also\nrelease a new Benchmark dataset of Salience Evaluation in E-commerce (BSEE) and\nhope to promote related research on commonsense knowledge salience evaluation.\nWe conduct experiments in the dataset with several representative baseline\nmodels. The experimental results show that salience evaluation is a hard task\nwhere models perform poorly on our evaluation set. We further propose a simple\nbut effective approach, PMI-tuning, which shows promise for solving this novel\nproblem.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Yincen Qu",
      "Ningyu Zhang",
      "Hui Chen",
      "Zelin Dai",
      "Zezhong Xu",
      "Chengming Wang",
      "Xiaoyu Wang",
      "Qiang Chen",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10843"
  },
  {
    "id": "arXiv:2205.10848",
    "title": "Robust Quantity-Aware Aggregation for Federated Learning",
    "abstract": "Federated learning (FL) enables multiple clients to collaboratively train\nmodels without sharing their local data, and becomes an important\nprivacy-preserving machine learning framework. However, classical FL faces\nserious security and robustness problem, e.g., malicious clients can poison\nmodel updates and at the same time claim large quantities to amplify the impact\nof their model updates in the model aggregation. Existing defense methods for\nFL, while all handling malicious model updates, either treat all quantities\nbenign or simply ignore/truncate the quantities of all clients. The former is\nvulnerable to quantity-enhanced attack, while the latter leads to sub-optimal\nperformance since the local data on different clients is usually in\nsignificantly different sizes. In this paper, we propose a robust\nquantity-aware aggregation algorithm for federated learning, called FedRA, to\nperform the aggregation with awareness of local data quantities while being\nable to defend against quantity-enhanced attacks. More specifically, we propose\na method to filter malicious clients by jointly considering the uploaded model\nupdates and data quantities from different clients, and performing\nquantity-aware weighted averaging on model updates from remaining clients.\nMoreover, as the number of malicious clients participating in the federated\nlearning may dynamically change in different rounds, we also propose a\nmalicious client number estimator to predict how many suspicious clients should\nbe filtered in each round. Experiments on four public datasets demonstrate the\neffectiveness of our FedRA method in defending FL against quantity-enhanced\nattacks.",
    "descriptor": "",
    "authors": [
      "Jingwei Yi",
      "Fangzhao Wu",
      "Huishuai Zhang",
      "Bin Zhu",
      "Tao Qi",
      "Guangzhong Sun",
      "Xing Xie"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10848"
  },
  {
    "id": "arXiv:2205.10850",
    "title": "AFEC: A Knowledge Graph Capturing Social Intelligence in Casual  Conversations",
    "abstract": "This paper introduces AFEC, an automatically curated knowledge graph based on\npeople's day-to-day casual conversations. The knowledge captured in this graph\nbears potential for conversational systems to understand how people offer\nacknowledgement, consoling, and a wide range of empathetic responses in social\nconversations. For this body of knowledge to be comprehensive and meaningful,\nwe curated a large-scale corpus from the r/CasualConversation SubReddit. After\ntaking the first two turns of all conversations, we obtained 134K speaker nodes\nand 666K listener nodes. To demonstrate how a chatbot can converse in social\nsettings, we built a retrieval-based chatbot and compared it with existing\nempathetic dialog models. Experiments show that our model is capable of\ngenerating much more diverse responses (at least 15% higher diversity scores in\nhuman evaluation), while still outperforming two out of the four baselines in\nterms of response quality.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Yubo Xie",
      "Junze Li",
      "Pearl Pu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10850"
  },
  {
    "id": "arXiv:2205.10851",
    "title": "Vision-based Anti-UAV Detection and Tracking",
    "abstract": "Unmanned aerial vehicles (UAV) have been widely used in various fields, and\ntheir invasion of security and privacy has aroused social concern. Several\ndetection and tracking systems for UAVs have been introduced in recent years,\nbut most of them are based on radio frequency, radar, and other media. We\nassume that the field of computer vision is mature enough to detect and track\ninvading UAVs. Thus we propose a visible light mode dataset called Dalian\nUniversity of Technology Anti-UAV dataset, DUT Anti-UAV for short. It contains\na detection dataset with a total of 10,000 images and a tracking dataset with\n20 videos that include short-term and long-term sequences. All frames and\nimages are manually annotated precisely. We use this dataset to train several\nexisting detection algorithms and evaluate the algorithms' performance. Several\ntracking methods are also tested on our tracking dataset. Furthermore, we\npropose a clear and simple tracking algorithm combined with detection that\ninherits the detector's high precision. Extensive experiments show that the\ntracking performance is improved considerably after fusing detection, thus\nproviding a new attempt at UAV tracking using our dataset.The datasets and\nresults are publicly available at: https://github.com/wangdongdut/DUT-Anti-UAV",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Intelligent Transportation Systems\n",
    "authors": [
      "Jie Zhao",
      "Jingshu Zhang",
      "Dongdong Li",
      "Dong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10851"
  },
  {
    "id": "arXiv:2205.10852",
    "title": "Relphormer: Relational Graph Transformer for Knowledge Graph  Representation",
    "abstract": "Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, in the knowledge graph representation, where translational distance\nparadigm dominates this area, vanilla Transformer architectures have not\nyielded promising improvements. Note that vanilla Transformer architectures\nstruggle to capture the intrinsically semantic and structural information of\nknowledge graphs and can hardly scale to long-distance neighbors due to\nquadratic dependency. To this end, we propose a new variant of Transformer for\nknowledge graph representation dubbed Relphormer. Specifically, we introduce\nTriple2Seq which can dynamically sample contextualized sub-graph sequences as\nthe input of the Transformer to alleviate the scalability issue. We then\npropose a novel structure-enhanced self-attention mechanism to encode the\nrelational information and keep the globally semantic information among\nsub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm\nfor knowledge graph representation learning to unify different link prediction\ntasks. Experimental results show that our approach can obtain better\nperformance on benchmark datasets compared with baselines.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Zhen Bi",
      "Siyuan Cheng",
      "Ningyu Zhang",
      "Xiaozhuan Liang",
      "Feiyu Xiong",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10852"
  },
  {
    "id": "arXiv:2205.10855",
    "title": "Secrecy Outage Probability Fairness for Intelligent Reflecting  Surface-Assisted Uplink Channel",
    "abstract": "This paper investigates physical layer security (PLS) in the intelligent\nreflecting surface (IRS)-assisted multiple-user uplink channel. Since the\ninstantaneous eavesdropper's channel state information (CSI) is unavailable,\nthe secrecy rate can not be measured. In this case, existing investigations\nusually focus on the maximization of the minimum (max-min) of signal to\ninterference plus noise power ratio (SINRs) among multiple users, and do not\nconsider secrecy outage probability caused by eavesdroppers. In this paper, we\nfirst formulate the minimization of the maximum (min-max) secrecy outage\nprobability among multiple users. The formulated problem is solved by\nalternately optimizing receiving matrix and phase shift matrix. Simulations\ndemonstrate that the maximum secrecy outage probability is significantly\nreduced with the proposed algorithm compared to max-min SINR strategies,\nmeaning our scheme has a higher security performance.",
    "descriptor": "",
    "authors": [
      "Xiangrui Cheng",
      "Yiliang Liu",
      "Zhou Su",
      "Wei Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10855"
  },
  {
    "id": "arXiv:2205.10857",
    "title": "RVAE-LAMOL: Residual Variational Autoencoder to Enhance Lifelong  Language Learning",
    "abstract": "Lifelong Language Learning (LLL) aims to train a neural network to learn a\nstream of NLP tasks while retaining knowledge from previous tasks. However,\nprevious works which followed data-free constraint still suffer from\ncatastrophic forgetting issue, where the model forgets what it just learned\nfrom previous tasks. In order to alleviate catastrophic forgetting, we propose\nthe residual variational autoencoder (RVAE) to enhance LAMOL, a recent LLL\nmodel, by mapping different tasks into a limited unified semantic space. In\nthis space, previous tasks are easy to be correct to their own distribution by\npseudo samples. Furthermore, we propose an identity task to make the model is\ndiscriminative to recognize the sample belonging to which task. For training\nRVAE-LAMOL better, we propose a novel training scheme Alternate Lag Training.\nIn the experiments, we test RVAE-LAMOL on permutations of three datasets from\nDecaNLP. The experimental results demonstrate that RVAE-LAMOL outperforms\nna\\\"ive LAMOL on all permutations and generates more meaningful pseudo-samples.",
    "descriptor": "\nComments: This paper has been accepted for publication at IJCNN 2022 on IEEE WCCI 2022; Oral presentation\n",
    "authors": [
      "Han Wang",
      "Ruiliu Fu",
      "Xuejun Zhang",
      "Jun Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10857"
  },
  {
    "id": "arXiv:2205.10860",
    "title": "Positioning Fog Computing for Smart Manufacturing",
    "abstract": "We study machine learning systems for real-time industrial quality control.\nIn many factory systems, production processes must be continuously controlled\nin order to maintain product quality. Especially challenging are the systems\nthat must balance in real-time between stringent resource consumption\nconstraints and the risk of defective end-product. There is a need for\nautomated quality control systems as human control is tedious and error-prone.\nWe see machine learning as a viable choice for developing automated quality\ncontrol systems, but integrating such system with existing factory automation\nremains a challenge. In this paper we propose introducing a new fog computing\nlayer to the standard hierarchy of automation control to meet the needs of\nmachine learning driven quality control.",
    "descriptor": "",
    "authors": [
      "Jaakko Harjuhahto",
      "Vesa Hirvisalo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10860"
  },
  {
    "id": "arXiv:2205.10866",
    "title": "Blackbird's language matrices (BLMs): a new benchmark to investigate  disentangled generalisation in neural networks",
    "abstract": "Current successes of machine learning architectures are based on\ncomputationally expensive algorithms and prohibitively large amounts of data.\nWe need to develop tasks and data to train networks to reach more complex and\nmore compositional skills. In this paper, we illustrate Blackbird's language\nmatrices (BLMs), a novel grammatical dataset developed to test a linguistic\nvariant of Raven's progressive matrices, an intelligence test usually based on\nvisual stimuli. The dataset consists of 44800 sentences, generatively\nconstructed to support investigations of current models' linguistic mastery of\ngrammatical agreement rules and their ability to generalise them. We present\nthe logic of the dataset, the method to automatically construct data on a large\nscale and the architecture to learn them. Through error analysis and several\nexperiments on variations of the dataset, we demonstrate that this language\ntask and the data that instantiate it provide a new challenging testbed to\nunderstand generalisation and abstraction.",
    "descriptor": "\nComments: 15 pages, 9 figures, 1 table\n",
    "authors": [
      "Paola Merlo",
      "Aixiu An",
      "Maria A. Rodriguez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10866"
  },
  {
    "id": "arXiv:2205.10868",
    "title": "Memory-efficient Reinforcement Learning with Knowledge Consolidation",
    "abstract": "Artificial neural networks are promising as general function approximators\nbut challenging to train on non-independent and identically distributed data\ndue to catastrophic forgetting. Experience replay, a standard component in deep\nreinforcement learning, is often used to reduce forgetting and improve sample\nefficiency by storing experiences in a large buffer and using them for training\nlater. However, a large replay buffer results in a heavy memory burden,\nespecially for onboard and edge devices with limited memory capacities. We\npropose memory-efficient reinforcement learning algorithms based on the deep\nQ-network algorithm to alleviate this problem. Our algorithms reduce forgetting\nand maintain high sample efficiency by consolidating knowledge from the target\nQ-network to the current Q-network. Compared to baseline methods, our\nalgorithms achieve comparable or better performance on both feature-based and\nimage-based tasks while easing the burden of large experience replay buffers.",
    "descriptor": "",
    "authors": [
      "Qingfeng Lan",
      "Yangchen Pan",
      "Jun Luo",
      "A. Rupam Mahmood"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10868"
  },
  {
    "id": "arXiv:2205.10872",
    "title": "Fusion Subspace Clustering for Incomplete Data",
    "abstract": "This paper introduces {\\em fusion subspace clustering}, a novel method to\nlearn low-dimensional structures that approximate large scale yet highly\nincomplete data. The main idea is to assign each datum to a subspace of its\nown, and minimize the distance between the subspaces of all data, so that\nsubspaces of the same cluster get {\\em fused} together. Our method allows low,\nhigh, and even full-rank data; it directly accounts for noise, and its sample\ncomplexity approaches the information-theoretic limit. In addition, our\napproach provides a natural model selection {\\em clusterpath}, and a direct\ncompletion method. We give convergence guarantees, analyze computational\ncomplexity, and show through extensive experiments on real and synthetic data\nthat our approach performs comparably to the state-of-the-art with complete\ndata, and dramatically better if data is missing.",
    "descriptor": "\nComments: Accepted at IJCNN 2022. arXiv admin note: substantial text overlap with arXiv:1808.00628\n",
    "authors": [
      "Usman Mahmood",
      "Daniel Pimentel-Alarc\u00f3n"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10872"
  },
  {
    "id": "arXiv:2205.10873",
    "title": "Dynamic Query Selection for Fast Visual Perceiver",
    "abstract": "Transformers have been matching deep convolutional networks for vision\narchitectures in recent works. Most work is focused on getting the best results\non large-scale benchmarks, and scaling laws seem to be the most successful\nstrategy: bigger models, more data, and longer training result in higher\nperformance. However, the reduction of network complexity and inference time\nremains under-explored. The Perceiver model offers a solution to this problem:\nby first performing a Cross-attention with a fixed number Q of latent query\ntokens, the complexity of the L-layers Transformer network that follows is\nbounded by O(LQ^2). In this work, we explore how to make Perceivers even more\nefficient, by reducing the number of queries Q during inference while limiting\nthe accuracy drop.",
    "descriptor": "\nComments: Accepted at the Transformer for Vision workshop, CVPR 2022\n",
    "authors": [
      "Corentin Dancette",
      "Matthieu Cord"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10873"
  },
  {
    "id": "arXiv:2205.10878",
    "title": "Geo-Localization via Ground-to-Satellite Cross-View Image Retrieval",
    "abstract": "The large variation of viewpoint and irrelevant content around the target\nalways hinder accurate image retrieval and its subsequent tasks. In this paper,\nwe investigate an extremely challenging task: given a ground-view image of a\nlandmark, we aim to achieve cross-view geo-localization by searching out its\ncorresponding satellite-view images. Specifically, the challenge comes from the\ngap between ground-view and satellite-view, which includes not only large\nviewpoint changes (some parts of the landmark may be invisible from front view\nto top view) but also highly irrelevant background (the target landmark tend to\nbe hidden in other surrounding buildings), making it difficult to learn a\ncommon representation or a suitable mapping.\nTo address this issue, we take advantage of drone-view information as a\nbridge between ground-view and satellite-view domains. We propose a Peer\nLearning and Cross Diffusion (PLCD) framework. PLCD consists of three parts: 1)\na peer learning across ground-view and drone-view to find visible parts to\nbenefit ground-drone cross-view representation learning; 2) a patch-based\nnetwork for satellite-drone cross-view representation learning; 3) a cross\ndiffusion between ground-drone space and satellite-drone space. Extensive\nexperiments conducted on the University-Earth and University-Google datasets\nshow that our method outperforms state-of-the-arts significantly.",
    "descriptor": "\nComments: 13 pages, 10 figures\n",
    "authors": [
      "Zelong Zeng",
      "Zheng Wang",
      "Fan Yang",
      "Shin'ichi Satoh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10878"
  },
  {
    "id": "arXiv:2205.10879",
    "title": "Fast Gaussian Process Posterior Mean Prediction via Local Cross  Validation and Precomputation",
    "abstract": "Gaussian processes (GPs) are Bayesian non-parametric models useful in a\nmyriad of applications. Despite their popularity, the cost of GP predictions\n(quadratic storage and cubic complexity with respect to the number of training\npoints) remains a hurdle in applying GPs to large data. We present a fast\nposterior mean prediction algorithm called FastMuyGPs to address this\nshortcoming. FastMuyGPs is based upon the MuyGPs hyperparameter estimation\nalgorithm and utilizes a combination of leave-one-out cross-validation,\nbatching, nearest neighbors sparsification, and precomputation to provide\nscalable, fast GP prediction. We demonstrate several benchmarks wherein\nFastMuyGPs prediction attains superior accuracy and competitive or superior\nruntime to both deep neural networks and state-of-the-art scalable GP\nalgorithms.",
    "descriptor": "\nComments: 9 pages content, 4 figures, 3 tables\n",
    "authors": [
      "Alec M. Dunton",
      "Benjamin W. Priest",
      "Amanda Muyskens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10879"
  },
  {
    "id": "arXiv:2205.10884",
    "title": "Sequence-to-Action: Grammatical Error Correction with Action Guided  Sequence Generation",
    "abstract": "The task of Grammatical Error Correction (GEC) has received remarkable\nattention with wide applications in Natural Language Processing (NLP) in recent\nyears. While one of the key principles of GEC is to keep the correct parts\nunchanged and avoid over-correction, previous sequence-to-sequence (seq2seq)\nmodels generate results from scratch, which are not guaranteed to follow the\noriginal sentence structure and may suffer from the over-correction problem. In\nthe meantime, the recently proposed sequence tagging models can overcome the\nover-correction problem by only generating edit operations, but are conditioned\non human designed language-specific tagging labels. In this paper, we combine\nthe pros and alleviate the cons of both models by proposing a novel\nSequence-to-Action~(S2A) module. The S2A module jointly takes the source and\ntarget sentences as input, and is able to automatically generate a token-level\naction sequence before predicting each token, where each action is generated\nfrom three choices named SKIP, COPY and GENerate. Then the actions are fused\nwith the basic seq2seq framework to provide final predictions. We conduct\nexperiments on the benchmark datasets of both English and Chinese GEC tasks.\nOur model consistently outperforms the seq2seq baselines, while being able to\nsignificantly alleviate the over-correction problem as well as holding better\ngenerality and diversity in the generation results compared to the sequence\ntagging models.",
    "descriptor": "\nComments: accepted in AAAI 2022\n",
    "authors": [
      "Jiquan Li",
      "Junliang Guo",
      "Yongxin Zhu",
      "Xin Sheng",
      "Deqiang Jiang",
      "Bo Ren",
      "Linli Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10884"
  },
  {
    "id": "arXiv:2205.10889",
    "title": "Wireless On-Chip Communications for Scalable In-memory Hyperdimensional  Computing",
    "abstract": "Hyperdimensional computing (HDC) is an emerging computing paradigm that\nrepresents, manipulates, and communicates data using very long random vectors\n(aka hypervectors). Among different hardware platforms capable of executing HDC\nalgorithms, in-memory computing (IMC) systems have been recently proved to be\none of the most energy-efficient options, due to hypervector manipulations in\nthe memory itself that reduces data movement. Although implementations of HDC\non single IMC cores have been made, their parallelization is still unresolved\ndue to the communication challenges that these novel architectures impose and\nthat traditional Networks-on-Chip and Networks-in-Package were not designed\nfor. To cope with this difficulty, we propose the use of wireless on-chip\ncommunication technology in unique ways. We are particularly interested in\nphysically distributing a large number of IMC cores performing similarity\nsearch across a chip, and maintaining the classification accuracy when each of\nwhich is queried with a slightly different version of a bundled hypervector. To\nachieve it, we introduce a novel over-the-air computing that consists of\ndefining different binary decision regions in the receivers so as to compute\nthe logical majority operation (i.e., bundling, or superposition) required in\nHDC. It introduces moderate overheads of a single antenna and receiver per IMC\ncore. By doing so, we achieve a joint broadcast distribution and computation\nwith a performance and efficiency unattainable with wired interconnects, which\nin turn enables massive parallelization of the architecture. It is demonstrated\nthat the proposed approach allows to both bundle at least three hypervectors\nand scale similarity search to 64 IMC cores seamlessly, while incurring an\naverage bit error ratio of 0.01 without any impact in the accuracy of a generic\nHDC-based classifier working with 512-bit vectors.",
    "descriptor": "\nComments: This paper has been accepted at 2022 IEEE International Joint Conference on Neural Networks (IJCNN)\n",
    "authors": [
      "Robert Guirado",
      "Abbas Rahimi",
      "Geethan Karunaratne",
      "Eduard Alarc\u00f3n",
      "Abu Sebastian",
      "Sergi Abadal"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.10889"
  },
  {
    "id": "arXiv:2205.10893",
    "title": "Thor: Wielding Hammers to Integrate Language Models and Automated  Theorem Provers",
    "abstract": "In theorem proving, the task of selecting useful premises from a large\nlibrary to unlock the proof of a given conjecture is crucially important. This\npresents a challenge for all theorem provers, especially the ones based on\nlanguage models, due to their relative inability to reason over huge volumes of\npremises in text form. This paper introduces Thor, a framework integrating\nlanguage models and automated theorem provers to overcome this difficulty. In\nThor, a class of methods called hammers that leverage the power of automated\ntheorem provers are used for premise selection, while all other tasks are\ndesignated to language models. Thor increases a language model's success rate\non the PISA dataset from $39\\%$ to $57\\%$, while solving $8.2\\%$ of problems\nneither language models nor automated theorem provers are able to solve on\ntheir own. Furthermore, with a significantly smaller computational budget, Thor\ncan achieve a success rate on the MiniF2F dataset that is on par with the best\nexisting methods. Thor can be instantiated for the majority of popular\ninteractive theorem provers via a straightforward protocol we provide.",
    "descriptor": "",
    "authors": [
      "Albert Q. Jiang",
      "Wenda Li",
      "Szymon Tworkowski",
      "Konrad Czechowski",
      "Tomasz Odrzyg\u00f3\u017ad\u017a",
      "Piotr Mi\u0142o\u015b",
      "Yuhuai Wu",
      "Mateja Jamnik"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10893"
  },
  {
    "id": "arXiv:2205.10894",
    "title": "Human and technological infrastructures of fact-checking",
    "abstract": "Increasing demands for fact-checking has led to a growing interest in\ndeveloping systems and tools to automate the fact-checking process. However,\nsuch systems are limited in practice because their system design often does not\ntake into account how fact-checking is done in the real world and ignores the\ninsights and needs of various stakeholder groups core to the fact-checking\nprocess. This paper unpacks the fact-checking process by revealing the\ninfrastructures -- both human and technological -- that support and shape\nfact-checking work. We interviewed 26 participants belonging to 16\nfact-checking teams and organizations with representation from 4 continents.\nThrough these interviews, we describe the human infrastructure of fact-checking\nby identifying and presenting, in-depth, the roles of six primary stakeholder\ngroups, 1) Editors, 2) External fact-checkers, 3) In-house fact-checkers, 4)\nInvestigators and researchers, 5) Social media managers, and 6) Advocators. Our\nfindings highlight that the fact-checking process is a collaborative effort\namong various stakeholder groups and associated technological and informational\ninfrastructures. By rendering visibility to the infrastructures, we reveal how\nfact-checking has evolved to include both short-term claims centric and\nlong-term advocacy centric fact-checking. Our work also identifies key social\nand technical needs and challenges faced by each stakeholder group. Based on\nour findings, we suggest that improving the quality of fact-checking requires\nsystematic changes in the civic, informational, and technological contexts.",
    "descriptor": "",
    "authors": [
      "Prerna Juneja",
      "Tanushree Mitra"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.10894"
  },
  {
    "id": "arXiv:2205.10895",
    "title": "Contextual Information-Directed Sampling",
    "abstract": "Information-directed sampling (IDS) has recently demonstrated its potential\nas a data-efficient reinforcement learning algorithm. However, it is still\nunclear what is the right form of information ratio to optimize when contextual\ninformation is available. We investigate the IDS design through two contextual\nbandit problems: contextual bandits with graph feedback and sparse linear\ncontextual bandits. We provably demonstrate the advantage of contextual IDS\nover conditional IDS and emphasize the importance of considering the context\ndistribution. The main message is that an intelligent agent should invest more\non the actions that are beneficial for the future unseen contexts while the\nconditional IDS can be myopic. We further propose a computationally-efficient\nversion of contextual IDS based on Actor-Critic and evaluate it empirically on\na neural network contextual bandit.",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Botao Hao",
      "Tor Lattimore",
      "Chao Qin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10895"
  },
  {
    "id": "arXiv:2205.10898",
    "title": "Mesh-free collocation for surface differential operators",
    "abstract": "We present a mesh-free collocation scheme to discretize intrinsic surface\ndifferential operators over surface point clouds with given normal vectors. The\nmethod is based on Discretization-Corrected Particle Strength Exchange\n(DC-PSE), which generalizes finite difference methods to mesh-free point clouds\nand moving Lagrangian particles. The resulting Surface DC-PSE method is derived\nfrom an embedding theorem, but we analytically reduce the operator kernels\nalong the surface normals, resulting in an embedding-free, purely\nsurface-intrinsic computational scheme. We benchmark the scheme by discretizing\nthe Laplace-Beltrami operator on a circle and a sphere, and present convergence\nresults for both explicit and implicit solvers. We then showcase the algorithm\non the problem of computing mean curvature of an ellipsoid and of the Stanford\nBunny by evaluating the surface divergence of the normal vector field with the\nproposed Surface DC-PSE method.",
    "descriptor": "\nComments: 15 pages, 4 figures, 28 references\n",
    "authors": [
      "Abhinav Singh",
      "Alejandra Foggia",
      "Pietro Incardona",
      "Ivo F. Sbalzarini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10898"
  },
  {
    "id": "arXiv:2205.10900",
    "title": "Visual Explanations from Deep Networks via Riemann-Stieltjes Integrated  Gradient-based Localization",
    "abstract": "Neural networks are becoming increasingly better at tasks that involve\nclassifying and recognizing images. At the same time techniques intended to\nexplain the network output have been proposed. One such technique is the\nGradient-based Class Activation Map (Grad-CAM), which is able to locate\nfeatures of an input image at various levels of a convolutional neural network\n(CNN), but is sensitive to the vanishing gradients problem. There are\ntechniques such as Integrated Gradients (IG), that are not affected by that\nproblem, but its use is limited to the input layer of a network. Here we\nintroduce a new technique to produce visual explanations for the predictions of\na CNN. Like Grad-CAM, our method can be applied to any layer of the network,\nand like Integrated Gradients it is not affected by the problem of vanishing\ngradients. For efficiency, gradient integration is performed numerically at the\nlayer level using a Riemann-Stieltjes sum approximation. Compared to Grad-CAM,\nheatmaps produced by our algorithm are better focused in the areas of interest,\nand their numerical computation is more stable. Our code is available at\nhttps://github.com/mlerma54/RSIGradCAM",
    "descriptor": "\nComments: 16 pages, 33 figures\n",
    "authors": [
      "Mirtha Lucas",
      "Miguel Lerma",
      "Jacob Furst",
      "Daniela Raicu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10900"
  },
  {
    "id": "arXiv:2205.10902",
    "title": "The Case for Perspective in Multimodal Datasets",
    "abstract": "This paper argues in favor of the adoption of annotation practices for\nmultimodal datasets that recognize and represent the inherently perspectivized\nnature of multimodal communication. To support our claim, we present a set of\nannotation experiments in which FrameNet annotation is applied to the Multi30k\nand the Flickr 30k Entities datasets. We assess the cosine similarity between\nthe semantic representations derived from the annotation of both pictures and\ncaptions for frames. Our findings indicate that: (i) frame semantic similarity\nbetween captions of the same picture produced in different languages is\nsensitive to whether the caption is a translation of another caption or not,\nand (ii) picture annotation for semantic frames is sensitive to whether the\nimage is annotated in presence of a caption or not.",
    "descriptor": "\nComments: Accepted submission for the 1st Workshop on Perspectivist Approaches to NLP (NLPerspectives)\n",
    "authors": [
      "Marcelo Viridiano",
      "Tiago Timponi Torrent",
      "Oliver Czulo",
      "Arthur Lorenzi Almeida",
      "Ely Edison da Silva Matos",
      "Frederico Belcavello"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10902"
  },
  {
    "id": "arXiv:2205.10906",
    "title": "Monitoring of Perception Systems: Deterministic, Probabilistic, and  Learning-based Fault Detection and Identification",
    "abstract": "This paper investigates runtime monitoring of perception systems. Perception\nis a critical component of high-integrity applications of robotics and\nautonomous systems, such as self-driving cars. In these applications, failure\nof perception systems may put human life at risk, and a broad adoption of these\ntechnologies requires the development of methodologies to guarantee and monitor\nsafe operation. Despite the paramount importance of perception, currently there\nis no formal approach for system-level perception monitoring. In this paper, we\nformalize the problem of runtime fault detection and identification in\nperception systems and present a framework to model diagnostic information\nusing a diagnostic graph. We then provide a set of deterministic,\nprobabilistic, and learning-based algorithms that use diagnostic graphs to\nperform fault detection and identification. Moreover, we investigate\nfundamental limits and provide deterministic and probabilistic guarantees on\nthe fault detection and identification results. We conclude the paper with an\nextensive experimental evaluation, which recreates several realistic failure\nmodes in the LGSVL open-source autonomous driving simulator, and applies the\nproposed system monitors to a state-of-the-art autonomous driving software\nstack (Baidu's Apollo Auto). The results show that the proposed system monitors\noutperform baselines, have the potential of preventing accidents in realistic\nautonomous driving scenarios, and incur a negligible computational overhead.",
    "descriptor": "",
    "authors": [
      "Pasquale Antonante",
      "Heath Nilsen",
      "Luca Carlone"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10906"
  },
  {
    "id": "arXiv:2205.10908",
    "title": "A note on the probabilistic stability of randomized Taylor schemes",
    "abstract": "We study the stability of randomized Taylor schemes for ODEs. We consider\nthree notions of probabilistic stability: asymptotic stability, mean-square\nstability, and stability in probability. We prove fundamental properties of the\nprobabilistic stability regions and benchmark them against the absolute\nstability regions for deterministic Taylor schemes.",
    "descriptor": "",
    "authors": [
      "Tomasz Bochacik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10908"
  },
  {
    "id": "arXiv:2205.10911",
    "title": "Power and accountability in reinforcement learning applications to  environmental policy",
    "abstract": "Machine learning (ML) methods already permeate environmental decision-making,\nfrom processing high-dimensional data on earth systems to monitoring compliance\nwith environmental regulations. Of the ML techniques available to address\npressing environmental problems (e.g., climate change, biodiversity loss),\nReinforcement Learning (RL) may both hold the greatest promise and present the\nmost pressing perils. This paper explores how RL-driven policy refracts\nexisting power relations in the environmental domain while also creating unique\nchallenges to ensuring equitable and accountable environmental decision\nprocesses. We leverage examples from RL applications to climate change\nmitigation and fisheries management to explore how RL technologies shift the\ndistribution of power between resource users, governing bodies, and private\nindustry.",
    "descriptor": "\nComments: 2022 Conference on Neural Information Processing Systems\n",
    "authors": [
      "Melissa Chapman",
      "Caleb Scoville",
      "Marcus Lapeyrolerie",
      "Carl Boettiger"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10911"
  },
  {
    "id": "arXiv:2205.10913",
    "title": "Improved Healthcare Access in Low-resource Regions: A Review of  Technological Solutions",
    "abstract": "Technological advancements have led to significant improvements in healthcare\nfor prevention, diagnosis, treatments, and care. While resourceful regions can\ncapitalize on state-of-the-art healthcare technologies, there might be barriers\nand delays in technology-enabled healthcare availability for a low-resource\nregion. Unique innovations guided by the constraints of low-resource regions\nare required to truly make healthcare technologies ubiquitous and achieve the\ngoal of \"healthcare for all\". In this review, we identified several research\nand development works that have investigated technology-based healthcare\ninnovations targeted at low-resource regions. We found three main pillars of\nwork towards this end: low-cost hardware for the affordability of medical\ndevices, use of information and communication technology (ICT) tools for\nscalability and operational efficiencies in healthcare services, and mobile\nhealth solutions. Several emerging technologies are also promising for\nhealthcare in low-resource regions, such as artificial intelligence, the\nInternet of Things (IoT), and blockchain technology. We discuss these emerging\ntechnologies too in this review.",
    "descriptor": "",
    "authors": [
      "Bishal Lamichhane",
      "Navaraj Neupane"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.10913"
  },
  {
    "id": "arXiv:2205.10914",
    "title": "Weisfeiler and Leman Go Walking: Random Walk Kernels Revisited",
    "abstract": "Random walk kernels have been introduced in seminal work on graph learning\nand were later largely superseded by kernels based on the Weisfeiler-Leman test\nfor graph isomorphism. We give a unified view on both classes of graph kernels.\nWe study walk-based node refinement methods and formally relate them to several\nwidely-used techniques, including Morgan's algorithm for molecule canonization\nand the Weisfeiler-Leman test. We define corresponding walk-based kernels on\nnodes that allow fine-grained parameterized neighborhood comparison, reach\nWeisfeiler-Leman expressiveness, and are computed using the kernel trick. From\nthis we show that classical random walk kernels with only minor modifications\nregarding definition and computation are as expressive as the widely-used\nWeisfeiler-Leman subtree kernel but support non-strict neighborhood comparison.\nWe verify experimentally that walk-based kernels reach or even surpass the\naccuracy of Weisfeiler-Leman kernels in real-world classification tasks.",
    "descriptor": "",
    "authors": [
      "Nils M. Kriege"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10914"
  },
  {
    "id": "arXiv:2205.10916",
    "title": "Privacy-Preserving Data-Enabled Predictive Leading Cruise Control in  Mixed Traffic",
    "abstract": "Data-driven predictive control of connected and automated vehicles (CAVs) has\nreceived increasing attention as it can achieve safe and optimal control\nwithout relying on explicit dynamical models. However, employing the\ndata-driven strategy involves the collection and sharing of privacy-sensitive\nvehicle information, which is vulnerable to privacy leakage and might further\nlead to malicious activities. In this paper, we develop a privacy-preserving\ndata-enabled predictive control scheme for CAVs in a mixed traffic environment,\nwhere human-driven vehicles and CAVs coexist. We tackle external eavesdropper\nand honest-but-curious central unit eavesdropper who wiretap the communication\nchannel of the mixed traffic system and intend to infer the CAVs' state and\ninput information. An affine masking-based privacy protection method is\ndesigned to conceal the true state and input signals, and an extended form of\nthe data-enabled predictive leading cruise control under different matrix\nstructures is derived to achieve privacy-preserving optimal control for CAVs.\nThe proposed scheme can protect the privacy of CAVs against the attackers\nwithout affecting control performance or incurring heavy computations.\nNumerical simulations demonstrate the efficacy of the developed approach.",
    "descriptor": "",
    "authors": [
      "Kaixiang Zhang",
      "Kaian Chen",
      "Zhaojian Li",
      "Jun Chen",
      "Yang Zheng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10916"
  },
  {
    "id": "arXiv:2205.10920",
    "title": "Test-Time Robust Personalization for Federated Learning",
    "abstract": "Federated Learning (FL) is a machine learning paradigm where many clients\ncollaboratively learn a shared global model with decentralized training data.\nPersonalization on FL model additionally adapts the global model to different\nclients, achieving promising results on consistent local training & test\ndistributions. However, for real-world personalized FL applications, it is\ncrucial to go one step further: robustifying FL models under evolving local\ntest set during deployment, where various types of distribution shifts can\narise. In this work, we identify the pitfalls of existing works under test-time\ndistribution shifts and propose a novel test-time robust personalization\nmethod, namely Federated Test-time Head Ensemble plus tuning (FedTHE+). We\nillustrate the advancement of FedTHE+ (and its degraded computationally\nefficient variant FedTHE) over strong competitors, for training various neural\narchitectures (CNN, ResNet, and Transformer) on CIFAR10 and ImageNet and\nevaluating on diverse test distributions. Along with this, we build a benchmark\nfor assessing performance and robustness of personalized FL methods during\ndeployment.",
    "descriptor": "\nComments: LJ and TL contribute equally\n",
    "authors": [
      "Liangze Jiang",
      "Tao Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10920"
  },
  {
    "id": "arXiv:2205.10926",
    "title": "Data-driven, Internet-inspired, and Scalable EV Charging for Power  Distribution Grid",
    "abstract": "Electric vehicles (EVs) are finally making their way onto the roads. However,\nthe challenges concerning their long charging times and their impact on\ncongestion of the power distribution grid are still waiting to be resolved.\nWith historical measurement data, EV chargers can take better-informed actions\nwhile staying mostly off-line. Proposed solutions that depend on heavy\ncommunication and rigorous computation for optimal operation are not scalable.\nThe solutions that do not depend on power distribution topology information,\nsuch as Droop control, are more practical as they only use local measurements.\nHowever, they result in sub-optimal operation due to a lack of a feedback\nmechanism. This study develops a distributed and data-driven congestion\ndetection methodology embedded in the Additive Increase Multiplicative Decrease\n(AIMD) algorithm to control mass EV charging in a distribution grid. The\nproposed distributed AIMD algorithm performs very closely to the ideal AIMD\nregarding fairness and congestion handling. Its communication need is almost as\nlow as the Droop control. The results can provide crucial insights on how we\ncan use data to reveal the inner dynamics and structure of the power grid and\nhelp develop more advanced data-driven algorithms for grid-integrated power\nelectronics control.",
    "descriptor": "\nComments: This work has been submitted to the IEEE Open Access Journal of Power and Energy (OAJPE) and is currently under review\n",
    "authors": [
      "Emin Ucer",
      "Mithat Kisacikoglu",
      "Murat Yuksel",
      "Ali C. Gurbuz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10926"
  },
  {
    "id": "arXiv:2205.10927",
    "title": "Fast ABC-Boost: A Unified Framework for Selecting the Base Class in  Multi-Class Classification",
    "abstract": "The work in ICML'09 showed that the derivatives of the classical multi-class\nlogistic regression loss function could be re-written in terms of a pre-chosen\n\"base class\" and applied the new derivatives in the popular boosting framework.\nIn order to make use of the new derivatives, one must have a strategy to\nidentify/choose the base class at each boosting iteration. The idea of\n\"adaptive base class boost\" (ABC-Boost) in ICML'09, adopted a computationally\nexpensive \"exhaustive search\" strategy for the base class at each iteration. It\nhas been well demonstrated that ABC-Boost, when integrated with trees, can\nachieve substantial improvements in many multi-class classification tasks.\nFurthermore, the work in UAI'10 derived the explicit second-order tree split\ngain formula which typically improved the classification accuracy considerably,\ncompared with using only the fist-order information for tree-splitting, for\nboth multi-class and binary-class classification tasks. In this paper, we\ndevelop a unified framework for effectively selecting the base class by\nintroducing a series of ideas to improve the computational efficiency of\nABC-Boost. Our framework has parameters $(s,g,w)$. At each boosting iteration,\nwe only search for the \"$s$-worst classes\" (instead of all classes) to\ndetermine the base class. We also allow a \"gap\" $g$ when conducting the search.\nThat is, we only search for the base class at every $g+1$ iterations. We\nfurthermore allow a \"warm up\" stage by only starting the search after $w$\nboosting iterations. The parameters $s$, $g$, $w$, can be viewed as tunable\nparameters and certain combinations of $(s,g,w)$ may even lead to better test\naccuracy than the \"exhaustive search\" strategy. Overall, our proposed framework\nprovides a robust and reliable scheme for implementing ABC-Boost in practice.",
    "descriptor": "",
    "authors": [
      "Ping Li",
      "Weijie Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10927"
  },
  {
    "id": "arXiv:2205.10929",
    "title": "rgpdOS: GDPR Enforcement By The Operating System",
    "abstract": "The General Data Protection Regulation (GDPR) forces IT companies to comply\nwith a number of principles when dealing with European citizens' personal data.\nNon-compliant companies are exposed to penalties which may represent up to 4%\nof their turnover. Currently, it is very hard for companies driven by personal\ndata to make their applications GDPR-compliant, especially if those\napplications were developed before the GDPR was established. We present rgpdOS,\na GDPR-aware operating system that aims to bring GDPR-compliance to every\napplication, while requiring minimal changes to application code.",
    "descriptor": "",
    "authors": [
      "Alain Tchana",
      "Raphael Colin",
      "Vincent Berger",
      "Benoit Combemale",
      "Natacha Crooks",
      "Ludovic Pailler"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10929"
  },
  {
    "id": "arXiv:2205.10932",
    "title": "Argumentative Explanations for Pattern-Based Text Classifiers",
    "abstract": "Recent works in Explainable AI mostly address the transparency issue of\nblack-box models or create explanations for any kind of models (i.e., they are\nmodel-agnostic), while leaving explanations of interpretable models largely\nunderexplored. In this paper, we fill this gap by focusing on explanations for\na specific interpretable model, namely pattern-based logistic regression (PLR)\nfor binary text classification. We do so because, albeit interpretable, PLR is\nchallenging when it comes to explanations. In particular, we found that a\nstandard way to extract explanations from this model does not consider\nrelations among the features, making the explanations hardly plausible to\nhumans. Hence, we propose AXPLR, a novel explanation method using (forms of)\ncomputational argumentation to generate explanations (for outputs computed by\nPLR) which unearth model agreements and disagreements among the features.\nSpecifically, we use computational argumentation as follows: we see features\n(patterns) in PLR as arguments in a form of quantified bipolar argumentation\nframeworks (QBAFs) and extract attacks and supports between arguments based on\nspecificity of the arguments; we understand logistic regression as a gradual\nsemantics for these QBAFs, used to determine the arguments' dialectic strength;\nand we study standard properties of gradual semantics for QBAFs in the context\nof our argumentative re-interpretation of PLR, sanctioning its suitability for\nexplanatory purposes. We then show how to extract intuitive explanations (for\noutputs computed by PLR) from the constructed QBAFs. Finally, we conduct an\nempirical evaluation and two experiments in the context of human-AI\ncollaboration to demonstrate the advantages of our resulting AXPLR method.",
    "descriptor": "",
    "authors": [
      "Piyawat Lertvittayakumjorn",
      "Francesca Toni"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10932"
  },
  {
    "id": "arXiv:2205.10933",
    "title": "AutoJoin: Efficient Adversarial Training for Robust Maneuvering via  Denoising Autoencoder and Joint Learning",
    "abstract": "As a result of increasingly adopted machine learning algorithms and\nubiquitous sensors, many 'perception-to-control' systems have been deployed in\nvarious settings. For these systems to be trustworthy, we need to improve their\nrobustness with adversarial training being one approach. In this work, we\npropose a gradient-free adversarial training technique, called AutoJoin.\nAutoJoin is a very simple yet effective and efficient approach to produce\nrobust models for imaged-based autonomous maneuvering. Compared to other SOTA\nmethods with testing on over 5M perturbed and clean images, AutoJoin achieves\nsignificant performance increases up to the 40% range under perturbed datasets\nwhile improving on clean performance for almost every dataset tested. In\nparticular, AutoJoin can triple the clean performance improvement compared to\nthe SOTA work by Shen et al. Regarding efficiency, AutoJoin demonstrates strong\nadvantages over other SOTA techniques by saving up to 83% time per training\nepoch and 90% training data. The core idea of AutoJoin is to use a decoder\nattachment to the original regression model creating a denoising autoencoder\nwithin the architecture. This allows the tasks 'steering' and 'denoising sensor\ninput' to be jointly learnt and enable the two tasks to reinforce each other's\nperformance.",
    "descriptor": "",
    "authors": [
      "Michael Villarreal",
      "Bibek Poudel",
      "Ryan Wickman",
      "Yu Shen",
      "Weizi Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10933"
  },
  {
    "id": "arXiv:2205.10936",
    "title": "On Elimination Strategies for Bandit Fixed-Confidence Identification",
    "abstract": "Elimination algorithms for bandit identification, which prune the plausible\ncorrect answers sequentially until only one remains, are computationally\nconvenient since they reduce the problem size over time. However, existing\nelimination strategies are often not fully adaptive (they update their sampling\nrule infrequently) and are not easy to extend to combinatorial settings, where\nthe set of answers is exponentially large in the problem dimension. On the\nother hand, most existing fully-adaptive strategies to tackle general\nidentification problems are computationally demanding since they repeatedly\ntest the correctness of every answer, without ever reducing the problem size.\nWe show that adaptive methods can be modified to use elimination in both their\nstopping and sampling rules, hence obtaining the best of these two worlds: the\nalgorithms (1) remain fully adaptive, (2) suffer a sample complexity that is\nnever worse of their non-elimination counterpart, and (3) provably eliminate\ncertain wrong answers early. We confirm these benefits experimentally, where\nelimination improves significantly the computational complexity of adaptive\nmethods on common tasks like best-arm identification in linear bandits.",
    "descriptor": "",
    "authors": [
      "Andrea Tirinzoni",
      "R\u00e9my Degenne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10936"
  },
  {
    "id": "arXiv:2205.10937",
    "title": "muNet: Evolving Pretrained Deep Neural Networks into Scalable  Auto-tuning Multitask Systems",
    "abstract": "Most uses of machine learning today involve training a model from scratch for\na particular task, or sometimes starting with a model pretrained on a related\ntask and then fine-tuning on a downstream task. Both approaches offer limited\nknowledge transfer between different tasks, time-consuming human-driven\ncustomization to individual tasks and high computational costs especially when\nstarting from randomly initialized models. We propose a method that uses the\nlayers of a pretrained deep neural network as building blocks to construct an\nML system that can jointly solve an arbitrary number of tasks. The resulting\nsystem can leverage cross tasks knowledge transfer, while being immune from\ncommon drawbacks of multitask approaches such as catastrophic forgetting,\ngradients interference and negative transfer. We define an evolutionary\napproach designed to jointly select the prior knowledge relevant for each task,\nchoose the subset of the model parameters to train and dynamically auto-tune\nits hyperparameters. Furthermore, a novel scale control method is employed to\nachieve quality/size trade-offs that outperform common fine-tuning techniques.\nCompared with standard fine-tuning on a benchmark of 10 diverse image\nclassification tasks, the proposed model improves the average accuracy by 2.39%\nwhile using 47% less parameters per task.",
    "descriptor": "",
    "authors": [
      "Andrea Gesmundo",
      "Jeff Dean"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10937"
  },
  {
    "id": "arXiv:2205.10938",
    "title": "Diversity Enhanced Table-to-Text Generation via Type Control",
    "abstract": "Generating natural language statements to convey information from tabular\ndata (i.e., Table-to-text) is a process with one input and a variety of valid\noutputs. This characteristic underscores the abilities to control the\ngeneration and produce a diverse set of outputs as two key assets. Thus, we\npropose a diversity enhancing scheme that builds upon an inherent property of\nthe statements, namely, their logic-types, by using a type-controlled\nTable-to-text generation model. Employing automatic and manual tests, we prove\nits twofold advantage: users can effectively tune the generated statement type,\nand, by sampling different types, can obtain a diverse set of statements for a\ngiven table.",
    "descriptor": "\nComments: 4 pages, 4 figures\n",
    "authors": [
      "Yotam Perlitz",
      "Liat Ein-Dot",
      "Dafna Sheinwald",
      "Noam Slonim",
      "Michal Shmueli-Scheuer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10938"
  },
  {
    "id": "arXiv:2205.10940",
    "title": "Toward smart composites: small-scale, untethered prediction and control  for soft sensor/actuator systems",
    "abstract": "We present a suite of algorithms and tools for model-predictive control of\nsensor/actuator systems with embedded microcontroller units (MCU). These MCUs\ncan be colocated with sensors and actuators, thereby enabling a new class of\nsmart composites capable of autonomous behavior that does not require an\nexternal computer. In this approach, kinematics are learned using a neural\nnetwork model from offline data and compiled into MCU code using nn4mc, an\nopen-source tool. Online Newton-Raphson optimization solves for the control\ninput. Shallow neural network models applied to 1D sensor signals allow for\nreduced model sizes and increased control loop frequencies. We validate this\napproach on a simulated mass-spring-damper system and two experimental setups\nwith different sensing, actuation, and computational hardware: a tendon-based\nplatform with embedded optical lace sensors and a HASEL-based platform with\nmagnetic sensors. Experimental results indicate effective high-bandwidth\ntracking of reference paths (120 Hz and higher) with a small memory footprint\n(less than or equal to 6.4% of available flash). The measured path following\nerror does not exceed 2 mm in the tendon-based platform, and the predicted path\nfollowing error does not exceed 1 mm in the HASEL-based platform. This\ncontroller code's mean power consumption in an ARM Cortex-M4 computer is 45.4\nmW. This control approach is also compatible with Tensorflow Lite models and\nequivalent compilers. Embedded intelligence in composite materials enables a\nnew class of composites that infuse intelligence into structures and systems,\nmaking them capable of responding to environmental stimuli using their\nproprioception.",
    "descriptor": "\nComments: Under Review at the Journal of Composite Materials. Special Issue: Multifunctional Composites for Autonomic, Adaptive and Self-Sustaining Systems\n",
    "authors": [
      "Sarah Aguasvivas Manzano",
      "Vani Sundaram",
      "Artemis Xu",
      "Khoi Ly",
      "Mark Rentschler",
      "Robert Shepherd",
      "Nikolaus Correll"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10940"
  },
  {
    "id": "arXiv:2205.10942",
    "title": "Lotteries for Shared Experiences",
    "abstract": "We study a setting where tickets for an experience are allocated by lottery.\nEach agent belongs to a group, and a group is successful if and only if its\nmembers receive enough tickets for everyone. A lottery is efficient if it\nmaximizes the number of agents in successful groups, and fair if it gives every\ngroup the same chance of success. We study the efficiency and fairness of\nexisting approaches, and propose practical alternatives.\nIf agents must identify the members of their group, a natural solution is the\nGroup Lottery, which orders groups uniformly at random and processes them\nsequentially. We provide tight bounds on the inefficiency and unfairness of\nthis mechanism, and describe modifications that obtain a fairer allocation.\nIf agents may request multiple tickets without identifying members of their\ngroup, the most common mechanism is the Individual Lottery, which orders agents\nuniformly at random and awards each their request until no tickets remain.\nBecause each member of a group may apply for (and win) tickets, this approach\ncan yield arbitrarily unfair and inefficient outcomes. As an alternative, we\npropose the Weighted Individual Lottery, in which the processing order is\nbiased against agents with large requests. Although it is still possible to\nhave multiple winners in a group, this simple modification makes this event\nmuch less likely. As a result, the Weighted Individual Lottery is approximately\nfair and approximately efficient, and similar to the Group Lottery when there\nare many more agents than tickets.",
    "descriptor": "",
    "authors": [
      "Nick Arnosti",
      "Carlos Bonet"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.10942"
  },
  {
    "id": "arXiv:2205.10945",
    "title": "A preconditioned deepest descent algorithm for a class of optimization  problems involving the $p(x)$-Laplacian operator",
    "abstract": "In this paper we are concerned with a class of optimization problems\ninvolving the $p(x)$-Laplacian operator, which arise in imaging and signal\nanalysis. We study the well-posedness of this kind of problems in an amalgam\nspace considering that the variable exponent $p(x)$ is a log-H\\\"older\ncontinuous function. Further, we propose a preconditioned descent algorithm for\nthe numerical solution of the problem, considering a \"frozen exponent\" approach\nin a finite dimension space. Finally, we carry on several numerical experiments\nto show the advantages of our method. Specifically, we study two detailed\nexample whose motivation lies in a possible extension of the proposed technique\nto image processing.",
    "descriptor": "",
    "authors": [
      "Sergio Gonz\u00e1lez-Andrade",
      "Mar\u00eda de los \u00c1ngeles Silva"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Functional Analysis (math.FA)"
    ],
    "url": "https://arxiv.org/abs/2205.10945"
  },
  {
    "id": "arXiv:2205.10947",
    "title": "Deep Discriminative Direct Decoders for High-dimensional Time-series  Analysis",
    "abstract": "Dynamical latent variable modeling has been significantly invested over the\nlast couple of decades with established solutions encompassing generative\nprocesses like the state-space model (SSM) and discriminative processes like a\nrecurrent or a deep neural network (DNN). These solutions are powerful tools\nwith promising results; however, surprisingly they were never put together in a\nunified model to analyze complex multivariate time-series data. A very recent\nmodeling approach, called the direct discriminative decoder (DDD) model,\nproposes a principal solution to combine SMM and DNN models, with promising\nresults in decoding underlying latent processes, e.g. rat movement trajectory,\nthrough high-dimensional neural recordings. The DDD consists of a) a state\ntransition process, as per the classical dynamical models, and b) a\ndiscriminative process, like DNN, in which the conditional distribution of\nstates is defined as a function of the current observations and their recent\nhistory. Despite promising results of the DDD model, no training solutions, in\nthe context of DNN, have been utilized for this model. Here, we propose how DNN\nparameters along with an optimal history term can be simultaneously estimated\nas a part of the DDD model. We use the D4 abbreviation for a DDD with a DNN as\nits discriminative process. We showed the D4 decoding performance in both\nsimulation and (relatively) high-dimensional neural data. In both datasets, D4\nperformance surpasses the state-of-art decoding solutions, including those of\nSSM and DNNs. The key success of DDD and potentially D4 is efficient\nutilization of the recent history of observation along with the state-process\nthat carries long-term information, which is not addressed in either SSM or DNN\nsolutions. We argue that D4 can be a powerful tool for the analysis of\nhigh-dimensional time-series data.",
    "descriptor": "",
    "authors": [
      "Mohammad R. Rezaei",
      "Milos R. Popovic",
      "Milad Lankarany",
      "Ali Yousefi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10947"
  },
  {
    "id": "arXiv:2205.10949",
    "title": "Evaluating deep tracking models for player tracking in broadcast ice  hockey video",
    "abstract": "Tracking and identifying players is an important problem in computer vision\nbased ice hockey analytics. Player tracking is a challenging problem since the\nmotion of players in hockey is fast-paced and non-linear. There is also\nsignificant player-player and player-board occlusion, camera panning and\nzooming in hockey broadcast video. Prior published research perform player\ntracking with the help of handcrafted features for player detection and\nre-identification. Although commercial solutions for hockey player tracking\nexist, to the best of our knowledge, no network architectures used, training\ndata or performance metrics are publicly reported. There is currently no\npublished work for hockey player tracking making use of the recent advancements\nin deep learning while also reporting the current accuracy metrics used in\nliterature. Therefore, in this paper, we compare and contrast several\nstate-of-the-art tracking algorithms and analyze their performance and failure\nmodes in ice hockey.",
    "descriptor": "\nComments: Accepted to Link\\\"oping Hockey Analytics Conference (LINHAC). arXiv admin note: substantial text overlap with arXiv:2110.03090\n",
    "authors": [
      "Kanav Vats",
      "Mehrnaz Fani",
      "David A. Clausi",
      "John S. Zelek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10949"
  },
  {
    "id": "arXiv:2205.10951",
    "title": "Incentivizing Federated Learning",
    "abstract": "Federated Learning is an emerging distributed collaborative learning paradigm\nused by many of applications nowadays. The effectiveness of federated learning\nrelies on clients' collective efforts and their willingness to contribute local\ndata. However, due to privacy concerns and the costs of data collection and\nmodel training, clients may not always contribute all the data they possess,\nwhich would negatively affect the performance of the global model. This paper\npresents an incentive mechanism that encourages clients to contribute as much\ndata as they can obtain. Unlike previous incentive mechanisms, our approach\ndoes not monetize data. Instead, we implicitly use model performance as a\nreward, i.e., significant contributors are paid off with better models. We\ntheoretically prove that clients will use as much data as they can possibly\npossess to participate in federated learning under certain conditions with our\nincentive mechanism",
    "descriptor": "",
    "authors": [
      "Shuyu Kong",
      "You Li",
      "Hai Zhou"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10951"
  },
  {
    "id": "arXiv:2205.10952",
    "title": "Analysis of functional neural codes of deep learning models",
    "abstract": "Deep neural networks (DNNs), the agents of deep learning (DL), require a\nmassive number of parallel/sequential operations. This makes it extremely\nchallenging to comprehend DNNs' operations and hinders proper diagnosis.\nConsequently, DNNs cannot be readily used in high-stakes domains, in which\nincorrect decisions can lead to catastrophic failures. Therefore, to build more\nreliable DNNs/DL to be deployed in high-stakes real-world problems, it is\nimperative that we develop proper analysis tools that will allow us to better\nunderstand DNNs' internal operations underlying their decision-making. Here, we\nused the self-organizing map (SOM) to analyze internal codes of DL models\nassociated with their decision-making. Our analyses suggest that hidden layer\nactivation patterns can be mapped onto a finite number of patterns and are\ncorrelated with DL predictions, raising the possibility that they could serve\nas functional codes of DL models. Encouraged by this observation, we further\nused SOM to estimate input features coded in hidden layers, analyzed the\neffects of adversarial inputs to better understand characterized internal\nrepresentations' evolution and adversarial perturbations' propagation in DL\nmodels.",
    "descriptor": "\nComments: 13 pages, 7 main figures, 5 supplemental figures, 1 main table, 2 supplemental tables\n",
    "authors": [
      "Jung Hoon Lee",
      "Sujith Vijayan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10952"
  },
  {
    "id": "arXiv:2205.10953",
    "title": "CYRUS Soccer Simulation 2D Team Description Paper 2022",
    "abstract": "Soccer Simulation 2D League is one of the major leagues of RoboCup\ncompetitions. In a Soccer Simulation 2D (SS2D) game, two teams of 11 players\nand one coach compete against each other. The players are only allowed to\ncommunicate with the server that is called Soccer Simulation Server. This paper\nintroduces the previous and current research of the CYRUS soccer simulation\nteam, the champion of RoboCup 2021. We will present our idea about improving\nUnmarking Decisioning and Positioning by using Pass Prediction Deep Neural\nNetwork. Based on our experimental results, this idea proven to be effective on\nincreasing the winning rate of Cyrus against opponents.",
    "descriptor": "",
    "authors": [
      "Nader Zare",
      "Arad Firouzkouhi",
      "Omid Amini",
      "Mahtab Sarvmaili",
      "Aref Sayareh",
      "Saba Ramezani Rad",
      "Stan Matwin",
      "Amilcar Soares"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.10953"
  },
  {
    "id": "arXiv:2205.10954",
    "title": "An Automated System for Detecting Visual Damages of Wind Turbine Blades",
    "abstract": "Wind energy's ability to compete with fossil fuels on a market level depends\non lowering wind's high operational costs. Since damages on wind turbine blades\nare the leading cause for these operational problems, identifying blade damages\nis critical. However, recent works in visual identification of blade damages\nare still experimental and focus on optimizing the traditional machine learning\nmetrics such as IoU. In this paper, we argue that pushing models to production\nlong before achieving the \"optimal\" model performance can still generate real\nvalue for this use case. We discuss the performance of our damage's suggestion\nmodel in production and how this system works in coordination with humans as\npart of a commercialized product and how it can contribute towards lowering\nwind energy's operational costs.",
    "descriptor": "",
    "authors": [
      "Linh Nguyen",
      "Akshay Iyer",
      "Shweta Khushu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10954"
  },
  {
    "id": "arXiv:2205.10955",
    "title": "Investigating classification learning curves for automatically generated  and labelled plant images",
    "abstract": "In the context of supervised machine learning a learning curve describes how\na model's performance on unseen data relates to the amount of samples used to\ntrain the model. In this paper we present a dataset of plant images with\nrepresentatives of crops and weeds common to the Manitoba prairies at different\ngrowth stages. We determine the learning curve for a classification task on\nthis data with the ResNet architecture. Our results are in accordance with\nprevious studies and add to the evidence that learning curves are governed by\npower-law relationships over large scales, applications, and models. We further\ninvestigate how label noise and the reduction of trainable parameters impacts\nthe learning curve on this dataset. Both effects lead to the model requiring\ndisproportionally larger training sets to achieve the same classification\nperformance as observed without these effects.",
    "descriptor": "",
    "authors": [
      "Michael A. Beck",
      "Christopher P. Bidinosti",
      "Christopher J. Henry",
      "Manisha Ajmani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10955"
  },
  {
    "id": "arXiv:2205.10956",
    "title": "CIRCLE: Continual Repair across Programming Languages",
    "abstract": "Automatic Program Repair (APR) aims at fixing buggy source code with less\nmanual debugging efforts, which plays a vital role in improving software\nreliability and development productivity. Recent APR works have achieved\nremarkable progress via applying deep learning (DL), particularly neural\nmachine translation (NMT) techniques. However, we observe that existing\nDL-based APR models suffer from at least two severe drawbacks: (1) Most of them\ncan only generate patches for a single programming language, as a result, to\nrepair multiple languages, we have to build and train many repairing models.\n(2) Most of them are developed in an offline manner. Therefore, they won't\nfunction when there are new-coming requirements. To address the above problems,\na T5-based APR framework equipped with continual learning ability across\nmultiple programming languages is proposed, namely \\emph{C}ont\\emph{I}nual\n\\emph{R}epair a\\emph{C}ross Programming \\emph{L}anguag\\emph{E}s\n(\\emph{CIRCLE}). Specifically, (1) CIRCLE utilizes a prompting function to\nnarrow the gap between natural language processing (NLP) pre-trained tasks and\nAPR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achieve\nlifelong learning for APR without access to the full historical data. (3) An\nelastic regularization method is employed to strengthen CIRCLE's continual\nlearning ability further, preventing it from catastrophic forgetting. (4)\nCIRCLE applies a simple but effective re-repairing method to revise generated\nerrors caused by crossing multiple programming languages. We train CIRCLE for\nfour languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on five\ncommonly used benchmarks. The experimental results demonstrate that CIRCLE not\nonly effectively and efficiently repairs multiple programming languages in\ncontinual learning settings, but also achieves state-of-the-art performance\nwith a single repair model.",
    "descriptor": "\nComments: This paper was accepted by ISSTA2022\n",
    "authors": [
      "Wei Yuan",
      "Quanjun Zhang",
      "Tieke He",
      "Chunrong Fang",
      "Nguyen Quoc Viet Hung",
      "Xiaodong Hao",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10956"
  },
  {
    "id": "arXiv:2205.10957",
    "title": "Fundamentals of Wobbling and Hardware Impairments-Aware Air-to-Ground  Channel Model",
    "abstract": "In this paper, we develop an impairments-aware air-to-ground unified channel\nmodel that incorporates the effect of both wobbling and hardware impairments,\nwhere the former is caused by random physical fluctuations of unmanned aerial\nvehicles (UAVs), and the latter by intrinsic radio frequency (RF) nonidealities\nat both the transmitter and receiver, such as phase noise, in-phase/quadrature\n(I/Q) imbalance, and power amplifier (PA) nonlinearity. The impact of UAV\nwobbling is modeled by two stochastic processes, i.e., the canonical Wiener\nprocess and the more realistic sinusoidal process. On the other hand, the\naggregate impact of all hardware impairments is modeled as two multiplicative\nand additive distortion noise processes, which is a well-accepted model. For\nthe sake of generality, we consider both wide-sense stationary (WSS) and\nnonstationary processes for the distortion noises. We then rigorously\ncharacterize the autocorrelation function (ACF) of the wireless channel, using\nwhich we provide a comprehensive analysis of four key channel-related metrics:\n(i) power delay profile (PDP), (ii) coherence time, (iii) coherence bandwidth,\nand (iv) power spectral density (PSD) of the distortion-plus-noise process.\nFurthermore, we evaluate these metrics with reasonable UAV wobbling and\nhardware impairment models to obtain useful insights. Quite noticeably, we\ndemonstrate that even for small UAV wobbling, the coherence time severely\ndegrades at high frequencies, which renders air-to-ground channel estimation\nvery difficult at these frequencies. To the best of our understanding, this is\nthe first work that characterizes the joint impact of UAV wobbling and hardware\nimpairments on the air-to-ground wireless channel.",
    "descriptor": "",
    "authors": [
      "Morteza Banagar",
      "Harpreet S. Dhillon"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.10957"
  },
  {
    "id": "arXiv:2205.10961",
    "title": "Scalable and Privacy-Focused Company-Centric Supply Chain Management",
    "abstract": "Blockchain technology promises to overcome trust and privacy concerns\ninherent to centralized information sharing. However, current decentralized\nsupply chain management systems do either not meet privacy and scalability\nrequirements or require a trustworthy consortium, which is challenging for\nincreasingly dynamic supply chains with constantly changing participants. In\nthis paper, we propose CCChain, a scalable and privacy-aware supply chain\nmanagement system that stores all information locally to give companies\ncomplete sovereignty over who accesses their data. Still, tamper protection of\nall data through a permissionless blockchain enables on-demand tracking and\ntracing of products as well as reliable information sharing while affording the\ndetection of data inconsistencies. Our evaluation confirms that CCChain offers\nsuperior scalability in comparison to alternatives while also enabling near\nreal-time tracking and tracing for many, less complex products.",
    "descriptor": "\nComments: IEEE ICBC'22\n",
    "authors": [
      "Eric Wagner",
      "Roman Matzutt",
      "Jan Pennekamp",
      "Lennart Bader",
      "Irakli Bajelidze",
      "Klaus Wehrle",
      "Martin Henze"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10961"
  },
  {
    "id": "arXiv:2205.10962",
    "title": "Digital Twin for Secure Semiconductor Lifecycle Management: Prospects  and Applications",
    "abstract": "The expansive globalization of the semiconductor supply chain has introduced\nnumerous untrusted entities into different stages of a device's lifecycle. To\nmake matters worse, the increase complexity in the design as well as aggressive\ntime to market requirements of the newer generation of integrated circuits can\nlead either designers to unintentionally introduce security vulnerabilities or\nverification engineers to fail in detecting them earlier in the design\nlifecycle. These overlooked or undetected vulnerabilities can be exploited by\nmalicious entities in subsequent stages of the lifecycle through an ever\nwidening variety of hardware attacks. The ability to ascertain the provenance\nof these vulnerabilities, therefore, becomes a pressing issue when the security\nassurance across the whole lifecycle is required to be ensured. We posit that\nif there is a malicious or unintentional breach of security policies of a\ndevice, it will be reflected in the form of anomalies in the traditional\ndesign, verification and testing activities throughout the lifecycle. With\nthat, a digital simulacrum of a device's lifecycle, called a digital twin (DT),\ncan be formed by the data gathered from different stages to secure the\nlifecycle of the device. In this paper, we put forward a realization of\nintertwined relationships of security vulnerabilities with data available from\nthe silicon lifecycle and formulate different components of an AI driven DT\nframework. The proposed DT framework leverages these relationships and\nrelational learning to achieve Forward and Backward Trust Analysis\nfunctionalities enabling security aware management of the entire lifecycle.\nFinally, we provide potential future research avenues and challenges for\nrealization of the digital twin framework to enable secure semiconductor\nlifecycle management.",
    "descriptor": "\nComments: 37 pages including citations, 14 figures\n",
    "authors": [
      "Hasan Al Shaikh",
      "Mohammad Bin Monjil",
      "Shigang Chen",
      "Navid Asadizanjani",
      "Farimah Farahmandi",
      "Mark Tehranipoor",
      "Fahim Rahman"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10962"
  },
  {
    "id": "arXiv:2205.10963",
    "title": "Protecting File Activities via Deception for ARM TrustZone",
    "abstract": "A TrustZone TEE often invokes an external filesystem. While filedata can be\nencrypted, the revealed file activities can leak secrets. To hide the file\nactivities from the filesystem and its OS, we propose Enigma, a deception-based\ndefense injecting sybil file activities as the cover of the actual file\nactivities.\nEnigma contributes three new designs. (1) To make the deception credible, the\nTEE generates sybil calls by replaying file calls from the TEE code under\nprotection. (2) To make sybil activities cheap, the TEE requests the OS to run\nK filesystem images simultaneously. Concealing the disk, the TEE backs only one\nimage with the actual disk while backing other images by only storing their\nmetadata. (3) To protect filesystem image identities, the TEE shuffles the\nimages frequently, preventing the OS from observing any image for long.\nEnigma works with unmodified filesystems shipped withLinux. On a low-cost Arm\nSoC with EXT4 and F2FS, our system can concurrently run as many as 50\nfilesystem images with 1% of disk overhead per additional image. Compared to\ncommon obfuscation for hiding addresses in a flat space, Enigma hides file\nactivities with richer semantics. Its cost is lower by one order of magnitude\nwhile achieving the same level of probabilistic security guarantees.",
    "descriptor": "\nComments: Under submission\n",
    "authors": [
      "Liwei Guo",
      "Felix Xiaozhu Lin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2205.10963"
  },
  {
    "id": "arXiv:2205.10964",
    "title": "The Geometry of Multilingual Language Model Representations",
    "abstract": "We assess how multilingual language models maintain a shared multilingual\nrepresentation space while still encoding language-sensitive information in\neach language. Using XLM-R as a case study, we show that languages occupy\nsimilar linear subspaces after mean-centering, evaluated based on causal\neffects on language modeling performance and direct comparisons between\nsubspaces for 88 languages. The subspace means differ along language-sensitive\naxes that are relatively stable throughout middle layers, and these axes encode\ninformation such as token vocabularies. Shifting representations by language\nmeans is sufficient to induce token predictions in different languages.\nHowever, we also identify stable language-neutral axes that encode information\nsuch as token positions and part-of-speech. We visualize representations\nprojected onto language-sensitive and language-neutral axes, identifying\nlanguage family and part-of-speech clusters, along with spirals, toruses, and\ncurves representing token position information. These results demonstrate that\nmultilingual language models encode information along orthogonal\nlanguage-sensitive and language-neutral axes, allowing the models to extract a\nvariety of features for downstream tasks and cross-lingual transfer learning.",
    "descriptor": "",
    "authors": [
      "Tyler A. Chang",
      "Zhuowen Tu",
      "Benjamin K. Bergen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10964"
  },
  {
    "id": "arXiv:2205.10967",
    "title": "Computational Storytelling and Emotions: A Survey",
    "abstract": "Storytelling has always been vital for human nature. From ancient times,\nhumans have used stories for several objectives including entertainment,\nadvertisement, and education. Various analyses have been conducted by\nresearchers and creators to determine the way of producing good stories. The\ndeep relationship between stories and emotions is a prime example. With the\nadvancement in deep learning technology, computers are expected to understand\nand generate stories. This survey paper is intended to summarize and further\ncontribute to the development of research being conducted on the relationship\nbetween stories and emotions. We believe creativity research is not to replace\nhumans with computers, but to find a way of collaboration between humans and\ncomputers to enhance the creativity. With the intention of creating a new\nintersection between computational storytelling research and human creative\nwriting, we introduced creative techniques used by professional storytellers.",
    "descriptor": "\nComments: 25 pages, 3 figures\n",
    "authors": [
      "Yusuke Mori",
      "Hiroaki Yamane",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10967"
  },
  {
    "id": "arXiv:2205.10970",
    "title": "Neural Subgraph Explorer: Reducing Noisy Information via Target-Oriented  Syntax Graph Pruning",
    "abstract": "Recent years have witnessed the emerging success of leveraging syntax graphs\nfor the target sentiment classification task. However, we discover that\nexisting syntax-based models suffer from two issues: noisy information\naggregation and loss of distant correlations. In this paper, we propose a novel\nmodel termed Neural Subgraph Explorer, which (1) reduces the noisy information\nvia pruning target-irrelevant nodes on the syntax graph; (2) introduces\nbeneficial first-order connections between the target and its related words\ninto the obtained graph. Specifically, we design a multi-hop actions score\nestimator to evaluate the value of each word regarding the specific target. The\ndiscrete action sequence is sampled through Gumble-Softmax and then used for\nboth of the syntax graph and the self-attention graph. To introduce the\nfirst-order connections between the target and its relevant words, the two\npruned graphs are merged. Finally, graph convolution is conducted on the\nobtained unified graph to update the hidden states. And this process is stacked\nwith multiple layers. To our knowledge, this is the first attempt of\ntarget-oriented syntax graph pruning in this task. Experimental results\ndemonstrate the superiority of our model, which achieves new state-of-the-art\nperformance.",
    "descriptor": "\nComments: To appear in IJCAI 2022\n",
    "authors": [
      "Bowen Xing",
      "Ivor W. Tsang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10970"
  },
  {
    "id": "arXiv:2205.10971",
    "title": "Numerical method for the Fokker-Planck equation of Brownian motion  subordinated by inverse tempered stable subordinator with drift",
    "abstract": "In this work, based on the complete Bernstein function, we propose a\ngeneralized regularity analysis including maximal $\\xLn{p}$ regularity for the\nFokker--Planck equation, which governs the subordinated Brownian motion with\nthe inverse tempered stable subordinator that has a drift. We derive a\ngeneralized time--stepping finite element scheme based on the backward Euler\nconvolution quadrature, and the optimal-order convergence of the numerical\nsolutions is established using the proven solution regularity. Further, the\nanalysis is generalized to more general diffusion equations. Numerical\nexperiments are provided to support the theoretical results.",
    "descriptor": "",
    "authors": [
      "Xiangong Tang",
      "Can Wang",
      "Weihua Deng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10971"
  },
  {
    "id": "arXiv:2205.10977",
    "title": "What should I Ask: A Knowledge-driven Approach for Follow-up Questions  Generation in Conversational Surveys",
    "abstract": "Conversational surveys, where an agent asks open-ended questions through\nnatural language interfaces, offer a new way to collect information from\npeople. A good follow-up question in a conversational survey prompts\nhigh-quality information and delivers engaging experiences. However, generating\nhigh-quality follow-up questions on the fly is a non-trivial task. The agent\nneeds to understand the diverse and complex participant responses, adhere to\nthe survey goal, and generate clear and coherent questions. In this study, we\npropose a knowledge-driven follow-up question generation framework. The\nframework combines a knowledge selection module to identify salient topics in\nparticipants' responses and a generative model guided by selected knowledge\nentity-relation pairs. To investigate the effectiveness of the proposed\nframework, we build a new dataset for open-domain follow-up question generation\nand present a new set of reference-free evaluation metrics based on Gricean\nMaxim. Our experiments demonstrate that our framework outperforms a GPT-based\nbaseline in both objective evaluation and human-expert evaluation.",
    "descriptor": "",
    "authors": [
      "Yubin Ge",
      "Ziang Xiao",
      "Jana Diesner",
      "Heng Ji",
      "Karrie Karahalios",
      "Hari Sundaram"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.10977"
  },
  {
    "id": "arXiv:2205.10981",
    "title": "Improving Short Text Classification With Augmented Data Using GPT-3",
    "abstract": "GPT-3 is a large-scale natural language model developed by OpenAI that can\nperform many different tasks, including topic classification. Although\nresearchers claim that it requires only a small number of in-context examples\nto learn a task, in practice GPT-3 requires these training examples to be\neither of exceptional quality or a higher quantity than easily created by hand.\nTo address this issue, this study teaches GPT-3 to classify whether a question\nis related to data science by augmenting a small training set with additional\nexamples generated by GPT-3 itself. This study compares two classifiers: the\nGPT-3 Classification Endpoint with augmented examples, and the GPT-3 Completion\nEndpoint with an optimal training set chosen using a genetic algorithm. We find\nthat while the augmented Completion Endpoint achieves upwards of 80 percent\nvalidation accuracy, using the augmented Classification Endpoint yields more\nconsistent accuracy on unseen examples. In this way, giving large-scale machine\nlearning models like GPT-3 the ability to propose their own additional training\nexamples can result in improved classification performance.",
    "descriptor": "\nComments: 27 pages, 7 figures, submitted to Natural Language Engineering\n",
    "authors": [
      "Salvador Balkus",
      "Donghui Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10981"
  },
  {
    "id": "arXiv:2205.10984",
    "title": "A mode-in-state contribution factor based on Koopman operator and its  application to power system analysis",
    "abstract": "This paper proposes a mode-in-state contribution factor for a class of\nnonlinear dynamical systems by utilizing spectral properties of the Koopman\noperator and sensitivity analysis. Using eigenfunctions of the Koopman operator\nfor a target nonlinear system, we show that the relative contribution between\nmodes and state variables can be quantified beyond a linear regime, where the\nnonlinearity of the system is taken into consideration. The proposed\ncontribution factor is applied to the numerical analysis of large-signal\nsimulations for an interconnected AC/multi-terminal DC power system.",
    "descriptor": "\nComments: 11 pages, 2 figures\n",
    "authors": [
      "Kenji Takamichi",
      "Yoshihiko Susuki",
      "Marcos Netto",
      "Atsushi Ishigame"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.10984"
  },
  {
    "id": "arXiv:2205.10986",
    "title": "Boosting Multi-Label Image Classification with Complementary Parallel  Self-Distillation",
    "abstract": "Multi-Label Image Classification (MLIC) approaches usually exploit label\ncorrelations to achieve good performance. However, emphasizing correlation like\nco-occurrence may overlook discriminative features of the target itself and\nlead to model overfitting, thus undermining the performance. In this study, we\npropose a generic framework named Parallel Self-Distillation (PSD) for boosting\nMLIC models. PSD decomposes the original MLIC task into several simpler MLIC\nsub-tasks via two elaborated complementary task decomposition strategies named\nCo-occurrence Graph Partition (CGP) and Dis-occurrence Graph Partition (DGP).\nThen, the MLIC models of fewer categories are trained with these sub-tasks in\nparallel for respectively learning the joint patterns and the category-specific\npatterns of labels. Finally, knowledge distillation is leveraged to learn a\ncompact global ensemble of full categories with these learned patterns for\nreconciling the label correlation exploitation and model overfitting. Extensive\nresults on MS-COCO and NUS-WIDE datasets demonstrate that our framework can be\neasily plugged into many MLIC approaches and improve performances of recent\nstate-of-the-art approaches. The explainable visual study also further\nvalidates that our method is able to learn both the category-specific and\nco-occurring features. The source code is released at\nhttps://github.com/Robbie-Xu/CPSD.",
    "descriptor": "\nComments: accepted by IJCAI2022\n",
    "authors": [
      "Jiazhi Xu",
      "Sheng Huang",
      "Fengtao Zhou",
      "Luwen Huangfu",
      "Daniel Zeng",
      "Bo Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10986"
  },
  {
    "id": "arXiv:2205.10990",
    "title": "Multiple Domain Cyberspace Attack and Defense Game Based on Reward  Randomization Reinforcement Learning",
    "abstract": "The existing network attack and defense method can be regarded as game, but\nmost of the game only involves network domain, not multiple domain cyberspace.\nTo address this challenge, this paper proposed a multiple domain cyberspace\nattack and defense game model based on reinforcement learning. We define the\nmultiple domain cyberspace include physical domain, network domain and digital\ndomain. By establishing two agents, representing the attacker and the defender\nrespectively, defender will select the multiple domain actions in the multiple\ndomain cyberspace to obtain defender's optimal reward by reinforcement\nlearning. In order to improve the defense ability of defender, a game model\nbased on reward randomization reinforcement learning is proposed. When the\ndefender takes the multiple domain defense action, the reward is randomly given\nand subject to linear distribution, so as to find the better defense policy and\nimprove defense success rate. The experimental results show that the game model\ncan effectively simulate the attack and defense state of multiple domain\ncyberspace, and the proposed method has a higher defense success rate than DDPG\nand DQN.",
    "descriptor": "\nComments: 10 pages,4 figures\n",
    "authors": [
      "Lei Zhang",
      "Yu Pan",
      "Yi Liu",
      "Qibin Zheng",
      "Zhisong Pan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10990"
  },
  {
    "id": "arXiv:2205.10992",
    "title": "Exploring Apache Incubator Project Trajectories with APEX",
    "abstract": "Open Source Software (OSS) is a major component of our digital\ninfrastructure, yet more than 80% of such projects fail. Seeking less\nuncertainty, many OSS projects join established software communities, e.g., the\nApache Software Foundation (ASF), with established rules and community support\nto guide projects toward sustainability. In their nascent stage, ASF projects\nare incubated in the ASF incubator (ASFI), which provides systematic mentorship\ntoward long-term sustainability. Projects in ASFI eventually conclude their\nincubation by either graduating, if successful, or retiring, if not.\nTime-stamped traces of developer activities are publicly available from ASF,\nand can be used for monitoring project trajectories toward sustainability. Here\nwe present a web app dashboard tool, APEX, that allows internal and external\nstakeholders to monitor and explore ASFI project sustainability trajectories,\nincluding social and technical networks.",
    "descriptor": "\nComments: MSR 2022\n",
    "authors": [
      "Anirudh Ramchandran",
      "Likang Yin",
      "Vladimir Filkov"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10992"
  },
  {
    "id": "arXiv:2205.10995",
    "title": "From Width-Based Model Checking to Width-Based Automated Theorem Proving",
    "abstract": "In the field of parameterized complexity theory, the study of graph width\nmeasures has been intimately connected with the development of width-based\nmodel checking algorithms for combinatorial properties on graphs. In this work,\nwe introduce a general framework to convert a large class of width-based\nmodel-checking algorithms into algorithms that can be used to test the validity\nof graph-theoretic conjectures on classes of graphs of bounded width. Our\nframework is modular and can be applied with respect to several well-studied\nwidth measures for graphs, including treewidth and cliquewidth.\nAs a quantitative application of our framework, we show that for several\nlong-standing graph-theoretic conjectures, there exists an algorithm that takes\na number $k$ as input and correctly determines in time double-exponential in\n$k^{O(1)}$ whether the conjecture is valid on all graphs of treewidth at most\n$k$. This improves significantly on upper bounds obtained using previously\navailable techniques.",
    "descriptor": "",
    "authors": [
      "Mateus de Oliveira Oliveira",
      "Farhad Vadiee"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.10995"
  },
  {
    "id": "arXiv:2205.10996",
    "title": "Generalized Weak Galerkin Methods For Stokes Equations",
    "abstract": "A new weak Galerkin finite element method, called generalized weak Galerkin\nmethod ({g}WG), is introduced for Stokes equations in this paper by using a new\ndefinition of the weak gradient. Error estimates in energy norm and $L^2$ norm\nfor the velocity and $L^2$ norm for the pressure are derived for elements with\narbitrary combination of polynomials. Some numerical examples are presented to\nverify the effectiveness, theoretical convergence orders, and robustness of the\nproposed scheme.",
    "descriptor": "",
    "authors": [
      "W. Qi",
      "P. Seshaiyer",
      "J. Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10996"
  },
  {
    "id": "arXiv:2205.10997",
    "title": "Data-Efficient Modeling for Precise Power Consumption Estimation of  Quadrotor Operations Using Ensemble Learning",
    "abstract": "Electric Take-Off and Landing (eVTOL) aircraft is considered as the major\naircraft type in the emerging urban air mobility. Accurate power consumption\nestimation is crucial to eVTOL, supporting advanced power management strategies\nand improving the efficiency and safety performance of flight operations. In\nthis study, a framework for power consumption modeling of eVTOL aircraft was\nestablished. We employed an ensemble learning method, namely stacking, to\ndevelop a data-driven model using flight records of three different types of\nquadrotors. Random forest and extreme gradient boosting, showing advantages in\nprediction, were chosen as base-models, and a linear regression model was used\nas the meta-model. The established stacking model can accurately estimate the\npower of a quadrotor. Error analysis shows that about 80% prediction errors\nfall within one standard deviation interval and less than 0.5% error in the\nprediction for an entire flight can be expected with a confidence of more than\n80%. Our model outperforms the existing models in two aspects: firstly, our\nmodel has a better prediction performance, and secondly, our model is more\ndata-efficient, requiring a much smaller dataset. Our model provides a powerful\ntool for operators of eVTOL aircraft in mission management and contributes to\npromoting safe and energy-efficient urban air traffic.",
    "descriptor": "",
    "authors": [
      "Wei Dai",
      "Mingcheng Zhang",
      "Kin Huat Low"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10997"
  },
  {
    "id": "arXiv:2205.10998",
    "title": "Semi-Decentralized Federated Learning with Collaborative Relaying",
    "abstract": "We present a semi-decentralized federated learning algorithm wherein clients\ncollaborate by relaying their neighbors' local updates to a central parameter\nserver (PS). At every communication round to the PS, each client computes a\nlocal consensus of the updates from its neighboring clients and eventually\ntransmits a weighted average of its own update and those of its neighbors to\nthe PS. We appropriately optimize these averaging weights to ensure that the\nglobal update at the PS is unbiased and to reduce the variance of the global\nupdate at the PS, consequently improving the rate of convergence. Numerical\nsimulations substantiate our theoretical claims and demonstrate settings with\nintermittent connectivity between the clients and the PS, where our proposed\nalgorithm shows an improved convergence rate and accuracy in comparison with\nthe federated averaging algorithm.",
    "descriptor": "\nComments: Accepted for presentation at the IEEE ISIT 2022. This is a conference version of arXiv:2202.11850\n",
    "authors": [
      "Michal Yemini",
      "Rajarshi Saha",
      "Emre Ozfatura",
      "Deniz G\u00fcnd\u00fcz",
      "Andrea J. Goldsmith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.10998"
  },
  {
    "id": "arXiv:2205.11004",
    "title": "PIXAL: Anomaly Reasoning with Visual Analytics",
    "abstract": "Anomaly detection remains an open challenge in many application areas. While\nthere are a number of available machine learning algorithms for detecting\nanomalies, analysts are frequently asked to take additional steps in reasoning\nabout the root cause of the anomalies and form actionable hypotheses that can\nbe communicated to business stakeholders. Without the appropriate tools, this\nreasoning process is time-consuming, tedious, and potentially error-prone. In\nthis paper we present PIXAL, a visual analytics system developed following an\niterative design process with professional analysts responsible for anomaly\ndetection. PIXAL is designed to fill gaps in existing tools commonly used by\nanalysts to reason with and make sense of anomalies. PIXAL consists of three\ncomponents: (1) an algorithm that finds patterns by aggregating multiple\nanomalous data points using first-order predicates, (2) a visualization tool\nthat allows the analyst to build trust in the algorithmically-generated\npredicates by performing comparative and counterfactual analyses, and (3) a\nvisualization tool that helps the analyst generate and validate hypotheses by\nexploring which features in the data most explain the anomalies. Finally, we\npresent the results of a qualitative observational study with professional\nanalysts. These results of the study indicate that PIXAL facilitates the\nanomaly reasoning process, allowing analysts to make sense of anomalies and\ngenerate hypotheses that are meaningful and actionable to business\nstakeholders.",
    "descriptor": "",
    "authors": [
      "Brian Montambault",
      "Camelia D. Brumar",
      "Michael Behrisch",
      "Remco Chang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.11004"
  },
  {
    "id": "arXiv:2205.11005",
    "title": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning",
    "abstract": "With the dramatically increased number of parameters in language models,\nsparsity methods have received ever-increasing research focus to compress and\naccelerate the models. While most research focuses on how to accurately retain\nappropriate weights while maintaining the performance of the compressed model,\nthere are challenges in the computational overhead and memory footprint of\nsparse training when compressing large-scale language models. To address this\nproblem, we propose a Parameter-efficient Sparse Training (PST) method to\nreduce the number of trainable parameters during sparse-aware training in\ndownstream tasks. Specifically, we first combine the data-free and data-driven\ncriteria to efficiently and accurately measure the importance of weights. Then\nwe investigate the intrinsic redundancy of data-driven weight importance and\nderive two obvious characteristics i.e., low-rankness and structuredness. Based\non that, two groups of small matrices are introduced to compute the data-driven\nimportance of weights, instead of using the original large importance score\nmatrix, which therefore makes the sparse training resource-efficient and\nparameter-efficient. Experiments with diverse networks (i.e., BERT, RoBERTa and\nGPT-2) on dozens of datasets demonstrate PST performs on par or better than\nprevious sparsity methods, despite only training a small number of parameters.\nFor instance, compared with previous sparsity methods, our PST only requires\n1.5% trainable parameters to achieve comparable performance on BERT.",
    "descriptor": "\nComments: This paper is published in IJCAI 2022\n",
    "authors": [
      "Yuchao Li",
      "Fuli Luo",
      "Chuanqi Tan",
      "Mengdi Wang",
      "Songfang Huang",
      "Shen Li",
      "Junjie Bai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11005"
  },
  {
    "id": "arXiv:2205.11008",
    "title": "Calibrate and Refine! A Novel and Agile Framework for ASR-error Robust  Intent Detection",
    "abstract": "The past ten years have witnessed the rapid development of text-based intent\ndetection, whose benchmark performances have already been taken to a remarkable\nlevel by deep learning techniques. However, automatic speech recognition (ASR)\nerrors are inevitable in real-world applications due to the environment noise,\nunique speech patterns and etc, leading to sharp performance drop in\nstate-of-the-art text-based intent detection models. Essentially, this\nphenomenon is caused by the semantic drift brought by ASR errors and most\nexisting works tend to focus on designing new model structures to reduce its\nimpact, which is at the expense of versatility and flexibility. Different from\nprevious one-piece model, in this paper, we propose a novel and agile framework\ncalled CR-ID for ASR error robust intent detection with two plug-and-play\nmodules, namely semantic drift calibration module (SDCM) and phonemic\nrefinement module (PRM), which are both model-agnostic and thus could be easily\nintegrated to any existing intent detection models without modifying their\nstructures. Experimental results on SNIPS dataset show that, our proposed CR-ID\nframework achieves competitive performance and outperform all the baseline\nmethods on ASR outputs, which verifies that CR-ID can effectively alleviate the\nsemantic drift caused by ASR errors.",
    "descriptor": "\nComments: Submit to INTERSPEECH 2022\n",
    "authors": [
      "Peilin Zhou",
      "Dading Chong",
      "Helin Wang",
      "Qingcheng Zeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.11008"
  },
  {
    "id": "arXiv:2205.11014",
    "title": "The Impact of Surrounding Road Objects and Conditions on Drivers Abrupt  Heart Rate Changes",
    "abstract": "Recent studies have pointed out the importance of mitigating drivers stress\nand negative emotions. These studies show that certain road objects such as big\nvehicles might be associated with higher stress levels based on drivers\nsubjective stress measures. Additionally, research shows strong correlations\nbetween drivers stress levels and increased heart rate (HR). In this paper,\nbased on a naturalistic multimodal driving dataset, we analyze the visual\nscenes of driving in the vicinity of abrupt increases in drivers HR for the\npresence of certain stress-inducing road objects. We show that the probability\nof the presence of such objects increases when becoming closer to the abrupt\nincrease in drivers HR. Additionally, we show that drivers facial engagement\nchanges significantly in the vicinity of abrupt increases in HR. Our results\nlay the ground for a human-centered driving experience by detecting and\nmitigating drivers stress levels in the wild.",
    "descriptor": "\nComments: Accepted to 66th Human Factors and Ergonomics Society International Annual Meeting 2022\n",
    "authors": [
      "Arash Tavakoli",
      "Arsalan Heydarian"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.11014"
  },
  {
    "id": "arXiv:2205.11015",
    "title": "Practical Considerations in Repairing Reed-Solomon Codes",
    "abstract": "The issue of repairing Reed-Solomon codes currently employed in industry has\nbeen sporadically discussed in the literature. In this work we carry out a\nsystematic study of these codes and investigate important aspects of repairing\nthem under the trace repair framework, including which evaluation points to\nselect and how to implement a trace repair scheme efficiently. In particular,\nwe employ different heuristic algorithms to search for low-bandwidth repair\nschemes for codes of short lengths with typical redundancies and establish\nthree tables of current best repair schemes for $[n, k]$ Reed-Solomon codes\nover GF(256) with $4 \\leq n \\leq 16$ and $r = n - k \\in \\{2,3,4\\}$. The tables\ncover most known codes currently used in the distributed storage industry.",
    "descriptor": "\nComments: 6 pages, accepted to the IEEE International Symposium on Information Theory\n",
    "authors": [
      "Thi Xinh Dinh",
      "Luu Y Nhi Nguyen",
      "Lakshmi J. Mohan",
      "Serdar Boztas",
      "Tran Thi Luong",
      "Son Hoang Dau"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.11015"
  },
  {
    "id": "arXiv:2205.11016",
    "title": "MolMiner: You only look once for chemical structure recognition",
    "abstract": "Molecular structures are always depicted as 2D printed form in scientific\ndocuments like journal papers and patents. However, these 2D depictions are not\nmachine-readable. Due to a backlog of decades and an increasing amount of these\nprinted literature, there is a high demand for the translation of printed\ndepictions into machine-readable formats, which is known as Optical Chemical\nStructure Recognition (OCSR). Most OCSR systems developed over the last three\ndecades follow a rule-based approach where the key step of vectorization of the\ndepiction is based on the interpretation of vectors and nodes as bonds and\natoms. Here, we present a practical software MolMiner, which is primarily built\nup using deep neural networks originally developed for semantic segmentation\nand object detection to recognize atom and bond elements from documents. These\nrecognized elements can be easily connected as a molecular graph with\ndistance-based construction algorithm. We carefully evaluate our software on\nfour benchmark datasets with the state-of-the-art performance. Various real\napplication scenarios are also tested, yielding satisfactory outcomes. The free\ndownload links of Mac and Windows versions are available: Mac:\nhttps://molminer-cdn.iipharma.cn/pharma-mind/artifact/latest/mac/PharmaMind-mac-latest-setup.dmg\nand Windows:\nhttps://molminer-cdn.iipharma.cn/pharma-mind/artifact/latest/win/PharmaMind-win-latest-setup.exe",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "Youjun Xu",
      "Jinchuan Xiao",
      "Chia-Han Chou",
      "Jianhang Zhang",
      "Jintao Zhu",
      "Qiwan Hu",
      "Hemin Li",
      "Ningsheng Han",
      "Bingyu Liu",
      "Shuaipeng Zhang",
      "Jinyu Han",
      "Zhen Zhang",
      "Shuhao Zhang",
      "Weilin Zhang",
      "Luhua Lai",
      "Jianfeng Pei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.11016"
  },
  {
    "id": "arXiv:2205.11018",
    "title": "LexiconNet: An End-to-End Handwritten Paragraph Text Recognition System",
    "abstract": "Historical documents present in the form of libraries needs to be digitised.\nThe recognition of these unconstrained cursive handwritten documents is a\nchallenging task. In the present work, neural network based classifier is used.\nThe recognition of scanned document images which are easy to train on neural\nnetwork based systems is usually done by a two step approach: segmentation\nfollowed by recognition. This approach has several shortcomings, which includes\nidentification of text regions, layout diversity analysis present within pages\nand ground truth segmentation. These processes are prone to errors that create\nbottleneck in the recognition accuracies. Thus in this study, an end-to-end\nparagraph recognition system is presented with internal line segmentation and\nlexicon decoder as post processing step, which is free from those errors. We\nnamed our model as LexiconNet. In LexiconNet, given a paragraph image a\ncombination of convolution and depth-wise separable convolutional modules\ngenerates the two dimension feature map of the image. The attention module is\nresponsible for internal line segmentation that consequently processing a page\nin a line by line manner. At decoding step, we have added connectionist\ntemporal classification based word beam search decoder as a post processing\nstep. Our approach reports state-of-the-art results on standard datasets. The\nreported character error rate is 3.24% on IAM dataset with 27.19% improvement,\n1.13% on RIMES with 40.83% improvement and 2.43% on READ-16 dataset with 32.31%\nimprovement from existing literature and the word error rate is 8.29% on IAM\ndataset with 43.02% improvement, 2.94% on RIMES dataset with 56.25% improvement\nand 7.35% on READ-2016 dataset with 47.27% improvement from the existing\nresults. The character error rate and word error rate reported in this work\nsurpasses the results reported in literature.",
    "descriptor": "",
    "authors": [
      "Lalita Kumari",
      "Sukhdeep Singh",
      "Vaibhav Varish Singh Rathore",
      "Anuj Sharma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11018"
  },
  {
    "id": "arXiv:2205.11019",
    "title": "Efficient Reinforcement Learning from Demonstration Using Local Ensemble  and Reparameterization with Split and Merge of Expert Policies",
    "abstract": "The current work on reinforcement learning (RL) from demonstrations often\nassumes the demonstrations are samples from an optimal policy, an unrealistic\nassumption in practice. When demonstrations are generated by sub-optimal\npolicies or have sparse state-action pairs, policy learned from sub-optimal\ndemonstrations may mislead an agent with incorrect or non-local action\ndecisions. We propose a new method called Local Ensemble and Reparameterization\nwith Split and Merge of expert policies (LEARN-SAM) to improve efficiency and\nmake better use of the sub-optimal demonstrations. First, LEARN-SAM employs a\nnew concept, the lambda-function, based on a discrepancy measure between the\ncurrent state to demonstrated states to \"localize\" the weights of the expert\npolicies during learning. Second, LEARN-SAM employs a split-and-merge (SAM)\nmechanism by separating the helpful parts in each expert demonstration and\nregrouping them into new expert policies to use the demonstrations selectively.\nBoth the lambda-function and SAM mechanism help boost the learning speed.\nTheoretically, we prove the invariant property of reparameterized policy before\nand after the SAM mechanism, providing theoretical guarantees for the\nconvergence of the employed policy gradient method. We demonstrate the\nsuperiority of the LEARN-SAM method and its robustness with varying\ndemonstration quality and sparsity in six experiments on complex continuous\ncontrol problems of low to high dimensions, compared to existing methods on RL\nfrom demonstration.",
    "descriptor": "\nComments: This paper was accepted by IEEE COMPSAC 2022\n",
    "authors": [
      "Yu Wang",
      "Fang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11019"
  },
  {
    "id": "arXiv:2205.11020",
    "title": "Artificial intelligence for topic modelling in Hindu philosophy: mapping  themes between the Upanishads and the Bhagavad Gita",
    "abstract": "A distinct feature of Hindu religious and philosophical text is that they\ncome from a library of texts rather than single source. The Upanishads is known\nas one of the oldest philosophical texts in the world that forms the foundation\nof Hindu philosophy. The Bhagavad Gita is core text of Hindu philosophy and is\nknown as a text that summarises the key philosophies of the Upanishads with\nmajor focus on the philosophy of karma. These texts have been translated into\nmany languages and there exists studies about themes and topics that are\nprominent; however, there is not much study of topic modelling using language\nmodels which are powered by deep learning. In this paper, we use advanced\nlanguage produces such as BERT to provide topic modelling of the key texts of\nthe Upanishads and the Bhagavad Gita. We analyse the distinct and overlapping\ntopics amongst the texts and visualise the link of selected texts of the\nUpanishads with Bhagavad Gita. Our results show a very high similarity between\nthe topics of these two texts with the mean cosine similarity of 73%. We find\nthat out of the fourteen topics extracted from the Bhagavad Gita, nine of them\nhave a cosine similarity of more than 70% with the topics of the Upanishads. We\nalso found that topics generated by the BERT-based models show very high\ncoherence as compared to that of conventional models. Our best performing model\ngives a coherence score of 73% on the Bhagavad Gita and 69% on The Upanishads.\nThe visualization of the low dimensional embeddings of these texts shows very\nclear overlapping among their topics adding another level of validation to our\nresults.",
    "descriptor": "",
    "authors": [
      "Rohitash Chandra",
      "Mukul Ranjan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11020"
  },
  {
    "id": "arXiv:2205.11023",
    "title": "AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable  Usage Representations",
    "abstract": "In software development, it is common for programmers to copy-paste code\nsnippets and then adapt them to their use case. This scenario motivates\n\\textit{code adaptation} task -- a variant of program repair which aims to\nadapt all variable identifiers in a pasted snippet of code to the surrounding,\npreexisting source code. Nevertheless, no existing approach have been shown to\neffectively address this task. In this paper, we introduce AdaptivePaste, a\nlearning-based approach to source code adaptation, based on the transformer\nmodel and a dedicated dataflow-aware deobfuscation pre-training task to learn\nmeaningful representations of variable usage patterns. We evaluate\nAdaptivePaste on a dataset of code snippets in Python. Evaluation results\nsuggest that our model can learn to adapt copy-pasted code with 79.8\\%\naccuracy.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Liu",
      "Jinu Jang",
      "Neel Sundaresan",
      "Miltiadis Allamanis",
      "Alexey Svyatkovskiy"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11023"
  },
  {
    "id": "arXiv:2205.11024",
    "title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language  Understanding",
    "abstract": "Prompt Tuning (PT) has been largely successful as a parameter-efficient way\nof conditioning large-scale pre-trained language models towards a downstream\ntask. More recently, soft prompt tuning has aimed to learn a fixed set of\ntask-specific continuous vectors, i.e., soft tokens that remain static across\nthe task samples. However, a fixed prompt may not generalize well to the\ndiverse kinds of inputs the task comprises. With this motivation, we propose a\nnovel way of prompting, Vector-quantized Input-contextualized Prompt Tuning or\nVIP. Essentially, VIP focuses on two aspects i) input-adaptation:\ninput-specific contextualization of the soft tokens; and ii) vector\nquantization: we pass the tokens through a quantizer which effectively reduces\nrepresentation variance by sampling prompts from a compact latent space. Over a\nwide range of natural language understanding tasks (SuperGLUE, QA, Relation\nClassification, NER, NLI), our proposed VIP framework beats the PT model by a\nmargin of 1.19\\%. Additionally, on Out-of-domain QA and Multi-Task setups over\n4 different tasks spanning over 12 domains, we find that VIP outperforms PT by\n0.75\\%.",
    "descriptor": "",
    "authors": [
      "Rishabh Bhardwaj",
      "Amrita Saha",
      "Steven C.H. Hoi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11024"
  },
  {
    "id": "arXiv:2205.11025",
    "title": "Flexible and Hierarchical Prior for Bayesian Nonnegative Matrix  Factorization",
    "abstract": "In this paper, we introduce a probabilistic model for learning nonnegative\nmatrix factorization (NMF) that is commonly used for predicting missing values\nand finding hidden patterns in the data, in which the matrix factors are latent\nvariables associated with each data dimension. The nonnegativity constraint for\nthe latent factors is handled by choosing priors with support on the\nnonnegative subspace. Bayesian inference procedure based on Gibbs sampling is\nemployed. We evaluate the model on several real-world datasets including\nMovieLens 100K and MovieLens 1M with different sizes and dimensions and show\nthat the proposed Bayesian NMF GRRN model leads to better predictions and\navoids overfitting compared to existing Bayesian NMF approaches.",
    "descriptor": "",
    "authors": [
      "Jun Lu",
      "Xuanyu Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11025"
  },
  {
    "id": "arXiv:2205.11027",
    "title": "Distance-Sensitive Offline Reinforcement Learning",
    "abstract": "In offline reinforcement learning (RL), one detrimental issue to policy\nlearning is the error accumulation of deep Q function in out-of-distribution\n(OOD) areas. Unfortunately, existing offline RL methods are often\nover-conservative, inevitably hurting generalization performance outside data\ndistribution. In our study, one interesting observation is that deep Q\nfunctions approximate well inside the convex hull of training data. Inspired by\nthis, we propose a new method, DOGE (Distance-sensitive Offline RL with better\nGEneralization). DOGE marries dataset geometry with deep function approximators\nin offline RL, and enables exploitation in generalizable OOD areas rather than\nstrictly constraining policy within data distribution. Specifically, DOGE\ntrains a state-conditioned distance function that can be readily plugged into\nstandard actor-critic methods as a policy constraint. Simple yet elegant, our\nalgorithm enjoys better generalization compared to state-of-the-art methods on\nD4RL benchmarks. Theoretical analysis demonstrates the superiority of our\napproach to existing methods that are solely based on data distribution or\nsupport constraints.",
    "descriptor": "\nComments: 29 pages, 14 figures, preprint\n",
    "authors": [
      "Jianxiong Li",
      "Xianyuan Zhan",
      "Haoran Xu",
      "Xiangyu Zhu",
      "Jingjing Liu",
      "Ya-Qin Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11027"
  },
  {
    "id": "arXiv:2205.11028",
    "title": "RCP: Recurrent Closest Point for Scene Flow Estimation on 3D Point  Clouds",
    "abstract": "3D motion estimation including scene flow and point cloud registration has\ndrawn increasing interest. Inspired by 2D flow estimation, recent methods\nemploy deep neural networks to construct the cost volume for estimating\naccurate 3D flow. However, these methods are limited by the fact that it is\ndifficult to define a search window on point clouds because of the irregular\ndata structure. In this paper, we avoid this irregularity by a simple yet\neffective method.We decompose the problem into two interlaced stages, where the\n3D flows are optimized point-wisely at the first stage and then globally\nregularized in a recurrent network at the second stage. Therefore, the\nrecurrent network only receives the regular point-wise information as the\ninput.In the experiments, we evaluate the proposed method on both the 3D scene\nflow estimation and the point cloud registration task. For 3D scene flow\nestimation, we make comparisons on the widely used FlyingThings3D and\nKITTIdatasets. For point cloud registration, we follow previous works and\nevaluate the data pairs with large pose and partially overlapping from\nModelNet40. The results show that our method outperforms the previous method\nand achieves a new state-of-the-art performance on both 3D scene flow\nestimation and point cloud registration, which demonstrates the superiority of\nthe proposed zero-order method on irregular point cloud data.",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Xiaodong Gu",
      "Chengzhou Tang",
      "Weihao Yuan",
      "Zuozhuo Dai",
      "Siyu Zhu",
      "Ping Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11028"
  },
  {
    "id": "arXiv:2205.11029",
    "title": "META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI",
    "abstract": "Task-oriented dialogue (TOD) systems have been widely used by mobile phone\nintelligent assistants to accomplish tasks such as calendar scheduling or hotel\nbooking. Current TOD systems usually focus on multi-turn text/speech\ninteraction and reply on calling back-end APIs to search database information\nor execute the task on mobile phone. However, this architecture greatly limits\nthe information searching capability of intelligent assistants and may even\nlead to task failure if APIs are not available or the task is too complicated\nto be executed by the provided APIs. In this paper, we propose a new TOD\narchitecture: GUI-based task-oriented dialogue system (GUI-TOD). A GUI-TOD\nsystem can directly perform GUI operations on real APPs and execute tasks\nwithout invoking backend APIs. Furthermore, we release META-GUI, a dataset for\ntraining a Multi-modal conversational agent on mobile GUI. We also propose a\nmulti-model action prediction and response model. It showed promising results\non META-GUI, but there is still room for further improvement. The dataset and\nmodels will be publicly available.",
    "descriptor": "\nComments: 14 pages, 10 figures\n",
    "authors": [
      "Liangtai Sun",
      "Xingyu Chen",
      "Lu Chen",
      "Tianle Dai",
      "Zichen Zhu",
      "Kai Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11029"
  },
  {
    "id": "arXiv:2205.11031",
    "title": "Body Composition Estimation Based on Multimodal Multi-task Deep Neural  Network",
    "abstract": "In addition to body weight and Body Mass Index (BMI), body composition is an\nessential data point that allows people to understand their overall health and\nbody fitness. However, body composition is largely made up of muscle, fat,\nbones, and water, which makes estimation not as easy and straightforward as\nmeasuring body weight. In this paper, we introduce a multimodal multi-task deep\nneural network to estimate body fat percentage and skeletal muscle mass by\nanalyzing facial images in addition to a person's height, gender, age, and\nweight information. Using a dataset representative of demographics in Japan, we\nconfirmed that the proposed approach performed better compared to the existing\nmethods. Moreover, the multi-task approach implemented in this study is also\nable to grasp the negative correlation between body fat percentage and skeletal\nmuscle mass gain/loss.",
    "descriptor": "\nComments: 11 pages, 8 figures, 1 table\n",
    "authors": [
      "Subas Chhatkuli",
      "Iris Jiang",
      "Kyohei Kamiyama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11031"
  },
  {
    "id": "arXiv:2205.11038",
    "title": "Computational Approach of Designing Magnetfree Nonreciprocal  Metamaterial",
    "abstract": "This article aims at discussing computational approach to design magnet-free\nnonreciprocal metamaterial. Detailed mathematical derivation on floquet mode\nanalysis is presented for Faraday and Kerr rotation. Non-reciprocity in the\ndesigned metasurface is achieved in the presence of biased transistor loaded in\nthe gap of circular ring resonator. Based on the synthesized mathematical\nmodel, We extract co-cross polarized components as well as Faraday and Kerr\nrotation from the developed synthesized model and compare/contrast reciprocal\nand nonreciprocal system.",
    "descriptor": "\nComments: 8 figures, 10 pages\n",
    "authors": [
      "Swadesh Poddar",
      "Md. Tanvir Hasan",
      "Md. Ragib Shakil Rafi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Applied Physics (physics.app-ph)",
      "Computational Physics (physics.comp-ph)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2205.11038"
  },
  {
    "id": "arXiv:2205.11039",
    "title": "FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph  Reasoning",
    "abstract": "Current best performing models for knowledge graph reasoning (KGR) are based\non complex distribution or geometry objects to embed entities and first-order\nlogical (FOL) queries in low-dimensional spaces. They can be summarized as a\ncenter-size framework (point/box/cone, Beta/Gaussian distribution, etc.) whose\nlogical reasoning ability is limited by the expressiveness of the relevant\nmathematical concepts. Because too deeply the center and the size depend on\neach other, it is difficult to integrate the logical reasoning ability with\nother models. To address these challenges, we instead propose a novel KGR\nframework named Feature-Logic Embedding framework, FLEX, which is the first KGR\nframework that can not only TRULY handle all FOL operations including\nconjunction, disjunction, negation and so on, but also support various feature\nspaces. Specifically, the logic part of feature-logic framework is based on\nvector logic, which naturally models all FOL operations. Experiments\ndemonstrate that FLEX significantly outperforms existing state-of-the-art\nmethods on benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Xueyuan Lin",
      "Haihong E",
      "Gengxian Zhou",
      "Tianyi Hu",
      "Li Ningyuan",
      "Mingzhi Sun",
      "Haoran Luo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11039"
  },
  {
    "id": "arXiv:2205.11044",
    "title": "Personalized Federated Learning with Server-Side Information",
    "abstract": "Personalized Federated Learning (FL) is an emerging research field in FL that\nlearns an easily adaptable global model in the presence of data heterogeneity\namong clients. However, one of the main challenges for personalized FL is the\nheavy reliance on clients' computing resources to calculate higher-order\ngradients since client data is segregated from the server to ensure privacy. To\nresolve this, we focus on a problem setting where the server may possess its\nown data independent of clients' data -- a prevalent problem setting in various\napplications, yet relatively unexplored in existing literature. Specifically,\nwe propose FedSIM, a new method for personalized FL that actively utilizes such\nserver data to improve meta-gradient calculation in the server for increased\npersonalization performance. Experimentally, we demonstrate through various\nbenchmarks and ablations that FedSIM is superior to existing methods in terms\nof accuracy, more computationally efficient by calculating the full\nmeta-gradients in the server, and converges up to 34.2% faster.",
    "descriptor": "",
    "authors": [
      "Jaehun Song",
      "Min-hwan Oh",
      "Hyung-Sin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11044"
  },
  {
    "id": "arXiv:2205.11047",
    "title": "Keypoint-Based Category-Level Object Pose Tracking from an RGB Sequence  with Uncertainty Estimation",
    "abstract": "We propose a single-stage, category-level 6-DoF pose estimation algorithm\nthat simultaneously detects and tracks instances of objects within a known\ncategory. Our method takes as input the previous and current frame from a\nmonocular RGB video, as well as predictions from the previous frame, to predict\nthe bounding cuboid and 6-DoF pose (up to scale). Internally, a deep network\npredicts distributions over object keypoints (vertices of the bounding cuboid)\nin image coordinates, after which a novel probabilistic filtering process\nintegrates across estimates before computing the final pose using PnP. Our\nframework allows the system to take previous uncertainties into consideration\nwhen predicting the current frame, resulting in predictions that are more\naccurate and stable than single frame methods. Extensive experiments show that\nour method outperforms existing approaches on the challenging Objectron\nbenchmark of annotated object videos. We also demonstrate the usability of our\nwork in an augmented reality setting.",
    "descriptor": "\nComments: ICRA 2022. Project site is at this https URL\n",
    "authors": [
      "Yunzhi Lin",
      "Jonathan Tremblay",
      "Stephen Tyree",
      "Patricio A. Vela",
      "Stan Birchfield"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.11047"
  },
  {
    "id": "arXiv:2205.11048",
    "title": "GBA: A Tuning-free Approach to Switch between Synchronous and  Asynchronous Training for Recommendation Model",
    "abstract": "High-concurrency asynchronous training upon parameter server (PS)\narchitecture and high-performance synchronous training upon all-reduce (AR)\narchitecture are the most commonly deployed distributed training modes for\nrecommender systems. Although the synchronous AR training is designed to have\nhigher training efficiency, the asynchronous PS training would be a better\nchoice on training speed when there are stragglers (slow workers) in the shared\ncluster, especially under limited computing resources. To take full advantages\nof these two training modes, an ideal way is to switch between them upon the\ncluster status. We find two obstacles to a tuning-free approach: the different\ndistribution of the gradient values and the stale gradients from the\nstragglers. In this paper, we propose Global Batch gradients Aggregation (GBA)\nover PS, which aggregates and applies gradients with the same global batch size\nas the synchronous training. A token-control process is implemented to assemble\nthe gradients and decay the gradients with severe staleness. We provide the\nconvergence analysis to demonstrate the robustness of GBA over the\nrecommendation models against the gradient staleness. Experiments on three\nindustrial-scale recommendation tasks show that GBA is an effective tuning-free\napproach for switching. Compared to the state-of-the-art derived asynchronous\ntraining, GBA achieves up to 0.2% improvement on the AUC metric, which is\nsignificant for the recommendation models. Meanwhile, under the strained\nhardware resource, GBA speeds up at least 2.4x compared to the synchronous\ntraining.",
    "descriptor": "",
    "authors": [
      "Wenbo Su",
      "Yuanxing Zhang",
      "Yufeng Cai",
      "Kaixu Ren",
      "Pengjie Wang",
      "Huimin Yi",
      "Yue Song",
      "Jing Chen",
      "Hongbo Deng",
      "Jian Xu",
      "Lin Qu",
      "Bo zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11048"
  },
  {
    "id": "arXiv:2205.11051",
    "title": "Flow-based Recurrent Belief State Learning for POMDPs",
    "abstract": "Partially Observable Markov Decision Process (POMDP) provides a principled\nand generic framework to model real world sequential decision making processes\nbut yet remains unsolved, especially for high dimensional continuous space and\nunknown models. The main challenge lies in how to accurately obtain the belief\nstate, which is the probability distribution over the unobservable environment\nstates given historical information. Accurately calculating this belief state\nis a precondition for obtaining an optimal policy of POMDPs. Recent advances in\ndeep learning techniques show great potential to learn good belief states.\nHowever, existing methods can only learn approximated distribution with limited\nflexibility. In this paper, we introduce the \\textbf{F}l\\textbf{O}w-based\n\\textbf{R}ecurrent \\textbf{BE}lief \\textbf{S}tate model (FORBES), which\nincorporates normalizing flows into the variational inference to learn general\ncontinuous belief states for POMDPs. Furthermore, we show that the learned\nbelief states can be plugged into downstream RL algorithms to improve\nperformance. In experiments, we show that our methods successfully capture the\ncomplex belief states that enable multi-modal predictions as well as high\nquality reconstructions, and results on challenging visual-motor control tasks\nshow that our method achieves superior performance and sample efficiency.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Chen",
      "Yao Mu",
      "Ping Luo",
      "Shengbo Li",
      "Jianyu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11051"
  },
  {
    "id": "arXiv:2205.11055",
    "title": "TempLM: Distilling Language Models into Template-Based Generators",
    "abstract": "While pretrained language models (PLMs) have greatly improved text\ngeneration, they have also been known to produce unfaithful or inappropriate\ncontent. In contrast, classic template-based systems provide strong guarantees\nof faithfulness at the cost of fluency. We propose TempLM, which achieves the\nbest of both worlds by distilling a PLM into a template-based generator. On the\nE2E and SynthBio data-to-text datasets, we show that TempLM is more faithful\nthan the original PLM and is more fluent than prior template systems. Notably,\non an out-of-domain evaluation, TempLM reduces a finetuned BART model's\nunfaithfulness rate from 83% to 0%. In a human study, we find that TempLM's\ntemplates substantially improve upon human-written ones in BERTScore.",
    "descriptor": "",
    "authors": [
      "Tianyi Zhang",
      "Mina Lee",
      "Lisa Li",
      "Ende Shen",
      "Tatsunori B. Hashimoto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11055"
  },
  {
    "id": "arXiv:2205.11057",
    "title": "Falsification of Multiple Requirements for Cyber-Physical Systems Using  Online Generative Adversarial Networks and Multi-Armed Bandits",
    "abstract": "We consider the problem of falsifying safety requirements of Cyber-Physical\nSystems expressed in signal temporal logic (STL). This problem can be turned\ninto an optimization problem via STL robustness functions. In this paper, our\nfocus is in falsifying systems with multiple requirements. We propose to solve\nsuch conjunctive requirements using online generative adversarial networks\n(GANs) as test generators. Our main contribution is an algorithm which\nfalsifies a conjunctive requirement $\\varphi_1 \\land \\cdots \\land \\varphi_n$ by\nusing a GAN for each requirement $\\varphi_i$ separately. Using ideas from\nmulti-armed bandit algorithms, our algorithm only trains a single GAN at every\nstep, which saves resources. Our experiments indicate that, in addition to\nsaving resources, this multi-armed bandit algorithm can falsify requirements\nwith fewer number of executions on the system under test when compared to (i)\nan algorithm training a single GAN for the complete conjunctive requirement and\n(ii) an algorithm always training $n$ GANs at each step.",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Jarkko Peltom\u00e4ki",
      "Ivan Porres"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11057"
  },
  {
    "id": "arXiv:2205.11060",
    "title": "Wasserstein Generative Adversarial Networks for Online Test Generation  for Cyber Physical Systems",
    "abstract": "We propose a novel online test generation algorithm WOGAN based on\nWasserstein Generative Adversarial Networks. WOGAN is a general-purpose\nblack-box test generator applicable to any system under test having a fitness\nfunction for determining failing tests. As a proof of concept, we evaluate\nWOGAN by generating roads such that a lane assistance system of a car fails to\nstay on the designated lane. We find that our algorithm has a competitive\nperformance respect to previously published algorithms.",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Jarkko Peltom\u00e4ki",
      "Frankie Spencer",
      "Ivan Porres"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11060"
  },
  {
    "id": "arXiv:2205.11061",
    "title": "Vegetation Mapping by UAV Visible Imagery and Machine Learning",
    "abstract": "An experimental field cropped with sugar-beet with a wide spreading of weeds\nhas been used to test vegetation identification from drone visible imagery.\nExpert masked and hue-filtered pictures have been used to train several Machine\nLearning algorithms to develop a semi-automatic methodology for identification\nand mapping species at high resolution. Results show that 5m altitude allows\nfor obtaining maps with an identification efficiency of more than 90%. Such a\nmethod can be easily integrated to present VRHA, as much as tools to obtain\ndetailed maps of vegetation.",
    "descriptor": "\nComments: 16 pages, numberedlines\n",
    "authors": [
      "Giuliano Vitali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11061"
  },
  {
    "id": "arXiv:2205.11063",
    "title": "Saliency-Driven Active Contour Model for Image Segmentation",
    "abstract": "Active contour models have achieved prominent success in the area of image\nsegmentation, allowing complex objects to be segmented from the background for\nfurther analysis. Existing models can be divided into region-based active\ncontour models and edge-based active contour models. However, both models use\ndirect image data to achieve segmentation and face many challenging problems in\nterms of the initial contour position, noise sensitivity, local minima and\ninefficiency owing to the in-homogeneity of image intensities. The saliency map\nof an image changes the image representation, making it more visual and\nmeaningful. In this study, we propose a novel model that uses the advantages of\na saliency map with local image information (LIF) and overcomes the drawbacks\nof previous models. The proposed model is driven by a saliency map of an image\nand the local image information to enhance the progress of the active contour\nmodels. In this model, the saliency map of an image is first computed to find\nthe saliency driven local fitting energy. Then, the saliency-driven local\nfitting energy is combined with the LIF model, resulting in a final novel\nenergy functional. This final energy functional is formulated through a level\nset formulation, and regulation terms are added to evolve the contour more\nprecisely across the object boundaries. The quality of the proposed method was\nverified on different synthetic images, real images and publicly available\ndatasets, including medical images. The image segmentation results, and\nquantitative comparisons confirmed the contour initialization independence,\nnoise insensitivity, and superior segmentation accuracy of the proposed model\nin comparison to the other segmentation models.",
    "descriptor": "",
    "authors": [
      "Ehtesham Iqbal",
      "Asim Niaz",
      "Asif Aziz Memon",
      "Usman Asim",
      "Kwang Nam Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11063"
  },
  {
    "id": "arXiv:2205.11064",
    "title": "WOGAN at the SBST 2022 CPS Tool Competition",
    "abstract": "WOGAN is an online test generation algorithm based on Wasserstein generative\nadversarial networks. In this note, we present how WOGAN works and summarize\nits performance in the SBST 2022 CPS tool competition concerning the AI of a\nself-driving car.",
    "descriptor": "\nComments: 2 pages, 2 figures\n",
    "authors": [
      "Jarkko Peltom\u00e4ki",
      "Frankie Spencer",
      "Ivan Porres"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11064"
  },
  {
    "id": "arXiv:2205.11069",
    "title": "Heterogeneity-aware P2P Wireless Energy Transfer for Balanced Energy  Distribution",
    "abstract": "The recent advances in wireless energy transfer (WET) provide an alternate\nand reliable option for replenishing the battery of pervasive and portable\ndevices, such as smartphones. The peer-to-peer (P2P) mode of WET brings\nimproved flexibility to the charging process among the devices as they can\nmaintain their mobility while replenishing their battery. Few existing works in\nP2P-WET unrealistically assume the nodes to be exchanging energy at every\nopportunity with any other node. Also, energy exchange between the nodes is not\nbounded by the energy transfer limit in that inter-node meeting duration. In\nthis regard, the parametric heterogeneity (in terms of device's battery\ncapacity and WET hardware) among the nodes also affects the energy transfer\nbound in each P2P interaction, and thus, may lead to unbalanced network energy\ndistributions. This inherent heterogeneity aspect has not been adequately\ncovered in the P2P-WET literature so far, especially from the point of view of\nmaintaining a balanced energy distribution in the networked population. In this\nwork, we present a Heterogeneity-aware Wireless Energy Transfer (HetWET)\nmethod. In contrast to the existing literature, we devise a fine-grained model\nof wireless energy transfer while considering the parametric heterogeneity of\nthe participating devices. Thereafter, we enable the nodes to explore and\ndynamically decide the peers for energy exchange. The performance of HetWET is\nevaluated using extensive simulations with varying heterogeneity settings. The\nevaluation results demonstrate that HetWET can maintain lower energy losses and\nachieve more balanced energy variation distance compared to three different\nstate-of-the-art methods.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was carried out during the tenure of an ERCIM 'Alain Bensoussan' Fellowship Programme of the first author\n",
    "authors": [
      "Tamoghna Ojha",
      "Theofanis P. Raptis",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.11069"
  },
  {
    "id": "arXiv:2205.11071",
    "title": "Self-distilled Knowledge Delegator for Exemplar-free Class Incremental  Learning",
    "abstract": "Exemplar-free incremental learning is extremely challenging due to\ninaccessibility of data from old tasks. In this paper, we attempt to exploit\nthe knowledge encoded in a previously trained classification model to handle\nthe catastrophic forgetting problem in continual learning. Specifically, we\nintroduce a so-called knowledge delegator, which is capable of transferring\nknowledge from the trained model to a randomly re-initialized new model by\ngenerating informative samples. Given the previous model only, the delegator is\neffectively learned using a self-distillation mechanism in a data-free manner.\nThe knowledge extracted by the delegator is then utilized to maintain the\nperformance of the model on old tasks in incremental learning. This simple\nincremental learning framework surpasses existing exemplar-free methods by a\nlarge margin on four widely used class incremental benchmarks, namely\nCIFAR-100, ImageNet-Subset, Caltech-101 and Flowers-102. Notably, we achieve\ncomparable performance to some exemplar-based methods without accessing any\nexemplars.",
    "descriptor": "\nComments: Accepted by IJCNN 2022\n",
    "authors": [
      "Fanfan Ye",
      "Liang Ma",
      "Qiaoyong Zhong",
      "Di Xie",
      "Shiliang Pu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11071"
  },
  {
    "id": "arXiv:2205.11081",
    "title": "BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural  Language Generation in Bangla",
    "abstract": "This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language in the web domain. We aggregate three challenging\nconditional text generation tasks under the BanglaNLG benchmark. Then, using a\nclean corpus of 27.5 GB of Bangla data, we pretrain BanglaT5, a\nsequence-to-sequence Transformer model for Bangla. BanglaT5 achieves\nstate-of-the-art performance in all of these tasks, outperforming mT5 (base) by\nup to 5.4%. We are making the BanglaT5 language model and a leaderboard\npublicly available in the hope of advancing future research and evaluation on\nBangla NLG. The resources can be found at\nhttps://github.com/csebuetnlp/BanglaNLG.",
    "descriptor": "",
    "authors": [
      "Abhik Bhattacharjee",
      "Tahmid Hasan",
      "Wasi Uddin Ahmad",
      "Rifat Shahriyar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11081"
  },
  {
    "id": "arXiv:2205.11082",
    "title": "YouTube Ad View Sentiment Analysis using Deep Learning and Machine  Learning",
    "abstract": "Sentiment Analysis is currently a vital area of research. With the\nadvancement in the use of the internet, the creation of social media, websites,\nblogs, opinions, ratings, etc. has increased rapidly. People express their\nfeedback and emotions on social media posts in the form of likes, dislikes,\ncomments, etc. The rapid growth in the volume of viewer-generated or\nuser-generated data or content on YouTube has led to an increase in YouTube\nsentiment analysis. Due to this, analyzing the public reactions has become an\nessential need for information extraction and data visualization in the\ntechnical domain. This research predicts YouTube Ad view sentiments using Deep\nLearning and Machine Learning algorithms like Linear Regression (LR), Support\nVector Machine (SVM), Decision Tree (DT), Random Forest (RF), and Artificial\nNeural Network (ANN). Finally, a comparative analysis is done based on\nexperimental results acquired from different models.",
    "descriptor": "\nComments: 5 pages, 9 figures, Published with International Journal of Computer Applications (IJCA)\n",
    "authors": [
      "Tanvi Mehta",
      "Ganesh Deshmukh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11082"
  },
  {
    "id": "arXiv:2205.11083",
    "title": "MonoFormer: Towards Generalization of self-supervised monocular depth  estimation with Transformers",
    "abstract": "Self-supervised monocular depth estimation has been widely studied recently.\nMost of the work has focused on improving performance on benchmark datasets,\nsuch as KITTI, but has offered a few experiments on generalization performance.\nIn this paper, we investigate the backbone networks (e.g. CNNs, Transformers,\nand CNN-Transformer hybrid models) toward the generalization of monocular depth\nestimation. We first evaluate state-of-the-art models on diverse public\ndatasets, which have never been seen during the network training. Next, we\ninvestigate the effects of texture-biased and shape-biased representations\nusing the various texture-shifted datasets that we generated. We observe that\nTransformers exhibit a strong shape bias and CNNs do a strong texture-bias. We\nalso find that shape-biased models show better generalization performance for\nmonocular depth estimation compared to texture-biased models. Based on these\nobservations, we newly design a CNN-Transformer hybrid network with a\nmulti-level adaptive feature fusion module, called MonoFormer. The design\nintuition behind MonoFormer is to increase shape bias by employing Transformers\nwhile compensating for the weak locality bias of Transformers by adaptively\nfusing multi-level representations. Extensive experiments show that the\nproposed method achieves state-of-the-art performance with various public\ndatasets. Our method also shows the best generalization ability among the\ncompetitive methods.",
    "descriptor": "",
    "authors": [
      "Jinwoo Bae",
      "Sungho Moon",
      "Sunghoon Im"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11083"
  },
  {
    "id": "arXiv:2205.11087",
    "title": "MetaSlicing: A Novel Resource Allocation Framework for Metaverse",
    "abstract": "Creating and maintaining the Metaverse requires enormous resources that have\nnever been seen before, especially computing resources for intensive data\nprocessing to support the extended reality, enormous storage resources, and\nmassive networking resources for maintaining ultra high-speed and low-latency\nconnections. Therefore, this work aims to propose a novel framework, namely\nMetaSlicing, that can provide a highly effective and comprehensive solution in\nmanaging and allocating different types of resources for Metaverse\napplications. In particular, by observing that Metaverse applications may have\ncommon functions, we first propose grouping applications into clusters, called\nMetaInstances. In a MetaInstance, common functions can be shared among\napplications. As such, the same resources can be used by multiple applications\nsimultaneously, thereby enhancing resource utilization dramatically. To address\nthe real-time characteristic and resource demand's dynamic and uncertainty in\nthe Metaverse, we develop an effective framework based on the semi-Markov\ndecision process and propose an intelligent admission control algorithm that\ncan maximize resource utilization and enhance the Quality-of-Service for\nend-users. Extensive simulation results show that our proposed solution\noutperforms the Greedy-based policy by up to 80% and 47% in terms of long-term\nrevenue for Metaverse providers and request acceptance probability,\nrespectively.",
    "descriptor": "",
    "authors": [
      "Nam H.Chu",
      "Dinh Thai Hoang",
      "Diep N. Nguyen",
      "Khoa T. Phan",
      "Eryk Dutkiewicz"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.11087"
  },
  {
    "id": "arXiv:2205.11088",
    "title": "FAST: A Fully-Concurrent Access Technique to All SRAM Rows for Enhanced  Speed and Energy Efficiency in Data-Intensive Applications",
    "abstract": "Compute-in-memory (CiM) is a promising approach to improving the computing\nspeed and energy efficiency in dataintensive applications. Beyond existing CiM\ntechniques of bitwise logic-in-memory operations and dot product operations,\nthis paper extends the CiM paradigm with FAST, a new shift-based inmemory\ncomputation technique to handle high-concurrency operations on multiple rows in\nan SRAM. Such high-concurrency operations are widely seen in both conventional\napplications (e.g. the table update in a database), and emerging applications\n(e.g. the parallel weight update in neural network accelerators), in which low\nlatency and low energy consumption are critical. The proposed shift-based CiM\narchitecture is enabled by integrating the shifter function into each SRAM\ncell, and by creating a datapath that exploits the high-parallelism of shifting\noperations in multiple rows in the array. A 128-row 16-column shiftable SRAM in\n65nm CMOS is designed to evaluate the proposed architecture. Postlayout SPICE\nsimulations show average improvements of 4.4x energy efficiency and 96.0x speed\nover a conventional fully-digital memory-computing-separated scheme, when\nperforming the 8-bit weight update task in a VGG-7 framework.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Yiming Chen",
      "Yushen Fu",
      "Mingyen Lee",
      "Sumitha George",
      "Yongpan Liu",
      "Vijaykrishnan Narayanan",
      "Huazhong Yang",
      "Xueqing Li"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.11088"
  },
  {
    "id": "arXiv:2205.11090",
    "title": "FaceMAE: Privacy-Preserving Face Recognition via Masked Autoencoders",
    "abstract": "Face recognition, as one of the most successful applications in artificial\nintelligence, has been widely used in security, administration, advertising,\nand healthcare. However, the privacy issues of public face datasets have\nattracted increasing attention in recent years. Previous works simply mask most\nareas of faces or synthesize samples using generative models to construct\nprivacy-preserving face datasets, which overlooks the trade-off between privacy\nprotection and data utility. In this paper, we propose a novel framework\nFaceMAE, where the face privacy and recognition performance are considered\nsimultaneously. Firstly, randomly masked face images are used to train the\nreconstruction module in FaceMAE. We tailor the instance relation matching\n(IRM) module to minimize the distribution gap between real faces and FaceMAE\nreconstructed ones. During the deployment phase, we use trained FaceMAE to\nreconstruct images from masked faces of unseen identities without extra\ntraining. The risk of privacy leakage is measured based on face retrieval\nbetween reconstructed and original datasets. Experiments prove that the\nidentities of reconstructed images are difficult to be retrieved. We also\nperform sufficient privacy-preserving face recognition on several public face\ndatasets (i.e. CASIA-WebFace and WebFace260M). Compared to previous state of\nthe arts, FaceMAE consistently \\textbf{reduces at least 50\\% error rate} on\nLFW, CFP-FP and AgeDB.",
    "descriptor": "\nComments: A new paradigm for privacy-preserving face recognition via MAE\n",
    "authors": [
      "Kai Wang",
      "Bo Zhao",
      "Xiangyu Peng",
      "Zheng Zhu",
      "Jiankang Deng",
      "Xinchao Wang",
      "Hakan Bilen",
      "Yang You"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11090"
  },
  {
    "id": "arXiv:2205.11097",
    "title": "A Fine-grained Interpretability Evaluation Benchmark for Neural NLP",
    "abstract": "While there is increasing concern about the interpretability of neural\nmodels, the evaluation of interpretability remains an open problem, due to the\nlack of proper evaluation datasets and metrics. In this paper, we present a\nnovel benchmark to evaluate the interpretability of both neural models and\nsaliency methods. This benchmark covers three representative NLP tasks:\nsentiment analysis, textual similarity and reading comprehension, each provided\nwith both English and Chinese annotated data. In order to precisely evaluate\nthe interpretability, we provide token-level rationales that are carefully\nannotated to be sufficient, compact and comprehensive. We also design a new\nmetric, i.e., the consistency between the rationales before and after\nperturbations, to uniformly evaluate the interpretability of models and\nsaliency methods on different tasks. Based on this benchmark, we conduct\nexperiments on three typical models with three saliency methods, and unveil\ntheir strengths and weakness in terms of interpretability. We will release this\nbenchmark at \\url{https://xyz} and hope it can facilitate the research in\nbuilding trustworthy systems.",
    "descriptor": "",
    "authors": [
      "Lijie Wang",
      "Yaozong Shen",
      "Shuyuan Peng",
      "Shuai Zhang",
      "Xinyan Xiao",
      "Hao Liu",
      "Hongxuan Tang",
      "Ying Chen",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11097"
  },
  {
    "id": "arXiv:2205.11098",
    "title": "PointDistiller: Structured Knowledge Distillation Towards Efficient and  Compact 3D Detection",
    "abstract": "The remarkable breakthroughs in point cloud representation learning have\nboosted their usage in real-world applications such as self-driving cars and\nvirtual reality. However, these applications usually have an urgent requirement\nfor not only accurate but also efficient 3D object detection. Recently,\nknowledge distillation has been proposed as an effective model compression\ntechnique, which transfers the knowledge from an over-parameterized teacher to\na lightweight student and achieves consistent effectiveness in 2D vision.\nHowever, due to point clouds' sparsity and irregularity, directly applying\nprevious image-based knowledge distillation methods to point cloud detectors\nusually leads to unsatisfactory performance. To fill the gap, this paper\nproposes PointDistiller, a structured knowledge distillation framework for\npoint clouds-based 3D detection. Concretely, PointDistiller includes local\ndistillation which extracts and distills the local geometric structure of point\nclouds with dynamic graph convolution and reweighted learning strategy, which\nhighlights student learning on the crucial points or voxels to improve\nknowledge distillation efficiency. Extensive experiments on both voxels-based\nand raw points-based detectors have demonstrated the effectiveness of our\nmethod over seven previous knowledge distillation methods. For instance, our 4X\ncompressed PointPillars student achieves 2.8 and 3.4 mAP improvements on BEV\nand 3D object detection, outperforming its teacher by 0.9 and 1.8 mAP,\nrespectively. Codes have been released at\nhttps://github.com/RunpeiDong/PointDistiller.",
    "descriptor": "",
    "authors": [
      "Linfeng Zhang",
      "Runpei Dong",
      "Hung-Shuo Tai",
      "Kaisheng Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11098"
  },
  {
    "id": "arXiv:2205.11100",
    "title": "Supporting Vision-Language Model Inference with Causality-pruning  Knowledge Prompt",
    "abstract": "Vision-language models are pre-trained by aligning image-text pairs in a\ncommon space so that the models can deal with open-set visual concepts by\nlearning semantic information from textual labels. To boost the transferability\nof these models on downstream tasks in a zero-shot manner, recent works explore\ngenerating fixed or learnable prompts, i.e., classification weights are\nsynthesized from natural language describing task-relevant categories, to\nreduce the gap between tasks in the training and test phases. However, how and\nwhat prompts can improve inference performance remains unclear. In this paper,\nwe explicitly provide exploration and clarify the importance of including\nsemantic information in prompts, while existing prompt methods generate prompts\nwithout exploring the semantic information of textual labels. A challenging\nissue is that manually constructing prompts, with rich semantic information,\nrequires domain expertise and is extremely time-consuming. To this end, we\npropose Causality-pruning Knowledge Prompt (CapKP) for adapting pre-trained\nvision-language models to downstream image recognition. CapKP retrieves an\nontological knowledge graph by treating the textual label as a query to explore\ntask-relevant semantic information. To further refine the derived semantic\ninformation, CapKP introduces causality-pruning by following the first\nprinciple of Granger causality. Empirically, we conduct extensive evaluations\nto demonstrate the effectiveness of CapKP, e.g., with 8 shots, CapKP\noutperforms the manual-prompt method by 12.51% and the learnable-prompt method\nby 1.39% on average, respectively. Experimental analyses prove the superiority\nof CapKP in domain generalization compared to benchmark approaches.",
    "descriptor": "",
    "authors": [
      "Jiangmeng Li",
      "Wenyi Mo",
      "Wenwen Qiang",
      "Bing Su",
      "Changwen Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11100"
  },
  {
    "id": "arXiv:2205.11101",
    "title": "FL Games: A federated learning framework for distribution shifts",
    "abstract": "Federated learning aims to train predictive models for data that is\ndistributed across clients, under the orchestration of a server. However,\nparticipating clients typically each hold data from a different distribution,\nwhereby predictive models with strong in-distribution generalization can fail\ncatastrophically on unseen domains. In this work, we argue that in order to\ngeneralize better across non-i.i.d. clients, it is imperative to only learn\ncorrelations that are stable and invariant across domains. We propose FL Games,\na game-theoretic framework for federated learning for learning causal features\nthat are invariant across clients. While training to achieve the Nash\nequilibrium, the traditional best response strategy suffers from high-frequency\noscillations. We demonstrate that FL Games effectively resolves this challenge\nand exhibits smooth performance curves. Further, FL Games scales well in the\nnumber of clients, requires significantly fewer communication rounds, and is\nagnostic to device heterogeneity. Through empirical evaluation, we demonstrate\nthat FL Games achieves high out-of-distribution performance on various\nbenchmarks.",
    "descriptor": "",
    "authors": [
      "Sharut Gupta",
      "Kartik Ahuja",
      "Mohammad Havaei",
      "Niladri Chatterjee",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11101"
  },
  {
    "id": "arXiv:2205.11102",
    "title": "The k-Server with Preferences Problem",
    "abstract": "The famous $k$-Server Problem covers plenty of resource allocation scenarios,\nand several variations have been studied extensively. However, to the best of\nour knowledge, no research has considered the problem if the servers are not\nidentical and requests can express which servers should serve them. Therefore,\nwe present a new model generalizing the $k$-Server Problem by preferences of\nthe requests and study it in uniform metrics for deterministic online\nalgorithms. In our model, requests can either demand to be answered by any\nserver (general requests) or by a specific one (specific requests). If only\ngeneral requests appear, the instance is one of the $k$-Server Problem, and a\nlower bound for the competitive ratio of $k$ applies. If only specific requests\nappear, a competitive ratio of $1$ becomes trivial since there is no freedom\nregarding the servers' movements. We show that if both kinds of requests\nappear, the lower bound raises to $2k-1$. We study deterministic online\nalgorithms in uniform metrics and present two algorithms. The first one has a\ncompetitive ratio dependent on the frequency of specific requests. It achieves\na worst-case competitive ratio of $3k-2$ while it is optimal when only general\nor only specific requests appear (ratio of $k$ and $1$). The second has a\nclose-to-optimal worst-case competitive ratio of $2k+14$. For the first\nalgorithm, we show a lower bound of $3k-2$, while the second one has one of\n$2k-1$ when only general requests appear. Both algorithms differ in only one\nbehavioral rule for each server that significantly influences the competitive\nratio. Each server acting according to the rule allows approaching the\nworst-case lower bound, while it implies an increased lower bound for\n$k$-Server instances. Thus, there is a trade-off between performing well\nagainst instances of the $k$-Server Problem and ones containing specific\nrequests.",
    "descriptor": "\nComments: A conference version of this paper was accepted at the 34th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA 2022)\n",
    "authors": [
      "Jannik Castenow",
      "Bj\u00f6rn Feldkord",
      "Till Knollmann",
      "Manuel Malatyali",
      "Friedhelm Meyer auf der Heide"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.11102"
  },
  {
    "id": "arXiv:2205.11103",
    "title": "Proceedings Seventeenth International Workshop on the ACL2 Theorem  Prover and its Applications",
    "abstract": "This volume contains a selection of papers presented at the 17th\nInternational Workshop on the ACL2 Theorem Prover and its Applications (ACL2\n2022). The workshops are the premier technical forum for presenting research\nand experiences related to ACL2.",
    "descriptor": "",
    "authors": [
      "Rob Sumners",
      "Cuong Chau"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11103"
  },
  {
    "id": "arXiv:2205.11104",
    "title": "Generalization, Mayhems and Limits in Recurrent Proximal Policy  Optimization",
    "abstract": "At first sight it may seem straightforward to use recurrent layers in Deep\nReinforcement Learning algorithms to enable agents to make use of memory in the\nsetting of partially observable environments. Starting from widely used\nProximal Policy Optimization (PPO), we highlight vital details that one must\nget right when adding recurrence to achieve a correct and efficient\nimplementation, namely: properly shaping the neural net's forward pass,\narranging the training data, correspondingly selecting hidden states for\nsequence beginnings and masking paddings for loss computation. We further\nexplore the limitations of recurrent PPO by benchmarking the contributed novel\nenvironments Mortar Mayhem and Searing Spotlights that challenge the agent's\nmemory beyond solely capacity and distraction tasks. Remarkably, we can\ndemonstrate a transition to strong generalization in Mortar Mayhem when scaling\nthe number of training seeds, while the agent does not succeed on Searing\nSpotlights, which seems to be a tough challenge for memory-based agents.",
    "descriptor": "\nComments: 17 pages, 18 figures, preprint\n",
    "authors": [
      "Marco Pleines",
      "Matthias Pallasch",
      "Frank Zimmer",
      "Mike Preuss"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11104"
  },
  {
    "id": "arXiv:2205.11107",
    "title": "Learning to branch with Tree MDPs",
    "abstract": "State-of-the-art Mixed Integer Linear Program (MILP) solvers combine\nsystematic tree search with a plethora of hard-coded heuristics, such as the\nbranching rule. The idea of learning branching rules from data has received\nincreasing attention recently, and promising results have been obtained by\nlearning fast approximations of the strong branching expert. In this work, we\ninstead propose to learn branching rules from scratch via Reinforcement\nLearning (RL). We revisit the work of Etheve et al. (2020) and propose tree\nMarkov Decision Processes, or tree MDPs, a generalization of temporal MDPs that\nprovides a more suitable framework for learning to branch. We derive a tree\npolicy gradient theorem, which exhibits a better credit assignment compared to\nits temporal counterpart. We demonstrate through computational experiments that\ntree MDPs improve the learning convergence, and offer a promising framework for\ntackling the learning-to-branch problem in MILPs.",
    "descriptor": "\nComments: 10 pages, 2 figures, plus supplementary material\n",
    "authors": [
      "Lara Scavuzzo",
      "Feng Yang Chen",
      "Didier Ch\u00e9telat",
      "Maxime Gasse",
      "Andrea Lodi",
      "Neil Yorke-Smith",
      "Karen Aardal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11107"
  },
  {
    "id": "arXiv:2205.11108",
    "title": "Paddy Doctor: A Visual Image Dataset for Paddy Disease Classification",
    "abstract": "One of the critical biotic stress factors paddy farmers face is diseases\ncaused by bacteria, fungi, and other organisms. These diseases affect plants'\nhealth severely and lead to significant crop loss. Most of these diseases can\nbe identified by regularly observing the leaves and stems under expert\nsupervision. In a country with vast agricultural regions and limited crop\nprotection experts, manual identification of paddy diseases is challenging.\nThus, to add a solution to this problem, it is necessary to automate the\ndisease identification process and provide easily accessible decision support\ntools to enable effective crop protection measures. However, the lack of\navailability of public datasets with detailed disease information limits the\npractical implementation of accurate disease detection systems. This paper\npresents Paddy Doctor, a visual image dataset for identifying paddy diseases.\nOur dataset contains 13,876 annotated paddy leaf images across ten classes\n(nine diseases and normal leaf). We benchmarked the Paddy Doctor using a\nConvolutional Neural Network (CNN) and two transfer learning approaches, VGG16\nand MobileNet. The experimental results show that MobileNet achieves the\nhighest classification accuracy of 93.83\\%. We release our dataset and\nreproducible code in the open source for community use.",
    "descriptor": "",
    "authors": [
      "Petchiammal A",
      "Briskline Kiruba S",
      "D. Murugan",
      "Pandarasamy A"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11108"
  },
  {
    "id": "arXiv:2205.11109",
    "title": "Gradient Hedging for Intensively Exploring Salient Interpretation beyond  Neuron Activation",
    "abstract": "Hedging is a strategy for reducing the potential risks in various types of\ninvestments by adopting an opposite position in a related asset. Motivated by\nthe equity technique, we introduce a method for decomposing output predictions\ninto intensive salient attributions by hedging the evidence for a decision. We\nanalyze the conventional approach applied to the evidence for a decision and\ndiscuss the paradox of the conservation rule. Subsequently, we define the\nviewpoint of evidence as a gap of positive and negative influence among the\ngradient-derived initial contribution maps and propagate the antagonistic\nelements to the evidence as suppressors, following the criterion of the degree\nof positive attribution defined by user preference. In addition, we reflect the\nseverance or sparseness contribution of inactivated neurons, which are mostly\nirrelevant to a decision, resulting in increased robustness to\ninterpretability. We conduct the following assessments in a verified\nexperimental environment: pointing game, most relevant first region insertion,\noutside-inside relevance ratio, and mean average precision on the PASCAL VOC\n2007, MS COCO 2014, and ImageNet datasets. The results demonstrate that our\nmethod outperforms existing attribution methods in distinctive, intensive, and\nintuitive visualization with robustness and applicability in general models.",
    "descriptor": "",
    "authors": [
      "Woo-Jeoung Nam",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11109"
  },
  {
    "id": "arXiv:2205.11110",
    "title": "Meta-Learning Regrasping Strategies for Physical-Agnostic Objects",
    "abstract": "Grasping inhomogeneous objects, practical use in real-world applications,\nremains a challenging task due to the unknown physical properties such as mass\ndistribution and coefficient of friction. In this study, we propose a\nvision-based meta-learning algorithm to learn physical properties in an\nagnostic way. In particular, we employ Conditional Neural Processes (CNPs) on\ntop of DexNet-2.0. CNPs learn physical embeddings rapidly from a few\nobservations where each observation is composed of i) the cropped depth image,\nii) the grasping height between the gripper and estimated grasping point, and\niii) the binary grasping result. Our modified conditional DexNet-2.0\n(DexNet-CNP) updates the predicted grasping quality iteratively from new\nobservations, which can be executed in an online fashion. We evaluate our\nmethod in the Pybullet simulator using various shape primitive objects with\ndifferent physical parameters. The results show that our model outperforms the\noriginal DexNet-2.0 and is able to generalize on unseen objects with different\nshapes.",
    "descriptor": "\nComments: Accepted as spotlight in ICRA 2022 Workshop: Scaling Robot Learning\n",
    "authors": [
      "Ruijie Chen",
      "Ning Gao",
      "Ngo Anh Vien",
      "Hanna Ziesche",
      "Gerhard Neumann"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11110"
  },
  {
    "id": "arXiv:2205.11111",
    "title": "DistilCamemBERT: a distillation of the French model CamemBERT",
    "abstract": "Modern Natural Language Processing (NLP) models based on Transformer\nstructures represent the state of the art in terms of performance on very\ndiverse tasks. However, these models are complex and represent several hundred\nmillion parameters for the smallest of them. This may hinder their adoption at\nthe industrial level, making it difficult to scale up to a reasonable\ninfrastructure and/or to comply with societal and environmental\nresponsibilities. To this end, we present in this paper a model that\ndrastically reduces the computational cost of a well-known French model\n(CamemBERT), while preserving good performance.",
    "descriptor": "\nComments: in French language. CAp (Conf{\\'e}rence sur l'Apprentissage automatique), Jul 2022, Vannes, France\n",
    "authors": [
      "Cyrile Delestre",
      "Abibatou Amar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11111"
  },
  {
    "id": "arXiv:2205.11113",
    "title": "What Drives the Use of Metaphorical Language? Negative Insights from  Abstractness, Affect, Discourse Coherence and Contextualized Word  Representations",
    "abstract": "Given a specific discourse, which discourse properties trigger the use of\nmetaphorical language, rather than using literal alternatives? For example,\nwhat drives people to say \"grasp the meaning\" rather than \"understand the\nmeaning\" within a specific context? Many NLP approaches to metaphorical\nlanguage rely on cognitive and (psycho-)linguistic insights and have\nsuccessfully defined models of discourse coherence, abstractness and affect. In\nthis work, we build five simple models relying on established cognitive and\nlinguistic properties -- frequency, abstractness, affect, discourse coherence\nand contextualized word representations -- to predict the use of a metaphorical\nvs. synonymous literal expression in context. By comparing the models' outputs\nto human judgments, our study indicates that our selected properties are not\nsufficient to systematically explain metaphorical vs. literal language choices.",
    "descriptor": "\nComments: 12 pages, 6 figures, 1 table. Accepted at *SEM2022\n",
    "authors": [
      "Prisca Piccirilli",
      "Sabine Schulte im Walde"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11113"
  },
  {
    "id": "arXiv:2205.11116",
    "title": "Summarize and Generate to Back-translate: Unsupervised Translation of  Programming Languages",
    "abstract": "Back-translation is widely known for its effectiveness for neural machine\ntranslation when little to no parallel data is available. In this approach, a\nsource-to-target model is coupled with a target-to-source model trained in\nparallel. The target-to-source model generates noisy sources, while the\nsource-to-target model is trained to reconstruct the targets and vice versa.\nRecent developments of multilingual pre-trained sequence-to-sequence models for\nprogramming languages have been very effective for a broad spectrum of\ndownstream software engineering tasks. Hence, it is compelling to train them to\nbuild programming language translation systems via back-translation. However,\nthese models cannot be further trained via back-translation since they learn to\noutput sequences in the same language as the inputs during pre-training. As an\nalternative, we propose performing back-translation via code summarization and\ngeneration. In code summarization, a model learns to generate natural language\n(NL) summaries given code snippets. In code generation, the model learns to do\nthe opposite. Therefore, target-to-source generation in back-translation can be\nviewed as target-to-NL-to-source generation. We show that our proposed approach\nperforms competitively with state-of-the-art methods.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Wasi Uddin Ahmad",
      "Saikat Chakraborty",
      "Baishakhi Ray",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.11116"
  },
  {
    "id": "arXiv:2205.11117",
    "title": "PyRelationAL: A Library for Active Learning Research and Development",
    "abstract": "In constrained real-world scenarios where it is challenging or costly to\ngenerate data, disciplined methods for acquiring informative new data points\nare of fundamental importance for the efficient training of machine learning\n(ML) models. Active learning (AL) is a subfield of ML focused on the\ndevelopment of methods to iteratively and economically acquire data through\nstrategically querying new data points that are the most useful for a\nparticular task. Here, we introduce PyRelationAL, an open source library for AL\nresearch. We describe a modular toolkit that is compatible with diverse ML\nframeworks (e.g. PyTorch, Scikit-Learn, TensorFlow, JAX). Furthermore, to help\naccelerate research and development in the field, the library implements a\nnumber of published methods and provides API access to wide-ranging benchmark\ndatasets and AL task configurations based on existing literature. The library\nis supplemented by an expansive set of tutorials, demos, and documentation to\nhelp users get started. We perform experiments on the PyRelationAL collection\nof benchmark datasets and showcase the considerable economies that AL can\nprovide. PyRelationAL is maintained using modern software engineering practices\n- with an inclusive contributor code of conduct - to promote long term library\nquality and utilisation.",
    "descriptor": "",
    "authors": [
      "Paul Scherer",
      "Thomas Gaudelet",
      "Alison Pouplin",
      "Suraj M S",
      "Jyothish Soman",
      "Lindsay Edwards",
      "Jake P. Taylor-King"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11117"
  },
  {
    "id": "arXiv:2205.11121",
    "title": "A normal approximation for joint frequency estimatation under Local  Differential Privacy",
    "abstract": "In the recent years, Local Differential Privacy (LDP) has been one of the\ncorner stone of privacy preserving data analysis. However, many challenges\nstill opposes its widespread application. One of these problems is the\nscalability of LDP to high dimensional data, in particular for estimating\njoint-distributions. In this paper, we develop an approximate estimator for\ncategory frequency joint-distribution under so-called pure LDP protocols.",
    "descriptor": "\nComments: Preliminary development, draft\n",
    "authors": [
      "Thomas Carette"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.11121"
  },
  {
    "id": "arXiv:2205.11124",
    "title": "ConvPoseCNN2: Prediction and Refinement of Dense 6D Object Poses",
    "abstract": "Object pose estimation is a key perceptual capability in robotics. We propose\na fully-convolutional extension of the PoseCNN method, which densely predicts\nobject translations and orientations. This has several advantages such as\nimproving the spatial resolution of the orientation predictions -- useful in\nhighly-cluttered arrangements, significant reduction in parameters by avoiding\nfull connectivity, and fast inference. We propose and discuss several\naggregation methods for dense orientation predictions that can be applied as a\npost-processing step, such as averaging and clustering techniques. We\ndemonstrate that our method achieves the same accuracy as PoseCNN on the\nchallenging YCB-Video dataset and provide a detailed ablation study of several\nvariants of our method. Finally, we demonstrate that the model can be further\nimproved by inserting an iterative refinement module into the middle of the\nnetwork, which enforces consistency of the prediction.",
    "descriptor": "",
    "authors": [
      "Arul Selvam Periyasamy",
      "Catherine Capellen",
      "Max Schwarz",
      "Sven Behnke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11124"
  },
  {
    "id": "arXiv:2205.11126",
    "title": "KRNet: Towards Efficient Knowledge Replay",
    "abstract": "The knowledge replay technique has been widely used in many tasks such as\ncontinual learning and continuous domain adaptation. The key lies in how to\neffectively encode the knowledge extracted from previous data and replay them\nduring current training procedure. A simple yet effective model to achieve\nknowledge replay is autoencoder. However, the number of stored latent codes in\nautoencoder increases linearly with the scale of data and the trained encoder\nis redundant for the replaying stage. In this paper, we propose a novel and\nefficient knowledge recording network (KRNet) which directly maps an arbitrary\nsample identity number to the corresponding datum. Compared with autoencoder,\nour KRNet requires significantly ($400\\times$) less storage cost for the latent\ncodes and can be trained without the encoder sub-network. Extensive experiments\nvalidate the efficiency of KRNet, and as a showcase, it is successfully applied\nin the task of continual learning.",
    "descriptor": "\nComments: Accepted by ICPR 2022\n",
    "authors": [
      "Yingying Zhang",
      "Qiaoyong Zhong",
      "Di Xie",
      "Shiliang Pu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11126"
  },
  {
    "id": "arXiv:2205.11127",
    "title": "A Survey of Research on Fair Recommender Systems",
    "abstract": "Recommender systems can strongly influence which information we see online,\ne.g, on social media, and thus impact our beliefs, decisions, and actions. At\nthe same time, these systems can create substantial business value for\ndifferent stakeholders. Given the growing potential impact of such AI-based\nsystems on individuals, organizations, and society, questions of fairness have\ngained increased attention in recent years. However, research on fairness in\nrecommender systems is still a developing area. In this survey, we first review\nthe fundamental concepts and notions of fairness that were put forward in the\narea in the recent past. Afterward, we provide a survey of how research in this\narea is currently operationalized, for example, in terms of the general\nresearch methodology, fairness metrics, and algorithmic approaches. Overall,\nour analysis of recent works points to certain research gaps. In particular, we\nfind that in many research works in computer science very abstract problem\noperationalizations are prevalent, which circumvent the fundamental and\nimportant question of what represents a fair recommendation in the context of a\ngiven application.",
    "descriptor": "",
    "authors": [
      "Yashar Deldjoo",
      "Dietmar Jannach",
      "Alejandro Bellogin",
      "Alessandro Diffonzo",
      "Dario Zanzonelli"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11127"
  },
  {
    "id": "arXiv:2205.11131",
    "title": "Heterogeneous Semantic Transfer for Multi-label Recognition with Partial  Labels",
    "abstract": "Multi-label image recognition with partial labels (MLR-PL), in which some\nlabels are known while others are unknown for each image, may greatly reduce\nthe cost of annotation and thus facilitate large-scale MLR. We find that strong\nsemantic correlations exist within each image and across different images, and\nthese correlations can help transfer the knowledge possessed by the known\nlabels to retrieve the unknown labels and thus improve the performance of the\nMLR-PL task (see Figure 1). In this work, we propose a novel heterogeneous\nsemantic transfer (HST) framework that consists of two complementary transfer\nmodules that explore both within-image and cross-image semantic correlations to\ntransfer the knowledge possessed by known labels to generate pseudo labels for\nthe unknown labels. Specifically, an intra-image semantic transfer (IST) module\nlearns an image-specific label co-occurrence matrix for each image and maps the\nknown labels to complement the unknown labels based on these matrices.\nAdditionally, a cross-image transfer (CST) module learns category-specific\nfeature-prototype similarities and then helps complement the unknown labels\nthat have high degrees of similarity with the corresponding prototypes.\nFinally, both the known and generated pseudo labels are used to train MLR\nmodels. Extensive experiments conducted on the Microsoft COCO, Visual Genome,\nand Pascal VOC 2007 datasets show that the proposed HST framework achieves\nsuperior performance to that of current state-of-the-art algorithms.\nSpecifically, it obtains mean average precision (mAP) improvements of 1.4%,\n3.3%, and 0.4% on the three datasets over the results of the best-performing\npreviously developed algorithm.",
    "descriptor": "\nComments: Technical Report. arXiv admin note: text overlap with arXiv:2112.10941\n",
    "authors": [
      "Tianshui Chen",
      "Tao Pu",
      "Lingbo Liu",
      "Yukai Shi",
      "Zhijing Yang",
      "Liang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11131"
  },
  {
    "id": "arXiv:2205.11134",
    "title": "Please, Don't Forget the Difference and the Confidence Interval when  Seeking for the State-of-the-Art Status",
    "abstract": "This paper argues for the widest possible use of bootstrap confidence\nintervals for comparing NLP system performances instead of the state-of-the-art\nstatus (SOTA) and statistical significance testing. Their main benefits are to\ndraw attention to the difference in performance between two systems and to help\nassessing the degree of superiority of one system over another. Two cases\nstudies, one comparing several systems and the other based on a K-fold\ncross-validation procedure, illustrate these benefits. A python module for\nobtaining these confidence intervals as well as a second function implementing\nthe Fisher-Pitman test for paired samples are freely available on PyPi.",
    "descriptor": "\nComments: Accepted at LREC 2022\n",
    "authors": [
      "Yves Bestgen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11134"
  },
  {
    "id": "arXiv:2205.11137",
    "title": "Decentralized Federated Learning Based on Committees and Blockchain",
    "abstract": "Machine learning algorithms are undoubtedly one of the most popular\nalgorithms in recent years, and neural networks have demonstrated unprecedented\nprecision. In daily life, different communities may have different user\ncharacteristics, which also means that training a strong model requires the\nunion of different communities, so the privacy issue needs to be solved\nurgently. Federated learning is a popular privacy solution, each community does\nnot need to expose specific data, but only needs to upload sub-models to the\ncoordination server to train more powerful models. However, federated learning\nalso has some problems, such as the security and fairness of the coordination\nserver. A proven solution to the problem is a decentralized implementation of\nfederated learning. In this paper, we apply decentralized tools such as\nblockchain and consensus algorithms to design a support system that supports\nthe decentralized operation of federated learning in an alliance environment,\ninvolving the exploration of incentives, security, fairness and other issues.\nFinally, we experimentally verify the performance of our system, the effect of\nfederated learning, and the availability of privacy protection.",
    "descriptor": "",
    "authors": [
      "Yang ChaoQun"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.11137"
  },
  {
    "id": "arXiv:2205.11139",
    "title": "GraphAD: A Graph Neural Network for Entity-Wise Multivariate Time-Series  Anomaly Detection",
    "abstract": "In recent years, the emergence and development of third-party platforms have\ngreatly facilitated the growth of the Online to Offline (O2O) business.\nHowever, the large amount of transaction data raises new challenges for\nretailers, especially anomaly detection in operating conditions. Thus,\nplatforms begin to develop intelligent business assistants with embedded\nanomaly detection methods to reduce the management burden on retailers.\nTraditional time-series anomaly detection methods capture underlying patterns\nfrom the perspectives of time and attributes, ignoring the difference between\nretailers in this scenario. Besides, similar transaction patterns extracted by\nthe platforms can also provide guidance to individual retailers and enrich\ntheir available information without privacy issues. In this paper, we pose an\nentity-wise multivariate time-series anomaly detection problem that considers\nthe time-series of each unique entity. To address this challenge, we propose\nGraphAD, a novel multivariate time-series anomaly detection model based on the\ngraph neural network. GraphAD decomposes the Key Performance Indicator (KPI)\ninto stable and volatility components and extracts their patterns in terms of\nattributes, entities and temporal perspectives via graph neural networks. We\nalso construct a real-world entity-wise multivariate time-series dataset from\nthe business data of Ele.me. The experimental results on this dataset show that\nGraphAD significantly outperforms existing anomaly detection methods.",
    "descriptor": "\nComments: SIGIR'22 Short Paper\n",
    "authors": [
      "Xu Chen",
      "Qiu Qiu",
      "Changshan Li",
      "Kunqing Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11139"
  },
  {
    "id": "arXiv:2205.11140",
    "title": "Human-in-the-loop: Provably Efficient Preference-based Reinforcement  Learning with General Function Approximation",
    "abstract": "We study human-in-the-loop reinforcement learning (RL) with trajectory\npreferences, where instead of receiving a numeric reward at each step, the\nagent only receives preferences over trajectory pairs from a human overseer.\nThe goal of the agent is to learn the optimal policy which is most preferred by\nthe human overseer. Despite the empirical successes, the theoretical\nunderstanding of preference-based RL (PbRL) is only limited to the tabular\ncase. In this paper, we propose the first optimistic model-based algorithm for\nPbRL with general function approximation, which estimates the model using\nvalue-targeted regression and calculates the exploratory policies by solving an\noptimistic planning problem. Our algorithm achieves the regret of $\\tilde{O}\n(\\operatorname{poly}(d H) \\sqrt{K} )$, where $d$ is the complexity measure of\nthe transition and preference model depending on the Eluder dimension and\nlog-covering numbers, $H$ is the planning horizon, $K$ is the number of\nepisodes, and $\\tilde O(\\cdot)$ omits logarithmic terms. Our lower bound\nindicates that our algorithm is near-optimal when specialized to the linear\nsetting. Furthermore, we extend the PbRL problem by formulating a novel problem\ncalled RL with $n$-wise comparisons, and provide the first sample-efficient\nalgorithm for this new setting. To the best of our knowledge, this is the first\ntheoretical result for PbRL with (general) function approximation.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Chen",
      "Han Zhong",
      "Zhuoran Yang",
      "Zhaoran Wang",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11140"
  },
  {
    "id": "arXiv:2205.11141",
    "title": "OPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization",
    "abstract": "As Deep Neural Networks (DNNs) usually are overparameterized and have\nmillions of weight parameters, it is challenging to deploy these large DNN\nmodels on resource-constrained hardware platforms, e.g., smartphones. Numerous\nnetwork compression methods such as pruning and quantization are proposed to\nreduce the model size significantly, of which the key is to find suitable\ncompression allocation (e.g., pruning sparsity and quantization codebook) of\neach layer. Existing solutions obtain the compression allocation in an\niterative/manual fashion while finetuning the compressed model, thus suffering\nfrom the efficiency issue. Different from the prior art, we propose a novel\nOne-shot Pruning-Quantization (OPQ) in this paper, which analytically solves\nthe compression allocation with pre-trained weight parameters only. During\nfinetuning, the compression module is fixed and only weight parameters are\nupdated. To our knowledge, OPQ is the first work that reveals pre-trained model\nis sufficient for solving pruning and quantization simultaneously, without any\ncomplex iterative/manual optimization at the finetuning stage. Furthermore, we\npropose a unified channel-wise quantization method that enforces all channels\nof each layer to share a common codebook, which leads to low bit-rate\nallocation without introducing extra overhead brought by traditional\nchannel-wise quantization. Comprehensive experiments on ImageNet with\nAlexNet/MobileNet-V1/ResNet-50 show that our method improves accuracy and\ntraining efficiency while obtains significantly higher compression rates\ncompared to the state-of-the-art.",
    "descriptor": "\nComments: Accepted in AAAI2021 and Just upload to retrieve Arxiv DOI for Project Record\n",
    "authors": [
      "Peng Hu",
      "Xi Peng",
      "Hongyuan Zhu",
      "Mohamed M. Sabry Aly",
      "Jie Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11141"
  },
  {
    "id": "arXiv:2205.11148",
    "title": "Distributed Computations in Fully-Defective Networks",
    "abstract": "We address fully-defective asynchronous networks, in which all links are\nsubject to an unlimited number of alteration errors, implying that all messages\nin the network may be completely corrupted. Despite the possible intuition that\nsuch a setting is too harsh for any reliable communication, we show how to\nsimulate any algorithm for a noiseless setting over any fully-defective\nsetting, given that the network is 2-edge connected. We prove that if the\nnetwork is not 2-edge connected, no non-trivial computation in the\nfully-defective setting is possible.\nThe key structural property of 2-edge-connected graphs that we leverage is\nthe existence of an oriented (non-simple) cycle that goes through all nodes\n[Robbins, 1939]. The core of our technical contribution is presenting a\nconstruction of such a Robbins cycle in fully-defective networks, and showing\nhow to communicate over it despite total message corruption. These are obtained\nin a content-oblivious manner, since nodes must ignore the content of received\nmessages.",
    "descriptor": "",
    "authors": [
      "Keren Censor-Hillel",
      "Shir Cohen",
      "Ran Gelles",
      "Gal Sela"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.11148"
  },
  {
    "id": "arXiv:2205.11149",
    "title": "Mixed finite elements for Bingham flow in a pipe",
    "abstract": "We consider mixed finite element approximations of viscous, plastic Bingham\nflow in a cylindrical pipe. A novel a priori and a posteriori error analysis is\nintroduced which is based on a discrete mesh dependent norm for the normalized\nLagrange multiplier. This allows proving stability for various conforming\nfinite elements. Numerical examples are presented to support the theory and to\ndemonstrate adaptive mesh refinement.",
    "descriptor": "",
    "authors": [
      "Tom Gustafsson",
      "Philip L. Lederer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11149"
  },
  {
    "id": "arXiv:2205.11152",
    "title": "Cross-lingual Lifelong Learning",
    "abstract": "The longstanding goal of multi-lingual learning has been to develop a\nuniversal cross-lingual model that can withstand the changes in multi-lingual\ndata distributions. However, most existing models assume full access to the\ntarget languages in advance, whereas in realistic scenarios this is not often\nthe case, as new languages can be incorporated later on. In this paper, we\npresent the Cross-lingual Lifelong Learning (CLL) challenge, where a model is\ncontinually fine-tuned to adapt to emerging data from different languages. We\nprovide insights into what makes multilingual sequential learning particularly\nchallenging. To surmount such challenges, we benchmark a representative set of\ncross-lingual continual learning algorithms and analyze their knowledge\npreservation, accumulation, and generalization capabilities compared to\nbaselines on carefully curated datastreams. The implications of this analysis\ninclude a recipe for how to measure and balance between different cross-lingual\ncontinual learning desiderata, which goes beyond conventional transfer\nlearning.",
    "descriptor": "",
    "authors": [
      "Meryem M'hamdi",
      "Xiang Ren",
      "Jonathan May"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11152"
  },
  {
    "id": "arXiv:2205.11156",
    "title": "Collaborative Adversarial Training",
    "abstract": "The vulnerability of deep neural networks (DNNs) to adversarial examples has\nattracted great attention in the machine learning community. The problem is\nrelated to local non-smoothness and steepness of normally obtained loss\nlandscapes. Training augmented with adversarial examples (a.k.a., adversarial\ntraining) is considered as an effective remedy. In this paper, we highlight\nthat some collaborative examples, nearly perceptually indistinguishable from\nboth adversarial and benign examples yet show extremely lower prediction loss,\ncan be utilized to enhance adversarial training. A novel method called\ncollaborative adversarial training (CoAT) is thus proposed to achieve new\nstate-of-the-arts.",
    "descriptor": "",
    "authors": [
      "Qizhang Li",
      "Yiwen Guo",
      "Wangmeng Zuo",
      "Hao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11156"
  },
  {
    "id": "arXiv:2205.11158",
    "title": "QEKD: Query-Efficient and Data-Free Knowledge Distillation from  Black-box Models",
    "abstract": "Knowledge distillation (KD) is a typical method for training a lightweight\nstudent model with the help of a well-trained teacher model. However, most KD\nmethods require access to either the teacher's training dataset or model\nparameter, which is unrealistic. To tackle this problem, recent works study KD\nunder data-free and black-box settings. Nevertheless, these works require a\nlarge number of queries to the teacher model, which involves significant\nmonetary and computational costs. To this end, we propose a novel method called\nQuery Efficient Knowledge Distillation (QEKD), which aims to query-efficiently\nlearn from black-box model APIs to train a good student without any real data.\nIn detail, QEKD trains the student model in two stages: data generation and\nmodel distillation. Note that QEKD does not require any query in the data\ngeneration stage and queries the teacher only once for each sample in the\ndistillation stage. Extensive experiments on various real-world datasets show\nthe effectiveness of the proposed QEKD. For instance, QEKD can improve the\nperformance of the best baseline method (DFME) by 5.83 on CIFAR10 dataset with\nonly 0.02x the query budget of DFME.",
    "descriptor": "",
    "authors": [
      "Jie Zhang",
      "Chen Chen",
      "Jiahua Dong",
      "Ruoxi Jia",
      "Lingjuan Lyu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11158"
  },
  {
    "id": "arXiv:2205.11159",
    "title": "RuNNE-2022 Shared Task: Recognizing Nested Named Entities",
    "abstract": "The RuNNE Shared Task approaches the problem of nested named entity\nrecognition. The annotation schema is designed in such a way, that an entity\nmay partially overlap or even be nested into another entity. This way, the\nnamed entity \"The Yermolova Theatre\" of type \"organization\" houses another\nentity \"Yermolova\" of type \"person\". We adopt the Russian NEREL dataset for the\nRuNNE Shared Task. NEREL comprises news texts written in the Russian language\nand collected from the Wikinews portal. The annotation schema includes 29\nentity types. The nestedness of named entities in NEREL reaches up to six\nlevels. The RuNNE Shared Task explores two setups. (i) In the general setup all\nentities occur more or less with the same frequency. (ii) In the few-shot setup\nthe majority of entity types occur often in the training set. However, some of\nthe entity types are have lower frequency, being thus challenging to recognize.\nIn the test set the frequency of all entity types is even.\nThis paper reports on the results of the RuNNE Shared Task. Overall the\nshared task has received 156 submissions from nine teams. Half of the\nsubmissions outperform a straightforward BERT-based baseline in both setups.\nThis paper overviews the shared task setup and discusses the submitted systems,\ndiscovering meaning insights for the problem of nested NER. The links to the\nevaluation platform and the data from the shared task are available in our\ngithub repository: https://github.com/dialogue-evaluation/RuNNE.",
    "descriptor": "\nComments: To appear in Dialogue 2022\n",
    "authors": [
      "Ekaterina Artemova",
      "Maxim Zmeev",
      "Natalia Loukachevitch",
      "Igor Rozhkov",
      "Tatiana Batura",
      "Vladimir Ivanov",
      "Elena Tutubalina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11159"
  },
  {
    "id": "arXiv:2205.11162",
    "title": "A Self-Paced Mixed Distillation Method for Non-Autoregressive Generation",
    "abstract": "Non-Autoregressive generation is a sequence generation paradigm, which\nremoves the dependency between target tokens. It could efficiently reduce the\ntext generation latency with parallel decoding in place of token-by-token\nsequential decoding. However, due to the known multi-modality problem,\nNon-Autoregressive (NAR) models significantly under-perform Auto-regressive\n(AR) models on various language generation tasks. Among the NAR models, BANG is\nthe first large-scale pre-training model on English un-labeled raw text corpus.\nIt considers different generation paradigms as its pre-training tasks including\nAuto-regressive (AR), Non-Autoregressive (NAR), and semi-Non-Autoregressive\n(semi-NAR) information flow with multi-stream strategy. It achieves\nstate-of-the-art performance without any distillation techniques. However, AR\ndistillation has been shown to be a very effective solution for improving NAR\nperformance. In this paper, we propose a novel self-paced mixed distillation\nmethod to further improve the generation quality of BANG. Firstly, we propose\nthe mixed distillation strategy based on the AR stream knowledge. Secondly, we\nencourage the model to focus on the samples with the same modality by\nself-paced learning. The proposed self-paced mixed distillation algorithm\nimproves the generation quality and has no influence on the inference latency.\nWe carry out extensive experiments on summarization and question generation\ntasks to validate the effectiveness. To further illustrate the commercial value\nof our approach, we conduct experiments on three generation tasks in real-world\nadvertisements applications. Experimental results on commercial data show the\neffectiveness of the proposed model. Compared with BANG, it achieves\nsignificant BLEU score improvement. On the other hand, compared with\nauto-regressive generation method, it achieves more than 7x speedup.",
    "descriptor": "",
    "authors": [
      "Weizhen Qi",
      "Yeyun Gong",
      "Yelong Shen",
      "Jian Jiao",
      "Yu Yan",
      "Houqiang Li",
      "Ruofei Zhang",
      "Weizhu Chen",
      "Nan Duan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11162"
  },
  {
    "id": "arXiv:2205.11163",
    "title": "Learning to Advise and Learning from Advice in Cooperative Multi-Agent  Reinforcement Learning",
    "abstract": "Learning to coordinate is a daunting problem in multi-agent reinforcement\nlearning (MARL). Previous works have explored it from many facets, including\ncognition between agents, credit assignment, communication, expert\ndemonstration, etc. However, less attention were paid to agents' decision\nstructure and the hierarchy of coordination. In this paper, we explore the\nspatiotemporal structure of agents' decisions and consider the hierarchy of\ncoordination from the perspective of multilevel emergence dynamics, based on\nwhich a novel approach, Learning to Advise and Learning from Advice (LALA), is\nproposed to improve MARL. Specifically, by distinguishing the hierarchy of\ncoordination, we propose to enhance decision coordination at meso level with an\nadvisor and leverage a policy discriminator to advise agents' learning at micro\nlevel. The advisor learns to aggregate decision information in both spatial and\ntemporal domains and generates coordinated decisions by employing a\nspatiotemporal dual graph convolutional neural network with a task-oriented\nobjective function. Each agent learns from the advice via a policy generative\nadversarial learning method where a discriminator distinguishes between the\npolicies of the agent and the advisor and boosts both of them based on its\njudgement. Experimental results indicate the advantage of LALA over baseline\napproaches in terms of both learning efficiency and coordination capability.\nCoordination mechanism is investigated from the perspective of multilevel\nemergence dynamics and mutual information point of view, which provides a novel\nperspective and method to analyze and improve MARL algorithms.",
    "descriptor": "",
    "authors": [
      "Yue Jin",
      "Shuangqing Wei",
      "Jian Yuan",
      "Xudong Zhang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11163"
  },
  {
    "id": "arXiv:2205.11164",
    "title": "Time-series Transformer Generative Adversarial Networks",
    "abstract": "Many real-world tasks are plagued by limitations on data: in some instances\nvery little data is available and in others, data is protected by privacy\nenforcing regulations (e.g. GDPR). We consider limitations posed specifically\non time-series data and present a model that can generate synthetic time-series\nwhich can be used in place of real data. A model that generates synthetic\ntime-series data has two objectives: 1) to capture the stepwise conditional\ndistribution of real sequences, and 2) to faithfully model the joint\ndistribution of entire real sequences. Autoregressive models trained via\nmaximum likelihood estimation can be used in a system where previous\npredictions are fed back in and used to predict future ones; in such models,\nerrors can accrue over time. Furthermore, a plausible initial value is required\nmaking MLE based models not really generative. Many downstream tasks learn to\nmodel conditional distributions of the time-series, hence, synthetic data drawn\nfrom a generative model must satisfy 1) in addition to performing 2). We\npresent TsT-GAN, a framework that capitalises on the Transformer architecture\nto satisfy the desiderata and compare its performance against five\nstate-of-the-art models on five datasets and show that TsT-GAN achieves higher\npredictive performance on all datasets.",
    "descriptor": "",
    "authors": [
      "Padmanaba Srinivasan",
      "William J. Knottenbelt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11164"
  },
  {
    "id": "arXiv:2205.11166",
    "title": "Prompt Tuning for Discriminative Pre-trained Language Models",
    "abstract": "Recent works have shown promising results of prompt tuning in stimulating\npre-trained language models (PLMs) for natural language processing (NLP) tasks.\nHowever, to the best of our knowledge, existing works focus on prompt-tuning\ngenerative PLMs that are pre-trained to generate target tokens, such as BERT.\nIt is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be\neffectively prompt-tuned. In this work, we present DPT, the first prompt tuning\nframework for discriminative PLMs, which reformulates NLP tasks into a\ndiscriminative language modeling problem. Comprehensive experiments on text\nclassification and question answering show that, compared with vanilla\nfine-tuning, DPT achieves significantly higher performance, and also prevents\nthe unstable problem in tuning large PLMs in both full-set and low-resource\nsettings. The source code and experiment details of this paper can be obtained\nfrom https://github.com/thunlp/DPT.",
    "descriptor": "\nComments: Accepted by Findings of ACL 2022\n",
    "authors": [
      "Yuan Yao",
      "Bowen Dong",
      "Ao Zhang",
      "Zhengyan Zhang",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Leyu Lin",
      "Maosong Sun",
      "Jianyong Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11166"
  },
  {
    "id": "arXiv:2205.11168",
    "title": "Logarithmic regret bounds for continuous-time average-reward Markov  decision processes",
    "abstract": "We consider reinforcement learning for continuous-time Markov decision\nprocesses (MDPs) in the infinite-horizon, average-reward setting. In contrast\nto discrete-time MDPs, a continuous-time process moves to a state and stays\nthere for a random holding time after an action is taken. With unknown\ntransition probabilities and rates of exponential holding times, we derive\ninstance-dependent regret lower bounds that are logarithmic in the time\nhorizon. Moreover, we design a learning algorithm and establish a finite-time\nregret bound that achieves the logarithmic growth rate. Our analysis builds\nupon upper confidence reinforcement learning, a delicate estimation of the mean\nholding times, and stochastic comparison of point processes.",
    "descriptor": "",
    "authors": [
      "Xuefeng Gao",
      "Xun Yu Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11168"
  },
  {
    "id": "arXiv:2205.11169",
    "title": "PEVL: Position-enhanced Pre-training and Prompt Tuning for  Vision-language Models",
    "abstract": "Vision-language pre-training (VLP) has shown impressive performance on a wide\nrange of cross-modal tasks, where VLP models without reliance on object\ndetectors are becoming the mainstream due to their superior computation\nefficiency and competitive performance. However, the removal of object\ndetectors also deprives the capability of VLP models in explicit object\nmodeling, which is essential to various position-sensitive vision-language (VL)\ntasks, such as referring expression comprehension and visual commonsense\nreasoning. To address the challenge, we introduce PEVL that enhances the\npre-training and prompt tuning of VLP models with explicit object position\nmodeling. Specifically, PEVL reformulates discretized object positions and\nlanguage in a unified language modeling framework, which facilitates explicit\nVL alignment during pre-training, and also enables flexible prompt tuning for\nvarious downstream tasks. We show that PEVL enables state-of-the-art\nperformance of detector-free VLP models on position-sensitive tasks such as\nreferring expression comprehension and phrase grounding, and also improves the\nperformance on position-insensitive tasks with grounded inputs. We make the\ndata and code for this paper publicly available at\nhttps://github.com/thunlp/PEVL.",
    "descriptor": "",
    "authors": [
      "Yuan Yao",
      "Qianyu Chen",
      "Ao Zhang",
      "Wei Ji",
      "Zhiyuan Liu",
      "Tat-Seng Chua",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11169"
  },
  {
    "id": "arXiv:2205.11171",
    "title": "Distributed Energy Resources Cybersecurity Outlook: Vulnerabilities,  Attacks, Impacts, and Mitigations",
    "abstract": "The digitalization and decentralization of the electric power grid are key\nthrusts towards an economically and environmentally sustainable future. Towards\nthis goal, distributed energy resources (DER), including rooftop solar panels,\nbattery storage, electric vehicles, etc., are becoming ubiquitous in power\nsystems, effectively replacing fossil-fuel based generation. Power utilities\nbenefit from DERs as they minimize transmission costs, provide voltage support\nthrough ancillary services, and reduce operational risks via their autonomous\noperation. Similarly, DERs grant users and aggregators control over the power\nthey produce and consume. Apart from their sustainability and operational\nobjectives, the cybersecurity of DER-supported power systems is of cardinal\nimportance. DERs are interconnected, interoperable, and support remotely\ncontrollable features, thus, their cybersecurity should be thoroughly\nconsidered. DER communication dependencies and the diversity of DER\narchitectures (e.g., hardware/software components of embedded devices,\ninverters, controllable loads, etc.) widen the threat surface and aggravate the\ncybersecurity posture of power systems. In this work, we focus on security\noversights that reside in the cyber and physical layers of DERs and can\njeopardize grid operations. We analyze adversarial capabilities and objectives\nwhen manipulating DER assets, and then present how protocol and device -level\nvulnerabilities can materialize into cyberattacks impacting power system\noperations. Finally, we provide mitigation strategies to thwart adversaries and\ndirections for future DER cybersecurity.",
    "descriptor": "",
    "authors": [
      "Ioannis Zografopoulos",
      "Charalambos Konstantinou",
      "Nikos D. Hatziargyriou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11171"
  },
  {
    "id": "arXiv:2205.11172",
    "title": "How Powerful are Spectral Graph Neural Networks",
    "abstract": "Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based\non graph signal filters, and some models able to learn arbitrary spectral\nfilters have emerged recently. However, few works analyze the expressive power\nof spectral GNNs. This paper studies spectral GNNs' expressive power\ntheoretically. We first prove that even spectral GNNs without nonlinearity can\nproduce arbitrary graph signals and give two conditions for reaching\nuniversality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2)\nno missing frequency components in node features. We also establish a\nconnection between the expressive power of spectral GNNs and Graph Isomorphism\n(GI) testing which is often used to characterize spatial GNNs' expressive\npower. Moreover, we study the difference in empirical performance among\ndifferent spectral GNNs with the same expressive power from an optimization\nperspective, and motivate the use of an orthogonal basis whose weight function\ncorresponds to the graph signal density in the spectrum. Inspired by the\nanalysis, we propose JacobiConv, which uses Jacobi polynomial basis due to\ntheir orthogonality and flexibility to adapt to a wide range of weight\nfunctions. JacobiConv deserts nonlinearity while outperforming all baselines on\nboth synthetic and real-world datasets.",
    "descriptor": "\nComments: To be published in ICML2022\n",
    "authors": [
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11172"
  },
  {
    "id": "arXiv:2205.11173",
    "title": "Multi-objective Optimization of Clustering-based Scheduling for  Multi-workflow On Clouds Considering Fairness",
    "abstract": "Distributed computing, such as cloud computing, provides promising platforms\nto execute multiple workflows. Workflow scheduling plays an important role in\nmulti-workflow execution with multi-objective requirements. Although there\nexist many multi-objective scheduling algorithms, they focus mainly on\noptimizing makespan and cost for a single workflow. There is a limited research\non multi-objective optimization for multi-workflow scheduling. Considering\nmulti-workflow scheduling, there is an additional key objective to maintain the\nfairness of workflows using the resources. To address such issues, this paper\nfirst defines a new multi-objective optimization model based on makespan, cost,\nand fairness, and then proposes a global clustering-based multi-workflow\nscheduling strategy for resource allocation. Experimental results show that the\nproposed approach performs better than the compared algorithms without\nsignificant compromise of the overall makespan and cost as well as individual\nfairness, which can guide the simulation workflow scheduling on clouds.",
    "descriptor": "",
    "authors": [
      "Feng Li",
      "Wen Jun",
      "Wentong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11173"
  },
  {
    "id": "arXiv:2205.11174",
    "title": "Design and Implementation of a Fuzzy Adaptive Controller for  Time-Varying Formation Leader-Follower Configuration of Nonholonomic Mobile  Robots",
    "abstract": "In this paper, a time-varying leader-follower formation control of\nnonholonomic mobile robots based on a trajectory tracking control strategy is\nconsidered. In the time-varying formation, the relative bearing and distance of\neach follower are variable parameters, and therefore, the followers can carry\nout various and complex behaviour even without changing the linear and angular\nvelocities of the leader robot. After proposing the kinematic model of the\ntime-varying leader-follower formation, the backstepping control method is\nexploited to keep the structure of the defined formation. The global stability\nof the formation is investigated using the Lyapunov theorem. Moreover, the\ndesigned nonlinear controller suffers from the ineffectual large input commands\nat the beginning of the formation. To rectify this problem, a fuzzy adaptive\nalgorithm is proposed to improve the backstepping controller and the global\nstability of the resulting fuzzy adaptive backstepping controller is\nguaranteed. Considering the rate change of relative distance and bearing in the\nkinematic model of the leader-follower formation and controller design\nprocedure, makes the formation more practical in dynamic and clutter\nenvironments, as well as capable of defining complicated behaviour for\nfollowers, and provides crash and obstacle avoidance without switching between\ndifferent control strategies. Finally, the performance of the proposed\nkinematics model and designed controllers are investigated through simulations\nand experimental studies.",
    "descriptor": "",
    "authors": [
      "Payam Nourizadeh",
      "Aghil Yousefi-Koma",
      "Moosa Ayati"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11174"
  },
  {
    "id": "arXiv:2205.11177",
    "title": "Consistency of UML class, object and statechart diagrams using ontology  reasoners",
    "abstract": "We propose an automatic approach to analyze the consistency and\nsatisfiability of Unified Modeling Language UML models containing multiple\nclass, object and statechart diagrams using logic reasoners for the Web\nOntology Language OWL 2. We describe how to translate UML models in OWL 2 and\nwe present a tool chain implementing this translation that can be used with any\nstandard compliant UML modeling tool. The proposed approach is limited in\nscope, but is fully automatic and does not require any expertise about OWL 2\nand its reasoners from the designer.",
    "descriptor": "",
    "authors": [
      "Ali Hanzala Khan",
      "Ivan Porres"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11177"
  },
  {
    "id": "arXiv:2205.11179",
    "title": "Online Hybrid Lightweight Representations Learning: Its Application to  Visual Tracking",
    "abstract": "This paper presents a novel hybrid representation learning framework for\nstreaming data, where an image frame in a video is modeled by an ensemble of\ntwo distinct deep neural networks; one is a low-bit quantized network and the\nother is a lightweight full-precision network. The former learns coarse primary\ninformation with low cost while the latter conveys residual information for\nhigh fidelity to original representations. The proposed parallel architecture\nis effective to maintain complementary information since fixed-point arithmetic\ncan be utilized in the quantized network and the lightweight model provides\nprecise representations given by a compact channel-pruned network. We\nincorporate the hybrid representation technique into an online visual tracking\ntask, where deep neural networks need to handle temporal variations of target\nappearances in real-time. Compared to the state-of-the-art real-time trackers\nbased on conventional deep neural networks, our tracking algorithm demonstrates\ncompetitive accuracy on the standard benchmarks with a small fraction of\ncomputational cost and memory footprint.",
    "descriptor": "\nComments: 7 pages, 1 figure, accepted at IJCAI2022\n",
    "authors": [
      "Ilchae Jung",
      "Minji Kim",
      "Eunhyeok Park",
      "Bohyung Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11179"
  },
  {
    "id": "arXiv:2205.11180",
    "title": "Distributed Downlink Precoding and Equalization in Satellite Swarms",
    "abstract": "In this paper, we propose a novel approach for downlink transmission from a\nsatellite swarm towards a ground station (GS). These swarms have the benefit of\nmuch higher spatial separation in the transmit antennas than traditional big\nsatellites with antenna arrays, promising a massive increase in spectral\nefficiency. The resulting precoder and equalizer have very low demands on\ncomputational complexity, inter-satellite coordination and channel estimation.\nThis is achieved by taking knowledge about the geometry between satellites and\nGS into account. For precoding, each satellite only requires its angle of\ndeparture (AoD) towards the GS and it turns out that almost optimal data rates\ncan be achieved if the satellites transmit independent data streams. For the\nequalizer, the GS requires only knowledge about the angles of arrival (AoAs)\nfrom all satellites. Based on the underlying geometrical channel approximation,\nthe optimal inter-satellite distance is obtained analytically. We show, that,\nby choosing a proper inter-satellite distance, the proposed low-complexity\napproach achieves the theoretical upper bound in terms of data rate.\nFurthermore, a novel approach to increase the robustness of the proposed\nprecoder and equalizer against imperfect AoD and AoA knowledge is proposed, by\nexploiting the statistics of the estimation error.",
    "descriptor": "\nComments: 30 pages, 10 figures, submitted to IEEE Transactions on Wireless Communications. arXiv admin note: text overlap with arXiv:2112.08791\n",
    "authors": [
      "Maik R\u00f6per",
      "Bho Matthiesen",
      "Dirk W\u00fcbben",
      "Petar Popovski",
      "Armin Dekorsy"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.11180"
  },
  {
    "id": "arXiv:2205.11181",
    "title": "Lotaru: Locally Estimating Runtimes of Scientific Workflow Tasks in  Heterogeneous Clusters",
    "abstract": "Many scientific workflow scheduling algorithms need to be informed about task\nruntimes a-priori to conduct efficient scheduling. In heterogeneous cluster\ninfrastructures, this problem becomes aggravated because these runtimes are\nrequired for each task-node pair. Using historical data is often not feasible\nas logs are typically not retained indefinitely and workloads as well as\ninfrastructure changes. In contrast, online methods, which predict task\nruntimes on specific nodes while the workflow is running, have to cope with the\nlack of example runs, especially during the start-up.\nIn this paper, we present Lotaru, a novel online method for locally\nestimating task runtimes in scientific workflows on heterogeneous clusters.\nLotaru first profiles all nodes of a cluster with a set of short-running and\nuniform microbenchmarks. Next, it runs the workflow to be scheduled on the\nuser's local machine with drastically reduced data to determine important task\ncharacteristics. Based on these measurements, Lotaru learns a Bayesian linear\nregression model to predict a task's runtime given the input size and finally\nadjusts the predicted runtime specifically for each task-node pair in the\ncluster based on the micro-benchmark results. Due to its Bayesian approach,\nLotaru can also compute robust uncertainty estimates and provides them as an\ninput for advanced scheduling methods.\nOur evaluation with five real-world scientific workflows and different\ndatasets shows that Lotaru significantly outperforms the baselines in terms of\nprediction errors for homogeneous and heterogeneous clusters.",
    "descriptor": "\nComments: paper accepted in 34th International Conference on Scientific and Statistical Database Management (SSDBM 2022)\n",
    "authors": [
      "Jonathan Bader",
      "Fabian Lehmann",
      "Lauritz Thamsen",
      "Jonathan Will",
      "Ulf Leser",
      "Odej Kao"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.11181"
  },
  {
    "id": "arXiv:2205.11182",
    "title": "A note on a stable algorithm for computing the fractional integrals of  orthogonal polynomials",
    "abstract": "In this note we provide an algorithm for computing the fractional integrals\nof orthogonal polynomials, which is more stable than that using the expression\nof the polynomials w.r.t. the canonical basis. This algorithm is aimed at\nsolving corresponding fractional differential equations. A few numerical\nexamples are reported.",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "P. Amodio",
      "L. Brugnano",
      "F. Iavernaro"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11182"
  },
  {
    "id": "arXiv:2205.11184",
    "title": "An Evaluation Study of Intrinsic Motivation Techniques applied to  Reinforcement Learning over Hard Exploration Environments",
    "abstract": "In the last few years, the research activity around reinforcement learning\ntasks formulated over environments with sparse rewards has been especially\nnotable. Among the numerous approaches proposed to deal with these hard\nexploration problems, intrinsic motivation mechanisms are arguably among the\nmost studied alternatives to date. Advances reported in this area over time\nhave tackled the exploration issue by proposing new algorithmic ideas to\ngenerate alternative mechanisms to measure the novelty. However, most efforts\nin this direction have overlooked the influence of different design choices and\nparameter settings that have also been introduced to improve the effect of the\ngenerated intrinsic bonus, forgetting the application of those choices to other\nintrinsic motivation techniques that may also benefit of them. Furthermore,\nsome of those intrinsic methods are applied with different base reinforcement\nalgorithms (e.g. PPO, IMPALA) and neural network architectures, being hard to\nfairly compare the provided results and the actual progress provided by each\nsolution. The goal of this work is to stress on this crucial matter in\nreinforcement learning over hard exploration environments, exposing the\nvariability and susceptibility of avant-garde intrinsic motivation techniques\nto diverse design factors. Ultimately, our experiments herein reported\nunderscore the importance of a careful selection of these design aspects\ncoupled with the exploration requirements of the environment and the task in\nquestion under the same setup, so that fair comparisons can be guaranteed.",
    "descriptor": "\nComments: 20 pages, 5 figures, 4 tables\n",
    "authors": [
      "Alain Andres",
      "Esther Villar-Rodriguez",
      "Javier Del Ser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11184"
  },
  {
    "id": "arXiv:2205.11186",
    "title": "Self-Adaptive RISs Beyond Free Space: Convergence of Localization,  Sensing and Communication under Rich-Scattering Conditions",
    "abstract": "We discuss the need for a confluence of localization, sensing and\ncommunications if RISs are to be deployed in a self-adaptive manner in the\ndynamically evolving rich-scattering settings that are typical for 6G\ndeployment scenarios such as factories. We establish that in such problems the\nrich-scattering wireless channels are subject to a highly nonlinear\ndeterministic double-parametrization through both the RIS and uncontrolled\nmoving objects. Therefore, acquiring full context-awareness through\nlocalization and sensing is a prerequisite for RIS-empowered communications.\nYet, the byproducts of this daunting communications overhead can feed many\nappliances that require context awareness, such that overhead concerns may\nvanish. We illustrate the essential steps for operating a self-adaptive RIS\nunder rich scattering based on a prototypical case study. We discover that\nself-adaptive RISs outperform context-ignorant RISs only below a certain noise\nthreshold that depends, among other factors, on how strongly uncontrolled\nperturbers impact the wireless channel. We also discuss ensuing future research\ndirections that will determine the conditions under which RISs may serve as\ntechnological enabler of 6G networks.",
    "descriptor": "\nComments: 7 pages, 5 figures, submitted to an IEEE Journal\n",
    "authors": [
      "Chlo\u00e9 Saigre-Tardif",
      "Philipp del Hougne"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.11186"
  },
  {
    "id": "arXiv:2205.11191",
    "title": "NPU-BOLT: A Dataset for Bolt Object Detection in Natural Scene Images",
    "abstract": "Bolt joints are very common and important in engineering structures. Due to\nextreme service environment and load factors, bolts often get loose or even\ndisengaged. To real-time or timely detect the loosed or disengaged bolts is an\nurgent need in practical engineering, which is critical to keep structural\nsafety and service life. In recent years, many bolt loosening detection methods\nusing deep learning and machine learning techniques have been proposed and are\nattracting more and more attention. However, most of these studies use bolt\nimages captured in laboratory for deep leaning model training. The images are\nobtained in a well-controlled light, distance, and view angle conditions. Also,\nthe bolted structures are well designed experimental structures with brand new\nbolts and the bolts are exposed without any shelter nearby. It is noted that in\npractical engineering, the above well controlled lab conditions are not easy\nrealized and the real bolt images often have blur edges, oblique perspective,\npartial occlusion and indistinguishable colors etc., which make the trained\nmodels obtained in laboratory conditions loss their accuracy or fails.\nTherefore, the aim of this study is to develop a dataset named NPU-BOLT for\nbolt object detection in natural scene images and open it to researchers for\npublic use and further development. In the first version of the dataset, it\ncontains 337 samples of bolt joints images mainly in the natural environment,\nwith image data sizes ranging from 400*400 to 6000*4000, totaling approximately\n1275 bolt targets. The bolt targets are annotated into four categories named\nblur bolt, bolt head, bolt nut and bolt side. The dataset is tested with\nadvanced object detection models including yolov5, Faster-RCNN and CenterNet.\nThe effectiveness of the dataset is validated.",
    "descriptor": "",
    "authors": [
      "Yadian Zhao",
      "Zhenglin Yang",
      "Chao Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11191"
  },
  {
    "id": "arXiv:2205.11192",
    "title": "Active Domain Adaptation with Multi-level Contrastive Units for Semantic  Segmentation",
    "abstract": "To further reduce the cost of semi-supervised domain adaptation (SSDA)\nlabeling, a more effective way is to use active learning (AL) to annotate a\nselected subset with specific properties. However, domain adaptation tasks are\nalways addressed in two interactive aspects: domain transfer and the\nenhancement of discrimination, which requires the selected data to be both\nuncertain under the model and diverse in feature space. Contrary to active\nlearning in classification tasks, it is usually challenging to select pixels\nthat contain both the above properties in segmentation tasks, leading to the\ncomplex design of pixel selection strategy. To address such an issue, we\npropose a novel Active Domain Adaptation scheme with Multi-level Contrastive\nUnits (ADA-MCU) for semantic image segmentation. A simple pixel selection\nstrategy followed with the construction of multi-level contrastive units is\nintroduced to optimize the model for both domain adaptation and active\nsupervised learning. In practice, MCUs are constructed from intra-image,\ncross-image, and cross-domain levels by using both labeled and unlabeled\npixels. At each level, we define contrastive losses from center-to-center and\npixel-to-pixel manners, with the aim of jointly aligning the category centers\nand reducing outliers near the decision boundaries. In addition, we also\nintroduce a categories correlation matrix to implicitly describe the\nrelationship between categories, which are used to adjust the weights of the\nlosses for MCUs. Extensive experimental results on standard benchmarks show\nthat the proposed method achieves competitive performance against\nstate-of-the-art SSDA methods with 50% fewer labeled pixels and significantly\noutperforms state-of-the-art with a large margin by using the same level of\nannotation cost.",
    "descriptor": "",
    "authors": [
      "Hao Zhang",
      "Ruimao Zhang",
      "Zhanglin Peng",
      "Junle Wang",
      "Yanqing Jing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11192"
  },
  {
    "id": "arXiv:2205.11194",
    "title": "UnifieR: A Unified Retriever for Large-Scale Retrieval",
    "abstract": "Large-scale retrieval is to recall relevant documents from a huge collection\ngiven a query. It relies on representation learning to embed documents and\nqueries into a common semantic encoding space. According to the encoding space,\nrecent retrieval methods based on pre-trained language models (PLM) can be\ncoarsely categorized into either dense-vector or lexicon-based paradigms. These\ntwo paradigms unveil the PLMs' representation capability in different\ngranularities, i.e., global sequence-level compression and local word-level\ncontexts, respectively. Inspired by their complementary global-local\ncontextualization and distinct representing views, we propose a new learning\nframework, UnifieR, which unifies dense-vector and lexicon-based retrieval in\none model with a dual-representing capability. Experiments on passage retrieval\nbenchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme\nis further presented with even better retrieval quality. We lastly evaluate the\nmodel on BEIR benchmark to verify its transferability.",
    "descriptor": "\nComments: 20 pages, 6 figures, 11 tables\n",
    "authors": [
      "Tao Shen",
      "Xiubo Geng",
      "Chongyang Tao",
      "Can Xu",
      "Kai Zhang",
      "Daxin Jiang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11194"
  },
  {
    "id": "arXiv:2205.11195",
    "title": "Deep Image Retrieval is not Robust to Label Noise",
    "abstract": "Large-scale datasets are essential for the success of deep learning in image\nretrieval. However, manual assessment errors and semi-supervised annotation\ntechniques can lead to label noise even in popular datasets. As previous works\nprimarily studied annotation quality in image classification tasks, it is still\nunclear how label noise affects deep learning approaches to image retrieval. In\nthis work, we show that image retrieval methods are less robust to label noise\nthan image classification ones. Furthermore, we, for the first time,\ninvestigate different types of label noise specific to image retrieval tasks\nand study their effect on model performance.",
    "descriptor": "",
    "authors": [
      "Stanislav Dereka",
      "Ivan Karpukhin",
      "Sergey Kolesnikov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11195"
  },
  {
    "id": "arXiv:2205.11196",
    "title": "Zero-Sum Games and Linear Programming Duality",
    "abstract": "The minimax theorem for zero-sum games is easily proved from the strong\nduality theorem of linear programming. For the converse direction, the standard\nproof by Dantzig (1951) is massively incomplete, as we argue in this article.\nWe explain and combine classical theorems about solving linear equations with\nnonnegative variables to give a correct alternative proof.",
    "descriptor": "",
    "authors": [
      "Bernhard von Stengel"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11196"
  },
  {
    "id": "arXiv:2205.11197",
    "title": "Feature-Distribution Perturbation and Calibration for Generalized Person  ReID",
    "abstract": "Person Re-identification (ReID) has been advanced remarkably over the last 10\nyears along with the rapid development of deep learning for visual recognition.\nHowever, the i.i.d. (independent and identically distributed) assumption\ncommonly held in most deep learning models is somewhat non-applicable to ReID\nconsidering its objective to identify images of the same pedestrian across\ncameras at different locations often of variable and independent domain\ncharacteristics that are also subject to view-biased data distribution. In this\nwork, we propose a Feature-Distribution Perturbation and Calibration (PECA)\nmethod to derive generic feature representations for person ReID, which is not\nonly discriminative across cameras but also agnostic and deployable to\narbitrary unseen target domains. Specifically, we perform per-domain\nfeature-distribution perturbation to refrain the model from overfitting to the\ndomain-biased distribution of each source (seen) domain by enforcing feature\ninvariance to distribution shifts caused by perturbation. Furthermore, we\ndesign a global calibration mechanism to align feature distributions across all\nthe source domains to improve the model generalization capacity by eliminating\ndomain bias. These local perturbation and global calibration are conducted\nsimultaneously, which share the same principle to avoid models overfitting by\nregularization respectively on the perturbed and the original distributions.\nExtensive experiments were conducted on eight person ReID datasets and the\nproposed PECA model outperformed the state-of-the-art competitors by\nsignificant margins.",
    "descriptor": "",
    "authors": [
      "Qilei Li",
      "Jiabo Huang",
      "Jian Hu",
      "Shaogang Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11197"
  },
  {
    "id": "arXiv:2205.11200",
    "title": "BBTv2: Pure Black-Box Optimization Can Be Comparable to Gradient Descent  for Few-Shot Learning",
    "abstract": "Black-Box Tuning (BBT) is a derivative-free approach to optimize continuous\nprompt tokens prepended to the input of language models. Although BBT has\nachieved comparable performance to full model tuning on simple classification\ntasks under few-shot settings, it requires pre-trained prompt embedding to\nmatch model tuning on hard tasks (e.g., entailment tasks), and therefore does\nnot completely get rid of the dependence on gradients. In this paper we present\nBBTv2, a pure black-box optimization approach that can drive language models to\nachieve comparable results to gradient-based optimization. In particular, we\nprepend continuous prompt tokens to every layer of the language model and\npropose a divide-and-conquer algorithm to alternately optimize the prompt\ntokens at different layers. For the optimization at each layer, we perform\nderivative-free optimization in a low-dimensional subspace, which is then\nrandomly projected to the original prompt parameter space. Experimental results\nshow that BBTv2 not only outperforms BBT by a large margin, but also achieves\ncomparable or even better performance than full model tuning and\nstate-of-the-art parameter-efficient methods (e.g., Adapter, LoRA, BitFit,\netc.) under few-shot learning settings, while maintaining much fewer tunable\nparameters.",
    "descriptor": "\nComments: Work in progress. Code is publicly available at this https URL\n",
    "authors": [
      "Tianxiang Sun",
      "Zhengfu He",
      "Hong Qian",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11200"
  },
  {
    "id": "arXiv:2205.11202",
    "title": "Denoising-based image reconstruction from pixels located at non-integer  positions",
    "abstract": "Digital images are commonly represented as regular 2D arrays, so pixels are\norganized in form of a matrix addressed by integers. However, there are many\nimage processing operations, such as rotation or motion compensation, that\nproduce pixels at non-integer positions. Typically, image reconstruction\ntechniques cannot handle samples at non-integer positions. In this paper, we\npropose to use triangulation-based reconstruction as initial estimate that is\nlater refined by a novel adaptive denoising framework. Simulations reveal that\nimprovements of up to more than 1.8 dB (in terms of PSNR) are achieved with\nrespect to the initial estimate.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.10138\n",
    "authors": [
      "J\u00e1n Koloda",
      "J\u00fcrgen Seiler",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.11202"
  },
  {
    "id": "arXiv:2205.11206",
    "title": "Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning",
    "abstract": "There is a growing interest in improving the conversational ability of models\nby filtering the raw dialogue corpora. Previous filtering strategies usually\nrely on a scoring method to assess and discard samples from one perspective,\nenabling the model to enhance the corresponding dialogue attributes (e.g.,\nconsistency) more easily. However, the discarded samples may obtain high scores\nin other perspectives and can provide regularization effects on the model\nlearning, which causes the performance improvement to be sensitive to the\nfiltering ratio. In this work, we propose a multi-view attribute-enhanced\ndialogue learning framework that strengthens the attribute-related features\nmore robustly and comprehensively. Instead of filtering the raw dataset to\ntrain the model, our framework first pre-trains the model on the raw dataset\nand then fine-tunes it through adapters on the selected sub-sets, which also\nenhances certain attributes of responses but without suffering from the\nproblems mentioned above. Considering the variety of the dialogue attribute, we\nfurther design a multi-view enhancement mechanism, including multi-view\nselection and inter-view fusion. It groups the high-quality samples from\nmultiple perspectives, respectively, and enhances different attributes of\nresponses with the corresponding sample sets and adapters, keeping knowledge\nindependent and allowing flexible integration. Empirical results and analysis\nshow that our framework can improve the performance significantly in terms of\nenhancing dialogue attributes and fusing view-specific knowledge.",
    "descriptor": "",
    "authors": [
      "Yiwei Li",
      "Bin Sun",
      "Shaoxiong Feng",
      "Kan Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11206"
  },
  {
    "id": "arXiv:2205.11211",
    "title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation",
    "abstract": "End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.",
    "descriptor": "\nComments: work in progress\n",
    "authors": [
      "Yichao Du",
      "Weizhi Wang",
      "Zhirui Zhang",
      "Boxing Chen",
      "Tong Xu",
      "Jun Xie",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11211"
  },
  {
    "id": "arXiv:2205.11212",
    "title": "CircleChain: Tokenizing Products with a Role-based Scheme for a Circular  Economy",
    "abstract": "In a circular economy, tracking the flow of second-life components for\nquality control is critical. Tokenization can enhance the transparency of the\nflow of second-life components. However, simple tokenization does not\ncorrespond to real economic models and lacks the ability to finely manage\ncomplex business processes. In particular, existing systems have to take into\naccount the different roles of the parties in the supply chain. Based on the\nAlgorand blockchain, we propose a role-based token management scheme, which can\nachieve authentication, synthesis, circulation, and reuse of these second-life\ncomponents in a trustless environment. The proposed scheme not only achieves\nfine-grained and scalable second-life component management, but also enables\non-chain trading, subsidies, and green-bond issuance. Furthermore, we\nimplemented and performed scalability tests for the proposed architecture on\nAlgorand blockchain using its smart contracts and Algorand Standard Assets\n(ASA). The open-source implementation, tests, along with results are available\non our Github page.",
    "descriptor": "",
    "authors": [
      "Mojtaba Eshghie",
      "Li Quan",
      "Gustav Andersson Kasche",
      "Filip Jacobson",
      "Cosimo Bassi",
      "Cyrille Artho"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.11212"
  },
  {
    "id": "arXiv:2205.11215",
    "title": "Document Intelligence Metrics for Visually Rich Document Evaluation",
    "abstract": "The processing of Visually-Rich Documents (VRDs) is highly important in\ninformation extraction tasks associated with Document Intelligence. We\nintroduce DI-Metrics, a Python library devoted to VRD model evaluation\ncomprising text-based, geometric-based and hierarchical metrics for information\nextraction tasks. We apply DI-Metrics to evaluate information extraction\nperformance using publicly available CORD dataset, comparing performance of\nthree SOTA models and one industry model. The open-source library is available\non GitHub.",
    "descriptor": "\nComments: Accepted to DAS 2022, 15TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS\n",
    "authors": [
      "Jonathan DeGange",
      "Swapnil Gupta",
      "Zhuoyu Han",
      "Krzysztof Wilkosz",
      "Adam Karwan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11215"
  },
  {
    "id": "arXiv:2205.11219",
    "title": "Higher-order causal theories are models of BV-logic",
    "abstract": "The Caus[-] construction takes a compact closed category of basic processes\nand yields a *-autonomous category of higher-order processes obeying certain\nsignalling/causality constraints, as dictated by the type system in the\nresulting category. This paper looks at instances where the base category C\nsatisfies additional properties yielding an affine-linear structure on Caus[C]\nand a substantially richer internal logic. While the original construction only\ngave multiplicative linear logic, here we additionally obtain additives and a\nnon-commutative, self-dual sequential product yielding a model of Guglielmi's\nBV logic. Furthermore, we obtain a natural interpretation for the sequential\nproduct as \"A can signal to B, but not vice-versa\", which sits as expected\nbetween the non-signalling tensor and the fully-signalling (i.e. unconstrained)\npar. Fixing matrices of positive numbers for C recovers the BV category\nstructure of probabilistic coherence spaces identified by Blute, Panangaden,\nand Slavnov, restricted to normalised maps. On the other hand, fixing the\ncategory of completely positive maps gives an entirely new model of BV\nconsisting of higher order quantum channels, encompassing recent work in the\nstudy of quantum and indefinite causal structures.",
    "descriptor": "",
    "authors": [
      "Will Simmons",
      "Aleks Kissinger"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.11219"
  },
  {
    "id": "arXiv:2205.11224",
    "title": "Around View Monitoring System for Hydraulic Excavators",
    "abstract": "This paper describes the Around View Monitoring (AVM) system for hydraulic\nexcavators that prevents the safety accidents caused by blind spots and\nincreases the operational efficiency. To verify the developed system,\nexperiments were conducted with its prototype. The experimental results\ndemonstrate its applicability in the field with the following values: 7m of a\nvisual range, 15fps of image refresh rate, 300ms of working information data\nreception rate, and 300ms of surface condition data reception rate.",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Dong Jun Yeom",
      "Yu Na Hong",
      "Yoojun Kim",
      "Hyun Seok Yoo",
      "Youngsuk Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.11224"
  },
  {
    "id": "arXiv:2205.11225",
    "title": "Using Wordle for Learning to Design and Compare Strategies",
    "abstract": "Wordle is a very popular word game that is owned by the New York Times. We\ncan design parameterized strategies for solving Wordle, based on probabilistic,\nstatistical, and information-theoretical information about the games. The\nstrategies can handle a reasonably large family of Wordle-like games both\nsystematically and dynamically, meaning that we do not rely on precomputations\nthat may work for non-fixed games. More specifically, the answer set can be\narbitrary, not confining to the current 2315 words. The answer words may\ninclude any specific number of letters (does not have to be five), and the set\nof symbols that form the words does not have to be limited to only the English\nalphabet.\nExploring possible strategies for solving the Wordle-like games offers an\nattractive learning challenges for students who are learning to design computer\ngames. This paper will provide the results of using two families of\nparameterized strategies to solve the current Wordle, using the simulator that\nabides by the hard-mode rules as the baseline. The baseline simulator used an\naverage of 4.078 guesses to find the 2315 answers, and needed more than six\ntrials to solve the game 1.77% of the time. The best performing strategy of\nours used an average of 3.674 guesses to find the 2315 answers, and failed\n0.65% of the time.",
    "descriptor": "\nComments: 2022 IEEE International Conference on Games; 8 pages, 4 figures, 10 tables, 12 equations\n",
    "authors": [
      "Chao-Lin Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11225"
  },
  {
    "id": "arXiv:2205.11226",
    "title": "Scalable Kernel-Based Minimum Mean Square Error Estimator for  Accelerated Image Error Concealment",
    "abstract": "Error concealment is of great importance for block-based video systems, such\nas DVB or video streaming services. In this paper, we propose a novel scalable\nspatial error concealment algorithm that aims at obtaining high quality\nreconstructions with reduced computational burden. The proposed technique\nexploits the excellent reconstructing abilities of the kernel-based minimum\nmean square error K-MMSE estimator. We propose to decompose this approach into\na set of hierarchically stacked layers. The first layer performs the basic\nreconstruction that the subsequent layers can eventually refine. In addition,\nwe design a layer management mechanism, based on profiles, that dynamically\nadapts the use of higher layers to the visual complexity of the area being\nreconstructed. The proposed technique outperforms other state-of-the-art\nalgorithms and produces high quality reconstructions, equivalent to K-MMSE,\nwhile requiring around one tenth of its computational time.",
    "descriptor": "",
    "authors": [
      "J\u00e1n Koloda",
      "J\u00fcrgen Seiler",
      "Antonio M. Peinado",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11226"
  },
  {
    "id": "arXiv:2205.11229",
    "title": "Exploration of the possibility of infusing Social Media Trends into  generating NFT Recommendations",
    "abstract": "Recommendations Systems have been identified to be one of the integral\nelements of driving sales in e-commerce sites. The utilization of opinion\nmining data extracted from trends has been attempted to improve the\nrecommendations that can be provided by baseline methods in this research when\nuser-click data is lacking or is difficult to be collected due to privacy\nconcerns.\nUtilizing social trends to influence the recommendations generated for a set\nof unique items has been explored with the use of a suggested scoring\nmechanism. Embracing concepts from decentralized networks that are expected to\nchange how users interact via the internet over the next couple of decades, the\nsuggested Recommendations System attempts to make use of multiple sources of\ninformation, applying coherent information retrieval techniques to extract\nprobable trending items.\nThe proposed Recommendations Architecture in the research presents a method\nto integrate social trends with recommendations to produce promising outputs.",
    "descriptor": "",
    "authors": [
      "Dinuka Ravijaya Piyadigama",
      "Guhanathan Poravi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.11229"
  },
  {
    "id": "arXiv:2205.11230",
    "title": "Geocentric Pose Analysis of Satellite Imagery Using Deep Learning",
    "abstract": "Roughly 6,800 natural disasters occur worldwide annually, and this alarming\nnumber continues to grow due to the effects of climate change. Effective\nmethods to improve natural disaster response include performing change\ndetection, map alignment, and vision-aided navigation to allow for the\ntime-efficient delivery of life-saving aid. Current software functions\noptimally only on nadir images taken ninety degrees above ground level. The\ninability to generalize to oblique images increases the need to compute an\nimage's geocentric pose, which is its spatial orientation with respect to\ngravity. This Deep Learning investigation presents three convolutional models\nto predict geocentric pose using 5,923 nadir and oblique RGB satellite images\nof cities worldwide. The first model is an autoencoder that condenses the 256 x\n256 x 3 images to 32 x 32 x 16 latent space representations, demonstrating the\nability to learn useful features from the data. The second model is a U-Net\nFully Convolutional Network with skip connections used to predict each image's\ncorresponding pixel-level elevation mask. This model achieves a median absolute\ndeviation of 0.335 meters and an R2 of 0.865 on test data. Afterward, the\nelevation masks are concatenated with the RGB images to form 256 x 256 x 4\ninputs of the third model, which predicts each image's rotation angle and\nscale, the components of its geocentric pose. This Deep Convolutional Neural\nNetwork achieves an R2 of 0.904 on test data, significantly outperforming\nprevious models designed by researchers. The high-accuracy software built in\nthis study contributes to crucial procedures that can accelerate disaster\nrelief and save human lives.",
    "descriptor": "",
    "authors": [
      "Christopher Sun",
      "Jai Sharma",
      "Milind Maiti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11230"
  },
  {
    "id": "arXiv:2205.11231",
    "title": "SUGER: A Subgraph-based Graph Convolutional Network Method for Bundle  Recommendation",
    "abstract": "Bundle recommendation is an emerging research direction in the recommender\nsystem with the focus on recommending customized bundles of items for users.\nAlthough Graph Neural Networks (GNNs) have been applied in this problem and\nachieve superior performance, existing methods underexplore the graph-level GNN\nmethods, which exhibit great potential in traditional recommender system.\nFurthermore, they usually lack the transferability from one domain with\nsufficient supervision to another domain which might suffer from the label\nscarcity issue. In this work, we propose a subgraph-based Graph Neural Network\nmodel, SUGER, for bundle recommendation to handle these limitations. SUGER\ngenerates heterogeneous subgraphs around the user-bundle pairs, and then maps\nthose subgraphs to the users' preference predictions via neural relational\ngraph propagation. Experimental results show that SUGER significantly\noutperforms the state-of-the-art baselines in both the basic and the transfer\nbundle recommendation problems.",
    "descriptor": "",
    "authors": [
      "Zhenning Zhang",
      "Boxin Du",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11231"
  },
  {
    "id": "arXiv:2205.11232",
    "title": "Deep Neural Network approaches for Analysing Videos of Music  Performances",
    "abstract": "This paper presents a framework to automate the labelling process for\ngestures in musical performance videos with a 3D Convolutional Neural Network\n(CNN). While this idea was proposed in a previous study, this paper introduces\nseveral novelties: (i) Presents a novel method to overcome the class imbalance\nchallenge and make learning possible for co-existent gestures by batch\nbalancing approach and spatial-temporal representations of gestures. (ii)\nPerforms a detailed study on 7 and 18 categories of gestures generated during\nthe performance (guitar play) of musical pieces that have been video-recorded.\n(iii) Investigates the possibility to use audio features. (iv) Extends the\nanalysis to multiple videos. The novel methods significantly improve the\nperformance of gesture identification by 12 %, when compared to the previous\nwork (51 % in this study over 39 % in previous work). We successfully validate\nthe proposed methods on 7 super classes (72 %), an ensemble of the 18\ngestures/classes, and additional videos (75 %).",
    "descriptor": "",
    "authors": [
      "Foteini Simistira Liwicki",
      "Richa Upadhya",
      "Prakash Chandra Chhipa",
      "Killian Murphy",
      "Federico Visi",
      "Stefan \u00d6stersj\u00f6",
      "Marcus Liwicki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.11232"
  },
  {
    "id": "arXiv:2205.11233",
    "title": "Poincar\u00e9 Heterogeneous Graph Neural Networks for Sequential  Recommendation",
    "abstract": "Sequential recommendation (SR) learns users' preferences by capturing the\nsequential patterns from users' behaviors evolution. As discussed in many\nworks, user-item interactions of SR generally present the intrinsic power-law\ndistribution, which can be ascended to hierarchy-like structures. Previous\nmethods usually handle such hierarchical information by making user-item\nsectionalization empirically under Euclidean space, which may cause distortion\nof user-item representation in real online scenarios. In this paper, we propose\na Poincar\\'{e}-based heterogeneous graph neural network named PHGR to model the\nsequential pattern information as well as hierarchical information contained in\nthe data of SR scenarios simultaneously. Specifically, for the purpose of\nexplicitly capturing the hierarchical information, we first construct a\nweighted user-item heterogeneous graph by aliening all the user-item\ninteractions to improve the perception domain of each user from a global view.\nThen the output of the global representation would be used to complement the\nlocal directed item-item homogeneous graph convolution. By defining a novel\nhyperbolic inner product operator, the global and local graph representation\nlearning are directly conducted in Poincar\\'{e} ball instead of commonly used\nprojection operation between Poincar\\'{e} ball and Euclidean space, which could\nalleviate the cumulative error issue of general bidirectional translation\nprocess. Moreover, for the purpose of explicitly capturing the sequential\ndependency information, we design two types of temporal attention operations\nunder Poincar\\'{e} ball space. Empirical evaluations on datasets from the\npublic and financial industry show that PHGR outperforms several comparison\nmethods.",
    "descriptor": "\nComments: 32 pages, 12 figuews\n",
    "authors": [
      "Naicheng Guo",
      "Xiaolei Liu",
      "Shaoshuai Li",
      "Qiongxu Ma",
      "Kaixin Gao",
      "Bing Han",
      "Lin Zheng",
      "Xiaobo Guo"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11233"
  },
  {
    "id": "arXiv:2205.11234",
    "title": "DagSim: Combining DAG-based model structure with unconstrained data  types and relations for flexible, transparent, and modularized data  simulation",
    "abstract": "Data simulation is fundamental for machine learning and causal inference, as\nit allows exploration of scenarios and assessment of methods in settings with\nfull control of ground truth. Directed acyclic graphs (DAGs) are well\nestablished for encoding the dependence structure over a collection of\nvariables in both inference and simulation settings. However, while modern\nmachine learning is applied to data of an increasingly complex nature,\nDAG-based simulation frameworks are still confined to settings with relatively\nsimple variable types and functional forms. We here present DagSim, a\nPython-based framework for DAG-based data simulation without any constraints on\nvariable types or functional relations. A succinct YAML format for defining the\nsimulation model structure promotes transparency, while separate user-provided\nfunctions for generating each variable based on its parents ensure simulation\ncode modularization. We illustrate the capabilities of DagSim through use cases\nwhere metadata variables control shapes in an image and patterns in\nbio-sequences.",
    "descriptor": "\nComments: 12 pages, 1 figure, 1 table\n",
    "authors": [
      "Ghadi S. Al Hajj",
      "Johan Pensar",
      "Geir Kjetil Sandve"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11234"
  },
  {
    "id": "arXiv:2205.11236",
    "title": "2-d signature of images and texture classification",
    "abstract": "We introduce a proper notion of 2-dimensional signature for images. This\nobject is inspired by the so-called rough paths theory, and it captures many\nessential features of a 2-dimensional object such as an image. It thus serves\nas a low-dimensional feature for pattern classification. Here we implement a\nsimple procedure for texture classification. In this context, we show that a\nlow dimensional set of features based on signatures produces an excellent\naccuracy.",
    "descriptor": "",
    "authors": [
      "Sheng Zhang",
      "Guang Lin",
      "Samy Tindel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11236"
  },
  {
    "id": "arXiv:2205.11237",
    "title": "Hyperspectral Image Classification With Contrastive Graph Convolutional  Network",
    "abstract": "Recently, Graph Convolutional Network (GCN) has been widely used in\nHyperspectral Image (HSI) classification due to its satisfactory performance.\nHowever, the number of labeled pixels is very limited in HSI, and thus the\navailable supervision information is usually insufficient, which will\ninevitably degrade the representation ability of most existing GCN-based\nmethods. To enhance the feature representation ability, in this paper, a GCN\nmodel with contrastive learning is proposed to explore the supervision signals\ncontained in both spectral information and spatial relations, which is termed\nContrastive Graph Convolutional Network (ConGCN), for HSI classification.\nFirst, in order to mine sufficient supervision signals from spectral\ninformation, a semi-supervised contrastive loss function is utilized to\nmaximize the agreement between different views of the same node or the nodes\nfrom the same land cover category. Second, to extract the precious yet implicit\nspatial relations in HSI, a graph generative loss function is leveraged to\nexplore supplementary supervision signals contained in the graph topology. In\naddition, an adaptive graph augmentation technique is designed to flexibly\nincorporate the spectral-spatial priors of HSI, which helps facilitate the\nsubsequent contrastive representation learning. The extensive experimental\nresults on four typical benchmark datasets firmly demonstrate the effectiveness\nof the proposed ConGCN in both qualitative and quantitative aspects.",
    "descriptor": "",
    "authors": [
      "Wentao Yu",
      "Sheng Wan",
      "Guangyu Li",
      "Jian Yang",
      "Chen Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11237"
  },
  {
    "id": "arXiv:2205.11239",
    "title": "Transformer in Computer Vision: ViT and its Progress",
    "abstract": "Transformer, an attention-based encoder-decoder architecture, has not only\nrevolutionized the field of natural language processing (NLP), but has also\ndone some pioneering work in the field of computer vision (CV). Compared to\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\nexcellent modeling capabilities to achieve very good performance on several\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\nself-attention mechanism in natural language processing, where word embeddings\nare replaced with patch embeddings.\nThis paper reviews the progress in the field of ViT and the\ncross-applications of ViT with other fields.",
    "descriptor": "",
    "authors": [
      "Zujun Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11239"
  },
  {
    "id": "arXiv:2205.11240",
    "title": "A Novel Face-Anti Spoofing Neural Network Model For Face Recognition And  Detection",
    "abstract": "Face Recognition (FR) systems are being used in a variety of applications,\nincluding road crossings, banking, and mobile banking. The widespread use of FR\nsystems has raised concerns about the safety of face biometrics against\nspoofing attacks, which use the use of a photo or video of a legitimate user's\nface to gain illegal access to the resources or activities. Despite the\ndevelopment of several FAS or liveness detection methods (which determine\nwhether a face is live or spoofed at the time of acquisition), the problem\nremains unsolved due to the difficulty of identifying discrimination and\noperationally reasonably priced spoof characteristics but also approaches.\nAdditionally, certain facial portions are frequently repeated or correlate to\nimage clutter, resulting in poor performance overall. This research proposes a\nface-anti-spoofing neural network model that outperforms existing models and\nhas an efficiency of 0.89 percent.",
    "descriptor": "\nComments: 9 Pages\n",
    "authors": [
      "Soham S. Sarpotdar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11240"
  },
  {
    "id": "arXiv:2205.11242",
    "title": "Fusing Multiscale Texture and Residual Descriptors for Multilevel 2D  Barcode Rebroadcasting Detection",
    "abstract": "Nowadays, 2D barcodes have been widely used for advertisement, mobile\npayment, and product authentication. However, in applications related to\nproduct authentication, an authentic 2D barcode can be illegally copied and\nattached to a counterfeited product in such a way to bypass the authentication\nscheme. In this paper, we employ a proprietary 2D barcode pattern and use\nmultimedia forensics methods to analyse the scanning and printing artefacts\nresulting from the copy (rebroadcasting) attack. A diverse and complementary\nfeature set is proposed to quantify the barcode texture distortions introduced\nduring the illegal copying process. The proposed features are composed of\nglobal and local descriptors, which characterize the multi-scale texture\nappearance and the points of interest distribution, respectively. The proposed\ndescriptors are compared against some existing texture descriptors and deep\nlearning-based approaches under various scenarios, such as cross-datasets and\ncross-size. Experimental results highlight the practicality of the proposed\nmethod in real-world settings.",
    "descriptor": "",
    "authors": [
      "Anselmo Ferreira",
      "Changcheng Chen",
      "Mauro Barni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11242"
  },
  {
    "id": "arXiv:2205.11244",
    "title": "A Silicon Photonic Accelerator for Convolutional Neural Networks with  Heterogeneous Quantization",
    "abstract": "Parameter quantization in convolutional neural networks (CNNs) can help\ngenerate efficient models with lower memory footprint and computational\ncomplexity. But, homogeneous quantization can result in significant degradation\nof CNN model accuracy. In contrast, heterogeneous quantization represents a\npromising approach to realize compact, quantized models with higher inference\naccuracies. In this paper, we propose HQNNA, a CNN accelerator based on\nnon-coherent silicon photonics that can accelerate both homogeneously quantized\nand heterogeneously quantized CNN models. Our analyses show that HQNNA achieves\nup to 73.8x better energy-per-bit and 159.5x better throughput-energy\nefficiency than state-of-the-art photonic CNN accelerators.",
    "descriptor": "",
    "authors": [
      "Febin Sunny",
      "Mahdi Nikdast",
      "Sudeep Pasricha"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11244"
  },
  {
    "id": "arXiv:2205.11245",
    "title": "PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for  Multi-stage Ranking",
    "abstract": "This paper describes the PASH participation in TREC 2021 Deep Learning Track.\nIn the recall stage, we adopt a scheme combining sparse and dense retrieval\nmethod. In the multi-stage ranking phase, point-wise and pair-wise ranking\nstrategies are used one after another based on model continual pre-trained on\ngeneral knowledge and document-level data. Compared to TREC 2020 Deep Learning\nTrack, we have additionally introduced the generative model T5 to further\nenhance the performance.",
    "descriptor": "\nComments: TREC 2021\n",
    "authors": [
      "Yixuan Qiao",
      "Hao Chen",
      "Yongquan Lai",
      "Jun Wang",
      "Tuozhen Liu",
      "Xianbin Ye",
      "Rui Fang",
      "Peng Gao",
      "Guotong Xie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11245"
  },
  {
    "id": "arXiv:2205.11246",
    "title": "[Re] Distilling Knowledge via Knowledge Review",
    "abstract": "This effort aims to reproduce the results of experiments and analyze the\nrobustness of the review framework for knowledge distillation introduced in the\nCVPR '21 paper 'Distilling Knowledge via Knowledge Review' by Chen et al.\nPrevious works in knowledge distillation only studied connections paths between\nthe same levels of the student and the teacher, and cross-level connection\npaths had not been considered. Chen et al. propose a new residual learning\nframework to train a single student layer using multiple teacher layers. They\nalso design a novel fusion module to condense feature maps across levels and a\nloss function to compare feature information stored across different levels to\nimprove performance. In this work, we consistently verify the improvements in\ntest accuracy across student models as reported in the original paper and study\nthe effectiveness of the novel modules introduced by conducting ablation\nstudies and new experiments.",
    "descriptor": "\nComments: This is a reproducibility effort based on the CVPR '21 paper 'Distilling Knowledge via Knowledge Review' by Chen et al\n",
    "authors": [
      "Apoorva Verma",
      "Pranjal Gulati",
      "Sarthak Gupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11246"
  },
  {
    "id": "arXiv:2205.11248",
    "title": "Efficient Mixed Dimension Embeddings for Matrix Factorization",
    "abstract": "Despite the prominence of neural network approaches in the field of\nrecommender systems, simple methods such as matrix factorization with quadratic\nloss are still used in industry for several reasons. These models can be\ntrained with alternating least squares, which makes them easy to implement in a\nmassively parallel manner, thus making it possible to utilize billions of\nevents from real-world datasets. Large-scale recommender systems need to\naccount for severe popularity skew in the distributions of users and items, so\na lot of research is focused on implementing sparse, mixed dimension or shared\nembeddings to reduce both the number of parameters and overfitting on rare\nusers and items. In this paper we propose two matrix factorization models with\nmixed dimension embeddings, which can be optimized in a massively parallel\nfashion using the alternating least squares approach.",
    "descriptor": "\nComments: 12 pages, 3 figures, submitted to RecSys 2022\n",
    "authors": [
      "Dmitrii Beloborodov",
      "Andrei Zimovnov",
      "Petr Molodyk",
      "Dmitrii Kirillov"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11248"
  },
  {
    "id": "arXiv:2205.11249",
    "title": "Topic Segmentation of Research Article Collections",
    "abstract": "Collections of research article data harvested from the web have become\ncommon recently since they are important resources for experimenting on tasks\nsuch as named entity recognition, text summarization, or keyword generation. In\nfact, certain types of experiments require collections that are both large and\ntopically structured, with records assigned to separate research disciplines.\nUnfortunately, the current collections of publicly available research articles\nare either small or heterogeneous and unstructured. In this work, we perform\ntopic segmentation of a paper data collection that we crawled and produce a\nmultitopic dataset of roughly seven million paper data records. We construct a\ntaxonomy of topics extracted from the data records and then annotate each\ndocument with its corresponding topic from that taxonomy. As a result, it is\npossible to use this newly proposed dataset in two modalities: as a\nheterogeneous collection of documents from various disciplines or as a set of\nhomogeneous collections, each from a single research topic.",
    "descriptor": "\nComments: 7 pages, 4 tables\n",
    "authors": [
      "Erion \u00c7ano",
      "Benjamin Roth"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11249"
  },
  {
    "id": "arXiv:2205.11252",
    "title": "Exploring the stimulative effect on following drivers in a consecutive  lane-change using microscopic vehicle trajectory data",
    "abstract": "Improper lane-changing behaviors may result in breakdown of traffic flow and\nthe occurrence of various types of collisions. This study investigates\nlane-changing behaviors of multiple vehicles and the stimulative effect on\nfollowing drivers in a consecutive lane-changing scenario. The microscopic\ntrajectory data from the dataset are used for driving behavior analysis.Two\ndiscretionary lane-changing vehicle groups constitute a consecutive\nlane-changing scenario, and not only distance- and speed-related factors but\nalso driving behaviors are taken into account to examine the impacts on the\nutility of following lane-changing vehicles.A random parameters logit model is\ndeveloped to capture the driver psychological heterogeneity in the consecutive\nlane-changing situation.Furthermore, a lane-changing utility prediction model\nis established based on three supervised learning algorithms to detect the\nimproper lane-changing decision. Results indicate that (1) the consecutive\nlane-changing behaviors have a significant negative effect on the following\nlane-changing vehicles after lane-change; (2) the stimulative effect exists in\na consecutive lane-change situation and its influence is heterogeneous due to\ndifferent psychological activities of drivers; and (3) the utility prediction\nmodel can be used to detect an improper lane-changing decision.",
    "descriptor": "\nComments: 22 PAGES\n",
    "authors": [
      "Ruifeng Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.11252"
  },
  {
    "id": "arXiv:2205.11255",
    "title": "A Template-based Method for Constrained Neural Machine Translation",
    "abstract": "Machine translation systems are expected to cope with various types of\nconstraints in many practical scenarios. While neural machine translation (NMT)\nhas achieved strong performance in unconstrained cases, it is non-trivial to\nimpose pre-specified constraints into the translation process of NMT models.\nAlthough many approaches have been proposed to address this issue, most\nexisting methods can not satisfy the following three desiderata at the same\ntime: (1) high translation quality, (2) high match accuracy, and (3) low\nlatency. In this work, we propose a template-based method that can yield\nresults with high translation quality and match accuracy while keeping the\ndecoding speed. Our basic idea is to rearrange the generation of constrained\nand unconstrained tokens through a template. The generation and derivation of\nthe template can be learned through one sequence-to-sequence training\nframework. Thus our method does not require any changes in the model\narchitecture and the decoding algorithm, making it easy to apply. Experimental\nresults show that the proposed template-based methods can outperform several\nrepresentative baselines in lexically and structurally constrained translation\ntasks.",
    "descriptor": "\nComments: 14 pages, 4 figures\n",
    "authors": [
      "Shuo Wang",
      "Peng Li",
      "Zhixing Tan",
      "Zhaopeng Tu",
      "Maosong Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11255"
  },
  {
    "id": "arXiv:2205.11257",
    "title": "Manifold-aligned Neighbor Embedding",
    "abstract": "In this paper, we introduce a neighbor embedding framework for manifold\nalignment. We demonstrate the efficacy of the framework using a\nmanifold-aligned version of the uniform manifold approximation and projection\nalgorithm. We show that our algorithm can learn an aligned manifold that is\nvisually competitive to embedding of the whole dataset.",
    "descriptor": "\nComments: Accepted at the ICLR 2022 Workshop on Geometrical and Topological Representation Learning\n",
    "authors": [
      "Mohammad Tariqul Islam",
      "Jason W. Fleischer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11257"
  },
  {
    "id": "arXiv:2205.11258",
    "title": "Neuro-Symbolic Regex Synthesis Framework via Neural Example Splitting",
    "abstract": "Due to the practical importance of regular expressions (regexes, for short),\nthere has been a lot of research to automatically generate regexes from\npositive and negative string examples. We tackle the problem of learning\nregexes faster from positive and negative strings by relying on a novel\napproach called `neural example splitting'. Our approach essentially split up\neach example string into multiple parts using a neural network trained to group\nsimilar substrings from positive strings. This helps to learn a regex faster\nand, thus, more accurately since we now learn from several short-length\nstrings. We propose an effective regex synthesis framework called `SplitRegex'\nthat synthesizes subregexes from `split' positive substrings and produces the\nfinal regex by concatenating the synthesized subregexes. For the negative\nsample, we exploit pre-generated subregexes during the subregex synthesis\nprocess and perform the matching against negative strings. Then the final regex\nbecomes consistent with all negative strings. SplitRegex is a\ndivided-and-conquer framework for learning target regexes; split (=divide)\npositive strings and infer partial regexes for multiple parts, which is much\nmore accurate than the whole string inferring, and concatenate (=conquer)\ninferred regexes while satisfying negative strings. We empirically demonstrate\nthat the proposed SplitRegex framework substantially improves the previous\nregex synthesis approaches over four benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Su-Hyeon Kim",
      "Hyunjoon Cheon",
      "Yo-Sub Han",
      "Sang-Ki Ko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.11258"
  },
  {
    "id": "arXiv:2205.11261",
    "title": "An Elastic Ephemeral Datastore using Cheap, Transient Cloud Resources",
    "abstract": "Spot instances are virtual machines offered at 60-90% lower cost that can be\nreclaimed at any time, with only a short warning period. Spot instances have\nalready been used to significantly reduce the cost of processing workloads in\nthe cloud. However, leveraging spot instances to reduce the cost of stateful\ncloud applications is much more challenging, as the sudden preemptions lead to\ndata loss. In this work, we propose leveraging spot instances to decrease the\ncost of ephemeral data management in distributed data analytics applications.\nWe specifically target ephemeral data as this large class of data in modern\nanalytics workloads has low durability requirements; if lost, the data can be\nregenerated by re-executing compute tasks. We design an elastic, distributed\nephemeral datastore that handles node preemptions transparently to user\napplications and minimizes data loss by redistributing data during node\npreemption warning periods. We implement our elastic datastore on top of the\nApache Crail datastore and evaluate the system with various workloads and VM\ntypes. By leveraging spot instances, we show that we can run TPC-DS queries\nwith 60\\% lower cost compared to using on-demand VMs for the datastore, while\nonly increasing end-to-end execution time by 2.1%.",
    "descriptor": "",
    "authors": [
      "Malte Brodmann",
      "Nikolas Ioannou",
      "Bernard Metzler",
      "Jonas Pfefferle",
      "Ana Klimovic"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.11261"
  },
  {
    "id": "arXiv:2205.11264",
    "title": "Adaptive Fairness-Aware Online Meta-Learning for Changing Environments",
    "abstract": "The fairness-aware online learning framework has arisen as a powerful tool\nfor the continual lifelong learning setting. The goal for the learner is to\nsequentially learn new tasks where they come one after another over time and\nthe learner ensures the statistic parity of the new coming task across\ndifferent protected sub-populations (e.g. race and gender). A major drawback of\nexisting methods is that they make heavy use of the i.i.d assumption for data\nand hence provide static regret analysis for the framework. However, low static\nregret cannot imply a good performance in changing environments where tasks are\nsampled from heterogeneous distributions. To address the fairness-aware online\nlearning problem in changing environments, in this paper, we first construct a\nnovel regret metric FairSAR by adding long-term fairness constraints onto a\nstrongly adapted loss regret. Furthermore, to determine a good model parameter\nat each round, we propose a novel adaptive fairness-aware online meta-learning\nalgorithm, namely FairSAOML, which is able to adapt to changing environments in\nboth bias control and model precision. The problem is formulated in the form of\na bi-level convex-concave optimization with respect to the model's primal and\ndual parameters that are associated with the model's accuracy and fairness,\nrespectively. The theoretic analysis provides sub-linear upper bounds for both\nloss regret and violation of cumulative fairness constraints. Our experimental\nevaluation on different real-world datasets with settings of changing\nenvironments suggests that the proposed FairSAOML significantly outperforms\nalternatives based on the best prior online learning approaches.",
    "descriptor": "\nComments: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2022. arXiv admin note: text overlap with arXiv:2108.09435\n",
    "authors": [
      "Chen Zhao",
      "Feng Mi",
      "Xintao Wu",
      "Kai Jiang",
      "Latifur Khan",
      "Feng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11264"
  },
  {
    "id": "arXiv:2205.11266",
    "title": "What You See is What You Classify: Black Box Attributions",
    "abstract": "An important step towards explaining deep image classifiers lies in the\nidentification of image regions that contribute to individual class scores in\nthe model's output. However, doing this accurately is a difficult task due to\nthe black-box nature of such networks. Most existing approaches find such\nattributions either using activations and gradients or by repeatedly perturbing\nthe input. We instead address this challenge by training a second deep network,\nthe Explainer, to predict attributions for a pre-trained black-box classifier,\nthe Explanandum. These attributions are in the form of masks that only show the\nclassifier-relevant parts of an image, masking out the rest. Our approach\nproduces sharper and more boundary-precise masks when compared to the saliency\nmaps generated by other methods. Moreover, unlike most existing approaches,\nours is capable of directly generating very distinct class-specific masks.\nFinally, the proposed method is very efficient for inference since it only\ntakes a single forward pass through the Explainer to generate all\nclass-specific masks. We show that our attributions are superior to established\nmethods both visually and quantitatively, by evaluating them on the PASCAL\nVOC-2007 and Microsoft COCO-2014 datasets.",
    "descriptor": "",
    "authors": [
      "Steven Stalder",
      "Nathana\u00ebl Perraudin",
      "Radhakrishna Achanta",
      "Fernando Perez-Cruz",
      "Michele Volpi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11266"
  },
  {
    "id": "arXiv:2205.11267",
    "title": "Fed-DART and FACT: A solution for Federated Learning in a production  environment",
    "abstract": "Federated Learning as a decentralized artificial intelligence (AI) solution\nsolves a variety of problems in industrial applications. It enables a\ncontinuously self-improving AI, which can be deployed everywhere at the edge.\nHowever, bringing AI to production for generating a real business impact is a\nchallenging task. Especially in the case of Federated Learning, expertise and\nresources from multiple domains are required to realize its full potential.\nHaving this in mind we have developed an innovative Federated Learning\nframework FACT based on Fed-DART, enabling an easy and scalable deployment,\nhelping the user to fully leverage the potential of their private and\ndecentralized data.",
    "descriptor": "",
    "authors": [
      "Nico Weber",
      "Patrick Holzer",
      "Tania Jacob",
      "Enislay Ramentol"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11267"
  },
  {
    "id": "arXiv:2205.11269",
    "title": "Dynamic Split Computing for Efficient Deep Edge Intelligence",
    "abstract": "Deploying deep neural networks (DNNs) on IoT and mobile devices is a\nchallenging task due to their limited computational resources. Thus, demanding\ntasks are often entirely offloaded to edge servers which can accelerate\ninference, however, it also causes communication cost and evokes privacy\nconcerns. In addition, this approach leaves the computational capacity of end\ndevices unused. Split computing is a paradigm where a DNN is split into two\nsections; the first section is executed on the end device, and the output is\ntransmitted to the edge server where the final section is executed. Here, we\nintroduce dynamic split computing, where the optimal split location is\ndynamically selected based on the state of the communication channel. By using\nnatural bottlenecks that already exist in modern DNN architectures, dynamic\nsplit computing avoids retraining and hyperparameter optimization, and does not\nhave any negative impact on the final accuracy of DNNs. Through extensive\nexperiments, we show that dynamic split computing achieves faster inference in\nedge computing environments where the data rate and server load vary over time.",
    "descriptor": "",
    "authors": [
      "Arian Bakhtiarnia",
      "Nemanja Milo\u0161evi\u0107",
      "Qi Zhang",
      "Dragana Bajovi\u0107",
      "Alexandros Iosifidis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11269"
  },
  {
    "id": "arXiv:2205.11273",
    "title": "GR-GAN: Gradual Refinement Text-to-image Generation",
    "abstract": "A good Text-to-Image model should not only generate high quality images, but\nalso ensure the consistency between the text and the generated image. Previous\nmodels failed to simultaneously fix both sides well. This paper proposes a\nGradual Refinement Generative Adversarial Network (GR-GAN) to alleviates the\nproblem efficiently. A GRG module is designed to generate images from low\nresolution to high resolution with the corresponding text constraints from\ncoarse granularity (sentence) to fine granularity (word) stage by stage, a ITM\nmodule is designed to provide image-text matching losses at both sentence-image\nlevel and word-region level for corresponding stages. We also introduce a new\nmetric Cross-Model Distance (CMD) for simultaneously evaluating image quality\nand image-text consistency. Experimental results show GR-GAN significant\noutperform previous models, and achieve new state-of-the-art on both FID and\nCMD. A detailed analysis demonstrates the efficiency of different generation\nstages in GR-GAN.",
    "descriptor": "\nComments: Accepted by ICME 2022\n",
    "authors": [
      "Bo Yang",
      "Fangxiang Feng",
      "Xiaojie Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11273"
  },
  {
    "id": "arXiv:2205.11275",
    "title": "RL with KL penalties is better viewed as Bayesian inference",
    "abstract": "Reinforcement learning (RL) is frequently employed in fine-tuning large\nlanguage models (LMs), such as GPT-3, to penalize them for undesirable features\nof generated sequences, such as offensiveness, social bias, harmfulness or\nfalsehood. The RL formulation involves treating the LM as a policy and updating\nit to maximise the expected value of a reward function which captures human\npreferences, such as non-offensiveness. In this paper, we analyze challenges\nassociated with treating a language model as an RL policy and show how avoiding\nthose challenges requires moving beyond the RL paradigm. We start by observing\nthat the standard RL approach is flawed as an objective for fine-tuning LMs\nbecause it leads to distribution collapse: turning the LM into a degenerate\ndistribution. Then, we analyze KL-regularised RL, a widely used recipe for\nfine-tuning LMs, which additionally constrains the fine-tuned LM to stay close\nto its original distribution in terms of Kullback-Leibler (KL) divergence. We\nshow that KL-regularised RL is equivalent to variational inference:\napproximating a Bayesian posterior which specifies how to update a prior LM to\nconform with evidence provided by the reward function. We argue that this\nBayesian inference view of KL-regularised RL is more insightful than the\ntypically employed RL perspective. The Bayesian inference view explains how\nKL-regularised RL avoids the distribution collapse problem and offers a\nfirst-principles derivation for its objective. While this objective happens to\nbe equivalent to RL (with a particular choice of parametric reward), there\nexist other objectives for fine-tuning LMs which are no longer equivalent to\nRL. That observation leads to a more general point: RL is not an adequate\nformal framework for problems such as fine-tuning language models. These\nproblems are best viewed as Bayesian inference: approximating a pre-defined\ntarget distribution.",
    "descriptor": "\nComments: Accepted fo \"RL as a Model of Agency\" workshop @ RLDM 2022\n",
    "authors": [
      "Tomasz Korbak",
      "Ethan Perez",
      "Christopher L Buckley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11275"
  },
  {
    "id": "arXiv:2205.11276",
    "title": "Memory-enriched computation and learning in spiking neural networks  through Hebbian plasticity",
    "abstract": "Memory is a key component of biological neural systems that enables the\nretention of information over a huge range of temporal scales, ranging from\nhundreds of milliseconds up to years. While Hebbian plasticity is believed to\nplay a pivotal role in biological memory, it has so far been analyzed mostly in\nthe context of pattern completion and unsupervised learning. Here, we propose\nthat Hebbian plasticity is fundamental for computations in biological neural\nsystems. We introduce a novel spiking neural network architecture that is\nenriched by Hebbian synaptic plasticity. We show that Hebbian enrichment\nrenders spiking neural networks surprisingly versatile in terms of their\ncomputational as well as learning capabilities. It improves their abilities for\nout-of-distribution generalization, one-shot learning, cross-modal generative\nassociation, language processing, and reward-based learning. As spiking neural\nnetworks are the basis for energy-efficient neuromorphic hardware, this also\nsuggests that powerful cognitive neuromorphic systems can be build based on\nthis principle.",
    "descriptor": "",
    "authors": [
      "Thomas Limbacher",
      "Ozan \u00d6zdenizci",
      "Robert Legenstein"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.11276"
  },
  {
    "id": "arXiv:2205.11277",
    "title": "When does Parameter-Efficient Transfer Learning Work for Machine  Translation?",
    "abstract": "Parameter-efficient fine-tuning methods (PEFTs) offer the promise of adapting\nlarge pre-trained models while only tuning a small number of parameters. They\nhave been shown to be competitive with full model fine-tuning for many\ndownstream tasks. However, prior work indicates that PEFTs may not work as well\nfor machine translation (MT), and there is no comprehensive study showing when\nPEFTs work for MT. We conduct a comprehensive empirical study of PEFTs for MT,\nconsidering (1) various parameter budgets, (2) a diverse set of language-pairs,\nand (3) different pre-trained models. We find that 'adapters', in which small\nfeed-forward networks are added after every layer, are indeed on par with full\nmodel fine-tuning when the parameter budget corresponds to 10% of total model\nparameters. Nevertheless, as the number of tuned parameters decreases, the\nperformance of PEFTs decreases. The magnitude of this decrease depends on the\nlanguage pair, with PEFTs particularly struggling for distantly related\nlanguage-pairs. We find that using PEFTs with a larger pre-trained model\noutperforms full fine-tuning with a smaller model, and for smaller training\ndata sizes, PEFTs outperform full fine-tuning for the same pre-trained model.",
    "descriptor": "",
    "authors": [
      "Ahmet \u00dcst\u00fcn",
      "Asa Cooper Stickland"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11277"
  },
  {
    "id": "arXiv:2205.11279",
    "title": "Tyger: Task-Type-Generic Active Learning for Molecular Property  Prediction",
    "abstract": "How to accurately predict the properties of molecules is an essential problem\nin AI-driven drug discovery, which generally requires a large amount of\nannotation for training deep learning models. Annotating molecules, however, is\nquite costly because it requires lab experiments conducted by experts. To\nreduce annotation cost, deep Active Learning (AL) methods are developed to\nselect only the most representative and informative data for annotating.\nHowever, existing best deep AL methods are mostly developed for a single type\nof learning task (e.g., single-label classification), and hence may not perform\nwell in molecular property prediction that involves various task types. In this\npaper, we propose a Task-type-generic active learning framework (termed Tyger)\nthat is able to handle different types of learning tasks in a unified manner.\nThe key is to learn a chemically-meaningful embedding space and perform active\nselection fully based on the embeddings, instead of relying on\ntask-type-specific heuristics (e.g., class-wise prediction probability) as done\nin existing works. Specifically, for learning the embedding space, we\ninstantiate a querying module that learns to translate molecule graphs into\ncorresponding SMILES strings. Furthermore, to ensure that samples selected from\nthe space are both representative and informative, we propose to shape the\nembedding space by two learning objectives, one based on domain knowledge and\nthe other leveraging feedback from the task learner (i.e., model that performs\nthe learning task at hand). We conduct extensive experiments on benchmark\ndatasets of different task types. Experimental results show that Tyger\nconsistently achieves high AL performance on molecular property prediction,\noutperforming baselines by a large margin. We also perform ablative experiments\nto verify the effectiveness of each component in Tyger.",
    "descriptor": "",
    "authors": [
      "Kuangqi Zhou",
      "Kaixin Wang",
      "Jiashi Feng",
      "Jian Tang",
      "Tingyang Xu",
      "Xinchao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.11279"
  },
  {
    "id": "arXiv:2205.11283",
    "title": "SelfReformer: Self-Refined Network with Transformer for Salient Object  Detection",
    "abstract": "The global and local contexts significantly contribute to the integrity of\npredictions in Salient Object Detection (SOD). Unfortunately, existing methods\nstill struggle to generate complete predictions with fine details. There are\ntwo major problems in conventional approaches: first, for global context,\nhigh-level CNN-based encoder features cannot effectively catch long-range\ndependencies, resulting in incomplete predictions. Second, downsampling the\nground truth to fit the size of predictions will introduce inaccuracy as the\nground truth details are lost during interpolation or pooling. Thus, in this\nwork, we developed a Transformer-based network and framed a supervised task for\na branch to learn the global context information explicitly. Besides, we adopt\nPixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the\nsize of ground truth instead of the reverse. Thus details in the ground truth\nare untouched. In addition, we developed a two-stage Context Refinement Module\n(CRM) to fuse global context and automatically locate and refine the local\ndetails in the predictions. The proposed network can guide and correct itself\nbased on the global and local context generated, thus is named, Self-Refined\nTransformer (SelfReformer). Extensive experiments and evaluation results on\nfive benchmark datasets demonstrate the outstanding performance of the network,\nand we achieved the state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Yi Ke Yun",
      "Weisi Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11283"
  },
  {
    "id": "arXiv:2205.11291",
    "title": "Cooperative Reinforcement Learning on Traffic Signal Control",
    "abstract": "Traffic signal control is a challenging real-world problem aiming to minimize\noverall travel time by coordinating vehicle movements at road intersections.\nExisting traffic signal control systems in use still rely heavily on\noversimplified information and rule-based methods. Specifically, the\nperiodicity of green/red light alternations can be considered as a prior for\nbetter planning of each agent in policy optimization. To better learn such\nadaptive and predictive priors, traditional\nRL-based methods can only return a fixed length from predefined action pool\nwith only local agents. If there is no cooperation between these agents, some\nagents often make conflicts to other agents and thus decrease the whole\nthroughput. This paper proposes a cooperative, multi-objective architecture\nwith age-decaying weights to better estimate multiple reward terms for traffic\nsignal control optimization, which termed COoperative Multi-Objective\nMulti-Agent Deep Deterministic Policy Gradient (COMMA-DDPG). Two types of\nagents running to maximize rewards of different goals - one for local traffic\noptimization at each intersection and the other for global traffic waiting time\noptimization. The global agent is used to guide the local agents as a means for\naiding faster learning but not used in the inference phase. We also provide an\nanalysis of solution existence together with convergence proof for the proposed\nRL optimization. Evaluation is performed using real-world traffic data\ncollected using traffic cameras from an Asian country. Our method can\neffectively reduce the total delayed time by 60\\%. Results demonstrate its\nsuperiority when compared to SoTA methods.",
    "descriptor": "",
    "authors": [
      "Chi-Chun Chao",
      "Jun-Wei Hsieh",
      "Bor-Shiun Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11291"
  },
  {
    "id": "arXiv:2205.11294",
    "title": "Constraint Energy Minimizing Generalized Multiscale Finite Element  Method for multi-continuum Richards equations",
    "abstract": "In fluid flow simulation, the multi-continuum model is a useful strategy.\nWhen the heterogeneity and contrast of coefficients are high, the system\nbecomes multiscale, and some kinds of reduced-order methods are demanded.\nCombining these techniques with nonlinearity, we will consider in this paper a\ndual-continuum model which is generalized as a multi-continuum model for a\ncoupled system of nonlinear Richards equations as unsaturated flows, in complex\nheterogeneous fractured porous media; and we will solve it by a novel\nmultiscale approach utilizing the constraint energy minimizing generalized\nmultiscale finite element method (CEM-GMsFEM). In particular, such a nonlinear\nsystem will be discretized in time and then linearized by Picard iteration\n(whose global convergence is proved theoretically). Subsequently, we tackle the\nresulting linearized equations by the CEM-GMsFEM and obtain proper offline\nmultiscale basis functions to span the multiscale space (which contains the\npressure solution). More specifically, we first introduce two new sources of\nsamples, and the GMsFEM is used over each coarse block to build local auxiliary\nmultiscale basis functions via solving local spectral problems, that are\ncrucial for detecting high-contrast channels. Second, per oversampled coarse\nregion, local multiscale basis functions are created through the CEM as\nconstrainedly minimizing an energy functional. Various numerical tests for our\napproach reveal that the error converges with the coarse-grid size alone and\nthat only a few oversampling layers, as well as basis functions, are needed.",
    "descriptor": "\nComments: 22 pages, 7 figures, 4 tables, submitted to Journal of Computational Physics, fixed some typos and notation\n",
    "authors": [
      "Tina Mai",
      "Siu Wun Cheung",
      "Jun Sur Richard Park"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.11294"
  },
  {
    "id": "arXiv:2205.11295",
    "title": "Pareto-Improving Data-Sharing",
    "abstract": "We study the effects of data sharing between firms on prices, profits, and\nconsumer welfare. Although indiscriminate sharing of consumer data decreases\nfirm profits due to the subsequent increase in competition, selective sharing\ncan be beneficial. We show that there are data-sharing mechanisms that are\nstrictly Pareto-improving, simultaneously increasing firm profits and consumer\nwelfare. Within the class of Pareto-improving mechanisms, we identify one that\nmaximizes firm profits and one that maximizes consumer welfare.",
    "descriptor": "",
    "authors": [
      "Ronen Gradwohl",
      "Moshe Tennenholtz"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2205.11295"
  },
  {
    "id": "arXiv:2205.11299",
    "title": "Multiple Offsets Multilateration: a new paradigm for sensor network  calibration with unsynchronized reference nodes",
    "abstract": "Positioning using wave signal measurements is used in several applications,\nsuch as GPS systems, structure from sound and Wifi based positioning.\nMathematically, such problems require the computation of the positions of\nreceivers and/or transmitters as well as time offsets if the devices are\nunsynchronized. In this paper, we expand the previous state-of-the-art on\npositioning formulations by introducing Multiple Offsets Multilateration (MOM),\na new mathematical framework to compute the receivers positions with\npseudoranges from unsynchronized reference transmitters at known positions.\nThis could be applied in several scenarios, for example structure from sound\nand positioning with LEO satellites. We mathematically describe MOM,\ndetermining how many receivers and transmitters are needed for the network to\nbe solvable, a study on the number of possible distinct solutions is presented\nand stable solvers based on homotopy continuation are derived. The solvers are\nshown to be efficient and robust to noise both for synthetic and real audio\ndata.",
    "descriptor": "\nComments: accepted to ICASSP2022\n",
    "authors": [
      "Luca Ferranti",
      "Kalle \u00c5str\u00f6m",
      "Magnus Oskarsson",
      "Jani Boutellier",
      "Juho Kannala"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.11299"
  },
  {
    "id": "arXiv:2205.11300",
    "title": "Designing Conversational Robots with Children during the Pandemic",
    "abstract": "Our research project (CHATTERS) is about designing a conversational robot for\nchildren's digital information search. We want to design a robot with a\nsuitable conversation, that fosters a responsible trust relationship between\nchild and robot. In this paper we give: 1) a preliminary view on an empirical\nstudy around children's trust in robots that provide information, which was\nconducted via video call due to the COVID-19 pandemic. 2) We also give a\npreliminary analysis of a co-design workshop we conducted, where the pandemic\nmay have impacted children's design choices. (3) We close by describing the\nupcoming research activities we are developing.",
    "descriptor": "\nComments: peer-reviewed and presented at the \"6th International and Interdisciplinary Perspectives on Children & Recommender and Information Retrieval Systems (KidRec) Information Retrieval Systems for Children in the COVID-19 Era; co-located with ACM IDC, June 27, 2022, Braga, Portugal\n",
    "authors": [
      "Thomas Beelen",
      "Ella Velner",
      "Roeland Ordelman",
      "Khiet P. Truong",
      "Vanessa Evers",
      "Theo Huibers"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11300"
  },
  {
    "id": "arXiv:2205.11303",
    "title": "Real-time Collaborative Multi-Level Modeling by Conflict-Free Replicated  Data Types",
    "abstract": "The need for real-time collaborative solutions in model-driven engineering\nhas been increasing over the past years. Conflict-free replicated data types\n(CRDT) provide scalable and robust replication mechanisms that align well with\nthe requirements of real-time collaborative environments. In this paper, we\npropose a real-time collaborative multi-level modeling framework to support\nadvanced modeling scenarios, built on a collection of custom CRDTs,\nspecifically tailored for the needs of modeling environments. We demonstrate\nthe benefits of the framework through an illustrative modeling case and compare\nit with other state-of-the-art modeling frameworks.",
    "descriptor": "",
    "authors": [
      "Istvan David",
      "Eugene Syriani"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11303"
  },
  {
    "id": "arXiv:2205.11304",
    "title": "Automatic Generation of Programming Exercises",
    "abstract": "Massive training of developers following the growing demands of the\ninformation technology industry requires teachers to automate their repetitive\ntasks. For training courses on programming, it is promising to use automatic\ngeneration and automatic grading of exercises that require a student to write a\nprogram. This article discusses the general scheme for constructing a\nprogramming exercises generator and identifies two classes of exercises, the\ngeneration of which can be automated: converting notation into code and\nconverting data formats. Several examples of programming exercise generators\nare discussed. The experience of using exercise generators for the Python\nprogramming course is briefly described.",
    "descriptor": "",
    "authors": [
      "Peter Sovietov"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.11304"
  },
  {
    "id": "arXiv:2205.11306",
    "title": "Sample Efficient Approaches for Idiomaticity Detection",
    "abstract": "Deep neural models, in particular Transformer-based pre-trained language\nmodels, require a significant amount of data to train. This need for data tends\nto lead to problems when dealing with idiomatic multiword expressions (MWEs),\nwhich are inherently less frequent in natural text. As such, this work explores\nsample efficient methods of idiomaticity detection. In particular we study the\nimpact of Pattern Exploit Training (PET), a few-shot method of classification,\nand BERTRAM, an efficient method of creating contextual embeddings, on the task\nof idiomaticity detection. In addition, to further explore generalisability, we\nfocus on the identification of MWEs not present in the training data. Our\nexperiments show that while these methods improve performance on English, they\nare much less effective on Portuguese and Galician, leading to an overall\nperformance about on par with vanilla mBERT. Regardless, we believe sample\nefficient methods for both identifying and representing potentially idiomatic\nMWEs are very encouraging and hold significant potential for future\nexploration.",
    "descriptor": "",
    "authors": [
      "Dylan Phelps",
      "Xuan-Rui Fan",
      "Edward Gow-Smith",
      "Harish Tayyar Madabushi",
      "Carolina Scarton",
      "Aline Villavicencio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11306"
  },
  {
    "id": "arXiv:2205.11308",
    "title": "Symptom Identification for Interpretable Detection of Multiple Mental  Disorders",
    "abstract": "Mental disease detection (MDD) from social media has suffered from poor\ngeneralizability and interpretability, due to lack of symptom modeling. This\npaper introduces PsySym, the first annotated symptom identification corpus of\nmultiple psychiatric disorders, to facilitate further research progress. PsySym\nis annotated according to a knowledge graph of the 38 symptom classes related\nto 7 mental diseases complied from established clinical manuals and scales, and\na novel annotation framework for diversity and quality. Experiments show that\nsymptom-assisted MDD enabled by PsySym can outperform strong pure-text\nbaselines. We also exhibit the convincing MDD explanations provided by symptom\npredictions with case studies, and point to their further potential\napplications.",
    "descriptor": "",
    "authors": [
      "Zhiling Zhang",
      "Siyuan Chen",
      "Mengyue Wu",
      "Kenny Q. Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11308"
  },
  {
    "id": "arXiv:2205.11313",
    "title": "Mechanism Design with Predictions",
    "abstract": "Improving algorithms via predictions is a very active research topic in\nrecent years. This paper initiates the systematic study of mechanism design in\nthis model. In a number of well-studied mechanism design settings, we make use\nof imperfect predictions to design mechanisms that perform much better than\ntraditional mechanisms if the predictions are accurate (consistency), while\nalways retaining worst-case guarantees even with very imprecise predictions\n(robustness). Furthermore, we refer to the largest prediction error sufficient\nto give a good performance as the error tolerance of a mechanism, and observe\nthat an intrinsic tradeoff among consistency, robustness and error tolerance is\ncommon for mechanism design with predictions.",
    "descriptor": "\nComments: To appear in IJCAI 2022\n",
    "authors": [
      "Chenyang Xu",
      "Pinyan Lu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.11313"
  },
  {
    "id": "arXiv:2205.11315",
    "title": "KOLD: Korean Offensive Language Dataset",
    "abstract": "Although large attention has been paid to the detection of hate speech, most\nwork has been done in English, failing to make it applicable to other\nlanguages. To fill this gap, we present a Korean offensive language dataset\n(KOLD), 40k comments labeled with offensiveness, target, and targeted group\ninformation. We also collect two types of span, offensive and target span that\njustifies the decision of the categorization within the text. Comparing the\ndistribution of targeted groups with the existing English dataset, we point out\nthe necessity of a hate speech dataset fitted to the language that best\nreflects the culture. Trained with our dataset, we report the baseline\nperformance of the models built on top of large pretrained language models. We\nalso show that title information serves as context and is helpful to discern\nthe target of hatred, especially when they are omitted in the comment.",
    "descriptor": "\nComments: 8 pages, 1 figure\n",
    "authors": [
      "Younghoon Jeong",
      "Juhyun Oh",
      "Jaimeen Ahn",
      "Jongwon Lee",
      "Jihyung Mon",
      "Sungjoon Park",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11315"
  },
  {
    "id": "arXiv:2205.11319",
    "title": "Continual Barlow Twins: continual self-supervised learning for remote  sensing semantic segmentation",
    "abstract": "In the field of Earth Observation (EO), Continual Learning (CL) algorithms\nhave been proposed to deal with large datasets by decomposing them into several\nsubsets and processing them incrementally. The majority of these algorithms\nassume that data is (a) coming from a single source, and (b) fully labeled.\nReal-world EO datasets are instead characterized by a large heterogeneity\n(e.g., coming from aerial, satellite, or drone scenarios), and for the most\npart they are unlabeled, meaning they can be fully exploited only through the\nemerging Self-Supervised Learning (SSL) paradigm. For these reasons, in this\npaper we propose a new algorithm for merging SSL and CL for remote sensing\napplications, that we call Continual Barlow Twins (CBT). It combines the\nadvantages of one of the simplest self-supervision techniques, i.e., Barlow\nTwins, with the Elastic Weight Consolidation method to avoid catastrophic\nforgetting. In addition, for the first time we evaluate SSL methods on a highly\nheterogeneous EO dataset, showing the effectiveness of these strategies on a\nnovel combination of three almost non-overlapping domains datasets (airborne\nPotsdam dataset, satellite US3D dataset, and drone UAVid dataset), on a crucial\ndownstream task in EO, i.e., semantic segmentation. Encouraging results show\nthe superiority of SSL in this setting, and the effectiveness of creating an\nincremental effective pretrained feature extractor, based on ResNet50, without\nthe need of relying on the complete availability of all the data, with a\nvaluable saving of time and resources.",
    "descriptor": "",
    "authors": [
      "Valerio Marsocci",
      "Simone Scardapane"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11319"
  },
  {
    "id": "arXiv:2205.11320",
    "title": "Active Learning Through a Covering Lens",
    "abstract": "Deep active learning aims to reduce the annotation cost for deep neural\nnetworks, which are notoriously data-hungry. Until recently, deep active\nlearning methods struggled in the low-budget regime, where only a small amount\nof samples are annotated. The situation has been alleviated by recent advances\nin self-supervised representation learning methods, which impart the geometry\nof the data representation with rich information about the points. Taking\nadvantage of this progress, we study the problem of subset selection for\nannotation through a \"covering\" lens, proposing ProbCover -- a new active\nlearning algorithm for the low budget regime, which seeks to maximize\nProbability Coverage. We describe a dual way to view our formulation, from\nwhich one can derive strategies suitable for the high budget regime of active\nlearning, related to existing methods like Coreset. We conclude with extensive\nexperiments, evaluating ProbCover in the low budget regime. We show that our\nprincipled active learning strategy improves the state-of-the-art in the\nlow-budget regime in several image recognition benchmarks. This method is\nespecially beneficial in semi-supervised settings, allowing state-of-the-art\nsemi-supervised methods to achieve high accuracy with only a few labels.",
    "descriptor": "",
    "authors": [
      "Ofer Yehuda",
      "Avihu Dekel",
      "Guy Hacohen",
      "Daphna Weinshall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11320"
  },
  {
    "id": "arXiv:2205.11322",
    "title": "Learning heterophilious edge to drop: A general framework for boosting  graph neural networks",
    "abstract": "Graph Neural Networks (GNNs) aim at integrating node contents with graph\nstructure to learn nodes/graph representations. Nevertheless, it is found that\nmost of existing GNNs do not work well on data with high heterophily level that\naccounts for a large proportion of edges between different class labels.\nRecently, many efforts to tackle this problem focus on optimizing the way of\nfeature learning. From another angle, this work aims at mitigating the negative\nimpacts of heterophily by optimizing graph structure for the first time.\nSpecifically, on assumption that graph smoothing along heterophilious edges can\nhurt prediction performance, we propose a structure learning method called LHE\nto identify heterophilious edges to drop. A big advantage of this solution is\nthat it can boost GNNs without careful modification of feature learning\nstrategy. Extensive experiments demonstrate the remarkable performance\nimprovement of GNNs with \\emph{LHE} on multiple datasets across full spectrum\nof homophily level.",
    "descriptor": "",
    "authors": [
      "Jincheng Huang",
      "Ping Li",
      "Rui Huang",
      "Chen Na"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11322"
  },
  {
    "id": "arXiv:2205.11324",
    "title": "Towards automatic detection of wildlife trade using machine vision  models",
    "abstract": "Unsustainable trade in wildlife is one of the major threats affecting the\nglobal biodiversity crisis. An important part of the trade now occurs on the\ninternet, especially on digital marketplaces and social media. Automated\nmethods to identify trade posts are needed as resources for conservation are\nlimited. Here, we developed machine vision models based on Deep Neural Networks\nwith the aim to automatically identify images of exotic pet animals for sale. A\nnew training dataset representing exotic pet animals advertised for sale on the\nweb was generated for this purpose. We trained 24 neural-net models spanning a\ncombination of five different architectures, three methods of training and two\ntypes of datasets. Specifically, model generalisation improved after setting a\nportion of the training images to represent negative features. Models were\nevaluated on both within and out of distribution data to test wider model\napplicability. The top performing models achieved an f-score of over 0.95 on\nwithin distribution evaluation and between 0.75 to 0.87 on the two out of\ndistribution datasets. Notably, feature visualisation indicated that models\nperformed well in detecting the surrounding context (e.g. a cage) in which an\nanimal was located, therefore helping to automatically detect images of animals\nin non-natural environments. The proposed methods can help investigate the\nonline wildlife trade, but can also be adapted to study other types of\npeople-nature interactions from digital platforms. Future studies can use these\nfindings to build robust machine learning models and new data collection\npipelines for more taxonomic groups.",
    "descriptor": "",
    "authors": [
      "Ritwik Kulkarni",
      "Enrico Di Minin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11324"
  },
  {
    "id": "arXiv:2205.11325",
    "title": "Sound Automation of Magic Wands (extended version)",
    "abstract": "The magic wand $\\mathbin{-\\!\\!*}$ (also called separating implication) is a\nseparation logic connective commonly used to specify properties of partial data\nstructures, for instance during iterative traversals. A footprint of a magic\nwand formula $A \\mathbin{-\\!\\!*} B$ is a state that, combined with any state in\nwhich $A$ holds, yields a state in which $B$ holds. The key challenge of\nproving a magic wand (also called packaging a wand) is to find such a\nfootprint. Existing package algorithms either have a high annotation overhead\nor, as we show in this paper, are unsound. We present a formal framework that\nprecisely characterises a wide design space of possible package algorithms\napplicable to a large class of separation logics. We prove in Isabelle/HOL that\nour formal framework is sound and complete, and use it to develop a novel\npackage algorithm that offers competitive automation and is sound. Moreover, we\npresent a novel, restricted definition of wands and prove in Isabelle/HOL that\nit is possible to soundly combine fractions of such wands, which is not the\ncase for arbitrary wands. We have implemented our techniques for the Viper\nlanguage, and demonstrate that they are effective in practice.",
    "descriptor": "\nComments: Extended version of CAV 2022 publication\n",
    "authors": [
      "Thibault Dardinier",
      "Gaurav Parthasarathy",
      "No\u00e9 Weeks",
      "Alexanders J. Summers",
      "Peter M\u00fcller"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.11325"
  },
  {
    "id": "arXiv:2205.11327",
    "title": "HyperLogLogLog: Cardinality Estimation With One Log More",
    "abstract": "We present HyperLogLogLog, a practical compression of the HyperLogLog sketch\nthat compresses the sketch from $O(m\\log\\log n)$ bits down to $m\n\\log_2\\log_2\\log_2 m + O(m+\\log\\log n)$ bits for estimating the number of\ndistinct elements~$n$ using $m$~registers. The algorithm works as a drop-in\nreplacement that preserves all estimation properties of the HyperLogLog sketch,\nit is possible to convert back and forth between the compressed and\nuncompressed representations, and the compressed sketch maintains mergeability\nin the compressed domain. The compressed sketch can be updated in amortized\nconstant time, assuming $n$ is sufficiently larger than $m$. We provide a C++\nimplementation of the sketch, and show by experimental evaluation against\nwell-known implementations by Google and Apache that our implementation\nprovides small sketches while maintaining competitive update and merge times.\nConcretely, we observed approximately a 40% reduction in the sketch size.\nFurthermore, we obtain as a corollary a theoretical algorithm that compresses\nthe sketch down to $m\\log_2\\log_2\\log_2\\log_2 m+O(m\\log\\log\\log m/\\log\\log\nm+\\log\\log n)$ bits.",
    "descriptor": "\nComments: 10 pages, 7 figures, KDD '22\n",
    "authors": [
      "Matti Karppa",
      "Rasmus Pagh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.11327"
  },
  {
    "id": "arXiv:2205.11328",
    "title": "Approximating CSPs with Outliers",
    "abstract": "Constraint satisfaction problems (CSPs) are ubiquitous in theoretical\ncomputer science. We study the problem of StrongCSPs, i.e. instances where a\nlarge induced sub-instance has a satisfying assignment. More formally, given a\nCSP instance $\\Psi(V, E, [k], \\{\\Pi_{ij}\\}_{(i,j) \\in E})$ consisting of a set\nof vertices $V$, a set of edges $E$, alphabet $[k]$, a constraint $\\Pi_{ij}\n\\subset [k] \\times [k]$ for each $(i,j) \\in E$, the goal of this problem is to\ncompute the largest subset $S \\subseteq V$ such that the instance induced on\n$S$ has an assignment that satisfies all the constraints.\nIn this paper, we study approximation algorithms for Unique Games and related\nproblems under the StrongCSP framework when the underlying constraint graph\nsatisfies mild expansion properties. In particular, we show that given a Strong\nUnique Games instance whose optimal solution $S^*$ is supported on a regular\nlow threshold rank graph, there exists an algorithm that runs in time\nexponential in the threshold rank, and recovers a large satisfiable\nsub-instance whose size is independent on the label set size and maximum degree\nof the graph. Our algorithm combines the techniques of\nBarak-Raghavendra-Steurer (FOCS'11), Guruswami-Sinop (FOCS'11) with several new\nideas and runs in time exponential in the threshold rank of the optimal set. A\nkey component of our algorithm is a new threshold rank based spectral\ndecomposition, which is used to compute a \"large\" induced subgraph of \"small\"\nthreshold rank; our techniques build on the work of Oveis Gharan and Rezaei\n(SODA'17) and could be of independent interest.",
    "descriptor": "\nComments: 45 Pages\n",
    "authors": [
      "Suprovat Ghoshal",
      "Anand Louis"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.11328"
  },
  {
    "id": "arXiv:2205.11331",
    "title": "Networked Sensing with AI-Empowered Environment Estimation: Exploiting  Macro-Diversity and Array Gain in Perceptive Mobile Networks",
    "abstract": "Sensing will be an important service for future wireless networks to assist\ninnovative applications like autonomous driving and environment monitoring.\nThis paper considers the design of perceptive mobile networks (PMNs) where\ntarget monitoring terminals (TMTs) are deployed over the traditional cellular\nnetworks for jointly sensing the targets in the presence of environment\nclutter. Different from traditional radar, the cellular structure of PMNs\noffers multiple perspectives for target sensing (TS), but the joint processing\namong distributed sensing nodes also causes heavy computation and communication\nworkload over the network. In this paper, we first propose a two-stage protocol\nwhere communication signals are utilized for environment estimation (EE) and TS\nin two consecutive time periods, respectively. A \\textit{networked} sensing\ndetector is then derived to exploit the perspectives provided by multiple TMTs\nfor sensing the same target. The macro-diversity from multiple TMTs and the\narray gain from multiple receive antennas at each TMT are analyzed to reveal\nthe benefit of networked sensing. Furthermore, we derive the sufficient\ncondition that one TMT's contribution to the networked sensing is positive,\nbased on which a TMT selection algorithm is proposed. To reduce the computation\nburden and efficiently estimate the environment, we propose a model-driven\ndeep-learning algorithm that utilizes partially-sampled data for EE. Simulation\nresults confirm the benefits of networked sensing and validate the higher\nefficiency of the proposed EE algorithm than existing methods.",
    "descriptor": "",
    "authors": [
      "Lei Xie",
      "S.H. Song"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.11331"
  },
  {
    "id": "arXiv:2205.11332",
    "title": "ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node  Classification",
    "abstract": "Graph contrastive learning (GCL) has attracted a surge of attention due to\nits superior performance for learning node/graph representations without\nlabels. However, in practice, unlabeled nodes for the given graph usually\nfollow an implicit imbalanced class distribution, where the majority of nodes\nbelong to a small fraction of classes (a.k.a., head class) and the rest classes\noccupy only a few samples (a.k.a., tail classes). This highly imbalanced class\ndistribution inevitably deteriorates the quality of learned node\nrepresentations in GCL. Indeed, we empirically find that most state-of-the-art\nGCL methods exhibit poor performance on imbalanced node classification.\nMotivated by this observation, we propose a principled GCL framework on\nImbalanced node classification (ImGCL), which automatically and adaptively\nbalances the representation learned from GCL without knowing the labels. Our\nmain inspiration is drawn from the recent progressively balanced sampling (PBS)\nmethod in the computer vision domain. We first introduce online clustering\nbased PBS, which balances the training sets based on pseudo-labels obtained\nfrom learned representations. We then develop the node centrality based PBS\nmethod to better preserve the intrinsic structure of graphs, which highlight\nthe important nodes of the given graph. Besides, we theoretically consolidate\nour method by proving that the classifier learned by balanced sampling without\nlabels on an imbalanced dataset can converge to the optimal balanced classifier\nwith a linear rate. Extensive experiments on multiple imbalanced graph datasets\nand imbalance settings verify the effectiveness of our proposed framework,\nwhich significantly improves the performance of the recent state-of-the-art GCL\nmethods. Further experimental ablations and analysis show that the ImGCL\nframework remarkably improves the representations of nodes in tail classes.",
    "descriptor": "",
    "authors": [
      "Liang Zeng",
      "Lanqing Li",
      "Ziqi Gao",
      "Peilin Zhao",
      "Jian Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11332"
  },
  {
    "id": "arXiv:2205.11333",
    "title": "Towards Deeper Understanding of Camouflaged Object Detection",
    "abstract": "Preys in the wild evolve to be camouflaged to avoid being recognized by\npredators. In this way, camouflage acts as a key defence mechanism across\nspecies that is critical to survival. To detect and segment the whole scope of\na camouflaged object, camouflaged object detection (COD) is introduced as a\nbinary segmentation task, with the binary ground truth camouflage map\nindicating the exact regions of the camouflaged objects. In this paper, we\nrevisit this task and argue that the binary segmentation setting fails to fully\nunderstand the concept of camouflage. We find that explicitly modeling the\nconspicuousness of camouflaged objects against their particular backgrounds can\nnot only lead to a better understanding about camouflage, but also provide\nguidance to designing more sophisticated camouflage techniques. Furthermore, we\nobserve that it is some specific parts of camouflaged objects that make them\ndetectable by predators. With the above understanding about camouflaged\nobjects, we present the first triple-task learning framework to simultaneously\nlocalize, segment and rank camouflaged objects, indicating the conspicuousness\nlevel of camouflage. As no corresponding datasets exist for either the\nlocalization model or the ranking model, we generate localization maps with an\neye tracker, which are then processed according to the instance level labels to\ngenerate our ranking-based training and testing dataset. We also contribute the\nlargest COD testing set to comprehensively analyse performance of the\ncamouflaged object detection models. Experimental results show that our\ntriple-task learning framework achieves new state-of-the-art, leading to a more\nexplainable camouflaged object detection network. Our code, data and results\nare available at:\nhttps://github.com/JingZhang617/COD-Rank-Localize-and-Segment.",
    "descriptor": "",
    "authors": [
      "Yunqiu Lv",
      "Jing Zhang",
      "Yuchao Dai",
      "Aixuan Li",
      "Nick Barnes",
      "Deng-Ping Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11333"
  },
  {
    "id": "arXiv:2205.11335",
    "title": "Leakage Subspace Precoding for Physical Layer Security in Multi-User  XL-MIMO Systems",
    "abstract": "We investigate the achievable secrecy sum-rate in a multi-user XL-MIMO\nsystem, on which user distances to the base station become comparable to the\nantenna array dimensions. We show that the consideration of spherical-wavefront\npropagation inherent to these set-ups is beneficial for physical-layer\nsecurity, as it provides immunity against eavesdroppers located in similar\nangular directions that would otherwise prevent secure communication under\nclassical planar-wavefront propagation. A leakage subspace precoding strategy\nis also proposed for joint secure precoding and user scheduling, which allows\nto improve the secrecy sum-rate compared to conventional zero-forcing based\nstrategies, under different eavesdropper collusion strategies.",
    "descriptor": "\nComments: 5 pages and 4 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Gonzalo J. Anaya-Lopez",
      "Jose P. Gonzalez-Coma",
      "F. Javier Lopez-Martinez"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.11335"
  },
  {
    "id": "arXiv:2205.11338",
    "title": "Temporal Network Analysis Using Zigzag Persistence",
    "abstract": "This work presents a framework for studying temporal networks using zigzag\npersistence, a tool from the field of Topological Data Analysis (TDA). The\nresulting approach is general and applicable to a wide variety of time-varying\ngraphs. For example, these graphs may correspond to a system modeled as a\nnetwork with edges whose weights are functions of time, or they may represent a\ntime series of a complex dynamical system. We use simplicial complexes to\nrepresent snapshots of the temporal networks that can then be analyzed using\nzigzag persistence. We show two applications of our method to dynamic networks:\nan analysis of commuting trends on multiple temporal scales, e.g., daily and\nweekly, in the Great Britain transportation network, and the detection of\nperiodic/chaotic transitions due to intermittency in dynamical systems\nrepresented by temporal ordinal partition networks. Our findings show that the\nresulting zero- and one-dimensional zigzag persistence diagrams can detect\nchanges in the networks' shapes that are missed by traditional connectivity and\ncentrality graph statistics.",
    "descriptor": "",
    "authors": [
      "Audun Myers",
      "Firas Khasawneh",
      "Elizabeth Munch"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2205.11338"
  },
  {
    "id": "arXiv:2205.11342",
    "title": "ScholarBERT: Bigger is Not Always Better",
    "abstract": "Transformer-based masked language models trained on general corpora, such as\nBERT and RoBERTa, have shown impressive performance on various downstream\ntasks. Increasingly, researchers are \"finetuning\" these models to improve\nperformance on domain-specific tasks. Here, we report a broad study in which we\napplied 14 transformer-based models to 11 scientific tasks in order to evaluate\nhow downstream performance is affected by changes along various dimensions\n(e.g., training data, model size, pretraining time, finetuning length). In this\nprocess, we created the largest and most diverse scientific language model to\ndate, ScholarBERT, by training a 770M-parameter BERT model on an 221B token\nscientific literature dataset spanning many disciplines. Counterintuitively,\nour evaluation of the 14 BERT-based models (seven versions of ScholarBERT, five\nscience-specific large language models from the literature, BERT-Base, and\nBERT-Large) reveals little difference in performance across the 11\nscience-focused tasks, despite major differences in model size and training\ndata. We argue that our results establish an upper bound for the performance\nachievable with BERT-based architectures on tasks from the scientific domain.",
    "descriptor": "\nComments: 16 pages. 4 figures. 8 tables\n",
    "authors": [
      "Zhi Hong",
      "Aswathy Ajith",
      "Gregory Pauloski",
      "Eamon Duede",
      "Carl Malamud",
      "Roger Magoulas",
      "Kyle Chard",
      "Ian Foster"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11342"
  },
  {
    "id": "arXiv:2205.11343",
    "title": "Heterogeneous Graph Neural Network for Session-Based Recommendation with  User-Session Constraint",
    "abstract": "The recommendation system provides users with an appropriate limit of recent\nonline large amounts of information. Session-based recommendation, a sub-area\nof recommender systems, attempts to recommend items by interpreting sessions\nthat consist of sequences of items. Recently, research to include user\ninformation in these sessions is progress. However, it is difficult to generate\nhigh-quality user information that includes session information generated by\nuser. In this paper, we consider various relationships in graph created by\nsessions through HAN. Constraints also force user information to take into\naccount information from the session. It seeks to increase performance through\nadditional optimization in the training process. The proposed model\noutperformed other methods on various real-world data sets.",
    "descriptor": "",
    "authors": [
      "Minjae Park"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11343"
  },
  {
    "id": "arXiv:2205.11344",
    "title": "Cyclic Redundancy Checks and Error Detection",
    "abstract": "This study investigates the capabilities of Cyclic Redundancy Checks(CRCs) to\ndetect burst and random errors. Researchers have favored these error detection\ncodes throughout the evolution of computing and have implemented them in\ncommunication protocols worldwide. CRCs are integrated into almost every\ndevice, in software and hardware. CRCs play a critical role in ensuring that\nour digital communication systems are efficient and erroneous packets are\ndetected. Because the quantity of data generated and transmitted has increased\nover the last twenty years, we are more likely to encounter errors. It is\nimportant that the tools and methodologies used to ensure the integrity of\ndigital communication systems are evaluated to handle higher frequencies of\ninformation.\nIn this study, we explore the need to improve the capabilities of\nerror-detecting codes to handle higher quantities of data by testing the error\ndetection properties of CRC's in a restricted domain.",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Waylon Jepsen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.11344"
  },
  {
    "id": "arXiv:2205.11353",
    "title": "Gaussian Persistence Curves",
    "abstract": "Topological data analysis (TDA) is a rising field in the intersection of\nmathematics, statistics, and computer science/data science. The cornerstone of\nTDA is persistent homology, which produces a summary of topological information\ncalled a persistence diagram. To utilize machine and deep learning methods on\npersistence diagrams, These diagrams are further summarized by transforming\nthem into functions. In this paper we investigate the stability and injectivity\nof a class of smooth, one-dimensional functional summaries called Gaussian\npersistence curves.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Yu-Min Chung",
      "Michael Hull",
      "Austin Lawson",
      "Neil Pritchard"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2205.11353"
  },
  {
    "id": "arXiv:2205.11357",
    "title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised  Reinforcement Learning",
    "abstract": "The goal of Unsupervised Reinforcement Learning (URL) is to find a\nreward-agnostic prior policy on a task domain, such that the sample-efficiency\non supervised downstream tasks is improved. Although agents initialized with\nsuch a prior policy can achieve a significantly higher reward with fewer\nsamples when finetuned on the downstream task, it is still an open question how\nan optimal pretrained prior policy can be achieved in practice. In this work,\nwe present POLTER (Policy Trajectory Ensemble Regularization) - a general\nmethod to regularize the pretraining that can be applied to any URL algorithm\nand is especially useful on data- and knowledge-based URL algorithms. It\nutilizes an ensemble of policies that are discovered during pretraining and\nmoves the policy of the URL algorithm closer to its optimal prior. Our method\nis theoretically justified, and we analyze its practical effects on a white-box\nbenchmark, allowing us to study POLTER with full control. In our main\nexperiments, we evaluate POLTER on the Unsupervised Reinforcement Learning\nBenchmark (URLB), which consists of 12 tasks in 3 domains. We demonstrate the\ngenerality of our approach by improving the performance of a diverse set of\ndata- and knowledge-based URL algorithms by 19% on average and up to 40% in the\nbest case. Under a fair comparison with tuned baselines and tuned POLTER, we\nestablish a new the state-of-the-art on the URLB.",
    "descriptor": "",
    "authors": [
      "Frederik Schubert",
      "Carolin Benjamins",
      "Sebastian D\u00f6hler",
      "Bodo Rosenhahn",
      "Marius Lindauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.11357"
  },
  {
    "id": "arXiv:2205.11359",
    "title": "Capacity Bounds for the DeepONet Method of Solving Differential  Equations",
    "abstract": "In recent times machine learning methods have made significant advances in\nbecoming a useful tool for analyzing physical systems. A particularly active\narea in this theme has been \"physics informed machine learning\" [1] which\nfocuses on using neural nets for numerically solving differential equations.\nAmong all the proposals for solving differential equations using deep-learning,\nin this paper we aim to advance the theory of generalization error for\nDeepONets - which is unique among all the available ideas because of its\nparticularly intriguing structure of having an inner-product of two neural\nnets.\nOur key contribution is to give a bound on the Rademacher complexity for a\nlarge class of DeepONets. Our bound does not explicitly scale with the number\nof parameters of the nets involved and is thus a step towards explaining the\nefficacy of overparameterized DeepONets. Additionally, a capacity bound such as\nours suggests a novel regularizer on the neural net weights that can help in\ntraining DeepONets - irrespective of the differential equation being solved.\n[1] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and\nL. Yang. Physics-informed machine learning. Nature Reviews Physics, 2021.",
    "descriptor": "\nComments: 25 pages, 1 figure\n",
    "authors": [
      "Pulkit Gopalani",
      "Sayar Karmakar",
      "Anirbit Mukherjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11359"
  },
  {
    "id": "arXiv:2205.11363",
    "title": "Competitive Equilibrium with Chores: Combinatorial Algorithm and  Hardness",
    "abstract": "We study the computational complexity of finding a competitive equilibrium\n(CE) with chores when agents have linear preferences. CE is one of the most\npreferred mechanisms for allocating a set of items among agents. CE with equal\nincomes (CEEI), Fisher, and Arrow-Debreu (exchange) are the fundamental\neconomic models to study allocation problems, where CEEI is a special case of\nFisher and Fisher is a special case of exchange. When the items are goods\n(giving utility), the CE set is convex even in the exchange model, facilitating\nseveral combinatorial polynomial-time algorithms (starting with the seminal\nwork of Devanur, Papadimitriou, Saberi and Vazirani) for all of these models.\nIn sharp contrast, when the items are chores (giving disutility), the CE set is\nknown to be non-convex and disconnected even in the CEEI model. Further, no\ncombinatorial algorithms or hardness results are known for these models. In\nthis paper, we give two main results for CE with chores:\n1) A combinatorial algorithm to compute a $(1-\\varepsilon)$-approximate CEEI\nin time $\\tilde{\\mathcal{O}}(n^4m^2 / \\varepsilon^2)$, where $n$ is the number\nof agents and $m$ is the number of chores.\n2) PPAD-hardness of finding a $(1-1/\\mathit{poly}(n))$-approximate CE in the\nexchange model under a sufficient condition. To the best of our knowledge,\nthese results show the first separation between the CEEI and exchange models\nwhen agents have linear preferences, assuming PPAD $\\neq $ P.\nFinally, we show that our new insight implies a straightforward proof of the\nexistence of an allocation that is both envy-free up to one chore (EF1) and\nPareto optimal (PO) in the discrete setting when agents have factored bivalued\npreferences.",
    "descriptor": "\nComments: Accepted in EC 2022. The PPAD hardness section also appeared in arXiv:2008.00285\n",
    "authors": [
      "Bhaskar Ray Chaudhury",
      "Jugal Garg",
      "Peter McGlaughlin",
      "Ruta Mehta"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.11363"
  },
  {
    "id": "arXiv:2205.11365",
    "title": "Graph-Based Methods for Discrete Choice",
    "abstract": "Choices made by individuals have widespread impacts--for instance, people\nchoose between political candidates to vote for, between social media posts to\nshare, and between brands to purchase--moreover, data on these choices are\nincreasingly abundant. Discrete choice models are a key tool for learning\nindividual preferences from such data. Additionally, social factors like\nconformity and contagion influence individual choice. Existing methods for\nincorporating these factors into choice models do not account for the entire\nsocial network and require hand-crafted features. To overcome these\nlimitations, we use graph learning to study choice in networked contexts. We\nidentify three ways in which graph learning techniques can be used for discrete\nchoice: learning chooser representations, regularizing choice model parameters,\nand directly constructing predictions from a network. We design methods in each\ncategory and test them on real-world choice datasets, including county-level\n2016 US election results and Android app installation and usage data. We show\nthat incorporating social network structure can improve the predictions of the\nstandard econometric choice model, the multinomial logit. We provide evidence\nthat app installations are influenced by social context, but we find no such\neffect on app usage among the same participants, which instead is habit-driven.\nIn the election data, we highlight the additional insights a discrete choice\nframework provides over classification or regression, the typical approaches.\nOn synthetic data, we demonstrate the sample complexity benefit of using social\ninformation in choice models.",
    "descriptor": "\nComments: 11 pages, 3 figures\n",
    "authors": [
      "Kiran Tomlinson",
      "Austin R. Benson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2205.11365"
  },
  {
    "id": "arXiv:2205.11367",
    "title": "Rethinking Task-Incremental Learning Baselines",
    "abstract": "It is common to have continuous streams of new data that need to be\nintroduced in the system in real-world applications. The model needs to learn\nnewly added capabilities (future tasks) while retaining the old knowledge (past\ntasks). Incremental learning has recently become increasingly appealing for\nthis problem. Task-incremental learning is a kind of incremental learning where\ntask identity of newly included task (a set of classes) remains known during\ninference. A common goal of task-incremental methods is to design a network\nthat can operate on minimal size, maintaining decent performance. To manage the\nstability-plasticity dilemma, different methods utilize replay memory of past\ntasks, specialized hardware, regularization monitoring etc. However, these\nmethods are still less memory efficient in terms of architecture growth or\ninput data costs. In this study, we present a simple yet effective adjustment\nnetwork (SAN) for task incremental learning that achieves near state-of-the-art\nperformance while using minimal architectural size without using memory\ninstances compared to previous state-of-the-art approaches. We investigate this\napproach on both 3D point cloud object (ModelNet40) and 2D image (CIFAR10,\nCIFAR100, MiniImageNet, MNIST, PermutedMNIST, notMNIST, SVHN, and FashionMNIST)\nrecognition tasks and establish a strong baseline result for a fair comparison\nwith existing methods. On both 2D and 3D domains, we also observe that SAN is\nprimarily unaffected by different task orders in a task-incremental setting.",
    "descriptor": "\nComments: Accepted in ICPR2022\n",
    "authors": [
      "Md Sazzad Hossain",
      "Pritom Saha",
      "Townim Faisal Chowdhury",
      "Shafin Rahman",
      "Fuad Rahman",
      "Nabeel Mohammed"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11367"
  },
  {
    "id": "arXiv:2205.11368",
    "title": "Dual-Numbers Reverse AD, Efficiently",
    "abstract": "Where dual-numbers forward-mode automatic differentiation (AD) pairs each\nscalar value with its tangent derivative, dual-numbers /reverse-mode/ AD\nattempts to achieve reverse AD using a similarly simple idea: by pairing each\nscalar value with a backpropagator function. Its correctness and efficiency on\nhigher-order input languages have been analysed by Brunel, Mazza and Pagani,\nbut this analysis was on a custom operational semantics for which it is unclear\nwhether it can be implemented efficiently. We take inspiration from their use\nof /linear factoring/ to optimise dual-numbers reverse-mode AD to an algorithm\nthat has the correct complexity and enjoys an efficient implementation in a\nstandard functional language with resource-linear types, such as Haskell. Aside\nfrom the linear factoring ingredient, our optimisation steps consist of\nwell-known ideas from the functional programming community. Furthermore, we\nobserve a connection with classical imperative taping-based reverse AD, as well\nas Kmett's 'ad' Haskell library, recently analysed by Krawiec et al. We\ndemonstrate the practical use of our technique by providing a performant\nimplementation that differentiates most of Haskell98.",
    "descriptor": "",
    "authors": [
      "Tom Smeding",
      "Matthijs V\u00e1k\u00e1r"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.11368"
  },
  {
    "id": "arXiv:2205.11370",
    "title": "Use of Transformer-Based Models for Word-Level Transliteration of the  Book of the Dean of Lismore",
    "abstract": "The Book of the Dean of Lismore (BDL) is a 16th-century Scottish Gaelic\nmanuscript written in a non-standard orthography. In this work, we outline the\nproblem of transliterating the text of the BDL into a standardised orthography,\nand perform exploratory experiments using Transformer-based models for this\ntask. In particular, we focus on the task of word-level transliteration, and\nachieve a character-level BLEU score of 54.15 with our best model, a BART\narchitecture pre-trained on the text of Scottish Gaelic Wikipedia and then\nfine-tuned on around 2,000 word-level parallel examples. Our initial\nexperiments give promising results, but we highlight the shortcomings of our\nmodel, and discuss directions for future work.",
    "descriptor": "\nComments: 4th Celtic Language Technology Workshop\n",
    "authors": [
      "Edward Gow-Smith",
      "Mark McConville",
      "William Gillies",
      "Jade Scott",
      "Roibeard \u00d3 Maolalaigh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11370"
  },
  {
    "id": "arXiv:2205.11371",
    "title": "Fractional-Order Partial Cancellation of Integer-Order Poles and Zeros",
    "abstract": "The key idea of this contribution is the partial compensation of non-minimum\nphase zeros or unstable poles. Therefore the integer-order zero/pole is split\ninto a product of fractional-order pseudo zeros/poles. The amplitude and phase\nresponse of these fractional-order terms is derived to include these\ncompensators into the loop-shaping design. Such compensators can be generalized\nto conjugate complex zeros/poles, and also implicit fractional-order terms can\nbe applied. In the case of the non-minimum phase zero, its compensation leads\nto a higher phase margin and a steeper open-loop amplitude response around the\ncrossover frequency resulting in a reduced undershooting in the step-response,\nas illustrated in the numerical example.",
    "descriptor": "",
    "authors": [
      "Benjamin Vo\u00df",
      "Christoph Weise",
      "Michael Ruderman",
      "Johann Reger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11371"
  },
  {
    "id": "arXiv:2205.11374",
    "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements",
    "abstract": "The growing capability and availability of generative language models has\nenabled a wide range of new downstream tasks. Academic research has identified,\nquantified and mitigated biases present in language models but is rarely\ntailored to downstream tasks where wider impact on individuals and society can\nbe felt. In this work, we leverage one popular generative language model,\nGPT-3, with the goal of writing unbiased and realistic job advertisements. We\nfirst assess the bias and realism of zero-shot generated advertisements and\ncompare them to real-world advertisements. We then evaluate prompt-engineering\nand fine-tuning as debiasing methods. We find that prompt-engineering with\ndiversity-encouraging prompts gives no significant improvement to bias, nor\nrealism. Conversely, fine-tuning, especially on unbiased real advertisements,\ncan improve realism and reduce bias.",
    "descriptor": "\nComments: Accepted for the 4th Workshop on Gender Bias in Natural Language Processing at NAACL 2022\n",
    "authors": [
      "Conrad Borchers",
      "Dalia Sara Gala",
      "Benjamin Gilburt",
      "Eduard Oravkin",
      "Wilfried Bounsi",
      "Yuki M. Asano",
      "Hannah Rose Kirk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11374"
  },
  {
    "id": "arXiv:2205.11375",
    "title": "Exploring the limits of multifunctionality across different reservoir  computers",
    "abstract": "Multifunctional neural networks are capable of performing more than one task\nwithout changing any network connections. In this paper we explore the\nperformance of a continuous-time, leaky-integrator, and next-generation\n`reservoir computer' (RC), when trained on tasks which test the limits of\nmultifunctionality. In the first task we train each RC to reconstruct a\ncoexistence of chaotic attractors from different dynamical systems. By moving\nthe data describing these attractors closer together, we find that the extent\nto which each RC can reconstruct both attractors diminishes as they begin to\noverlap in state space. In order to provide a greater understanding of this\ninhibiting effect, in the second task we train each RC to reconstruct a\ncoexistence of two circular orbits which differ only in the direction of\nrotation. We examine the critical effects that certain parameters can have in\neach RC to achieve multifunctionality in this extreme case of completely\noverlapping training data.",
    "descriptor": "\nComments: Accepted for publication in the proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN 2022)\n",
    "authors": [
      "Andrew Flynn",
      "Oliver Heilmann",
      "Daniel K\u00f6glmayr",
      "Vassilios A. Tsachouridis",
      "Christoph R\u00e4th",
      "Andreas Amann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Dynamical Systems (math.DS)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2205.11375"
  },
  {
    "id": "arXiv:2205.11378",
    "title": "Markedness in Visual Semantic AI",
    "abstract": "We evaluate the state-of-the-art multimodal \"visual semantic\" model CLIP\n(\"Contrastive Language Image Pretraining\") for biases related to the marking of\nage, gender, and race or ethnicity. Given the option to label an image as \"a\nphoto of a person\" or to select a label denoting race or ethnicity, CLIP\nchooses the \"person\" label 47.9% of the time for White individuals, compared\nwith 5.0% or less for individuals who are Black, East Asian, Southeast Asian,\nIndian, or Latino or Hispanic. The model is more likely to rank the unmarked\n\"person\" label higher than labels denoting gender for Male individuals (26.7%\nof the time) vs. Female individuals (15.2% of the time). Age affects whether an\nindividual is marked by the model: Female individuals under the age of 20 are\nmore likely than Male individuals to be marked with a gender label, but less\nlikely to be marked with an age label, while Female individuals over the age of\n40 are more likely to be marked based on age than Male individuals. We also\nexamine the self-similarity (mean pairwise cosine similarity) for each social\ngroup, where higher self-similarity denotes greater attention directed by CLIP\nto the shared characteristics (age, race, or gender) of the social group. As\nage increases, the self-similarity of representations of Female individuals\nincreases at a higher rate than for Male individuals, with the disparity most\npronounced at the \"more than 70\" age range. All ten of the most self-similar\nsocial groups are individuals under the age of 10 or over the age of 70, and\nsix of the ten are Female individuals. Existing biases of self-similarity and\nmarkedness between Male and Female gender groups are further exacerbated when\nthe groups compared are individuals who are White and Male and individuals who\nare Black and Female. Results indicate that CLIP reflects the biases of the\nlanguage and society which produced its training data.",
    "descriptor": "\nComments: To be published at ACM FAccT 2022\n",
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11378"
  },
  {
    "id": "arXiv:2205.11379",
    "title": "Fractional SEIR Model and Data-Driven Predictions of COVID-19 Dynamics  of Omicron Variant",
    "abstract": "We study the dynamic evolution of COVID-19 cased by the Omicron variant via a\nfractional susceptible-exposedinfected-removed (SEIR) model. Preliminary data\nsuggest that the symptoms of Omicron infection are not prominent and the\ntransmission is therefore more concealed, which causes a relatively slow\nincrease in the detected cases of the new infected at the beginning of the\npandemic. To characterize the specific dynamics, the Caputo-Hadamard fractional\nderivative is adopted to refined the classical SEIR model. Based on the\nreported data, we infer the fractional order, timedependent parameters, as well\nas unobserved dynamics of the fractional SEIR model via fractional\nphysics-informed neural networks (fPINNs). Then, we make short-time predictions\nusing the learned fractional SEIR model.",
    "descriptor": "",
    "authors": [
      "Min Cai",
      "George Em Karniadakis",
      "Changpin Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Physics and Society (physics.soc-ph)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2205.11379"
  },
  {
    "id": "arXiv:2205.11380",
    "title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency",
    "abstract": "Transformer-based language models are known to display anisotropic behavior:\nthe token embeddings are not homogeneously spread in space, but rather\naccumulate along certain directions. A related recent finding is the outlier\nphenomenon: the parameters in the final element of Transformer layers that\nconsistently have unusual magnitude in the same dimension across the model, and\nsignificantly degrade its performance if disabled. We replicate the evidence\nfor the outlier phenomenon and we link it to the geometry of the embedding\nspace. Our main finding is that in both BERT and RoBERTa the token frequency,\nknown to contribute to anisotropicity, also contributes to the outlier\nphenomenon. In its turn, the outlier phenomenon contributes to the \"vertical\"\nself-attention pattern that enables the model to focus on the special tokens.\nWe also find that, surprisingly, the outlier effect on the model performance\nvaries by layer, and that variance is also related to the correlation between\noutlier magnitude and encoded token frequency.",
    "descriptor": "",
    "authors": [
      "Giovanni Puccetti",
      "Anna Rogers",
      "Aleksandr Drozd",
      "Felice Dell'Orletta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11380"
  },
  {
    "id": "arXiv:2205.11384",
    "title": "Learning Long-Horizon Robot Exploration Strategies for Multi-Object  Search in Continuous Action Spaces",
    "abstract": "Recent advances in vision-based navigation and exploration have shown\nimpressive capabilities in photorealistic indoor environments. However, these\nmethods still struggle with long-horizon tasks and require large amounts of\ndata to generalize to unseen environments. In this work, we present a novel\nreinforcement learning approach for multi-object search that combines\nshort-term and long-term reasoning in a single model while avoiding the\ncomplexities arising from hierarchical structures. In contrast to existing\nmulti-object search methods that act in granular discrete action spaces, our\napproach achieves exceptional performance in continuous action spaces. We\nperform extensive experiments and show that it generalizes to unseen apartment\nenvironments with limited data. Furthermore, we demonstrate zero-shot transfer\nof the learned policies to an office environment in real world experiments.",
    "descriptor": "",
    "authors": [
      "F. Schmalstieg",
      "D. Honerkamp",
      "T. Welschehold",
      "A. Valada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.11384"
  },
  {
    "id": "arXiv:2205.11387",
    "title": "Robust Constrained Multi-objective Evolutionary Algorithm based on  Polynomial Chaos Expansion for Trajectory Optimization",
    "abstract": "An integrated optimization method based on the constrained multi-objective\nevolutionary algorithm (MOEA) and non-intrusive polynomial chaos expansion\n(PCE) is proposed, which solves robust multi-objective optimization problems\nunder time-series dynamics. The constraints in such problems are difficult to\nhandle, not only because the number of the dynamic constraints is multiplied by\nthe discretized time steps but also because each of them is probabilistic. The\nproposed method rewrites a robust formulation into a deterministic problem via\nthe PCE, and then sequentially processes the generated constraints in\npopulation generation, trajectory generation, and evaluation by the MOEA. As a\ncase study, the landing trajectory design of supersonic transport (SST) with\nwind uncertainty is optimized. Results demonstrate the quantitative influence\nof the constraint values over the optimized solution sets and corresponding\ntrajectories, proposing robust flight controls.",
    "descriptor": "\nComments: 9 pages, 9 figures, 3 tables. Accepted to IEEE World Congress on Computational Intelligence 2022, Congress on Evolutionary Computation 2022\n",
    "authors": [
      "Yuji Takubo",
      "Masahiro Kanazaki"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.11387"
  },
  {
    "id": "arXiv:2205.11388",
    "title": "StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in  Question Answering Models",
    "abstract": "Knowledge and language understanding of models evaluated through question\nanswering (QA) has been usually studied on static snapshots of knowledge, like\nWikipedia. However, our world is dynamic, evolves over time, and our models'\nknowledge becomes outdated. To study how semi-parametric QA models and their\nunderlying parametric language models (LMs) adapt to evolving knowledge, we\nconstruct a new large-scale dataset, StreamingQA, with human written and\ngenerated questions asked on a given date, to be answered from 14 years of\ntime-stamped news articles. We evaluate our models quarterly as they read new\narticles not seen in pre-training. We show that parametric models can be\nupdated without full retraining, while avoiding catastrophic forgetting. For\nsemi-parametric models, adding new articles into the search space allows for\nrapid adaptation, however, models with an outdated underlying LM under-perform\nthose with a retrained LM. For questions about higher-frequency named entities,\nparametric updates are particularly beneficial. In our dynamic world, the\nStreamingQA dataset enables a more realistic evaluation of QA models, and our\nexperiments highlight several promising directions for future research.",
    "descriptor": "",
    "authors": [
      "Adam Li\u0161ka",
      "Tom\u00e1\u0161 Ko\u010disk\u00fd",
      "Elena Gribovskaya",
      "Tayfun Terzi",
      "Eren Sezener",
      "Devang Agrawal",
      "Cyprien de Masson d'Autume",
      "Tim Scholtes",
      "Manzil Zaheer",
      "Susannah Young",
      "Ellen Gilsenan-McMahon",
      "Sophia Austin",
      "Phil Blunsom",
      "Angeliki Lazaridou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11388"
  },
  {
    "id": "arXiv:2205.11389",
    "title": "Fictitious Play in Markov Games with Single Controller",
    "abstract": "Certain but important classes of strategic-form games, including zero-sum and\nidentical-interest games, have the fictitious-play-property (FPP), i.e.,\nbeliefs formed in fictitious play dynamics always converge to a Nash\nequilibrium (NE) in the repeated play of these games. Such convergence results\nare seen as a (behavioral) justification for the game-theoretical equilibrium\nanalysis. Markov games (MGs), also known as stochastic games, generalize the\nrepeated play of strategic-form games to dynamic multi-state settings with\nMarkovian state transitions. In particular, MGs are standard models for\nmulti-agent reinforcement learning -- a reviving research area in learning and\ngames, and their game-theoretical equilibrium analyses have also been conducted\nextensively. However, whether certain classes of MGs have the FPP or not (i.e.,\nwhether there is a behavioral justification for equilibrium analysis or not)\nremains largely elusive. In this paper, we study a new variant of fictitious\nplay dynamics for MGs and show its convergence to an NE in n-player\nidentical-interest MGs in which a single player controls the state transitions.\nSuch games are of interest in communications, control, and economics\napplications. Our result together with the recent results in [Sayin et al.\n2020] establishes the FPP of two-player zero-sum MGs and n-player\nidentical-interest MGs with a single controller (standing at two different ends\nof the MG spectrum from fully competitive to fully cooperative).",
    "descriptor": "\nComments: Accepted to ACM Conference on Economics and Computation (EC) 2022\n",
    "authors": [
      "Muhammed O. Sayin",
      "Kaiqing Zhang",
      "Asuman Ozdaglar"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.11389"
  },
  {
    "id": "arXiv:2205.11393",
    "title": "Generic bounds on the approximation error for physics-informed (and)  operator learning",
    "abstract": "We propose a very general framework for deriving rigorous bounds on the\napproximation error for physics-informed neural networks (PINNs) and operator\nlearning architectures such as DeepONets and FNOs as well as for\nphysics-informed operator learning. These bounds guarantee that PINNs and\n(physics-informed) DeepONets or FNOs will efficiently approximate the\nunderlying solution or solution operator of generic partial differential\nequations (PDEs). Our framework utilizes existing neural network approximation\nresults to obtain bounds on more involved learning architectures for PDEs. We\nillustrate the general framework by deriving the first rigorous bounds on the\napproximation error of physics-informed operator learning and by showing that\nPINNs (and physics-informed DeepONets and FNOs) mitigate the curse of\ndimensionality in approximating nonlinear parabolic PDEs.",
    "descriptor": "",
    "authors": [
      "Tim De Ryck",
      "Siddhartha Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11393"
  },
  {
    "id": "arXiv:2205.11394",
    "title": "Detection of Fights in Videos: A Comparison Study of Anomaly Detection  and Action Recognition",
    "abstract": "Detection of fights is an important surveillance application in videos. Most\nexisting methods use supervised binary action recognition. Since frame-level\nannotations are very hard to get for anomaly detection, weakly supervised\nlearning using multiple instance learning is widely used. This paper explores\nthe detection of fights in videos as one special type of anomaly detection and\nas binary action recognition. We use the UBI-Fight and NTU-CCTV-Fight datasets\nfor most of the study since they have frame-level annotations. We find that the\nanomaly detection has similar or even better performance than the action\nrecognition. Furthermore, we study to use anomaly detection as a toolbox to\ngenerate training datasets for action recognition in an iterative way\nconditioned on the performance of the anomaly detection. Experiment results\nshould show that we achieve state-of-the-art performance on three fight\ndetection datasets.",
    "descriptor": "",
    "authors": [
      "Weijun Tan",
      "Jingfeng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11394"
  },
  {
    "id": "arXiv:2205.11395",
    "title": "Multi-Temporal Spatial-Spectral Comparison Network for Hyperspectral  Anomalous Change Detection",
    "abstract": "Hyperspectral anomalous change detection has been a challenging task for its\nemphasis on the dynamics of small and rare objects against the prevalent\nchanges. In this paper, we have proposed a Multi-Temporal spatial-spectral\nComparison Network for hyperspectral anomalous change detection (MTC-NET). The\nwhole model is a deep siamese network, aiming at learning the prevalent\nspectral difference resulting from the complex imaging conditions from the\nhyperspectral images by contrastive learning. A three-dimensional spatial\nspectral attention module is designed to effectively extract the spatial\nsemantic information and the key spectral differences. Then the gaps between\nthe multi-temporal features are minimized, boosting the alignment of the\nsemantic and spectral features and the suppression of the multi-temporal\nbackground spectral difference. The experiments on the \"Viareggio 2013\"\ndatasets demonstrate the effectiveness of proposed MTC-NET.",
    "descriptor": "\nComments: 4pages; 5 figure; IGARSS2022\n",
    "authors": [
      "Meiqi Hu",
      "Chen Wu",
      "Bo Du"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11395"
  },
  {
    "id": "arXiv:2205.11397",
    "title": "Super Vision Transformer",
    "abstract": "We attempt to reduce the computational costs in vision transformers (ViTs),\nwhich increase quadratically in the token number. We present a novel training\nparadigm that trains only one ViT model at a time, but is capable of providing\nimproved image recognition performance with various computational costs. Here,\nthe trained ViT model, termed super vision transformer (SuperViT), is empowered\nwith the versatile ability to solve incoming patches of multiple sizes as well\nas preserve informative tokens with multiple keeping rates (the ratio of\nkeeping tokens) to achieve good hardware efficiency for inference, given that\nthe available hardware resources often change from time to time. Experimental\nresults on ImageNet demonstrate that our SuperViT can considerably reduce the\ncomputational costs of ViT models with even performance increase. For example,\nwe reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and\n0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existing\nstudies on efficient vision transformers. For example, when consuming the same\namount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SoTA) EViT\nby 1.1% when using DeiT-S as their backbones. The project of this work is made\npublicly available at https://github.com/lmbxmu/SuperViT.",
    "descriptor": "",
    "authors": [
      "Mingbao Lin",
      "Mengzhao Chen",
      "Yuxin Zhang",
      "Ke Li",
      "Yunhang Shen",
      "Chunhua Shen",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11397"
  },
  {
    "id": "arXiv:2205.11398",
    "title": "Fine-Grained Counting with Crowd-Sourced Supervision",
    "abstract": "Crowd-sourcing is an increasingly popular tool for image analysis in animal\necology. Computer vision methods that can utilize crowd-sourced annotations can\nhelp scale up analysis further. In this work we study the potential to do so on\nthe challenging task of fine-grained counting. As opposed to the standard crowd\ncounting task, fine-grained counting also involves classifying attributes of\nindividuals in dense crowds. We introduce a new dataset from animal ecology to\nenable this study that contains 1.7M crowd-sourced annotations of 8\nfine-grained classes. It is the largest available dataset for fine-grained\ncounting and the first to enable the study of the task with crowd-sourced\nannotations. We introduce methods for generating aggregate \"ground truths\" from\nthe collected annotations, as well as a counting method that can utilize the\naggregate information. Our method improves results by 8% over a comparable\nbaseline, indicating the potential for algorithms to learn fine-grained\ncounting using crowd-sourced supervision.",
    "descriptor": "\nComments: In Computer Vision for Animal Behavior Tracking and Modeling Workshop at CVPR 2022. 4 pages, 3 figures\n",
    "authors": [
      "Justin Kay",
      "Catherine M. Foley",
      "Tom Hart"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11398"
  },
  {
    "id": "arXiv:2205.11399",
    "title": "Energy Efficiency of Web Browsers in the Android Ecosystem",
    "abstract": "This paper presents an empirical study regarding the energy consumption of\nthe most used web browsers on the Android ecosystem. In order to properly\ncompare the web browsers in terms of energy consumption, we defined a set of\ntypical usage scenarios to be replicated in the different browsers, executed in\nthe same testing environment and conditions. The results of our study show that\nthere are significant differences in terms of energy consumption among the\nconsidered browsers. Furthermore, we conclude that some browsers are energy\nefficient in several user actions, but energy greedy in other ones, allowing us\nto conclude that no browser is universally more efficient for all usage\nscenarios.",
    "descriptor": "",
    "authors": [
      "N\u00e9lson Gon\u00e7alves",
      "Rui Rua",
      "J\u00e1come Cunha",
      "Rui Pereira",
      "Jo\u00e3o Saraiva"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11399"
  },
  {
    "id": "arXiv:2205.11400",
    "title": "Model Predictive Control of Non-Holonomic Vehicles: Beyond  Differential-Drive",
    "abstract": "Non-holonomic vehicles are of immense practical value and increasingly\nsubject to automation. However, controlling them accurately, e.g., when\nparking, is known to be challenging for automatic control methods, including\nmodel predictive control (MPC). Combining results from MPC theory and\nsub-Riemannian geometry in the form of homogeneous nilpotent system\napproximations, this paper proposes a comprehensive, ready-to-apply design\nprocedure for MPC controllers to steer controllable, driftless non-holonomic\nvehicles into given setpoints. It can be ascertained that the resulting\ncontrollers nominally asymptotically stabilize the setpoint for a large-enough\nprediction horizon. The design procedure is exemplarily applied to four\nvehicles, including the kinematic car and a differentially driven mobile robot\nwith up to two trailers. The controllers use a non-quadratic cost function\ntailored to the non-holonomic kinematics. Novelly, for the considered example\nvehicles, it is proven that a quadratic cost employed in an otherwise similar\ncontroller is insufficient to reliably asymptotically stabilize the closed\nloop. Since quadratic costs are the conventional choice in control, this\nhighlights the relevance of the findings. To the knowledge of the authors, it\nis the first time that MPC controllers of the proposed structure are applied to\nnon-holonomic vehicles beyond very simple ones, in particular (partly) on\nhardware.",
    "descriptor": "\nComments: 14 pages, 4 figures, 5 videos and 2 corresponding text files; corresponding author Henrik Ebel, henrik.ebel@itm.uni-stuttgart.de\n",
    "authors": [
      "Mario Rosenfelder",
      "Henrik Ebel",
      "Jasmin Krauspenhaar",
      "Peter Eberhard"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.11400"
  },
  {
    "id": "arXiv:2205.11402",
    "title": "Causal Machine Learning for Healthcare and Precision Medicine",
    "abstract": "Causal machine learning (CML) has experienced increasing popularity in\nhealthcare. Beyond the inherent capabilities of adding domain knowledge into\nlearning systems, CML provides a complete toolset for investigating how a\nsystem would react to an intervention (e.g.\\ outcome given a treatment).\nQuantifying effects of interventions allows actionable decisions to be made\nwhilst maintaining robustness in the presence of confounders. Here, we explore\nhow causal inference can be incorporated into different aspects of clinical\ndecision support (CDS) systems by using recent advances in machine learning.\nThroughout this paper, we use Alzheimer's disease (AD) to create examples for\nillustrating how CML can be advantageous in clinical scenarios. Furthermore, we\ndiscuss important challenges present in healthcare applications such as\nprocessing high-dimensional and unstructured data, generalisation to\nout-of-distribution samples, and temporal relationships, that despite the great\neffort from the research community remain to be solved. Finally, we review\nlines of research within causal representation learning, causal discovery and\ncausal reasoning which offer the potential towards addressing the\naforementioned challenges.",
    "descriptor": "\nComments: 20 pages, 4 figures, 1 table\n",
    "authors": [
      "Pedro Sanchez",
      "Jeremy P. Voisey",
      "Tian Xia",
      "Hannah I. Watson",
      "Alison Q. ONeil",
      "Sotirios A. Tsaftaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11402"
  },
  {
    "id": "arXiv:2205.11404",
    "title": "Variable-Input Deep Operator Networks",
    "abstract": "Existing architectures for operator learning require that the number and\nlocations of sensors (where the input functions are evaluated) remain the same\nacross all training and test samples, significantly restricting the range of\ntheir applicability. We address this issue by proposing a novel operator\nlearning framework, termed Variable-Input Deep Operator Network (VIDON), which\nallows for random sensors whose number and locations can vary across samples.\nVIDON is invariant to permutations of sensor locations and is proved to be\nuniversal in approximating a class of continuous operators. We also prove that\nVIDON can efficiently approximate operators arising in PDEs. Numerical\nexperiments with a diverse set of PDEs are presented to illustrate the robust\nperformance of VIDON in learning operators.",
    "descriptor": "",
    "authors": [
      "Michael Prasthofer",
      "Tim De Ryck",
      "Siddhartha Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11404"
  },
  {
    "id": "arXiv:2205.11406",
    "title": "A Model-Driven-Engineering Approach for Detecting Privilege Escalation  in IoT Systems",
    "abstract": "Software vulnerabilities in access control models can represent a serious\nthreat in a system. In fact, OWASP lists broken access control as number 5 in\nseverity among the top 10 vulnerabilities. In this paper, we study the\npermission model of an emerging Smart-Home platform, SmartThings, and explore\nan approach that detects privilege escalation in its permission model. Our\napproach is based on Model Driven Engineering (MDE) in addition to static\nanalysis. This approach allows for better coverage of privilege escalation\ndetection than static analysis alone, and takes advantage of analyzing\nfree-form text that carries extra permissions details. Our experimental results\ndemonstrate a very high accuracy for detecting over-privilege vulnerabilities\nin IoT applications",
    "descriptor": "",
    "authors": [
      "Atheer Abu Zaid",
      "Manar H. Alalfi",
      "Ali Miri"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11406"
  },
  {
    "id": "arXiv:2205.11409",
    "title": "Many-Class Text Classification with Matching",
    "abstract": "In this work, we formulate \\textbf{T}ext \\textbf{C}lassification as a\n\\textbf{M}atching problem between the text and the labels, and propose a simple\nyet effective framework named TCM. Compared with previous text classification\napproaches, TCM takes advantage of the fine-grained semantic information of the\nclassification labels, which helps distinguish each class better when the class\nnumber is large, especially in low-resource scenarios. TCM is also easy to\nimplement and is compatible with various large pretrained language models. We\nevaluate TCM on 4 text classification datasets (each with 20+ labels) in both\nfew-shot and full-data settings, and this model demonstrates significant\nimprovements over other text classification paradigms. We also conduct\nextensive experiments with different variants of TCM and discuss the underlying\nfactors of its success. Our method and analyses offer a new perspective on text\nclassification.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Yi Song",
      "Yuxian Gu",
      "Minlie Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11409"
  },
  {
    "id": "arXiv:2205.11412",
    "title": "Instance-Based Uncertainty Estimation for Gradient-Boosted Regression  Trees",
    "abstract": "We propose Instance-Based Uncertainty estimation for Gradient-boosted\nregression trees~(IBUG), a simple method for extending any GBRT point predictor\nto produce probabilistic predictions. IBUG computes a non-parametric\ndistribution around a prediction using the k-nearest training instances, where\ndistance is measured with a tree-ensemble kernel. The runtime of IBUG depends\non the number of training examples at each leaf in the ensemble, and can be\nimproved by sampling trees or training instances. Empirically, we find that\nIBUG achieves similar or better performance than the previous state-of-the-art\nacross 22 benchmark regression datasets. We also find that IBUG can achieve\nimproved probabilistic performance by using different base GBRT models, and can\nmore flexibly model the posterior distribution of a prediction than competing\nmethods. We also find that previous methods suffer from poor probabilistic\ncalibration on some datasets, which can be mitigated using a scalar factor\ntuned on the validation data.",
    "descriptor": "\nComments: 26 pages, 7 figures, 3 tables, and 3 algorithms\n",
    "authors": [
      "Jonathan Brophy",
      "Daniel Lowd"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11412"
  },
  {
    "id": "arXiv:2205.11413",
    "title": "QASem Parsing: Text-to-text Modeling of QA-based Semantics",
    "abstract": "Several recent works have suggested to represent semantic relations with\nquestions and answers, decomposing textual information into separate\ninterrogative natural language statements. In this paper, we consider three\nQA-based semantic tasks - namely, QA-SRL, QANom and QADiscourse, each targeting\na certain type of predication - and propose to regard them as jointly providing\na comprehensive representation of textual information. To promote this goal, we\ninvestigate how to best utilize the power of sequence-to-sequence (seq2seq)\npre-trained language models, within the unique setup of semi-structured\noutputs, consisting of an unordered set of question-answer pairs. We examine\ndifferent input and output linearization strategies, and assess the effect of\nmultitask learning and of simple data augmentation techniques in the setting of\nimbalanced training data. Consequently, we release the first unified QASem\nparsing tool, practical for downstream applications who can benefit from an\nexplicit, QA-based account of information units in a text.",
    "descriptor": "",
    "authors": [
      "Ayal Klein",
      "Eran Hirsch",
      "Ron Eliav",
      "Valentina Pyatkin",
      "Avi Caciularu",
      "Ido Dagan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11413"
  },
  {
    "id": "arXiv:2205.11416",
    "title": "The Importance of Being Parameters: An Intra-Distillation Method for  Serious Gains",
    "abstract": "Recent model pruning methods have demonstrated the ability to remove\nredundant parameters without sacrificing model performance. Common methods\nremove redundant parameters according to the parameter sensitivity, a\ngradient-based measure reflecting the contribution of the parameters. In this\npaper, however, we argue that redundant parameters can be trained to make\nbeneficial contributions. We first highlight the large sensitivity\n(contribution) gap among high-sensitivity and low-sensitivity parameters and\nshow that the model generalization performance can be significantly improved\nafter balancing the contribution of all parameters. Our goal is to balance the\nsensitivity of all parameters and encourage all of them to contribute equally.\nWe propose a general task-agnostic method, namely intra-distillation, appended\nto the regular training loss to balance parameter sensitivity. Moreover, we\nalso design a novel adaptive learning method to control the strength of\nintra-distillation loss for faster convergence. Our experiments show the strong\neffectiveness of our methods on machine translation, natural language\nunderstanding, and zero-shot cross-lingual transfer across up to 48 languages,\ne.g., a gain of 3.54 BLEU on average across 8 language pairs from the IWSLT'14\ntranslation dataset.",
    "descriptor": "",
    "authors": [
      "Haoran Xu",
      "Philipp Koehn",
      "Kenton Murray"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11416"
  },
  {
    "id": "arXiv:2205.11418",
    "title": "On non-monimial APcN permutations over finite fields of even  characteristic",
    "abstract": "Recently, a new concept called the $c$-differential uniformity was proposed\nby Ellingsen et al. (2020), which allows to simplify some types of differential\ncryptanalysis. Since then, finding functions having low $c$-differential\nuniformity has attracted the attention of many researchers. However it seems\nthat, at this moment, there are not many non-monomial permutations having low\n$c$-differential uniformity. In this paper, we propose new classes of almost\nperfect $c$-nonlinear non-monomial permutations over a binary field.",
    "descriptor": "",
    "authors": [
      "Jaeseong Jeong",
      "Namhun Koo",
      "Soonhak Kwon"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11418"
  },
  {
    "id": "arXiv:2205.11419",
    "title": "Enhanced Prototypical Learning for Unsupervised Domain Adaptation in  LiDAR Semantic Segmentation",
    "abstract": "Despite its importance, unsupervised domain adaptation (UDA) on LiDAR\nsemantic segmentation is a task that has not received much attention from the\nresearch community. Only recently, a completion-based 3D method has been\nproposed to tackle the problem and formally set up the adaptive scenarios.\nHowever, the proposed pipeline is complex, voxel-based and requires multi-stage\ninference, which inhibits it for real-time inference. We propose a range\nimage-based, effective and efficient method for solving UDA on LiDAR\nsegmentation. The method exploits class prototypes from the source domain to\npseudo label target domain pixels, which is a research direction showing good\nperformance in UDA for natural image semantic segmentation. Applying such\napproaches to LiDAR scans has not been considered because of the severe domain\nshift and lack of pre-trained feature extractor that is unavailable in the\nLiDAR segmentation setup. However, we show that proper strategies, including\nreconstruction-based pre-training, enhanced prototypes, and selective pseudo\nlabeling based on distance to prototypes, is sufficient enough to enable the\nuse of prototypical approaches. We evaluate the performance of our method on\nthe recently proposed LiDAR segmentation UDA scenarios. Our method achieves\nremarkable performance among contemporary methods.",
    "descriptor": "\nComments: accepted to IEEE International Conference on Robotics and Automation (ICRA2022) (7 pages, 1 figure, 4 tables)\n",
    "authors": [
      "Eojindl Yi",
      "Juyoung Yang",
      "Junmo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11419"
  },
  {
    "id": "arXiv:2205.11420",
    "title": "LILA-BOTI : Leveraging Isolated Letter Accumulations By Ordering Teacher  Insights for Bangla Handwriting Recognition",
    "abstract": "Word-level handwritten optical character recognition (OCR) remains a\nchallenge for morphologically rich languages like Bangla. The complexity arises\nfrom the existence of a large number of alphabets, the presence of several\ndiacritic forms, and the appearance of complex conjuncts. The difficulty is\nexacerbated by the fact that some graphemes occur infrequently but remain\nindispensable, so addressing the class imbalance is required for satisfactory\nresults. This paper addresses this issue by introducing two knowledge\ndistillation methods: Leveraging Isolated Letter Accumulations By Ordering\nTeacher Insights (LILA-BOTI) and Super Teacher LILA-BOTI. In both cases, a\nConvolutional Recurrent Neural Network (CRNN) student model is trained with the\ndark knowledge gained from a printed isolated character recognition teacher\nmodel. We conducted inter-dataset testing on \\emph{BN-HTRd} and\n\\emph{BanglaWriting} as our evaluation protocol, thus setting up a challenging\nproblem where the results would better reflect the performance on unseen data.\nOur evaluations achieved up to a 3.5% increase in the F1-Macro score for the\nminor classes and up to 4.5% increase in our overall word recognition rate when\ncompared with the base model (No KD) and conventional KD.",
    "descriptor": "\nComments: Accepted in ICPR2022\n",
    "authors": [
      "Md. Ismail Hossain",
      "Mohammed Rakib",
      "Sabbir Mollah",
      "Fuad Rahman",
      "Nabeel Mohammed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11420"
  },
  {
    "id": "arXiv:2205.11423",
    "title": "Decoder Denoising Pretraining for Semantic Segmentation",
    "abstract": "Semantic segmentation labels are expensive and time consuming to acquire.\nHence, pretraining is commonly used to improve the label-efficiency of\nsegmentation models. Typically, the encoder of a segmentation model is\npretrained as a classifier and the decoder is randomly initialized. Here, we\nargue that random initialization of the decoder can be suboptimal, especially\nwhen few labeled examples are available. We propose a decoder pretraining\napproach based on denoising, which can be combined with supervised pretraining\nof the encoder. We find that decoder denoising pretraining on the ImageNet\ndataset strongly outperforms encoder-only supervised pretraining. Despite its\nsimplicity, decoder denoising pretraining achieves state-of-the-art results on\nlabel-efficient semantic segmentation and offers considerable gains on the\nCityscapes, Pascal Context, and ADE20K datasets.",
    "descriptor": "",
    "authors": [
      "Emmanuel Brempong Asiedu",
      "Simon Kornblith",
      "Ting Chen",
      "Niki Parmar",
      "Matthias Minderer",
      "Mohammad Norouzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11423"
  },
  {
    "id": "arXiv:2205.11432",
    "title": "Logical Reasoning with Span Predictions: Span-level Logical Atoms for  Interpretable and Robust NLI Models",
    "abstract": "Current Natural Language Inference (NLI) models achieve impressive results,\nsometimes outperforming humans when evaluating on in-distribution test sets.\nHowever, as these models are known to learn from annotation artefacts and\ndataset biases, it is unclear to what extent the models are learning the task\nof NLI instead of learning from shallow heuristics in their training data. We\naddress this issue by introducing a logical reasoning framework for NLI,\ncreating highly transparent model decisions that are based on logical rules.\nUnlike prior work, we show that the improved interpretability can be achieved\nwithout decreasing the predictive accuracy. We almost fully retain performance\non SNLI while identifying the exact hypothesis spans that are responsible for\neach model prediction. Using the e-SNLI human explanations, we also verify that\nour model makes sensible decisions at a span level, despite not using any\nspan-level labels during training. We can further improve model performance and\nthe span-level decisions by using the e-SNLI explanations during training.\nFinally, our model outperforms its baseline in a reduced data setting. When\ntraining with only 100 examples, in-distribution performance improves by 18%,\nwhile out-of-distribution performance improves on SNLI-hard, MNLI-mismatched,\nMNLI-matched and SICK by 11%, 26%, 22%, and 21% respectively.",
    "descriptor": "",
    "authors": [
      "Joe Stacey",
      "Pasquale Minervini",
      "Haim Dubossarsky",
      "Marek Rei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11432"
  },
  {
    "id": "arXiv:2205.11433",
    "title": "Informed Pre-Training on Prior Knowledge",
    "abstract": "When training data is scarce, the incorporation of additional prior knowledge\ncan assist the learning process. While it is common to initialize neural\nnetworks with weights that have been pre-trained on other large data sets,\npre-training on more concise forms of knowledge has rather been overlooked. In\nthis paper, we propose a novel informed machine learning approach and suggest\nto pre-train on prior knowledge. Formal knowledge representations, e.g. graphs\nor equations, are first transformed into a small and condensed data set of\nknowledge prototypes. We show that informed pre-training on such knowledge\nprototypes (i) speeds up the learning processes, (ii) improves generalization\ncapabilities in the regime where not enough training data is available, and\n(iii) increases model robustness. Analyzing which parts of the model are\naffected most by the prototypes reveals that improvements come from deeper\nlayers that typically represent high-level features. This confirms that\ninformed pre-training can indeed transfer semantic knowledge. This is a novel\neffect, which shows that knowledge-based pre-training has additional and\ncomplementary strengths to existing approaches.",
    "descriptor": "",
    "authors": [
      "Laura von Rueden",
      "Sebastian Houben",
      "Kostadin Cvejoski",
      "Christian Bauckhage",
      "Nico Piatkowski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11433"
  },
  {
    "id": "arXiv:2205.11434",
    "title": "SiPRNet: End-to-End Learning for Single-Shot Phase Retrieval",
    "abstract": "Traditional optimization algorithms have been developed to deal with the\nphase retrieval problem. However, multiple measurements with different random\nor non-random masks are needed for giving a satisfactory performance. This\nbrings a burden to the implementation of the algorithms in practical systems.\nEven worse, expensive optical devices are required to implement the optical\nmasks. Recently, deep learning, especially convolutional neural networks (CNN),\nhas played important roles in various image reconstruction tasks. However,\ntraditional CNN structure fails to reconstruct the original images from their\nFourier measurements because of tremendous domain discrepancy. In this paper,\nwe design a novel CNN structure, named SiPRNet, to recover a signal from a\nsingle Fourier intensity measurement. To effectively utilize the spectral\ninformation of the measurements, we propose a new Multi-Layer Perception block\nembedded with the dropout layer to extract the global representations. Two\nUp-sampling and Reconstruction blocks with self-attention are utilized to\nrecover the signals from the extracted features. Extensive evaluations of the\nproposed model are performed using different testing datasets on both\nsimulation and optical experimentation platforms. The results demonstrate that\nthe proposed approach consistently outperforms other CNN-based and traditional\noptimization-based methods in single-shot maskless phase retrieval. The source\ncodes of the proposed method have been released on Github:\nhttps://github.com/Qiustander/SiPRNet.",
    "descriptor": "",
    "authors": [
      "Qiuliang Ye",
      "Li-Wen Wang",
      "Daniel P.K. Lun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2205.11434"
  },
  {
    "id": "arXiv:2205.11438",
    "title": "Contrastive Representation Learning for Cross-Document Coreference  Resolution of Events and Entities",
    "abstract": "Identifying related entities and events within and across documents is\nfundamental to natural language understanding. We present an approach to entity\nand event coreference resolution utilizing contrastive representation learning.\nEarlier state-of-the-art methods have formulated this problem as a binary\nclassification problem and leveraged large transformers in a cross-encoder\narchitecture to achieve their results. For large collections of documents and\ncorresponding set of $n$ mentions, the necessity of performing $n^{2}$\ntransformer computations in these earlier approaches can be computationally\nintensive. We show that it is possible to reduce this burden by applying\ncontrastive learning techniques that only require $n$ transformer computations\nat inference time. Our method achieves state-of-the-art results on a number of\nkey metrics on the ECB+ corpus and is competitive on others.",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Benjamin Hsu",
      "Graham Horwood"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11438"
  },
  {
    "id": "arXiv:2205.11443",
    "title": "Unsupervised Tokenization Learning",
    "abstract": "In the presented study, we discover that so called \"transition freedom\"\nmetric appears superior for unsupervised tokenization purposes, compared to\nstatistical metrics such as mutual information and conditional probability,\nproviding F-measure scores in range from 0.71 to 1.0 across explored corpora.\nWe find that different languages require different derivatives of that metric\n(such as variance and \"peak values\") for successful tokenization. Larger\ntraining corpora does not necessarily effect in better tokenization quality,\nwhile compacting the models eliminating statistically weak evidence tends to\nimprove performance. Proposed unsupervised tokenization technique provides\nquality better or comparable to lexicon-based one, depending on the language.",
    "descriptor": "\nComments: 10 pages, 10 figures\n",
    "authors": [
      "Anton Kolonin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2205.11443"
  },
  {
    "id": "arXiv:2205.11448",
    "title": "Data augmentation for efficient learning from parametric experts",
    "abstract": "We present a simple, yet powerful data-augmentation technique to enable\ndata-efficient learning from parametric experts for reinforcement and imitation\nlearning. We focus on what we call the policy cloning setting, in which we use\nonline or offline queries of an expert or expert policy to inform the behavior\nof a student policy. This setting arises naturally in a number of problems, for\ninstance as variants of behavior cloning, or as a component of other algorithms\nsuch as DAGGER, policy distillation or KL-regularized RL. Our approach,\naugmented policy cloning (APC), uses synthetic states to induce\nfeedback-sensitivity in a region around sampled trajectories, thus dramatically\nreducing the environment interactions required for successful cloning of the\nexpert. We achieve highly data-efficient transfer of behavior from an expert to\na student policy for high-degrees-of-freedom control problems. We demonstrate\nthe benefit of our method in the context of several existing and widely used\nalgorithms that include policy cloning as a constituent part. Moreover, we\nhighlight the benefits of our approach in two practically relevant settings (a)\nexpert compression, i.e. transfer to a student with fewer parameters; and (b)\ntransfer from privileged experts, i.e. where the expert has a different\nobservation space than the student, usually including access to privileged\ninformation.",
    "descriptor": "",
    "authors": [
      "Alexandre Galashov",
      "Josh Merel",
      "Nicolas Heess"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11448"
  },
  {
    "id": "arXiv:2205.11449",
    "title": "Nancy: an efficient parallel Network Calculus library",
    "abstract": "This paper describes Nancy, a Network Calculus (NC) library that allows users\nto perform complex min-plus and max-plus algebra operations efficiently. To the\nbest of our knowledge, Nancy is the only open-source library that implements\noperators working on arbitrary piecewise-linear functions (as opposed to only\nconcave/convex ones), as well as to implement some of them (e.g. sub-additive\nclosure and function composition). Nancy allows researchers to compute NC\nresults using a straightforward syntax, which matches the algebraic one.\nMoreover, it is designed having computational efficiency in mind: it exploits\nclever data representation, it uses inheritance to allow for faster algorithms\nwhen they are available (e.g., for specific subclasses of functions), and it is\nnatively parallel, thus reaping the benefit of multicore hardware. This makes\nit usable to solve NC problems which were previously considered beyond the\nrealm of tractable.",
    "descriptor": "\nComments: Preprint submitted to Software Impacts\n",
    "authors": [
      "Raffaele Zippo",
      "Giovanni Stea"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.11449"
  },
  {
    "id": "arXiv:2205.11454",
    "title": "What is Your Metric Telling You? Evaluating Classifier Calibration under  Context-Specific Definitions of Reliability",
    "abstract": "Classifier calibration has received recent attention from the machine\nlearning community due both to its practical utility in facilitating decision\nmaking, as well as the observation that modern neural network classifiers are\npoorly calibrated. Much of this focus has been towards the goal of learning\nclassifiers such that their output with largest magnitude (the \"predicted\nclass\") is calibrated. However, this narrow interpretation of classifier\noutputs does not adequately capture the variety of practical use cases in which\nclassifiers can aid in decision making. In this work, we argue that more\nexpressive metrics must be developed that accurately measure calibration error\nfor the specific context in which a classifier will be deployed. To this end,\nwe derive a number of different metrics using a generalization of Expected\nCalibration Error (ECE) that measure calibration error under different\ndefinitions of reliability. We then provide an extensive empirical evaluation\nof commonly used neural network architectures and calibration techniques with\nrespect to these metrics. We find that: 1) definitions of ECE that focus solely\non the predicted class fail to accurately measure calibration error under a\nselection of practically useful definitions of reliability and 2) many common\ncalibration techniques fail to improve calibration performance uniformly across\nECE metrics derived from these diverse definitions of reliability.",
    "descriptor": "\nComments: Accepted in the ICLR 2022 Machine Learning Evaluation Standards Workshop\n",
    "authors": [
      "John Kirchenbauer",
      "Jacob Oaks",
      "Eric Heim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11454"
  },
  {
    "id": "arXiv:2205.11456",
    "title": "Multilingual Extraction and Categorization of Lexical Collocations with  Graph-aware Transformers",
    "abstract": "Recognizing and categorizing lexical collocations in context is useful for\nlanguage learning, dictionary compilation and downstream NLP. However, it is a\nchallenging task due to the varying degrees of frozenness lexical collocations\nexhibit. In this paper, we put forward a sequence tagging BERT-based model\nenhanced with a graph-aware transformer architecture, which we evaluate on the\ntask of collocation recognition in context. Our results suggest that explicitly\nencoding syntactic dependencies in the model architecture is helpful, and\nprovide insights on differences in collocation typification in English, Spanish\nand French.",
    "descriptor": "\nComments: Accepted to *SEM2022\n",
    "authors": [
      "Luis Espinosa-Anke",
      "Alexander Shvets",
      "Alireza Mohammadshahi",
      "James Henderson",
      "Leo Wanner"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11456"
  },
  {
    "id": "arXiv:2205.11458",
    "title": "Groundhog: Efficient Request Isolation in FaaS",
    "abstract": "Security is a core responsibility for Function-as-a-Service (FaaS) providers.\nThe prevailing approach has each function execute in its own container to\nisolate concurrent executions of different functions. However, successive\ninvocations of the same function commonly reuse the runtime state of a previous\ninvocation in order to avoid container cold-start delays when invoking a\nfunction. Although efficient, this container reuse has security implications\nfor functions that are invoked on behalf of differently privileged users or\nadministrative domains: bugs in a function's implementation, third-party\nlibrary, or the language runtime may leak private data from one invocation of\nthe function to subsequent invocations of the same function.\nGroundhog isolates sequential invocations of a function by efficiently\nreverting to a clean state, free from any private data, after each invocation.\nThe system exploits two properties of typical FaaS platforms: each container\nexecutes at most one function at a time and legitimate functions do not retain\nstate across invocations. This enables Groundhog to efficiently snapshot and\nrestore function state between invocations in a manner that is independent of\nthe programming language/runtime and does not require any changes to existing\nfunctions, libraries, language runtimes, or OS kernels. We describe the design\nof Groundhog and its implementation in OpenWhisk, a popular production-grade\nopen-source FaaS framework. On three existing benchmark suites, Groundhog\nisolates sequential invocations with modest overhead on end-to-end latency\n(median: 1.5%, 95p: 7%) and throughput (median: 2.5%, 95p: 49.6%), relative to\nan insecure baseline that reuses the container and runtime state.",
    "descriptor": "",
    "authors": [
      "Mohamed Alzayat",
      "Jonathan Mace",
      "Peter Druschel",
      "Deepak Garg"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11458"
  },
  {
    "id": "arXiv:2205.11459",
    "title": "CELEST: Federated Learning for Globally Coordinated Threat Detection",
    "abstract": "The cyber-threat landscape has evolved tremendously in recent years, with new\nthreat variants emerging daily, and large-scale coordinated campaigns becoming\nmore prevalent. In this study, we propose CELEST (CollaborativE LEarning for\nScalable Threat detection), a federated machine learning framework for global\nthreat detection over HTTP, which is one of the most commonly used protocols\nfor malware dissemination and communication. CELEST leverages federated\nlearning in order to collaboratively train a global model across multiple\nclients who keep their data locally, thus providing increased privacy and\nconfidentiality assurances. Through a novel active learning component\nintegrated with the federated learning technique, our system continuously\ndiscovers and learns the behavior of new, evolving, and globally-coordinated\ncyber threats. We show that CELEST is able to expose attacks that are largely\ninvisible to individual organizations. For instance, in one challenging attack\nscenario with data exfiltration malware, the global model achieves a three-fold\nincrease in Precision-Recall AUC compared to the local model. We deploy CELEST\non two university networks and show that it is able to detect the malicious\nHTTP communication with high precision and low false positive rates.\nFurthermore, during its deployment, CELEST detected a set of previously unknown\n42 malicious URLs and 20 malicious domains in one day, which were confirmed to\nbe malicious by VirusTotal.",
    "descriptor": "",
    "authors": [
      "Talha Ongun",
      "Simona Boboila",
      "Alina Oprea",
      "Tina Eliassi-Rad",
      "Jason Hiser",
      "Jack Davidson"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11459"
  },
  {
    "id": "arXiv:2205.11460",
    "title": "Graph-theoretical approach to robust 3D normal extraction of LiDAR data",
    "abstract": "Low dimensional primitive feature extraction from LiDAR point clouds (such as\nplanes) forms the basis of majority of LiDAR data processing tasks. A major\nchallenge in LiDAR data analysis arises from the irregular nature of LiDAR data\nthat forces practitioners to either regularize the data using some form of\ngridding or utilize a triangular mesh such as triangulated irregular network\n(TIN). While there have been a handful applications using LiDAR data as a\nconnected graph, a principled treatment of utilizing graph-theoretical approach\nfor LiDAR data modelling is still lacking. In this paper, we try to bridge this\ngap by utilizing graphical approach for normal estimation from LiDAR point\nclouds. We formulate the normal estimation problem in an optimization\nframework, where we find the corresponding normal vector for each LiDAR point\nby utilizing its nearest neighbors and simultaneously enforcing a graph\nsmoothness assumption based on point samples. This is a non-linear constrained\nconvex optimization problem which can then be solved using projected conjugate\ngradient descent to yield an unique solution. As an enhancement to our\noptimization problem, we also provide different weighted solutions based on the\ndot product of the normals and Euclidean distance between the points. In order\nto assess the performance of our proposed normal extraction method and\nweighting strategies, we first provide a detailed analysis on repeated randomly\ngenerated datasets with four different noise levels and four different tuning\nparameters. Finally, we benchmark our proposed method against existing\nstate-of-the-art approaches on a large scale synthetic plane extraction\ndataset. The code for the proposed approach along with the simulations and\nbenchmarking is available at\nhttps://github.com/arpan-kusari/graph-plane-extraction-simulation.",
    "descriptor": "\nComments: Published at ISPRS Annals of Photogrammetry and Remote Sensing\n",
    "authors": [
      "Arpan Kusari",
      "Wenbo Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.11460"
  },
  {
    "id": "arXiv:2205.11461",
    "title": "The Undecidability of Network Coding, Conditional Information  Inequalities, and Conditional Independence Implication",
    "abstract": "We resolve three long-standing open problems, namely the (algorithmic)\ndecidability of network coding, the decidability of conditional information\ninequalities, and the decidability of conditional independence implication\namong random variables, by showing that these problems are undecidable. The\nproof is composed of a construction inspired by Herrmann's arguments on\nembedded multivalued database dependencies, together with a novel construction\ncalled the Fano-non-Fano network based on the Fano matroid and the non-Fano\nmatroid.",
    "descriptor": "\nComments: 18 pages, 7 figures\n",
    "authors": [
      "Cheuk Ting Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.11461"
  },
  {
    "id": "arXiv:2205.11463",
    "title": "Context Limitations Make Neural Language Models More Human-Like",
    "abstract": "Do modern natural language processing (NLP) models exhibit human-like\nlanguage processing? How can they be made more human-like? These questions are\nmotivated by psycholinguistic studies for understanding human language\nprocessing as well as engineering efforts. In this study, we demonstrate the\ndiscrepancies in context access between modern neural language models (LMs) and\nhumans in incremental sentence processing. Additional context limitation was\nneeded to make LMs better simulate human reading behavior. Our analyses also\nshowed that human-LM gaps in memory access are associated with specific\nsyntactic constructions; incorporating additional syntactic factors into LMs'\ncontext access could enhance their cognitive plausibility.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Tatsuki Kuribayashi",
      "Yohei Oseki",
      "Ana Brassard",
      "Kentaro Inui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11463"
  },
  {
    "id": "arXiv:2205.11465",
    "title": "SQuALITY: Building a Long-Document Summarization Dataset the Hard Way",
    "abstract": "Summarization datasets are often assembled either by scraping naturally\noccurring public-domain summaries -- which are nearly always in\ndifficult-to-work-with technical domains-- or by using approximate heuristics\nto extract them from everyday text -- which frequently yields unfaithful\nsummaries. In this work, we turn to a slower but more straightforward approach\nto developing summarization benchmark data: We hire highly-qualified\ncontractors to read stories and write original summaries from scratch. To\namortize reading time, we collect five summaries per document, with the first\ngiving an overview and the subsequent four addressing specific questions. We\nuse this protocol to collect SQuALITY, a dataset of question-focused summaries\nbuilt on the same public-domain short stories as the multiple-choice dataset\nQuALITY (Pang et al., 2021). Experiments with state-of-the-art summarization\nsystems show that our dataset is challenging and that existing automatic\nevaluation metrics are weak indicators of quality.",
    "descriptor": "",
    "authors": [
      "Alex Wang",
      "Richard Yuanzhe Pang",
      "Angelica Chen",
      "Jason Phang",
      "Samuel R. Bowman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11465"
  },
  {
    "id": "arXiv:2205.11467",
    "title": "A Question-Answer Driven Approach to Reveal Affirmative Interpretations  from Verbal Negations",
    "abstract": "This paper explores a question-answer driven approach to reveal affirmative\ninterpretations from verbal negations (i.e., when a negation cue grammatically\nmodifies a verb). We create a new corpus consisting of 4,472 verbal negations\nand discover that 67.1% of them convey that an event actually occurred.\nAnnotators generate and answer 7,277 questions for the 3,001 negations that\nconvey an affirmative interpretation. We first cast the problem of revealing\naffirmative interpretations from negations as a natural language inference\n(NLI) classification task. Experimental results show that state-of-the-art\ntransformers trained with existing NLI corpora are insufficient to reveal\naffirmative interpretations. We also observe, however, that fine-tuning brings\nsmall improvements. In addition to NLI classification, we also explore the more\nrealistic task of generating affirmative interpretations directly from\nnegations with the T5 transformer. We conclude that the generation task remains\na challenge as T5 substantially underperforms humans.",
    "descriptor": "\nComments: Accepted at the Findings of NAACL 2022\n",
    "authors": [
      "Md Mosharaf Hossain",
      "Luke Holman",
      "Anusha Kakileti",
      "Tiffany Iris Kao",
      "Nathan Raul Brito",
      "Aaron Abraham Mathews",
      "Eduardo Blanco"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11467"
  },
  {
    "id": "arXiv:2205.11469",
    "title": "Advanced Transient Diagnostic with Ensemble Digital Twin Modeling",
    "abstract": "The use of machine learning (ML) model as digital-twins for\nreduced-order-modeling (ROM) in lieu of system codes has grown traction over\nthe past few years. However, due to the complex and non-linear nature of\nnuclear reactor transients as well as the large range of tasks required, it is\ninfeasible for a single ML model to generalize across all tasks. In this paper,\nwe incorporate issue specific digital-twin ML models with ensembles to enhance\nthe prediction outcome. The ensemble also utilizes an indirect probabilistic\ntracking method of surrogate state variables to produce accurate predictions of\nunobservable safety goals. The unique method named Ensemble Diagnostic\nDigital-twin Modeling (EDDM) can select not only the most appropriate\npredictions from the incorporated diagnostic digital-twin models but can also\nreduce generalization error associated with training as opposed to single\nmodels.",
    "descriptor": "\nComments: 9 pages, 4 figures, 3 tables, presented in the American Nuclear Society Mathematics and Computation 2021 Annual Conference\n",
    "authors": [
      "Edward Chen",
      "Linyu Lin",
      "Nam T. Dinh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11469"
  },
  {
    "id": "arXiv:2205.11470",
    "title": "Exploiting the Curvature of Feasible Sets for Faster Projection-Free  Online Learning",
    "abstract": "In this paper, we develop new efficient projection-free algorithms for Online\nConvex Optimization (OCO). Online Gradient Descent (OGD) is an example of a\nclassical OCO algorithm that guarantees the optimal $O(\\sqrt{T})$ regret bound.\nHowever, OGD and other projection-based OCO algorithms need to perform a\nEuclidean projection onto the feasible set $\\mathcal{C}\\subset \\mathbb{R}^d$\nwhenever their iterates step outside $\\mathcal{C}$. For various sets of\ninterests, this projection step can be computationally costly, especially when\nthe ambient dimension is large. This has motivated the development of\nprojection-free OCO algorithms that swap Euclidean projections for often much\ncheaper operations such as Linear Optimization (LO). However, state-of-the-art\nLO-based algorithms only achieve a suboptimal $O(T^{3/4})$ regret for general\nOCO. In this paper, we leverage recent results in parameter-free Online\nLearning, and develop an OCO algorithm that makes two calls to an LO Oracle per\nround and achieves the near-optimal $\\widetilde{O}(\\sqrt{T})$ regret whenever\nthe feasible set is strongly convex. We also present an algorithm for general\nconvex sets that makes $\\widetilde O(d)$ expected number of calls to an LO\nOracle per round and guarantees a $\\widetilde O(T^{2/3})$ regret, improving on\nthe previous best $O(T^{3/4})$. We achieve the latter by approximating any\nconvex set $\\mathcal{C}$ by a strongly convex one, where LO can be performed\nusing $\\widetilde {O}(d)$ expected number of calls to an LO Oracle for\n$\\mathcal{C}$.",
    "descriptor": "",
    "authors": [
      "Zakaria Mhammedi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11470"
  },
  {
    "id": "arXiv:2205.11472",
    "title": "On the Effect of Sample and Topic Sizes for Argument Mining Datasets",
    "abstract": "The task of Argument Mining, that is extracting argumentative sentences for a\nspecific topic from large document sources, is an inherently difficult task for\nmachine learning models and humans alike, as large datasets are rare and\nrecognition of argumentative sentences requires expert knowledge. The task\nbecomes even more difficult when it also involves stance detection of retrieved\narguments. Recent datasets for the task tend to grow evermore large and hence\nmore costly. In this work, we inquire whether it is necessary for acceptable\nperformance of argument mining to have datasets growing in size or, if not, how\nsmaller datasets have to be composed for optimal performance. We also publish a\nnewly created dataset for future benchmarking.",
    "descriptor": "",
    "authors": [
      "Benjamin Schiller",
      "Johannes Daxenberger",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11472"
  },
  {
    "id": "arXiv:2205.11473",
    "title": "Rethinking Streaming Machine Learning Evaluation",
    "abstract": "While most work on evaluating machine learning (ML) models focuses on\ncomputing accuracy on batches of data, tracking accuracy alone in a streaming\nsetting (i.e., unbounded, timestamp-ordered datasets) fails to appropriately\nidentify when models are performing unexpectedly. In this position paper, we\ndiscuss how the nature of streaming ML problems introduces new real-world\nchallenges (e.g., delayed arrival of labels) and recommend additional metrics\nto assess streaming ML performance.",
    "descriptor": "\nComments: ML Evaluation Standards Workshop (ICLR 2022)\n",
    "authors": [
      "Shreya Shankar",
      "Bernease Herman",
      "Aditya G. Parameswaran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11473"
  },
  {
    "id": "arXiv:2205.11474",
    "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero  Outlier Images",
    "abstract": "Traditionally anomaly detection (AD) is treated as an unsupervised problem\nutilizing only normal samples due to the intractability of characterizing\neverything that looks unlike the normal data. However, it has recently been\nfound that unsupervised image anomaly detection can be drastically improved\nthrough the utilization of huge corpora of random images to represent\nanomalousness; a technique which is known as Outlier Exposure. In this paper we\nshow that specialized AD learning methods seem actually superfluous and huge\ncorpora of data expendable. For a common AD benchmark on ImageNet, standard\nclassifiers and semi-supervised one-class methods trained to discern between\nnormal samples and just a few random natural images are able to outperform the\ncurrent state of the art in deep AD, and only one useful outlier sample is\nsufficient to perform competitively. We investigate this phenomenon and reveal\nthat one-class methods are more robust towards the particular choice of\ntraining outliers. Furthermore, we find that a simple classifier based on\nrepresentations from CLIP, a recent foundation model, achieves state-of-the-art\nresults on CIFAR-10 and also outperforms all previous AD methods on ImageNet\nwithout any training samples (i.e., in a zero-shot setting).",
    "descriptor": "\nComments: 37 pages, preprint. arXiv admin note: text overlap with arXiv:2006.00339\n",
    "authors": [
      "Philipp Liznerski",
      "Lukas Ruff",
      "Robert A. Vandermeulen",
      "Billy Joe Franks",
      "Klaus-Robert M\u00fcller",
      "Marius Kloft"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11474"
  },
  {
    "id": "arXiv:2205.11482",
    "title": "Tracing Knowledge in Language Models Back to the Training Data",
    "abstract": "Neural language models (LMs) have been shown to memorize a great deal of\nfactual knowledge. But when an LM generates an assertion, it is often difficult\nto determine where it learned this information and whether it is true. In this\npaper, we introduce a new benchmark for fact tracing: tracing language models'\nassertions back to the training examples that provided evidence for those\npredictions. Prior work has suggested that dataset-level \\emph{influence\nmethods} might offer an effective framework for tracing predictions back to\ntraining data. However, such methods have not been evaluated for fact tracing,\nand researchers primarily have studied them through qualitative analysis or as\na data cleaning technique for classification/regression tasks. We present the\nfirst experiments that evaluate influence methods for fact tracing, using\nwell-understood information retrieval (IR) metrics. We compare two popular\nfamilies of influence methods -- gradient-based and embedding-based -- and show\nthat neither can fact-trace reliably; indeed, both methods fail to outperform\nan IR baseline (BM25) that does not even access the LM. We explore \\emph{why}\nthis occurs (e.g., gradient saturation) and demonstrate that existing influence\nmethods must be improved significantly before they can reliably attribute\nfactual predictions in LMs.",
    "descriptor": "\nComments: 14 pages, 5 Tables, 5 Figures\n",
    "authors": [
      "Ekin Aky\u00fcrek",
      "Tolga Bolukbasi",
      "Frederick Liu",
      "Binbin Xiong",
      "Ian Tenney",
      "Jacob Andreas",
      "Kelvin Guu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11482"
  },
  {
    "id": "arXiv:2205.11483",
    "title": "Learning differential equations from data",
    "abstract": "Differential equations are used to model problems that originate in\ndisciplines such as physics, biology, chemistry, and engineering. In recent\ntimes, due to the abundance of data, there is an active search for data-driven\nmethods to learn Differential equation models from data. However, many\nnumerical methods often fall short. Advancements in neural networks and deep\nlearning, have motivated a shift towards data-driven deep learning methods of\nlearning differential equations from data. In this work, we propose a\nforward-Euler based neural network model and test its performance by learning\nODEs such as the FitzHugh-Nagumo equations from data using different number of\nhidden layers and different neural network width.",
    "descriptor": "",
    "authors": [
      "K. D. Olumoyin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.11483"
  },
  {
    "id": "arXiv:2205.11484",
    "title": "Towards Automated Document Revision: Grammatical Error Correction,  Fluency Edits, and Beyond",
    "abstract": "Natural language processing technology has rapidly improved automated\ngrammatical error correction tasks, and the community begins to explore\ndocument-level revision as one of the next challenges. To go beyond\nsentence-level automated grammatical error correction to NLP-based\ndocument-level revision assistant, there are two major obstacles: (1) there are\nfew public corpora with document-level revisions being annotated by\nprofessional editors, and (2) it is not feasible to elicit all possible\nreferences and evaluate the quality of revision with such references because\nthere are infinite possibilities of revision. This paper tackles these\nchallenges. First, we introduce a new document-revision corpus, TETRA, where\nprofessional editors revised academic papers sampled from the ACL anthology\nwhich contain few trivial grammatical errors that enable us to focus more on\ndocument- and paragraph-level edits such as coherence and consistency. Second,\nwe explore reference-less and interpretable methods for meta-evaluation that\ncan detect quality improvements by document revision. We show the uniqueness of\nTETRA compared with existing document revision corpora and demonstrate that a\nfine-tuned pre-trained language model can discriminate the quality of documents\nafter revision even when the difference is subtle. This promising result will\nencourage the community to further explore automated document revision models\nand metrics in future.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Masato Mita",
      "Keisuke Sakaguchi",
      "Masato Hagiwara",
      "Tomoya Mizumoto",
      "Jun Suzuki",
      "Kentaro Inui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11484"
  },
  {
    "id": "arXiv:2205.11485",
    "title": "Conditional Supervised Contrastive Learning for Fair Text Classification",
    "abstract": "Contrastive representation learning has gained much attention due to its\nsuperior performance in learning representations from both image and sequential\ndata. However, the learned representations could potentially lead to\nperformance disparities in downstream tasks, such as increased silencing of\nunderrepresented groups in toxicity comment classification. In light of this\nchallenge, in this work, we study learning fair representations that satisfy a\nnotion of fairness known as equalized odds for text classification via\ncontrastive learning. Specifically, we first theoretically analyze the\nconnections between learning representations with fairness constraint and\nconditional supervised contrastive objectives. Inspired by our theoretical\nfindings, we propose to use conditional supervised contrastive objectives to\nlearn fair representations for text classification. We conduct experiments on\ntwo text datasets to demonstrate the effectiveness of our approaches in\nbalancing the trade-offs between task performance and bias mitigation among\nexisting baselines for text classification. Furthermore, we also show that the\nproposed methods are stable in different hyperparameter settings.",
    "descriptor": "",
    "authors": [
      "Jianfeng Chi",
      "William Shand",
      "Yaodong Yu",
      "Kai-Wei Chang",
      "Han Zhao",
      "Yuan Tian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11485"
  },
  {
    "id": "arXiv:2205.11487",
    "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language  Understanding",
    "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented\ndegree of photorealism and a deep level of language understanding. Imagen\nbuilds on the power of large transformer language models in understanding text\nand hinges on the strength of diffusion models in high-fidelity image\ngeneration. Our key discovery is that generic large language models (e.g. T5),\npretrained on text-only corpora, are surprisingly effective at encoding text\nfor image synthesis: increasing the size of the language model in Imagen boosts\nboth sample fidelity and image-text alignment much more than increasing the\nsize of the image diffusion model. Imagen achieves a new state-of-the-art FID\nscore of 7.27 on the COCO dataset, without ever training on COCO, and human\nraters find Imagen samples to be on par with the COCO data itself in image-text\nalignment. To assess text-to-image models in greater depth, we introduce\nDrawBench, a comprehensive and challenging benchmark for text-to-image models.\nWith DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,\nLatent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen\nover other models in side-by-side comparisons, both in terms of sample quality\nand image-text alignment. See https://imagen.research.google/ for an overview\nof the results.",
    "descriptor": "",
    "authors": [
      "Chitwan Saharia",
      "William Chan",
      "Saurabh Saxena",
      "Lala Li",
      "Jay Whang",
      "Emily Denton",
      "Seyed Kamyar Seyed Ghasemipour",
      "Burcu Karagol Ayan",
      "S. Sara Mahdavi",
      "Rapha Gontijo Lopes",
      "Tim Salimans",
      "Jonathan Ho",
      "David J Fleet",
      "Mohammad Norouzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11487"
  },
  {
    "id": "arXiv:2205.11490",
    "title": "Local Byte Fusion for Neural Machine Translation",
    "abstract": "Subword tokenization schemes are the dominant technique used in current NLP\nmodels. However, such schemes can be rigid and tokenizers built on one corpus\ndo not adapt well to other parallel corpora. It has also been observed that in\nmultilingual corpora, subword tokenization schemes over-segment low-resource\nlanguages leading to a drop in translation performance. A simple alternative to\nsubword tokenizers is byte-based methods i.e. tokenization into byte sequences\nusing encoding schemes such as UTF-8. Byte tokens often represent inputs at a\nsub-character granularity i.e. one character can be represented by a sequence\nof multiple byte tokens. This results in byte sequences that are significantly\nlonger than character sequences. Enforcing aggregation of local information in\nthe lower layers can guide the model to build higher-level semantic\ninformation. We propose a Local Byte Fusion (LOBEF) method for byte-based\nmachine translation -- utilizing byte $n$-gram and word boundaries -- to\naggregate local semantic information. Extensive experiments on multilingual\ntranslation, zero-shot cross-lingual transfer, and domain adaptation reveal a\nconsistent improvement over traditional byte-based models and even over subword\ntechniques. Further analysis also indicates that our byte-based models are\nparameter-efficient and can be trained faster than subword models.",
    "descriptor": "",
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Xiangpeng Wan",
      "Yu Cheng",
      "Junjie Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11490"
  },
  {
    "id": "arXiv:2205.11491",
    "title": "HyperTree Proof Search for Neural Theorem Proving",
    "abstract": "We propose an online training procedure for a transformer-based automated\ntheorem prover. Our approach leverages a new search algorithm, HyperTree Proof\nSearch (HTPS), inspired by the recent success of AlphaZero. Our model learns\nfrom previous proof searches through online training, allowing it to generalize\nto domains far from the training distribution. We report detailed ablations of\nour pipeline's main components by studying performance on three environments of\nincreasing complexity. In particular, we show that with HTPS alone, a model\ntrained on annotated proofs manages to prove 65.4% of a held-out set of\nMetamath theorems, significantly outperforming the previous state of the art of\n56.5% by GPT-f. Online training on these unproved theorems increases accuracy\nto 82.6%. With a similar computational budget, we improve the state of the art\non the Lean-based miniF2F-curriculum dataset from 31% to 42% proving accuracy.",
    "descriptor": "",
    "authors": [
      "Guillaume Lample",
      "Marie-Anne Lachaux",
      "Thibaut Lavril",
      "Xavier Martinet",
      "Amaury Hayat",
      "Gabriel Ebner",
      "Aur\u00e9lien Rodriguez",
      "Timoth\u00e9e Lacroix"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11491"
  },
  {
    "id": "arXiv:2205.11495",
    "title": "Flexible Diffusion Modeling of Long Videos",
    "abstract": "We present a framework for video modeling based on denoising diffusion\nprobabilistic models that produces long-duration video completions in a variety\nof realistic environments. We introduce a generative model that can at\ntest-time sample any arbitrary subset of video frames conditioned on any other\nsubset and present an architecture adapted for this purpose. Doing so allows us\nto efficiently compare and optimize a variety of schedules for the order in\nwhich frames in a long video are sampled and use selective sparse and\nlong-range conditioning on previously sampled frames. We demonstrate improved\nvideo modeling over prior work on a number of datasets and sample temporally\ncoherent videos over 25 minutes in length. We additionally release a new video\nmodeling dataset and semantically meaningful metrics based on videos generated\nin the CARLA self-driving car simulator.",
    "descriptor": "\nComments: 17 pages, 12 figures\n",
    "authors": [
      "William Harvey",
      "Saeid Naderiparizi",
      "Vaden Masrani",
      "Christian Weilbach",
      "Frank Wood"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11495"
  },
  {
    "id": "arXiv:2205.11498",
    "title": "Domain Adaptation for Memory-Efficient Dense Retrieval",
    "abstract": "Dense retrievers encode documents into fixed dimensional embeddings. However,\nstoring all the document embeddings within an index produces bulky indexes\nwhich are expensive to serve. Recently, BPR (Yamada et al., 2021) and JPQ (Zhan\net al., 2021a) have been proposed which train the model to produce binary\ndocument vectors, which reduce the index 32x and more. The authors showed these\nbinary embedding models significantly outperform more traditional index\ncompression techniques like Product Quantization (PQ). Previous work evaluated\nthese approaches just in-domain, i.e. the methods were evaluated on tasks for\nwhich training data is available. In practice, retrieval models are often used\nin an out-of-domain setting, where they have been trained on a publicly\navailable dataset, like MS MARCO, but are then used for some custom dataset for\nwhich no training data is available.\nIn this work, we show that binary embedding models like BPR and JPQ can\nperform significantly worse than baselines once there is a domain-shift\ninvolved. We propose a modification to the training procedure of BPR and JPQ\nand combine it with a corpus specific generative procedure which allow the\nadaptation of BPR and JPQ to any corpus without requiring labeled training\ndata. Our domain-adapted strategy known as GPL is model agnostic, achieves an\nimprovement by up-to 19.3 and 11.6 points in nDCG@10 across the BEIR benchmark\nin comparison to BPR and JPQ while maintaining its 32x memory efficiency.\nJPQ+GPL even outperforms our upper baseline: uncompressed TAS-B model on\naverage by 2.0 points.",
    "descriptor": "",
    "authors": [
      "Nandan Thakur",
      "Nils Reimers",
      "Jimmy Lin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11498"
  },
  {
    "id": "arXiv:2205.11501",
    "title": "VQA-GNN: Reasoning with Multimodal Semantic Graph for Visual Question  Answering",
    "abstract": "Visual understanding requires seamless integration between recognition and\nreasoning: beyond image-level recognition (e.g., detecting objects), systems\nmust perform concept-level reasoning (e.g., inferring the context of objects\nand intents of people). However, existing methods only model the image-level\nfeatures, and do not ground them and reason with background concepts such as\nknowledge graphs (KGs). In this work, we propose a novel visual question\nanswering method, VQA-GNN, which unifies the image-level information and\nconceptual knowledge to perform joint reasoning of the scene. Specifically,\ngiven a question-image pair, we build a scene graph from the image, retrieve a\nrelevant linguistic subgraph from ConceptNet and visual subgraph from\nVisualGenome, and unify these three graphs and the question into one joint\ngraph, multimodal semantic graph. Our VQA-GNN then learns to aggregate messages\nand reason across different modalities captured by the multimodal semantic\ngraph. In the evaluation on the VCR task, our method outperforms the previous\nscene graph-based Trans-VL models by over 4%, and VQA-GNN-Large, our model that\nfuses a Trans-VL further improves the state of the art by 2%, attaining the top\nof the VCR leaderboard at the time of submission. This result suggests the\nefficacy of our model in performing conceptual reasoning beyond image-level\nrecognition for visual understanding. Finally, we demonstrate that our model is\nthe first work to provide interpretability across visual and textual knowledge\ndomains for the VQA task.",
    "descriptor": "",
    "authors": [
      "Yanan Wang",
      "Michihiro Yasunaga",
      "Hongyu Ren",
      "Shinya Wada",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11501"
  },
  {
    "id": "arXiv:2205.11502",
    "title": "On the Paradox of Learning to Reason from Data",
    "abstract": "Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be\ntrained end-to-end to solve logical reasoning problems presented in natural\nlanguage? We attempt to answer this question in a confined problem space where\nthere exists a set of parameters that perfectly simulates logical reasoning. We\nmake observations that seem to contradict each other: BERT attains near-perfect\naccuracy on in-distribution test examples while failing to generalize to other\ndata distributions over the exact same problem space. Our study provides an\nexplanation for this paradox: instead of learning to emulate the correct\nreasoning function, BERT has in fact learned statistical features that\ninherently exist in logical reasoning problems. We also show that it is\ninfeasible to jointly remove statistical features from data, illustrating the\ndifficulty of learning to reason in general. Our result naturally extends to\nother neural models and unveils the fundamental difference between learning to\nreason and learning to achieve high performance on NLP benchmarks using\nstatistical features.",
    "descriptor": "",
    "authors": [
      "Honghua Zhang",
      "Liunian Harold Li",
      "Tao Meng",
      "Kai-Wei Chang",
      "Guy Van den Broeck"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11502"
  },
  {
    "id": "arXiv:2205.11503",
    "title": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual  Style Transfer with Small Language Models",
    "abstract": "We propose a method for arbitrary textual style transfer (TST)--the task of\ntransforming a text into any given style--utilizing general-purpose pre-trained\nlanguage models. Our method, Prompt-and-Rerank, is based on a mathematical\nformulation of the TST task, decomposing it into three constituent components:\ntextual similarity, target style strength, and fluency. Specifically, our\nmethod first uses zero-shot or few-shot prompting to obtain a set of candidate\ngenerations in the target style, and then re-ranks these candidates according\nto a combination of the three components above. Empirically, our method enables\nsmall pre-trained language models to perform on par with state-of-the-art\nlarge-scale models while consuming two orders of magnitude less compute and\nmemory. Finally, we conduct a systematic investigation of the effect of model\nsize and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on\nstyle transfer quality across seven diverse textual style transfer datasets.",
    "descriptor": "\nComments: GitHub page: this https URL Project page: this https URL\n",
    "authors": [
      "Mirac Suzgun",
      "Luke Melas-Kyriazi",
      "Dan Jurafsky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11503"
  },
  {
    "id": "arXiv:2205.11505",
    "title": "What Makes Data-to-Text Generation Hard for Pretrained Language Models?",
    "abstract": "Expressing natural language descriptions of structured facts or relations --\ndata-to-text generation (D2T) -- increases the accessibility of structured\nknowledge repositories. Previous work shows that pre-trained language\nmodels(PLMs) perform remarkably well on this task after fine-tuning on a\nsignificant amount of task-specific training data. On the other hand, while\nauto-regressive PLMs can generalize from a few task examples, their efficacy at\nD2T is largely unexplored. Furthermore, we have an incomplete understanding of\nthe limits of PLMs on D2T.\nIn this work, we conduct an empirical study of both fine-tuned and\nauto-regressive PLMs on the DART multi-domain D2T dataset. We consider their\nperformance as a function of the amount of task-specific data and how these\ndata are incorporated into the models: zero and few-shot learning, and\nfine-tuning of model weights. In addition, we probe the limits of PLMs by\nmeasuring performance on subsets of the evaluation data: novel predicates and\nabstractive test examples. To improve the performance on these subsets, we\ninvestigate two techniques: providing predicate descriptions in the context and\nre-ranking generated candidates by information reflected in the source.\nFinally, we conduct a human evaluation of model errors and show that D2T\ngeneration tasks would benefit from datasets with more careful manual curation.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Moniba Keymanesh",
      "Adrian Benton",
      "Mark Dredze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11505"
  },
  {
    "id": "arXiv:2205.11506",
    "title": "Orchestra: Unsupervised Federated Learning via Globally Consistent  Clustering",
    "abstract": "Federated learning is generally used in tasks where labels are readily\navailable (e.g., next word prediction). Relaxing this constraint requires\ndesign of unsupervised learning techniques that can support desirable\nproperties for federated training: robustness to statistical/systems\nheterogeneity, scalability with number of participants, and communication\nefficiency. Prior work on this topic has focused on directly extending\ncentralized self-supervised learning techniques, which are not designed to have\nthe properties listed above. To address this situation, we propose Orchestra, a\nnovel unsupervised federated learning technique that exploits the federation's\nhierarchy to orchestrate a distributed clustering task and enforce a globally\nconsistent partitioning of clients' data into discriminable clusters. We show\nthe algorithmic pipeline in Orchestra guarantees good generalization\nperformance under a linear probe, allowing it to outperform alternative\ntechniques in a broad range of conditions, including variation in\nheterogeneity, number of clients, participation ratio, and local epochs.",
    "descriptor": "\nComments: Accepted at ICML, 2022\n",
    "authors": [
      "Ekdeep Singh Lubana",
      "Chi Ian Tang",
      "Fahim Kawsar",
      "Robert P. Dick",
      "Akhil Mathur"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11506"
  },
  {
    "id": "arXiv:2205.11507",
    "title": "Computationally Efficient Horizon-Free Reinforcement Learning for Linear  Mixture MDPs",
    "abstract": "Recent studies have shown that episodic reinforcement learning (RL) is not\nmore difficult than contextual bandits, even with a long planning horizon and\nunknown state transitions. However, these results are limited to either tabular\nMarkov decision processes (MDPs) or computationally inefficient algorithms for\nlinear mixture MDPs. In this paper, we propose the first computationally\nefficient horizon-free algorithm for linear mixture MDPs, which achieves the\noptimal $\\tilde O(d\\sqrt{K} +d^2)$ regret up to logarithmic factors. Our\nalgorithm adapts a weighted least square estimator for the unknown transitional\ndynamic, where the weight is both \\emph{variance-aware} and\n\\emph{uncertainty-aware}. When applying our weighted least square estimator to\nheterogeneous linear bandits, we can obtain an $\\tilde O(d\\sqrt{\\sum_{k=1}^K\n\\sigma_k^2} +d)$ regret in the first $K$ rounds, where $d$ is the dimension of\nthe context and $\\sigma_k^2$ is the variance of the reward in the $k$-th round.\nThis also improves upon the best-known algorithms in this setting when\n$\\sigma_k^2$'s are known.",
    "descriptor": "\nComments: 33 pages, 1 table\n",
    "authors": [
      "Dongruo Zhou",
      "Quanquan Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11507"
  },
  {
    "id": "arXiv:2205.11508",
    "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods",
    "abstract": "Self-Supervised Learning (SSL) surmises that inputs and pairwise positive\nrelationships are enough to learn meaningful representations. Although SSL has\nrecently reached a milestone: outperforming supervised methods in many\nmodalities... the theoretical foundations are limited, method-specific, and\nfail to provide principled design guidelines to practitioners. In this paper,\nwe propose a unifying framework under the helm of spectral manifold learning to\naddress those limitations. Through the course of this study, we will rigorously\ndemonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous\nspectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.\nThis unification will then allow us to obtain (i) the closed-form optimal\nrepresentation for each method, (ii) the closed-form optimal network parameters\nin the linear regime for each method, (iii) the impact of the pairwise\nrelations used during training on each of those quantities and on downstream\ntask performances, and most importantly, (iv) the first theoretical bridge\nbetween contrastive and non-contrastive methods towards global and local\nspectral embedding methods respectively, hinting at the benefits and\nlimitations of each. For example, (a) if the pairwise relation is aligned with\nthe downstream task, any SSL method can be employed successfully and will\nrecover the supervised method, but in the low data regime, SimCLR or VICReg\nwith high invariance hyper-parameter should be preferred; (b) if the pairwise\nrelation is misaligned with the downstream task, BarlowTwins or VICReg with\nsmall invariance hyper-parameter should be preferred.",
    "descriptor": "",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Spectral Theory (math.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11508"
  },
  {
    "id": "arXiv:2205.11509",
    "title": "Information Propagation by Composited Labels in Natural Language  Processing",
    "abstract": "In natural language processing (NLP), labeling on regions of text, such as\nwords, sentences and paragraphs, is a basic task. In this paper, label is\ndefined as map between mention of entity in a region on text and context of\nentity in a broader region on text containing the mention. This definition\nnaturally introduces linkage of entities induced from inclusion relation of\nregions, and connected entities form a graph representing information flow\ndefined by map. It also enables calculation of information loss through map\nusing entropy, and entropy lost is regarded as distance between two entities\nover a path on graph.",
    "descriptor": "",
    "authors": [
      "Takeshi Inagaki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11509"
  },
  {
    "id": "arXiv:2109.04522",
    "title": "Asynchronous Iterations in Optimization: New Sequence Results and  Sharper Algorithmic Guarantees",
    "abstract": "We introduce novel convergence results for asynchronous iterations which\nappear in the analysis of parallel and distributed optimization algorithms. The\nresults are simple to apply and give explicit estimates for how the degree of\nasynchrony impacts the convergence rates of the iterates. Our results shorten,\nstreamline and strengthen existing convergence proofs for several asynchronous\noptimization methods, and allow us to establish convergence guarantees for\npopular algorithms that were thus far lacking a complete theoretical\nunderstanding. Specifically, we use our results to derive better iteration\ncomplexity bounds for proximal incremental aggregated gradient methods, to\nprovide less conservative analyses of the speedup conditions for asynchronous\nblock-coordinate implementations of Krasnoselskii-Mann iterations, and to\nquantify the convergence rates for totally asynchronous iterations under\nvarious assumptions on communication delays and update rates.",
    "descriptor": "\nComments: 44 pages, 1 Figure\n",
    "authors": [
      "Hamid Reza Feyzmahdavian",
      "Mikael Johansson"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.04522"
  },
  {
    "id": "arXiv:2205.10353",
    "title": "Dual Branch Prior-SegNet: CNN for Interventional CBCT using Planning  Scan and Auxiliary Segmentation Loss",
    "abstract": "This paper proposes an extension to the Dual Branch Prior-Net for sparse view\ninterventional CBCT reconstruction incorporating a high quality planning scan.\nAn additional head learns to segment interventional instruments and thus guides\nthe reconstruction task. The prior scans are misaligned by up to +-5deg\nin-plane during training. Experiments show that the proposed model, Dual Branch\nPrior-SegNet, significantly outperforms any other evaluated model by >2.8dB\nPSNR. It also stays robust wrt. rotations of up to +-5.5deg.",
    "descriptor": "\nComments: 3 pages, 1 figure, accepted short paper submission at MIDL 2022\n",
    "authors": [
      "Philipp Ernst",
      "Suhita Ghosh",
      "Georg Rose",
      "Andreas N\u00fcrnberger"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10353"
  },
  {
    "id": "arXiv:2205.10354",
    "title": "Prediction of stent under-expansion in calcified coronary arteries using  machine-learning on intravascular optical coherence tomography",
    "abstract": "BACKGROUND Careful evaluation of the risk of stent under-expansions before\nthe intervention will aid treatment planning, including the application of a\npre-stent plaque modification strategy.\nOBJECTIVES It remains challenging to achieve a proper stent expansion in the\npresence of severely calcified coronary lesions. Building on our work in deep\nlearning segmentation, we created an automated machine learning approach that\nuses lesion attributes to predict stent under-expansion from pre-stent images,\nsuggesting the need for plaque modification.\nMETHODS Pre- and post-stent intravascular optical coherence tomography image\ndata were obtained from 110 coronary lesions. Lumen and calcifications in\npre-stent images were segmented using deep learning, and numerous features per\nlesion were extracted. We analyzed stent expansion along the lesion, enabling\nframe, segmental, and whole-lesion analyses. We trained regression models to\npredict the poststent lumen area and then to compute the stent expansion index\n(SEI). Stents with an SEI < or >/= 80% were classified as \"under-expanded\" and\n\"well-expanded,\" respectively.\nRESULTS Best performance (root-mean-square-error = 0.04+/-0.02 mm2, r =\n0.94+/-0.04, p < 0.0001) was achieved when we used features from both the lumen\nand calcification to train a Gaussian regression model for a segmental analysis\nover a segment length of 31 frames. Under-expansion classification results\n(AUC=0.85+/-0.02) were significantly improved over other approaches.\nCONCLUSIONS We used calcifications and lumen features to identify lesions at\nrisk of stent under-expansion. Results suggest that the use of pre-stent images\ncan inform physicians of the need to apply plaque modification approaches.",
    "descriptor": "\nComments: 25 pages, 7 figures, 1 table, 6 supplemental figures, 3 supplemental tables\n",
    "authors": [
      "Yazan Gharaibeh",
      "Juhwan Lee",
      "Vladislav N. Zimin",
      "Chaitanya Kolluru",
      "Luis A. P. Dallan",
      "Gabriel T. R. Pereira",
      "Armando Vergara-Martel",
      "Justin N. Kim",
      "Ammar Hoori",
      "Pengfei Dong",
      "Peshala T. Gamage",
      "Linxia Gu",
      "Hiram G. Bezerra",
      "Sadeer Al-Kindi",
      "David L. Wilson"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10354"
  },
  {
    "id": "arXiv:2205.10367",
    "title": "Nonlinear motion separation via untrained generator networks with  disentangled latent space variables and applications to cardiac MRI",
    "abstract": "In this paper, a nonlinear approach to separate different motion types in\nvideo data is proposed. This is particularly relevant in dynamic medical\nimaging (e.g. PET, MRI), where patient motion poses a significant challenge due\nto its effects on the image reconstruction as well as for its subsequent\ninterpretation. Here, a new method is proposed where dynamic images are\nrepresented as the forward mapping of a sequence of latent variables via a\ngenerator neural network. The latent variables are structured so that temporal\nvariations in the data are represented via dynamic latent variables, which are\nindependent of static latent variables characterizing the general structure of\nthe frames. In particular, different kinds of motion are also characterized\nindependently of each other via latent space disentanglement using\none-dimensional prior information on all but one of the motion types. This\nrepresentation allows to freeze any selection of motion types, and to obtain\naccurate independent representations of other dynamics of interest. Moreover,\nthe proposed algorithm is training-free, i.e., all the network parameters are\nlearned directly from a single video. We illustrate the performance of this\nmethod on phantom and real-data MRI examples, where we successfully separate\nrespiratory and cardiac motion.",
    "descriptor": "",
    "authors": [
      "Abdullah",
      "Martin Holler",
      "Karl Kunisch",
      "Malena Sabate Landman"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.10367"
  },
  {
    "id": "arXiv:2205.10368",
    "title": "Automatic Generation of Synthetic Colonoscopy Videos for Domain  Randomization",
    "abstract": "An increasing number of colonoscopic guidance and assistance systems rely on\nmachine learning algorithms which require a large amount of high-quality\ntraining data. In order to ensure high performance, the latter has to resemble\na substantial portion of possible configurations. This particularly addresses\nvarying anatomy, mucosa appearance and image sensor characteristics which are\nlikely deteriorated by motion blur and inadequate illumination. The limited\namount of readily available training data hampers to account for all of these\npossible configurations which results in reduced generalization capabilities of\nmachine learning models. We propose an exemplary solution for synthesizing\ncolonoscopy videos with substantial appearance and anatomical variations which\nenables to learn discriminative domain-randomized representations of the\ninterior colon while mimicking real-world settings.",
    "descriptor": "\nComments: 4 pages, 5 figures\n",
    "authors": [
      "Abhishek Dinkar Jagtap",
      "Mattias Heinrich",
      "Marian Himstedt"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10368"
  },
  {
    "id": "arXiv:2205.10373",
    "title": "A SSIM Guided cGAN Architecture For Clinically Driven Generative Image  Synthesis of Multiplexed Spatial Proteomics Channels",
    "abstract": "Here we present a structural similarity index measure (SSIM) guided\nconditional Generative Adversarial Network (cGAN) that generatively performs\nimage-to-image (i2i) synthesis to generate photo-accurate protein channels in\nmultiplexed spatial proteomics images. This approach can be utilized to\naccurately generate missing spatial proteomics channels that were not included\nduring experimental data collection either at the bench or the clinic.\nExperimental spatial proteomic data from the Human BioMolecular Atlas Program\n(HuBMAP) was used to generate spatial representations of missing proteins\nthrough a U-Net based image synthesis pipeline. HuBMAP channels were\nhierarchically clustered by the (SSIM) as a heuristic to obtain the minimal set\nneeded to recapitulate the underlying biology represented by the spatial\nlandscape of proteins. We subsequently prove that our SSIM based architecture\nallows for scaling of generative image synthesis to slides with up to 100\nchannels, which is better than current state of the art algorithms which are\nlimited to data with 11 channels. We validate these claims by generating a new\nexperimental spatial proteomics data set from human lung adenocarcinoma tissue\nsections and show that a model trained on HuBMAP can accurately synthesize\nchannels from our new data set. The ability to recapitulate experimental data\nfrom sparsely stained multiplexed histological slides containing spatial\nproteomic will have tremendous impact on medical diagnostics and drug\ndevelopment, and also raises important questions on the medical ethics of\nutilizing data produced by generative image synthesis in the clinical setting.\nThe algorithm that we present in this paper will allow researchers and\nclinicians to save time and costs in proteomics based histological staining\nwhile also increasing the amount of data that they can generate through their\nexperiments.",
    "descriptor": "",
    "authors": [
      "Jillur Rahman Saurav",
      "Mohammad Sadegh Nasr",
      "Paul Koomey",
      "Michael Robben",
      "Manfred Huber",
      "Jon Weidanz",
      "Br\u00edd Ryan",
      "Eytan Ruppin",
      "Peng Jiang",
      "Jacob M. Luber"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2205.10373"
  },
  {
    "id": "arXiv:2205.10401",
    "title": "NeuralEcho: A Self-Attentive Recurrent Neural Network For Unified  Acoustic Echo Suppression And Speech Enhancement",
    "abstract": "Acoustic echo cancellation (AEC) plays an important role in the full-duplex\nspeech communication as well as the front-end speech enhancement for\nrecognition in the conditions when the loudspeaker plays back. In this paper,\nwe present an all-deep-learning framework that implicitly estimates the second\norder statistics of echo/noise and target speech, and jointly solves echo and\nnoise suppression through an attention based recurrent neural network. The\nproposed model outperforms the state-of-the-art joint echo cancellation and\nspeech enhancement method F-T-LSTM in terms of objective speech quality\nmetrics, speech recognition accuracy and model complexity. We show that this\nmodel can work with speaker embedding for better target speech enhancement and\nfurthermore develop a branch for automatic gain control (AGC) task to form an\nall-in-one front-end speech enhancement system.",
    "descriptor": "\nComments: Submitted to INTERSPEECH 2022\n",
    "authors": [
      "Meng Yu",
      "Yong Xu",
      "Chunlei Zhang",
      "Shi-Xiong Zhang",
      "Dong Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.10401"
  },
  {
    "id": "arXiv:2205.10448",
    "title": "Approximate Message Passing with Parameter Estimation for Heavily  Quantized Measurements",
    "abstract": "Designing efficient sparse recovery algorithms that could handle noisy\nquantized measurements is important in a variety of applications -- from radar\nto source localization, spectrum sensing and wireless networking. We take\nadvantage of the approximate message passing (AMP) framework to achieve this\ngoal given its high computational efficiency and state-of-the-art performance.\nIn AMP, the signal of interest is assumed to follow certain prior distribution\nwith unknown parameters. Previous works focused on finding the parameters that\nmaximize the measurement likelihood via expectation maximization -- an\nincreasingly difficult problem to solve in cases involving complicated\nprobability models. In this paper, we treat the parameters as unknown variables\nand compute their posteriors via AMP. The parameters and signal of interest can\nthen be jointly recovered. Compared to previous methods, the proposed approach\nleads to a simple and elegant parameter estimation scheme, allowing us to\ndirectly work with 1-bit quantization noise model. We then further extend our\napproach to general multi-bit quantization noise model. Experimental results\nshow that the proposed framework provides significant improvement over\nstate-of-the-art methods across a wide range of sparsity and noise levels.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2007.07679\n",
    "authors": [
      "Shuai Huang",
      "Deqiang Qiu",
      "Trac D. Tran"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.10448"
  },
  {
    "id": "arXiv:2205.10488",
    "title": "Cryptanalysis of Three Quantum Money Schemes",
    "abstract": "We investigate the security assumptions behind three public-key quantum money\nschemes. Aaronson and Christiano proposed a scheme based on hidden subspaces of\nthe vector space $\\mathbb{F}_2^n$ in 2012. It was conjectured by Pena et al in\n2015 that the hard problem underlying the scheme can be solved in\nquasi-polynomial time. We confirm this conjecture by giving a polynomial time\nquantum algorithm for the underlying problem. Our algorithm is based on\ncomputing the Zariski tangent space of a random point in the hidden subspace.\nZhandry proposed a scheme based on multivariate hash functions in 2017. We\ngive a polynomial time quantum algorithm for cloning a money state with high\nprobability. Our algorithm uses the verification circuit of the scheme to\nproduce a banknote from a given serial number.\nKane proposed a scheme based on modular forms in 2018. The underlying hard\nproblem in Kane's scheme is cloning a quantum state that represents an\neigenvector of a set of Hecke operators. We give a polynomial time quantum\nreduction from this hard problem to a linear algebra problem. The latter\nproblem is much easier to understand, and we hope that our reduction opens new\navenues to future cryptanalyses of this scheme.",
    "descriptor": "",
    "authors": [
      "Andriyan Bilyk",
      "Javad Doliskani",
      "Zhiyong Gong"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10488"
  },
  {
    "id": "arXiv:2205.10499",
    "title": "Towards Balanced Three-phase Charging: Phase Optimization in Adaptive  Charging Networks",
    "abstract": "We study the problem of phase optimization for electric-vehicle (EV)\ncharging. We formulate our problem as a non-convex mixed-integer programming\nproblem whose objective is to minimize the charging loss. Despite the hardness\nof directly solving this non-convex problem, we solve a relaxation of the\noriginal problem by proposing the PXA algorithm where \"P\", \"X\", and \"A\" stand\nfor three variable matrices in the formed phase optimization problems. We show\nthat under certain conditions, the solution is given by the PXA precisely\nconverges to the global optimum. In addition, using the idea of model\npredictive control (MPC), we design the {PXA-MPC}, which is an online\nimplementation of the PXA. Compared to other empirical phase balancing\nstrategies, the PXA algorithm significantly improves the charging performance\nby maximizing energy delivery, minimizing charging price, and assisting future\nenergy planning. The efficacy of our algorithm is demonstrated using data\ncollected from a real-world adaptive EV charging network (ACN).",
    "descriptor": "\nComments: 8 pages, 6 figures, accepted by PSCC 2022\n",
    "authors": [
      "Zixin Ye",
      "Tongxin Li",
      "Steven H. Low"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10499"
  },
  {
    "id": "arXiv:2205.10501",
    "title": "Making Video Quality Assessment Models Sensitive to Frame Rate  Distortions",
    "abstract": "We consider the problem of capturing distortions arising from changes in\nframe rate as part of Video Quality Assessment (VQA). Variable frame rate (VFR)\nvideos have become much more common, and streamed videos commonly range from 30\nframes per second (fps) up to 120 fps. VFR-VQA offers unique challenges in\nterms of distortion types as well as in making non-uniform comparisons of\nreference and distorted videos having different frame rates. The majority of\ncurrent VQA models require compared videos to be of the same frame rate, but\nare unable to adequately account for frame rate artifacts. The recently\nproposed Generalized Entropic Difference (GREED) VQA model succeeds at this\ntask, using natural video statistics models of entropic differences of temporal\nband-pass coefficients, delivering superior performance on predicting video\nquality changes arising from frame rate distortions. Here we propose a simple\nfusion framework, whereby temporal features from GREED are combined with\nexisting VQA models, towards improving model sensitivity towards frame rate\ndistortions. We find through extensive experiments that this feature fusion\nsignificantly boosts model performance on both HFR/VFR datasets as well as\nfixed frame rate (FFR) VQA databases. Our results suggest that employing\nefficient temporal representations can result much more robust and accurate VQA\nmodels when frame rate variations can occur.",
    "descriptor": "",
    "authors": [
      "Pavan C. Madhusudana",
      "Neil Birkbeck",
      "Yilin Wang",
      "Balu Adsumilli",
      "Alan C. Bovik"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.10501"
  },
  {
    "id": "arXiv:2205.10515",
    "title": "Visualizing CoAtNet Predictions for Aiding Melanoma Detection",
    "abstract": "Melanoma is considered to be the most aggressive form of skin cancer. Due to\nthe similar shape of malignant and benign cancerous lesions, doctors spend\nconsiderably more time when diagnosing these findings. At present, the\nevaluation of malignancy is performed primarily by invasive histological\nexamination of the suspicious lesion. Developing an accurate classifier for\nearly and efficient detection can minimize and monitor the harmful effects of\nskin cancer and increase patient survival rates. This paper proposes a\nmulti-class classification task using the CoAtNet architecture, a hybrid model\nthat combines the depthwise convolution matrix operation of traditional\nconvolutional neural networks with the strengths of Transformer models and\nself-attention mechanics to achieve better generalization and capacity. The\nproposed multi-class classifier achieves an overall precision of 0.901, recall\n0.895, and AP 0.923, indicating high performance compared to other\nstate-of-the-art networks.",
    "descriptor": "",
    "authors": [
      "Daniel Kvak"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10515"
  },
  {
    "id": "arXiv:2205.10541",
    "title": "Neuroevolutionary Feature Representations for Causal Inference",
    "abstract": "Within the field of causal inference, we consider the problem of estimating\nheterogeneous treatment effects from data. We propose and validate a novel\napproach for learning feature representations to aid the estimation of the\nconditional average treatment effect or CATE. Our method focuses on an\nintermediate layer in a neural network trained to predict the outcome from the\nfeatures. In contrast to previous approaches that encourage the distribution of\nrepresentations to be treatment-invariant, we leverage a genetic algorithm that\noptimizes over representations useful for predicting the outcome to select\nthose less useful for predicting the treatment. This allows us to retain\ninformation within the features useful for predicting outcome even if that\ninformation may be related to treatment assignment. We validate our method on\nsynthetic examples and illustrate its use on a real life dataset.",
    "descriptor": "",
    "authors": [
      "Michael C. Burkhart",
      "Gabriel Ruiz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10541"
  },
  {
    "id": "arXiv:2205.10548",
    "title": "Three-Dimensional Segmentation of the Left Ventricle in Late Gadolinium  Enhanced MR Images of Chronic Infarction Combining Long- and Short-Axis  Information",
    "abstract": "Automatic segmentation of the left ventricle (LV) in late gadolinium enhanced\n(LGE) cardiac MR (CMR) images is difficult due to the intensity heterogeneity\narising from accumulation of contrast agent in infarcted myocardium. In this\npaper, we present a comprehensive framework for automatic 3D segmentation of\nthe LV in LGE CMR images. Given myocardial contours in cine images as a priori\nknowledge, the framework initially propagates the a priori segmentation from\ncine to LGE images via 2D translational registration. Two meshes representing\nrespectively endocardial and epicardial surfaces are then constructed with the\npropagated contours. After construction, the two meshes are deformed towards\nthe myocardial edge points detected in both short-axis and long-axis LGE images\nin a unified 3D coordinate system. Taking into account the intensity\ncharacteristics of the LV in LGE images, we propose a novel parametric model of\nthe LV for consistent myocardial edge points detection regardless of\npathological status of the myocardium (infarcted or healthy) and of the type of\nthe LGE images (short-axis or long-axis). We have evaluated the proposed\nframework with 21 sets of real patient and 4 sets of simulated phantom data.\nBoth distance- and region-based performance metrics confirm the observation\nthat the framework can generate accurate and reliable results for myocardial\nsegmentation of LGE images. We have also tested the robustness of the framework\nwith respect to varied a priori segmentation in both practical and simulated\nsettings. Experimental results show that the proposed framework can greatly\ncompensate variations in the given a priori knowledge and consistently produce\naccurate segmentations.",
    "descriptor": "\nComments: Medical Image Analysis, Volume 17, Issue 6, August 2013, Pages 685-697\n",
    "authors": [
      "Dong Wei",
      "Ying Sun",
      "Sim-Heng Ong",
      "Ping Chai",
      "Lynette L. Teo",
      "Adrian F. Low"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10548"
  },
  {
    "id": "arXiv:2205.10572",
    "title": "A Comprehensive 3-D Framework for Automatic Quantification of Late  Gadolinium Enhanced Cardiac Magnetic Resonance Images",
    "abstract": "Late gadolinium enhanced (LGE) cardiac magnetic resonance (CMR) can directly\nvisualize nonviable myocardium with hyperenhanced intensities with respect to\nnormal myocardium. For heart attack patients, it is crucial to facilitate the\ndecision of appropriate therapy by analyzing and quantifying their LGE CMR\nimages. To achieve accurate quantification, LGE CMR images need to be processed\nin two steps: segmentation of the myocardium followed by classification of\ninfarcts within the segmented myocardium. However, automatic segmentation is\ndifficult usually due to the intensity heterogeneity of the myocardium and\nintensity similarity between the infarcts and blood pool. Besides, the slices\nof an LGE CMR dataset often suffer from spatial and intensity distortions,\ncausing further difficulties in segmentation and classification. In this paper,\nwe present a comprehensive 3-D framework for automatic quantification of LGE\nCMR images. In this framework, myocardium is segmented with a novel method that\ndeforms coupled endocardial and epicardial meshes and combines information in\nboth short- and long-axis slices, while infarcts are classified with a\ngraph-cut algorithm incorporating intensity and spatial information. Moreover,\nboth spatial and intensity distortions are effectively corrected with specially\ndesigned countermeasures. Experiments with 20 sets of real patient data show\nvisually good segmentation and classification results that are quantitatively\nin strong agreement with those manually obtained by experts.",
    "descriptor": "\nComments: IEEE Transactions on Biomedical Engineering ( Volume: 60, Issue: 6, June 2013)\n",
    "authors": [
      "Dong Wei",
      "Ying Sun",
      "Sim-Heng Ong",
      "Ping Chai",
      "Lynette L Teo",
      "Adrian F Low"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10572"
  },
  {
    "id": "arXiv:2205.10595",
    "title": "Myocardial Segmentation of Late Gadolinium Enhanced MR Images by  Propagation of Contours from Cine MR Images",
    "abstract": "Automatic segmentation of myocardium in Late Gadolinium Enhanced (LGE)\nCardiac MR (CMR) images is often difficult due to the intensity heterogeneity\nresulting from accumulation of contrast agent in infarcted areas. In this\npaper, we propose an automatic segmentation framework that fully utilizes\nshared information between corresponding cine and LGE images of a same patient.\nGiven myocardial contours in cine CMR images, the proposed framework achieves\naccurate segmentation of LGE CMR images in a coarse-to-fine manner. Affine\nregistration is first performed between the corresponding cine and LGE image\npair, followed by nonrigid registration, and finally local deformation of\nmyocardial contours driven by forces derived from local features of the LGE\nimage. Experimental results on real patient data with expert outlined ground\ntruth show that the proposed framework can generate accurate and reliable\nresults for myocardial segmentation of LGE CMR images.",
    "descriptor": "\nComments: MICCAI 2011\n",
    "authors": [
      "Dong Wei",
      "Ying Sun",
      "Ping Chai",
      "Adrian Low",
      "Sim Heng Ong"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10595"
  },
  {
    "id": "arXiv:2205.10596",
    "title": "Not All SWAPs Have the Same Cost: A Case for Optimization-Aware Qubit  Routing",
    "abstract": "Despite rapid advances in quantum computing technologies, the qubit\nconnectivity limitation remains to be a critical challenge. Both near-term NISQ\nquantum computers and relatively long-term scalable quantum architectures do\nnot offer full connectivity. As a result, quantum circuits may not be directly\nexecuted on quantum hardware, and a quantum compiler needs to perform qubit\nrouting to make the circuit compatible with the device layout. During the qubit\nrouting step, the compiler inserts SWAP gates and performs circuit\ntransformations. Given the connectivity topology of the target hardware, there\nare typically multiple qubit routing candidates. The state-of-the-art compilers\nuse a cost function to evaluate the number of SWAP gates for different routes\nand then select the one with the minimum number of SWAP gates. After qubit\nrouting, the quantum compiler performs gate optimizations upon the circuit with\nthe newly inserted SWAP gates.\nIn this paper, we observe that the aforementioned qubit routing is not\noptimal, and qubit routing should \\textit{not} be independent on subsequent\ngate optimizations. We find that with the consideration of gate optimizations,\nnot all of the SWAP gates have the same basis-gate cost. These insights lead to\nthe development of our qubit routing algorithm, NASSC (Not All Swaps have the\nSame Cost). NASSC is the first algorithm that considers the subsequent\noptimizations during the routing step. Our optimization-aware qubit routing\nleads to better routing decisions and benefits subsequent optimizations. We\nalso propose a new optimization-aware decomposition for the inserted SWAP\ngates. Our experiments show that the routing overhead compiled with our routing\nalgorithm is reduced by up to $69.30\\%$ ($21.30\\%$ on average) in the number of\nCNOT gates and up to $43.50\\%$ ($7.61\\%$ on average) in the circuit depth\ncompared with the state-of-the-art scheme, SABRE.",
    "descriptor": "\nComments: 17 pages, 11 figures\n",
    "authors": [
      "Ji Liu",
      "Peiyi Li",
      "Huiyang Zhou"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.10596"
  },
  {
    "id": "arXiv:2205.10605",
    "title": "Brain Cortical Functional Gradients Predict Cortical Folding Patterns  via Attention Mesh Convolution",
    "abstract": "Since gyri and sulci, two basic anatomical building blocks of cortical\nfolding patterns, were suggested to bear different functional roles, a precise\nmapping from brain function to gyro-sulcal patterns can provide profound\ninsights into both biological and artificial neural networks. However, there\nlacks a generic theory and effective computational model so far, due to the\nhighly nonlinear relation between them, huge inter-individual variabilities and\na sophisticated description of brain function regions/networks distribution as\nmosaics, such that spatial patterning of them has not been considered. we\nadopted brain functional gradients derived from resting-state fMRI to embed the\n\"gradual\" change of functional connectivity patterns, and developed a novel\nattention mesh convolution model to predict cortical gyro-sulcal segmentation\nmaps on individual brains. The convolution on mesh considers the spatial\norganization of functional gradients and folding patterns on a cortical sheet\nand the newly designed channel attention block enhances the interpretability of\nthe contribution of different functional gradients to cortical folding\nprediction. Experiments show that the prediction performance via our model\noutperforms other state-of-the-art models. In addition, we found that the\ndominant functional gradients contribute less to folding prediction. On the\nactivation maps of the last layer, some well-studied cortical landmarks are\nfound on the borders of, rather than within, the highly activated regions.\nThese results and findings suggest that a specifically designed artificial\nneural network can improve the precision of the mapping between brain functions\nand cortical folding patterns, and can provide valuable insight of brain\nanatomy-function relation for neuroscience.",
    "descriptor": "",
    "authors": [
      "Li Yang",
      "Zhibin He",
      "Changhe Li",
      "Junwei Han",
      "Dajiang Zhu",
      "Tianming Liu",
      "Tuo Zhang"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10605"
  },
  {
    "id": "arXiv:2205.10619",
    "title": "A Pilot Study of Relating MYCN-Gene Amplification with  Neuroblastoma-Patient CT Scans",
    "abstract": "Neuroblastoma is one of the most common cancers in infants, and the initial\ndiagnosis of this disease is difficult. At present, the MYCN gene amplification\n(MNA) status is detected by invasive pathological examination of tumor samples.\nThis is time-consuming and may have a hidden impact on children. To handle this\nproblem, we adopt multiple machine learning (ML) algorithms to predict the\npresence or absence of MYCN gene amplification. The dataset is composed of\nretrospective CT images of 23 neuroblastoma patients. Different from previous\nwork, we develop the algorithm without manually-segmented primary tumors which\nis time-consuming and not practical. Instead, we only need the coordinate of\nthe center point and the number of tumor slices given by a subspecialty-trained\npediatric radiologist. Specifically, CNN-based method uses pre-trained\nconvolutional neural network, and radiomics-based method extracts radiomics\nfeatures. Our results show that CNN-based method outperforms the\nradiomics-based method.",
    "descriptor": "",
    "authors": [
      "Zihan Zhang",
      "Xiang Xiang",
      "Xuehua Peng",
      "Jianbo Shao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10619"
  },
  {
    "id": "arXiv:2205.10651",
    "title": "Tensor Shape Search for Optimum Data Compression",
    "abstract": "Various tensor decomposition methods have been proposed for data compression.\nIn real world applications of the tensor decomposition, selecting the tensor\nshape for the given data poses a challenge and the shape of the tensor may\naffect the error and the compression ratio. In this work, we study the effect\nof the tensor shape on the tensor decomposition and propose an optimization\nmodel to find an optimum shape for the tensor train (TT) decomposition. The\nproposed optimization model maximizes the compression ratio of the TT\ndecomposition given an error bound. We implement a genetic algorithm (GA)\nlinked with the TT-SVD algorithm to solve the optimization model. We apply the\nproposed method for the compression of RGB images. The results demonstrate the\neffectiveness of the proposed evolutionary tensor shape search for the TT\ndecomposition.",
    "descriptor": "",
    "authors": [
      "Ryan Solgi",
      "Zichang He",
      "William Jiahua Liang",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10651"
  },
  {
    "id": "arXiv:2205.10663",
    "title": "Transformer based Generative Adversarial Network for Liver Segmentation",
    "abstract": "Automated liver segmentation from radiology scans (CT, MRI) can improve\nsurgery and therapy planning and follow-up assessment in addition to\nconventional use for diagnosis and prognosis. Although convolutional neural\nnetworks (CNNs) have become the standard image segmentation tasks, more\nrecently this has started to change towards Transformers based architectures\nbecause Transformers are taking advantage of capturing long range dependence\nmodeling capability in signals, so called attention mechanism. In this study,\nwe propose a new segmentation approach using a hybrid approach combining the\nTransformer(s) with the Generative Adversarial Network (GAN) approach. The\npremise behind this choice is that the self-attention mechanism of the\nTransformers allows the network to aggregate the high dimensional feature and\nprovide global information modeling. This mechanism provides better\nsegmentation performance compared with traditional methods. Furthermore, we\nencode this generator into the GAN based architecture so that the discriminator\nnetwork in the GAN can classify the credibility of the generated segmentation\nmasks compared with the real masks coming from human (expert) annotations. This\nallows us to extract the high dimensional topology information in the mask for\nbiomedical image segmentation and provide more reliable segmentation results.\nOur model achieved a high dice coefficient of 0.9433, recall of 0.9515, and\nprecision of 0.9376 and outperformed other Transformer based approaches.",
    "descriptor": "",
    "authors": [
      "Ugur Demir",
      "Zheyuan Zhang",
      "Bin Wang",
      "Matthew Antalek",
      "Elif Keles",
      "Debesh Jha",
      "Amir Borhani",
      "Daniela Ladner",
      "Ulas Bagci"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10663"
  },
  {
    "id": "arXiv:2205.10691",
    "title": "Producing Histopathology Phantom Images using Generative Adversarial  Networks to improve Tumor Detection",
    "abstract": "Advance in medical imaging is an important part in deep learning research.\nOne of the goals of computer vision is development of a holistic, comprehensive\nmodel which can identify tumors from histology slides obtained via biopsies. A\nmajor problem that stands in the way is lack of data for a few cancer-types. In\nthis paper, we ascertain that data augmentation using GANs can be a viable\nsolution to reduce the unevenness in the distribution of different cancer types\nin our dataset. Our demonstration showed that a dataset augmented to a 50%\nincrease causes an increase in tumor detection from 80% to 87.5%",
    "descriptor": "",
    "authors": [
      "Vidit Gautam"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10691"
  },
  {
    "id": "arXiv:2205.10697",
    "title": "The Selectively Adaptive Lasso",
    "abstract": "Machine learning regression methods allow estimation of functions without\nunrealistic parametric assumptions. Although they can perform exceptionally in\nprediction error, most lack theoretical convergence rates necessary for\nsemi-parametric efficient estimation (e.g. TMLE, AIPW) of parameters like\naverage treatment effects. The Highly Adaptive Lasso (HAL) is the only\nregression method proven to converge quickly enough for a meaningfully large\nclass of functions, independent of the dimensionality of the predictors.\nUnfortunately, HAL is not computationally scalable. In this paper we build upon\nthe theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new\nalgorithm which retains HAL's dimension-free, nonparametric convergence rate\nbut which also scales computationally to massive datasets. To accomplish this,\nwe prove some general theoretical results pertaining to empirical loss\nminimization in nested Donsker classes. Our resulting algorithm is a form of\ngradient tree boosting with an adaptive learning rate, which makes it fast and\ntrivial to implement with off-the-shelf software. Finally, we show that our\nalgorithm retains the performance of standard gradient boosting on a diverse\ngroup of real-world datasets. SAL makes semi-parametric efficient estimators\npractically possible and theoretically justifiable in many big data settings.",
    "descriptor": "",
    "authors": [
      "Alejandro Schuler",
      "Mark van der Laan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.10697"
  },
  {
    "id": "arXiv:2205.10727",
    "title": "Regularization path-following methods with the trust-region updating  strategy for linear complementarity problems",
    "abstract": "In this article, we consider the regularization path-following method with\nthe trust-region updating strategy for the linear complementarity problem.\nMoreover, we prove the global convergence of the new method under the standard\nassumptions without the condition of the priority to feasibility over\ncomplementarity. Numerical results show that the new method is robust and\nefficient for the linear complementarity problem, especially for the dense\nlinear complementarity problem. And it is more robust and faster than some\nstate-of-the-art solvers such as the built-in subroutines PATH and MILES of the\nGAMS v28.2 (2019) environment. The computational time of the new method is\nabout 1/3 to 1/10 of that of PATH for the dense linear complementarity problem.",
    "descriptor": "",
    "authors": [
      "Xin-long Luo",
      "Sen Zhang",
      "Hang Xiao"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Mathematical Software (cs.MS)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.10727"
  },
  {
    "id": "arXiv:2205.10732",
    "title": "Robust Flow-based Conformal Inference (FCI) with Statistical Guarantee",
    "abstract": "Conformal prediction aims to determine precise levels of confidence in\npredictions for new objects using past experience. However, the commonly used\nexchangeable assumptions between the training data and testing data limit its\nusage in dealing with contaminated testing sets. In this paper, we develop a\nseries of conformal inference methods, including building predictive sets and\ninferring outliers for complex and high-dimensional data. We leverage ideas\nfrom adversarial flow to transfer the input data to a random vector with known\ndistributions, which enable us to construct a non-conformity score for\nuncertainty quantification. We can further learn the distribution of input data\nin each class directly through the learned transformation. Therefore, our\napproach is applicable and more robust when the test data is contaminated. We\nevaluate our method, robust flow-based conformal inference, on benchmark\ndatasets. We find that it produces effective prediction sets and accurate\noutlier detection and is more powerful relative to competing approaches.",
    "descriptor": "",
    "authors": [
      "Youhui Ye",
      "Meimei Liu",
      "Xin Xing"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10732"
  },
  {
    "id": "arXiv:2205.10740",
    "title": "Exact SDP Formulation for Discrete-Time Covariance Steering with  Wasserstein Terminal Cost",
    "abstract": "In this paper, we present new results on the covariance steering problem with\nWasserstein distance terminal cost. We show that the state history feedback\ncontrol policy parametrization, which has been used before to solve this class\nof problems, requires an unnecessarily large number of variables and can be\nreplaced by a randomized state feedback policy which leads to more tractable\nproblem formulations without any performance loss. In particular, we show that\nunder the latter policy, the problem can be equivalently formulated as a\nsemi-definite program (SDP) which is in sharp contrast with our previous\nresults that could only guarantee that the stochastic optimal control problem\ncan be reduced to a difference of convex functions program. Then, we show that\nthe optimal policy that is found by solving the associated SDP corresponds to a\ndeterministic state feedback policy. Finally, we present non-trivial numerical\nsimulations which show the benefits of our proposed randomized state feedback\npolicy derived from the SDP formulation of the problem over existing approaches\nin the field in terms of computational efficacy and controller performance.",
    "descriptor": "",
    "authors": [
      "Isin M. Balci",
      "Efstathios Bakolas"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10740"
  },
  {
    "id": "arXiv:2205.10757",
    "title": "Deep Feature Fusion via Graph Convolutional Network for Intracranial  Artery Labeling",
    "abstract": "Intracranial arteries are critical blood vessels that supply the brain with\noxygenated blood. Intracranial artery labels provide valuable guidance and\nnavigation to numerous clinical applications and disease diagnoses. Various\nmachine learning algorithms have been carried out for automation in the\nanatomical labeling of cerebral arteries. However, the task remains challenging\nbecause of the high complexity and variations of intracranial arteries. This\nstudy investigates a novel graph convolutional neural network with deep feature\nfusion for cerebral artery labeling. We introduce stacked graph convolutions in\nan encoder-core-decoder architecture, extracting high-level representations\nfrom graph nodes and their neighbors. Furthermore, we efficiently aggregate\nintermediate features from different hierarchies to enhance the proposed\nmodel's representation capability and labeling performance. We perform\nextensive experiments on public datasets, in which the results prove the\nsuperiority of our approach over baselines by a clear margin.",
    "descriptor": "\nComments: Accepted by the 44th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2022)\n",
    "authors": [
      "Yaxin Zhu",
      "Peisheng Qian",
      "Ziyuan Zhao",
      "Zeng Zeng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10757"
  },
  {
    "id": "arXiv:2205.10758",
    "title": "Residual Channel Attention Network for Brain Glioma Segmentation",
    "abstract": "A glioma is a malignant brain tumor that seriously affects cognitive\nfunctions and lowers patients' life quality. Segmentation of brain glioma is\nchallenging because of interclass ambiguities in tumor regions. Recently, deep\nlearning approaches have achieved outstanding performance in the automatic\nsegmentation of brain glioma. However, existing algorithms fail to exploit\nchannel-wise feature interdependence to select semantic attributes for glioma\nsegmentation. In this study, we implement a novel deep neural network that\nintegrates residual channel attention modules to calibrate intermediate\nfeatures for glioma segmentation. The proposed channel attention mechanism\nadaptively weights feature channel-wise to optimize the latent representation\nof gliomas. We evaluate our method on the established dataset BraTS2017.\nExperimental results indicate the superiority of our method.",
    "descriptor": "\nComments: Accepted by the 44th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2022)\n",
    "authors": [
      "Yiming Yao",
      "Peisheng Qian",
      "Ziyuan Zhao",
      "Zeng Zeng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10758"
  },
  {
    "id": "arXiv:2205.10772",
    "title": "Fast Instrument Learning with Faster Rates",
    "abstract": "We investigate nonlinear instrumental variable (IV) regression given\nhigh-dimensional instruments. We propose a simple algorithm which combines\nkernelized IV methods and an arbitrary, adaptive regression algorithm, accessed\nas a black box. Our algorithm enjoys faster-rate convergence and adapts to the\ndimensionality of informative latent features, while avoiding an expensive\nminimax optimization procedure, which has been necessary to establish similar\nguarantees. It further brings the benefit of flexible machine learning models\nto quasi-Bayesian uncertainty quantification, likelihood-based model selection,\nand model averaging. Simulation studies demonstrate the competitive performance\nof our method.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Ziyu Wang",
      "Yuhao Zhou",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2205.10772"
  },
  {
    "id": "arXiv:2205.10861",
    "title": "Contrastive Learning of Coarse-Grained Force Fields",
    "abstract": "Coarse-grained models have proven helpful for simulating complex systems over\nlong timescales to provide molecular insights into various processes.\nMethodologies for systematic parameterization of the underlying energy\nfunction, or force field that describes the interactions among different\ncomponents of the system are of great interest for ensuring simulation\naccuracy. We present a new method, potential contrasting, to enable efficient\nlearning of force fields that can accurately reproduce the conformational\ndistribution produced with all-atom simulations. Potential contrasting\ngeneralizes the noise contrastive estimation method with umbrella sampling to\nbetter learn the complex energy landscape of molecular systems. When applied to\nthe Trp-cage protein, we found that the technique produces force fields that\nthoroughly capture the thermodynamics of the folding process despite the use of\nonly $\\alpha$-Carbons in the coarse-grained model. We further showed that\npotential contrasting could be applied over large datasets that combine the\nconformational ensembles of many proteins to ensure the transferability of\ncoarse-grained force fields. We anticipate potential contrasting to be a\npowerful tool for building general-purpose coarse-grained force fields.",
    "descriptor": "",
    "authors": [
      "Xinqiang Ding",
      "Bin Zhang"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Artificial Intelligence (cs.AI)",
      "Biological Physics (physics.bio-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.10861"
  },
  {
    "id": "arXiv:2205.10864",
    "title": "Federated Learning Aggregation: New Robust Algorithms with Guarantees",
    "abstract": "Federated Learning has been recently proposed for distributed model training\nat the edge. The principle of this approach is to aggregate models learned on\ndistributed clients to obtain a new more general \"average\" model (FedAvg). The\nresulting model is then redistributed to clients for further training. To date,\nthe most popular federated learning algorithm uses coordinate-wise averaging of\nthe model parameters for aggregation. In this paper, we carry out a complete\ngeneral mathematical convergence analysis to evaluate aggregation strategies in\na federated learning framework. From this, we derive novel aggregation\nalgorithms which are able to modify their model architecture by differentiating\nclient contributions according to the value of their losses. Moreover, we go\nbeyond the assumptions introduced in theory, by evaluating the performance of\nthese strategies and by comparing them with the one of FedAvg in classification\ntasks in both the IID and the Non-IID framework without additional hypothesis.",
    "descriptor": "",
    "authors": [
      "Adnan Ben Mansour",
      "Gaia Carenini",
      "Alexandre Duplessis",
      "David Naccache"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10864"
  },
  {
    "id": "arXiv:2205.10885",
    "title": "Improving AMD diagnosis by the simultaneous identification of associated  retinal lesions",
    "abstract": "Age-related Macular Degeneration (AMD) is the predominant cause of blindness\nin developed countries, specially in elderly people. Moreover, its prevalence\nis increasing due to the global population ageing. In this scenario, early\ndetection is crucial to avert later vision impairment. Nonetheless,\nimplementing large-scale screening programmes is usually not viable, since the\npopulation at-risk is large and the analysis must be performed by expert\nclinicians. Also, the diagnosis of AMD is considered to be particularly\ndifficult, as it is characterized by many different lesions that, in many\ncases, resemble those of other macular diseases. To overcome these issues,\nseveral works have proposed automatic methods for the detection of AMD in\nretinography images, the most widely used modality for the screening of the\ndisease. Nowadays, most of these works use Convolutional Neural Networks (CNNs)\nfor the binary classification of images into AMD and non-AMD classes. In this\nwork, we propose a novel approach based on CNNs that simultaneously performs\nAMD diagnosis and the classification of its potential lesions. This latter\nsecondary task has not yet been addressed in this domain, and provides\ncomplementary useful information that improves the diagnosis performance and\nhelps understanding the decision. A CNN model is trained using retinography\nimages with image-level labels for both AMD and lesion presence, which are\nrelatively easy to obtain. The experiments conducted in several public datasets\nshow that the proposed approach improves the detection of AMD, while achieving\nsatisfactory results in the identification of most lesions.",
    "descriptor": "\nComments: Accepted at 21st International Conference on Image Analysis and Processing (ICIAP 2021). The final authenticated publication is available online at this https URL\n",
    "authors": [
      "Jos\u00e9 Morano",
      "\u00c1lvaro S. Hervella",
      "Jos\u00e9 Rouco",
      "Jorge Novo",
      "Jos\u00e9 I. Fern\u00e1ndez-Vigo",
      "Marcos Ortega"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10885"
  },
  {
    "id": "arXiv:2205.10890",
    "title": "Nonparametric likelihood-free inference with Jensen-Shannon divergence  for simulator-based models with categorical output",
    "abstract": "Likelihood-free inference for simulator-based statistical models has recently\nattracted a surge of interest, both in the machine learning and statistics\ncommunities. The primary focus of these research fields has been to approximate\nthe posterior distribution of model parameters, either by various types of\nMonte Carlo sampling algorithms or deep neural network -based surrogate models.\nFrequentist inference for simulator-based models has been given much less\nattention to date, despite that it would be particularly amenable to\napplications with big data where implicit asymptotic approximation of the\nlikelihood is expected to be accurate and can leverage computationally\nefficient strategies. Here we derive a set of theoretical results to enable\nestimation, hypothesis testing and construction of confidence intervals for\nmodel parameters using asymptotic properties of the Jensen--Shannon divergence.\nSuch asymptotic approximation offers a rapid alternative to more\ncomputation-intensive approaches and can be attractive for diverse applications\nof simulator-based models. 61",
    "descriptor": "\nComments: 61 pages, 14 figures\n",
    "authors": [
      "Jukka Corander",
      "Ulpu Remes",
      "Ida Holopainen",
      "Timo Koski"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10890"
  },
  {
    "id": "arXiv:2205.10907",
    "title": "Improved Modeling of Persistence Diagram",
    "abstract": "High-dimensional reduction methods are powerful tools for describing the main\npatterns in big data. One of these methods is the topological data analysis\n(TDA), which modeling the shape of the data in terms of topological properties.\nThis method specifically translates the original data into two-dimensional\nsystem, which is graphically represented via the 'persistence diagram'. The\noutliers points on this diagram present the data pattern, whereas the other\npoints behave as a random noise. In order to determine which points are\nsignificant outliers, replications of the original data set are needed. Once\nonly one original data is available, replications can be created by fitting a\nmodel for the points on the persistence diagram, and then using the MCMC\nmethods. One of such model is the RST (Replicating Statistical Topology). In\nthis paper we suggest a modification of the RST model. Using a simulation\nstudy, we show that the modified RST improves the performance of the RST in\nterms of goodness of fit. We use the MCMC Metropolis-Hastings algorithm for\nsampling according to the fitted model.",
    "descriptor": "",
    "authors": [
      "Sarit Agami"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10907"
  },
  {
    "id": "arXiv:2205.10912",
    "title": "Limitations of a proposed correction for slow drifts in decision  criterion",
    "abstract": "Trial history biases in decision-making tasks are thought to reflect\nsystematic updates of decision variables, therefore their precise nature\ninforms conclusions about underlying heuristic strategies and learning\nprocesses. However, random drifts in decision variables can corrupt this\ninference by mimicking the signatures of systematic updates. Hence, identifying\nthe trial-by-trial evolution of decision variables requires methods that can\nrobustly account for such drifts. Recent studies (Lak'20, Mendon\\c{c}a'20) have\nmade important advances in this direction, by proposing a convenient method to\ncorrect for the influence of slow drifts in decision criterion, a key decision\nvariable. Here we apply this correction to a variety of updating scenarios, and\nevaluate its performance. We show that the correction fails for a wide range of\ncommonly assumed systematic updating strategies, distorting one's inference\naway from the veridical strategies towards a narrow subset. To address these\nlimitations, we propose a model-based approach for disambiguating systematic\nupdates from random drifts, and demonstrate its success on real and synthetic\ndatasets. We show that this approach accurately recovers the latent trajectory\nof drifts in decision criterion as well as the generative systematic updates\nfrom simulated data. Our results offer recommendations for methods to account\nfor the interactions between history biases and slow drifts, and highlight the\nadvantages of incorporating assumptions about the generative process directly\ninto models of decision-making.",
    "descriptor": "\nComments: 18 pages, 4 figures\n",
    "authors": [
      "Diksha Gupta",
      "Carlos D. Brody"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.10912"
  },
  {
    "id": "arXiv:2205.10968",
    "title": "Eigenvalue bounds of the Kirchhoff Laplacian",
    "abstract": "We prove that each eigenvalue l(k) of the Kirchhoff Laplacian K of a graph is\nbounded above by d(k)+d(k-1) for all k in {1,...,n}. Here l(1),...,l(n) is a\nnon-decreasing list of the eigenvalues of K and d(1),..,d(n) is a\nnon-decreasing list of vertex degrees with the additional assumption d(0)=0.\nThe case k=2 is a case for the Schur-Horn inequality l(1)+l(2)+...+l(k) bounded\nabove by d(1)+d(2)+...+d(k). The case k=n is a result of Anderson and Morley.\nAlready the corollary l(k) less or equal to 2 d(k) is in this case stronger\nthan the Gershgorin circle theorem assuring that every disk of radius d(k)\ncentered at d(k) contains the eigenvalue of K. We prove our theorem using the\nCauchy interlace theorem.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Oliver Knill"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.10968"
  },
  {
    "id": "arXiv:2205.10969",
    "title": "Application of tropical optimization for solving multicriteria problems  of pairwise comparisons using log-Chebyshev approximation",
    "abstract": "We consider a decision-making problem to find absolute ratings of\nalternatives that are compared in pairs under multiple criteria, subject to\nconstraints in the form of two-sided bounds on ratios between the ratings.\nGiven matrices of pairwise comparisons made according to the criteria, the\nproblem is formulated as the log-Chebyshev approximation of these matrices by a\ncommon consistent matrix (a symmetrically reciprocal matrix of unit rank) to\nminimize the approximation errors for all matrices simultaneously. We rearrange\nthe approximation problem as a constrained multiobjective optimization problem\nof finding a vector that determines the approximating consistent matrix. The\noptimization problem is then represented in the framework of tropical algebra\nconcerning the theory and applications of algebraic systems with idempotent\noperations. We apply methods and results of tropical optimization to handle the\nmultiobjective optimization problem according to various principles of\noptimality. Complete solutions in the sense of the max-ordering, lexicographic\nordering and lexicographic max-ordering optimality are obtained, which are\ngiven in a compact vector form ready for formal analysis and efficient\ncomputation. We present numerical examples of solving a multi-criteria problem\nof rating four alternatives from pairwise comparisons.",
    "descriptor": "\nComments: 34 pages\n",
    "authors": [
      "Nikolai Krivulin"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.10969"
  },
  {
    "id": "arXiv:2205.10972",
    "title": "Global Extreme Heat Forecasting Using Neural Weather Models",
    "abstract": "Heat waves are projected to increase in frequency and severity with global\nwarming. Improved warning systems would help reduce the associated loss of\nlives, wildfires, power disruptions, and reduction in crop yields. In this\nwork, we explore the potential for deep learning systems trained on historical\ndata to forecast extreme heat on short, medium and subseasonal timescales. To\nthis purpose, we train a set of neural weather models (NWMs) with convolutional\narchitectures to forecast surface temperature anomalies globally, 1 to 28 days\nahead, at $\\sim200~\\mathrm{km}$ resolution and on the cubed sphere. The NWMs\nare trained using the ERA5 reanalysis product and a set of candidate loss\nfunctions, including the mean squared error and exponential losses targeting\nextremes. We find that training models to minimize custom losses tailored to\nemphasize extremes leads to significant skill improvements in the heat wave\nprediction task, compared to NWMs trained on the mean squared error loss. This\nimprovement is accomplished with almost no skill reduction in the general\ntemperature prediction task, and it can be efficiently realized through\ntransfer learning, by re-training NWMs with the custom losses for a few epochs.\nIn addition, we find that the use of a symmetric exponential loss reduces the\nsmoothing of NWM forecasts with lead time. Our best NWM is able to outperform\npersistence in a regressive sense for all lead times and temperature anomaly\nthresholds considered, and shows positive regressive skill compared to the\nECMWF subseasonal-to-seasonal control forecast within the first two forecast\ndays and after two weeks.",
    "descriptor": "",
    "authors": [
      "Ignacio Lopez-Gomez",
      "Amy McGovern",
      "Shreya Agrawal",
      "Jason Hickey"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10972"
  },
  {
    "id": "arXiv:2205.10999",
    "title": "The Tree-Forest Ratio",
    "abstract": "The number of rooted spanning forests divided by the number of spanning\nrooted trees in a graph G with Kirchhoff matrix K is the spectral quantity\ntau(G)= det(1+K)/det(K) of G by the matrix tree and matrix forest theorems. We\nprove that that under Barycentric refinements, the tree index\nT(G)=log(det(K))/n and forest index F(G)=log(det(1+K))/n and so the tree-forest\nindex i=F-G=log(tau(G))/n converge to numbers that only depend on the size of\nthe maximal clique in the graph. In the 1-dimensional case, all numbers are\nknown: T(G)=0, F(G)=i(G) =2 log(phi), where phi is the golden ratio. The\nconvergent proof uses the Barycentral limit theorem assuring the Kirchhoff\nspectrum converges weakly to a measure dk on the positive real axis that only\ndepends on dimension of G. Trees and forests indices are potential values i =\nU(-1)-U(0) for the subharmonic function U(z)=int_R log|x-z| dk(x) defined by\nthe Riesz measure dk=Delta U which only depends on the dimension of G. The\npotential U(z) is defined for all z away from the support of dk and finite at\nz=0. Convergence follows from the tail estimate k[x,infty] < C exp(-a x) where\nthe decay rate a only depends on the maximal dimension. With the normalized\nzeta function zeta(s) = (1/n) sum_k lambda_k^-s, we have for all finite graphs\nof maximal dimension larger than 1 the identity i(G) = sum_t (-1)^(s+1)\nzeta(s)/s. The limiting zeta function zeta(s) = int_R x^(-s) dk(x) is analytic\nin s for s<0. The Hurwitz spectral zeta function zeta_z(s)=U_s(z) = int_R\n(x-z)^(-s) dk(x) complements U(z) = int_R log(x-z) dk(x) and is analytic for z\nin C - R^+ and for fixed z in C-R^+ is an entire function in s in C.",
    "descriptor": "\nComments: 19 pages, 3 figures\n",
    "authors": [
      "Oliver Knill"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.10999"
  },
  {
    "id": "arXiv:2205.11006",
    "title": "Nonparametric learning of kernels in nonlocal operators",
    "abstract": "Nonlocal operators with integral kernels have become a popular tool for\ndesigning solution maps between function spaces, due to their efficiency in\nrepresenting long-range dependence and the attractive feature of being\nresolution-invariant. In this work, we provide a rigorous identifiability\nanalysis and convergence study for the learning of kernels in nonlocal\noperators. It is found that the kernel learning is an ill-posed or even\nill-defined inverse problem, leading to divergent estimators in the presence of\nmodeling errors or measurement noises. To resolve this issue, we propose a\nnonparametric regression algorithm with a novel data adaptive RKHS Tikhonov\nregularization method based on the function space of identifiability. The\nmethod yields a noisy-robust convergent estimator of the kernel as the data\nresolution refines, on both synthetic and real-world datasets. In particular,\nthe method successfully learns a homogenized model for the stress wave\npropagation in a heterogeneous solid, revealing the unknown governing laws from\nreal-world data at microscale. Our regularization method outperforms baseline\nmethods in robustness, generalizability and accuracy.",
    "descriptor": "",
    "authors": [
      "Fei Lu",
      "Qingci An",
      "Yue Yu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11006"
  },
  {
    "id": "arXiv:2205.11017",
    "title": "Generalized fusible numbers and their ordinals",
    "abstract": "Erickson defined the fusible numbers as a set $\\mathcal F$ of reals generated\nby repeated application of the function $\\frac{x+y+1}{2}$. Erickson, Nivasch,\nand Xu showed that $\\mathcal F$ is well ordered, with order type\n$\\varepsilon_0$. They also investigated a recursively defined function $M\\colon\n\\mathbb{R}\\to\\mathbb{R}$. They showed that the set of points of discontinuity\nof $M$ is a subset of $\\mathcal F$ of order type $\\varepsilon_0$. They also\nshowed that, although $M$ is a total function on $\\mathbb R$, the fact that the\nrestriction of $M$ to $\\mathbb{Q}$ is total is not provable in first-order\nPeano arithmetic $\\mathsf{PA}$.\nIn this paper we explore the problem (raised by Friedman) of whether similar\napproaches can yield well-ordered sets $\\mathcal F$ of larger order types. As\nFriedman pointed out, Kruskal's tree theorem yields an upper bound of the small\nVeblen ordinal for the order type of any set generated in a similar way by\nrepeated application of a monotone function $g:\\mathbb R^n\\to\\mathbb R$.\nThe most straightforward generalization of $\\frac{x+y+1}{2}$ to an $n$-ary\nfunction is the function $\\frac{x_1+\\ldots+x_n+1}{n}$. We show that this\nfunction generates a set $\\mathcal F_n$ whose order type is just\n$\\varphi_{n-1}(0)$. For this, we develop recursively defined functions\n$M_n\\colon \\mathbb{R}\\to\\mathbb{R}$ naturally generalizing the function $M$.\nFurthermore, we prove that for any linear function $g:\\mathbb R^n\\to\\mathbb\nR$, the order type of the resulting $\\mathcal F$ is at most $\\varphi_{n-1}(0)$.\nFinally, we show that there do exist continuous functions $g:\\mathbb\nR^n\\to\\mathbb R$ for which the order types of the resulting sets $\\mathcal F$\napproach the small Veblen ordinal.",
    "descriptor": "\nComments: 23 pages, 1 figure\n",
    "authors": [
      "Alexander I. Bufetov",
      "Gabriel Nivasch",
      "Fedor Pakhomov"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.11017"
  },
  {
    "id": "arXiv:2205.11030",
    "title": "HessianFR: An Efficient Hessian-based Follow-the-Ridge Algorithm for  Minimax Optimization",
    "abstract": "Wide applications of differentiable two-player sequential games (e.g., image\ngeneration by GANs) have raised much interest and attention of researchers to\nstudy efficient and fast algorithms. Most of the existing algorithms are\ndeveloped based on nice properties of simultaneous games, i.e., convex-concave\npayoff functions, but are not applicable in solving sequential games with\ndifferent settings. Some conventional gradient descent ascent algorithms\ntheoretically and numerically fail to find the local Nash equilibrium of the\nsimultaneous game or the local minimax (i.e., local Stackelberg equilibrium) of\nthe sequential game. In this paper, we propose the HessianFR, an efficient\nHessian-based Follow-the-Ridge algorithm with theoretical guarantees.\nFurthermore, the convergence of the stochastic algorithm and the approximation\nof Hessian inverse are exploited to improve algorithm efficiency. A series of\nexperiments of training generative adversarial networks (GANs) have been\nconducted on both synthetic and real-world large-scale image datasets (e.g.\nMNIST, CIFAR-10 and CelebA). The experimental results demonstrate that the\nproposed HessianFR outperforms baselines in terms of convergence and image\ngeneration quality.",
    "descriptor": "",
    "authors": [
      "Yihang Gao",
      "Huafeng Liu",
      "Michael K. Ng",
      "Mingjie Zhou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11030"
  },
  {
    "id": "arXiv:2205.11033",
    "title": "Augmented Newton Method for Optimization: Global Linear Rate and  Momentum Interpretation",
    "abstract": "We propose two variants of Newton method for solving unconstrained\nminimization problem. Our method leverages optimization techniques such as\npenalty and augmented Lagrangian method to generate novel variants of the\nNewton method namely the Penalty Newton method and the Augmented Newton method.\nIn doing so, we recover several well-known existing Newton method variants such\nas Damped Newton, Levenberg, and Levenberg-Marquardt methods as special cases.\nMoreover, the proposed Augmented Newton method can be interpreted as Newton\nmethod with adaptive heavy ball momentum. We provide global convergence results\nfor the proposed methods under mild assumptions that hold for a wide variety of\nproblems. The proposed methods can be sought as the penalty and augmented\nextensions of the results obtained by Karimireddy et. al [24].",
    "descriptor": "",
    "authors": [
      "Md Sarowar Morshed"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11033"
  },
  {
    "id": "arXiv:2205.11034",
    "title": "Watermarking PRFs against Quantum Adversaries",
    "abstract": "We initiate the study of software watermarking against quantum adversaries. A\nquantum adversary generates a quantum state as a pirate software that\npotentially removes an embedded message from a classical marked software.\nExtracting an embedded message from quantum pirate software is difficult since\nmeasurement could irreversibly alter the quantum state.\nIn this work, we define secure watermarking PRFs for quantum adversaries\n(unremovability against quantum adversaries). We also present two watermarking\nPRFs as follows.\n- We construct a privately extractable watermarking PRF against quantum\nadversaries from the quantum hardness of the learning with errors (LWE)\nproblem. The marking and extraction algorithms use a public parameter and a\nprivate extraction key, respectively. The watermarking PRF is unremovable even\nif adversaries have (the public parameter and) access to the extraction oracle,\nwhich returns a result of extraction for a queried quantum circuit.\n- We construct a publicly extractable watermarking PRF against quantum\nadversaries from indistinguishability obfuscation (IO) and the quantum hardness\nof the LWE problem. The marking and extraction algorithms use a public\nparameter and a public extraction key, respectively. The watermarking PRF is\nunremovable even if adversaries have the extraction key (and the public\nparameter).\nWe develop a quantum extraction technique to extract information (a classical\nstring) from a quantum state without destroying the state too much. We also\nintroduce the notion of extraction-less watermarking PRFs as a crucial building\nblock to achieve the results above by combining the tool with our quantum\nextraction technique.",
    "descriptor": "",
    "authors": [
      "Fuyuki Kitagawa",
      "Ryo Nishimaki"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11034"
  },
  {
    "id": "arXiv:2205.11052",
    "title": "ALPINE: A set of performance portable plasma physics particle-in-cell  mini-apps for exascale computing",
    "abstract": "Alpine consists of a set of mini-apps that makes use of exascale computing\ncapabilities to numerically solve some classical problems in plasma physics. It\nis based on IPPL (Independent Parallel Particle Layer), a framework that is\ndesigned around performance portable and dimension independent particles and\nfields. In this work, IPPL is used to implement a particle-in-cell scheme. The\narticle describes in detail the following mini-apps: weak and strong Landau\ndamping, bump-on-tail and two-stream instabilities, and the dynamics of an\nelectron bunch in a charge-neutral Penning trap. We benchmark the simulations\nwith varying parameters such as grid resolutions ($512^3$ to $2048^3$) and\nnumber of simulation particles ($10^9$ to $10^{11}$). We show strong and weak\nscaling and analyze the performance of different components on several\npre-exascale architectures such as Piz-Daint, Cori, Summit and Perlmutter.",
    "descriptor": "",
    "authors": [
      "Sriramkrishnan Muralikrishnan",
      "Matthias Frey",
      "Alessandro Vinciguerra",
      "Michael Ligotino",
      "Antoine J. Cerfon",
      "Miroslav Stoyanov",
      "Rahulkumar Gayatri",
      "Andreas Adelmann"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Numerical Analysis (math.NA)",
      "Plasma Physics (physics.plasm-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.11052"
  },
  {
    "id": "arXiv:2205.11078",
    "title": "Beyond EM Algorithm on Over-specified Two-Component Location-Scale  Gaussian Mixtures",
    "abstract": "The Expectation-Maximization (EM) algorithm has been predominantly used to\napproximate the maximum likelihood estimation of the location-scale Gaussian\nmixtures. However, when the models are over-specified, namely, the chosen\nnumber of components to fit the data is larger than the unknown true number of\ncomponents, EM needs a polynomial number of iterations in terms of the sample\nsize to reach the final statistical radius; this is computationally expensive\nin practice. The slow convergence of EM is due to the missing of the locally\nstrong convexity with respect to the location parameter on the negative\npopulation log-likelihood function, i.e., the limit of the negative sample\nlog-likelihood function when the sample size goes to infinity. To efficiently\nexplore the curvature of the negative log-likelihood functions, by specifically\nconsidering two-component location-scale Gaussian mixtures, we develop the\nExponential Location Update (ELU) algorithm. The idea of the ELU algorithm is\nthat we first obtain the exact optimal solution for the scale parameter and\nthen perform an exponential step-size gradient descent for the location\nparameter. We demonstrate theoretically and empirically that the ELU iterates\nconverge to the final statistical radius of the models after a logarithmic\nnumber of iterations. To the best of our knowledge, it resolves the\nlong-standing open question in the literature about developing an optimization\nalgorithm that has optimal statistical and computational complexities for\nsolving parameter estimation even under some specific settings of the\nover-specified Gaussian mixture models.",
    "descriptor": "\nComments: 38 pages, 4 figures. Tongzheng Ren and Fuheng Cui contributed equally to this work\n",
    "authors": [
      "Tongzheng Ren",
      "Fuheng Cui",
      "Sujay Sanghavi",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.11078"
  },
  {
    "id": "arXiv:2205.11079",
    "title": "Finite volume scheme and renormalized solutions for nonlinear elliptic  Neumann problem with L1 data",
    "abstract": "In this paper we study the convergence of a finite volume approximation of a\nconvective diffusive elliptic problem with Neumann boundary conditions and L 1\ndata. To deal with the non-coercive character of the equation and the low\nregularity of the right hand-side we mix the finite volume tools and the\nrenormalized techniques. To handle the Neumann boundary conditions we choose\nsolutions having a null median and we prove a convergence result.",
    "descriptor": "",
    "authors": [
      "Mirella Aoun",
      "Olivier Guib\u00e9"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11079"
  },
  {
    "id": "arXiv:2205.11096",
    "title": "FedNorm: Modality-Based Normalization in Federated Learning for  Multi-Modal Liver Segmentation",
    "abstract": "Given the high incidence and effective treatment options for liver diseases,\nthey are of great socioeconomic importance. One of the most common methods for\nanalyzing CT and MRI images for diagnosis and follow-up treatment is liver\nsegmentation. Recent advances in deep learning have demonstrated encouraging\nresults for automatic liver segmentation. Despite this, their success depends\nprimarily on the availability of an annotated database, which is often not\navailable because of privacy concerns. Federated Learning has been recently\nproposed as a solution to alleviate these challenges by training a shared\nglobal model on distributed clients without access to their local databases.\nNevertheless, Federated Learning does not perform well when it is trained on a\nhigh degree of heterogeneity of image data due to multi-modal imaging, such as\nCT and MRI, and multiple scanner types. To this end, we propose Fednorm and its\nextension \\fednormp, two Federated Learning algorithms that use a\nmodality-based normalization technique. Specifically, Fednorm normalizes the\nfeatures on a client-level, while Fednorm+ employs the modality information of\nsingle slices in the feature normalization. Our methods were validated using\n428 patients from six publicly available databases and compared to\nstate-of-the-art Federated Learning algorithms and baseline models in\nheterogeneous settings (multi-institutional, multi-modal data). The\nexperimental results demonstrate that our methods show an overall acceptable\nperformance, achieve Dice per patient scores up to 0.961, consistently\noutperform locally trained models, and are on par or slightly better than\ncentralized models.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Tobias Bernecker",
      "Annette Peters",
      "Christopher L. Schlett",
      "Fabian Bamberg",
      "Fabian Theis",
      "Daniel Rueckert",
      "Jakob Wei\u00df",
      "Shadi Albarqouni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11096"
  },
  {
    "id": "arXiv:2205.11099",
    "title": "B\u00e9zier Flow: a Surface-wise Gradient Descent Method for  Multi-objective Optimization",
    "abstract": "In this paper, we propose a strategy to construct a multi-objective\noptimization algorithm from a single-objective optimization algorithm by using\nthe B\\'ezier simplex model. Also, we extend the stability of optimization\nalgorithms in the sense of Probability Approximately Correct (PAC) learning and\ndefine the PAC stability. We prove that it leads to an upper bound on the\ngeneralization with high probability. Furthermore, we show that multi-objective\noptimization algorithms derived from a gradient descent-based single-objective\noptimization algorithm are PAC stable. We conducted numerical experiments and\ndemonstrated that our method achieved lower generalization errors than the\nexisting multi-objective optimization algorithm.",
    "descriptor": "",
    "authors": [
      "Akiyoshi Sannai",
      "Yasunari Hikima",
      "Ken Kobayashi",
      "Akinori Tanaka",
      "Naoki Hamada"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11099"
  },
  {
    "id": "arXiv:2205.11106",
    "title": "An improved neural network model for treatment effect estimation",
    "abstract": "Nowadays, in many scientific and industrial fields there is an increasing\nneed for estimating treatment effects and answering causal questions. The key\nfor addressing these problems is the wealth of observational data and the\nprocesses for leveraging this data. In this work, we propose a new model for\npredicting the potential outcomes and the propensity score, which is based on a\nneural network architecture. The proposed model exploits the covariates as well\nas the outcomes of neighboring instances in training data. Numerical\nexperiments illustrate that the proposed model reports better treatment effect\nestimation performance compared to state-of-the-art models.",
    "descriptor": "\nComments: This paper has been accepted for publication on the 18th International Conference on Artificial Intelligence Applications and Innovations\n",
    "authors": [
      "Niki Kiriakidou",
      "Christos Diou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11106"
  },
  {
    "id": "arXiv:2205.11115",
    "title": "DTU-Net: Learning Topological Similarity for Curvilinear Structure  Segmentation",
    "abstract": "Curvilinear structure segmentation plays an important role in many\napplications. The standard formulation of segmentation as pixel-wise\nclassification often fails to capture these structures due to the small size\nand low contrast. Some works introduce prior topological information to address\nthis problem with the cost of expensive computations and the need for extra\nlabels. Moreover, prior work primarily focuses on avoiding false splits by\nencouraging the connection of small gaps. Less attention has been given to\navoiding missed splits, namely the incorrect inference of structures that are\nnot visible in the image.\nIn this paper, we present DTU-Net, a dual-decoder and topology-aware deep\nneural network consisting of two sequential light-weight U-Nets, namely a\ntexture net, and a topology net. The texture net makes a coarse prediction\nusing image texture information. The topology net learns topological\ninformation from the coarse prediction by employing a triplet loss trained to\nrecognize false and missed splits, and provides a topology-aware separation of\nthe foreground and background. The separation is further utilized to correct\nthe coarse prediction. We conducted experiments on a challenging multi-class\nultrasound scan segmentation dataset and an open dataset for road extraction.\nResults show that our model achieves state-of-the-art results in both\nsegmentation accuracy and continuity. Compared to existing methods, our model\ncorrects both false positive and false negative examples more effectively with\nno need for prior knowledge.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Manxi Lin",
      "Zahra Bashir",
      "Martin Gr\u00f8nneb\u00e6k Tolsgaard",
      "Anders Nymark Christensen",
      "Aasa Feragen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11115"
  },
  {
    "id": "arXiv:2205.11119",
    "title": "Nested Primal-dual Gradient Algorithms for Distributed  Constraint-coupled Optimization",
    "abstract": "We study a class of distributed optimization problems with a globally coupled\nequality constraint. A novel nested primal-dual gradient algorithm (NPGA) is\nproposed from the dual perspective, which can achieve linear convergence under\na quite weak condition. Furthermore, the upper bounds of the step-sizes and the\nconverge rate are explicitly given. It is worth noting that NPGA is not only an\nalgorithm but also an algorithmic framework. By choosing different parameter\nmatrices, we can obtain many different versions of NPGA, which offers us a\nchance to design more efficient algorithms. Finally, the convergence rates of\nNPGA and existing algorithms are compared in numerical experiments.",
    "descriptor": "",
    "authors": [
      "Jingwang Li",
      "Housheng Su"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11119"
  },
  {
    "id": "arXiv:2205.11142",
    "title": "Stability of the scattering transform for deformations with minimal  regularity",
    "abstract": "Within the mathematical analysis of deep convolutional neural networks, the\nwavelet scattering transform introduced by St\\'ephane Mallat is a unique\nexample of how the ideas of multiscale analysis can be combined with a cascade\nof modulus nonlinearities to build a nonexpansive, translation invariant signal\nrepresentation with provable geometric stability properties, namely Lipschitz\ncontinuity to the action of small $C^2$ diffeomorphisms - a remarkable result\nfor both theoretical and practical purposes, inherently depending on the choice\nof the filters and their arrangement into a hierarchical architecture. In this\nnote, we further investigate the intimate relationship between the scattering\nstructure and the regularity of the deformation in the H\\\"older regularity\nscale $C^\\alpha$, $\\alpha >0$. We are able to precisely identify the stability\nthreshold, proving that stability is still achievable for deformations of class\n$C^{\\alpha}$, $\\alpha>1$, whereas instability phenomena can occur at lower\nregularity levels modelled by $C^\\alpha$, $0\\le \\alpha <1$. While the behaviour\nat the threshold given by Lipschitz (or even $C^1$) regularity remains beyond\nreach, we are able to prove a stability bound in that case, up to $\\varepsilon$\nlosses.",
    "descriptor": "\nComments: 28 pages, 1 figure\n",
    "authors": [
      "Fabio Nicola",
      "S. Ivan Trapasso"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11142"
  },
  {
    "id": "arXiv:2205.11145",
    "title": "A Coupling Enhancement Algorithm for ZrO2 Ceramic Bearing Ball Surface  Defect Detection Based on Cartoon-texture Decomposition Model and Multi-Scale  Filtering Method",
    "abstract": "This study aimed to improve the surface defect detection accuracy of ZrO2\nceramic bearing balls. Combined with the noise damage of the image samples, a\nsurface defect detection method for ZrO2 ceramic bearing balls based on\ncartoon-texture decomposition model was proposed. Building a ZrO2 ceramic\nbearing ball surface defect detection system. The ZrO2 ceramic bearing ball\nsurface defect image was decomposed by using the Gaussian curvature model and\nthe decomposed image layer was filtered by using Winner filter and wavelet\nvalue domain filter. Then they were fused into a clear and undamaged ZrO2\nceramic bearing ball surface defect image and detected. The experimental\nresults show that the image denoising method of ZrO2 ceramic bearing ball\nsurface defect based on cartoon-texture decomposition model can denoise while\nretaining the image details. The PSNR of image is 34.1 dB, the SSIM is 0.9476,\nthe detection accuracy is 95.8%, and the detection speed of a single defect\nimage is 191ms / img. This method can effectively improve the efficiency and\naccuracy of ZrO2 ceramic bearing ball surface defect detection.",
    "descriptor": "\nComments: 15pages,8 figures\n",
    "authors": [
      "Wei Wang",
      "Xin Zhang",
      "Jiaqi Yi",
      "Xianqi Liao",
      "Wenjie Li",
      "Zhenhong Li"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11145"
  },
  {
    "id": "arXiv:2205.11151",
    "title": "Split personalities in Bayesian Neural Networks: the case for full  marginalisation",
    "abstract": "The true posterior distribution of a Bayesian neural network is massively\nmultimodal. Whilst most of these modes are functionally equivalent, we\ndemonstrate that there remains a level of real multimodality that manifests in\neven the simplest neural network setups. It is only by fully marginalising over\nall posterior modes, using appropriate Bayesian sampling tools, that we can\ncapture the split personalities of the network. The ability of a network\ntrained in this manner to reason between multiple candidate solutions\ndramatically improves the generalisability of the model, a feature we contend\nis not consistently captured by alternative approaches to the training of\nBayesian neural networks. We provide a concise minimal example of this, which\ncan provide lessons and a future path forward for correctly utilising the\nexplainability and interpretability of Bayesian neural networks.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "David Yallup",
      "Will Handley",
      "Mike Hobson",
      "Anthony Lasenby",
      "Pablo Lemos"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11151"
  },
  {
    "id": "arXiv:2205.11153",
    "title": "Decoupling multivariate functions using a nonparametric filtered tensor  decomposition",
    "abstract": "Multivariate functions emerge naturally in a wide variety of data-driven\nmodels. Popular choices are expressions in the form of basis expansions or\nneural networks. While highly effective, the resulting functions tend to be\nhard to interpret, in part because of the large number of required parameters.\nDecoupling techniques aim at providing an alternative representation of the\nnonlinearity. The so-called decoupled form is often a more efficient\nparameterisation of the relationship while being highly structured, favouring\ninterpretability. In this work two new algorithms, based on filtered tensor\ndecompositions of first order derivative information are introduced. The method\nreturns nonparametric estimates of smooth decoupled functions. Direct\napplications are found in, i.a. the fields of nonlinear system identification\nand machine learning.",
    "descriptor": "\nComments: Preprint submitted to Journal of Mechanical Systems and Signal Processing\n",
    "authors": [
      "Jan Decuyper",
      "Koen Tiels",
      "Siep Weiland",
      "Mark C. Runacres",
      "Johan Schoukens"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11153"
  },
  {
    "id": "arXiv:2205.11220",
    "title": "Exploiting Array Geometry for Reduced-Subspace Channel Estimation in  RIS-Aided Communications",
    "abstract": "A reconfigurable intelligent surface (RIS) can be used to improve the channel\ngain between a base station (BS) and user equipment (UE), but only if its $N$\nreflecting elements are configured properly. This requires accurate estimation\nof the cascaded channel from the UE to the BS through each RIS element. If the\nchannel structure is not exploited, pilot sequences of length $N$ must be used,\nwhich is a major practical challenge since $N$ is typically at the order of\nhundreds. To address this problem without requiring user-specific channel\nstatistics, we propose a novel estimator, called reduced-subspace least squares\n(RS-LS) estimator, that only uses knowledge of the array geometry. The RIS\nphase-shift pattern is optimized to minimize the mean-square error of the\nchannel estimates. The RS-LS estimator largely outperforms the conventional\nleast-squares estimator, and can be utilized with a much shorter pilot length\nsince it exploits the fact that the array geometry confines the possible\nchannel realizations to a reduced-rank subspace.",
    "descriptor": "\nComments: Accepted for presentation in IEEE SAM 2022, 5 pages, 2 figures\n",
    "authors": [
      "\u00d6zlem Tu\u011ffe Demir",
      "Emil Bj\u00f6rnson",
      "Luca Sanguinetti"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.11220"
  },
  {
    "id": "arXiv:2205.11243",
    "title": "Spatial Transcriptomics Dimensionality Reduction using Wavelet Bases",
    "abstract": "Spatially resolved transcriptomics (ST) measures gene expression along with\nthe spatial coordinates of the measurements. The analysis of ST data involves\nsignificant computation complexity. In this work, we propose gene expression\ndimensionality reduction algorithm that retains spatial structure. We combine\nthe wavelet transformation with matrix factorization to select\nspatially-varying genes. We extract a low-dimensional representation of these\ngenes. We consider Empirical Bayes setting, imposing regularization through the\nprior distribution of factor genes. Additionally, We provide visualization of\nextracted representation genes capturing the global spatial pattern. We\nillustrate the performance of our methods by spatial structure recovery and\ngene expression reconstruction in simulation. In real data experiments, our\nmethod identifies spatial structure of gene factors and outperforms regular\ndecomposition regarding reconstruction error. We found the connection between\nthe fluctuation of gene patterns and wavelet technique, providing smoother\nvisualization. We develop the package and share the workflow generating\nreproducible quantitative results and gene visualization. The package is\navailable at https://github.com/OliverXUZY/waveST.",
    "descriptor": "",
    "authors": [
      "Zhuoyan Xu",
      "Kris Sankaran"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.11243"
  },
  {
    "id": "arXiv:2205.11287",
    "title": "Recovery of Plane Curves from Branch Points",
    "abstract": "We recover plane curves from their branch points under projection onto a\nline. Our focus lies on cubics and quartics. These have 6 and 12 branch points\nrespectively. The plane Hurwitz numbers 40 and 120 count the orbits of\nsolutions. We determine the numbers of real solutions, and we present exact\nalgorithms for recovery. Our approach relies on 150 years of beautiful\nalgebraic geometry, from Clebsch to Vakil and beyond.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Daniele Agostini",
      "Hannah Markwig",
      "Clemens Nollau",
      "Victoria Schleis",
      "Javier Sendra-Arranz",
      "Bernd Sturmfels"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2205.11287"
  },
  {
    "id": "arXiv:2205.11311",
    "title": "The Topology of Circular Synthetic Aperture Sonar Targets",
    "abstract": "This report presents a connection between the physical acoustics of an object\nand the topology of the space of echoes resulting from a circular synthetic\naperture sonar (CSAS) collection of that object. A simple theoretical model is\ndeveloped that yields a precise, yet qualitative, description of the space of\nechoes. This theoretical model is validated in simulation and with experimental\ndata from a laboratory sonar system.",
    "descriptor": "\nComments: 21 pages, 23 figures\n",
    "authors": [
      "Michael Robinson",
      "Zander Memon"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2205.11311"
  },
  {
    "id": "arXiv:2205.11346",
    "title": "Arbitrary Reduction of MRI Slice Spacing Based on Local-Aware Implicit  Representation",
    "abstract": "Magnetic resonance (MR) images are often acquired in 2D settings for real\nclinical applications. The 3D volumes reconstructed by stacking multiple 2D\nslices have large inter-slice spacing, resulting in lower inter-slice\nresolution than intra-slice resolution. Super-resolution is a powerful tool to\nreduce the inter-slice spacing of 3D images to facilitate subsequent\nvisualization and computation tasks. However, most existing works train the\nsuper-resolution network at a fixed ratio, which is inconvenient in clinical\nscenes due to the heterogeneous parameters in MR scanning. In this paper, we\npropose a single super-resolution network to reduce the inter-slice spacing of\nMR images at an arbitrarily adjustable ratio. Specifically, we view the input\nimage as a continuous implicit function of coordinates. The intermediate slices\nof different spacing ratios could be constructed according to the implicit\nrepresentation up-sampled in the continuous domain. We particularly propose a\nnovel local-aware spatial attention mechanism and long-range residual learning\nto boost the quality of the output image. The experimental results demonstrate\nthe superiority of our proposed method, even compared to the models trained at\na fixed ratio.",
    "descriptor": "",
    "authors": [
      "Xin Wang",
      "Kai Xuan",
      "Sheng Wang",
      "Honglin Xiong",
      "Lichi Zhang",
      "Qian Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11346"
  },
  {
    "id": "arXiv:2205.11361",
    "title": "Chaotic Regularization and Heavy-Tailed Limits for Deterministic  Gradient Descent",
    "abstract": "Recent studies have shown that gradient descent (GD) can achieve improved\ngeneralization when its dynamics exhibits a chaotic behavior. However, to\nobtain the desired effect, the step-size should be chosen sufficiently large, a\ntask which is problem dependent and can be difficult in practice. In this\nstudy, we incorporate a chaotic component to GD in a controlled manner, and\nintroduce multiscale perturbed GD (MPGD), a novel optimization framework where\nthe GD recursion is augmented with chaotic perturbations that evolve via an\nindependent dynamical system. We analyze MPGD from three different angles: (i)\nBy building up on recent advances in rough paths theory, we show that, under\nappropriate assumptions, as the step-size decreases, the MPGD recursion\nconverges weakly to a stochastic differential equation (SDE) driven by a\nheavy-tailed L\\'evy-stable process. (ii) By making connections to recently\ndeveloped generalization bounds for heavy-tailed processes, we derive a\ngeneralization bound for the limiting SDE and relate the worst-case\ngeneralization error over the trajectories of the process to the parameters of\nMPGD. (iii) We analyze the implicit regularization effect brought by the\ndynamical regularization and show that, in the weak perturbation regime, MPGD\nintroduces terms that penalize the Hessian of the loss function. Empirical\nresults are provided to demonstrate the advantages of MPGD.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Soon Hoe Lim",
      "Yijun Wan",
      "Umut \u015eim\u015fekli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.11361"
  },
  {
    "id": "arXiv:2205.11366",
    "title": "Statistical inference as Green's functions",
    "abstract": "Statistical inference from data is foundational task in science. Recently, it\nreceives growing attention for its central role in inference systems of primary\ninterest in data science, artificial intelligence, or machine learning.\nHowever, the understanding of statistical inference itself is not that solid\nwhile regarded as a matter of subjective choice or implemented in obscure ways.\nWe here show that statistical inference has rigorous scientific description for\nlong sequence of exchangeable binary random variables, the prototypal\nstochasticity in theories and applications. A linear differential equation is\nderived from the exchangeability, and it turns out that statistical inference\nis given by the Green's functions. Our finding is the answer to the normative\nand foundational issue in science, and its significance will be far-reaching in\nall pure and applied fields.",
    "descriptor": "\nComments: 19 pages, 1 figure\n",
    "authors": [
      "Hyun Keun Lee",
      "Chulan Kwon",
      "Yong Woon Kim"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11366"
  },
  {
    "id": "arXiv:2205.11373",
    "title": "User Clustering for Rate Splitting using Machine Learning",
    "abstract": "Hierarchical Rate Splitting (HRS) schemes proposed in recent years have shown\nto provide significant improvements in exploiting spatial diversity in wireless\nnetworks and provide high throughput for all users while minimising\ninterference among them. Hence, one of the major challenges for such HRS\nschemes is the necessity to know the optimal clustering of these users based\nonly on their Channel State Information (CSI). This clustering problem is known\nto be NP hard and, to deal with the unmanageable complexity of finding an\noptimal solution, in this work a scalable and much lighter clustering mechanism\nbased on Neural Network (NN) is proposed. The accuracy and performance metrics\nshow that the NN is able to learn and cluster the users based on the noisy\nchannel response and is able to achieve a rate comparable to other more complex\nclustering schemes from the literature.",
    "descriptor": "",
    "authors": [
      "Roberto Pereira",
      "Anay Ajit Deshpande",
      "Cristian J. Vaca-Rubio",
      "Xavier Mestre",
      "Andrea Zanella",
      "David Gregoratti",
      "Elisabeth de Carvalho",
      "Petar Popovski"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11373"
  },
  {
    "id": "arXiv:2205.11376",
    "title": "Learned Digital Back-Propagation for Dual-Polarization Dispersion  Managed Systems",
    "abstract": "Digital back-propagation (DBP) and learned DBP (LDBP) are proposed for\nnonlinearity mitigation in WDM dual-polarization dispersion-managed systems.\nLDBP achieves Q-factor improvement of 1.8 dB and 1.2 dB, respectively, over\nlinear equalization and a variant of DBP adapted to DM systems.",
    "descriptor": "",
    "authors": [
      "Mohannad Abu-romoh",
      "Nelson Costa",
      "Antonio Napoli",
      "Bernhard Spinnler",
      "Yves Jaou\u00ebn",
      "Mansoor Yousefi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11376"
  },
  {
    "id": "arXiv:2205.11396",
    "title": "Cross-mode Stabilized Stochastic Shallow Water Systems Using Stochastic  Finite Element Methods",
    "abstract": "The development of surrogate models to study uncertainties in hydrologic\nsystems requires significant effort in the development of sampling strategies\nand forward model simulations. Furthermore, in applications where prediction\ntime is critical, such as prediction of hurricane storm surge, the predictions\nof system response and uncertainties can be required within short time frames.\nHere, we develop an efficient stochastic shallow water model to address these\nissues. To discretize the physical and probability spaces we use a Stochastic\nGalerkin method and a Incremental Pressure Correction scheme to advance the\nsolution in time. To overcome discrete stability issues, we propose cross-mode\nstabilization methods which employs existing stabilization methods in the\nprobability space by adding stabilization terms to every stochastic mode in a\nmodes-coupled way. We extensively verify the developed method for both\nidealized shallow water test cases and hindcasting of past hurricanes. We\nsubsequently use the developed and verified method to perform a comprehensive\nstatistical analysis of the established shallow water surrogate models.\nFinally, we propose a predictor for hurricane storm surge under uncertain wind\ndrag coefficients and demonstrate its effectivity for Hurricanes Ike and\nHarvey.",
    "descriptor": "",
    "authors": [
      "Chen Chen",
      "Clint Dawson",
      "Eirik Valseth"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11396"
  },
  {
    "id": "arXiv:2205.11407",
    "title": "Deep-learning-based prediction of nanoparticle phase transitions during  in situ transmission electron microscopy",
    "abstract": "We develop the machine learning capability to predict a time sequence of\nin-situ transmission electron microscopy (TEM) video frames based on the\ncombined long-short-term-memory (LSTM) algorithm and the features\nde-entanglement method. We train deep learning models to predict a sequence of\nfuture video frames based on the input of a sequence of previous frames. This\nunique capability provides insight into size dependent structural changes in Au\nnanoparticles under dynamic reaction condition using in-situ environmental TEM\ndata, informing models of morphological evolution and catalytic properties. The\nmodel performance and achieved accuracy of predictions are desirable based on,\nfor scientific data characteristic, based on limited size of training data\nsets. The model convergence and values for the loss function mean square error\nshow dependence on the training strategy, and structural similarity measure\nbetween predicted structure images and ground truth reaches the value of about\n0.7. This computed structural similarity is smaller than values obtained when\nthe deep learning architecture is trained using much larger benchmark data\nsets, it is sufficient to show the structural transition of Au nanoparticles.\nWhile performance parameters of our model applied to scientific data fall short\nof those achieved for the non-scientific big data sets, we demonstrate model\nability to predict the evolution, even including the particle structural phase\ntransformation, of Au nano particles as catalyst for CO oxidation under the\nchemical reaction conditions. Using this approach, it may be possible to\nanticipate the next steps of a chemical reaction for emerging automated\nexperimentation platforms.",
    "descriptor": "\nComments: 16 pages, 13 figures\n",
    "authors": [
      "Wenkai Fu",
      "Steven R. Spurgeon",
      "Chongmin Wang",
      "Yuyan Shao",
      "Wei Wang",
      "Amra Peles"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11407"
  },
  {
    "id": "arXiv:2205.11428",
    "title": "Spreading Factor and RSSI for Localization in LoRa Networks: A Deep  Reinforcement Learning Approach",
    "abstract": "Recent advancements in Internet of Things (IoT) technologies have resulted in\na tightening of requirements from various applications including localization\nin LoRa networks. To address the growing demand for LoRaWAN-powered IoT\nlocation-based services, accurate localization solutions are more crucial than\never. As such, in this work, we develop an accurate deep neural network based\nlocalization framework over a LoRa network by proposing a novel approach that\nbuilds the network radio map with the combination of RSSI recordings and the\nspreading factors (SF) used by LoRa devices during the transmissions. Then, we\nvalidate our framework using a publicly available experimental dataset recorded\nin an urban LoRa network. The performance evaluation shows the prominence of\nadding the spreading factor as an additional fingerprint, since we can achieve,\nby our approach, an improvement in localization accuracy by up to 6.67%\ncompared to the state-of-the-art methods which employ uniquely the RSSI\nfingerprints. Additionally, we provide an analysis of the impact of the SF on\nthe localization performance which reveals that the localization accuracy\nrelies on the SF used for position request. Finally, we propose a deep\nreinforcement learning based localization system to capture the ever-growing\ncomplexity of LoRa networks environment and cope with the scalability issue in\nLoRa enabled massive IoT, and the results show an improvement of 63.3% in terms\nof accuracy.",
    "descriptor": "",
    "authors": [
      "Yaya Etiabi",
      "Mohammed JOUHARI",
      "El Mehdi Amhoud"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.11428"
  },
  {
    "id": "arXiv:2205.11440",
    "title": "Federated Distillation based Indoor Localization for IoT Networks",
    "abstract": "Federated distillation (FD) paradigm has been recently proposed as a\npromising alternative to federated learning (FL) especially in wireless sensor\nnetworks with limited communication resources. However, all state-of-the art FD\nalgorithms are designed for only classification tasks and less attention has\nbeen given to regression tasks. In this work, we propose an FD framework that\nproperly operates on regression learning problems. Afterwards, we present a\nuse-case implementation by proposing an indoor localization system that shows a\ngood trade-off communication load vs. accuracy compared to federated learning\n(FL) based indoor localization. With our proposed framework, we reduce the\nnumber of transmitted bits by up to 98%. Moreover, we show that the proposed\nframework is much more scalable than FL, thus more likely to cope with the\nexpansion of wireless networks.",
    "descriptor": "",
    "authors": [
      "Yaya Etiabi",
      "Marwa Chafii",
      "El Mehdi Amhoud"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.11440"
  },
  {
    "id": "arXiv:2205.11446",
    "title": "Overfitting in quantum machine learning and entangling dropout",
    "abstract": "The ultimate goal in machine learning is to construct a model function that\nhas a generalization capability for unseen dataset, based on given training\ndataset. If the model function has too much expressibility power, then it may\noverfit to the training data and as a result lose the generalization\ncapability. To avoid such overfitting issue, several techniques have been\ndeveloped in the classical machine learning regime, and the dropout is one such\neffective method. This paper proposes a straightforward analogue of this\ntechnique in the quantum machine learning regime, the entangling dropout,\nmeaning that some entangling gates in a given parametrized quantum circuit are\nrandomly removed during the training process to reduce the expressibility of\nthe circuit. Some simple case studies are given to show that this technique\nactually suppresses the overfitting.",
    "descriptor": "\nComments: 6 pages, 8 figures\n",
    "authors": [
      "Masahiro Kobayashi",
      "Kohei Nakaji",
      "Naoki Yamamoto"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11446"
  },
  {
    "id": "arXiv:2205.11466",
    "title": "Low-Rank Univariate Sum of Squares Has No Spurious Local Minima",
    "abstract": "We study the problem of decomposing a polynomial $p$ into a sum of $r$\nsquares by minimizing a quadratically-penalized objective $f_p(\\mathbf{u}) =\n\\left\\lVert \\sum_{i=1}^r u_i^2 - p\\right\\lVert^2$. This objective is non-convex\nand is equivalent to the rank-$r$ Burer-Monteiro factorization of a\nsemidefinite program (SDP) encoding the sum of squares decomposition. We show\nthat for all univariate polynomials $p$, if $r \\ge 2$ then $f_p(\\mathbf{u})$\nhas no spurious second-order critical points, showing that all local optima are\nalso global optima. This is in contrast to previous work showing that for\ngeneral SDPs, in addition to genericity conditions, $r$ has to be roughly the\nsquare root of the number of constraints (the degree of $p$) for there to be no\nspurious second-order critical points. Our proof uses tools from computational\nalgebraic geometry and can be interpreted as constructing a certificate using\nthe first and second order necessary conditions. We also show that by choosing\na norm based on sampling equally-spaced points on the circle, the gradient\n$\\nabla f_p$ can be computed in nearly linear time using fast Fourier\ntransforms. Experimentally we demonstrate that this method has very fast\nconvergence using first-order optimization algorithms such as L-BFGS, with\nnear-linear scaling to million-degree polynomials.",
    "descriptor": "\nComments: 22 pages, 2 figures\n",
    "authors": [
      "Beno\u00eet Legat",
      "Chenyang Yuan",
      "Pablo A. Parrilo"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Algebraic Geometry (math.AG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.11466"
  },
  {
    "id": "arXiv:2205.11480",
    "title": "Novel Light Field Imaging Device with Enhanced Light Collection for Cold  Atom Clouds",
    "abstract": "We present a light field imaging system that captures multiple views of an\nobject with a single shot. The system is designed to maximize the total light\ncollection by accepting a larger solid angle of light than a conventional lens\nwith equivalent depth of field. This is achieved by populating a plane of\nvirtual objects using mirrors and fully utilizing the available field of view\nand depth of field. Simulation results demonstrate that this design is capable\nof single-shot tomography of objects of size $\\mathcal{O}$(1 mm$^3$),\nreconstructing the 3-dimensional (3D) distribution and features not accessible\nfrom any single view angle in isolation. In particular, for atom clouds used in\natom interferometry experiments, the system can reconstruct 3D fringe patterns\nwith size $\\mathcal{O}$(100 $\\mu$m). We also demonstrate this system with a\n3D-printed prototype. The prototype is used to take images of $\\mathcal{O}$(1\nmm$^{3}$) sized objects, and 3D reconstruction algorithms running on a\nsingle-shot image successfully reconstruct $\\mathcal{O}$(100 $\\mu$m) internal\nfeatures. The prototype also shows that the system can be built with 3D\nprinting technology and hence can be deployed quickly and cost-effectively in\nexperiments with needs for enhanced light collection or 3D reconstruction.\nImaging of cold atom clouds in atom interferometry is a key application of this\nnew type of imaging device where enhanced light collection, high depth of\nfield, and 3D tomographic reconstruction can provide new handles to\ncharacterize the atom clouds.",
    "descriptor": "",
    "authors": [
      "Sanha Cheong",
      "Josef C. Frisch",
      "Sean Gasiorowski",
      "Jason M. Hogan",
      "Michael Kagan",
      "Murtaza Safdari",
      "Ariel Schwartzman",
      "Maxime Vandegar"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Atomic Physics (physics.atom-ph)",
      "Optics (physics.optics)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.11480"
  },
  {
    "id": "arXiv:2205.11486",
    "title": "Robust and Agnostic Learning of Conditional Distributional Treatment  Effects",
    "abstract": "The conditional average treatment effect (CATE) is the best point prediction\nof individual causal effects given individual baseline covariates and can help\npersonalize treatments. However, as CATE only reflects the (conditional)\naverage, it can wash out potential risks and tail events, which are crucially\nrelevant to treatment choice. In aggregate analyses, this is usually addressed\nby measuring distributional treatment effect (DTE), such as differences in\nquantiles or tail expectations between treatment groups. Hypothetically, one\ncan similarly fit covariate-conditional quantile regressions in each treatment\ngroup and take their difference, but this would not be robust to\nmisspecification or provide agnostic best-in-class predictions. We provide a\nnew robust and model-agnostic methodology for learning the conditional DTE\n(CDTE) for a wide class of problems that includes conditional quantile\ntreatment effects, conditional super-quantile treatment effects, and\nconditional treatment effects on coherent risk measures given by\n$f$-divergences. Our method is based on constructing a special pseudo-outcome\nand regressing it on baseline covariates using any given regression learner.\nOur method is model-agnostic in the sense that it can provide the best\nprojection of CDTE onto the regression model class. Our method is robust in the\nsense that even if we learn these nuisances nonparametrically at very slow\nrates, we can still learn CDTEs at rates that depend on the class complexity\nand even conduct inferences on linear projections of CDTEs. We investigate the\nperformance of our proposal in simulation studies, and we demonstrate its use\nin a case study of 401(k) eligibility effects on wealth.",
    "descriptor": "",
    "authors": [
      "Nathan Kallus",
      "Miruna Oprescu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.11486"
  },
  {
    "id": "arXiv:2205.11488",
    "title": "Entanglements",
    "abstract": "Robertson and Seymour constructed for every graph $G$ a tree-decomposition\nthat efficiently distinguishes all the tangles in $G$. While all previous\nconstructions of these decompositions are iterative in nature, we give an\nexplicit one-step construction. The key ingredient is an axiomatisation of\n'local properties' of tangles. Generalisations to locally finite graphs and\nmatroids are also discussed.",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Johannes Carmesin",
      "Jan Kurkofka"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.11488"
  },
  {
    "id": "arXiv:1701.05530",
    "title": "Regression of exchangeable relational arrays",
    "abstract": "Comments: To appear in Biometrika",
    "descriptor": "\nComments: To appear in Biometrika\n",
    "authors": [
      "Frank W. Marrs",
      "Bailey K. Fosdick",
      "Tyler H. McCormick"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/1701.05530"
  },
  {
    "id": "arXiv:1708.05853",
    "title": "Convergence of HX Preconditioner for Maxwell's Equations with Jump  Coefficients (ii): The Main Results",
    "abstract": "Comments: with 25 pages, 2 figures",
    "descriptor": "\nComments: with 25 pages, 2 figures\n",
    "authors": [
      "Qiya Hu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1708.05853"
  },
  {
    "id": "arXiv:1807.01874",
    "title": "Road surface 3d reconstruction based on dense subpixel disparity map  estimation",
    "abstract": "Comments: 11 pages, 16 figures, IEEE Transactions on Image Processing",
    "descriptor": "\nComments: 11 pages, 16 figures, IEEE Transactions on Image Processing\n",
    "authors": [
      "Rui Fan",
      "Xiao Ai",
      "Naim Dahnoun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1807.01874"
  },
  {
    "id": "arXiv:1902.06199",
    "title": "Context-Based Dynamic Pricing with Online Clustering",
    "abstract": "Context-Based Dynamic Pricing with Online Clustering",
    "descriptor": "",
    "authors": [
      "Sentao Miao",
      "Xi Chen",
      "Xiuli Chao",
      "Jiaxi Liu",
      "Yidong Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1902.06199"
  },
  {
    "id": "arXiv:1905.00640",
    "title": "Tight Approximation Bounds for Maximum Multi-Coverage",
    "abstract": "Comments: 27 pages",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Siddharth Barman",
      "Omar Fawzi",
      "Suprovat Ghoshal",
      "Emirhan G\u00fcrp\u0131nar"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1905.00640"
  },
  {
    "id": "arXiv:1905.04325",
    "title": "Seeding with Costly Network Information",
    "abstract": "Seeding with Costly Network Information",
    "descriptor": "",
    "authors": [
      "Dean Eckles",
      "Hossein Esfandiari",
      "Elchanan Mossel",
      "M. Amin Rahimian"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computational Complexity (cs.CC)",
      "Probability (math.PR)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/1905.04325"
  },
  {
    "id": "arXiv:1908.00894",
    "title": "Pothole Detection Based on Disparity Transformation and Road Surface  Modeling",
    "abstract": "Comments: 12 pages, 15 figures, IEEE Transactions on Image Processing",
    "descriptor": "\nComments: 12 pages, 15 figures, IEEE Transactions on Image Processing\n",
    "authors": [
      "Rui Fan",
      "Umar Ozgunalp",
      "Brett Hosking",
      "Ming Liu",
      "Ioannis Pitas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/1908.00894"
  },
  {
    "id": "arXiv:1910.07779",
    "title": "Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic  Bayesian Optimisation",
    "abstract": "Comments: Published in Machine Learning: Science and Technology 2021 (this https URL) Earlier version accepted to the 2019 NeurIPS Workshop on Safety and Robustness in Decision Making",
    "descriptor": "\nComments: Published in Machine Learning: Science and Technology 2021 (this https URL) Earlier version accepted to the 2019 NeurIPS Workshop on Safety and Robustness in Decision Making\n",
    "authors": [
      "Ryan-Rhys Griffiths",
      "Alexander A. Aldrick",
      "Miguel Garcia-Ortegon",
      "Vidhi R. Lalchand",
      "Alpha A. Lee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1910.07779"
  },
  {
    "id": "arXiv:2004.03029",
    "title": "A BDF2-Semismooth Newton Algorithm for the Numerical Solution of the  Bingham Flow with Temperature Dependent Parameters",
    "abstract": "A BDF2-Semismooth Newton Algorithm for the Numerical Solution of the  Bingham Flow with Temperature Dependent Parameters",
    "descriptor": "",
    "authors": [
      "Sergio Gonz\u00e1lez-Andrade"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2004.03029"
  },
  {
    "id": "arXiv:2004.04371",
    "title": "MDCNN-SID: Multi-scale Dilated Convolution Network for Singer  Identification",
    "abstract": "Comments: Accepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)",
    "descriptor": "\nComments: Accepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2004.04371"
  },
  {
    "id": "arXiv:2005.03468",
    "title": "Indexing Metric Spaces for Exact Similarity Search",
    "abstract": "Indexing Metric Spaces for Exact Similarity Search",
    "descriptor": "",
    "authors": [
      "Lu Chen",
      "Yunjun Gao",
      "Xuan Song",
      "Zheng Li",
      "Yifan Zhu",
      "Xiaoye Miao",
      "Christian S. Jensen"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2005.03468"
  },
  {
    "id": "arXiv:2005.12582",
    "title": "Differentials and distances in probabilistic coherence spaces",
    "abstract": "Comments: extended version of arXiv:1902.04836 . Improved redaction of the proof of the main result of Section 2 (expectation of computation time)",
    "descriptor": "\nComments: extended version of arXiv:1902.04836 . Improved redaction of the proof of the main result of Section 2 (expectation of computation time)\n",
    "authors": [
      "Thomas Ehrhard"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2005.12582"
  },
  {
    "id": "arXiv:2006.03568",
    "title": "Graph Layer Security: Encrypting Information via Common Networked  Physics",
    "abstract": "Graph Layer Security: Encrypting Information via Common Networked  Physics",
    "descriptor": "",
    "authors": [
      "Zhuangkun Wei",
      "Liang Wang",
      "Schyler Chengyao Sun",
      "Bin Li",
      "Weisi Guo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2006.03568"
  },
  {
    "id": "arXiv:2007.14245",
    "title": "Bayesian Multi Scale Neural Network for Crowd Counting",
    "abstract": "Comments: This work makes assumptions which were found wrong later by the author",
    "descriptor": "\nComments: This work makes assumptions which were found wrong later by the author\n",
    "authors": [
      "Abhinav Sagar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.14245"
  },
  {
    "id": "arXiv:2008.04891",
    "title": "Semantic Clone Detection via Probabilistic Software Modeling",
    "abstract": "Comments: 22 pages, 3 pages of references, 4 listings, 2 figures, 3 tables",
    "descriptor": "\nComments: 22 pages, 3 pages of references, 4 listings, 2 figures, 3 tables\n",
    "authors": [
      "Hannes Thaller",
      "Lukas Linsbauer",
      "Alexander Egyed"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2008.04891"
  },
  {
    "id": "arXiv:2008.05263",
    "title": "An Ansatz for computational undecidability in RNA automata",
    "abstract": "An Ansatz for computational undecidability in RNA automata",
    "descriptor": "",
    "authors": [
      "Adam J. Svahn",
      "Mikhail Prokopenko"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2008.05263"
  },
  {
    "id": "arXiv:2008.11247",
    "title": "Photonic reservoir computer based on frequency multiplexing",
    "abstract": "Comments: 19 pages. Published version, including supplementary material",
    "descriptor": "\nComments: 19 pages. Published version, including supplementary material\n",
    "authors": [
      "Lorenz Butschek",
      "Akram Akrout",
      "Evangelia Dimitriadou",
      "Alessandro Lupo",
      "Marc Haelterman",
      "Serge Massar"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2008.11247"
  },
  {
    "id": "arXiv:2008.12226",
    "title": "The complexity of L(p,q)-Edge-Labelling",
    "abstract": "The complexity of L(p,q)-Edge-Labelling",
    "descriptor": "",
    "authors": [
      "Gaetan Berthe",
      "Barnaby Martin",
      "Daniel Paulusma",
      "Siani Smith"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2008.12226"
  },
  {
    "id": "arXiv:2008.13763",
    "title": "Anomaly Detection by Recombining Gated Unsupervised Experts",
    "abstract": "Comments: Accepted at IJCNN 2022",
    "descriptor": "\nComments: Accepted at IJCNN 2022\n",
    "authors": [
      "J.-P. Schulze",
      "P. Sperl",
      "K. B\u00f6ttinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2008.13763"
  },
  {
    "id": "arXiv:2009.02955",
    "title": "A Perturbation-Based Kernel Approximation Framework",
    "abstract": "Comments: 25 pages, 6 figures",
    "descriptor": "\nComments: 25 pages, 6 figures\n",
    "authors": [
      "Roy Mitz",
      "Yoel Shkolnisky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.02955"
  },
  {
    "id": "arXiv:2009.09786",
    "title": "Cloud-gaming:Analysis of Google Stadia traffic",
    "abstract": "Cloud-gaming:Analysis of Google Stadia traffic",
    "descriptor": "",
    "authors": [
      "Marc Carrascosa",
      "Boris Bellalta"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2009.09786"
  },
  {
    "id": "arXiv:2010.08311",
    "title": "Formal Verification of Robustness and Resilience of Learning-Enabled  State Estimation Systems for Robotics",
    "abstract": "Formal Verification of Robustness and Resilience of Learning-Enabled  State Estimation Systems for Robotics",
    "descriptor": "",
    "authors": [
      "Wei Huang",
      "Yifan Zhou",
      "Gaojie Jin",
      "Youcheng Sun",
      "Alec Banks",
      "Jie Meng",
      "James Sharp",
      "Simon Maskell",
      "Xiaowei Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2010.08311"
  },
  {
    "id": "arXiv:2010.09629",
    "title": "PAC$^m$-Bayes: Narrowing the Empirical Risk Gap in the Misspecified  Bayesian Regime",
    "abstract": "Comments: Accepted at AISTATS2022",
    "descriptor": "\nComments: Accepted at AISTATS2022\n",
    "authors": [
      "Warren R. Morningstar",
      "Alexander A. Alemi",
      "Joshua V. Dillon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.09629"
  },
  {
    "id": "arXiv:2010.09800",
    "title": "A Contour Stochastic Gradient Langevin Dynamics Algorithm for  Simulations of Multi-modal Distributions",
    "abstract": "Comments: Accepted by NeurIPS 2020",
    "descriptor": "\nComments: Accepted by NeurIPS 2020\n",
    "authors": [
      "Wei Deng",
      "Guang Lin",
      "Faming Liang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2010.09800"
  },
  {
    "id": "arXiv:2010.10296",
    "title": "Definitional Quantifiers Realise Semantic Reasoning for Proof by  Induction",
    "abstract": "Comments: This is the preprint of our paper accepted at Tests and Proofs 2022. arXiv admin note: substantial text overlap with arXiv:2009.09215",
    "descriptor": "\nComments: This is the preprint of our paper accepted at Tests and Proofs 2022. arXiv admin note: substantial text overlap with arXiv:2009.09215\n",
    "authors": [
      "Yutaka Nagashima"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2010.10296"
  },
  {
    "id": "arXiv:2010.13934",
    "title": "Accelerate the Warm-up Stage in the Lasso Computation via a Homotopic  Approach",
    "abstract": "Comments: 19 pages, 3 figures, 3 tables",
    "descriptor": "\nComments: 19 pages, 3 figures, 3 tables\n",
    "authors": [
      "Yujie Zhao",
      "Xiaoming Huo"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2010.13934"
  },
  {
    "id": "arXiv:2011.08552",
    "title": "Agafonov's Theorem for finite and infinite alphabets and probability  distributions different from equidistribution",
    "abstract": "Agafonov's Theorem for finite and infinite alphabets and probability  distributions different from equidistribution",
    "descriptor": "",
    "authors": [
      "Thomas Seiller",
      "Jakob Grue Simonsen"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2011.08552"
  },
  {
    "id": "arXiv:2012.01821",
    "title": "D-Unet: A Dual-encoder U-Net for Image Splicing Forgery Detection and  Localization",
    "abstract": "Comments: 13 pages, 13 figures",
    "descriptor": "\nComments: 13 pages, 13 figures\n",
    "authors": [
      "Bo Liu",
      "Ranglei Wu",
      "Xiuli Bi",
      "Bin Xiao",
      "Weisheng Li",
      "Guoyin Wang",
      "Xinbo Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.01821"
  },
  {
    "id": "arXiv:2012.08626",
    "title": "Computation Against a Neighbour: Addressing Large-Scale Distribution and  Adaptivity with Functional Programming and Scala",
    "abstract": "Comments: 58 pages, 17 figures, 1 table",
    "descriptor": "\nComments: 58 pages, 17 figures, 1 table\n",
    "authors": [
      "Giorgio Audrito",
      "Roberto Casadei",
      "Ferruccio Damiani",
      "Mirko Viroli"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2012.08626"
  },
  {
    "id": "arXiv:2012.08735",
    "title": "Reconstructing decision trees",
    "abstract": "Comments: To appear in ICALP 2022",
    "descriptor": "\nComments: To appear in ICALP 2022\n",
    "authors": [
      "Guy Blanc",
      "Jane Lange",
      "Li-Yang Tan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.08735"
  },
  {
    "id": "arXiv:2012.11070",
    "title": "Energy Efficient Federated Learning over Heterogeneous Mobile Devices  via Joint Design of Weight Quantization and Wireless Transmission",
    "abstract": "Comments: 14 pages, 8 figures",
    "descriptor": "\nComments: 14 pages, 8 figures\n",
    "authors": [
      "Rui Chen",
      "Liang Li",
      "Kaiping Xue",
      "Chi Zhang",
      "Miao Pan",
      "Yuguang Fang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.11070"
  },
  {
    "id": "arXiv:2012.13453",
    "title": "Quantum Circuit Evolution on NISQ Devices",
    "abstract": "Comments: 8 pages, 7 figures. To appear in the proceedings of IEEE Congress on Evolutionary Computation (CEC) 2022",
    "descriptor": "\nComments: 8 pages, 7 figures. To appear in the proceedings of IEEE Congress on Evolutionary Computation (CEC) 2022\n",
    "authors": [
      "Lukas Franken",
      "Bogdan Georgiev",
      "Sascha M\u00fccke",
      "Moritz Wolter",
      "Raoul Heese",
      "Christian Bauckhage",
      "Nico Piatkowski"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.13453"
  },
  {
    "id": "arXiv:2012.13619",
    "title": "On self-supervised multi-modal representation learning: An application  to Alzheimer's disease",
    "abstract": "On self-supervised multi-modal representation learning: An application  to Alzheimer's disease",
    "descriptor": "",
    "authors": [
      "Alex Fedorov",
      "Lei Wu",
      "Tristan Sylvain",
      "Margaux Luck",
      "Thomas P. DeRamus",
      "Dmitry Bleklov",
      "Sergey M. Plis",
      "Vince D. Calhoun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.13619"
  },
  {
    "id": "arXiv:2101.03778",
    "title": "Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain  Detection",
    "abstract": "Comments: AAAI 2021",
    "descriptor": "\nComments: AAAI 2021\n",
    "authors": [
      "Alexander Podolskiy",
      "Dmitry Lipin",
      "Andrey Bout",
      "Ekaterina Artemova",
      "Irina Piontkovskaya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.03778"
  },
  {
    "id": "arXiv:2101.04645",
    "title": "Double-Adversarial Activation Anomaly Detection: Adversarial  Autoencoders are Anomaly Generators",
    "abstract": "Comments: Accepted at IJCNN 2022",
    "descriptor": "\nComments: Accepted at IJCNN 2022\n",
    "authors": [
      "J.-P. Schulze",
      "P. Sperl",
      "K. B\u00f6ttinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2101.04645"
  },
  {
    "id": "arXiv:2101.05975",
    "title": "Multi-layer Feature Fusion Convolution Network for Audio-visual Speech  Enhancement",
    "abstract": "Multi-layer Feature Fusion Convolution Network for Audio-visual Speech  Enhancement",
    "descriptor": "",
    "authors": [
      "Xinmeng Xu",
      "Jianjun Hao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2101.05975"
  },
  {
    "id": "arXiv:2101.09038",
    "title": "A Decentralized Analysis of Multiparty Protocols",
    "abstract": "Comments: extended proofs following anonymous reviews",
    "descriptor": "\nComments: extended proofs following anonymous reviews\n",
    "authors": [
      "Bas van den Heuvel",
      "Jorge A. P\u00e9rez"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2101.09038"
  },
  {
    "id": "arXiv:2102.03607",
    "title": "Bootstrapping Fitted Q-Evaluation for Off-Policy Inference",
    "abstract": "Comments: Accepted at ICML 2021",
    "descriptor": "\nComments: Accepted at ICML 2021\n",
    "authors": [
      "Botao Hao",
      "Xiang Ji",
      "Yaqi Duan",
      "Hao Lu",
      "Csaba Szepesv\u00e1ri",
      "Mengdi Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2102.03607"
  },
  {
    "id": "arXiv:2102.03758",
    "title": "Non-stationary Online Learning with Memory and Non-stochastic Control",
    "abstract": "Non-stationary Online Learning with Memory and Non-stochastic Control",
    "descriptor": "",
    "authors": [
      "Peng Zhao",
      "Yu-Hu Yan",
      "Yu-Xiang Wang",
      "Zhi-Hua Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.03758"
  },
  {
    "id": "arXiv:2102.08501",
    "title": "DEUP: Direct Epistemic Uncertainty Prediction",
    "abstract": "DEUP: Direct Epistemic Uncertainty Prediction",
    "descriptor": "",
    "authors": [
      "Salem Lahlou",
      "Moksh Jain",
      "Hadi Nekoei",
      "Victor Butoi",
      "Paul Bertin",
      "Jarrid Rector-Brooks",
      "Maksym Korablyov",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.08501"
  },
  {
    "id": "arXiv:2102.10287",
    "title": "Deep Structured Feature Networks for Table Detection and Tabular Data  Extraction from Scanned Financial Document Images",
    "abstract": "Comments: Works need further review",
    "descriptor": "\nComments: Works need further review\n",
    "authors": [
      "Siwen Luo",
      "Mengting Wu",
      "Yiwen Gong",
      "Wanying Zhou",
      "Josiah Poon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.10287"
  },
  {
    "id": "arXiv:2103.03223",
    "title": "A Comparative Evaluation of Quantification Methods",
    "abstract": "Comments: 30 pages, 12 figures",
    "descriptor": "\nComments: 30 pages, 12 figures\n",
    "authors": [
      "Tobias Schumacher",
      "Markus Strohmaier",
      "Florian Lemmerich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2103.03223"
  },
  {
    "id": "arXiv:2103.07863",
    "title": "Imperative process algebra with abstraction",
    "abstract": "Comments: 33 pages, a polished revision of v4",
    "descriptor": "\nComments: 33 pages, a polished revision of v4\n",
    "authors": [
      "C.A. Middelburg"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2103.07863"
  },
  {
    "id": "arXiv:2103.10060",
    "title": "Approximation for Probability Distributions by Wasserstein GAN",
    "abstract": "Approximation for Probability Distributions by Wasserstein GAN",
    "descriptor": "",
    "authors": [
      "Yihang Gao",
      "Michael K. Ng",
      "Mingjie Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.10060"
  },
  {
    "id": "arXiv:2103.10807",
    "title": "Linear Coding for AWGN channels with Noisy Output Feedback via Dynamic  Programming",
    "abstract": "Comments: 27 pages, 10 figures",
    "descriptor": "\nComments: 27 pages, 10 figures\n",
    "authors": [
      "Rajesh Mishra",
      "Deepanshu Vasal",
      "Hyeji Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.10807"
  },
  {
    "id": "arXiv:2103.11553",
    "title": "Two Metrics on Rooted Unordered Trees with Labels",
    "abstract": "Two Metrics on Rooted Unordered Trees with Labels",
    "descriptor": "",
    "authors": [
      "Yue Wang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2103.11553"
  },
  {
    "id": "arXiv:2103.14620",
    "title": "LiGCN: Label-interpretable Graph Convolutional Networks for Multi-label  Text Classification",
    "abstract": "Comments: 8 tables, 3 figures",
    "descriptor": "\nComments: 8 tables, 3 figures\n",
    "authors": [
      "Irene Li",
      "Aosong Feng",
      "Hao Wu",
      "Tianxiao Li",
      "Toyotaro Suzumura",
      "Ruihai Dong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2103.14620"
  },
  {
    "id": "arXiv:2103.15914",
    "title": "Tasting the cake: evaluating self-supervised generalization on  out-of-distribution multimodal MRI data",
    "abstract": "Comments: Presented as a RobustML workshop paper at ICLR 2021",
    "descriptor": "\nComments: Presented as a RobustML workshop paper at ICLR 2021\n",
    "authors": [
      "Alex Fedorov",
      "Eloy Geenjaar",
      "Lei Wu",
      "Thomas P. DeRamus",
      "Vince D. Calhoun",
      "Sergey M. Plis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.15914"
  },
  {
    "id": "arXiv:2103.16896",
    "title": "Lowest order stabilization free Virtual Element Method for the Poisson  equation",
    "abstract": "Comments: 39 pages, 8 figures",
    "descriptor": "\nComments: 39 pages, 8 figures\n",
    "authors": [
      "Stefano Berrone",
      "Andrea Borio",
      "Francesca Marcon"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.16896"
  },
  {
    "id": "arXiv:2104.01231",
    "title": "Diverse Gaussian Noise Consistency Regularization for Robustness and  Uncertainty Calibration",
    "abstract": "Comments: Under review. Preliminary version accepted to ICML 2021 Uncertainty & Robustness in Deep Learning Workshop",
    "descriptor": "\nComments: Under review. Preliminary version accepted to ICML 2021 Uncertainty & Robustness in Deep Learning Workshop\n",
    "authors": [
      "Theodoros Tsiligkaridis",
      "Athanasios Tsiligkaridis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.01231"
  },
  {
    "id": "arXiv:2104.02230",
    "title": "Achieving Domain Generalization in Underwater Object Detection by Domain  Mixup and Contrastive Learning",
    "abstract": "Achieving Domain Generalization in Underwater Object Detection by Domain  Mixup and Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Pinhao Song",
      "Hong Liu",
      "Linhui Dai",
      "Peipei Yuan",
      "Runwei Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.02230"
  },
  {
    "id": "arXiv:2104.03414",
    "title": "PrivateSNN: Privacy-Preserving Spiking Neural Networks",
    "abstract": "Comments: Accepted to AAAI2022",
    "descriptor": "\nComments: Accepted to AAAI2022\n",
    "authors": [
      "Youngeun Kim",
      "Yeshwanth Venkatesha",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.03414"
  },
  {
    "id": "arXiv:2104.05858",
    "title": "Exploring Geometric Consistency for Monocular 3D Object Detection",
    "abstract": "Exploring Geometric Consistency for Monocular 3D Object Detection",
    "descriptor": "",
    "authors": [
      "Qing Lian",
      "Botao Ye",
      "Ruijia Xu",
      "Weilong Yao",
      "Tong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.05858"
  },
  {
    "id": "arXiv:2104.08811",
    "title": "Human Schema Curation via Causal Association Rule Mining",
    "abstract": "Comments: 12 pages, 6 figures, 6 tables",
    "descriptor": "\nComments: 12 pages, 6 figures, 6 tables\n",
    "authors": [
      "Noah Weber",
      "Anton Belyy",
      "Nils Holzenberger",
      "Rachel Rudinger",
      "Benjamin Van Durme"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.08811"
  },
  {
    "id": "arXiv:2104.09202",
    "title": "Monitoring Data Requests in Decentralized Data Storage Systems: A Case  Study of IPFS",
    "abstract": "Comments: Accepted at ICDCS 2022",
    "descriptor": "\nComments: Accepted at ICDCS 2022\n",
    "authors": [
      "Leonhard Balduf",
      "Sebastian Henningsen",
      "Martin Florian",
      "Sebastian Rust",
      "Bj\u00f6rn Scheuermann"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2104.09202"
  },
  {
    "id": "arXiv:2104.13122",
    "title": "Why Does Propositional Quantification Make Logics on Trees Robustly  Hard?",
    "abstract": "Comments: Accepted to LMCS. Full version of our LICS 2019 paper",
    "descriptor": "\nComments: Accepted to LMCS. Full version of our LICS 2019 paper\n",
    "authors": [
      "Bartosz Bednarczyk",
      "St\u00e9phane Demri"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.13122"
  },
  {
    "id": "arXiv:2105.03538",
    "title": "Equivalent formulations of the oxygen depletion problem, other implicit  free boundary value problems, and implications for numerical approximation",
    "abstract": "Comments: 30 pages, 4 figures",
    "descriptor": "\nComments: 30 pages, 4 figures\n",
    "authors": [
      "Xinyu Cheng",
      "Zhaohui Fu",
      "Brian Wetton"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.03538"
  },
  {
    "id": "arXiv:2105.07608",
    "title": "Hamiltonian Cycle Problem is in P",
    "abstract": "Comments: 40 pages, 9 figures",
    "descriptor": "\nComments: 40 pages, 9 figures\n",
    "authors": [
      "Aimin Hou"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2105.07608"
  },
  {
    "id": "arXiv:2105.07636",
    "title": "DOC3-Deep One Class Classification using Contradictions",
    "abstract": "Comments: Deep Learning, Anomaly Detection, Visual Inspection, Learning from Contradictions, Disjoint Auxiliary, Outlier Exposure, MVTec-AD",
    "descriptor": "\nComments: Deep Learning, Anomaly Detection, Visual Inspection, Learning from Contradictions, Disjoint Auxiliary, Outlier Exposure, MVTec-AD\n",
    "authors": [
      "Sauptik Dhar",
      "Bernardo Gonzalez Torres"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.07636"
  },
  {
    "id": "arXiv:2105.07654",
    "title": "Dependency Parsing as MRC-based Span-Span Prediction",
    "abstract": "Comments: Accepted by ACL 2022 Main Conference",
    "descriptor": "\nComments: Accepted by ACL 2022 Main Conference\n",
    "authors": [
      "Leilei Gan",
      "Yuxian Meng",
      "Kun Kuang",
      "Xiaofei Sun",
      "Chun Fan",
      "Fei Wu",
      "Jiwei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.07654"
  },
  {
    "id": "arXiv:2105.07879",
    "title": "Conscious AI",
    "abstract": "Conscious AI",
    "descriptor": "",
    "authors": [
      "Hadi Esmaeilzadeh",
      "Reza Vaezi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2105.07879"
  },
  {
    "id": "arXiv:2105.07926",
    "title": "Towards Robust Vision Transformer",
    "abstract": "Comments: Accepted to CVPR 2022, this https URL",
    "descriptor": "\nComments: Accepted to CVPR 2022, this https URL\n",
    "authors": [
      "Xiaofeng Mao",
      "Gege Qi",
      "Yuefeng Chen",
      "Xiaodan Li",
      "Ranjie Duan",
      "Shaokai Ye",
      "Yuan He",
      "Hui Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.07926"
  },
  {
    "id": "arXiv:2105.10488",
    "title": "Understanding the Performance of Knowledge Graph Embeddings in Drug  Discovery",
    "abstract": "Understanding the Performance of Knowledge Graph Embeddings in Drug  Discovery",
    "descriptor": "",
    "authors": [
      "Stephen Bonner",
      "Ian P Barrett",
      "Cheng Ye",
      "Rowan Swiers",
      "Ola Engkvist",
      "Charles Tapley Hoyt",
      "William L Hamilton"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.10488"
  },
  {
    "id": "arXiv:2105.11367",
    "title": "FedScale: Benchmarking Model and System Performance of Federated  Learning at Scale",
    "abstract": "FedScale: Benchmarking Model and System Performance of Federated  Learning at Scale",
    "descriptor": "",
    "authors": [
      "Fan Lai",
      "Yinwei Dai",
      "Sanjay S. Singapuram",
      "Jiachen Liu",
      "Xiangfeng Zhu",
      "Harsha V. Madhyastha",
      "Mosharaf Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2105.11367"
  },
  {
    "id": "arXiv:2105.11686",
    "title": "Towards Understanding the Condensation of Neural Networks at Initial  Training",
    "abstract": "Towards Understanding the Condensation of Neural Networks at Initial  Training",
    "descriptor": "",
    "authors": [
      "Hanxu Zhou",
      "Qixuan Zhou",
      "Tao Luo",
      "Yaoyu Zhang",
      "Zhi-Qin John Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.11686"
  },
  {
    "id": "arXiv:2105.13120",
    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
    "abstract": "Sequence Parallelism: Long Sequence Training from System Perspective",
    "descriptor": "",
    "authors": [
      "Shenggui Li",
      "Fuzhao Xue",
      "Chaitanya Baranwal",
      "Yongbin Li",
      "Yang You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2105.13120"
  },
  {
    "id": "arXiv:2105.14636",
    "title": "LEAP: Learnable Pruning for Transformer-based Models",
    "abstract": "Comments: 20 pages, 4 figures, 9 tables",
    "descriptor": "\nComments: 20 pages, 4 figures, 9 tables\n",
    "authors": [
      "Zhewei Yao",
      "Xiaoxia Wu",
      "Linjian Ma",
      "Sheng Shen",
      "Kurt Keutzer",
      "Michael W. Mahoney",
      "Yuxiong He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.14636"
  },
  {
    "id": "arXiv:2106.00761",
    "title": "Motif Prediction with Graph Neural Networks",
    "abstract": "Motif Prediction with Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Maciej Besta",
      "Raphael Grob",
      "Cesare Miglioli",
      "Nicola Bernold",
      "Grzegorz Kwasniewski",
      "Gabriel Gjini",
      "Raghavendra Kanakagiri",
      "Saleh Ashkboos",
      "Lukas Gianinazzi",
      "Nikoli Dryden",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.00761"
  },
  {
    "id": "arXiv:2106.02558",
    "title": "Bayesian Risk Markov Decision Processes",
    "abstract": "Bayesian Risk Markov Decision Processes",
    "descriptor": "",
    "authors": [
      "Yifan Lin",
      "Yuxuan Ren",
      "Enlu Zhou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02558"
  },
  {
    "id": "arXiv:2106.02969",
    "title": "FedNL: Making Newton-Type Methods Applicable to Federated Learning",
    "abstract": "Comments: 65 pages, 7 algorithms, 14 figures --- Accepted to ICML 2022",
    "descriptor": "\nComments: 65 pages, 7 algorithms, 14 figures --- Accepted to ICML 2022\n",
    "authors": [
      "Mher Safaryan",
      "Rustem Islamov",
      "Xun Qian",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.02969"
  },
  {
    "id": "arXiv:2106.04178",
    "title": "White Paper Assistance: A Step Forward Beyond the Shortcut Learning",
    "abstract": "Comments: 10 pages, 4 figures",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Xuan Cheng",
      "Tianshu Xie",
      "Xiaomin Wang",
      "Jiali Deng",
      "Minghui Liu",
      "Ming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.04178"
  },
  {
    "id": "arXiv:2106.06580",
    "title": "Revealing the canalizing structure of Boolean functions: Algorithms and  applications",
    "abstract": "Comments: 17 pages, 2 figures",
    "descriptor": "\nComments: 17 pages, 2 figures\n",
    "authors": [
      "Elena Dimitrova",
      "Brandilyn Stigler",
      "Claus Kadelka",
      "David Murrugarra"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.06580"
  },
  {
    "id": "arXiv:2106.07202",
    "title": "Optimal transport in multilayer networks for traffic flow optimization",
    "abstract": "Comments: 11 pages, 6 figures",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Abdullahi Adinoyi Ibrahim",
      "Alessandro Lonardi",
      "Caterina De Bacco"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.07202"
  },
  {
    "id": "arXiv:2106.12270",
    "title": "Weighted Random Sampling on GPUs",
    "abstract": "Weighted Random Sampling on GPUs",
    "descriptor": "",
    "authors": [
      "Hans-Peter Lehmann",
      "Lorenz H\u00fcbschle-Schneider",
      "Peter Sanders"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.12270"
  },
  {
    "id": "arXiv:2106.15611",
    "title": "The penumbra of open source: projects outside of centralized platforms  are longer maintained, more academic and more collaborative",
    "abstract": "Comments: 20 pages, 7 figures, 3 tables",
    "descriptor": "\nComments: 20 pages, 7 figures, 3 tables\n",
    "authors": [
      "Milo Z. Trujillo",
      "Laurent H\u00e9bert-Dufresne",
      "James Bagrow"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2106.15611"
  },
  {
    "id": "arXiv:2107.00637",
    "title": "Generalization and Robustness Implications in Object-Centric Learning",
    "abstract": "Comments: Published at ICML 2022",
    "descriptor": "\nComments: Published at ICML 2022\n",
    "authors": [
      "Andrea Dittadi",
      "Samuele Papa",
      "Michele De Vita",
      "Bernhard Sch\u00f6lkopf",
      "Ole Winther",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.00637"
  },
  {
    "id": "arXiv:2107.03153",
    "title": "Reshaping Convex Polyhedra",
    "abstract": "Comments: Research monograph. 236 pages, 107 figures, 60 references. arXiv admin note: text overlap with arXiv:2008.01759. v2 adds a new section 9.3 P Continuously Folding onto Q",
    "descriptor": "\nComments: Research monograph. 236 pages, 107 figures, 60 references. arXiv admin note: text overlap with arXiv:2008.01759. v2 adds a new section 9.3 P Continuously Folding onto Q\n",
    "authors": [
      "Joseph O'Rourke",
      "Costin Vilcu"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2107.03153"
  },
  {
    "id": "arXiv:2107.05011",
    "title": "Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient  Descent",
    "abstract": "Comments: Published in the IEEE Transactions on Signal Processing (15 pages, 11 figures, and 6 tables)",
    "descriptor": "\nComments: Published in the IEEE Transactions on Signal Processing (15 pages, 11 figures, and 6 tables)\n",
    "authors": [
      "Qiyou Duan",
      "Hadi Ghauch",
      "Taejoon Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2107.05011"
  },
  {
    "id": "arXiv:2107.13431",
    "title": "AI assisted method for efficiently generating breast ultrasound  screening reports",
    "abstract": "AI assisted method for efficiently generating breast ultrasound  screening reports",
    "descriptor": "",
    "authors": [
      "Shuang Ge",
      "Qiongyu Ye",
      "Wenquan Xie",
      "Desheng Sun",
      "Huabin Zhang",
      "Xiaobo Zhou",
      "Kehong Yuan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2107.13431"
  },
  {
    "id": "arXiv:2107.14396",
    "title": "Combined Radar and Communications with Phase-Modulated Frequency  Permutations",
    "abstract": "Combined Radar and Communications with Phase-Modulated Frequency  Permutations",
    "descriptor": "",
    "authors": [
      "Tian Han",
      "Rajitha Senanayake",
      "Peter Smith",
      "Jamie Evans",
      "William Moran",
      "Robin Evans"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2107.14396"
  },
  {
    "id": "arXiv:2108.00103",
    "title": "Anomaly Detection with Neural Parsers That Never Reject",
    "abstract": "Comments: 10 pages, 3 figures",
    "descriptor": "\nComments: 10 pages, 3 figures\n",
    "authors": [
      "Alexander Grushin",
      "Walt Woods"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2108.00103"
  },
  {
    "id": "arXiv:2108.00406",
    "title": "Discovering Distinctive \"Semantics\" in Super-Resolution Networks",
    "abstract": "Comments: discovering and interpreting deep degradation representations (DDR) in super-resolution networks",
    "descriptor": "\nComments: discovering and interpreting deep degradation representations (DDR) in super-resolution networks\n",
    "authors": [
      "Yihao Liu",
      "Anran Liu",
      "Jinjin Gu",
      "Zhipeng Zhang",
      "Wenhao Wu",
      "Yu Qiao",
      "Chao Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.00406"
  },
  {
    "id": "arXiv:2108.00478",
    "title": "CERL: A Unified Optimization Framework for Light Enhancement with  Realistic Noise",
    "abstract": "CERL: A Unified Optimization Framework for Light Enhancement with  Realistic Noise",
    "descriptor": "",
    "authors": [
      "Zeyuan Chen",
      "Yifan Jiang",
      "Dong Liu",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2108.00478"
  },
  {
    "id": "arXiv:2108.02234",
    "title": "Multi-Branch with Attention Network for Hand-Based Person Recognition",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2101.05260",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2101.05260\n",
    "authors": [
      "Nathanael L. Baisa",
      "Bryan Williams",
      "Hossein Rahmani",
      "Plamen Angelov",
      "Sue Black"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.02234"
  },
  {
    "id": "arXiv:2108.02642",
    "title": "Robust interior penalty discontinuous Galerkin methods",
    "abstract": "Robust interior penalty discontinuous Galerkin methods",
    "descriptor": "",
    "authors": [
      "Zhaonan Dong",
      "Emmanuil H. Georgoulis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2108.02642"
  },
  {
    "id": "arXiv:2108.08139",
    "title": "Towards Mapping Control Theory and Software Engineering Properties using  Specification Patterns",
    "abstract": "Towards Mapping Control Theory and Software Engineering Properties using  Specification Patterns",
    "descriptor": "",
    "authors": [
      "Ricardo Caldas",
      "Razan Ghzouli",
      "Alessandro V. Papadopoulos",
      "Patrizio Pelliccione",
      "Danny Weyns",
      "Thorsten Berger"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2108.08139"
  },
  {
    "id": "arXiv:2108.09892",
    "title": "Dynamic Orthogonal Matching Pursuit for Sparse Data Reconstruction",
    "abstract": "Dynamic Orthogonal Matching Pursuit for Sparse Data Reconstruction",
    "descriptor": "",
    "authors": [
      "Yun-Bin Zhao",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2108.09892"
  },
  {
    "id": "arXiv:2108.12603",
    "title": "WALNUT: A Benchmark on Semi-weakly Supervised Learning for Natural  Language Understanding",
    "abstract": "Comments: Accepted to NAACL 2022 (Long Paper)",
    "descriptor": "\nComments: Accepted to NAACL 2022 (Long Paper)\n",
    "authors": [
      "Guoqing Zheng",
      "Giannis Karamanolakis",
      "Kai Shu",
      "Ahmed Hassan Awadallah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.12603"
  },
  {
    "id": "arXiv:2108.13178",
    "title": "Modular Meta-Learning for Power Control via Random Edge Graph Neural  Networks",
    "abstract": "Comments: Submitted for publication",
    "descriptor": "\nComments: Submitted for publication\n",
    "authors": [
      "Ivana Nikoloska",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.13178"
  },
  {
    "id": "arXiv:2108.13268",
    "title": "Sensor-Based Navigation Using Hierarchical Reinforcement Learning",
    "abstract": "Comments: Accepted at IAS22",
    "descriptor": "\nComments: Accepted at IAS22\n",
    "authors": [
      "Christopher Gebauer",
      "Nils Dengler",
      "Maren Bennewitz"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2108.13268"
  },
  {
    "id": "arXiv:2108.13559",
    "title": "Music Demixing Challenge 2021",
    "abstract": "Music Demixing Challenge 2021",
    "descriptor": "",
    "authors": [
      "Yuki Mitsufuji",
      "Giorgio Fabbro",
      "Stefan Uhlich",
      "Fabian-Robert St\u00f6ter",
      "Alexandre D\u00e9fossez",
      "Minseok Kim",
      "Woosung Choi",
      "Chin-Yun Yu",
      "Kin-Wai Cheuk"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2108.13559"
  },
  {
    "id": "arXiv:2109.00162",
    "title": "Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces",
    "abstract": "Comments: Version 3, 7 pages",
    "descriptor": "\nComments: Version 3, 7 pages\n",
    "authors": [
      "Hui Guo",
      "Shu Hu",
      "Xin Wang",
      "Ming-Ching Chang",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.00162"
  },
  {
    "id": "arXiv:2109.01226",
    "title": "So Cloze yet so Far: N400 Amplitude is Better Predicted by  Distributional Information than Human Predictability Judgements",
    "abstract": "Comments: Accepted",
    "descriptor": "\nComments: Accepted\n",
    "authors": [
      "James A. Michaelov",
      "Seana Coulson",
      "Benjamin K. Bergen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.01226"
  },
  {
    "id": "arXiv:2109.01904",
    "title": "Estimating Categorical Counterfactuals via Deep Twin Networks",
    "abstract": "Comments: 8 pages + appendix",
    "descriptor": "\nComments: 8 pages + appendix\n",
    "authors": [
      "Athanasios Vlontzos",
      "Bernhard Kainz",
      "Ciaran M. Gilligan-Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.01904"
  },
  {
    "id": "arXiv:2109.04307",
    "title": "OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via  Distribution Matching",
    "abstract": "Comments: ICRA2022",
    "descriptor": "\nComments: ICRA2022\n",
    "authors": [
      "Hana Hoshino",
      "Kei Ota",
      "Asako Kanezaki",
      "Rio Yokota"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.04307"
  },
  {
    "id": "arXiv:2109.04374",
    "title": "IFBiD: Inference-Free Bias Detection",
    "abstract": "Comments: AAAI Workshop on Artificial Intelligence Safety (SafeAI)",
    "descriptor": "\nComments: AAAI Workshop on Artificial Intelligence Safety (SafeAI)\n",
    "authors": [
      "Ignacio Serna",
      "Daniel DeAlcala",
      "Aythami Morales",
      "Julian Fierrez",
      "Javier Ortega-Garcia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.04374"
  },
  {
    "id": "arXiv:2109.04771",
    "title": "Learning Visual Feedback Control for Dynamic Cloth Folding",
    "abstract": "Comments: 8 pages, 7 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: 8 pages, 7 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Julius Hietala",
      "David Blanco-Mulero",
      "Gokhan Alcan",
      "Ville Kyrki"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.04771"
  },
  {
    "id": "arXiv:2109.05549",
    "title": "Federated Ensemble Model-based Reinforcement Learning in Edge Computing",
    "abstract": "Federated Ensemble Model-based Reinforcement Learning in Edge Computing",
    "descriptor": "",
    "authors": [
      "Jin Wang",
      "Jia Hu",
      "Jed Mills",
      "Geyong Min",
      "Ming Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2109.05549"
  },
  {
    "id": "arXiv:2109.05623",
    "title": "Sequential Detection and Estimation of Multipath Channel Parameters  Using Belief Propagation",
    "abstract": "Comments: 20 pages (two column), 10 figures, IEEE Transaction on Wireless Communications",
    "descriptor": "\nComments: 20 pages (two column), 10 figures, IEEE Transaction on Wireless Communications\n",
    "authors": [
      "Xuhong Li",
      "Erik Leitinger",
      "Alexander Venus",
      "Fredrik Tufvesson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2109.05623"
  },
  {
    "id": "arXiv:2109.07319",
    "title": "InceptionXML: A Lightweight Framework with Synchronized Negative  Sampling for Short Text Extreme Classification",
    "abstract": "InceptionXML: A Lightweight Framework with Synchronized Negative  Sampling for Short Text Extreme Classification",
    "descriptor": "",
    "authors": [
      "Siddhant Kharbanda",
      "Atmadeep Banerjee",
      "Akash Palrecha",
      "Devaansh Gupta",
      "Rohit Babbar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.07319"
  },
  {
    "id": "arXiv:2109.10885",
    "title": "Geographic-style maps for 2-dimensional lattices",
    "abstract": "Comments: 24 pages, 14 figures. The second version (the latest pdf at this http URL) focuses on all 2D lattices extracted from 870K+ crystals in the Cambridge Structural Database. The mathematical details are in the paper arxiv:2201.05150, whose latest version is at this http URL",
    "descriptor": "\nComments: 24 pages, 14 figures. The second version (the latest pdf at this http URL) focuses on all 2D lattices extracted from 870K+ crystals in the Cambridge Structural Database. The mathematical details are in the paper arxiv:2201.05150, whose latest version is at this http URL\n",
    "authors": [
      "Matthew Bright",
      "Andrew I Cooper",
      "Vitaliy Kurlin"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2109.10885"
  },
  {
    "id": "arXiv:2109.13046",
    "title": "The Spread of Propaganda by Coordinated Communities on Social Media",
    "abstract": "Comments: The 14th ACM Web Science Conference 2022 (WebSci '22)",
    "descriptor": "\nComments: The 14th ACM Web Science Conference 2022 (WebSci '22)\n",
    "authors": [
      "Kristina Hristakieva",
      "Stefano Cresci",
      "Giovanni Da San Martino",
      "Mauro Conti",
      "Preslav Nakov"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.13046"
  },
  {
    "id": "arXiv:2109.13910",
    "title": "Online Object Model Reconstruction and Reuse for Lifelong Improvement of  Robot Manipulation",
    "abstract": "Online Object Model Reconstruction and Reuse for Lifelong Improvement of  Robot Manipulation",
    "descriptor": "",
    "authors": [
      "Shiyang Lu",
      "Rui Wang",
      "Yinglong Miao",
      "Chaitanya Mitash",
      "Kostas Bekris"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.13910"
  },
  {
    "id": "arXiv:2109.14860",
    "title": "Physics and Equality Constrained Artificial Neural Networks: Application  to Forward and Inverse Problems with Multi-fidelity Data Fusion",
    "abstract": "Physics and Equality Constrained Artificial Neural Networks: Application  to Forward and Inverse Problems with Multi-fidelity Data Fusion",
    "descriptor": "",
    "authors": [
      "Shamsulhaq Basir",
      "Inanc Senocak"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2109.14860"
  },
  {
    "id": "arXiv:2110.01591",
    "title": "Evaluation of Two Complementary Modeling Approaches for Fiber-Reinforced  Soft Actuators",
    "abstract": "Evaluation of Two Complementary Modeling Approaches for Fiber-Reinforced  Soft Actuators",
    "descriptor": "",
    "authors": [
      "Soheil Habibian",
      "Benjamin B. Wheatley",
      "Suehye Bae",
      "Joon Shin",
      "Keith W. Buffinton"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.01591"
  },
  {
    "id": "arXiv:2110.03313",
    "title": "Distributed Methods with Compressed Communication for Solving  Variational Inequalities, with Theoretical Guarantees",
    "abstract": "Comments: Big update in v2: 71 pages, 7 algorithms, 7 theorems. New analysis for contractive compression, non-monotone analysis for unbiased and contractive compressions, partial participation",
    "descriptor": "\nComments: Big update in v2: 71 pages, 7 algorithms, 7 theorems. New analysis for contractive compression, non-monotone analysis for unbiased and contractive compressions, partial participation\n",
    "authors": [
      "Aleksandr Beznosikov",
      "Peter Richt\u00e1rik",
      "Michael Diskin",
      "Max Ryabinin",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03313"
  },
  {
    "id": "arXiv:2110.03744",
    "title": "Voice Reenactment with F0 and timing constraints and adversarial  learning of conversions",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2107.12346",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2107.12346\n",
    "authors": [
      "Frederik Bous",
      "Laurent Benaroya",
      "Nicolas Obin",
      "Axel Roebel"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03744"
  },
  {
    "id": "arXiv:2110.04193",
    "title": "On Fast Johnson-Lindenstrauss Embeddings of Compact Submanifolds of  $\\mathbb{R}^N$ with Boundary",
    "abstract": "Comments: Revision comments: Fixed typos in the text and lemma 2.1. Renumbered theorems by section",
    "descriptor": "\nComments: Revision comments: Fixed typos in the text and lemma 2.1. Renumbered theorems by section\n",
    "authors": [
      "Mark A. Iwen",
      "Benjamin Schmidt",
      "Arman Tavakoli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.04193"
  },
  {
    "id": "arXiv:2110.04598",
    "title": "Self-explaining Neural Network with Concept-based Explanations for ICU  Mortality Prediction",
    "abstract": "Comments: ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB) 14 pages, 4 figures",
    "descriptor": "\nComments: ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB) 14 pages, 4 figures\n",
    "authors": [
      "Sayantan Kumar",
      "Sean C. Yu",
      "Thomas Kannampallil",
      "Zachary Abrams",
      "Andrew Michelson",
      "Philip R.O. Payne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.04598"
  },
  {
    "id": "arXiv:2110.05025",
    "title": "Self-supervised Learning is More Robust to Dataset Imbalance",
    "abstract": "Self-supervised Learning is More Robust to Dataset Imbalance",
    "descriptor": "",
    "authors": [
      "Hong Liu",
      "Jeff Z. HaoChen",
      "Adrien Gaidon",
      "Tengyu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.05025"
  },
  {
    "id": "arXiv:2110.05461",
    "title": "Implicit gradients based conservative numerical scheme for compressible  flows",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2106.01738",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2106.01738\n",
    "authors": [
      "Amareshwara Sainadh Chamarthi",
      "Natan Hoffmann",
      "Hiroaki Nishikawa",
      "Steven H. Frankel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.05461"
  },
  {
    "id": "arXiv:2110.06324",
    "title": "A Multi-scale Time-series Dataset with Benchmark for Machine Learning in  Decarbonized Energy Grids",
    "abstract": "A Multi-scale Time-series Dataset with Benchmark for Machine Learning in  Decarbonized Energy Grids",
    "descriptor": "",
    "authors": [
      "Xiangtian Zheng",
      "Nan Xu",
      "Loc Trinh",
      "Dongqi Wu",
      "Tong Huang",
      "S. Sivaranjani",
      "Yan Liu",
      "Le Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06324"
  },
  {
    "id": "arXiv:2110.07150",
    "title": "Cross-Lingual GenQA: Open-Domain Question Answering with Answer Sentence  Generation",
    "abstract": "Cross-Lingual GenQA: Open-Domain Question Answering with Answer Sentence  Generation",
    "descriptor": "",
    "authors": [
      "Benjamin Muller",
      "Luca Soldaini",
      "Rik Koncel-Kedziorski",
      "Eric Lind",
      "Alessandro Moschitti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07150"
  },
  {
    "id": "arXiv:2110.08173",
    "title": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge  of Pre-trained Language Models",
    "abstract": "Comments: ACL 2022; code and data are released at this https URL",
    "descriptor": "\nComments: ACL 2022; code and data are released at this https URL\n",
    "authors": [
      "Zaiqiao Meng",
      "Fangyu Liu",
      "Ehsan Shareghi",
      "Yixuan Su",
      "Charlotte Collins",
      "Nigel Collier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08173"
  },
  {
    "id": "arXiv:2110.08395",
    "title": "DS-TOD: Efficient Domain Specialization for Task Oriented Dialog",
    "abstract": "Comments: Findings of ACL 2022",
    "descriptor": "\nComments: Findings of ACL 2022\n",
    "authors": [
      "Chia-Chien Hung",
      "Anne Lauscher",
      "Simone Paolo Ponzetto",
      "Goran Glava\u0161"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08395"
  },
  {
    "id": "arXiv:2110.09035",
    "title": "Edge Rewiring Goes Neural: Boosting Network Resilience without Rich  Features",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Shanchao Yang",
      "Kaili Ma",
      "Baoxiang Wang",
      "Tianshu Yu",
      "Hongyuan Zha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.09035"
  },
  {
    "id": "arXiv:2110.09722",
    "title": "Lipschitz Bandits with Batched Feedback",
    "abstract": "Comments: Largely revised, results are improved",
    "descriptor": "\nComments: Largely revised, results are improved\n",
    "authors": [
      "Yasong Feng",
      "Zengfeng Huang",
      "Tianyu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.09722"
  },
  {
    "id": "arXiv:2110.09943",
    "title": "Bayesian Active Meta-Learning for Black-Box Optimization",
    "abstract": "Comments: accepted for presentation, SPAWC 2022",
    "descriptor": "\nComments: accepted for presentation, SPAWC 2022\n",
    "authors": [
      "Ivana Nikoloska",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.09943"
  },
  {
    "id": "arXiv:2110.09968",
    "title": "Can Dynamic TDD Enabled Half-Duplex Cell-Free Massive MIMO Outperform  Full-Duplex Cellular Massive MIMO?",
    "abstract": "Comments: Accepted, IEEE Transactions on Communications",
    "descriptor": "\nComments: Accepted, IEEE Transactions on Communications\n",
    "authors": [
      "Anubhab Chowdhury",
      "Ribhu Chopra",
      "Chandra R. Murthy"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.09968"
  },
  {
    "id": "arXiv:2110.10116",
    "title": "On the Global Optimum Convergence of Momentum-based Policy Gradient",
    "abstract": "Comments: AISTATS 2022",
    "descriptor": "\nComments: AISTATS 2022\n",
    "authors": [
      "Yuhao Ding",
      "Junzi Zhang",
      "Javad Lavaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.10116"
  },
  {
    "id": "arXiv:2110.11749",
    "title": "Feature Learning and Signal Propagation in Deep Neural Networks",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Yizhang Lou",
      "Chris Mingard",
      "Yoonsoo Nam",
      "Soufiane Hayou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.11749"
  },
  {
    "id": "arXiv:2110.11929",
    "title": "Double Trouble: How to not explain a text classifier's decisions using  counterfactuals synthesized by masked language models?",
    "abstract": "Comments: 9+8 pages, 4+13 figures",
    "descriptor": "\nComments: 9+8 pages, 4+13 figures\n",
    "authors": [
      "Thang M. Pham",
      "Trung Bui",
      "Long Mai",
      "Anh Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.11929"
  },
  {
    "id": "arXiv:2110.13400",
    "title": "Scale-Free Adversarial Multi-Armed Bandit with Arbitrary Feedback Delays",
    "abstract": "Scale-Free Adversarial Multi-Armed Bandit with Arbitrary Feedback Delays",
    "descriptor": "",
    "authors": [
      "Jiatai Huang",
      "Yan Dai",
      "Longbo Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.13400"
  },
  {
    "id": "arXiv:2110.14209",
    "title": "Fast Distributed Stochastic Scheduling for A Multi-Energy Industrial  Park",
    "abstract": "Fast Distributed Stochastic Scheduling for A Multi-Energy Industrial  Park",
    "descriptor": "",
    "authors": [
      "Dafeng Zhu",
      "Bo Yang",
      "Zhaojian Wang",
      "Chengbin Ma",
      "Kai Ma",
      "Shanying Zhu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14209"
  },
  {
    "id": "arXiv:2110.14636",
    "title": "Pay attention to emoji: Feature Fusion Network with EmoGraph2vec Model  for Sentiment Analysis",
    "abstract": "Comments: Camera-ready verison accepted by ICPR 2022",
    "descriptor": "\nComments: Camera-ready verison accepted by ICPR 2022\n",
    "authors": [
      "Xiaowei Yuan",
      "Jingyuan Hu",
      "Xiaodan Zhang",
      "Honglei Lv"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14636"
  },
  {
    "id": "arXiv:2111.00537",
    "title": "A network-based approach to QAnon user dynamics and topic diversity  during the COVID-19 infodemic",
    "abstract": "Comments: accepted by APSIPA Transactions on Signal and Information Processing (Special issue: Multi-Disciplinary Dis/Misinformation Analysis and Countermeasures), 2022",
    "descriptor": "\nComments: accepted by APSIPA Transactions on Signal and Information Processing (Special issue: Multi-Disciplinary Dis/Misinformation Analysis and Countermeasures), 2022\n",
    "authors": [
      "Wentao Xu",
      "Kazutoshi Sasahara"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2111.00537"
  },
  {
    "id": "arXiv:2111.01022",
    "title": "Dropout in Training Neural Networks: Flatness of Solution and Noise  Structure",
    "abstract": "Dropout in Training Neural Networks: Flatness of Solution and Noise  Structure",
    "descriptor": "",
    "authors": [
      "Zhongwang Zhang",
      "Hanxu Zhou",
      "Zhi-Qin John Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.01022"
  },
  {
    "id": "arXiv:2111.02719",
    "title": "SPEEDEX: A Scalable, Parallelizable, and Economically Efficient Digital  EXchange",
    "abstract": "Comments: 22 pages, 5 figures",
    "descriptor": "\nComments: 22 pages, 5 figures\n",
    "authors": [
      "Geoffrey Ramseyer",
      "Ashish Goel",
      "David Mazi\u00e8res"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2111.02719"
  },
  {
    "id": "arXiv:2111.02790",
    "title": "LassoBench: A High-Dimensional Hyperparameter Optimization Benchmark  Suite for Lasso",
    "abstract": "Comments: 21 pages, 13 figures, Accepted as a conference paper at AutoML2022",
    "descriptor": "\nComments: 21 pages, 13 figures, Accepted as a conference paper at AutoML2022\n",
    "authors": [
      "Kenan \u0160ehi\u0107",
      "Alexandre Gramfort",
      "Joseph Salmon",
      "Luigi Nardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.02790"
  },
  {
    "id": "arXiv:2111.02921",
    "title": "Map-Assisted Power Allocation and Constellation Design for mmWave WDM  with OAM in Short-Range LOS Environment",
    "abstract": "Map-Assisted Power Allocation and Constellation Design for mmWave WDM  with OAM in Short-Range LOS Environment",
    "descriptor": "",
    "authors": [
      "Yuan Wang",
      "Chen Gong",
      "Zhengyuan Xu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2111.02921"
  },
  {
    "id": "arXiv:2111.02972",
    "title": "Stein Variational Probabilistic Roadmaps",
    "abstract": "Comments: Pre-print",
    "descriptor": "\nComments: Pre-print\n",
    "authors": [
      "Alexander Lambert",
      "Brian Hou",
      "Rosario Scalise",
      "Siddhartha S. Srinivasa",
      "Byron Boots"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2111.02972"
  },
  {
    "id": "arXiv:2111.03085",
    "title": "Application of Machine Learning to Sleep Stage Classification",
    "abstract": "Comments: 6 pages, IEEE Annual Conf. on Computational Science & Computational Intelligence (CSCI), December 2021",
    "descriptor": "\nComments: 6 pages, IEEE Annual Conf. on Computational Science & Computational Intelligence (CSCI), December 2021\n",
    "authors": [
      "Andrew Smith",
      "Hardik Anand",
      "Snezana Milosavljevic",
      "Katherine M. Rentschler",
      "Ana Pocivavsek",
      "Homayoun Valafar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.03085"
  },
  {
    "id": "arXiv:2111.04647",
    "title": "Composition and Style Attributes Guided Image Aesthetic Assessment",
    "abstract": "Composition and Style Attributes Guided Image Aesthetic Assessment",
    "descriptor": "",
    "authors": [
      "Luigi Celona",
      "Marco Leonardi",
      "Paolo Napoletano",
      "Alessandro Rozza"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.04647"
  },
  {
    "id": "arXiv:2111.06061",
    "title": "Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI",
    "abstract": "Comments: 20 pages, Transactions on Knowledge and Data Engineering",
    "descriptor": "\nComments: 20 pages, Transactions on Knowledge and Data Engineering\n",
    "authors": [
      "Jiangchao Yao",
      "Shengyu Zhang",
      "Yang Yao",
      "Feng Wang",
      "Jianxin Ma",
      "Jianwei Zhang",
      "Yunfei Chu",
      "Luo Ji",
      "Kunyang Jia",
      "Tao Shen",
      "Anpeng Wu",
      "Fengda Zhang",
      "Ziqi Tan",
      "Kun Kuang",
      "Chao Wu",
      "Fei Wu",
      "Jingren Zhou",
      "Hongxia Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.06061"
  },
  {
    "id": "arXiv:2111.09410",
    "title": "FedEdge: Towards Network-Accelerated Federated Learning over Wireless  Edge",
    "abstract": "Comments: 14 pages, submitted to Journal of Computer Networks 2022",
    "descriptor": "\nComments: 14 pages, submitted to Journal of Computer Networks 2022\n",
    "authors": [
      "Pinyarash Pinyoanuntapong",
      "Prabhu Janakaraj",
      "Ravikumar Balakrishnan",
      "Minwoo Lee",
      "Chen Chen",
      "Pu Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.09410"
  },
  {
    "id": "arXiv:2111.12082",
    "title": "PhysFormer: Facial Video-based Physiological Measurement with Temporal  Difference Transformer",
    "abstract": "Comments: Accepted by CVPR2022",
    "descriptor": "\nComments: Accepted by CVPR2022\n",
    "authors": [
      "Zitong Yu",
      "Yuming Shen",
      "Jingang Shi",
      "Hengshuang Zhao",
      "Philip Torr",
      "Guoying Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.12082"
  },
  {
    "id": "arXiv:2111.12128",
    "title": "On the Unreasonable Effectiveness of Feature propagation in Learning on  Graphs with Missing Node Features",
    "abstract": "On the Unreasonable Effectiveness of Feature propagation in Learning on  Graphs with Missing Node Features",
    "descriptor": "",
    "authors": [
      "Emanuele Rossi",
      "Henry Kenlay",
      "Maria I. Gorinova",
      "Benjamin Paul Chamberlain",
      "Xiaowen Dong",
      "Michael Bronstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.12128"
  },
  {
    "id": "arXiv:2111.14839",
    "title": "New PCA-based Category Encoder for Cybersecurity and Processing Data in  IoT Devices",
    "abstract": "Comments: 6 pages, 4 figures, 5 tables",
    "descriptor": "\nComments: 6 pages, 4 figures, 5 tables\n",
    "authors": [
      "Hamed Farkhari",
      "Joseanne Viana",
      "Luis Miguel Campos",
      "Pedro Sebastiao",
      "Luis Bernardo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.14839"
  },
  {
    "id": "arXiv:2112.01336",
    "title": "Simultaneously Transmitting and Reflecting Reconfigurable Intelligent  Surface Assisted NOMA Networks",
    "abstract": "Comments: 15 pages, 12 figures",
    "descriptor": "\nComments: 15 pages, 12 figures\n",
    "authors": [
      "Xinwei Yue",
      "Jin Xie",
      "Yuanwei Liu",
      "Zhihao Han",
      "Rongke Liu",
      "Zhiguo Ding"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2112.01336"
  },
  {
    "id": "arXiv:2112.02275",
    "title": "A Multi-Strategy based Pre-Training Method for Cold-Start Recommendation",
    "abstract": "A Multi-Strategy based Pre-Training Method for Cold-Start Recommendation",
    "descriptor": "",
    "authors": [
      "Bowen Hao",
      "Hongzhi Yin",
      "Jing Zhang",
      "Cuiping Li",
      "Hong Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.02275"
  },
  {
    "id": "arXiv:2112.02803",
    "title": "Multi-User Holographic MIMO Surfaces: Channel Modeling and Spectral  Efficiency Analysis",
    "abstract": "Multi-User Holographic MIMO Surfaces: Channel Modeling and Spectral  Efficiency Analysis",
    "descriptor": "",
    "authors": [
      "Li Wei",
      "Chongwen Huang",
      "George C. Alexandropoulos",
      "Wei E. I. Sha",
      "Zhaoyang Zhang",
      "Merouane Debbah",
      "Chau Yuen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.02803"
  },
  {
    "id": "arXiv:2112.03608",
    "title": "Synthesis of Pure and Impure Petri nets With Restricted  Place-environments: Complexity Issues",
    "abstract": "Synthesis of Pure and Impure Petri nets With Restricted  Place-environments: Complexity Issues",
    "descriptor": "",
    "authors": [
      "Raymond Devillers",
      "Ronny Tredup"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.03608"
  },
  {
    "id": "arXiv:2112.04415",
    "title": "On the Ergodic Mutual Information of Keyhole MIMO Channels With  Finite-Alphabet Inputs",
    "abstract": "Comments: 5 pages",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Chongjun Ouyang",
      "Ali Bereyhi",
      "Saba Asaad",
      "Ralf R. M\u00fcller",
      "Julian Cheng",
      "Hongwen Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.04415"
  },
  {
    "id": "arXiv:2112.05717",
    "title": "Discourse-Aware Soft Prompting for Text Generation",
    "abstract": "Discourse-Aware Soft Prompting for Text Generation",
    "descriptor": "",
    "authors": [
      "Marjan Ghazvininejad",
      "Vladimir Karpukhin",
      "Vera Gor",
      "Asli Celikyilmaz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.05717"
  },
  {
    "id": "arXiv:2112.06295",
    "title": "Towards More Efficient Insertion Transformer with Fractional Positional  Encoding",
    "abstract": "Towards More Efficient Insertion Transformer with Fractional Positional  Encoding",
    "descriptor": "",
    "authors": [
      "Zhisong Zhang",
      "Yizhe Zhang",
      "Bill Dolan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.06295"
  },
  {
    "id": "arXiv:2112.06351",
    "title": "Neural Point Process for Learning Spatiotemporal Event Dynamics",
    "abstract": "Neural Point Process for Learning Spatiotemporal Event Dynamics",
    "descriptor": "",
    "authors": [
      "Zihao Zhou",
      "Xingyi Yang",
      "Ryan Rossi",
      "Handong Zhao",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.06351"
  },
  {
    "id": "arXiv:2112.06395",
    "title": "Consensus-Based Distributed Filtering with Fusion Step Analysis",
    "abstract": "Consensus-Based Distributed Filtering with Fusion Step Analysis",
    "descriptor": "",
    "authors": [
      "Jiachen Qian",
      "Peihu Duan",
      "Zhisheng Duan",
      "Guanrong Chen",
      "Ling Shi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.06395"
  },
  {
    "id": "arXiv:2112.06974",
    "title": "Control-Oriented Modeling of Pipe Flow through Intersecting Pipe  Geometries",
    "abstract": "Control-Oriented Modeling of Pipe Flow through Intersecting Pipe  Geometries",
    "descriptor": "",
    "authors": [
      "Sven Br\u00fcggemann",
      "Robert R. Bitmead"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2112.06974"
  },
  {
    "id": "arXiv:2112.07080",
    "title": "Fast Footstep Planning on Uneven Terrain Using Deep Sequential Models",
    "abstract": "Comments: 6 pages, 4 figures, accepted to ICRA 2022",
    "descriptor": "\nComments: 6 pages, 4 figures, accepted to ICRA 2022\n",
    "authors": [
      "Hersh Sanghvi",
      "Camillo Jose Taylor"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2112.07080"
  },
  {
    "id": "arXiv:2112.07308",
    "title": "Conversational Search with Mixed-Initiative -- Asking Good Clarification  Questions backed-up by Passage Retrieval",
    "abstract": "Conversational Search with Mixed-Initiative -- Asking Good Clarification  Questions backed-up by Passage Retrieval",
    "descriptor": "",
    "authors": [
      "Yosi Mass",
      "Doron Cohen",
      "Asaf Yehudai",
      "David Konopnicki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.07308"
  },
  {
    "id": "arXiv:2112.07640",
    "title": "How and Why to Manipulate Your Own Agent: Modeling Games between Users  of Learning Agents",
    "abstract": "How and Why to Manipulate Your Own Agent: Modeling Games between Users  of Learning Agents",
    "descriptor": "",
    "authors": [
      "Yoav Kolumbus",
      "Noam Nisan"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.07640"
  },
  {
    "id": "arXiv:2112.07887",
    "title": "Knowledge-Rich Self-Supervision for Biomedical Entity Linking",
    "abstract": "Knowledge-Rich Self-Supervision for Biomedical Entity Linking",
    "descriptor": "",
    "authors": [
      "Sheng Zhang",
      "Hao Cheng",
      "Shikhar Vashishth",
      "Cliff Wong",
      "Jinfeng Xiao",
      "Xiaodong Liu",
      "Tristan Naumann",
      "Jianfeng Gao",
      "Hoifung Poon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.07887"
  },
  {
    "id": "arXiv:2112.08754",
    "title": "CLIN-X: pre-trained language models and a study on cross-task transfer  for concept extraction in the clinical domain",
    "abstract": "Comments: This article has been accepted for publication in Bioinformatics \\c{opyright}: 2022 The Author(s). Published by Oxford University Press. All rights reserved. The published manuscript can be found here: this https URL",
    "descriptor": "\nComments: This article has been accepted for publication in Bioinformatics \\c{opyright}: 2022 The Author(s). Published by Oxford University Press. All rights reserved. The published manuscript can be found here: this https URL\n",
    "authors": [
      "Lukas Lange",
      "Heike Adel",
      "Jannik Str\u00f6tgen",
      "Dietrich Klakow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.08754"
  },
  {
    "id": "arXiv:2112.09195",
    "title": "Mitigating the Bias of Centered Objects in Common Datasets",
    "abstract": "Mitigating the Bias of Centered Objects in Common Datasets",
    "descriptor": "",
    "authors": [
      "Gergely Szabo",
      "Andras Horvath"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.09195"
  },
  {
    "id": "arXiv:2112.11701",
    "title": "Maximum Entropy Population-Based Training for Zero-Shot Human-AI  Coordination",
    "abstract": "Comments: Accepted by NeurIPS Cooperative AI Workshop, 2021, link: this https URL Under review at a conference",
    "descriptor": "\nComments: Accepted by NeurIPS Cooperative AI Workshop, 2021, link: this https URL Under review at a conference\n",
    "authors": [
      "Rui Zhao",
      "Jinming Song",
      "Yufeng Yuan",
      "Hu Haifeng",
      "Yang Gao",
      "Yi Wu",
      "Zhongqian Sun",
      "Yang Wei"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.11701"
  },
  {
    "id": "arXiv:2112.14834",
    "title": "Training Quantized Deep Neural Networks via Cooperative Coevolution",
    "abstract": "Comments: 13 pages, 4 figures, accepted for publication of ICSI",
    "descriptor": "\nComments: 13 pages, 4 figures, accepted for publication of ICSI\n",
    "authors": [
      "Fu Peng",
      "Shengcai Liu",
      "Ning Lu",
      "Ke Tang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.14834"
  },
  {
    "id": "arXiv:2112.15113",
    "title": "Quantum secure direct communication with private dense coding using  general preshared quantum state",
    "abstract": "Comments: This paper is accepted for publicaiotn in Physical Review Applied",
    "descriptor": "\nComments: This paper is accepted for publicaiotn in Physical Review Applied\n",
    "authors": [
      "Jiawei Wu",
      "Gui-Lu Long",
      "Masahito Hayashi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2112.15113"
  },
  {
    "id": "arXiv:2112.15439",
    "title": "Facial-Sketch Synthesis: A New Challenge",
    "abstract": "Comments: MIR-minor",
    "descriptor": "\nComments: MIR-minor\n",
    "authors": [
      "Deng-Ping Fan",
      "Ziling Huang",
      "Peng Zheng",
      "Hong Liu",
      "Xuebin Qin",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.15439"
  },
  {
    "id": "arXiv:2201.03452",
    "title": "Most Clicks Problem in Lights Out",
    "abstract": "Comments: 8 pages, 3 figures, corrected mistaken result",
    "descriptor": "\nComments: 8 pages, 3 figures, corrected mistaken result\n",
    "authors": [
      "William Boyles"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2201.03452"
  },
  {
    "id": "arXiv:2201.04543",
    "title": "Eigenvalue Distribution of Large Random Matrices Arising in Deep Neural  Networks: Orthogonal Case",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2011.11439",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2011.11439\n",
    "authors": [
      "Leonid Pastur"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2201.04543"
  },
  {
    "id": "arXiv:2201.04699",
    "title": "The Recurrent Reinforcement Learning Crypto Agent",
    "abstract": "The Recurrent Reinforcement Learning Crypto Agent",
    "descriptor": "",
    "authors": [
      "Gabriel Borrageiro",
      "Nick Firoozye",
      "Paolo Barucca"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2201.04699"
  },
  {
    "id": "arXiv:2201.05072",
    "title": "SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real  Processing-In-Memory Systems",
    "abstract": "Comments: To appear in the Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS) 2022 and the ACM SIGMETRICS 2022 conference",
    "descriptor": "\nComments: To appear in the Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS) 2022 and the ACM SIGMETRICS 2022 conference\n",
    "authors": [
      "Christina Giannoula",
      "Ivan Fernandez",
      "Juan G\u00f3mez-Luna",
      "Nectarios Koziris",
      "Georgios Goumas",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2201.05072"
  },
  {
    "id": "arXiv:2201.05098",
    "title": "Neural Koopman Lyapunov Control",
    "abstract": "Neural Koopman Lyapunov Control",
    "descriptor": "",
    "authors": [
      "Vrushabh Zinage",
      "Efstathios Bakolas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.05098"
  },
  {
    "id": "arXiv:2201.06374",
    "title": "RestoreFormer: High-Quality Blind Face Restoration from Undegraded  Key-Value Pairs",
    "abstract": "Comments: Accepted by CVPR 2022",
    "descriptor": "\nComments: Accepted by CVPR 2022\n",
    "authors": [
      "Zhouxia Wang",
      "Jiawei Zhang",
      "Runjian Chen",
      "Wenping Wang",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.06374"
  },
  {
    "id": "arXiv:2201.08081",
    "title": "LEMON: Language-Based Environment Manipulation via Execution-Guided  Pre-training",
    "abstract": "LEMON: Language-Based Environment Manipulation via Execution-Guided  Pre-training",
    "descriptor": "",
    "authors": [
      "Qi Shi",
      "Qian Liu",
      "Bei Chen",
      "Yu Zhang",
      "Ting Liu",
      "Jian-Guang Lou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.08081"
  },
  {
    "id": "arXiv:2201.09332",
    "title": "Investigating Expressiveness of Transformer in Spectral Domain for  Graphs",
    "abstract": "Comments: Corrections and minor updates",
    "descriptor": "\nComments: Corrections and minor updates\n",
    "authors": [
      "Anson Bastos",
      "Abhishek Nadgeri",
      "Kuldeep Singh",
      "Hiroki Kanezashi",
      "Toyotaro Suzumura",
      "Isaiah Onando Mulang'"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.09332"
  },
  {
    "id": "arXiv:2201.09457",
    "title": "Homotopic Policy Mirror Descent: Policy Convergence, Implicit  Regularization, and Improved Sample Complexity",
    "abstract": "Comments: (1) We fixed an error in the initial analysis of the improved sample complexity result. (2) We also established new results for showing policy convergence of HPMD with decomposable distance-generating functions. As a by-product, finite-time exact convergences of HPMD with some common divergence functions are proved",
    "descriptor": "\nComments: (1) We fixed an error in the initial analysis of the improved sample complexity result. (2) We also established new results for showing policy convergence of HPMD with decomposable distance-generating functions. As a by-product, finite-time exact convergences of HPMD with some common divergence functions are proved\n",
    "authors": [
      "Yan Li",
      "Tuo Zhao",
      "Guanghui Lan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.09457"
  },
  {
    "id": "arXiv:2201.10326",
    "title": "ShapeFormer: Transformer-based Shape Completion via Sparse  Representation",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Xingguang Yan",
      "Liqiang Lin",
      "Niloy J. Mitra",
      "Dani Lischinski",
      "Daniel Cohen-Or",
      "Hui Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.10326"
  },
  {
    "id": "arXiv:2201.10890",
    "title": "One Student Knows All Experts Know: From Sparse to Dense",
    "abstract": "One Student Knows All Experts Know: From Sparse to Dense",
    "descriptor": "",
    "authors": [
      "Fuzhao Xue",
      "Xiaoxin He",
      "Xiaozhe Ren",
      "Yuxuan Lou",
      "Yang You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10890"
  },
  {
    "id": "arXiv:2201.11640",
    "title": "Towards Data-driven LQR with Koopmanizing Flows",
    "abstract": "Comments: Final version, accepted for presentation at the 6th IFAC Conference on Intelligent Control and Automation Sciences (ICONS), 2022. arXiv admin note: text overlap with arXiv:2112.04085",
    "descriptor": "\nComments: Final version, accepted for presentation at the 6th IFAC Conference on Intelligent Control and Automation Sciences (ICONS), 2022. arXiv admin note: text overlap with arXiv:2112.04085\n",
    "authors": [
      "Petar Bevanda",
      "Max Beier",
      "Shahab Heshmati-Alamdari",
      "Stefan Sosnowski",
      "Sandra Hirche"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.11640"
  },
  {
    "id": "arXiv:2201.11965",
    "title": "Provably Efficient Primal-Dual Reinforcement Learning for CMDPs with  Non-stationary Objectives and Constraints",
    "abstract": "Provably Efficient Primal-Dual Reinforcement Learning for CMDPs with  Non-stationary Objectives and Constraints",
    "descriptor": "",
    "authors": [
      "Yuhao Ding",
      "Javad Lavaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11965"
  },
  {
    "id": "arXiv:2201.12006",
    "title": "Provably Improving Expert Predictions with Prediction Sets",
    "abstract": "Provably Improving Expert Predictions with Prediction Sets",
    "descriptor": "",
    "authors": [
      "Eleni Straitouri",
      "Lequn Wang",
      "Nastaran Okati",
      "Manuel Gomez Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12006"
  },
  {
    "id": "arXiv:2201.12059",
    "title": "Learning Summary Statistics for Bayesian Inference with Autoencoders",
    "abstract": "Learning Summary Statistics for Bayesian Inference with Autoencoders",
    "descriptor": "",
    "authors": [
      "Carlo Albert",
      "Simone Ulzega",
      "Firat Ozdemir",
      "Fernando Perez-Cruz",
      "Antonietta Mira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12059"
  },
  {
    "id": "arXiv:2201.12380",
    "title": "Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
    "abstract": "Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
    "descriptor": "",
    "authors": [
      "Shichang Zhang",
      "Neil Shah",
      "Yozen Liu",
      "Yizhou Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12380"
  },
  {
    "id": "arXiv:2201.12427",
    "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
    "abstract": "Comments: Added SWU scores and more visualizations",
    "descriptor": "\nComments: Added SWU scores and more visualizations\n",
    "authors": [
      "Haonan Yu",
      "Wei Xu",
      "Haichao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12427"
  },
  {
    "id": "arXiv:2202.00455",
    "title": "HCSC: Hierarchical Contrastive Selective Coding",
    "abstract": "Comments: Accepted by CVPR 2022. arXiv v3: 800 epoch multi-crop model released; arXiv v2: more model weights released; arXiv v1: code & model weights released",
    "descriptor": "\nComments: Accepted by CVPR 2022. arXiv v3: 800 epoch multi-crop model released; arXiv v2: more model weights released; arXiv v1: code & model weights released\n",
    "authors": [
      "Yuanfan Guo",
      "Minghao Xu",
      "Jiawen Li",
      "Bingbing Ni",
      "Xuanyu Zhu",
      "Zhenbang Sun",
      "Yi Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.00455"
  },
  {
    "id": "arXiv:2202.01263",
    "title": "NoisyMix: Boosting Model Robustness to Common Corruptions",
    "abstract": "NoisyMix: Boosting Model Robustness to Common Corruptions",
    "descriptor": "",
    "authors": [
      "N. Benjamin Erichson",
      "Soon Hoe Lim",
      "Winnie Xu",
      "Francisco Utrera",
      "Ziang Cao",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.01263"
  },
  {
    "id": "arXiv:2202.01268",
    "title": "DASHA: Distributed Nonconvex Optimization with Communication  Compression, Optimal Oracle Complexity, and No Client Synchronization",
    "abstract": "DASHA: Distributed Nonconvex Optimization with Communication  Compression, Optimal Oracle Complexity, and No Client Synchronization",
    "descriptor": "",
    "authors": [
      "Alexander Tyurin",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.01268"
  },
  {
    "id": "arXiv:2202.01351",
    "title": "Technology Ethics in Action: Critical and Interdisciplinary Perspectives",
    "abstract": "Technology Ethics in Action: Critical and Interdisciplinary Perspectives",
    "descriptor": "",
    "authors": [
      "Ben Green"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.01351"
  },
  {
    "id": "arXiv:2202.01664",
    "title": "Removing Distortion Effects in Music Using Deep Neural Networks",
    "abstract": "Comments: Audio clips available at this https URL; 10 pages, 7 figures",
    "descriptor": "\nComments: Audio clips available at this https URL; 10 pages, 7 figures\n",
    "authors": [
      "Johannes Imort",
      "Giorgio Fabbro",
      "Marco A. Mart\u00ednez Ram\u00edrez",
      "Stefan Uhlich",
      "Yuichiro Koyama",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2202.01664"
  },
  {
    "id": "arXiv:2202.01770",
    "title": "Extremely Weak Supervision Inversion of Multi-physical Properties",
    "abstract": "Comments: Accepted for presentation at The International Meeting for Applied Geoscience & Energy (IMAGE)",
    "descriptor": "\nComments: Accepted for presentation at The International Meeting for Applied Geoscience & Energy (IMAGE)\n",
    "authors": [
      "Shihang Feng",
      "Peng Jin",
      "Xitong Zhang",
      "Yinpeng Chen",
      "David Alumbaugh",
      "Michael Commer",
      "Youzuo Lin"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.01770"
  },
  {
    "id": "arXiv:2202.03255",
    "title": "OCSM : Finding Overlapping Cohesive Subgraphs with Minimum Degree",
    "abstract": "OCSM : Finding Overlapping Cohesive Subgraphs with Minimum Degree",
    "descriptor": "",
    "authors": [
      "Junghoon Kim",
      "Sungsu Lim",
      "Jungeun Kim"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2202.03255"
  },
  {
    "id": "arXiv:2202.03338",
    "title": "Robust Semantic Communications Against Semantic Noise",
    "abstract": "Comments: 7 pages, 6 figures",
    "descriptor": "\nComments: 7 pages, 6 figures\n",
    "authors": [
      "Qiyu Hu",
      "Guangyi Zhang",
      "Zhijin Qin",
      "Yunlong Cai",
      "Guanding Yu",
      "Geoffrey Ye Li"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03338"
  },
  {
    "id": "arXiv:2202.03874",
    "title": "Bankruptcy Prediction via Mixing Intra-Risk and Conductive-Risk",
    "abstract": "Comments: 12 pages, 8 figures",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Yu Zhao",
      "Shaopeng Wei",
      "Yu Guo",
      "Qing Yang",
      "Qing Li",
      "Fuzhen Zhuang",
      "Ji Liu",
      "Gang Kou"
    ],
    "subjectives": [
      "Risk Management (q-fin.RM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03874"
  },
  {
    "id": "arXiv:2202.04777",
    "title": "Exact Solutions of a Deep Linear Network",
    "abstract": "Comments: theorem 3 fixed and minor changes",
    "descriptor": "\nComments: theorem 3 fixed and minor changes\n",
    "authors": [
      "Liu Ziyin",
      "Botao Li",
      "Xiangming Meng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04777"
  },
  {
    "id": "arXiv:2202.05306",
    "title": "Characterizing and overcoming the greedy nature of learning in  multi-modal deep neural networks",
    "abstract": "Characterizing and overcoming the greedy nature of learning in  multi-modal deep neural networks",
    "descriptor": "",
    "authors": [
      "Nan Wu",
      "Stanis\u0142aw Jastrz\u0119bski",
      "Kyunghyun Cho",
      "Krzysztof J. Geras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.05306"
  },
  {
    "id": "arXiv:2202.05913",
    "title": "Improved Upper Bounds for Finding Tarski Fixed Points",
    "abstract": "Comments: To appear in EC 2022",
    "descriptor": "\nComments: To appear in EC 2022\n",
    "authors": [
      "Xi Chen",
      "Yuhao Li"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2202.05913"
  },
  {
    "id": "arXiv:2202.06009",
    "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1  Adam",
    "abstract": "Maximizing Communication Efficiency for Large-scale Training via 0/1  Adam",
    "descriptor": "",
    "authors": [
      "Yucheng Lu",
      "Conglong Li",
      "Minjia Zhang",
      "Christopher De Sa",
      "Yuxiong He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06009"
  },
  {
    "id": "arXiv:2202.06232",
    "title": "A Geometric Understanding of Natural Gradient",
    "abstract": "A Geometric Understanding of Natural Gradient",
    "descriptor": "",
    "authors": [
      "Qinxun Bai",
      "Steven Rosenberg",
      "Wei Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06232"
  },
  {
    "id": "arXiv:2202.06282",
    "title": "Distributed Periodic Event-triggered Control of Nonlinear Multi-Agent  Systems",
    "abstract": "Comments: To appear in the proceedings of NecSys2022",
    "descriptor": "\nComments: To appear in the proceedings of NecSys2022\n",
    "authors": [
      "Koen J. A. Scheres",
      "Victor S. Dolk",
      "Michelle S. Chong",
      "Romain Postoyan",
      "W. P. Maurice H. Heemels"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.06282"
  },
  {
    "id": "arXiv:2202.06570",
    "title": "Anytime Capacity Expansion in Medical Residency Match by Monte Carlo  Tree Search",
    "abstract": "Anytime Capacity Expansion in Medical Residency Match by Monte Carlo  Tree Search",
    "descriptor": "",
    "authors": [
      "Kenshi Abe",
      "Junpei Komiyama",
      "Atsushi Iwasaki"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.06570"
  },
  {
    "id": "arXiv:2202.07549",
    "title": "Robust Multi-Objective Bayesian Optimization Under Input Noise",
    "abstract": "Comments: To appear at ICML 2022. 35 pages. Code is available at this https URL",
    "descriptor": "\nComments: To appear at ICML 2022. 35 pages. Code is available at this https URL\n",
    "authors": [
      "Samuel Daulton",
      "Sait Cakmak",
      "Maximilian Balandat",
      "Michael A. Osborne",
      "Enlu Zhou",
      "Eytan Bakshy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.07549"
  },
  {
    "id": "arXiv:2202.07614",
    "title": "Judging a socially assistive robot (SAR) by its cover; The effect of  body structure, outline, and color on users' perception",
    "abstract": "Comments: Submitted to Transactions on Human-Robot Interaction",
    "descriptor": "\nComments: Submitted to Transactions on Human-Robot Interaction\n",
    "authors": [
      "Ela Liberman-Pincu",
      "Yisrael Parmet",
      "Tal Oron-Gilad"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2202.07614"
  },
  {
    "id": "arXiv:2202.09407",
    "title": "Blockchain Driven Privacy Preserving Contact Tracing Framework in  Pandemics",
    "abstract": "Blockchain Driven Privacy Preserving Contact Tracing Framework in  Pandemics",
    "descriptor": "",
    "authors": [
      "Xiao Li",
      "Weili Wu",
      "Tiantian Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2202.09407"
  },
  {
    "id": "arXiv:2202.09497",
    "title": "Gradient Estimation with Discrete Stein Operators",
    "abstract": "Gradient Estimation with Discrete Stein Operators",
    "descriptor": "",
    "authors": [
      "Jiaxin Shi",
      "Yuhao Zhou",
      "Jessica Hwang",
      "Michalis K. Titsias",
      "Lester Mackey"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.09497"
  },
  {
    "id": "arXiv:2202.09661",
    "title": "Detection of Stealthy Adversaries for Networked Unmanned Aerial  Vehicles*",
    "abstract": "Comments: to appear at the 2022 Int'l Conference on Unmanned Aircraft Systems (ICUAS)",
    "descriptor": "\nComments: to appear at the 2022 Int'l Conference on Unmanned Aircraft Systems (ICUAS)\n",
    "authors": [
      "Mohammad Bahrami",
      "Hamidreza Jafarnejadsani"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.09661"
  },
  {
    "id": "arXiv:2202.10419",
    "title": "Interpreting Language Models with Contrastive Explanations",
    "abstract": "Interpreting Language Models with Contrastive Explanations",
    "descriptor": "",
    "authors": [
      "Kayo Yin",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10419"
  },
  {
    "id": "arXiv:2202.11094",
    "title": "GroupViT: Semantic Segmentation Emerges from Text Supervision",
    "abstract": "Comments: CVPR 2022. Project page and code: this https URL",
    "descriptor": "\nComments: CVPR 2022. Project page and code: this https URL\n",
    "authors": [
      "Jiarui Xu",
      "Shalini De Mello",
      "Sifei Liu",
      "Wonmin Byeon",
      "Thomas Breuel",
      "Jan Kautz",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.11094"
  },
  {
    "id": "arXiv:2202.11911",
    "title": "When Transformer Meets Robotic Grasping: Exploits Context for Efficient  Grasp Detection",
    "abstract": "When Transformer Meets Robotic Grasping: Exploits Context for Efficient  Grasp Detection",
    "descriptor": "",
    "authors": [
      "Shaochen Wang",
      "Zhangli Zhou",
      "Zhen Kan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.11911"
  },
  {
    "id": "arXiv:2202.12328",
    "title": "Cutting Some Slack for SGD with Adaptive Polyak Stepsizes",
    "abstract": "Comments: 48 pages, 7 figures",
    "descriptor": "\nComments: 48 pages, 7 figures\n",
    "authors": [
      "Robert M. Gower",
      "Mathieu Blondel",
      "Nidham Gazagnadou",
      "Fabian Pedregosa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12328"
  },
  {
    "id": "arXiv:2202.12431",
    "title": "Thompson Sampling with Unrestricted Delays",
    "abstract": "Thompson Sampling with Unrestricted Delays",
    "descriptor": "",
    "authors": [
      "Han Wu",
      "Stefan Wager"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2202.12431"
  },
  {
    "id": "arXiv:2202.13013",
    "title": "Sign and Basis Invariant Networks for Spectral Graph Representation  Learning",
    "abstract": "Comments: 40 pages. New experiments",
    "descriptor": "\nComments: 40 pages. New experiments\n",
    "authors": [
      "Derek Lim",
      "Joshua Robinson",
      "Lingxiao Zhao",
      "Tess Smidt",
      "Suvrit Sra",
      "Haggai Maron",
      "Stefanie Jegelka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.13013"
  },
  {
    "id": "arXiv:2202.13846",
    "title": "Improved bounds for acyclic coloring parameters",
    "abstract": "Comments: Lemma 4 is wrong",
    "descriptor": "\nComments: Lemma 4 is wrong\n",
    "authors": [
      "Lefteris Kirousis",
      "John Livieratos"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2202.13846"
  },
  {
    "id": "arXiv:2203.00131",
    "title": "A Data-scalable Transformer for Medical Image Segmentation:  Architecture, Model Efficiency, and Benchmark",
    "abstract": "A Data-scalable Transformer for Medical Image Segmentation:  Architecture, Model Efficiency, and Benchmark",
    "descriptor": "",
    "authors": [
      "Yunhe Gao",
      "Mu Zhou",
      "Di Liu",
      "Zhennan Yan",
      "Shaoting Zhang",
      "Dimitris N. Metaxas"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.00131"
  },
  {
    "id": "arXiv:2203.00246",
    "title": "An Information-Theoretic Framework for Deep Learning",
    "abstract": "An Information-Theoretic Framework for Deep Learning",
    "descriptor": "",
    "authors": [
      "Hong Jun Jeon",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.00246"
  },
  {
    "id": "arXiv:2203.00936",
    "title": "Continual Learning of Multi-modal Dynamics with External Memory",
    "abstract": "Continual Learning of Multi-modal Dynamics with External Memory",
    "descriptor": "",
    "authors": [
      "Abdullah Akg\u00fcl",
      "Gozde Unal",
      "Melih Kandemir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.00936"
  },
  {
    "id": "arXiv:2203.01447",
    "title": "Learning Stochastic Parametric Differentiable Predictive Control  Policies",
    "abstract": "Comments: Full version for the paper accepted at the 10th IFAC Symposium on Robust Control Design (ROCOND) 2022",
    "descriptor": "\nComments: Full version for the paper accepted at the 10th IFAC Symposium on Robust Control Design (ROCOND) 2022\n",
    "authors": [
      "J\u00e1n Drgo\u0148a",
      "Sayak Mukherjee",
      "Aaron Tuor",
      "Mahantesh Halappanavar",
      "Draguna Vrabie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.01447"
  },
  {
    "id": "arXiv:2203.01629",
    "title": "Learning Selection Bias and Group Importance: Differentiable  Reparameterization for the Hypergeometric Distribution",
    "abstract": "Learning Selection Bias and Group Importance: Differentiable  Reparameterization for the Hypergeometric Distribution",
    "descriptor": "",
    "authors": [
      "Thomas M. Sutter",
      "Laura Manduchi",
      "Alain Ryser",
      "Julia E. Vogt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.01629"
  },
  {
    "id": "arXiv:2203.02721",
    "title": "Consistent Representation Learning for Continual Relation Extraction",
    "abstract": "Comments: Accepted to Findings of ACL 2022",
    "descriptor": "\nComments: Accepted to Findings of ACL 2022\n",
    "authors": [
      "Kang Zhao",
      "Hua Xu",
      "Jiangong Yang",
      "Kai Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.02721"
  },
  {
    "id": "arXiv:2203.03358",
    "title": "Turbocharging Heuristics for Weak Coloring Numbers",
    "abstract": "Comments: 25 pages, 15 figures",
    "descriptor": "\nComments: 25 pages, 15 figures\n",
    "authors": [
      "Alexander Dobler",
      "Manuel Sorge",
      "Ana\u00efs Villedieu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2203.03358"
  },
  {
    "id": "arXiv:2203.05056",
    "title": "SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for  Autonomous Driving",
    "abstract": "SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for  Autonomous Driving",
    "descriptor": "",
    "authors": [
      "Ahmed Rida Sekkat",
      "Yohan Dupuis",
      "Varun Ravi Kumar",
      "Hazem Rashed",
      "Senthil Yogamani",
      "Pascal Vasseur",
      "Paul Honeine"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.05056"
  },
  {
    "id": "arXiv:2203.05115",
    "title": "Internet-augmented language models through few-shot prompting for  open-domain question answering",
    "abstract": "Internet-augmented language models through few-shot prompting for  open-domain question answering",
    "descriptor": "",
    "authors": [
      "Angeliki Lazaridou",
      "Elena Gribovskaya",
      "Wojciech Stokowiec",
      "Nikolai Grigorev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.05115"
  },
  {
    "id": "arXiv:2203.05335",
    "title": "Non-generative Generalized Zero-shot Learning via Task-correlated  Disentanglement and Controllable Samples Synthesis",
    "abstract": "Comments: 12 pages, 5 figures",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Yaogong Feng",
      "Xiaowen Huang",
      "Pengbo Yang",
      "Jian Yu",
      "Jitao Sang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.05335"
  },
  {
    "id": "arXiv:2203.06125",
    "title": "Protein Representation Learning by Geometric Structure Pretraining",
    "abstract": "Protein Representation Learning by Geometric Structure Pretraining",
    "descriptor": "",
    "authors": [
      "Zuobai Zhang",
      "Minghao Xu",
      "Arian Jamasb",
      "Vijil Chenthamarakshan",
      "Aurelie Lozano",
      "Payel Das",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.06125"
  },
  {
    "id": "arXiv:2203.06147",
    "title": "Memristive, Spintronic and 2D-Materials-Based Devices to Improve and  Complement Computing Hardware",
    "abstract": "Comments: 28 pages, 7 figures",
    "descriptor": "\nComments: 28 pages, 7 figures\n",
    "authors": [
      "Dovydas Joksas",
      "AbdulAziz AlMutairi",
      "Oscar Lee",
      "Murat Cubukcu",
      "Antonio Lombardo",
      "Hidekazu Kurebayashi",
      "Anthony J. Kenyon",
      "Adnan Mehonic"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2203.06147"
  },
  {
    "id": "arXiv:2203.07172",
    "title": "RED-ACE: Robust Error Detection for ASR using Confidence Embeddings",
    "abstract": "RED-ACE: Robust Error Detection for ASR using Confidence Embeddings",
    "descriptor": "",
    "authors": [
      "Zorik Gekhman",
      "Dina Zverinski",
      "Jonathan Mallinson",
      "Genady Beryozkin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.07172"
  },
  {
    "id": "arXiv:2203.07259",
    "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for  Large Language Models",
    "abstract": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for  Large Language Models",
    "descriptor": "",
    "authors": [
      "Eldar Kurtic",
      "Daniel Campos",
      "Tuan Nguyen",
      "Elias Frantar",
      "Mark Kurtz",
      "Benjamin Fineran",
      "Michael Goin",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.07259"
  },
  {
    "id": "arXiv:2203.08013",
    "title": "End-to-End Modeling via Information Tree for One-Shot Natural Language  Spatial Video Grounding",
    "abstract": "End-to-End Modeling via Information Tree for One-Shot Natural Language  Spatial Video Grounding",
    "descriptor": "",
    "authors": [
      "Mengze Li",
      "Tianbao Wang",
      "Haoyu Zhang",
      "Shengyu Zhang",
      "Zhou Zhao",
      "Jiaxu Miao",
      "Wenqiao Zhang",
      "Wenming Tan",
      "Jin Wang",
      "Peng Wang",
      "Shiliang Pu",
      "Fei Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.08013"
  },
  {
    "id": "arXiv:2203.08263",
    "title": "Performance Comparison of Python Translators for a Multi-threaded  CPU-bound Application",
    "abstract": "Comments: In: Pesado, P., Gil, G. (eds). Computer Science - CACIC 2021. Springer Communications in Computer and Information Science, vol 1584",
    "descriptor": "\nComments: In: Pesado, P., Gil, G. (eds). Computer Science - CACIC 2021. Springer Communications in Computer and Information Science, vol 1584\n",
    "authors": [
      "Andr\u00e9s Milla",
      "Enzo Rucci"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2203.08263"
  },
  {
    "id": "arXiv:2203.08568",
    "title": "In-Context Learning for Few-Shot Dialogue State Tracking",
    "abstract": "In-Context Learning for Few-Shot Dialogue State Tracking",
    "descriptor": "",
    "authors": [
      "Yushi Hu",
      "Chia-Hsuan Lee",
      "Tianbao Xie",
      "Tao Yu",
      "Noah A. Smith",
      "Mari Ostendorf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08568"
  },
  {
    "id": "arXiv:2203.09313",
    "title": "EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with  Large-Scale Pre-Training",
    "abstract": "Comments: 12 pages, 5 figures. The code and pre-trained models are publicly available at this https URL",
    "descriptor": "\nComments: 12 pages, 5 figures. The code and pre-trained models are publicly available at this https URL\n",
    "authors": [
      "Yuxian Gu",
      "Jiaxin Wen",
      "Hao Sun",
      "Yi Song",
      "Pei Ke",
      "Chujie Zheng",
      "Zheng Zhang",
      "Jianzhu Yao",
      "Xiaoyan Zhu",
      "Jie Tang",
      "Minlie Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.09313"
  },
  {
    "id": "arXiv:2203.09855",
    "title": "Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion",
    "abstract": "Comments: 26 pages, 13 figures",
    "descriptor": "\nComments: 26 pages, 13 figures\n",
    "authors": [
      "Zhiqiang Yan",
      "Xiang Li",
      "Kun Wang",
      "Zhenyu Zhang",
      "Jun Li",
      "Jian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.09855"
  },
  {
    "id": "arXiv:2203.10623",
    "title": "Calibration of Machine Reading Systems at Scale",
    "abstract": "Comments: Accepted at ACL 2022 Findings",
    "descriptor": "\nComments: Accepted at ACL 2022 Findings\n",
    "authors": [
      "Shehzaad Dhuliawala",
      "Leonard Adolphs",
      "Rajarshi Das",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.10623"
  },
  {
    "id": "arXiv:2203.11178",
    "title": "Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance",
    "abstract": "Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance",
    "descriptor": "",
    "authors": [
      "Qinqin Yang",
      "Zi Wang",
      "Kunyuan Guo",
      "Congbo Cai",
      "Xiaobo Qu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2203.11178"
  },
  {
    "id": "arXiv:2203.11437",
    "title": "Self-Supervised Representation Learning as Multimodal Variational  Inference",
    "abstract": "Comments: 12 pages, 9 figures, work in progress",
    "descriptor": "\nComments: 12 pages, 9 figures, work in progress\n",
    "authors": [
      "Hiroki Nakamura",
      "Masashi Okada",
      "Tadahiro Taniguchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.11437"
  },
  {
    "id": "arXiv:2203.11667",
    "title": "TS-Reconfiguration of $k$-Path Vertex Covers in Caterpillars",
    "abstract": "Comments: 16 pages, 3 figures, improve some proofs and algorithms",
    "descriptor": "\nComments: 16 pages, 3 figures, improve some proofs and algorithms\n",
    "authors": [
      "Duc A. Hoang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2203.11667"
  },
  {
    "id": "arXiv:2203.12928",
    "title": "The Fixed Sub-Center: A Better Way to Capture Data Complexity",
    "abstract": "The Fixed Sub-Center: A Better Way to Capture Data Complexity",
    "descriptor": "",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.12928"
  },
  {
    "id": "arXiv:2203.13712",
    "title": "Effective and Efficient Core Computation in Signed Networks",
    "abstract": "Effective and Efficient Core Computation in Signed Networks",
    "descriptor": "",
    "authors": [
      "Junghoon Kim",
      "Sungsu Lim",
      "Jungeun Kim"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2203.13712"
  },
  {
    "id": "arXiv:2203.13778",
    "title": "L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and  BERT models",
    "abstract": "L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and  BERT models",
    "descriptor": "",
    "authors": [
      "Abhishek Velankar",
      "Hrushikesh Patil",
      "Amol Gore",
      "Shubham Salunke",
      "Raviraj Joshi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.13778"
  },
  {
    "id": "arXiv:2203.14452",
    "title": "Optimisation-free Classification and Density Estimation with Quantum  Circuits",
    "abstract": "Comments: Paper condensing experiments shown in QTML 2021",
    "descriptor": "\nComments: Paper condensing experiments shown in QTML 2021\n",
    "authors": [
      "Vladimir Vargas-Calder\u00f3n",
      "Fabio A. Gonz\u00e1lez",
      "Herbert Vinck-Posada"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.14452"
  },
  {
    "id": "arXiv:2203.14960",
    "title": "Domino: Discovering Systematic Errors with Cross-Modal Embeddings",
    "abstract": "Comments: ICLR 2022 (Oral)",
    "descriptor": "\nComments: ICLR 2022 (Oral)\n",
    "authors": [
      "Sabri Eyuboglu",
      "Maya Varma",
      "Khaled Saab",
      "Jean-Benoit Delbrouck",
      "Christopher Lee-Messer",
      "Jared Dunnmon",
      "James Zou",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.14960"
  },
  {
    "id": "arXiv:2203.16309",
    "title": "Zero-shot meta-learning for small-scale data from human subjects",
    "abstract": "Comments: 10 pages, 7 figures",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Julie Jiang",
      "Kristina Lerman",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.16309"
  },
  {
    "id": "arXiv:2203.16511",
    "title": "Super-exponential distinguishability of correlated quantum states",
    "abstract": "Comments: v2: 15 pages, the proof is extended to composite state discrimination, added discussion on the relation between super-exponential distinguishability and various notions of orthogonality of states",
    "descriptor": "\nComments: v2: 15 pages, the proof is extended to composite state discrimination, added discussion on the relation between super-exponential distinguishability and various notions of orthogonality of states\n",
    "authors": [
      "Gergely Bunth",
      "G\u00e1bor Mar\u00f3ti",
      "Mil\u00e1n Mosonyi",
      "Zolt\u00e1n Zimbor\u00e1s"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2203.16511"
  },
  {
    "id": "arXiv:2203.16983",
    "title": "Self-distillation Augmented Masked Autoencoders for Histopathological  Image Classification",
    "abstract": "Self-distillation Augmented Masked Autoencoders for Histopathological  Image Classification",
    "descriptor": "",
    "authors": [
      "Yang Luo",
      "Zhineng Chen",
      "Xieping Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.16983"
  },
  {
    "id": "arXiv:2204.00164",
    "title": "Filter-based Discriminative Autoencoders for Children Speech Recognition",
    "abstract": "Comments: Published in EUSIPCO 2022",
    "descriptor": "\nComments: Published in EUSIPCO 2022\n",
    "authors": [
      "Chiang-Lin Tai",
      "Hung-Shin Lee",
      "Yu Tsao",
      "Hsin-Min Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.00164"
  },
  {
    "id": "arXiv:2204.01205",
    "title": "Model-Parallel Fourier Neural Operators as Learned Surrogates for  Large-Scale Parametric PDEs",
    "abstract": "Model-Parallel Fourier Neural Operators as Learned Surrogates for  Large-Scale Parametric PDEs",
    "descriptor": "",
    "authors": [
      "Thomas J. Grady II",
      "Rishi Khan",
      "Mathias Louboutin",
      "Ziyi Yin",
      "Philipp A. Witte",
      "Ranveer Chandra",
      "Russell J. Hewett",
      "Felix J. Herrmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.01205"
  },
  {
    "id": "arXiv:2204.01321",
    "title": "PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking  Models",
    "abstract": "PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking  Models",
    "descriptor": "",
    "authors": [
      "Chen Wu",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Maarten de Rijke",
      "Yixing Fan",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.01321"
  },
  {
    "id": "arXiv:2204.01722",
    "title": "Performance Portable Solid Mechanics via Matrix-Free $p$-Multigrid",
    "abstract": "Performance Portable Solid Mechanics via Matrix-Free $p$-Multigrid",
    "descriptor": "",
    "authors": [
      "Jed Brown",
      "Valeria Barra",
      "Natalie Beams",
      "Leila Ghaffari",
      "Matthew Knepley",
      "William Moses",
      "Rezgar Shakeri",
      "Karen Stengel",
      "Jeremy L. Thompson",
      "Junchao Zhang"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.01722"
  },
  {
    "id": "arXiv:2204.02064",
    "title": "Persistent Kernels for Iterative Memory-bound GPU Applications",
    "abstract": "Persistent Kernels for Iterative Memory-bound GPU Applications",
    "descriptor": "",
    "authors": [
      "Lingqi Zhang",
      "Mohamed Wahib",
      "Peng Chen",
      "Jintao Meng",
      "Xiao Wang",
      "Satoshi Matsuoka"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2204.02064"
  },
  {
    "id": "arXiv:2204.02461",
    "title": "Less is More: Fairness in Wide-Area Proof-of-Work Blockchain Networks",
    "abstract": "Less is More: Fairness in Wide-Area Proof-of-Work Blockchain Networks",
    "descriptor": "",
    "authors": [
      "Yifan Mao",
      "Shaileshh Bojja Venkatakrishnan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2204.02461"
  },
  {
    "id": "arXiv:2204.03197",
    "title": "MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and  Reconstruction for Complex Missing",
    "abstract": "Comments: This work has been submitted to journal for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to journal for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Yimin Dou",
      "Kewen Li",
      "Hongjie Duan",
      "Timing Li",
      "Lin Dong",
      "Zongchao Huang"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.03197"
  },
  {
    "id": "arXiv:2204.03354",
    "title": "Predictive coding and stochastic resonance as fundamental principles of  auditory perception",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2010.01914",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2010.01914\n",
    "authors": [
      "Achim Schilling",
      "William Sedley",
      "Richard Gerum",
      "Claus Metzner",
      "Konstantin Tziridis",
      "Andreas Maier",
      "Holger Schulze",
      "Fan-Gang Zeng",
      "Karl J. Friston",
      "Patrick Krauss"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.03354"
  },
  {
    "id": "arXiv:2204.03410",
    "title": "Incremental Prototype Prompt-tuning with Pre-trained Representation for  Class Incremental Learning",
    "abstract": "Incremental Prototype Prompt-tuning with Pre-trained Representation for  Class Incremental Learning",
    "descriptor": "",
    "authors": [
      "Jieren Deng",
      "Jianhua Hu",
      "Haojian Zhang",
      "Yunkuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.03410"
  },
  {
    "id": "arXiv:2204.04090",
    "title": "Neural Tangent Generalization Attacks",
    "abstract": "Neural Tangent Generalization Attacks",
    "descriptor": "",
    "authors": [
      "Chia-Hung Yuan",
      "Shan-Hung Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.04090"
  },
  {
    "id": "arXiv:2204.05125",
    "title": "ESCM$^2$: Entire Space Counterfactual Multi-Task Model for Post-Click  Conversion Rate Estimation",
    "abstract": "ESCM$^2$: Entire Space Counterfactual Multi-Task Model for Post-Click  Conversion Rate Estimation",
    "descriptor": "",
    "authors": [
      "Hao Wang",
      "Tai-Wei Chang",
      "Tianqiao Liu",
      "Jianmin Huang",
      "Zhichao Chen",
      "Chao Yu",
      "Ruopeng Li",
      "Wei Chu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.05125"
  },
  {
    "id": "arXiv:2204.05164",
    "title": "Generative Biomedical Entity Linking via Knowledge Base-Guided  Pre-training and Synonyms-Aware Fine-tuning",
    "abstract": "Comments: Accepted by NAACL 2022",
    "descriptor": "\nComments: Accepted by NAACL 2022\n",
    "authors": [
      "Hongyi Yuan",
      "Zheng Yuan",
      "Sheng Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.05164"
  },
  {
    "id": "arXiv:2204.05426",
    "title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors",
    "abstract": "Comments: Accepted in ACL Main 2022",
    "descriptor": "\nComments: Accepted in ACL Main 2022\n",
    "authors": [
      "Anubrata Das",
      "Chitrank Gupta",
      "Venelin Kovatchev",
      "Matthew Lease",
      "Junyi Jessy Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2204.05426"
  },
  {
    "id": "arXiv:2204.05453",
    "title": "Glass Segmentation with RGB-Thermal Image Pairs",
    "abstract": "Glass Segmentation with RGB-Thermal Image Pairs",
    "descriptor": "",
    "authors": [
      "Dong Huo",
      "Jian Wang",
      "Yiming Qian",
      "Yee-Hong Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.05453"
  },
  {
    "id": "arXiv:2204.06115",
    "title": "Integrating Distributed Energy Resources: Optimal Prosumer Decisions and  Impacts of Net Metering Tariffs",
    "abstract": "Comments: 20 pages, 11 figures, 6 tables",
    "descriptor": "\nComments: 20 pages, 11 figures, 6 tables\n",
    "authors": [
      "Ahmed S. Alahmed",
      "Lang Tong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2204.06115"
  },
  {
    "id": "arXiv:2204.07050",
    "title": "Recent Advances and New Frontiers in Spiking Neural Networks",
    "abstract": "Comments: Accepted at IJCAI2022",
    "descriptor": "\nComments: Accepted at IJCAI2022\n",
    "authors": [
      "Duzhen Zhang",
      "Tielin Zhang",
      "Shuncheng Jia",
      "Qingyu Wang",
      "Bo Xu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.07050"
  },
  {
    "id": "arXiv:2204.07554",
    "title": "Efficient Architecture Search for Diverse Tasks",
    "abstract": "Comments: Code available at this https URL",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Junhong Shen",
      "Mikhail Khodak",
      "Ameet Talwalkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.07554"
  },
  {
    "id": "arXiv:2204.07714",
    "title": "Pushing the Performance Limit of Scene Text Recognizer without Human  Annotation",
    "abstract": "Comments: 10 pages, 5 figures, accepted by CVPR-2022",
    "descriptor": "\nComments: 10 pages, 5 figures, accepted by CVPR-2022\n",
    "authors": [
      "Caiyuan Zheng",
      "Hui Li",
      "Seon-Min Rhee",
      "Seungju Han",
      "Jae-Joon Han",
      "Peng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.07714"
  },
  {
    "id": "arXiv:2204.08772",
    "title": "Strategies for Asymptotic Normalization",
    "abstract": "Comments: FSCD 2022",
    "descriptor": "\nComments: FSCD 2022\n",
    "authors": [
      "Claudia Faggian",
      "Giulio Guerrieri"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2204.08772"
  },
  {
    "id": "arXiv:2204.09655",
    "title": "Syntax-informed Question Answering with Heterogeneous Graph Transformer",
    "abstract": "Syntax-informed Question Answering with Heterogeneous Graph Transformer",
    "descriptor": "",
    "authors": [
      "Fangyi Zhu",
      "Lok You Tan",
      "See-Kiong Ng",
      "St\u00e9phane Bressan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.09655"
  },
  {
    "id": "arXiv:2204.09720",
    "title": "Vehicle Models and Optimal Control on a Nonplanar Surface",
    "abstract": "Vehicle Models and Optimal Control on a Nonplanar Surface",
    "descriptor": "",
    "authors": [
      "Thomas Fork",
      "H. Eric Tseng",
      "Francesco Borrelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.09720"
  },
  {
    "id": "arXiv:2204.10011",
    "title": "MedFACT: Modeling Medical Feature Correlations in Patient Health  Representation Learning via Feature Clustering",
    "abstract": "MedFACT: Modeling Medical Feature Correlations in Patient Health  Representation Learning via Feature Clustering",
    "descriptor": "",
    "authors": [
      "Xinyu Ma",
      "Xu Chu",
      "Yasha Wang",
      "Hailong Yu",
      "Liantao Ma",
      "Wen Tang",
      "Junfeng Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.10011"
  },
  {
    "id": "arXiv:2204.10085",
    "title": "Forgetting Prevention for Cross-regional Fraud Detection with  Heterogeneous Trade Graph",
    "abstract": "Forgetting Prevention for Cross-regional Fraud Detection with  Heterogeneous Trade Graph",
    "descriptor": "",
    "authors": [
      "Yujie Li",
      "Yuxuan Yang",
      "Xin Yang",
      "Qiang Gao",
      "Fan Zhou"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2204.10085"
  },
  {
    "id": "arXiv:2204.10305",
    "title": "People are not coins. Morally distinct types of predictions necessitate  different fairness constraints",
    "abstract": "People are not coins. Morally distinct types of predictions necessitate  different fairness constraints",
    "descriptor": "",
    "authors": [
      "Eleonora Vigano'",
      "Corinna Hertweck",
      "Christoph Heitz",
      "Michele Loi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2204.10305"
  },
  {
    "id": "arXiv:2204.10446",
    "title": "Overtaking Maneuvers on a Nonplanar Racetrack",
    "abstract": "Overtaking Maneuvers on a Nonplanar Racetrack",
    "descriptor": "",
    "authors": [
      "Thomas Fork",
      "H. Eric Tseng",
      "Francesco Borrelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.10446"
  },
  {
    "id": "arXiv:2204.10826",
    "title": "A Fully-autonomous Framework of Unmanned Surface Vehicles in Maritime  Environments using Gaussian Process Motion Planning",
    "abstract": "Comments: 17 pages, 15 figures",
    "descriptor": "\nComments: 17 pages, 15 figures\n",
    "authors": [
      "Jiawei Meng",
      "Ankita Humne",
      "Richard Bucknall",
      "Brendan Englot",
      "Yuanchang Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.10826"
  },
  {
    "id": "arXiv:2204.11038",
    "title": "Dimension free non-asymptotic bounds on the accuracy of high dimensional  Laplace approximation",
    "abstract": "Dimension free non-asymptotic bounds on the accuracy of high dimensional  Laplace approximation",
    "descriptor": "",
    "authors": [
      "Vladimir Spokoiny"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.11038"
  },
  {
    "id": "arXiv:2204.11467",
    "title": "Local Hypergraph-based Nested Named Entity Recognition as Query-based  Sequence Labeling",
    "abstract": "Local Hypergraph-based Nested Named Entity Recognition as Query-based  Sequence Labeling",
    "descriptor": "",
    "authors": [
      "Yukun Yan",
      "Sen Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.11467"
  },
  {
    "id": "arXiv:2204.11897",
    "title": "Reinforcement Teaching",
    "abstract": "Comments: First two authors contributed equally",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Alex Lewandowski",
      "Calarina Muslimani",
      "Dale Schuurmans",
      "Matthew E. Taylor",
      "Jun Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.11897"
  },
  {
    "id": "arXiv:2204.12071",
    "title": "Modeling the Noticeability of User-Avatar Movement Inconsistency for  Sense of Body Ownership Intervention",
    "abstract": "Comments: To be published in IMWUT'22. 23 pages without references, 15 figures, and 2 tables",
    "descriptor": "\nComments: To be published in IMWUT'22. 23 pages without references, 15 figures, and 2 tables\n",
    "authors": [
      "Zhipeng Li",
      "Yu Jiang",
      "Yihao Zhu",
      "Ruijia Chen",
      "Ruolin Wang",
      "Yuntao Wang",
      "Yukang Yan",
      "Yuanchun Shi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2204.12071"
  },
  {
    "id": "arXiv:2204.12150",
    "title": "Where and What: Driver Attention-based Object Detection",
    "abstract": "Comments: 22 pages",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Yao Rong",
      "Naemi-Rebecca Kassautzki",
      "Wolfgang Fuhl",
      "Enkelejda Kasneci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2204.12150"
  },
  {
    "id": "arXiv:2204.12568",
    "title": "Toward Policy Explanations for Multi-Agent Reinforcement Learning",
    "abstract": "Comments: 9 pages, 3 figures, 2 tables, 2 algorithms, IJCAI 2022",
    "descriptor": "\nComments: 9 pages, 3 figures, 2 tables, 2 algorithms, IJCAI 2022\n",
    "authors": [
      "Kayla Boggess",
      "Sarit Kraus",
      "Lu Feng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.12568"
  },
  {
    "id": "arXiv:2204.13324",
    "title": "Controllable Image Captioning",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:1908.11782, arXiv:2107.14178 by other authors",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1908.11782, arXiv:2107.14178 by other authors\n",
    "authors": [
      "Luka Maxwell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.13324"
  },
  {
    "id": "arXiv:2204.13599",
    "title": "Signal Recovery with Non-Expansive Generative Network Priors",
    "abstract": "Signal Recovery with Non-Expansive Generative Network Priors",
    "descriptor": "",
    "authors": [
      "Jorio Cocola"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.13599"
  },
  {
    "id": "arXiv:2204.14000",
    "title": "Robust Solutions for Multi-Defender Stackelberg Security Games",
    "abstract": "Robust Solutions for Multi-Defender Stackelberg Security Games",
    "descriptor": "",
    "authors": [
      "Dolev Mutzari",
      "Yonatan Aumann",
      "Sarit Kraus"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.14000"
  },
  {
    "id": "arXiv:2205.00159",
    "title": "SVTR: Scene Text Recognition with a Single Visual Model",
    "abstract": "Comments: Accepted by IJCAI 2022",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Yongkun Du",
      "Zhineng Chen",
      "Caiyan Jia",
      "Xiaoting Yin",
      "Tianlun Zheng",
      "Chenxia Li",
      "Yuning Du",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00159"
  },
  {
    "id": "arXiv:2205.00208",
    "title": "Security and Privacy in Virtual Reality - A Literature Survey",
    "abstract": "Comments: 15 pages, 3 figures, 4 tables",
    "descriptor": "\nComments: 15 pages, 3 figures, 4 tables\n",
    "authors": [
      "Alberto Giaretta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.00208"
  },
  {
    "id": "arXiv:2205.00224",
    "title": "Loss Function Entropy Regularization for Diverse Decision Boundaries",
    "abstract": "Comments: 7 pages",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Sue Sin Chong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.00224"
  },
  {
    "id": "arXiv:2205.00242",
    "title": "Approximating Permutations with Neural Network Components for Travelling  Photographer Problem",
    "abstract": "Comments: 11 pages",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Sue Sin Chong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.00242"
  },
  {
    "id": "arXiv:2205.00904",
    "title": "Positive-Unlabeled Learning with Adversarial Data Augmentation for  Knowledge Graph Completion",
    "abstract": "Comments: Accepted by IJCAI 2022",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Zhenwei Tang",
      "Shichao Pei",
      "Zhao Zhang",
      "Yongchun Zhu",
      "Fuzhen Zhuang",
      "Robert Hoehndorf",
      "Xiangliang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.00904"
  },
  {
    "id": "arXiv:2205.00976",
    "title": "Knowledge Graph Contrastive Learning for Recommendation",
    "abstract": "Comments: This paper has been published as a full paper at SIGIR 2022",
    "descriptor": "\nComments: This paper has been published as a full paper at SIGIR 2022\n",
    "authors": [
      "Yuhao Yang",
      "Chao Huang",
      "Lianghao Xia",
      "Chenliang Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.00976"
  },
  {
    "id": "arXiv:2205.01052",
    "title": "HTTPA/2: a Trusted End-to-End Protocol for Web Services",
    "abstract": "Comments: 23 pages, 6 figures",
    "descriptor": "\nComments: 23 pages, 6 figures\n",
    "authors": [
      "Gordon King",
      "Hans Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.01052"
  },
  {
    "id": "arXiv:2205.01550",
    "title": "Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution  Neural Network",
    "abstract": "Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution  Neural Network",
    "descriptor": "",
    "authors": [
      "Yunzheng Su",
      "Lei Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.01550"
  },
  {
    "id": "arXiv:2205.01586",
    "title": "Simpler is Better: off-the-shelf Continual Learning Through Pretrained  Backbones",
    "abstract": "Comments: Accepted at \"T4V: Transformers for Vision Workshop\" at CVPR2022",
    "descriptor": "\nComments: Accepted at \"T4V: Transformers for Vision Workshop\" at CVPR2022\n",
    "authors": [
      "Francesco Pelosin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.01586"
  },
  {
    "id": "arXiv:2205.01605",
    "title": "Toward Modeling Creative Processes for Algorithmic Painting",
    "abstract": "Comments: Proc. ICCC 2022",
    "descriptor": "\nComments: Proc. ICCC 2022\n",
    "authors": [
      "Aaron Hertzmann"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.01605"
  },
  {
    "id": "arXiv:2205.02383",
    "title": "Supervisor Obfuscation Against Covert Actuator Attackers",
    "abstract": "Supervisor Obfuscation Against Covert Actuator Attackers",
    "descriptor": "",
    "authors": [
      "Ruochen Tai",
      "Liyong Lin",
      "Rong Su"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.02383"
  },
  {
    "id": "arXiv:2205.02397",
    "title": "Compressive Ptychography using Deep Image and Generative Priors",
    "abstract": "Compressive Ptychography using Deep Image and Generative Priors",
    "descriptor": "",
    "authors": [
      "Semih Barutcu",
      "Do\u011fa G\u00fcrsoy",
      "Aggelos K. Katsaggelos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.02397"
  },
  {
    "id": "arXiv:2205.02490",
    "title": "FastRE: Towards Fast Relation Extraction with Convolutional Encoder and  Improved Cascade Binary Tagging Framework",
    "abstract": "Comments: Accepted to IJCAI-ECAI 2022",
    "descriptor": "\nComments: Accepted to IJCAI-ECAI 2022\n",
    "authors": [
      "Guozheng Li",
      "Xu Chen",
      "Peng Wang",
      "Jiafeng Xie",
      "Qiqing Luo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02490"
  },
  {
    "id": "arXiv:2205.02973",
    "title": "Large Scale Transfer Learning for Differentially Private Image  Classification",
    "abstract": "Large Scale Transfer Learning for Differentially Private Image  Classification",
    "descriptor": "",
    "authors": [
      "Harsh Mehta",
      "Abhradeep Thakurta",
      "Alexey Kurakin",
      "Ashok Cutkosky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.02973"
  },
  {
    "id": "arXiv:2205.03835",
    "title": "On the Use of BERT for Automated Essay Scoring: Joint Learning of  Multi-Scale Essay Representation",
    "abstract": "Comments: Accepted to NAACL 2022 as a long paper",
    "descriptor": "\nComments: Accepted to NAACL 2022 as a long paper\n",
    "authors": [
      "Yongjie Wang",
      "Chuan Wang",
      "Ruobing Li",
      "Hui Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.03835"
  },
  {
    "id": "arXiv:2205.03966",
    "title": "Chart Question Answering: State of the Art and Future Directions",
    "abstract": "Chart Question Answering: State of the Art and Future Directions",
    "descriptor": "",
    "authors": [
      "Enamul Hoque",
      "Parsa Kavehzadeh",
      "Ahmed Masry"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.03966"
  },
  {
    "id": "arXiv:2205.04069",
    "title": "Ultra-log-concavity and discrete degrees of freedom",
    "abstract": "Comments: First part of the previous version was removed as it turned out that the results are known due to the work Y. Yu, On the Entropy of Compound Distributions on Nonnegative Integers, IEEE Trans. Inf. Theory, Vol. 55, no. 8 (2009), 3645--3650",
    "descriptor": "\nComments: First part of the previous version was removed as it turned out that the results are known due to the work Y. Yu, On the Entropy of Compound Distributions on Nonnegative Integers, IEEE Trans. Inf. Theory, Vol. 55, no. 8 (2009), 3645--3650\n",
    "authors": [
      "Piotr Nayar",
      "Semen S\u0142obodianiuk"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.04069"
  },
  {
    "id": "arXiv:2205.04326",
    "title": "HierAttn: Effectively Learn Representations from Stage Attention and  Branch Attention for Skin Lesions Diagnosis",
    "abstract": "Comments: The code is available at this https URL",
    "descriptor": "\nComments: The code is available at this https URL\n",
    "authors": [
      "Wei Dai",
      "Rui Liu",
      "Tianyi Wu",
      "Min Wang",
      "Jianqin Yin",
      "Jun Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.04326"
  },
  {
    "id": "arXiv:2205.05055",
    "title": "Data Distributional Properties Drive Emergent In-Context Learning in  Transformers",
    "abstract": "Data Distributional Properties Drive Emergent In-Context Learning in  Transformers",
    "descriptor": "",
    "authors": [
      "Stephanie C.Y. Chan",
      "Adam Santoro",
      "Andrew K. Lampinen",
      "Jane X. Wang",
      "Aaditya Singh",
      "Pierre H. Richemond",
      "Jay McClelland",
      "Felix Hill"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05055"
  },
  {
    "id": "arXiv:2205.05166",
    "title": "Soft Robotic Mannequin: Design and Algorithm for Deformation Control",
    "abstract": "Soft Robotic Mannequin: Design and Algorithm for Deformation Control",
    "descriptor": "",
    "authors": [
      "Yingjun Tian",
      "Guoxin Fang",
      "Justas Petrulis",
      "Andrew Weightman",
      "Charlie C.L. Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.05166"
  },
  {
    "id": "arXiv:2205.05677",
    "title": "HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense  Contact Guidance",
    "abstract": "HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense  Contact Guidance",
    "descriptor": "",
    "authors": [
      "Soshi Shimada",
      "Zhi Li",
      "Vladislav Golyanik",
      "Patrick P\u00e9rez",
      "Weipeng Xu",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.05677"
  },
  {
    "id": "arXiv:2205.06090",
    "title": "NeuralTree: A 256-Channel 0.227uJ/class Versatile Neural Activity  Classification and Closed-Loop Neuromodulation SoC",
    "abstract": "NeuralTree: A 256-Channel 0.227uJ/class Versatile Neural Activity  Classification and Closed-Loop Neuromodulation SoC",
    "descriptor": "",
    "authors": [
      "Uisub Shin",
      "Cong Ding",
      "Bingzhao Zhu",
      "Yashwanth Vyza",
      "Alix Trouillet",
      "Emilie C. M. Revol",
      "St\u00e9phanie P. Lacour",
      "Mahsa Shoaran"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.06090"
  },
  {
    "id": "arXiv:2205.06267",
    "title": "Topologically-Aware Deformation Fields for Single-View 3D Reconstruction",
    "abstract": "Comments: CVPR 2022. Website at this https URL",
    "descriptor": "\nComments: CVPR 2022. Website at this https URL\n",
    "authors": [
      "Shivam Duggal",
      "Deepak Pathak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.06267"
  },
  {
    "id": "arXiv:2205.06837",
    "title": "Strategic Latency Reduction in Blockchain Peer-to-Peer Networks",
    "abstract": "Strategic Latency Reduction in Blockchain Peer-to-Peer Networks",
    "descriptor": "",
    "authors": [
      "Weizhao Tang",
      "Lucianna Kiffer",
      "Giulia Fanti",
      "Ari Juels"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.06837"
  },
  {
    "id": "arXiv:2205.07050",
    "title": "Generalization error bounds for DECONET: a deep unfolding network for  analysis Compressive Sensing",
    "abstract": "Generalization error bounds for DECONET: a deep unfolding network for  analysis Compressive Sensing",
    "descriptor": "",
    "authors": [
      "Vasiliki Kouni"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.07050"
  },
  {
    "id": "arXiv:2205.07154",
    "title": "Evaluating Generalizability of Fine-Tuned Models for Fake News Detection",
    "abstract": "Evaluating Generalizability of Fine-Tuned Models for Fake News Detection",
    "descriptor": "",
    "authors": [
      "Abhijit Suprem",
      "Calton Pu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.07154"
  },
  {
    "id": "arXiv:2205.07266",
    "title": "Discovering the Representation Bottleneck of Graph Neural Networks from  Multi-order Interactions",
    "abstract": "Discovering the Representation Bottleneck of Graph Neural Networks from  Multi-order Interactions",
    "descriptor": "",
    "authors": [
      "Fang Wu",
      "Siyuan Li",
      "Lirong Wu",
      "Dragomir Radev",
      "Qiang Zhang",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.07266"
  },
  {
    "id": "arXiv:2205.07537",
    "title": "Problem Decomposition and Multi-shot ASP Solving for Job-shop Scheduling",
    "abstract": "Comments: Paper presented at the 38th International Conference on Logic Programming (ICLP 2022), 16 pages",
    "descriptor": "\nComments: Paper presented at the 38th International Conference on Logic Programming (ICLP 2022), 16 pages\n",
    "authors": [
      "Mohammed M. S. El-Kholany",
      "Martin Gebser",
      "Konstantin Schekotihin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.07537"
  },
  {
    "id": "arXiv:2205.08075",
    "title": "Collaborative Attention Memory Network for Video Object Segmentation",
    "abstract": "Comments: Technical Report. Proposed systems attain 6th in YouTube-VOS challenge 2021",
    "descriptor": "\nComments: Technical Report. Proposed systems attain 6th in YouTube-VOS challenge 2021\n",
    "authors": [
      "Zhixing Huang",
      "Junli Zha",
      "Fei Xie",
      "Yuwei Zheng",
      "Yuandong Zhong",
      "Jinpeng Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.08075"
  },
  {
    "id": "arXiv:2205.08095",
    "title": "Reasoning About Vectors using an SMT Theory of Sequences",
    "abstract": "Comments: IJCAR 2022",
    "descriptor": "\nComments: IJCAR 2022\n",
    "authors": [
      "Ying Sheng",
      "Andres N\u00f6tzli",
      "Andrew Reynolds",
      "Yoni Zohar",
      "David Dill",
      "Wolfgang Grieskamp",
      "Junkil Park",
      "Shaz Qadeer",
      "Clark Barrett",
      "Cesare Tinelli"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.08095"
  },
  {
    "id": "arXiv:2205.08147",
    "title": "Pairwise Comparison Network for Remote Sensing Scene Classification",
    "abstract": "Comments: 6 pages, 4 figures, published to GRSL",
    "descriptor": "\nComments: 6 pages, 4 figures, published to GRSL\n",
    "authors": [
      "Zhang Yue",
      "Zheng Xiangtao",
      "Lu Xiaoqiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.08147"
  },
  {
    "id": "arXiv:2205.08324",
    "title": "Unified Interactive Image Matting",
    "abstract": "Unified Interactive Image Matting",
    "descriptor": "",
    "authors": [
      "Stephen D.H. Yang",
      "Bin Wang",
      "Weijia Li",
      "YiQi Lin",
      "Conghui He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2205.08324"
  },
  {
    "id": "arXiv:2205.08628",
    "title": "Mechanized Analysis of Anselm's Modal Ontological Argument",
    "abstract": "Mechanized Analysis of Anselm's Modal Ontological Argument",
    "descriptor": "",
    "authors": [
      "John Rushby"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.08628"
  },
  {
    "id": "arXiv:2205.08897",
    "title": "FiLM: Frequency improved Legendre Memory Model for Long-term Time Series  Forecasting",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2201.12740",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2201.12740\n",
    "authors": [
      "Tian Zhou",
      "Ziqing Ma",
      "Xue wang",
      "Qingsong Wen",
      "Liang Sun",
      "Tao Yao",
      "Rong Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.08897"
  },
  {
    "id": "arXiv:2205.09008",
    "title": "Distributional Robustness: From Pricing to Auctions",
    "abstract": "Distributional Robustness: From Pricing to Auctions",
    "descriptor": "",
    "authors": [
      "Nir Bachrach",
      "Inbal Talgam-Cohen"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.09008"
  },
  {
    "id": "arXiv:2205.09048",
    "title": "Global Contrast Masked Autoencoders Are Powerful Pathological  Representation Learners",
    "abstract": "Global Contrast Masked Autoencoders Are Powerful Pathological  Representation Learners",
    "descriptor": "",
    "authors": [
      "Hao Quan",
      "Xingyu Li",
      "Weixing Chen",
      "Qun Bai",
      "Mingchen Zou",
      "Ruijie Yang",
      "Tingting Zheng",
      "Ruiqun Qi",
      "Xinghua Gao",
      "Xiaoyu Cui"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09048"
  },
  {
    "id": "arXiv:2205.09054",
    "title": "Position Aided Beam Prediction in the Real World: How Useful GPS  Locations Actually Are?",
    "abstract": "Comments: Submitted to IEEE. Datasets and code files are available on the DeepSense website: this https URL",
    "descriptor": "\nComments: Submitted to IEEE. Datasets and code files are available on the DeepSense website: this https URL\n",
    "authors": [
      "Jo\u00e3o Morais",
      "Arash Behboodi",
      "Hamed Pezeshki",
      "Ahmed Alkhateeb"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09054"
  },
  {
    "id": "arXiv:2205.09233",
    "title": "Rensets and Renaming-Based Recursion for Syntax with Bindings",
    "abstract": "Comments: This is an extended technical report associated to an identically titled conference paper that will appear in IJCAR 2022",
    "descriptor": "\nComments: This is an extended technical report associated to an identically titled conference paper that will appear in IJCAR 2022\n",
    "authors": [
      "Andrei Popescu"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.09233"
  },
  {
    "id": "arXiv:2205.09579",
    "title": "TRT-ViT: TensorRT-oriented Vision Transformer",
    "abstract": "TRT-ViT: TensorRT-oriented Vision Transformer",
    "descriptor": "",
    "authors": [
      "Xin Xia",
      "Jiashi Li",
      "Jie Wu",
      "Xing Wang",
      "Xuefeng Xiao",
      "Min Zheng",
      "Rui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09579"
  },
  {
    "id": "arXiv:2205.09651",
    "title": "Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT",
    "abstract": "Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT",
    "descriptor": "",
    "authors": [
      "Mustafa Jarrar",
      "Mohammed Khalilia",
      "Sana Ghanem"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09651"
  },
  {
    "id": "arXiv:2205.09732",
    "title": "Enhancing Slot Tagging with Intent Features for Task Oriented Natural  Language Understanding using BERT",
    "abstract": "Comments: 11 pages, 1 figure",
    "descriptor": "\nComments: 11 pages, 1 figure\n",
    "authors": [
      "Shruthi Hariharan",
      "Vignesh Kumar Krishnamurthy",
      "Utkarsh",
      "Jayantha Gowda Sarapanahalli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09732"
  },
  {
    "id": "arXiv:2205.09738",
    "title": "AIGenC: AI generalisation via creativity",
    "abstract": "AIGenC: AI generalisation via creativity",
    "descriptor": "",
    "authors": [
      "Corina Catarau-Cotutiu",
      "Esther Mondragon",
      "Eduardo Alonso"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09738"
  },
  {
    "id": "arXiv:2205.09787",
    "title": "Causal Discovery and Injection for Feed-Forward Neural Networks",
    "abstract": "Causal Discovery and Injection for Feed-Forward Neural Networks",
    "descriptor": "",
    "authors": [
      "Fabrizio Russo",
      "Francesca Toni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09787"
  },
  {
    "id": "arXiv:2205.09825",
    "title": "Algorithms for Weak Optimal Transport with an Application to Economics",
    "abstract": "Algorithms for Weak Optimal Transport with an Application to Economics",
    "descriptor": "",
    "authors": [
      "Fran\u00e7ois-Pierre Paty",
      "Philippe Chon\u00e9",
      "Francis Kramarz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2205.09825"
  },
  {
    "id": "arXiv:2205.09884",
    "title": "Time Series Anomaly Detection via Reinforcement Learning-Based Model  Selection",
    "abstract": "Comments: 6 pages, 3 figures, submitted to IEEE Canadian Conference on Electrical and Computer Engineering (CCECE) 2022",
    "descriptor": "\nComments: 6 pages, 3 figures, submitted to IEEE Canadian Conference on Electrical and Computer Engineering (CCECE) 2022\n",
    "authors": [
      "Jiuqi Elise Zhang",
      "Di Wu",
      "Benoit Boulet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09884"
  },
  {
    "id": "arXiv:2205.09940",
    "title": "Conformal Prediction with Temporal Quantile Adjustments",
    "abstract": "Comments: 12 pages (main paper, including references) + 11 pages (supplementary material)",
    "descriptor": "\nComments: 12 pages (main paper, including references) + 11 pages (supplementary material)\n",
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.09940"
  },
  {
    "id": "arXiv:2205.09963",
    "title": "Sample Complexity of Learning Heuristic Functions for Greedy-Best-First  and A* Search",
    "abstract": "Sample Complexity of Learning Heuristic Functions for Greedy-Best-First  and A* Search",
    "descriptor": "",
    "authors": [
      "Shinsaku Sakaue",
      "Taihei Oki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.09963"
  },
  {
    "id": "arXiv:2205.09965",
    "title": "Few-Shot Font Generation by Learning Fine-Grained Local Styles",
    "abstract": "Comments: Accepted to CVPR 2022",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Licheng Tang",
      "Yiyang Cai",
      "Jiaming Liu",
      "Zhibin Hong",
      "Mingming Gong",
      "Minhu Fan",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.09965"
  },
  {
    "id": "arXiv:2205.09978",
    "title": "HeadText: Exploring Hands-free Text Entry using Head Gestures by Motion  Sensing on a Smart Earpiece",
    "abstract": "Comments: 23 pages",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Songlin Xu",
      "Guanjie Wang",
      "Ziyuan Fang",
      "Guangwei Zhang",
      "Guangzhu Shang",
      "Rongde Lu",
      "Liqun He"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09978"
  },
  {
    "id": "arXiv:2205.10014",
    "title": "A Survey of Trustworthy Graph Learning: Reliability, Explainability, and  Privacy Protection",
    "abstract": "Comments: Preprint; Work in progress. arXiv admin note: substantial text overlap with arXiv:2202.07114",
    "descriptor": "\nComments: Preprint; Work in progress. arXiv admin note: substantial text overlap with arXiv:2202.07114\n",
    "authors": [
      "Bingzhe Wu",
      "Jintang Li",
      "Junchi Yu",
      "Yatao Bian",
      "Hengtong Zhang",
      "CHaochao Chen",
      "Chengbin Hou",
      "Guoji Fu",
      "Liang Chen",
      "Tingyang Xu",
      "Yu Rong",
      "Xiaolin Zheng",
      "Junzhou Huang",
      "Ran He",
      "Baoyuan Wu",
      "GUangyu Sun",
      "Peng Cui",
      "Zibin Zheng",
      "Zhe Liu",
      "Peilin Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10014"
  },
  {
    "id": "arXiv:2205.10101",
    "title": "MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer  with Multi-Stage Fusion",
    "abstract": "Comments: 8 pages, 4 figures",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Jing Wang",
      "Haotian Fan",
      "Xiaoxia Hou",
      "Yitian Xu",
      "Tao Li",
      "Xuechao Lu",
      "Lean Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10101"
  },
  {
    "id": "arXiv:2205.10124",
    "title": "The Fellowship of the Dyson Ring: ACT&Friends' Results and Methods for  GTOC 11",
    "abstract": "The Fellowship of the Dyson Ring: ACT&Friends' Results and Methods for  GTOC 11",
    "descriptor": "",
    "authors": [
      "Marcus M\u00e4rtens",
      "Dario Izzo",
      "Emmanuel Blazquez",
      "Moritz von Looz",
      "Pablo G\u00f3mez",
      "Anne Mergy",
      "Giacomo Acciarini",
      "Chit Hong Yam",
      "Javier Hernando Ayuso",
      "Yuri Shimane"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10124"
  },
  {
    "id": "arXiv:2205.10330",
    "title": "A Review of Safe Reinforcement Learning: Methods, Theory and  Applications",
    "abstract": "A Review of Safe Reinforcement Learning: Methods, Theory and  Applications",
    "descriptor": "",
    "authors": [
      "Shangding Gu",
      "Long Yang",
      "Yali Du",
      "Guang Chen",
      "Florian Walter",
      "Jun Wang",
      "Yaodong Yang",
      "Alois Knoll"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10330"
  }
]